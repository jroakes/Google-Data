CN110603545A - Organizing messages exchanged in a human-machine conversation with an automated assistant - Google Patents
Organizing messages exchanged in a human-machine conversation with an automated assistant Download PDFInfo
- Publication number
- CN110603545A CN110603545A CN201880027624.9A CN201880027624A CN110603545A CN 110603545 A CN110603545 A CN 110603545A CN 201880027624 A CN201880027624 A CN 201880027624A CN 110603545 A CN110603545 A CN 110603545A
- Authority
- CN
- China
- Prior art keywords
- messages
- subset
- user
- conversation
- chronological
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 claims abstract description 43
- 238000004458 analytical method Methods 0.000 claims abstract description 6
- 230000000694 effects Effects 0.000 claims description 16
- 230000015654 memory Effects 0.000 claims description 10
- 230000004044 response Effects 0.000 claims description 9
- 230000008520 organization Effects 0.000 description 17
- 238000012545 processing Methods 0.000 description 11
- 238000012552 review Methods 0.000 description 7
- 238000004891 communication Methods 0.000 description 6
- 230000003993 interaction Effects 0.000 description 6
- 238000011160 research Methods 0.000 description 6
- 239000000463 material Substances 0.000 description 5
- 230000008569 process Effects 0.000 description 5
- 230000002452 interceptive effect Effects 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 230000009471 action Effects 0.000 description 3
- 230000008901 benefit Effects 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 235000013550 pizza Nutrition 0.000 description 3
- 238000009877 rendering Methods 0.000 description 3
- 238000013475 authorization Methods 0.000 description 2
- 235000013305 food Nutrition 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000003213 activating effect Effects 0.000 description 1
- 230000000386 athletic effect Effects 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 238000013523 data management Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000010006 flight Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000009434 installation Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/04817—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance using icons
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
Abstract
Techniques for organizing messages exchanged between a user and an automated assistant into different conversations are described herein. In various implementations, a chronological record of messages exchanged as part of a human-machine conversation session between a user and an automated assistant can be analyzed. Based on the analysis, a subset of a chronological record of messages related to tasks performed by the user via the human-machine conversation session may be identified. Based on the subset and the content of the task, conversational metadata may be generated that causes the client computing device to provide selectable elements that convey the task. Selecting the selectable element may cause the client computing device to present a representation associated with at least one recorded message related to the task.
Description
Background
People may engage in human-computer conversations using interactive software applications referred to herein as "automated assistants" (also referred to as "chat robots," "interactive personal assistants," "intelligent personal assistants," "personal voice assistants," "conversation agents," etc.). For example, a person (which may be referred to as a "user" when they interact with an automated assistant) may provide commands, queries, and/or requests using spoken natural language input (i.e., utterances) that may be converted to text and then processed in some cases, and/or by providing textual (e.g., typed) natural language input. The user may engage the automated assistant in a variety of different "conversations". Each conversation may contain one or more individual messages that are semantically related to a particular topic, performance of a particular task, and the like. In many cases, the messages of a given conversation may be contained in a single human-to-machine conversation session between the user and the automated assistant. However, the messages that form the conversation may also span multiple sessions with the automated assistant.
As an example of a conversation, during a human-machine conversation session with an automated assistant, a user may submit a series of queries to an automated help laptop related to formulating a travel plan. Such queries (and responses by automated assistants) may relate to, for example, making traffic arrangements, understanding points of interest at or near a particular location, understanding activities at or near a particular location, and so forth. In some cases, a user may purchase one or more items related to their travel itinerary, such as tickets, vouchers, passes, travel-related products (e.g., athletic equipment, luggage, clothing, etc.). As another example, a user may interact with an automated assistant to query and/or respond to bills, notifications, and the like. In some cases, one or more users may interact with the automated assistant (and in some cases with each other) to plan activities such as gathering, withdrawing, and the like. Regardless of what task a user performs when interacting with the automated assistant, in many cases, the task may have consequences, such as acquiring an item, scheduling an activity, making a schedule, and so forth.
The more times a user interacts with the automated assistant, the more messages between the user and the automated assistant (and other users as the case may be) may remain in the log. If the user wishes to revisit a previous conversation with the automated assistant, the user may have to peruse such logs to find individual messages related to the previous conversation. This can be particularly difficult/tedious if the particular task performed by the user through interaction with the automated assistant occurs in the relatively distant past and/or over multiple different conversations between the user and the automated assistant. In the former case, a large number of extraneous messages may remain in the log since the user engaged the automated assistant in a previous conversation sought by the user. In the latter case, there may be many intermediate messages unrelated to the previous conversation sought by the user.
Disclosure of Invention
Techniques are described herein for organizing messages exchanged as part of a human-to-machine conversation session between a user and an automated assistant into clusters representing different conversations between the user and the automated assistant. In some implementations, different clusters/conversations may be determined (e.g., delineated) based on tasks performed by the user through interaction with the automated assistant. Additionally or alternatively, in some implementations, different clusters/conversations may be determined based on other signals, such as results of tasks performed by the user through interaction with the automated assistant, timestamps associated with respective messages (e.g., messages that are proximate in time to one another, particularly that occur within a single human-machine conversation session may be assumed to be part of the same conversation between the user and the automated assistant), topics of conversation between the user and the automated assistant, and so forth.
In various embodiments, so-called "conversation metadata" may be generated for each cluster of messages/conversations. Conversation metadata may include various information about the content of the conversation and/or the various messages that form the conversation/cluster, such as tasks performed by the user while interacting with the automated assistant, results of the tasks, topics of the conversation, one or more times associated with the conversation (e.g., when the conversation begins/ends, duration of the conversation), how many individual human-machine conversation sessions the conversation spans, other participants in the conversation are involved in addition to the particular user, who is involved, and so forth.
The conversation metadata may be generated in whole or in part on a client device operated by the user, or remotely, for example, on one or more server computers forming what is commonly referred to as a "cloud" computing system. In various implementations, conversation metadata may be used by client devices operated by users, such as smartphones, tablets, and the like, to present organized clusters of messages to users in an abbreviated manner that allows users to quickly peruse/search different conversations for a particular conversation of interest.
The manner in which the organized cluster/conversation is presented may be determined based on the conversation metadata mentioned above. For example, selectable elements may be presented (e.g., visually), and in some cases may take the form of a collared thread (collared thread) that expands when selected to provide an original message selected as part of a conversation/conversation. In some implementations, the selectable elements may convey various summary information about the conversations they represent, such as the tasks being performed (e.g., "smart light bulb research," "travel to barcelona," "cook pot," etc.), the results of the tasks (e.g., "acquisition of an item," planned activity details, etc.), potential next actions (e.g., "complete booked flight," "purchase smart light bulb," etc.), topic of the conversation (e.g., "research on george washington," "research on spain," etc.), and so forth. By presenting these selectable elements to the user in addition to or instead of presenting all past messages to the user, the user is able to quickly search for and identify conversations of interest. Moreover, the data processing burden on the computing resources implementing the process may be reduced, as it may no longer be necessary to present a complete log of earlier conversations to allow the user to perform this function. Further, a mechanism is provided to allow input via selectable elements associated with conversational metadata such that intuitive and responsive user interaction may be provided that effectively associates a user intent with underlying data. Employing selectable elements may also make more efficient use of available screen space when visually presented than when presenting an entire log of conversation.
In some embodiments, the selectable elements may be rendered by themselves without requiring the underlying individual messages that make up the cluster on which the selectable elements are based. In other embodiments, selectable elements may be displayed alongside and/or simultaneously with the underlying message. For example, when a user scrolls through a log of past messages (e.g., a record of a previous human-machine conversation session), selectable elements associated with a conversation represented in whole or in part by a currently displayed message may be provided. In some embodiments, the optional element may take the form of the message itself. For example, assume that the user selects a particular message in the past message log. Other messages that form part of the same conversation as the selected message may be highlighted or otherwise presented in a highlighted form. In some implementations, the user may then be able to "toggle" (e.g., by pressing a button, operating a scroll wheel, etc.) messages related to the same conversation, while skipping intermediate messages that do not form part of the same conversation.
In some implementations, a method performed by one or more processors is provided, comprising: analyzing, by the one or more processors, a chronological record of messages exchanged as part of one or more human-machine conversation sessions between the at least one user and the automated assistant; based on the analysis, identifying, by the one or more processors, at least a subset of the chronological record of the messages, the subset relating to tasks performed by the at least one user via the one or more human-computer conversation sessions; and generating, by one or more processors, conversation metadata associated with the subset of the chronological record of the message based on the content of the subset of the chronological record of the message and the task. In various implementations, the conversation metadata can cause a client computing device to provide, via an output device associated with the client computing device, a selectable element that conveys the task, wherein selection of the selectable element causes the client computing device to present, via the output device, a representation associated with at least one of the recorded messages related to the task.
These and other embodiments of the technology disclosed herein may optionally include one or more of the following features.
In various implementations, the method may further include identifying, by the one or more processors, a result of the task based on content of the subset of the chronological record of the message. In various embodiments, the optional elements may convey the results of the task. In various embodiments, the method may further include identifying, by the one or more processors, a next step for completing the task based on content of the subset of the chronological record of messages. In various embodiments, the optional element may convey the next step. In various embodiments, identifying a subset of the chronological record of the message may be based on the results of the task. In various embodiments, the results of the task may include the acquisition of the item. In various embodiments, a task may include an organizational activity. In various embodiments, the results of a task may include details associated with the organized activity.
In various implementations, identifying the subset of the chronological record of the messages may be based on timestamps associated with respective messages of the chronological record of the messages. In various embodiments, the selectable elements may include foldable threads that expand upon selection to provide a subset of the chronological record of the message. In various embodiments, the selectable element may include individual messages of the subset, and selection of the individual messages of the subset may cause one or more other individual messages of the subset to be presented in a first manner that is visually distinct from a second manner in which other messages of the chronological recording of the messages are presented.
In various embodiments, the representation may include an icon associated with or included in a subset of the chronological record of the message. In various embodiments, the representation includes one or more hyperlinks that may be included in a subset of the chronological record of the message. In various embodiments, the representation may comprise a subset of a chronological record of the message. In various implementations, the messages in the chronologically recorded subset of messages may be presented chronologically. In various embodiments, the messages in the subset of chronological records of the messages may be presented in order of relevance.
Additionally, some embodiments include one or more processors of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in associated memory, and wherein the instructions are configured to cause performance of any of the above-described methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to implement any of the above-described methods.
It should be appreciated that all combinations of the foregoing concepts and additional concepts discussed in greater detail herein are contemplated as part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
Drawings
FIG. 1 is a block diagram of an example environment in which embodiments disclosed herein may be implemented.
2A, 2B, 2C, and 2D illustrate example human-machine conversations between various users and an automated assistant, according to various embodiments.
Fig. 2E, 2F, and 2G illustrate additional user interfaces presented in accordance with embodiments disclosed herein.
FIG. 3 illustrates an example method for performing selected aspects of the present disclosure.
FIG. 4 illustrates an example architecture of a computing device.
Detailed Description
Turning now to FIG. 1, an example environment is illustrated in which the techniques disclosed herein may be implemented. The example environment includes a plurality of client metersComputing device 1061-NAnd an automated assistant 120. Although the automated assistant 120 is illustrated in FIG. 1 as being independent of the client computing device 1061-NHowever, in some implementations, all or aspects of the automated assistant 120 may be performed by one or more client computing devices 1061-NAnd (5) implementing. For example, the client device 1061One example of one or more aspects of the automated assistant 120 can be implemented while the client device 106NSeparate instances of these one or more aspects of the automated assistant 120 may also be implemented. By being remote from the client computing device 106 in one or more aspects of the automated assistant 1201-NIn one or more computing device implemented embodiments, the client computing device 1061-NAnd the automated assistant 120 may communicate via one or more networks, such as a Local Area Network (LAN) and/or a Wide Area Network (WAN) (e.g., the internet).
Client device 1061-NMay include, for example, one or more of the following: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a standalone interactive speaker, a so-called "smart" television, and/or a wearable apparatus comprising a user of a computing device (e.g., a watch of the user having the computing device, glasses of the user having the computing device, a virtual or augmented reality computing device). Additional and/or alternative client computing devices may be provided. In some implementations, a given user can communicate with the automated assistant 120 using multiple client computing devices that collectively form a coordinated "ecosystem" of computing devices. In some implementations, the automated assistant 120 can be thought of as "serving" that particular user, e.g., giving the automated assistant 120 enhanced access to resources (e.g., content, documents, etc.) whose access is controlled by the "served" user. However, for simplicity, some examples described herein will focus on a user operating a single client computing device 106.
Each client computing device 1061-NVarious different applications may operate, such as a message exchange client 1071-NA respective one of the. Messaging client 1071-NMay take various forms, and the forms may be at the client computing device 1061-NMay differ from one another and/or may be on the client computing device 1061-NOf a single client computing device 1061-NThe above operation is performed in various forms. In some implementations, one or more message exchange clients 1071-NMay take the form of a short messaging service ("SMS") and/or multimedia messaging service ("MMS") client, an online chat client (e.g., instant messaging software, internet relay chat or "IRC," etc.), a messaging application associated with a social network, a personal assistant messaging service dedicated to conversations with the automated assistant 120, and so forth. In some implementations, one or more message exchange clients 1071-NMay be implemented via a web page or other resource rendered by a web browser (not shown) or other application of the client computing device 106.
As described in greater detail herein, the automated assistant 120 is via one or more client devices 1061-NTo participate in a human-computer conversation session with one or more users. In some implementations, the automated assistant 120 can respond to the request by the user via the client device 1061-NThe user interface input provided by the one or more user interface input devices of one of the client devices to participate in a human-computer conversation session with the user. For example, the automated assistant 120 may respond via the client device 1061-NThe response content is generated from free-form input provided by one of the client devices. As used herein, a free-form input is an input formulated by a user and is not limited to a set of options presented for selection by the user.
In some implementations, the user interface input is explicitly directed to the automated assistant 120. For example, message exchange client 1071-NCan be a personal assistant messaging service dedicated to talking with the automated assistant 120, and can be provided via the oneThe user interface input provided by the personal assistant messaging service is automatically provided to the automated assistant 120. Also, for example, based on a particular user interface input indicating that the automated assistant 120 is to be invoked, at one or more message exchange clients 1071-NThe user interface input may be explicitly directed to the automated assistant 120. For example, a particular user interface input may be one or more typed characters (e.g., @ automated assistant), user interaction with hardware buttons and/or virtual buttons (e.g., tap, long tap), spoken commands (e.g., "Hey automated assistant"), and/or other particular user interface inputs. In some implementations, the automated assistant 120 can participate in the conversation session in response to the user interface input even when the user interface input is not explicitly directed to the automated assistant 120. For example, the automated assistant 120 may examine the content of the user interface input and participate in the conversation session in response to the presence of certain terms in the user interface input and/or based on other cues. In many implementations, the automated assistant 120 can engage in interactive voice response ("IVR") so that the user can issue commands, searches, etc., and the automated assistant can utilize natural language processing and/or one or more grammars to convert utterances into text and respond to the text accordingly.
Client computing device 1061-NAnd the automated assistant 120 may each include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. May be provided by one or more client computing devices 1061-NAnd/or the operations performed by the automated assistant 120 may be distributed across multiple computer systems. For example, the automated assistant 120 may be implemented as a computer program running on one or more computers in one or more locations coupled to each other by a network.
The automated assistant 120 can include, among other things, a natural language processor 122, a message organization module 126, and a message presentation module 128. In some implementations, one or more engines and/or modules of the automated assistant can be omitted, combined, and/or implemented in a component separate from the automated assistant 120.
As used herein, a "conversation session" may include a logically independent exchange of one or more messages between a user and the automated assistant 120. The automated assistant 120 can distinguish between multiple conversation sessions with the user based on various signals such as the passage of time between conversations, changes in user context between conversations (e.g., location, before/during/after scheduling a meeting, etc.), detection of one or more intermediate interactions between the user and the client device other than the conversation between the user and the automated assistant (e.g., the user temporarily switches applications, the user walks away and returns to a separate voice-activated speaker), locking/sleeping of the client device between sessions, changes in the client device for interacting with one or more instances of the automated assistant 120, etc.
In some implementations, when the automated assistant 120 provides a prompt requesting user feedback, the automated assistant 120 can preemptively activate one or more components configured as a client device (via which the prompt is provided) that process user interface input received in response to the prompt. For example, to be via the client device 1061Where the microphone of (a) provides a user interface input, the automated assistant 120 may provide one or more commands to cause: preemptively "turning on" the microphone (thereby preventing the need to tap an interface element or speak a "hotword" to turn on the microphone), preemptively activating the client device 1061Local speech to text processor, preemptive client device 1061Establishing a communication session with a remote speech-to-text processor, and/or at a client device 1061A graphical user interface (e.g., an interface including one or more selectable elements that may be selected to provide feedback) is rendered. This may enable user interface input to be provided and/or processed more quickly than if the component were not preemptively activated.
Processing by the natural language processor 122 of the automated assistant 120 via the client device 1061-NUser-generated natural language inputIn turn, and may generate annotation output for use by one or more other components of the automated assistant 120 (including components not shown in fig. 1). For example, the natural language processor 122 may process data received by a user via the client device 1061The one or more user interface input devices of (a) generating a natural language free form input. The generated annotation output includes one or more annotations of the natural language input, and optionally one or more (e.g., all) terms of the natural language input.
In some implementations, the natural language processor 122 is configured to recognize and annotate various grammatical information in the natural language input. For example, the natural language processor 122 may include a part-of-speech tagger configured to annotate terms with their grammatical roles. For example, a part-of-speech tagger may tag each term with its part-of-speech such as "noun", "verb", "adjective", "pronoun", and the like. Also, for example, in some implementations, the natural language processor 122 can additionally and/or alternatively include a relevance analyzer configured to determine syntactic relationships between terms in the natural language input. For example, the relevance analyzer may determine which terms modify other terms, subjects, and verbs (e.g., parse trees) of the sentence, and may make annotations of such relevance.
In some implementations, the natural language processor 122 can additionally and/or alternatively include an entity annotator configured to annotate entity references in one or more segments, such as references to people (e.g., including literary characters), organizations, locations (real and fictional), and so forth. An entity tagger may tag references to entities at a high level of granularity (e.g., to enable identification of all references to a class of entities such as a person) and/or at a low level of granularity (e.g., to enable identification of all references to a particular entity such as a particular person). The entity labeler can rely on the content of the natural language input to resolve a particular entity and/or can optionally communicate with a knowledge graph or other entity database to resolve a particular entity.
In some implementations, the natural language processor 122 can additionally and/or alternatively include a same-referenceresolver configured to group or "cluster" references to the same entity based on one or more contextual cues. For example, the term "ther" in the natural language input "I liked the previous contextual Caf we" may be parsed into "contextual Caf using the same parser as that which refers to.
In some implementations, one or more components of the natural language processor 122 may rely on annotations from one or more other components of the natural language processor 122. For example, in some implementations, a designated entity annotator can rely on annotations from a nominally identical parser and/or relevance analyzer when annotating all of the referenced particular entities. Also, for example, in some implementations, it is stated that the same parser may rely on annotations from the relevance analyzer when clustering references of the same entity. In some implementations, in processing a particular natural language input, one or more components of the natural language processor 122 may determine one or more annotations using relevant prior inputs and/or other relevant data in addition to the particular natural language input.
The message organization module 126 may access an archive, log, or record of messages 124 previously exchanged between one or more users and the automated assistant 120. In some implementations, the record of the message 124 can be stored as a chronological record of the message. Thus, a user who wishes to find one or more particular messages from a user's past conversations with the automated assistant 120 may be required to scroll through a potentially large number of messages. The greater the number of times the user (or users) interacts with the automated assistant 120, the longer the chronological record of the messages 124 may be, which in turn makes locating past messages/conversations of interest more difficult and tedious. In addition, the process consumes computational resources, including those used to render recordings, as well as battery usage required to maintain interactivity for long periods of time, if applicable. Alternatively, the user may be able to perform a keyword search (e.g., using a search bar) to locate a particular message. However, if the conversation of interest occurs a relatively long time ago, the user may not remember which keywords to search for, and there may be intermediate conversations that also contain the keywords.
Thus, in various embodiments, the message organization module 126 may be configured to analyze a chronological record of messages 124 exchanged as part of one or more human-machine conversation sessions between one or more users and the automated assistant 120. Based on this analysis, the message organization module 126 may be configured to group the chronological records of the messages 124 into one or more message subsets (or message "clusters"). Each subset or cluster may contain messages that are grammatically and/or semantically related, e.g., as an independent conversation.
In some implementations, each subset or cluster can relate to tasks performed by one or more users via one or more human-machine conversation sessions with the automated assistant 120. For example, assume that one or more users exchange messages with the automated assistant 120 (and in some cases with each other) to organize an activity, such as a party. These messages may be grouped together, for example, by message organization module 126 as part of a conversation related to the task of organizing the party. As another example, assume that the user is engaged in a human-machine conversation with the automated assistant 120 to study and ultimately obtain an airline ticket. In various embodiments, these messages may be grouped together, for example, by message organization module 126, as part of another conversation related to the task of researching and purchasing airline tickets. For example, similar clusters or subsets of messages may be identified by, for example, message organization module 126 as being related to any number of tasks, such as obtaining items (e.g., products, services), setting and responding to reminders, and so forth.
Additionally or alternatively, in some implementations, each subset or cluster may relate to topics discussed during one or more human-machine conversation sessions with the automated assistant 120. For example, assume that one or more users are engaged in one or more human-machine conversation sessions with the automated assistant 120 to study Ronald Reagan. In various embodiments, these messages may be grouped together, for example, by message organization module 126, as part of a conversation related to the subject matter of Ronald Reagan. In some implementations, topic classifiers 127 associated with (e.g., a portion of, employed by, etc.) the message organization module 126 can be used to identify topics of conversation. For example, topic classifier 127 may cluster related words using topic models (e.g., statistical models) and determine topics based on these clusters.
In various embodiments, message organization module 126 may be configured to generate so-called "conversation metadata" to be associated with each subset of messages 124 based on the content of each subset of messages recorded in chronological order of the messages. In some implementations, the conversation metadata associated with a particular subset of messages can take the form of a data structure stored in memory that includes one or more fields for tasks (or topics), one or more fields (e.g., identifiers or pointers) that can be used to identify individual messages that form part of the subset of messages, and so forth.
In various embodiments, the message presentation module 128 (which may be integrated with the message organization module 126 in other embodiments) may be configured to obtain conversation metadata from the message organization module 126 and, based on the conversation metadata, generate a message that causes the client computing device 106 to provide the selectable element via an output device (not shown) associated with the client computing device. In various embodiments, the selectable elements may convey various aspects of the task, such as the task itself, the results of the task, the next potential step, the targets of the task, the subject matter, and/or other relevant conversational details (e.g., time/date/location of the activity, price paid, bill paid, etc.). To this end, in some implementations, the conversation metadata may be encoded, for example, by the message rendering module 128 using a markup language such as extensible markup language ("XML") or hypertext markup language ("HTML"), although this is not required. As will be described in greater detail below, the selectable elements presented on the client device 106 may take various forms, such as one or more graphical "cards" presented on a display screen, one or more options audibly presented via a speaker (from which a user may audibly select), one or more collapsible message threads, and so forth. The optional elements may eliminate the need for the user to browse through the chronological records to identify problems of interest, thereby relieving the computational resources provided to facilitate this process and increasing data management efficiency.
In various implementations, selecting the selectable element may cause the client computing device 106 to present, via one or more output devices (e.g., displays), a representation associated with at least one recorded message related to the task. For example, in some embodiments in which the selectable elements include a foldline, selecting the selectable element may toggle the foldline between a collapsed state in which only the selected pieces of information (e.g., tasks, topics, etc.) are presented and an expanded state in which one or more messages of the subset of messages are visible. In some implementations, the foldover contexts can include multiple levels, e.g., similar to a tree, where responses to certain messages (e.g., messages from another user or from the automated assistant 120) can be folded under statements from the user.
In other implementations, selecting a selectable element may simply open a time record of the message 124, e.g., visible on a display of the client device 106, and automatically scan to the first message forming the conversation represented by the selectable element. In some implementations, only those messages that form part of the conversation represented by the selectable element will be presented. In other embodiments, all messages of the chronological message exchange record 124 may be presented, and messages of a conversation represented by the selectable elements may be presented more prominently, e.g., in a different color, highlight, boldface, etc. In some implementations, the user may be able to "toggle" a conversation message represented by a selectable element, for example, by selecting an up/down arrow, a "next"/"previous" button, and so on. If other intermediate messages are interspersed among the messages of the conversation of interest, in some embodiments, those intermediate messages may be skipped.
In some implementations, selecting a selectable element representing a conversation may cause links (e.g., hyperlinks, so-called "deep links") included in the conversation message to be displayed, for example, as a list. In this manner, the user may quickly click on a presentation selectable element of the conversation to view links in the conversation that are mentioned, for example, by the user, the automated assistant 120, and/or by other participants in the conversation. Additionally or alternatively, selecting the selectable element may simply cause a message from the automated assistant 120 to be presented, with the message from the user being omitted or made less obvious. Providing these so-called "highlights" of past conversations may provide a technical advantage of allowing users, particularly users with limited input capabilities (e.g., disabled users, users who are driving or otherwise not free, etc.), to view portions of conversations (e.g., messages) that are most likely to be of interest, while less-interesting messages are ignored or rendered less noticeable.
Fig. 2A-D illustrate examples of four different human-machine conversation sessions (or "conversations") between a user ("YOU" in the figures) and an instance of an automated assistant (120 in fig. 1, not shown in fig. 2A-D). The client device 206 in the form of a smart phone or tablet (but not limited to) includes a touch screen 240. Visually presented on the touch screen 240 is a record 242 of at least a portion of a human-machine conversation session between a user of the client device 206 ("YOU" in fig. 2A-D) and an instance of the automated assistant 120 executing on the client device 206. An input field 244 is also provided where the user can provide natural language content, as well as other types of input, such as images, sounds, etc.
In fig. 2A, a user initiates a human-machine conversation session with a question of "How much money Is? in the store a" < store a > ", the term contained in < brackets > Is intended to represent a general indicator of a particular (e.g. general) type, rather than a particular entity, the automated assistant 120 (" AA "in fig. 2A-D) performs and responds to any necessary search, the" < store a > Is selected < item > for $39.95. "(store a sells < item > at a price of 39.95 >)" then the user asks "Is an inquiry line selector? (somebody sells cheaper?)" to perform and respond to "Yes," B > Is an item < item of interest < item > for < item > sells > for "< store B > which Is a store B < 6 > and a map server" (B < find B > open) Is a map server that Can perform and respond to a map server B < store B < find that a store B < store B > Is a store B < find < item > for "and a map 7" (B < store B < find a map) Can be a map B < store B < find a map 7, a map B < store B < find a map B > and a map B < find a map B > Can be a map B < find a map B > that a map 7, a map Can be a map B < a map Can be a map B < a map of a map for a map Can be a map B < a map and a map Can be a map of a store B < a map and a map Can be a map of a store B that Can be a store B < a store B that Can be a store B < a store.
FIG. 2B again illustrates the client device 206 with a touch screen 240 and user input fields 244 and a record 242 of a human-machine dialog session, in this example, the user ("you") interacts with the automated assistant 120 to study and ultimately make a reservation with a painter. the user initiates the human-machine dialog by typing and/or speaking (which can be recognized and converted to text) natural language input "" Whichpainter has reviews "", < painter _ A > or < painter _ B >? (which painter's better comment, < painter A > or < painter B >?) "the automated assistant 120 (" AA ") answers," painter _ B > has reviews "< painter _ review". an average of 4.5 dates-th _ A >, "with comments-pages < pages > 0, with a" assessment ". 3.7. the text is" interesting "< painter B > has a better comment, < average of < 4.5 dates-pages < pages > which is a > for a search," see if you make a possible hyperlink "< pages > with a rating" PM 2. the user "meeting" < pages > who is a meeting ". 2. the user performs a search once again with a meeting". 2. the index B. (PM < pages > meeting ". 2. the" meeting a necessary.
The user then answers "" "OK, book me. Are THER any other pages in town with compatible reviews? (good, book. town also other painters get similar reviews?)", after performing the necessary search/processing, the automated assistant will answer "You mentioned book for next wedding at 2:00PM. < page _ C > has had family favorite visiting place reviews-an overview of 4.4stars. Here's < page _ C's > page (next Wednesday at 2:00 has reserved. < painter C > rather good reviews, average 4.4stars. this is < painter C's > webpage" > detail text "dnesday at 2: 00. filled in with PM < page C > and open a relevant appointment link to provide calendar information < page > C.
In FIG. 2C, the user interacts with the automated assistant 120 in a human-machine dialog to perform a study on tickets flying to Chicago and eventually purchase tickets related thereto. after the user begins to say, "How much money? for tickets to Chicago on this Thursday" after having made the necessary search and/or processing (e.g., querying flights and prices from various airlines), "the automated assistant 120 would answer" It's 400 $ on < airine > if flight > is 400.) "and then the user would suddenly change? by asking" at done of reviewed < movie > get "after performing any necessary search/topic processing, the automated assistant 120 only answers 5star 5 (star 5).
The user then turns the conversation to the general topic of Chicago asking "What is the weather forecast for Chicago this Thursday?" (What is? for the weather forecast for Chicago this Thursday) "after performing any necessary search/processing, the automated assistant 120 responds" Party close and 70degree "(Partly cloudy, at 70 degrees)". then, the user says "OK. Buy me a ticket to Chicago with my < credit card > (it may be assumed that there is one or more of the user's credit cards in the records of the automated assistant 120.) the automated assistant 120 performs any necessary search/reservation/processing and answers" Done. herk is your itinerary entry into the website "(this is an application that is installed on your airline company's" and this is an application that is installed on your airline reservation "website" and this is an application is installed on your airline reservation "website" and this is an airline reservation "application that is installed on your website" C "and the reservation" website "is executed using a schedule" action "website" C-book ".
In FIG. 2D, the user and another participant in the message exchange line ("Frank") organize the activities related to the birthday of his friend Sarah. the user starts saying "What was done on birthday? (birthday of Monmonday Sharah)" Frank answer "Let's social person for pizza" (we find that we have eaten locally) "after indicating" Sarah a food is a food house ", then the user talks with the automated assistant 120 by asking" @ AA: What is done on the household pizza place surface? "(evaluate What is done by the highest pizza shop on AA: 7. this corresponds to the automated assistant 120. the automated assistant 120 performs any necessary search/processing (e.g., scanning around and ordering about the restaurant) and responds to the appointment of the restaurant in the evening" @ PM 7. this publication is done. the ad 7. this patent application is called "find that You have subscribed to" find that this restaurant "find" appointment ". 7. this publication is a book 7. the automated assistant 120. this example shows that does not like. the restaurant does not like. the automated assistant 120 perform any necessary search/processing (e.7. 7. the appointment of the restaurant).
Any of the conversations illustrated in fig. 2A-D may include information, links, selectable elements, or other content that the user may wish to revisit at a later time. In many cases, all of the messages exchanged in the conversation of fig. 2A-D may be stored in a chronological record (e.g., 124) that the user may later revisit. However, if the user interacts extensively with the automated assistant 120, the time record 124 may be long because the messages illustrated in fig. 2A-D may be interspersed among other messages that form part of different conversations. Simply scrolling chronological record 124 to locate a particular conversation of interest can be tedious and/or challenging, particularly for users with limited input capabilities provided (e.g., physically disabled users or users engaged in other activities such as driving).
Thus, as described above, for example, the message organization module 126 may group messages into clusters or "conversations" based on various signals, shared properties, and the like. Conversation metadata associated with each cluster may be generated, for example, by message organization module 126. Conversation metadata may be used, for example, by the message rendering module 128 to generate selectable elements associated with each cluster/conversation. The user may then be able to more quickly sweep through these selectable elements, rather than all messages behind the conversation represented by these selectable elements, to find a particular past conversation of interest. One non-limiting example is illustrated in fig. 4E.
FIG. 4E illustrates the client device 206 rendering a series of selectable elements 260 on the touch screen 2401-4Thereafter, client device 206, each selectable element represents an underlying cluster of messages that form a different conversation. First optional element 2601Representing a conversation related to the price study illustrated in fig. 2A. Second optional element 2602Representing a conversation associated with the painter illustrated in fig. 2B. Third optional element 2603Representing a conversation associated with the trip of chicago illustrated in fig. 2C. Fourth optional element 2604Representing conversations related to the birthday activity of organization Sarah, illustrated in figure 2D. Thus, it can be seen that the user is presented with four selectable elements 260 in a single screen1-4Collectively, these elements represent numerous messages that the user would otherwise have to scroll through the chronological message record 124 to locate. In some implementations, the user can simply click or otherwise select (e.g., tap, double-click, etc.) the selectable element 260 representing the presentation to be associated with the at least one recorded message. Although selectable element 260 is illustrated in FIG. 4E as a "card" appearing on touch screen 240, this is not meant to be limiting. In various embodiments, the selectable elements may take other forms, such as foldable threads, links, and the like.
In fig. 2E, each selectable element 260 conveys various information extracted from the corresponding underlying conversation. First optional element 2601Including a title ("Price research on") that generally conveys the topic/task of the conversation<item>(pair)<Article with a cover>Price research) "and two links incorporated into the conversation by the automated assistant 120. In some implementations, any links incorporated into the underlying conversation or other components of interest (e.g., deep links) may likewise be incorporated (although in some cases in abbreviated form) into the optional element 260 representing the conversation. In some implementations, if the conversation includes a relatively large number of links, a particular number of links that occurred most recently (i.e., last time) (e.g., selected or determined by the user based on available touch screen real estate) may be incorporated into the respective selectable elements 260. In some implementations, only those links (e.g., get item, reserve ticket, organize event details) that are relevant to the goal or result of the task may be incorporated into the respective selectable elements 260. In the first optional element 2601In the case of (2), only two links are involved in the underlying conversation, and therefore these two links have been merged into the first selectable element 2601In (1). Notably, the first link is a deep link that, when selected, opens an installation on the client device 206 with preloaded directions.
Second optional element 2602But also titles ("Research on painters") that are typically related to the topic/task of the underlying conversation. And a first selectable element 2601Likewise, a second selectable element 2602Including a plurality of links that are incorporated into the conversation shown in fig. 2B. Selecting a first link to open to include<painter_B's>A browser for a web page of an online reservation system. A second link may be selected to open a calendar entry for the scheduled appointment. Second optional element 2602Also included in (A) are those included, e.g. with<painter_C>Additional information is relevant because this is the last piece of information that the automated assistant 120 incorporates into the conversation (which may suggest that the user would be interested in it).
Third optional element 2603Including graphics of the airplane indicating that it is relevant to the conversation relating to the task of making the travel itinerary and the result of booking the airline ticket. If the conversation does not result in a ticket purchase, a third selectable element 2603A link may be included that is selectable, for example, to complete a ticket purchase. Third optional element 2603Also included is a link to the user itinerary on the airline's website, as well as the amount paid and the amount used<Credit card>. As with the other selectable elements 260, via a third selectable element 2603The message organization module 126 and/or the message presentation module 128 attempt to emerge (i.e., present to the user) the most relevant data points resulting from the underlying conversation.
Fourth optional element 2604The title "Sarah's birthday" is included. Fourth optional element 2604The selectable elements 260 may be ordered or ranked based on various signals in some implementations, e.g., the selectable element representing the most recent (or earliest) conversation is at the top, the selectable elements 260 may be ordered chronologically, e.g., the selectable element representing the most recent (or earliest) conversation is at the top, in other implementations, other signals such as results/goals/next steps (e.g., whether a purchase? was made), number of messages in conversation, number of participants in conversation, etcVolume, task importance, task immediacy (e.g., conversations associated with upcoming activities may be ranked higher than conversations associated with previous activities) to rank the selectable elements 260.
As described above, in FIG. 4E, the user may select any selectable elements 260 to be presented that have representations associated with each underlying conversation1-4(v-shapes above and to the right of each element, in areas outside the links, etc.). However, the user may also click on or otherwise select individual links to be passed directly to the respective destinations/applications without having to view the underlying messages.
The conversations illustrated in fig. 2A, 2B and 2D are relatively independent conversations (primarily for clarity and brevity). However, this is not meant to be limiting. A single conversation (or cluster of related messages) need not necessarily be part of a single human-to-machine conversation session. In practice, the user may engage the automated assistant 120 in discussions regarding topics in a first conversation, while engaging the automated assistant 120 in any number of other conversations regarding other topics, and then revisit the topics of the first conversation in subsequent human-to-machine conversations. However, these temporally separated but semantically related messages may be organized as clusters. This is one technical advantage provided by the techniques described herein: semantically or otherwise related time-dispersed messages can be consolidated into clusters or conversations that are easy for a user to retrieve without providing a large amount of input (e.g., scrolling, keyword searching, etc.). Of course, in some embodiments, messages may also be organized, in whole or in part, as clusters or conversations based on temporal proximity, conversational proximity (i.e., a human-to-machine conversation session contained in the same human-to-machine conversation session or proximate in time), and so forth.
FIG. 2F illustrates selection of the third selectable element 260 at the user3Thereafter, the client device 206 may illustrate one non-limiting example. As described above, a third selectable element 260 is illustrated in FIG. 2C3The conversation represented. The conversation includes two messages ("at kind of reviews did") related to the schedule to Chicago, unrelated to the rest of the messages illustrated in FIG. 2C<movie>get? "and" Negative, only1.5stars on average "). Thus, in fig. 2F, ellipses 262 are shown to indicate that those messages unrelated to the underlying conversation have been omitted. In some implementations, the user may be able to select the ellipses 262 to view those messages. Of course, other symbols may be used to represent omitted intermediate messages. The ellipses are only one example.
FIG. 2G illustrates an alternative manner in which selectable element 360 may be substituted1-NPresented to the optional elements in fig. 2E. In fig. 2G, the user is operating the client device 206 to scroll through the messages of the record 242 (purposely left blank for brevity and clarity), particularly using a first vertically oriented scroll bar 270A. At the same time, a graphical element 272 is presented, which graphical element 272 illustrates a selectable element 360 representing the current visual conversation on the touch screen 240. A second horizontally oriented scrollbar 270B, which may alternatively be operated by the user, indicates the relative position of the conversation represented by the message currently displayed on the touchscreen. In other words, scrollbars 270A and 270B work in concert: when the user scrolls the scroll bar 270A downward, the scroll bar 270B moves to the right; when the user scrolls the scroll bar 270A upward, the scroll bar 270B moves to the left. Likewise, when the user scrolls the scroll bar 270B to the right, the scroll bar 270A moves down, and when the user scrolls the scroll bar 270B to the left, the scroll bar 270A moves up.
In some implementations, the user can select (e.g., click, tap, etc.) selectable element 360 to scroll the messages vertically so that the first message of the underlying conversation is presented at the top. In some implementations, the user can perform various actions on the message cluster (or conversation) by acting on the respective selectable elements 360. For example, in some embodiments, the user may be able to "swipe" the optional elements 360 to perform certain operations on the underlying messages together, such as deleting them, sharing them, saving them to a different location, marking them, and so forth. Although the graphical element 272 is illustrated as being superimposed on a message, this is not meant to be limiting. In various implementations, the graphical element 272 (or the optional element 360 itself) may be presented on a different or separate portion of the touch screen 240 than the screen containing the message.
Fig. 3 illustrates an example method 300 for implementing selected aspects of the present disclosure, in accordance with various embodiments. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include various components of various computer systems, including automated assistant 120, message organization module 126, message presentation module 128, and so forth. Further, while the operations of method 300 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 302, the system can analyze a chronological record of messages exchanged as part of one or more human-machine conversation sessions between at least one user and the automated assistant. As described above, these human-machine dialog sessions may involve only one user and/or may involve multiple users. The analysis may include, for example, the topic classifier 127 identifying topics for individual messages, topics for groups of messages that are close in time, clustering messages by various words, clustering messages in time, clustering messages in space, and so forth.
At block 304, the system may identify at least a subset (or "cluster" or "conversation") of the chronological record of messages related to tasks performed by the at least one user via the one or more human-machine conversation sessions based on the analysis. For example, the system may identify messages that, when clustered, form different conversations illustrated in fig. 2A-D.
At block 306, the system may generate conversation metadata associated with the subset of the chronological record of the message based on the content of the subset of the chronological record of the message and the tasks. For example, the system may select a topic (or task) identified as a title by topic classifier 127, and may select a link and/or other relevant data (e.g., the first/last message of a conversation) to incorporate into a data structure, which may be stored in memory and/or transmitted as a packet to a remote computing device.
At optional block 308, the system may provide the conversation metadata (or other information indicative thereof, such as XML, HTML, etc.) to the client device (e.g., 106, 206) over one or more networks. In some implementations where operation 302 is performed at the client device and operation 306, operation 308 may be omitted as is apparent. At block 310, the client computing device (e.g., 106, 206) may provide a selectable element conveying the task or topic via an output device associated with the client computing device, as shown in fig. 2E and 2G. In various implementations, selecting the selectable element may cause the client computing device to present, via an output device, a representation associated with at least one of the recorded messages related to the task or topic. These representations may include, for example, the message itself, links extracted from the message, and so forth.
FIG. 4 is a block diagram of an example computing device 410 that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing device, the automated assistant 120, and/or other components may include one or more components of the example computing device 410.
Computing device 410 typically includes at least one processor 414 that communicates with a number of peripheral devices via a bus subsystem 412. These peripheral devices may include a storage subsystem 424 including, for example, a memory subsystem 425 and a file storage subsystem 426, a user interface output device 420, a user interface input device 422, and a network interface subsystem 416. Input devices and output devices allow a user to interact with computing device 410. Network interface subsystem 416 provides an interface to external networks and couples to corresponding interface devices in other computing devices.
The user interface input devices 422 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or tablet, a scanner, a touch screen incorporated into the display, an audio input device such as a voice recognition system, microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and methods of inputting information into computing device 410 or onto a communication network.
User interface output devices 420 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visual image. The display subsystem may also provide a non-visual display, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and methods that output information from computing device 410 to a user or to another machine or computing device.
Storage subsystem 424 stores programming and data structures that provide the functionality of some or all of the modules described herein. For example, the storage subsystem 424 may include logic to perform selected aspects of the method 300, as well as to implement various components illustrated in FIG. 1.
These software modules are typically executed by the processor 414 alone or in combination with other processors. Memory 425 used in storage subsystem 424 may include a plurality of memories including a main Random Access Memory (RAM)430 for storing instructions and data during program execution and a Read Only Memory (ROM)432 that stores fixed instructions. File storage subsystem 426 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical disk drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 426 in storage subsystem 424 or may be stored in other machines accessible by processor 414.
Bus subsystem 412 provides a mechanism for the various components and subsystems of computing device 410 to communicate with one another as intended. Although bus subsystem 412 is shown schematically as a single bus, alternative embodiments of the bus subsystem may use multiple buses.
The computing device 410 may be of various types, including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Because the nature of computers and networks vary, the description of computing device 410 illustrated in FIG. 4 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computing device 410 are possible with more or fewer components than the computing device illustrated in fig. 4.
Where certain embodiments discussed herein may collect or use personal information about a user (e.g., user data extracted from other electronic communications, information about the user's social network, the user's location, the user's time, the user's biometric information, as well as the user's activities and demographic information, relationships between users, etc.), the user is provided with one or more opportunities to control whether information is collected, stored, used, and how information about the user is collected, stored, and used. That is, the systems and methods discussed herein collect, store, and/or use user personal information only upon receiving explicit authorization from the relevant user that can do so.
For example, a user is provided with control over whether the program or feature collects user information about that particular user or other users associated with the program or feature. Each user who will collect personal information is presented with one or more options: allowing control of information collection associated with the user, providing permission or authorization as to whether to collect information and as to which portions of the information to collect. For example, one or more of such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways before being stored or used, thereby removing personally identifiable information. As one example, the identity of the user may be processed such that no personal identity information can be determined. As another example, the user's geographic location may be generalized to a larger area such that the user's specific location cannot be determined. In the context of the present disclosure, any relationships captured by the system, such as parent-child relationships, may be maintained in a secure manner such that they are not accessible outside of the automated assistant to analyze and/or interpret natural language input through the use of these relationships.
While multiple embodiments have been described and illustrated herein, various other means and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be utilized and each of such variations and/or modifications is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application for which the present teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (18)
1. A method, comprising:
analyzing, by the one or more processors, a chronological record of messages exchanged as part of one or more human-machine conversation sessions between the at least one user and the automated assistant;
based on the analysis, identifying, by the one or more processors, at least a subset of the chronological record of the messages, the subset relating to tasks performed by the at least one user via the one or more human-computer conversation sessions; and
generating, by the one or more processors, conversation metadata associated with the subset of the chronological record of the message based on the tasks and the content of the subset of the chronological record of the message;
wherein the conversation metadata causes a client computing device to provide, via an output device associated with the client computing device, a selectable element that conveys the task, wherein selection of the selectable element causes the client computing device to present, via the output device, a representation associated with at least one of the recorded messages related to the task.
2. The method of claim 1, further comprising identifying, by the one or more processors, a result of the task based on content of the subset of the chronological record of the message.
3. The method of claim 2, wherein the selectable element conveys the result of the task.
4. A method as claimed in claim 2 or claim 3, wherein the result of the task comprises the acquisition of an item.
5. A method as claimed in claim 2 or claim 3, wherein the task comprises an organizational activity.
6. The method of claim 5, wherein the results of the task include details associated with the organized activity.
7. The method of any of the preceding claims, further comprising identifying, by the one or more processors, a next step for completing the task based on content of the subset of chronological records of the messages, wherein the selectable element conveys the next step.
8. The method of any of the preceding claims, wherein identifying the subset of chronological records of the messages is based on results of the tasks.
9. The method of any of the preceding claims, wherein identifying the subset of chronological records of the messages is based on timestamps associated with respective ones of the chronological records of the messages.
10. The method of any preceding claim, wherein the selectable elements comprise a foldaway thread that, when selected, expands to provide the subset of chronological records of the message.
11. The method of any one of the preceding claims, wherein the selectable elements include individual messages of the subset, and selection of the individual messages of the subset causes one or more other individual messages of the subset to be presented in a first manner that is visually distinct from a second manner in which other messages of the chronological recording of messages are presented.
12. A method as claimed in any preceding claim, wherein the representation comprises an icon associated with or contained within the subset of the chronological record of the message.
13. The method of any of the preceding claims, wherein the representation comprises one or more hyperlinks included in the subset of chronological records of the message.
14. The method of any preceding claim, wherein the representation comprises the subset of chronological records of the message.
15. The method of claim 14, wherein the messages in the subset of the chronological record of messages are presented chronologically.
16. The method of claim 14, wherein the messages in the subset of chronological records of messages are presented in order of relevance.
17. A system comprising one or more processors and memory operably coupled with the one or more processors, wherein the memory stores instructions that, in response to execution by the one or more processors, cause the one or more processors to perform the method of any one of claims 1-16.
18. At least one non-transitory computer-readable medium comprising instructions that, in response to execution by the one or more processors, cause the one or more processors to perform the method of any one of claims 1-16.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/498,173 US20180314532A1 (en) | 2017-04-26 | 2017-04-26 | Organizing messages exchanged in human-to-computer dialogs with automated assistants |
US15/498,173 | 2017-04-26 | ||
PCT/US2018/029361 WO2018200673A1 (en) | 2017-04-26 | 2018-04-25 | Organizing messages exchanged in human-to-computer dialogs with automated assistants |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110603545A true CN110603545A (en) | 2019-12-20 |
CN110603545B CN110603545B (en) | 2024-03-12 |
Family
ID=62196711
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880027624.9A Active CN110603545B (en) | 2017-04-26 | 2018-04-25 | Method, system and non-transitory computer readable medium for organizing messages |
Country Status (4)
Country | Link |
---|---|
US (1) | US20180314532A1 (en) |
EP (1) | EP3602426A1 (en) |
CN (1) | CN110603545B (en) |
WO (1) | WO2018200673A1 (en) |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10452251B2 (en) * | 2017-05-23 | 2019-10-22 | Servicenow, Inc. | Transactional conversation-based computing system |
US20190075069A1 (en) * | 2017-09-01 | 2019-03-07 | Qualcomm Incorporated | Behaviorally modelled smart notification regime |
US10431219B2 (en) * | 2017-10-03 | 2019-10-01 | Google Llc | User-programmable automated assistant |
US20190138996A1 (en) * | 2017-11-03 | 2019-05-09 | Sap Se | Automated Intelligent Assistant for User Interface with Human Resources Computing System |
US11437045B1 (en) * | 2017-11-09 | 2022-09-06 | United Services Automobile Association (Usaa) | Virtual assistant technology |
KR102607666B1 (en) * | 2018-08-08 | 2023-11-29 | 삼성전자 주식회사 | Apparatus and method for providing feedback for confirming intent of a user in an electronic device |
US10817317B2 (en) * | 2019-01-24 | 2020-10-27 | Snap Inc. | Interactive informational interface |
CN110619099B (en) * | 2019-05-21 | 2022-06-17 | 北京无限光场科技有限公司 | Comment content display method, device, equipment and storage medium |
US11367429B2 (en) | 2019-06-10 | 2022-06-21 | Microsoft Technology Licensing, Llc | Road map for audio presentation of communications |
US11269590B2 (en) * | 2019-06-10 | 2022-03-08 | Microsoft Technology Licensing, Llc | Audio presentation of conversation threads |
US11887586B2 (en) * | 2021-03-03 | 2024-01-30 | Spotify Ab | Systems and methods for providing responses from media content |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101004737A (en) * | 2007-01-24 | 2007-07-25 | 贵阳易特软件有限公司 | Individualized document processing system based on keywords |
US20120016678A1 (en) * | 2010-01-18 | 2012-01-19 | Apple Inc. | Intelligent Automated Assistant |
EP2575128A2 (en) * | 2011-09-30 | 2013-04-03 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
CN104519205A (en) * | 2013-10-04 | 2015-04-15 | 三星电子株式会社 | Method for managing communication records and electronic device thereof |
US20160132773A1 (en) * | 2014-11-06 | 2016-05-12 | International Business Machines Corporation | Method for Automatic Near-Real-Time Prediction, Classification, and Notification of Events in Natural Language Systems |
CN106575503A (en) * | 2014-06-18 | 2017-04-19 | 微软技术许可有限责任公司 | Session context modeling for conversational understanding systems |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040162724A1 (en) * | 2003-02-11 | 2004-08-19 | Jeffrey Hill | Management of conversations |
US7409641B2 (en) * | 2003-12-29 | 2008-08-05 | International Business Machines Corporation | Method for replying to related messages |
JP4197344B2 (en) * | 2006-02-20 | 2008-12-17 | インターナショナル・ビジネス・マシーンズ・コーポレーション | Spoken dialogue system |
US20140122083A1 (en) * | 2012-10-26 | 2014-05-01 | Duan Xiaojiang | Chatbot system and method with contextual input and output messages |
US20140245140A1 (en) * | 2013-02-22 | 2014-08-28 | Next It Corporation | Virtual Assistant Transfer between Smart Devices |
US10445115B2 (en) * | 2013-04-18 | 2019-10-15 | Verint Americas Inc. | Virtual assistant focused user interfaces |
KR101959188B1 (en) * | 2013-06-09 | 2019-07-02 | 애플 인크. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US11004154B2 (en) * | 2015-03-02 | 2021-05-11 | Dropbox, Inc. | Collection of transaction receipts using an online content management service |
-
2017
- 2017-04-26 US US15/498,173 patent/US20180314532A1/en not_active Abandoned
-
2018
- 2018-04-25 EP EP18725713.4A patent/EP3602426A1/en not_active Withdrawn
- 2018-04-25 WO PCT/US2018/029361 patent/WO2018200673A1/en unknown
- 2018-04-25 CN CN201880027624.9A patent/CN110603545B/en active Active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101004737A (en) * | 2007-01-24 | 2007-07-25 | 贵阳易特软件有限公司 | Individualized document processing system based on keywords |
US20120016678A1 (en) * | 2010-01-18 | 2012-01-19 | Apple Inc. | Intelligent Automated Assistant |
EP2575128A2 (en) * | 2011-09-30 | 2013-04-03 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
CN104519205A (en) * | 2013-10-04 | 2015-04-15 | 三星电子株式会社 | Method for managing communication records and electronic device thereof |
CN106575503A (en) * | 2014-06-18 | 2017-04-19 | 微软技术许可有限责任公司 | Session context modeling for conversational understanding systems |
US20160132773A1 (en) * | 2014-11-06 | 2016-05-12 | International Business Machines Corporation | Method for Automatic Near-Real-Time Prediction, Classification, and Notification of Events in Natural Language Systems |
Also Published As
Publication number | Publication date |
---|---|
WO2018200673A1 (en) | 2018-11-01 |
CN110603545B (en) | 2024-03-12 |
EP3602426A1 (en) | 2020-02-05 |
US20180314532A1 (en) | 2018-11-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110603545B (en) | Method, system and non-transitory computer readable medium for organizing messages | |
JP7419485B2 (en) | Proactively incorporating unsolicited content into human-to-computer dialogs | |
JP7443407B2 (en) | Automated assistant with conferencing capabilities | |
EP3369011B1 (en) | Providing suggestions for interaction with an automated assistant in a multi-user message exchange thread | |
CN110770694B (en) | Obtaining response information from multiple corpora | |
US10438172B2 (en) | Automatic ranking and scoring of meetings and its attendees within an organization | |
US10110544B2 (en) | Method and system for classifying a question | |
US10540666B2 (en) | Method and system for updating an intent space and estimating intent based on an intent space | |
JP6538277B2 (en) | Identify query patterns and related aggregate statistics among search queries | |
US20170097939A1 (en) | Methods, systems and techniques for personalized search query suggestions | |
US9043413B2 (en) | System and method for extracting, collecting, enriching and ranking of email objects | |
US11899728B2 (en) | Methods, systems and techniques for ranking personalized and generic search query suggestions | |
US11836169B2 (en) | Methods, systems and techniques for providing search query suggestions based on non-personal data and user personal data according to availability of user personal data | |
US11216735B2 (en) | Method and system for providing synthetic answers to a personal question | |
US10942979B2 (en) | Collaborative creation of content snippets | |
KR102433734B1 (en) | Methods and Computer-Readable Medium for Providing User-customized National Assembly Minutes Information | |
EP3942744A1 (en) | Generating content endorsements using machine learning nominator(s) |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |