CN115244528A - Automatic data quality improvement - Google Patents
Automatic data quality improvement Download PDFInfo
- Publication number
- CN115244528A CN115244528A CN202180019440.XA CN202180019440A CN115244528A CN 115244528 A CN115244528 A CN 115244528A CN 202180019440 A CN202180019440 A CN 202180019440A CN 115244528 A CN115244528 A CN 115244528A
- Authority
- CN
- China
- Prior art keywords
- user
- design
- data
- feedback
- item
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/21—Design, administration or maintenance of databases
- G06F16/215—Improving data quality; Data cleansing, e.g. de-duplication, removing invalid entries or correcting typographical errors
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/23—Updating
- G06F16/2379—Updates performed during online database operations; commit processing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/248—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04847—Interaction techniques to control parameter settings, e.g. interaction with sliders or dials
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/12—Computing arrangements based on biological models using genetic models
- G06N3/126—Evolutionary algorithms, e.g. genetic algorithms or genetic programming
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/01—Probabilistic graphical models, e.g. probabilistic networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/02—Marketing; Price estimation or determination; Fundraising
- G06Q30/0201—Market modelling; Market analysis; Collecting market data
- G06Q30/0204—Market segmentation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/02—Marketing; Price estimation or determination; Fundraising
- G06Q30/0282—Rating or review of business operators or products
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/06—Buying, selling or leasing transactions
- G06Q30/0601—Electronic shopping [e-shopping]
- G06Q30/0621—Item configuration or customization
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/06—Buying, selling or leasing transactions
- G06Q30/0601—Electronic shopping [e-shopping]
- G06Q30/0631—Item recommendations
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/06—Buying, selling or leasing transactions
- G06Q30/0601—Electronic shopping [e-shopping]
- G06Q30/0641—Shopping interfaces
- G06Q30/0643—Graphical representation of items or shoppers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
Abstract
Methods, systems, and computer readable media include: receiving a request for a digital component from a user device, determining attributes of a user based on information provided by or contained in the request, identifying a behavioral model corresponding to the attributes, dynamically changing a presentation of an item depicted by the digital component based on the identified behavioral model, determining that the user corresponds to an insufficiently represented subdivision of a population of users in a database containing information about the item, and in response, generating a digital component comprising a presentation of the dynamically changed item, soliciting feedback from the user regarding the item and including a feedback mechanism, updating the database to include the obtained feedback, and modifying the presentation of the item when distributed to other users having the attributes of the user based at least in part on the obtained feedback.
Description
Cross Reference to Related Applications
This application claims benefit of U.S. application Ser. No. 16/942,937 filed on 30/7/2020 and U.S. provisional application Ser. No. 63/010,399 filed on 15/4/2020, the contents of which are incorporated herein by reference.
Background
This document relates to providing a data collection and model generation process that continuously reduces the bias introduced by using data points from non-uniformly distributed or representative deficient populations and explores the design space.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods for improving the robustness of a data set executed by one or more data processing apparatus, the methods including receiving a request from a user device for a digital component for presentation at the user device, determining one or more attributes of the user based on one or more of information provided by the user or information contained in the request for the digital component, identifying a behavioral model corresponding to the one or more attributes of the user, dynamically altering the presentation of an item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user, determining that the user has particular attributes corresponding to insufficiently represented segments of a population of users in a database containing information about the item, in response to determining that the user has particular attributes corresponding to insufficiently represented segments of the population of users: in response to the request, a digital component is generated that includes a dynamically changing presentation of the item, solicits feedback from the user regarding the item, and includes a feedback mechanism that enables the user to submit feedback regarding the item, updates the database to include feedback obtained from the user regarding the item, and modifies the presentation of the item when distributed to other users having one or more attributes of the user based at least in part on the feedback obtained from the user.
These and other embodiments may each optionally include one or more of the following features.
In some implementations, the method includes, in response to receiving feedback from a user having particular attributes corresponding to the underrepresented segments of the user population, tagging the feedback information with one or more attributes of the user, and storing the tagged feedback information in a tagged, searchable database.
In some implementations, determining that the user has particular attributes corresponding to the under-represented segments of the user population in the database includes using statistical analysis to identify the under-represented segments of the user population.
In some implementations, modifying the presentation of the item when distributed to other users having one or more attributes of the user based at least in part on feedback obtained from the user includes selecting a particular feedback mechanism included with the digital component.
In some implementations, dynamically changing the presentation of the item depicted by the digital components based on the identified behavioral model corresponding to one or more attributes of the user includes using machine learning or artificial intelligence techniques to identify feedback about the item to be solicited from the user.
In some implementations, determining the one or more attributes of the user is based on information provided by the user, and dynamically changing presentation of the item depicted by the digital composition based on the identified behavioral model corresponding to the one or more attributes of the user includes updating the identified behavioral model based on the information provided by the user.
In some implementations, determining the one or more attributes of the user is based on information contained in the request for the digital component, and dynamically changing presentation of the item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user includes updating the identified behavioral model based on the information contained in the request for the digital component.
In some embodiments, the method includes for a numeric component soliciting feedback from a user regarding an item, selecting a format in which the feedback is solicited, and validating the information solicited by the numeric component based on a particular attribute corresponding to an underrepresented subdivision of the user population.
In some embodiments, the digital component displays the suit and two different product design shapes for the footwear, and the feedback obtained from the user regarding the item includes a selection of one of the two different product design shapes for the footwear the user believes fits better with the suit.
In some embodiments, the numeric component displays two different product design shapes for the automobile, the numeric component specifies a particular subjective product design shape descriptor, the feedback mechanism is a slider, and the feedback obtained from the user regarding the item includes a selection of one of the two different product design shapes for the automobile that the user believes can be better described by the particular subjective product design shape descriptor.
In some embodiments, the digital component displays three or more different product design shapes for three or more different item types, the feedback obtained from the user regarding the item includes a selection of two or more different product design shapes that the user considers visually harmonious, and the method further includes using the feedback obtained from the user in a separate model.
In some embodiments, the digital component asks the user to create a new product design, the feedback mechanism receives user input indicating the new product design, and the method further comprises providing the user with a product having the new product design.
In some embodiments, the digital component requires a user to modify an existing product design to produce a customized product design, the feedback mechanism receives user input modifying one or more aspects of the existing product design, and the method further comprises providing the product with the customized product design to the user.
In some embodiments, the digital component displays two different software attributes, the feedback obtained from the user regarding the item includes a selection of one of the two different software attributes preferred by the user, and the method further comprises providing the user with a software package having the selected one of the two different software attributes preferred by the user.
Other embodiments of this aspect include corresponding systems, apparatus, and computer programs, encoded on computer storage devices, configured to perform the actions of the methods.
Particular embodiments of the subject matter described in this document can be implemented to realize one or more of the following advantages. In certain circumstances, there has been no previous way to automatically and systematically reduce bias in datasets that are under-representative of a population or determined to be deficient in data in a particular subdivision, and this disadvantage has been addressed by the techniques, apparatus and systems discussed herein.
In some embodiments of the new system, underrepresented segments of the population for which data is collected are identified and tasks are generated for distribution to users in the segments. These tasks solicit responses from the user regarding a particular topic or domain so that the responses supplement the existing data set. For example, the system may determine that there are less than a threshold amount of responses from users within a particular age range regarding a preferred choice between two products, and then generate a task that requires the user to choose between the two products for distribution to users within that age range. The system receives and processes these responses as a supplement to the existing data set, thereby improving data quality. The system continuously monitors the data set, providing a solution that automatically maintains data quality even as the data set is updated.
This new system has access to a complex data processing infrastructure that can clean, process and maintain a comprehensive tagged, searchable dataset that can be used in many different situations to improve the results of previously unsolved models to automatically improve and maintain data quality as more data is gathered. If the model uses an incomplete or representatively deficient data set, the model may produce results that are representatively deficient in the actual behavior of the population: the new system automatically supplements the data set obtained by the model, and improves the robustness of the data and the accuracy of the model result. By automatically identifying population subdivisions where there is insufficient data or a representative insufficient amount of data, the system reduces bias in the data set used as input to the various models. These more robust data sets, in turn, improve the reliability and accuracy of the data-dependent model results. The improved data set may be tagged and used by different parties, including content providers and product manufacturers who may not have the infrastructure or access to the system maintained tagged and searchable data set.
In addition to improving the data set, the system can automatically explore the design space. The design space is a conceptual representation of design values or attributes and may be referred to as a continuous shape. The design space may be associated with a particular product or service and may be multidimensional, representing possible design values for a parameter of interest. In some implementations, the design space can be enhanced to map design parameters to semantic values. For example, the system may create a model using a mapping of semantic attributes and geometric features of the product; these models exist in a single, continuous shape space that represents a hierarchy of possible attribute values. The design may be based on one or more sensory characteristics. For example, the design may be visual, auditory, tactile, odor-based, or taste-based. For example, the design of an automobile seat may include visual and tactile features.
This new system can automatically explore a design space by identifying subdivisions of the design space where little or no data exists and generating tasks that are distributed to users regarding the identified subdivisions. By gathering these subdivided data, the system allows previously unexplored designs to be considered. For example, the system may automatically generate a design with parameter values that have not yet been presented to the user for feedback. Based on the feedback received, the system can continually update existing designs and generate new designs for which user feedback is solicited prior to the prototyping, manufacturing, and distribution stages. The feedback may be provided as input to an output such as a model of user preference prediction. The system can determine directions to lead the product design based on data analysis and output of behavioral models, and allow content providers, product designers, and manufacturers to focus on designs that are most likely to be popular with targeted consumer segments. In other words, the improved update process may reduce the number of feedback cycles required to complete a product design, thereby reducing the computational resources required to complete the entire design cycle from initial data gathering to the design completion.
Utilizing this approach allows for rapid design development of products that more accurately and reliably meet consumer needs and expectations of various demographics. In addition, the improved update process provides efficiency in designing the system by improving data quality and reducing the number of feedback cycles required to gather user data for input to the behavioral model. The system provides a way for product designers, developers and manufacturers to quickly receive a variety of feedback on a global scale and determine user preferences from demographic data prior to the manufacturing and shipping stages, requiring significant resource expenditures. Such an approach may allow for personalized design and manufacture for an individual or user segmentation. For example, a manufacturer may use this method to review personal preferences to measure acceptance statistics for a larger market.
The techniques described in this document enable a system to generate high quality data related to a particular product or service using fewer resources and performing fewer operations. By automatically detecting and reducing deviations in the data set, the system enables models using the data to provide more accurate and reliable results. In addition, the system reduces the amount of resources required to complete a new product or service design by allowing for targeted exploration of the exploration design space and continued improvement in data quality.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example environment for data quality improvement and design space exploration.
Fig. 2A illustrates an example data flow for a data quality improvement process.
Fig. 2B and 2C depict the model training process.
Fig. 3 shows an example data flow of a data quality improvement process.
FIG. 4 illustrates an example data flow for a design space exploration process.
Fig. 5A and 5B depict data flows for a specific example of a system generating tasks for a user using a behavior model.
Fig. 6A and 6B depict data flows for a specific example of a system integrating user feedback into a design cycle.
Fig. 7A and 7B depict data flows for specific examples of systems implementing user feedback to customize existing designs and products.
7C-7F depict specific examples of the system implementing user feedback into a design cycle.
FIG. 8 is a flow diagram of an example process for data quality improvement.
FIG. 9 is a flow diagram of an example process for automated design space exploration.
FIG. 10 is a block diagram of an example computing system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This document describes methods, systems, and apparatus that improve data quality, reduce inherent biases, and allow for automatic, intelligent design space exploration. The model is only as accurate and representative of the population as the input data provided to the model. The proposed system improves the quality of data that can be provided to various systems for modeling and product development. User feedback data and behavior data may be gathered in various ways.
In some embodiments, the system generates tasks for distribution to users. Each task may be a question, task, or other form of solicitation for input from the user. The user's input may be tagged and used as part of a comprehensive database that may be used in different contexts. For example, a user may be presented with a task to be completed in which the user must select a single logo design from a plurality of logo designs that appears most appealing. The user's selections may then be tagged with the user's demographic information and stored as part of a tagged searchable database. The system can determine that data from a particular user demographic or about a particular product segment (segment) is missing, deficient, or less representative of a known population based on an analysis of the tagged data set, and automatically generate a task or question to gather more data and reduce inherent bias in the data set of the representative deficiency. The supplemental tagged data set can be provided as input to the various models. For example, the labeled data set may be provided to a behavioral model to predict whether a particular design will appeal to a particular user segment. Various models may be used to predict user reactions and acceptance of, for example, a particular design.
The system also allows for automatic and intelligent exploration of a particular design space. For example, the system may determine that data related to a particular region of the design space is missing, deficient, insufficient to be representative of a known population based on an analysis of the tagged data set, and automatically generate tasks or questions to gather more data. The system can explore a design space by generating a product design based on an undeveloped region of the design space using artificial intelligence and machine learning models. These manually generated designs may then be presented to the user along with the task of soliciting feedback.
FIG. 1 is a block diagram of an example environment 100 for data quality improvement and design space exploration. The example environment 100 includes a network 102, such as a Local Area Network (LAN), a Wide Area Network (WAN), the Internet, or a combination thereof. The network 102 connects an electronic document server 104 ("electronic document server"), user devices 106, and a digital component distribution system 110 (also referred to as a DCDS 110). The example environment 100 may include many different electronic document servers 104 and user devices 106.
The user device 106 is an electronic device capable of requesting and receiving resources (e.g., electronic documents) over the network 102. Example user devices 106 include personal computers, mobile communication devices, and other devices that can send and receive data over the network 102. The user device 106 typically includes a user application, such as a web browser, to facilitate sending and receiving data over the network 102, but a native application executed by the user device 106 may also facilitate sending and receiving data over the network 102.
The one or more third parties 140 include content providers, product designers, product manufacturers, and other parties involved in the design, development, manufacture, marketing, or distribution of products or services.
An electronic document is data that presents a set of content at a user device 106. Examples of electronic documents include web pages, word processing documents, portable Document Format (PDF) documents, images, videos, search result pages, and feed sources. Native applications (e.g., "apps"), such as applications installed on mobile, tablet, or desktop computing devices, are also examples of electronic documents. An electronic document 105 ("electronic document") may be provided by the electronic document server 104 to the user device 106. For example, the electronic document server 104 may include a server hosting a publisher's website. In this example, the user device 106 may initiate a request for a given publisher web page, and the electronic document server 104 hosting the given publisher web page may respond to the request by sending machine hypertext markup language (HTML) code that initiates presentation of the given web page at the user device 106.
The electronic document may include a variety of content. For example, the electronic document 105 may include static content (e.g., text or other specified content) that is within the electronic document itself and/or that does not change over time. The electronic document may also include dynamic content that may change over time or on a per-request basis. For example, a publisher of a given electronic document may maintain a data source for populating portions of the electronic document. In this example, the given electronic document may include a tag or script that causes the user device 106 to request content from a data source when the given electronic document is processed (e.g., rendered or executed) by the user device 106. The user device 106 integrates content obtained from the data source into the presentation of the given electronic document to create a composite electronic document that includes the content obtained from the data source.
In some cases, a given electronic document may include a digital content tag or digital content script that references the DCDS 110. In these cases, the digital content tag or digital content script is executed by the user device 106 when a given electronic document is processed by the user device 106. Execution of the digital content tag or digital content script configures the user device 106 to generate a request 108 for digital content that is transmitted over the network 102 to the DCDS 110. For example, a digital content tag or digital content script may enable the user device 106 to generate a packet data request including a header and payload data. The request 108 may include data, such as the name (or network location) of the server from which the digital content was requested, the name (or network location) of the requesting device (e.g., user device 106), and/or information that the DCDS110 may use to select the digital content to provide in response to the request. The request 108 is transmitted by the user device 106 to a server of the DCDS110 over the network 102 (e.g., a telecommunications network).
The request 108 may include data specifying characteristics of the electronic document and the location where the digital content may be presented. For example, data specifying a reference (e.g., a URL) to an electronic document (e.g., a web page) in which digital content is presented, available locations of the electronic document (e.g., a digital content slot) that are available for presenting the digital content, sizes of the available locations, locations of the available locations in the presentation of the electronic document, and/or media types that are eligible for presentation in those locations may be provided to the DCDS 110. Similarly, data specifying keywords specified for selecting an electronic document ("document keywords") or an entity (e.g., a person, place, or thing) referenced by the electronic document may also be included in the request 108 (e.g., as payload data) and provided to the DCDS110 to facilitate identification of digital content items that are eligible for presentation with the electronic document.
The request 108 may also include data related to other information, such as user-provided information, geographic information indicating a state or region from which the request was submitted, or a context that provides an environment in which the digital content is to be displayed (e.g., a type of device, such as a mobile device or tablet device, on which the digital content is to be displayed). The user-provided information may include demographic data of the user device 106. For example, demographic information may include age, gender, geographic location, education level, marital status, household income, occupation, hobbies, social media data, and whether the user owns a particular project, among other characteristics.
For the cases where the systems discussed herein collect personal information about a user or potentially utilize personal information, the user may be provided with an opportunity to control whether programs or functions collect personal information (e.g., social information network, social behavior or activity, profession, user preferences, or user current location about the user), or whether and/or how to receive content from a content server that may be more relevant to the user. In addition, some data may be anonymized in one or more ways prior to storage or use, thereby deleting personally identifiable information. For example, the identity of the user may be anonymous, and thus unable to determine personally identifiable information for the user, or the geographic location of the user may be generalized where location information is obtained (such as at a city, zip code, or state level), and thus unable to determine the particular location of the user. Thus, the user may control how information about him or her is collected and used by the content server.
Data specifying characteristics of the user device 106 may also be provided in the request 108, such as information identifying a model of the user device 106, a configuration of the user device 106, or a size (e.g., physical size or resolution) of an electronic display (e.g., a touchscreen or desktop display) presenting the electronic document. The request 108 may be transmitted over a packet network, for example, and the request 108 itself may be formatted as packet data having a header and payload data. The header may specify the destination of the packet and the payload data may include any of the information discussed above.
The DCDS110 selects digital content to be presented with a given electronic document in response to receiving the request 108 and/or using information included in the request 108. In some implementations, the DCDS110 is implemented in a distributed computing system (or environment) that includes, for example, a server and a set of multiple computing devices interconnected and that identify and distribute digital content in response to requests 108. The set of multiple computing devices operate together to identify a set of digital content that is eligible to be presented in electronic documents in a corpus of millions or more of available digital content. For example, millions or more of available digital content may be indexed in the digital composition part database 112. Each digital content index entry may reference corresponding digital content and/or include distribution parameters (e.g., selection criteria) that adjust the distribution of the corresponding digital content.
In some implementations, the digital components from the digital components database 112 may include content provided by a third party 140. For example, the digital component database 112 may receive photographs of public intersections from third parties 140 that use machine learning and/or artificial intelligence to navigate public streets. In another example, the digital component database 112 can receive specific questions from a third party 140 that the third party 140 wishes the user to respond to, from the third party 140 that provides services to the bicycle rider.
The identification of eligible digital content may be segmented into a plurality of tasks, which are then distributed among computing devices within a set of multiple computing devices. For example, different computing devices of the set of multiple computing devices may each analyze different portions of the digital component database 112 to identify various digital content having distribution parameters that match the information included in the request 108.
The DCDS110 aggregates the results received from the set of multiple computing devices and uses information associated with the aggregated results to select one or more instances of digital content to be provided in response to the request 108. In turn, the DCDS110 may generate and transmit reply data 114 (e.g., digital data representing a reply) over the network 102, the reply data 114 enabling the user device 106 to integrate the selected set of digital content into a given electronic document such that the selected set of digital content is presented together with the content of the electronic document at the display of the user device 106. The digital content distributed by the DCDS110 and represented by the reply data 114 may include, for example, digital content that solicits input from a user. The input may be analyzed, tagged, and stored as part of an integrated database, such as tagged database 130. The tagged database 130 stores tagged data that has been analyzed and classified. The tagged database 130 may be searched and may store data related to the user, including user demographic information, user response data, and other user characteristics. For example, the tagged database 130 may store and associate anonymous demographic information for a user in association with the user's response to questions previously presented to the user. The input from the user is transmitted from the user device 106 as response data 116 to the data quality processor 120.
The data quality processor 120 generates such digital content that solicits user input, receives and processes user input data, and generates and modifies design space and designs. The data quality processor 120 includes a task processor 122, a data processor 124, and a model generator 126. The task processor 122 generates tasks to be distributed to users. The data processor 124 analyzes and labels the input received in response to the task. Model generator 126 generates and modifies design spaces and designs based on the markup data and input from third parties, such as content providers, product designers, and manufacturers. For ease of explanation, the task processor 122, the data processor 124, and the model generator 126 are shown in FIG. 1 as separate components of the data quality processor 120. Data quality processor 120 may be implemented as a single system on a computer-readable medium, which may be non-transitory. In some embodiments, one or more of the task processor 122, the data processor 124, and the model generator 126 may be implemented as integrated components of a single system.
The task processor 122 creates a digital content or task that solicits input from the user. The task processor 122 communicates with the DCDS110, the electronic document server 104, and the third party 140. The data quality processor 120 may gather data from tasks published directly by the task processor 122 or from third parties, such as third parties 140, that provide the data quality processor 120 with access to its data sources. Tasks may include content for different levels of required interaction, ranging from activities that require a user to draw a picture, to questions that only require the user to select an answer, to single click activities that require the user to grant system permission to access the user's data. In some implementations, the task can include requesting the user to answer the input question. For example, the task presented to the user may include the question "Do you prefer Chocalate or vanilla ice create? (do you like chocolate or vanilla ice cream. In some implementations, the task may include an activity that requires more participation by the user. For example, a task presented to a user may require the user to select one or more portions of an image of a traffic intersection that includes a bicycle, and the user may click on or otherwise indicate the appropriate portion. In some implementations, the task can include a verification protocol challenge, such as a CAPTCHA or a reaccaptcha.
In addition to generating tasks for presentation to the user, the task processor 122 may also modify the tasks. For example, the task processor 122 may modify a task that has been previously provided to one or more users and modify the task to gather different data, to pose a more targeted question, or to otherwise change the direction of the task. The task processor 122 and its output are described in more detail below.
The data processor 124 receives and processes the data to identify missing, inaccurate, under-represented, or under-represented data, and automatically determines a data quality improvement solution. The data processor 124 analyzes a particular data set and determines whether existing data meets a quality threshold based on design guidelines and other inputs. The data processor 124 may process the response data received from the user device and the existing response data. For example, the data processor 124 may determine whether the existing user response data stored in the tagged database 130 for a particular camping backpack design includes a representative number of responses from consumers between the ages of 45 and 54 by: the proportion of responses received from consumers in this age demographic to consumers of other age demographics is determined and the existing ratios are compared to expected or actual ratios for the camping backpack target market. The data processor 124 may also determine whether design values within the design space have been explored or whether there is sufficient data about these values. For example, the data processor 124 may determine whether a particular sized trackpad on a laptop has received a sufficient number of user responses by comparing the existing number of user responses to a threshold amount of user responses.
The data processor 124 may also receive and extract data collected from the digital content distribution process. For example, the data analyzer 124 may receive the request data 108 and the reply data 114 to determine the community and characteristics of the user represented by cookies indicated in the request data 108 and the reply data 114. The data analyzer 124 may store demographic and other characteristic data in a database, such as the tagged database 130. In some implementations, the data analyzer 124 can retrieve data from the tagged database 130 that has been analyzed and tagged by other systems. The data analyzer 124 may, for example, retrieve data from the tagged database 130 indicating demographic data of the user that provided the request data 108 and received the reply data 114. The data processor 124 may segment the data, for example, based on user demographic information. The data processor 124 and its output are described in more detail below.
In addition to the above, the user may be provided with controls that allow the user to select whether and when the systems, programs, or features described herein are capable of collecting user information (e.g., information about the user's social network, social behavior or activity, profession, the user's preferences, or the user's current location), and whether to send content or communications from the server to the user. In addition, certain data may be processed in one or more ways before it is stored or used, thereby deleting personally identifiable information. For example, the user's identity may be processed so that no personally identifiable information can be determined for the user, or the user's geographic location may be summarized (such as at the city, zip code, or state level) with location information being obtained so that no particular location of the user can be determined. Thus, the user may have control over which information is collected about the user, how the information is used, and which information is provided to the user.
The output of such a model may range from a particular design to a predicted user response to the particular design. The model generator 126 and its output will be described in more detail below.
The techniques described below enable the system to continuously and automatically improve data quality and explore design space.
Fig. 2A illustrates an example data flow 200 of a data quality improvement process in the example environment of fig. 1. The operations of data flow 200 are performed by various components of system 100. For example, the operations of data flow 200 may be performed by data quality processor 120 in communication with DCDS110, user device 106, third party 140, and tagged database 130.
The flow begins at step a, where the data processor 124 of the data quality processor 120 analyzes and segments the data set associated with the user population. In some implementations, the data set is existing data retrieved from the tagged database 130. For example, the data set may include a set of user inputs in response to requiring the user to select a reCAPTCHA showing all squares of a portion of a type of vehicle, such as a bicycle, from the photo grid. In another example, the data set may include a set of user inputs in response to a question asking the user how much they typically spend on dog food each month. The user input may be associated with user characteristic data of the user providing the input. For example, user characteristic data may include demographic data and browsing history, as well as other data that the system 100 has access to and is permitted to use by the system 100. In another example, the data set may include a set of user inputs responsive to the following questions: users are asked to rate their likelihood of purchasing a particular handbag product design among a series of handbag designs mapped to subjective descriptors (e.g., "practical" or "fashion").
The data processor 124 segments the data based on various parameters, including user characteristic data. For example, the data processor 124 may divide the data set into segments based on the user age, user location, and/or user interests, as well as other user characteristic data that the system 100 has access to and permits use of the system 100. In another example, the data processor 124 may segment the data based on characteristics of the data itself. For example, the data processor 124 may partition the data set based on the value of a particular subjective factor of the product design, such as the degree to which the user-provided feedback perceives "fashion" in the design of the handbag product.
The flow continues to step B where the data processor 124 identifies inadequate subdivisions in the data set based on one or more metrics. In some embodiments, the metrics are provided by a third party 140, such as a product designer. For example, the metrics may be the number of responding users and the target demographics of the responding users. In some embodiments, the metric is automatically determined. For example, the metric may be a threshold difference in population proportion of the responding users, where a data set with a population proportion difference greater than the threshold difference may be considered less representative of the actual population from which a response is solicited. In one example, the data processor 124 may identify the bicycle detection subdivisions of the vehicle detection set as having insufficient granularity.
The flow continues to step C where the task processor 122 dynamically changes the tasks to be presented to the user based on the identified subdivisions. In some implementations, the task processor 122 dynamically changes existing tasks that have been previously generated and/or presented to the user. In some implementations, the task processor 122 generates a new task to be presented to the user. The task processor 122 may perform such changes in real-time in response to identifying the subdivision. For example, the data quality processor 120 may continuously monitor the quality of the data set and update its metrics based on the new and updated information received.
In one example, the task processor 122 automatically changes aspects of the previously distributed task that requires the user to select all squares that contain a portion of a bicycle by modifying the grid system to use smaller squares to provide better resolution. In step B, the data processor 124 automatically determines that greater granularity is needed and, using this information, the task processor 122 can divide the intersection photograph with the bicycle into smaller squares.
The data quality processor 120 automatically modifies or generates new tasks based on analysis of existing data sets and performs additional operations that facilitate generating tasks. For example, the data quality processor 120 may determine that the modified task includes providing photo data of an intersection with a bicycle in the field of view to the user. The data processor 122 can receive intersection-specific photo data from the third party 140. The data processor 122 may also automatically find the source of the data to be provided as part of the task. For example, the data processor 122 can retrieve photo data from, for example, the tagged database 130 for a public intersection tagged as having at least one bicycle in view. Data processor 122 may then perform data scrubbing operations including scrubbing data of personal identification information, scrubbing data and adjusting data to make data available, among other operations. For example, the data processor 122 may adjust the real-time photo stream from a street camera trained at a public intersection by filtering out images that do not include bicycles in the field of view, adjusting lighting, and creating a larger dynamic range in the images, among other operations. The data processor 122 may perform complex data manipulation operations including operations to remove an object obstructing another object and to enhance the focus of a particular object.
The task processor 122 may also determine one or more distribution parameters that must be satisfied before distributing the task based on the identified subdivision. The distribution parameters may include user characteristics that the user must have in order for the task to be provided. For example, the distribution parameters may include a particular demographic for women living between 18 and 24 years of age on the west coast of the united states.
The task processor 122 may change the task to sample an existing or new region of space for an image, video or audio object within the tagged database 130, for example. The task processor 122 may also test the removal or addition of brand information to, for example, evaluate a user's reaction to a brand or user preferences.
The flow continues to step D where the task processor 122 transmits the dynamically changed or generated task to the DCDS110 for distribution to the user. For example, the task processor 122 may transmit task data indicating tasks to be presented to the user and content to be presented to the user as part of the tasks. The task processor 122 may include distribution parameters that must be satisfied in order to distribute the task to a particular user. For example, the task processor 122 may include demographic data of the target user to whom the task may be presented.
The flow continues to step E where the DCDS110 receives a request 108 for content from the user device 106. When the client device interacts with the digital content, the request 108 is transmitted by the user device 106 to the DCDS 110. For example, if the user of the user device 106 clicks on a link to download a shopping application, the link may cause the user device 106 to transmit a request 108 to the DCDS 110. The request 108 may include interaction tracking data from the client device 106. For example, the request 108 may include tracking data, such as an indication of the interaction, digital content with which the user device 106 interacted, and an identifier that uniquely identifies the user device 106. In some implementations, the request 108 includes an indication of the digital content provider and the location of the target server hosting the requested resource.
The flow continues to step F where the DCDS110 transmits the reply data 114 to the user equipment 106. As described above, the reply data 114 may indicate a task to be distributed to users that satisfy particular distribution parameters in addition to the requested electronic document. In response to the DCDS110 receiving the request 108 and determining that the distribution parameters are satisfied based on the received distribution parameters and the user data indicated in the request 108, the DCDS110 transmits reply data 114 to the user device 106. For example, the DCDS110 may determine, based on receiving the request data 108, that the user of the user device 106 is a 22 year old female living in oregon, and thus satisfies the distribution parameters. The DCDS110 may then transmit the requested electronic document and the dynamically changing task to the user device 106 in the form of reply data 114.
The flow continues to step G where the DCDS110 receives response data 116 from the user equipment 106. In response to the user of the user device 106 completing the task provided in the reply data 114, the response data 116 is transmitted by the user device 106 to the DCDS 110. The response data 116 includes user information such as demographic data, device data, information about user responses, and includes user input in response to tasks provided in the reply data 114. For example, response data 116 may include a user's selection of a square containing a portion of a bicycle, the amount of time the user spends making the selection, her mouse movement pattern, and her anonymous demographic data, device data, and browsing history, all of which she grants access to system 100. The response data may include semantic descriptors provided by the user about the task, product, or design. Semantic descriptors may include any descriptor that provides semantic information about an object, such as a product or design. Semantic descriptors can be generated by humans or artificial intelligence, and can take the form of words (e.g., keywords or key phrases), sentences, symbols, or other descriptors that convey semantic information. Further, semantic descriptors may be assigned to objects based on other actions, such as interacting with presented information (e.g., a photograph or icon), interacting with a rating element (e.g., a product rating tool), or submitting free-form text feedback about the object. The DCDS110 may then provide the data to the data processor 124 for analysis and tagging.
The flow continues to step H where the data processor 124 analyzes the response data 116 from the user device 106. The data processor 124 may analyze the response data 116 to classify the data and tag the data with user information so that the data becomes searchable. For example, the data processor 124 may mark a user-selected square with her demographic information, the amount of time she took to make the selection, and the accuracy of her selection compared to the set of real-valued squares.
The flow proceeds to step I where the data quality processor 120 provides the analyzed data to the tagged database 130. The data processor 124 may provide the tagged data for storage in the tagged database 130 so that the data is searchable.
The system reduces bias in the user feedback profile by selectively soliciting additional feedback from user demographics that are not adequately represented or represented at all.
Fig. 2B and 2C depict the model training process. Server 250 maintains baseline model 252 and task repository 254. The baseline model 252 is a model that serves as a baseline behavior model and can be updated. Task store 254 maintains a set of tasks that can be distributed to users.
Devices a 260a, B260B, … and N260N (collectively referred to as devices 260), each include a model A, B, … N262 a, 262B,. 262N (collectively referred to as models 262), respectively. Each locally maintained model 262 may be updated and refined based on the tasks and model updates provided by server 250.
Each of the devices 260n receives input from a user 270a, 270b,. 270n (collectively referred to as users 270), displays information to the user 270a, 270b,. 270n, and may be controlled by the user 270a, 270b,. 270 n. For example, the device 260 may provide each user 270 with tasks as described above. The tasks may be provided from, for example, task store 254. Each user 270 may provide a semantic map 272a, 272b, … n (collectively semantic maps 272) to device 260 in response to a task. Based on the response provided by the user 270, the device 260 may update the model 262.
In fig. 2C, model training module 256 may generate an updated baseline model 258 based on information from the response provided by user 270 and provide updated baseline model 258 to server 250 to replace or update baseline model 252.
FIG. 3 illustrates an example design space 300. Design space 300 is a visual representation of a conceptual hierarchy of possible design values. The design space 300 may be generated by a system, such as the system 100 shown in fig. 1. For example, model generator 126 of data quality processor 120 may generate design space 300 based on user response data from tagged database 130.
The design space 300 may be multi-dimensional. In this particular example, design space 300 includes two dimensions and is generated as a result of user submitted data in response to questions that require the user to rate various package designs. In other examples, design space 300 may include more than two dimensions and may be represented as a three or more dimensional model. The dimensions include design features such as shape, color, texture, size, or relative distance from another object, among other features.
By automatically generating and modifying the design space 300, the new system reduces the amount of time and resources used to arrive at the final design. The system may focus the design exploration on areas that are most likely to be productive by the metrics specified by the interested third party. For example, the data quality processor 120 may automatically focus the design exploration of the package design into areas most likely to be purchased by consumers between 25 and 34 years old, the third party package designers and the target demographics of the manufacturer 140. The data quality processor 120 may focus the user's request for response data on the package design of most interest to the investigated 25-34 year old user by automatically generating package shapes and designs that fall within the design space 300 that is most likely to be of interest to the user.
In this particular example, the design space 300 maps semantic attributes to geometric features of the design that fall within the space. For example, the design space 300 maps semantic attributes "utility" and "fashion" to specific shapes and forms of package designs, including package designs 302 and 304. These semantic attributes are subjective factors that represent specific qualitative design goals and demographics. Based on the user response data 306 and 308 from the tagged database 130, the data quality processor 120 has determined that the user is most interested in packages that are a mix of "practical" and "fashion". In this particular example, the data processor 124 of the data quality processor 120 has analyzed the user response data from the tagged database 130 and determined that there is a great deal of interest in packages that are more "practical" and at least somewhat "fashionable". Using this determination, model generator 126 may limit design space 300 to focus on package designs that exceed some threshold amount of "fashion" and some other threshold amount of "practical". In some embodiments, model generator 126 may automatically generate a design that satisfies these design criteria without further input from the designer. In some embodiments, model generator 126 may generate package designs that meet "fashion" and "utility" thresholds that have not been previously generated. For example, model generator 126 may generate package designs 312 and 314 without input from third party package designer 140, and designs 312 and 314 may be new, previously unknown package designs.
In some implementations, a probabilistic model can be used that defines a probabilistic ranking of attributes for a given product design or shape or a probabilistic ranking of product designs or shapes given one or more attributes.
In some implementations, the model generator 126 can use machine learning to determine an objective function for a particular user based on subjective feedback from the user. The objective function can be simple and can make general modifications to the product design. As the system 100 collects user response data, the system 100 anonymizes the data and provides the data to a central database that stores and analyzes the collected data to improve the universal behavioral model and allow the system 100 to provide a more personalized policy for each user 102.
For example, the system 100 may utilize a generic profile of users of a particular age, location, interest, and the like. The system 100 may summarize the support configurations across users predicted to have similar interests. In some implementations, the system 100 accepts input from the user's profile information, such as parameters of the user's age, location, and interests.
The system 100 may utilize, for example, a somewhat personalized "shoe size" model. For example, the system 110 may use a generic profile for people of a particular age group, new york people, people who like motorcycles, and so forth. For example, if multiple users indicate similar preferences, the system 100 may determine whether a matching product exists. If a matching product exists, the system 100 can return the product to the user. If there are no matching products, the system 100 can modify the existing product design or generate a new design. In some implementations, the system 100 may perform aggregated personalization or clustering by mapping particular users to existing user clusters or user segments or forming new user clusters or user segments. The system 100 may also be used to identify products or purchasing trends within a particular user. In some implementations, the system 100 can perform aggregated configuration clustering by mapping users to existing customer segments or products or mapping the users' preferences to an existing feature set.
In addition, each model can be personalized. For example, each model may be created from a generic model by varying the model parameters based on characteristics of each user determined from the collected data. Each model may change over long and short periods of time for a particular user. For example, the system 100 may track the level of user interest in a particular design element and adjust the behavioral model when it is determined that the user has lost interest. In some embodiments, each model may also be created from a model that has been personalized using a common profile and further changed for each user. For example, the model may be created by varying model parameters based on characteristics of each user determined from the collected data.
In some implementations, the models can be personalized without using a base model. For example, user response data may be input to model generator 126 and provided to a product designer, manufacturer, or design program for mapping to a product configuration without adjustment. In one example, model generator 126 allows a user to immediately purchase a particular item or set an alert when a particular item is available.
FIG. 4 illustrates an example data flow 400 for a design space exploration process in the example environment of FIG. 1. The operations of data flow 400 are performed by various components of system 100. For example, the operations of data flow 400 may be performed by data quality processor 120 in communication with DCDS110, user device 106, third party 140, and tagged database 130.
The flow begins at step a, where the data processor 124 of the data quality processor 120 analyzes and segments the data set associated with the user population. In some embodiments, the data set is existing data retrieved from the tagged database 130. For example, the data set may include a set of user responses to a question that requires a user to rate two different bag designs with respect to two semantic descriptors at a slidable rate. The user input may be associated with user characteristic data of a user providing the input. For example, user characteristic data may include demographic data and browsing history, as well as other data that the system 100 can access and be licensed to.
The flow continues to step B where the data processor 124 identifies inadequate subdivisions in the data set based on one or more metrics. Details of this step can be found in the description of fig. 2A above with respect to step B.
The flow continues to step C where model generator 126 generates a design space. As described above with respect to fig. 3, the design space is a visual representation of a possible design system. In some embodiments, model generator 126 simply updates the existing design space provided by third party 140. For example, the model generator 126 may receive the design space from the third party package designer 140 and update the design space based on the data set.
The flow continues to step D where the data processor 124 uses the behavioral model to determine one or more subdivisions of the design space to target. For example, data processor 124 uses the output of the behavioral model generated by model generator 126 in step C to determine that a very "fashionable" but also very "practical" design has no threshold amount of user reaction to the design, or that such a design has not yet been generated.
The flow continues to step E where the task processor 122 dynamically changes the tasks to be presented to the user based on the determined one or more segments. As described above with respect to FIG. 2A, the task processor 122 can change an existing task that has been previously generated and/or presented to the user, or generate an entirely new task to be presented to the user. For example, the task processor 122 may generate a new package design that is very "fashion" and also very "practical" to present to the user for feedback.
In some embodiments, model generator 126 may display two or more product designs to allow a user to visualize the differences or deformations between two or more product instances produced by the model.
In some embodiments, the model generator 126 may be integrated with a computer-aided generation design program, and the design of a product or service package may be improved, modified, or changed by the integrated program.
The flow continues to step F where the task processor 122 transmits the dynamically changed or generated task to the DCDS110 for distribution to the user. Details of this step can be found in the description of fig. 2A above with respect to step D.
The flow continues to step G where the DCDS110 receives a request 108 for content from the user device 106. Details of this step can be found in the description of fig. 2A above with respect to step E.
The flow continues with step H where the DCDS110 transmits the reply data 114 to the user equipment 106. Details of this step can be found in the description of fig. 2A above with respect to step F.
The flow continues to step I where the DCDS110 receives response data 116 from the user equipment 106. Details of this step can be found in the description of fig. 2A above with respect to step G.
The flow continues to step J where the data processor 124 analyzes the response data 116 from the user device 106. Details of this step can be found in the description of fig. 2A above with respect to step H.
The flow continues with step K where the model generator 126 updates the design space and/or the behavioral model based on the analyzed response data from the user device 106. Model generator 126 may narrow or enlarge the design space based on feedback from the user. For example, model generator 126 may discard a portion of the design space that has been determined to have a threshold number or percentage of user responses and to have less than a threshold amount of positive responses. Model generator 126 may update the behavioral model to reflect the updated data set. For example, model generator 126 may input the analyzed response data as input to train a behavioral model that predicts the user's acceptance of a particular package design.
The data quality processor 120 may also provide the analyzed data to the tagged database 130. The model and design space updates may be performed concurrently with the data quality processor 120 transmitting the analyzed data to the labeled database 130. In some embodiments, these portions of step K may be performed asynchronously.
The system as described above with respect to fig. 1 and 3-4 automatically changes tasks to provide data quality improvements. In some implementations, the data quality processor 120 may alter the tasks based on data indicating, for example, particular characteristics of the data subdivisions themselves, such as lack of consistent response to one or more types of designs, to different image locations, or to images having particular characteristics, and other factors. In some implementations, the data quality processor 120 may alter the task based on data that indicates, for example, particular characteristics in the user's segment that take an unusually short amount of time to complete the task, such as a particular segment of the user, or lack of consistent response from the user's segment, among other factors. The system 100 automatically creates product designs based on consumer feedback and obtains further feedback on these designs, allowing innovative designs that meet consumer preferences and needs without the need to design and maintain a focal group. Design improvements can be made faster with the help of more representative data. Furthermore, additional feedback may be used as input to a network, e.g., training a behavioral model, to improve model classification of what constitutes a positive or negative example.
Fig. 5A and 5B depict the data flow of a system generating tasks for a user using a behavioral model.
FIG. 5A depicts a data flow 500 in which the system has tasks that the user agrees to personalize the user's reception in the example environment of FIG. 1. The operations of data flow 500 are performed by various components of system 100. For example, the operations of the data flow 400 may be performed by the data quality processor 120 in communication with the DCDS110, the user device 106, the third party 140, and the labeled database 130.
The flow 500 begins with a person uploading one or more design primitives and/or semantic descriptors, such as keywords or ways of expressing user preferences, such as user clicks or text entry (502). The uploaded data may provide information that allows a human to express preferences, semantic descriptions, or ratings, which may be mapped to visual or auditory elements or features of a given design or more of the products or services being designed for use. For example, a market researcher may provide the system 100 with the product design of a backpack and a set of keywords associated with the backpack, such as "sports," functions, "" utilities, "and" specialties. The design elements may be specific to a product or service, and may include the design of physical products and digital products. In another example, a task may present a design, description, or feature to determine a value or price of a product or service. These designs and/or keywords may be stored, for example, in a database of product designs and keywords. The person may also upload a lab plan that determines which design classes or instances should be displayed with which semantic descriptions. The experimental plan may also include statistical measures or other guiding criteria of how, when, or where to display a given design type and semantic description. In some embodiments, the designs and/or keywords may be stored in the numeric component database 112 and/or the tagged database 130.
The flow 500 continues with the system selecting a format for content to be provided to the user (504). For example, the DCDS110 of the system 100 may select a layout of content to be provided to the user. For example, the layout may include the types of user interface elements available and the types of information provided. In one example, the DCDS110 may select a layout for the content that includes sliders and radio buttons for tasks provided to the user at the time of distribution. In another example, the format layout may include a reward provided to the user once the user has responded.
The flow 500 continues with the system preprocessing the data and verifying the correctness of the task to be provided to the user (506). For example, the task processor 122 and the data processor 124 of the data quality processor 120 may pre-process the data and verify the correctness of the task to be provided to the user based on the determined layout from (504).
The process 500 continues with the task of storing the pre-processed data and verifying that it is to be provided to the user (508). In flow 500, the system 100 obtains user consent to provide personalized content, and thus may modify and/or personalize the pre-processed data and verified tasks based on user information.
The flow 500 continues with a user of a website or application interacting with the content of the service (510). For example, the DCDS110 may select preprocessed data and tasks to provide authentication to a particular user of a website or application, as described above with respect to fig. 1, and receive user input from the user's interaction with the content of the service.
The process 500 continues with storing the user's response (512). For example, the DCDS110 may receive a user's response that includes user information, such as user demographic data after the response data has been analyzed and processed by the data processor 124 of the data quality processor 120. The analyzed response data may be tagged 124 by the data processor and stored in a tagged database 130.
The flow 500 continues with building one or more behavioral models based on the user response data (514). Examples of behavioral models include mapping a design, design feature, design deformation, or graphic-based design representation to a semantic description or rating using linear or non-linear function approximation (e.g., a convolutional neural network that enables deep learning). A reversible model may be used that allows mapping from input to output and vice versa, or multiple models may be used to map input to output and vice versa. The behavioral model may also include user demographic information stored with the given user's responses. One example is to use the task data to create a model that allows people to view apparel design feedback for women living in london between the ages of 20 and 30. For example, the behavioral model may take user demographics as input, or be probabilistic in nature, allowing the computer to tune the model response for a given set of demographic parameters. For example, the model generator 126 of the data quality processor 120 may generate a behavioral model based on the user response data, as described above with respect to fig. 1-4. The model or model-based analysis may also include information or input from other sources, such as online surveys, pricing and conversion data, sales attribution models, topic clinics, and focus group feedback. The model or model-based analysis may also use pricing information obtained from the task feedback.
The flow 500 continues with analysis and identification (516) of new ideas and/or concepts based on one or more behavioral models. For example, the model generator 126 of the data quality processor 120 may analyze and identify new design ideas and/or new design concepts based on the output of the behavioral model.
FIG. 5B depicts a data flow 550 in which the system has no user consent to personalize the tasks that the user receives in the example environment of FIG. 1. The operations of data flow 550 are performed by various components of system 100. For example, the operations of data flow 550 may be performed by data quality processor 120 in communication with DCDS110, user device 106, third party 140, and tagged database 130.
The process 550 follows the process 500. Flow 550 begins with the customer uploading one or more design primitives and/or keywords (552). Details of this step can be found in the description of fig. 5A above with respect to (502).
Flow 550 preprocesses the data with the system and verifies the correctness of the task to be provided to the user (556). Details of this step may be found in the description of FIG. 5A above with respect to (506).
The flow 550 continues with the task of storing the pre-processed data and the verified data to be provided to the user (558). In flow 550, the system 100 does not provide user consent to personalize the content, and thus neither the pre-processing data nor the verification task is modified or personalized based on the user information.
The flow 550 continues with a user of the website or application interacting with the content of the service (560). Details of this step can be found in the description of fig. 5A above with respect to (510).
The flow 550 continues with building one or more behavioral models based on the user response data (564). Details of this step can be found in the description of fig. 5A above with respect to (514).
The flow 550 continues with analysis and identification of new ideas and/or concepts based on one or more behavioral models (566). Details of this step may be found in the description of FIG. 5A above with respect to (516).
The flow 550 continues with exploring and/or optimizing the design to generate new design and/or semantic descriptors (568). Details of this step can be found in the description of fig. 5A above with respect to (518).
Fig. 6A and 6B depict the data flow of a system integrating user feedback into a design cycle. Fig. 6A and 6B build on the data flow depicted in fig. 5A and 5B. In some implementations, the system 100 has user consent to personalize the content. In some implementations, the system 100 has no user consent to personalize the content.
FIG. 6A depicts a data flow 600 in which a system integrates user feedback into a design cycle, subject to designer input in the example environment of FIG. 1. The operations of data flow 600 are performed by various components of system 100. For example, the operations of data flow 600 may be performed by data quality processor 120 in communication with DCDS110, user device 106, third party 140, and tagged database 130.
The process 600 follows the process 500 or 550. The flow 600 begins with a person selecting a task format and uploading design primitives and semantic descriptors (602). Details of this step may be found in the description of fig. 5A of (502) or fig. 5B of (552) above. For example, a user may upload design primitives or design shapes and associated keywords to a database. The database stores the initial product design shapes and keywords and any updates based on the user's responses to view the task. For example, the database may store the update based on step 618.
The flow 600 continues with the system selecting a format for content to be provided to the user (604). Details of this step may be found in the description of FIG. 5A above with respect to (504) or FIG. 5B with respect to (554). The format of the primitives and semantic descriptors in storage is selected by a person or may be automatically configured by the system.
The flow 600 continues with the system preprocessing the data and verifying the correctness of the task to be provided to the user (606). Details of this step may be found in the description of FIG. 5A above with respect to (506) or FIG. 5B with respect to (556). For example, the data may be preprocessed to identify a set of keywords that are feasible for a particular design shape. In addition, the data may be verified for appropriateness or correctness for a given format or desired audience of the user. In some implementations, the tasks may be related to a particular country, geographic, or demographic characteristic. For example, interpretation of the French description may be unrelated to the tasks of the large US and European subdivided non-verbal users. These tasks may be stored and provided with content distributed via the internet or via applications.
The process 600 continues with the task of storing the pre-processed data and verifying that it is to be provided to the user (608). Details of this step can be found in the description of fig. 5A above with respect to (508) or fig. 5B with respect to (558).
The flow 600 continues with a user of a website or application interacting with content of the service (610). Details of this step can be found in the description of fig. 5A above with respect to (510) or fig. 5B with respect to (560). For example, a user interacts with content and makes requests for tasks. The user may then interact with the task, providing a response. For example, the user may select the most appropriate keyword or tag for a photograph or image of a product design.
The flow 600 continues with analysis and identification (616) of new ideas and/or concepts based on one or more behavioral models. Details of this step may be found in the description of FIG. 5A above with respect to (516) or FIG. 5B with respect to (566). For example, the model is analyzed and identified to generate data and results that can be used to help determine new tasks.
The flow 600 may optionally include manufacturing and transporting a product having a modified design shape (626). For example, the design process may be integrated with the manufacturing and distribution workflow of the product. In some implementations, products may be distributed to warehouses, stores, and directly to users with similar preferences as the user providing the initial response, such as other users within a group, cluster, location, or user segment, among other shared groups.
FIG. 6B depicts a data flow 650 in which the system integrates user feedback into the design cycle to automatically generate a new design in the example environment of FIG. 1. The operations of data flow 650 are performed by various components of system 100. For example, the operations of data flow 650 may be performed by data quality processor 120 in communication with DCDS110, user device 106, third party 140, and tagged database 130.
A key feature of the flow 650 is that the generative design can be used to enhance the field of view of the human designer, and in some embodiments, the human designer can be completely replaced based on the desired results.
The flow 650 follows the flow 500, 550 or 600. Flow 650 begins with the customer uploading one or more design elements and/or keywords (652). Details of this step may be found in the description of FIG. 5A above with respect to (502), FIG. 5B with respect to (552), or FIG. 6A with respect to (602).
The flow 650 continues with the system preprocessing the data and verifying the correctness of the task to be provided to the user (656). Details of this step may be found in the description of FIG. 5A above with respect to (506), FIG. 5B with respect to (556), or FIG. 6A with respect to (606).
The process 650 continues with the task of storing the pre-processed data and verifying to be provided to the user (658). Details of this step may be found in the description above with respect to fig. 5A for (508), fig. 5B for (558), or fig. 6A for (608).
The process 650 continues with the user of the website or application interacting with the served content 660. Details of this step may be found in the description above with respect to fig. 5A for (510), fig. 5B for (560), or fig. 6A for (610).
The process 650 continues with storing 662 the user's response. Details of this step may be found in the description above with respect to fig. 5A for (512), fig. 5B for (562), or fig. 6A for (612).
The process 650 continues with building one or more behavioral models based on the user response data (664). Details of this step may be found in the description above with respect to fig. 5A for (514), fig. 5B for (564), or fig. 6A for (614).
The flow 650 continues with analysis and identification (666) of new ideas and/or concepts based on the one or more behavior models. Details of this step may be found in the description above with respect to fig. 5A for (516), fig. 5B for (566), or fig. 6A for (616).
The flow 650 continues with generating a modified design shape (672). Details of this step can be found in the description of fig. 6A above with respect to (622).
The process 650 may optionally include manufacturing and transporting the product with the modified design shape (676). Details of this step can be found in the description of FIG. 6A above with respect to (626).
In one example, a task may include showing a user two different products, such as shoes, and asking the user which shoes are more matched to the clothing shown. For example, the garment may be a suit. The garment may include a plurality of different items of apparel and require the user to select a set.
In one example, a task may include a way for a user to configure or modify a design and solicit feedback on how to create a new product or improve an existing product. The improved product may then be provided to users in a new task, who may iteratively improve the product design.
In one example, a task may include showing a user two different product designs, such as a design of an automobile, and providing the user with a feedback mechanism, such as a slider or button, to provide a rating of the degree to which the user agrees with a given attribute, such as a subjective product design descriptor (or adjective) "compact" describing each design.
The computer aided design process may be automatically driven to generate and modify a design according to standards and specifications. The cost function may be used to trade-off constraints and design criteria or factors within the specification. The behavioral model may be used with a cost function to maximize user preferences within constraints, specifications, or manufacturing rules. For example, generative design methods may be used to iterate across a design space guided by behavioral models and cost functions or decision criteria. The use of generative designs includes the use of AI and reinforcement learning to iterate across many design decisions, such as body style of an automobile or color combinations of running shoes, that are allowed or within specifications or constraints, but may represent different customer preferences. The generation type algorithm method comprises one or more of an evolutionary algorithm, a variation automatic encoder and a generation type countermeasure network. These methods may utilize cloud computing to iterate through a large number of design iterations to optimize for a single customer, a customer segment, or multiple customer segments. Examples of algorithms that may be used include evolutionary algorithms, including genetic algorithms that evolve a given design or create a mixture of multiple designs. In some embodiments, the system may use a generative countermeasure network that inputs multiple existing designs to generate an entirely new design. These methods may be iterated to produce a new design, with the task presented to the user to provide feedback regarding the new design. The entire process can be automatically iterated and optimized to produce a new design. In some embodiments, the system may be integrated into an automated manufacturing flow using robotics or 3D printing. In some embodiments, the system may be used to personalize products for a user or user segment based on preferences provided by the user or user segment.
Variational autoencoders and associated generative countermeasure networks can be used with task-generated behavioral models to evolve new designs which are then fed into the system for additional user feedback. The system iterates and scores until a stopping criterion defined by the cost function is reached.
The variational auto-encoder may be used with a library of designs represented as images, shapes, shape-related data structures, shape-related graphs, or polygon data. A variational auto-encoder may be used to create a set of latent factors that effectively represent a reduced set of features that describe a given design. Once the variational auto-encoder is trained, the design is encoded into its latent form and then decoded into its reconstructed design. In one example, an existing library of design representations may be used to build the initial mapping to speed up convergence. Behavioral models developed using task-based feedback can be used to map a given reconstructed design representation to user characterizations or classifications. In some implementations, the user classification can be a good (preferred) and a bad (non-preferred) score. User/human-based characterization may be used with a cost function to create a mathematical representation or score that an optimizer may use to create or refine a design. The output of the optimizer is the design type associated with the latent factor description. Optimization may even use gene mutations or crossovers to create new potential factors. New latent factors may be passed through the decoder/generator to produce a new design representation into which a new task may be inserted. The designs, the latent factors, and the keywords all form a design space and distance metrics that can be used to relate or cluster one design with another design when creating a new task that is shown to the user. The system can automatically iterate around the design space until the cost-based score converges, with the resulting latent factors meeting stopping criteria or being modified very little. At this point, the design is considered complete and can be put into production.
In another embodiment, creating or optimizing a design may include creating a mathematical representation of a design space, where each car or garment design is represented by design features defined by each dimension of the space. The N-dimensional feature space is represented in N space. In many cases, the space may be converted to a low dimensional space. In other cases, the distance metric may be used to cluster or partition the design instances into classes. The distance from each design instance or class to the other can be calculated and stored. The behavioral model represents a mapping between the design instance and a set of keywords or semantic descriptions of the task evaluator/user. This mapping can be explored to find designs similar to the most promising designs ranked so far (optimization) or dissimilar to those designs (exploration) to discover and explore new portions of the design space. The system may be used with distance-based clustering methods, such as K-nearest neighbor or collaborative filtering, to define design instances for future tasks shown to the user.
Fig. 7A and 7B depict the dataflow of a system implementing user feedback to customize existing designs and products.
FIG. 7A depicts a data flow 700 in which a system implements user feedback to modify existing designs and products in the example environment of FIG. 1. The operations of data flow 700 are performed by various components of system 100. For example, the operations of data flow 700 may be performed by data quality processor 120 in communication with DCDS110, user device 106, third party 140, and tagged database 130.
The data stream 700 may be integrated with various manufacturing processes, including 3D printing or automated manufacturing.
The process 700 follows the processes 500, 550, 600, or 650. Flow 700 begins with a customer uploading one or more design elements and/or keywords (702). Details of this step can be found in the description above with respect to fig. 5A for (502), fig. 5B for (552), or fig. 6A for (602), fig. 6B for (652).
The flow 700 continues with the system preprocessing the data and verifying the correctness of the task to be provided to the user (706). Details of this step may be found in the description above with respect to fig. 5A for (506), fig. 5B for (556), fig. 6A for (606), or fig. 6B for (656).
The process 700 continues with the task of storing the pre-processed data and verifying to provide to the user (708). Details of this step may be found in the description above with respect to fig. 5A for (508), fig. 5B for (558), fig. 6A for (608), or fig. 6B for (658).
The flow 700 continues with a user of a website or application interacting with content of a service (710). Details of this step may be found in the description above with respect to fig. 5A for (510), fig. 5B for (560), fig. 6A for (610), or fig. 6B for (660).
The flow 700 continues with building one or more behavioral models based on the user response data (714). Details of this step may be found in the description above for fig. 5A for (514), fig. 5B for (564), fig. 6A for (614), or fig. 6B for (664).
The flow 700 continues with analysis and identification of new ideas and/or concepts based on one or more behavioral models (716). Details of this step may be found in the description above with respect to fig. 5A for (516), fig. 5B for (566), fig. 6A for (616), or fig. 6B for (666).
The flow 700 includes identifying an existing product or product suite using the behavioral model, the existing product or product suite having features that most closely follow the design identified by the behavioral model (720). In some implementations, model generator 126 can access a database or other searchable structure that stores existing product designs and features with or without pre-existing keyword descriptions.
The flow 700 may optionally include manufacturing and shipping a product having the modified design shape (726). Details of this step may be found in the description of FIG. 6A above with respect to (626) or FIG. 6B with respect to (676).
In one example, the task includes displaying a product design kit. For example, a product design kit may include different product designs for different item types, such as room furniture (e.g., designs of chairs, tables, beds, accessories, artwork, etc.), and require a user to select which of the different product designs the user deems visually appealing with the different item types. In some implementations, the user feedback can be used in subsequent tasks to test existing and form new marketing strategies, such as cross-selling and up-selling strategies.
In another example, the tasks include the manner in which the user designs or configures a product, such as a shoe or automobile, and the manner in which the user receives the product or the manufacturer manufactures and transports the product to the user. For example, a user may provide input through a feedback mechanism to indicate a new design or a design configured using a predetermined set of characteristic values.
In another example, the task includes the user modifying an existing product design and having the manufacturer provide (e.g., using a 3D printer) the user with a way to customize the product for the customer. For example, a user may provide input through a feedback mechanism to indicate a modification to an existing design.
FIG. 7B depicts a data flow 750 in which a system implements user feedback to customize software designs and products in the example environment of FIG. 1. The operations of data flow 750 are performed by various components of system 100. For example, the operations of data flow 750 may be performed by data quality processor 120 in communication with DCDS110, user device 106, third party 140, and tagged database 130.
The flow 750 follows the flow 500, 550, 600, 650, or 700. Flow 750 begins with a client uploading one or more design primitives and/or semantic descriptors (752). Details of this step may be found in the description above for fig. 5A for (502), fig. 5B for (552), fig. 6A for (602), fig. 6B for (652), or fig. 7A for (702).
The flow 750 continues with the system selecting a format of content to be provided to the user (754). Details of this step may be found in the description above for fig. 5A for (504), fig. 5B for (554), fig. 6A for (604), fig. 6B for (654), or fig. 7A for (704).
The flow 750 continues with the system preprocessing the data and verifying the correctness of the task to be provided to the user (756). Details of this step may be found in the description above for fig. 5A for (506), fig. 5B for (556), fig. 6A for (606), fig. 6B for (656), or fig. 7A for (706).
The flow 750 continues with the task of storing the pre-processed data and verifying to be provided to the user (758). Details of this step may be found in the description above with respect to fig. 5A for (508), fig. 5B for (558), fig. 6A for (608), fig. 6B for (658), or fig. 7A for (708).
The flow 750 continues with a user of the website or application interacting with the content of the service (760). Details of this step may be found in the description above with respect to fig. 5A for (510), fig. 5B for (560), fig. 6A for (610), fig. 6B for (660), or fig. 7A for (710).
The flow 750 continues with building one or more behavioral models (764) based on the user response data. Details of this step can be found in the description above of fig. 5A for (514), fig. 5B for (564), fig. 6A for (614), fig. 6B for (664), or fig. 7A for (714).
The flow 750 continues with the analysis and identification of new ideas and/or concepts based on the one or more behavioral models (766). Details of this step may be found in the description above for fig. 5A for (516), fig. 5B for (566), fig. 6A for (616), fig. 6B for (666), or fig. 7A for (716).
For example, if the user information indicates that a particular user likes the star wars series and the software product is a mini-game, the data quality processor 120, and in particular the model generator 126, may modify the mini-game to include characters, audio, items, etc. from the star wars series under the approval of the user and the Lucas movie industry.
In another example, a game developer takes an existing or new game that may have many levels, characters, weapons, and allows a user to select through tasks portions that may be configured with user feedback to allow a small portion to be playable in an online or downloadable form.
Flow 750 may optionally include distributing or providing download options with the modified game design to users interacting with the system, as well as providing feedback (774) and actual delivery of the modified game design to other users (775). Details of this step can be found in the description above with respect to fig. 6A of (626), fig. 6B of (676), or fig. 7A of (726).
In one example, the task includes showing the user two different software game characters or attributes related to the game application, such as window size, and asking the user which is more interesting. In some examples, the method may further include providing software preconfigured with the selected persona or attribute for download by the user. In another example, the method may include an article of manufacture for a user to play a game online using software preconfigured with a selected character or attribute. In another example, a behavioral model is created through task feedback from one or more users to design new software, such as apps or games, that may have multiple forms that take advantage of roles, settings, backgrounds tailored to one or more user demographics. In another example, multiple users may use tasks to collaboratively design game features that are inserted into a multiplayer gaming environment that the users have access to.
Flow 750 may optionally include distributing or providing the download option with the modified game design to a user other than the user interacting with the system and providing feedback (776), and actually delivering the modified game design to the other user (777). The embodiment details are the same as (774).
Fig. 7C is a specific example where the system implements user feedback into the design cycle as described with respect to fig. 7B. The system may receive a product (780), such as a software title. For example, the system may receive a game or application. The system may identify attributes and assets of the product. For example, the system may determine attributes of the game, including characters, weapons, scenes, environments, keywords, and so forth. The system may identify assets of the game including thumbnails, demonstration versions, multiple configurations, use/game videos, comments, descriptions, keywords, super-casual versions, and the like. Attributes of the game may be presented to the user (782). For example, character A, character B, weapon A, and weapon B may be presented to the user. The user may make a selection and choose to play the game. The assets of the game may be presented to the user (783). For example, the user may be presented with thumbnail 1 and thumbnail 2 and two sliders that require user input. The first slider asks "while game is this label: 'Role Playing'," and the user can drag the slider to the side that represents their answer. The second slider asks for "Which game is this label: 'puzzle')"
Fig. 7D is a specific example where the system implements user feedback into a design cycle as described with respect to fig. 7B. The user may interact with the task to choose settings or configuration preferences (784). The user may be presented with UI elements related to the task (785). The game or application may be configured using the flow outlined by (786) - (786). For example, the flow may include settings, configuration, or game state descriptions, such as character/skin, inventory/load, settings/environment, tasks (786). These settings and configurations may be encrypted to control the use of the sub-game and restrict the permissions of the sub-game. The engine may then use the settings, configuration, or game state descriptions to take input and generate a usable and feasible environment (787). The game or application may then be a configured/personalized experience (788). The configured game or application may be provided to the user through a UI element (789) associated with the task. The user may then receive or download the configured game or application (790). The user may be the same user in (784) interacting with the task or a different user.
Fig. 7E is a specific example where the system implements user feedback into the design cycle as described with respect to fig. 7B. The user may interact with the task to select settings or configuration preferences (791). The user may be presented with UI elements related to the task (792). The game or application may be configured using the flow outlined by (793) - (795). For example, the flow may include settings, configuration, or game state descriptions, such as character/skin, inventory/load, settings/environment, tasks (793). These settings and configurations may be encrypted to control the use of the sub-game and restrict the permissions of the sub-game. The engine may then map the user settings, configurations, or game state preferences to a preconfigured or precompiled game/application from the library (794). The game or application may then be a configured/personalized experience (795). The configured game or application may be provided to the user through a task-related UI element (796). The user may then receive or download the configured game or application (797). The user may be the same user interacting with the task in (791) or a different user.
FIG. 7F is a specific example of a system implementing user feedback as design feedback using an auto-encoder. As described above, an auto-encoder may be used with a library of designs and used to create a set of latent factors that effectively represent a reduced set of features that describe a given design. The system may access a library of design representations, including shapes, images, graphics, data structures, and the like (a). The system may then generate a reconstructed design (B) using the set of latent factors that effectively represent the set of features describing the design. The system may automatically create an automobile design (C) based on raw data or evolutionary latent factors or improved design features. The system may receive results (D) from a behavioral model created through the user-defined task. The system may use behavioral models as well as user classifications of automobile designs (e.g., sports, family, compact) in the form of semantic representations (E). The system uses a cost function based scoring method (F) and performs optimization to create new feature vectors to automatically create a design object (G). If the optimization converges to a particular set of criteria, the method is complete (H).
Fig. 8 is a flow diagram of an example process 800 for data quality improvement. In some implementations, the process 800 may be performed by one or more systems. For example, the process 800 may be implemented by the data quality processor 120, the DCDS110, the user device 106, and the third party 140 of fig. 1-2 and 4. In some implementations, the process 800 may be implemented as instructions stored on a computer-readable medium, which may be non-transitory, and when executed by one or more servers, the instructions may cause the one or more servers to perform the operations of the process 800.
The process 800 continues by identifying a behavior model corresponding to one or more attributes of the user (806). For example, model generator 126 may identify a behavior model that predicts user behavior based on one or more attributes of the user. In one example, the model generator 126 may identify a behavioral model for a male over 65.
The process 800 continues by dynamically changing the presentation (808) of the item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user. For example, task processor 122 and/or data processor 124 may dynamically change the presentation of an item depicted by a digital component, such as a task question, based on an identified behavioral model corresponding to one or more attributes of a user. In one example, the task processor 122 may modify the task questions regarding the mug design depicted in the task based on the behavioral model for males over 65.
In some implementations, the task processor 122 selects the format in which to solicit feedback from the user for the numeric components that solicit feedback about the project. For example, the task processor 122 selects a format for the user's reaction to a particular mug design.
In some implementations, the data quality processor 120 dynamically changing the presentation of the item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user includes: machine learning or artificial intelligence techniques are used to identify feedback about the item to be solicited from the user. For example, the data quality processor 120 may use the output of the model generator 126 to identify feedback about the mug design to be solicited from the user.
In some implementations, the task processor 122 validates the information requested by the digital components based on specific attributes corresponding to the sub-divisions of the user population that are not adequately represented. For example, the task processor 122 validates information solicited by the task to be distributed based on the user age attribute.
The process 800 continues by determining that the user has particular attributes corresponding to the underrepresented subdivisions of the user population in the database containing information about the item (810). For example, the data processor 124 may determine that a male user segment older than 65 years is an under-represented segment based on a threshold number of responses. In some implementations, the data processor 124 uses statistical analysis to identify segments of the user population that are not adequately represented. The data processor 124 may then determine that the user of the user device 106 has an age attribute corresponding to an under-represented subdivision of the user population in the database containing information about the mug design.
The process 800 continues by generating a presentation including dynamic changes to the item in response to the request, soliciting feedback from the user regarding the item, and including a digital component (812) of a feedback mechanism that enables the user to submit feedback regarding the item in response to determining that the user has particular attributes corresponding to the underrepresented subdivisions of the user population. For example, the task processor 122 may generate a changed presentation of the project or a changed task that solicits feedback from the user of the user device 106 regarding the mug design and includes a feedback mechanism, such as a voting feature, that enables the user to submit feedback regarding the project. In some embodiments, the DCDS110 generates or selects digital components for distribution to the user devices 106, as described above with respect to fig. 1-7.
In some implementations, in response to receiving feedback from users having particular attributes corresponding to the underrepresented subdivisions of the user population, the data quality processor 120 tags the feedback information using one or more attributes of the users and stores the tagged feedback information in a tagged, searchable database, such as the tagged database 130.
The process 800 continues by modifying the presentation (816) of the item as it is distributed to other users having one or more attributes of the user based at least in part on feedback obtained from the user. For example, the data quality processor 120 may modify the presentation as items are distributed to other users sharing one or more attributes of the user based on feedback obtained from the user. This allows the data quality processor 120 to adjust its task and product design based on the attributes of the user interacting with the system.
In some implementations, the data quality processor 120 can modify the presentation of the item when distributed to other users having one or more attributes of the user by selecting a particular feedback mechanism included with the digital component.
FIG. 9 is a flow diagram of an example process 900 for automated design space exploration. In some implementations, the process 900 can be performed by one or more systems. For example, the process 900 may be implemented by the data quality processor 120, the DCDS110, the user device 106, and the third party 140 of fig. 1-2 and 4. In some implementations, the process 600 may be implemented as instructions stored on a computer-readable medium, which may be non-transitory, and when the instructions are executed by one or more servers, the instructions may cause the one or more servers to perform the operations of the process 900.
The process 900 continues by receiving a data set of user-provided information about a particular product design (904). For example, the data quality processor 120 may receive a user-provided response regarding a particular product design, such as a handbag design. In some embodiments, the product design is product specific. The product design may be a service or a software product. For example, the particular product design may be a user interface design of a software application.
The process 900 continues by generating a visual representation (906) that maps the design factors to continuous shapes representing potential product design geometries based on the data set of user-provided information. As described above with respect to fig. 3-4, model generator 126 may generate a design space that maps subjective factors to continuous shapes representing potential product designs. For example, model generator 126 may generate a design space that maps semantic factors, such as descriptors, to continuous shapes representing a possible design hierarchy.
The design space may be reversible such that generating a visual representation that maps the design factors to successive shapes representing potential product design geometries based on the dataset of user-provided information comprises: the visual representation is generated by mapping the potential product design geometries to the design factors.
The process 900 continues by segmenting the visual representation into multiple subdivisions based on the design factor values (908). As described above with respect to fig. 3-4, the model generator 126 may partition the design space into groups based on the values of the subjective factors.
In some implementations, segmenting the visual representation into the plurality of subdivisions based on the design factor value includes segmenting the visual representation into the plurality of subdivisions based on the design factor value such that each subdivision of the visual representation shares a design factor value within a defined range of values. For example, the data processor 124 may partition the design space into multiple subdivisions based on a range of design factor values, such as a subjective rating of how comfortable the handbag design is perceived.
The process 900 continues by selecting a subdivision of the visual representation that contains less than a threshold amount of data points (910). As described above with respect to fig. 3-4, the data processor 124 may identify subdivisions of the data according to a metric such as a threshold number of data points. For example, the data processor 124 may identify very "fashion" and very "utility" packet subdivisions as subdivisions having less than a threshold amount of data points or not reaching other metrics.
In some embodiments, the task processor 122 selects the format in which solicitation is fed back for the numeric components of soliciting information from the user. For example, the task processor 122 selects a format for the user's reaction to the handbag design.
In some embodiments, selecting the format of the solicited information includes selecting a particular feedback mechanism to be provided with the dynamically changing numeric components.
In some implementations, the task processor 122 validates the information solicited by the digital components based on specific attributes corresponding to the sub-divisions of the user population that are not adequately represented. For example, the task processor 122 validates information solicited by the task to be distributed based on the user age attribute.
The process 900 continues by dynamically changing the presentation of the digital components based on the selected subdivision of the visual representation (914), the digital components soliciting information from the user regarding subdivisions of the visual representation that contain less than a threshold amount of data points. For example, as described above with respect to fig. 3-4, the task processor 122 may dynamically change an existing task or generate a new task. In one example, task processor 122 may alter existing tasks to present a user with a new product design that has not been previously generated or presented to the user.
In some implementations, dynamically changing the presentation of the content item includes using machine learning or artificial intelligence techniques to specify information to be requested by the digital component. For example, the data quality processor 120 may use a machine learning model generated by the model generator 126 to determine and specify information to be requested by the task.
In some implementations, dynamically changing the presentation of the digital component includes determining that a user of the user device is in a first user cluster interested in a particular product design based on a request for the digital component presented at the user device, identifying a user interface element of the digital component based on determining that the user of the user device is in the first user cluster interested in the particular product design, and changing the presented user interface element of the digital component. For example, the data processor 124 may determine that the user of the user device 106 is in a cluster of interested users, identify user interface elements for the tasks, and change the user interface elements to customize the tasks for the users who have been interested in designing the handbag.
In some implementations, dynamically changing presentation of the digital components includes determining that a user of the user device is in a first user cluster interested in the particular product design based on a request for digital components presented at the user device, wherein the request for digital components presented at the user device indicates one or more attributes of the user based on information provided by the user, identifying user interface elements of the digital components based on determining that the user of the user device is in the first user cluster interested in the particular product design, and changing the user interface elements presented by the digital components.
In some implementations, the process 900 includes building a behavioral model that predicts user acceptance of potential product design geometries based on the feedback information. For example, the model generator 126 may generate a behavioral model that predicts the user's acceptance of the handbag product design. The modification of the design factors for a particular product design is based at least in part on the behavioral model.
The process 900 continues by distributing the dynamically changing digital components for presentation at the user device (916). For example, the DCDS110 may distribute the task and any requested content as a reply 114 to the user device 106.
The process 900 continues by obtaining feedback information (918) from the user device via a feedback mechanism regarding the subdivisions that contain visual representations of less than a threshold amount of data points. For example, the DCDS110 may receive response data 116 from the user equipment 106 and provide the response data 116 to the data processor 124. As described above with respect to fig. 3-4, the data quality processor 120 may receive feedback information from the user regarding a particular subdivision of the visual representation. For example, the data processor 124 and the DCDS110 may receive feedback information from the user device 106 regarding design space subdivisions having less than a threshold amount of data points.
In some implementations, the request for the digital components for presentation at the user device indicates user demographic information of a user of the user device. Process 900 may also include determining that the user of the user device is in a first user group, such as a group of female users in the state of california, based on the request for the digital composition for presentation at the user device.
In some implementations, the process 900 can also include receiving, from the second user device, a request for a numeric component for presentation at the second user device, the numeric component indicating user demographic information of a user of the second user device. The system 100 (e.g., the data processor 124) may then determine that the user of the second user device is in the same first user group as the user of the user device based on the request for the digital composition presented at the second user device. For example, system 100 (e.g., data processor 124) may determine that the user of the second device is a female in california. In response to determining that the user of the second user device is in the same first user group as the user of the user device, the system 100 may provide the modified product design instead of the particular product design. For example, due to similarities between the user of the first user device and the user of the second user device, the task processor 122 may provide the user of the second user device with a modified handbag design instead of the original handbag design.
3-4, model generator 126 may update the design space and/or the behavioral model. For example, the model generator 126 may update the data set by providing feedback information as input to a training system of a behavioral model or design generator.
In some implementations, the process 900 includes identifying, based on the modified product design, a closest existing product design from a plurality of existing product designs that has a maximum number of common design factor values with the modified product design. For example, the data quality processor 120 may identify an existing product that most closely follows the modified design. For example, rather than creating an entirely new product, the system 100 may modify an existing product and its method of manufacture. In some embodiments, the system 100 may provide the modified product design to an integrated manufacturing system. For example, the data quality processor 120 may provide the modified product design to a 3D printing system or an automated manufacturing system for immediate production.
FIG. 10 is a block diagram of an example computer system 1000 that may be used to perform the operations described above. The system 1000 includes a processor 1010, a memory 1020, a storage device 1030, and an input/output device 1040. Each of the components 1010, 1020, 1030, and 1040 may be interconnected, for example, using a system bus 1050. Processor 1010 is capable of processing instructions for execution within system 1000. In one implementation, the processor 1010 is a single-threaded processor. In another implementation, the processor 1010 is a multi-threaded processor. The processor 1010 is capable of processing instructions stored in the memory 1020 or on the storage device 1030.
The storage device 1030 is capable of providing mass storage for the system 1000. In one implementation, the storage device 1030 is a computer-readable medium. In various different implementations, the storage 1030 may include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices (e.g., cloud storage) over a network, or some other mass storage device.
Input/output device 1040 provides input/output operations for system 1000. In one embodiment, input/output device 1040 may include one or more network interface devices, such as an ethernet card, a serial communication device, such as an RS-232 port, and/or a wireless interface device, such as an 802.11 card. In another embodiment, the input/output devices may include driver devices configured to receive input data and transmit output data to other input/output devices (e.g., keyboard, printer, and display device 1060). However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and so forth.
Although an example processing system has been described in fig. 10, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
An electronic document (which will be simply referred to as a document for brevity) does not necessarily correspond to a file. A document may be stored in a portion of a file that contains other documents, in a single file dedicated to the document in question, or in multiple coordinated files.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage medium (or media) for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus. The computer storage medium can be or be embodied in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, while a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be contained in one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification may be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or a plurality or combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment can implement a variety of different computing model infrastructures, such as Web services, distributed computing and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that contains other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with the instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such a device. Further, the computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; a magneto-optical disk; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a Web page to a Web browser on the user's client device in response to a request received from the Web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server; or include middleware components, such as application servers; or comprises a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification; or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the client device (e.g., for displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) may be received at the server from the client device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated within a single software product or packaged into multiple software products.
Thus, particular embodiments of the present subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (21)
1. A method performed by one or more data processing apparatus for improving the robustness of a data set, comprising:
receiving a request from a user device for a digital component for display at the user device;
determining one or more attributes of the user based on one or more of the information provided by the user or information contained in the request for digital components;
identifying a behavioral model corresponding to the one or more attributes of the user;
dynamically changing the provision of an item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user;
determining that the user has particular attributes corresponding to underrepresented subdivisions of a population of users in a database containing information about the item; and
in response to determining that the user has the particular attribute corresponding to the underrepresented segment of the user population:
in response to the request, generating a digital component that includes the dynamically changed offering of the item, soliciting feedback from the user regarding the item, and including a feedback mechanism that enables the user to submit feedback regarding the item;
updating the database to include the feedback obtained from the user regarding the item; and
modifying, based at least in part on feedback obtained from the user, provision of the item when distributed to other users having the one or more attributes of the user.
2. The method of claim 1, further comprising:
responsive to receiving the feedback from the user having particular attributes corresponding to the underrepresented segments of a user population, tagging the feedback information with the one or more attributes of the user; and
the tagged feedback information is stored in a tagged, searchable database.
3. The method of claim 1 or claim 2, wherein determining that the user has particular attributes corresponding to insufficiently represented segments of a population of users in the database comprises using statistical analysis to identify the insufficiently represented segments of the population of users.
4. The method of any preceding claim, wherein modifying the provision of the item when distributed to other users having the one or more attributes of the user based at least in part on the feedback obtained from the user comprises selecting a particular feedback mechanism that the digital component comprises.
5. The method of any preceding claim, wherein dynamically changing the provision of an item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user comprises using machine learning or artificial intelligence techniques to identify feedback about the item to be solicited from the user.
6. The method of any preceding claim, wherein determining one or more attributes of the user is based on information provided by the user, and
wherein dynamically changing the provision of the item depicted by the digital composition based on the identified behavioral model corresponding to the one or more attributes of the user comprises updating the identified behavioral model based on the information provided by the user.
7. A method according to any preceding claim, wherein determining one or more attributes of the user is based on information contained in the request for the digital component, and
wherein dynamically changing the provision of the item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user comprises updating the identified behavioral model based on the information contained in the request for the digital component.
8. The method of any preceding claim, further comprising:
for the numerical components soliciting feedback from the user regarding the item, selecting a format in which the feedback is solicited; and
validating the information solicited by the digital component based on the particular attribute corresponding to the underrepresented subdivision of the user population.
9. The method according to any of the preceding claims, wherein the digital component displays a suit and two different product design shapes for footwear, and wherein the feedback obtained from the user regarding the item includes a selection of one of the two different product design shapes for footwear that the user believes fits better with the suit.
10. The method of any preceding claim, wherein the digital component displays two different product design shapes of an automobile,
wherein the numerical component specifies a particular subjective product design shape descriptor,
wherein the feedback mechanism is a slider, and
wherein the feedback obtained from the user regarding the item comprises a selection of one of the two different product design shapes of the automobile that the user believes can be better described by the particular subjective product design shape descriptor.
11. The method of any preceding claim, wherein the numerical component displays three or more different product design shapes for three or more different item types,
wherein the feedback obtained from the user regarding the item comprises a selection of two or more different product design shapes that the user considers visually harmonious, an
Wherein the method further comprises using the feedback obtained from the user in a separate model.
12. The method of any preceding claim, wherein the digital component requires the user to create a new product design,
wherein the feedback mechanism receives user input indicating a new product design, an
Wherein the method further comprises providing the user with a product having the new product design.
13. The method of any preceding claim, wherein the digital component requires the user to modify an existing product design to produce a customized product design,
wherein the feedback mechanism receives user input modifying one or more aspects of the existing product design, an
Wherein the method further comprises providing the product with the customized product design to the user.
14. The method of any preceding claim, wherein the digital component displays two different software attributes,
wherein the feedback obtained from the user regarding the item comprises a selection of one of the two different software attributes of the user preferences, an
Wherein the method further comprises providing the user with a software package having the selected one of the two different software attributes of the user preference.
15. A system, comprising:
one or more processors; and
one or more memory elements comprising instructions that, when executed, cause the one or more processors to perform operations comprising:
receiving, from a user device, a request for a digital composition for display at the user device;
determining one or more attributes of the user based on one or more of the information provided by the user or information contained in the request for digital components;
identifying a behavioral model corresponding to the one or more attributes of the user;
dynamically changing the provision of an item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user;
determining that the user has particular attributes corresponding to underrepresented subdivisions of a population of users in a database containing information about the item; and
in response to determining that the user has the particular attribute corresponding to the underrepresented segment of the user population:
in response to the request, generating a digital component comprising a dynamically changed offer of the item, soliciting feedback from the user regarding the item, and including a feedback mechanism that enables the user to submit feedback regarding the item;
updating the database to include the feedback obtained from the user about the item; and
modifying, based at least in part on feedback obtained from the user, provision of the item when distributed to other users having the one or more attributes of the user.
16. The system of claim 15, the operations further comprising:
responsive to receiving the feedback from the user having particular attributes corresponding to the underrepresented segments of a user population, tagging the feedback information with the one or more attributes of the user; and
the tagged feedback information is stored in a tagged, searchable database.
17. The system of claim 15 or claim 16, wherein determining that the user has particular attributes corresponding to insufficiently represented segments of a population of users in the database comprises using statistical analysis to identify the insufficiently represented segments of the population of users.
18. The system of any of claims 15-17, wherein modifying the provision of the item when distributed to other users having the one or more attributes of the user based at least in part on the feedback obtained from the user comprises selecting a particular feedback mechanism that the digital component includes.
19. The system of any of claims 15-18, wherein dynamically changing provision of an item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user comprises using machine learning or artificial intelligence techniques to identify feedback about the item to be solicited from the user.
20. A computer storage medium encoded with instructions that, when executed by a distributed computing system, cause the distributed computing system to perform operations comprising:
receiving a request from a user device for a digital component for display at the user device;
determining one or more attributes of the user based on one or more of information provided by the user or information contained in the request for the digital component;
identifying a behavioral model corresponding to the one or more attributes of the user;
dynamically changing the provision of an item depicted by the digital component based on the identified behavioral model corresponding to the one or more attributes of the user;
determining that the user has particular attributes corresponding to underrepresented subdivisions of a population of users in a database containing information about the item; and
in response to determining that the user has the particular attribute corresponding to the underrepresented segment of the user population:
in response to the request, generating a digital component comprising a dynamically changed offer of the item, soliciting feedback from the user regarding the item, and including a feedback mechanism that enables the user to submit feedback regarding the item;
updating the database to include the feedback obtained from the user regarding the item; and
modifying, based at least in part on the feedback obtained from the user, a provision of the item when distributed to other users having one or more attributes of the user.
21. A computer storage medium encoded with instructions that, when executed by a distributed computing system, cause the distributed computing system to perform operations comprising the method of any of claims 2 to 14.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063010399P | 2020-04-15 | 2020-04-15 | |
US63/010,399 | 2020-04-15 | ||
US16/942,937 | 2020-07-30 | ||
US16/942,937 US11531655B2 (en) | 2020-04-15 | 2020-07-30 | Automatically improving data quality |
PCT/US2021/026848 WO2021211434A1 (en) | 2020-04-15 | 2021-04-12 | Automatically improving data quality |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115244528A true CN115244528A (en) | 2022-10-25 |
Family
ID=78081779
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180019440.XA Pending CN115244528A (en) | 2020-04-15 | 2021-04-12 | Automatic data quality improvement |
Country Status (6)
Country | Link |
---|---|
US (1) | US11531655B2 (en) |
EP (1) | EP4097609A1 (en) |
JP (1) | JP7464741B2 (en) |
KR (1) | KR20220129645A (en) |
CN (1) | CN115244528A (en) |
WO (1) | WO2021211434A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210326494A1 (en) * | 2020-04-15 | 2021-10-21 | Google Llc | Automatically and intelligently exploring design spaces |
EP3951673A1 (en) * | 2020-08-04 | 2022-02-09 | Aptiv Technologies Limited | Method and system of collecting training data suitable for training an autonomous driving system of a vehicle |
US11954424B2 (en) | 2022-05-02 | 2024-04-09 | International Business Machines Corporation | Automatic domain annotation of structured data |
US11689601B1 (en) * | 2022-06-17 | 2023-06-27 | International Business Machines Corporation | Stream quality enhancement |
KR102654474B1 (en) | 2023-02-03 | 2024-04-04 | 서울대학교산학협력단 | Quantum computing device and its quantum error correction method |
Family Cites Families (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100169313A1 (en) * | 2008-12-30 | 2010-07-01 | Expanse Networks, Inc. | Pangenetic Web Item Feedback System |
US9836703B2 (en) * | 2009-12-29 | 2017-12-05 | Advanced Brain Monitoring, Inc. | Systems and methods for assessing team dynamics and effectiveness |
US8255293B1 (en) * | 2011-10-10 | 2012-08-28 | Google Inc. | Product catalog dynamically tailored to user-selected media content |
US8965897B2 (en) * | 2012-02-29 | 2015-02-24 | International Business Machines Corporation | Intelligent product feedback analytics tool |
KR101821284B1 (en) * | 2013-08-22 | 2018-01-23 | 비스포크, 인코포레이티드 | Method and system to create custom products |
US20150228002A1 (en) * | 2014-02-10 | 2015-08-13 | Kelly Berger | Apparatus and method for online search, imaging, modeling, and fulfillment for interior design applications |
US9881332B2 (en) | 2014-05-22 | 2018-01-30 | LogoMix, Inc. | Systems and methods for customizing search results and recommendations |
US11106760B2 (en) * | 2015-05-01 | 2021-08-31 | Einstein Industries, Inc. | Enhanced metadata collection and output |
US11226831B2 (en) * | 2016-12-05 | 2022-01-18 | Facebook, Inc. | Customizing content based on predicted user preferences |
US20180196895A1 (en) | 2017-01-11 | 2018-07-12 | Benjamin James Joseph Soppitt | Reusable product design model |
US10460525B1 (en) * | 2017-03-30 | 2019-10-29 | Amazon Technologies, Inc. | Clothing item measurement and visualization system |
JP6500151B1 (en) | 2018-06-06 | 2019-04-10 | 株式会社電通 | Product proposal support system |
US11100560B2 (en) * | 2019-03-19 | 2021-08-24 | Stitch Fix, Inc. | Extending machine learning training data to generate an artificial intelligence recommendation engine |
-
2020
- 2020-07-30 US US16/942,937 patent/US11531655B2/en active Active
-
2021
- 2021-04-12 WO PCT/US2021/026848 patent/WO2021211434A1/en unknown
- 2021-04-12 CN CN202180019440.XA patent/CN115244528A/en active Pending
- 2021-04-12 KR KR1020227030056A patent/KR20220129645A/en unknown
- 2021-04-12 EP EP21724408.6A patent/EP4097609A1/en active Pending
- 2021-04-12 JP JP2022554661A patent/JP7464741B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
KR20220129645A (en) | 2022-09-23 |
JP7464741B2 (en) | 2024-04-09 |
JP2023519816A (en) | 2023-05-15 |
US20210326312A1 (en) | 2021-10-21 |
EP4097609A1 (en) | 2022-12-07 |
US11531655B2 (en) | 2022-12-20 |
WO2021211434A1 (en) | 2021-10-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Adomavicius et al. | Multi-criteria recommender systems | |
US20220156302A1 (en) | Implementing a graphical user interface to collect information from a user to identify a desired document based on dissimilarity and/or collective closeness to other identified documents | |
JP7464741B2 (en) | Automatically improving data quality | |
JP7361942B2 (en) | Automatic and intelligent exploration of design space | |
US10127596B1 (en) | Systems, methods, and devices for generating recommendations of unique items | |
De Mauro et al. | Machine learning and artificial intelligence use in marketing: a general taxonomy | |
Horváth et al. | Evolutionary computing in recommender systems: a review of recent research | |
Pan et al. | Learning adaptive trust strength with user roles of truster and trustee for trust-aware recommender systems | |
Taghavi et al. | New insights towards developing recommender systems | |
Yan et al. | Implementation of a product-recommender system in an IoT-based smart shopping using fuzzy logic and apriori algorithm | |
Kang et al. | A personalized point-of-interest recommendation system for O2O commerce | |
Yıldız et al. | A Hyper-Personalized Product Recommendation System Focused on Customer Segmentation: An Application in the Fashion Retail Industry | |
Zhang et al. | Recommendation system in social networks with topical attention and probabilistic matrix factorization | |
Elahi et al. | Recommender systems: Challenges and opportunities in the age of big data and artificial intelligence | |
Amato et al. | Agents based multi-criteria decision-aid | |
Fareed et al. | A collaborative filtering recommendation framework utilizing social networks | |
Lim et al. | No. 3. Hybrid-based Recommender System for Online Shopping: A Review: Manuscript Received: 8 February 2023, Accepted: 21 February 2023, Published: 15 March 2023, ORCiD: 0000-0002-7190-0837 | |
Tsafarakis et al. | Applications of MCDA in Marketing and e-Commerce | |
McIlwraith | Algorithms of the intelligent web | |
Schwartz et al. | Style similarity as feedback for product design | |
Nasir et al. | A Survey and Taxonomy of Sequential Recommender Systems for E-commerce Product Recommendation | |
Karlapalepu | A Taxonomy of Sequential Patterns Based Recommendation Systems | |
Da Costa et al. | Improving personalized ranking in recommender systems with multimodal interactions | |
Obaje | A Performant Predict Analytics Approach to Recommender Systems Using Deep Learning Methods | |
Sharma et al. | ADOPTION OF BOOKS RECOMMENDATIONS TECHNIQUES WHILE USING FILTERING METHOS FOR UPHOLDING ACADEMICS IN THE EDUCATIONAL INSTITUTIONS. |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |