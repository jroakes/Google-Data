JP2024519326A - Embedding decision trees in neural networks - Google Patents
Embedding decision trees in neural networks Download PDFInfo
- Publication number
- JP2024519326A JP2024519326A JP2023569726A JP2023569726A JP2024519326A JP 2024519326 A JP2024519326 A JP 2024519326A JP 2023569726 A JP2023569726 A JP 2023569726A JP 2023569726 A JP2023569726 A JP 2023569726A JP 2024519326 A JP2024519326 A JP 2024519326A
- Authority
- JP
- Japan
- Prior art keywords
- layers
- group
- layer
- neural network
- decision tree
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 156
- 238000003066 decision tree Methods 0.000 title claims abstract description 144
- 238000010801 machine learning Methods 0.000 claims abstract description 70
- 238000000034 method Methods 0.000 claims abstract description 61
- 238000012549 training Methods 0.000 claims description 68
- 238000013139 quantization Methods 0.000 claims description 29
- 230000008569 process Effects 0.000 claims description 21
- 230000006870 function Effects 0.000 claims description 18
- 238000004590 computer program Methods 0.000 abstract description 13
- 239000010410 layer Substances 0.000 description 276
- 238000004364 calculation method Methods 0.000 description 19
- 238000012545 processing Methods 0.000 description 18
- 239000011159 matrix material Substances 0.000 description 15
- 238000012986 modification Methods 0.000 description 15
- 230000004048 modification Effects 0.000 description 15
- 230000004913 activation Effects 0.000 description 8
- 238000013459 approach Methods 0.000 description 6
- 230000009471 action Effects 0.000 description 5
- 238000004891 communication Methods 0.000 description 5
- 238000007792 addition Methods 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- 230000004044 response Effects 0.000 description 4
- 239000013598 vector Substances 0.000 description 4
- 230000003993 interaction Effects 0.000 description 3
- 229920002803 thermoplastic polyurethane Polymers 0.000 description 3
- 238000013519 translation Methods 0.000 description 3
- 230000001755 vocal effect Effects 0.000 description 3
- 238000004422 calculation algorithm Methods 0.000 description 2
- 239000003795 chemical substances by application Substances 0.000 description 2
- 238000003062 neural network model Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000010200 validation analysis Methods 0.000 description 2
- 241000009334 Singa Species 0.000 description 1
- 238000009825 accumulation Methods 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 230000003044 adaptive effect Effects 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000003709 image segmentation Methods 0.000 description 1
- 239000011229 interlayer Substances 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 238000011176 pooling Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 238000013138 pruning Methods 0.000 description 1
- 230000002787 reinforcement Effects 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000004088 simulation Methods 0.000 description 1
- 239000002356 single layer Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000013518 transcription Methods 0.000 description 1
- 230000035897 transcription Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Abstract
計算グラフに表されている演算をスケジュールするために、コンピュータ記憶媒体にエンコードされているコンピュータプログラムを含む方法、システム、および装置。方法のうちの１つは、シーケンスに配置されている複数の層を備えるニューラルネットワークを表すデータを受け取ることと、シーケンスにおいて互いに隣接する１つまたは複数の層を各々がえる層の１つまたは複数のグループを選択することと、層の各々のグループについて、層のグループを置き換えるそれぞれの決定ツリーを備える、新たな機械学習モデルを生成することとを含み、それぞれの決定ツリーは、グループにおけるそれぞれの最初の層への入力の量子化バージョンを入力として受け取り、グループにおけるそれぞれの最後の層の出力の量子化バージョンを出力として生成し、それぞれの決定ツリーのツリー深さは、グループの層の数に少なくとも部分的に基づく。
Methods, systems, and apparatuses including a computer program encoded on a computer storage medium for scheduling operations represented in a computation graph, one of the methods includes receiving data representing a neural network comprising a plurality of layers arranged in a sequence, selecting one or more groups of layers, each group of layers having one or more layers adjacent to one another in the sequence, and generating, for each group of layers, a new machine learning model comprising a respective decision tree that replaces the group of layers, each decision tree receiving as input a quantized version of an input to a respective first layer in the group and producing as output a quantized version of an output of a respective last layer in the group, a tree depth of each decision tree based at least in part on the number of layers in the group.
Description
技術分野
本明細書は、大規模なニューラルネットワークにおける決定ツリーの組込みに関する。
TECHNICAL FIELD This specification relates to incorporating decision trees in large-scale neural networks.
背景
ニューラルネットワークは、受け取られる入力に対する出力を予測するために、１つまたは複数の非線形ユニットの層を適用する機械学習モデルである。いくつかのニューラルネットワークは、出力層に加えて、１つまたは複数の隠れ層を含んでいる。各々の隠れ層の出力は、ネットワークにおける次の層、すなわち、次の隠れ層または出力層への入力として使用される。ネットワークの各々の層は、それぞれのパラメータのセットの現在値に従って、受け取られる入力から出力を生成する。
Background Neural networks are machine learning models that apply one or more layers of nonlinear units to predict an output for a received input. Some neural networks contain one or more hidden layers in addition to an output layer. The output of each hidden layer is used as an input to the next layer in the network, i.e., the next hidden layer or the output layer. Each layer of the network generates an output from the received input according to the current values of a set of respective parameters.
より具体的には、各々のニューラルネットワーク層は、複数のノードを含み、各々の層は、ニューラルネットワークによって定義されている一連の演算を表す。一般に、これらの演算は、たとえば、加算や乗算などの線形演算と、たとえば、「Ｒｅｌｕ」または「Ｓｉｇｍｏｉｄ」関数などの非線形活性化関数である、非線形演算とを含むことができる算術演算である。線形演算は、層入力と層の重みとを結合する。各々の層の線形演算は、層の重みが行列またはテンソルの形態で表され、層の層入力がベクトルの形態で表されているテンソル演算を使用して実施できる。 More specifically, each neural network layer includes multiple nodes, and each layer represents a set of operations defined by the neural network. In general, these operations are arithmetic operations that can include linear operations, e.g., addition and multiplication, and nonlinear operations, e.g., nonlinear activation functions, such as the "Relu" or "Sigmoid" functions. The linear operations combine the layer inputs and the layer weights. The linear operations of each layer can be implemented using tensor operations, where the layer weights are represented in the form of matrices or tensors, and the layer inputs of the layer are represented in the form of vectors.
大規模なニューラルネットワーク、すなわち、多くの層と多数のパラメータとを有するニューラルネットワークは、様々な機械学習タスクで、優れたパフォーマンスを示している。しかしながら、これらの大規模なニューラルネットワークは、レイテンシが大きく、大量の計算リソースを消費する可能性があり、たとえば、予測を行うために大量のメモリを必要とし、かなりの数のプロセッササイクルを消費する。ニューラルネットワークの計算効率を高めるための従来の技術の１つは、重み行列の一部を操作して、疎行列にすることである。疎行列は、多数の項がゼロである行列である。 Large neural networks, i.e., neural networks with many layers and a large number of parameters, have shown good performance in various machine learning tasks. However, these large neural networks can have high latency and consume large amounts of computational resources, for example, requiring large amounts of memory and consuming a significant number of processor cycles to make predictions. One conventional technique to improve the computational efficiency of neural networks is to manipulate parts of the weight matrix to make it a sparse matrix. A sparse matrix is a matrix in which many terms are zero.
概要
本明細書は、大規模なニューラルネットワークに決定ツリーを組み込んで、新たな機械学習モデルを生成する技術を説明する。
Overview This document describes techniques for incorporating decision trees into large-scale neural networks to generate new machine learning models.
一般に、本明細書で説明される主題の１つの革新的な態様は、以下の動作を含む方法で具現化することができ、方法は、シーケンスに配置されている複数の層を備えるニューラルネットワークを表すデータを受け取る動作と、複数の層から、層の１つまたは複数のグループを選択する動作とを含み、層の各々のグループは、このシーケンスにおいて互いに隣接する１つまたは複数の層を備えており、方法は、ニューラルネットワークに対応する新たな機械学習モデルを生成する動作をさらに含んでいる。 In general, one innovative aspect of the subject matter described herein may be embodied in a method including the following operations: receiving data representing a neural network having a plurality of layers arranged in a sequence; and selecting one or more groups of layers from the plurality of layers, each group of layers having one or more layers adjacent to one another in the sequence; and the method further including the operation of generating a new machine learning model corresponding to the neural network.
新たな機械学習モデルの生成は、層の各々のグループについて、層のグループを置き換えるそれぞれの決定ツリーを選択することを含む。それぞれの決定ツリーは、グループにおけるそれぞれの最初の層への入力の量子化バージョンを、入力として受け取り、グループにおけるそれぞれの最後の層の出力の量子化バージョンを、出力として生成する。それぞれの決定ツリーのツリー深さは、グループの層の数に少なくとも部分的に基づいている。 Generating the new machine learning model includes, for each group of layers, selecting a respective decision tree to replace the group of layers. Each decision tree receives as input a quantized version of the input to a respective first layer in the group and produces as output a quantized version of the output of a respective last layer in the group. A tree depth of each decision tree is based at least in part on the number of layers in the group.
この態様の他の実施形態は、対応するコンピュータシステムと、装置と、１つまたは複数のコンピュータ記憶デバイスに記録されているコンピュータプログラムとを含んでおり、各々が、方法の動作を実行するように構成されている。 Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs stored on one or more computer storage devices, each configured to perform the operations of the method.
前述した実施形態および他の実施形態は各々、任意選択的に、以下の特徴のうちの１つまたは複数を、単独で、または組み合わせて含むことができる。特に、１つの実施形態は、以下のすべての特徴を組み合わせて含んでいる。 Each of the above and other embodiments may optionally include one or more of the following features, either alone or in combination. In particular, one embodiment includes all of the following features in combination:
この方法は、さらに、ニューラルネットワークのトレーニングデータに基づいて、それぞれの決定ツリーによって置き換えられなかったニューラルネットワークにおける層の少なくとも一部をトレーニングすることによって、新たな機械学習モデルをトレーニングする動作を含むことができる。 The method may further include an act of training a new machine learning model by training at least a portion of the layers in the neural network that were not replaced by the respective decision trees based on the training data of the neural network.
上記で論じたように、層の１つまたは複数のグループの各々を選択する動作はさらに、ニューラルネットワークにおけるそれぞれの初期層を選択することと、各々が、それぞれの初期層を、候補グループにおける最初の層として有する、それぞれの複数の候補グループを生成することと、それぞれの複数の候補グループの各々について、それぞれの決定ツリーによって置き換えられる候補グループに層を有する、対応する新たな機械学習モデルのパフォーマンスを測定する候補グループの、それぞれのパフォーマンス尺度を判定することと、それぞれの複数の候補グループについて、それぞれのパフォーマンス尺度に基づいて、候補グループのうちの１つをグループとして選択することとからなる動作を含むことができる。 As discussed above, the act of selecting each of the one or more groups of layers may further include the acts of selecting a respective initial layer in the neural network, generating a respective plurality of candidate groups, each having the respective initial layer as a first layer in the candidate group, determining, for each of the respective plurality of candidate groups, a respective performance measure of the candidate group that measures the performance of a corresponding new machine learning model having a layer in the candidate group replaced by the respective decision tree, and, for each of the respective plurality of candidate groups, selecting one of the candidate groups as a group based on the respective performance measure.
それぞれの初期層の選択は、ランダムプロセスによって、またはニューラルネットワークのシーケンスに基づいて実施できる。グループにおけるそれぞれの最初の層への入力の量子化バージョンと、グループにおけるそれぞれの最後の層の出力の量子化バージョンとは、２値量子化または３値量子化を使用して生成できる。層のグループを置き換えるそれぞれの決定ツリー層は、ＧｒａｄｉｅｎｔＢｏｏｓｔ決定ツリーまたはＡｄａＢｏｏｓｔ決定ツリーを含むことができる。 The selection of each initial layer can be performed by a random process or based on a sequence of neural networks. A quantized version of the input to each first layer in the group and a quantized version of the output of each last layer in the group can be generated using binary or ternary quantization. Each decision tree layer that replaces a group of layers can include a GradientBoost decision tree or an AdaBoost decision tree.
本方法はさらに、新たな機械学習モデルを、新たな機械学習モデルを実施するように構成されているシステムに出力することを含み得、このシステムは、追加機能、選択機能、または切替機能から選択される１つまたは複数の機能によって決定ツリーを実施するための１つまたは複数の計算ユニットを備える。つまり、（たとえば、トレーニングされている）新たな機械学習モデルは、乗算を実行するために、乗算アキュムレータユニット（ＭＡＣ）などの、より高価な計算ユニットを必要とせずに、決定ツリーを実施するための１つまたは複数の計算ユニット（マルチプレクサまたは算術論理ユニットなど）を含むシステムに出力され得る。 The method may further include outputting the new machine learning model to a system configured to implement the new machine learning model, the system comprising one or more computational units for implementing a decision tree with one or more functions selected from an add function, a select function, or a switch function. That is, the new machine learning model (e.g., trained) may be output to a system including one or more computational units (such as a multiplexer or arithmetic logic unit) for implementing a decision tree without requiring a more expensive computational unit, such as a multiply-accumulator unit (MAC), to perform the multiplications.
本明細書において説明される主題は、以下の利点のうちの１つまたは複数を実現するために特定の実施形態において実施することができる。 The subject matter described herein can be implemented in particular embodiments to realize one or more of the following advantages:
以下に説明される技術を実施する、説明されるシステムは、計算コストを低減し、大規模なニューラルネットワークのための推論計算を実行する効率を向上させることができる。 The described system, which implements the techniques described below, can reduce the computational cost and improve the efficiency of performing inference calculations for large neural networks.
第１に、大規模なニューラルネットワークの１つまたは複数のネットワーク層を、決定ツリーによって置き換えるための、説明される技術は、システムが、ニューラルネットワークのための推論計算を実行する場合、演算量を低減できる。たとえば、ニューラルネットワークの１つまたは複数の層を置き換える決定ツリーは、１つまたはいくつかの層（たとえば、ツリースタンプまたはシャローツリー）のみを有することができる。ツリースタンプおよびシャローツリーに対する演算を実行するための計算コストは、大規模なニューラルネットワークのニューラルネットワーク層を計算することによって必要とされる計算コストよりもはるかに少ない。別の例として、決定ツリーがＡｄａｂｏｏｓｔツリーである場合、Ａｄａｂｏｏｓｔツリーのために乗算演算を実行する必要がないため、システムは、通常、乗算と加算との両方を必要とする従来のニューラルネットワーク層を計算するよりも少ない演算を実行して、効率を向上させることができる。 First, the described technique for replacing one or more network layers of a large neural network with a decision tree can reduce the amount of computation when the system performs inference calculations for the neural network. For example, a decision tree that replaces one or more layers of a neural network can have only one or a few layers (e.g., tree stumps or shallow trees). The computational cost of performing operations on tree stumps and shallow trees is much less than the computational cost required by computing the neural network layers of a large neural network. As another example, if the decision tree is an Adaboost tree, because there is no need to perform multiplication operations for an Adaboost tree, the system can perform fewer operations than computing a traditional neural network layer, which typically requires both multiplications and additions, improving efficiency.
第２に、説明される技術は、１つまたは複数の決定ツリーによって置き換えられるネットワーク層の少なくとも入力および出力を量子化することによって、ニューラルネットワークの合計サイズを低減できる。これらの入力および出力を量子化して、計算のために含まれる有効桁数を削減すると、特に挿入されている決定ツリーおよび少なくとも決定ツリーの隣のニューラルネットワーク層（すなわち、決定ツリーに先行する、または後続する層）の計算コストを低減することができる。量子化によってニューラルネットワークのサイズが低減されているため、量子化によって、計算システムの総メモリ／ストレージ要件も低減できる。これを考慮すると、説明される技術により、より少ないメモリおよび計算能力を有するデバイス（たとえば、スマートフォン、タブレット）は、変更されているニューラルネットワークの推論計算を効率的に実行できるようになる。いくつかの状況では、デバイスにおける１つまたは複数のハードウェアアクセラレータを、特定の変更されているニューラルネットワークの推論計算を実行するようにカスタマイズでき（すなわち、１つまたは複数の層は、１つまたは複数の決定ツリーによって置き換えられ、１つまたは複数の層の入力および出力は、量子化されており）、これにより、デバイスのメモリ使用量を減少させ、消費電力が低減し、推論計算を、より効率的かつ高速に実行できる。 Second, the described technique can reduce the total size of the neural network by quantizing at least the inputs and outputs of the network layers that are replaced by one or more decision trees. Quantizing these inputs and outputs to reduce the number of significant digits involved for the calculation can reduce the computational cost, particularly of the inserted decision tree and at least the neural network layers next to the decision tree (i.e., the layers preceding or following the decision tree). Because quantization reduces the size of the neural network, quantization can also reduce the total memory/storage requirements of the computing system. In light of this, the described technique can enable devices (e.g., smartphones, tablets) with less memory and computing power to efficiently perform the inference calculations of the modified neural network. In some circumstances, one or more hardware accelerators in the device can be customized to perform the inference calculations of the particular modified neural network (i.e., one or more layers are replaced by one or more decision trees and the inputs and outputs of one or more layers are quantized), thereby reducing the memory usage of the device, reducing power consumption, and allowing the inference calculations to be performed more efficiently and faster.
加えて、多くの場合、ニューラルネットワークの推論計算を実行する場合、重要な特徴の有無を正確に検出および表すために、量子化されていない入力および出力によって提供されている高い精度は不要である。つまり、決定ツリーによって置き換えられる層への入力および出力を量子化することによってもたらされる誤差は、効率の向上に比べて最小である。 In addition, in many cases, when performing neural network inference computations, the high precision provided by unquantized inputs and outputs is not required to accurately detect and represent the presence or absence of important features. That is, the error introduced by quantizing the inputs and outputs to layers replaced by decision trees is minimal compared to the efficiency gains.
ニューラルネットワークをトレーニングおよび計算するために、実際に、量子化データが使用されているが、それぞれの決定ツリーと、残りのニューラルネットワーク層とに適するようになる入力および出力を量子化することは、以下で説明される技術のために重要である。より具体的には、システムは、浮動小数点数を量子化して、浮動小数点数を表す桁数を低減できるため、システムは、浮動小数点数の符号、指数、および仮数のために、より少ない桁数を使用することができる。２値量子化および３値量子化の場合、いくつか例を挙げると、システムは、浮動小数点を、｛１、－１｝や｛１、０、－１｝などの整数にマッピングできる。 While quantized data is actually used to train and compute the neural network, quantizing the inputs and outputs to be suitable for each decision tree and the remaining neural network layers is important for the techniques described below. More specifically, the system can quantize floating-point numbers to reduce the number of digits to represent the floating-point number, so that the system can use fewer digits for the sign, exponent, and mantissa of the floating-point number. In the case of binary and ternary quantization, the system can map the floating point to integers such as {1, -1} or {1, 0, -1}, to name a few.
また、説明される技術は、変更されているニューラルネットワーク（すなわち、層が決定ツリーによって置き換えられている新たなニューラルネットワーク）を効率的にトレーニングすることができる。システムは、元のニューラルネットワークをトレーニングするために、同じトレーニング例の少なくとも一部を使用して、変更されているニューラルネットワークにおけるパラメータを、単に微調整するだけでよい。したがって、変更されているニューラルネットワークをトレーニングするために必要な時間期間を、元のニューラルネットワークをトレーニングするよりも大幅に短縮することができる。 The described techniques can also efficiently train the modified neural network (i.e., a new neural network in which layers have been replaced by decision trees). The system simply fine-tunes the parameters in the modified neural network using at least some of the same training examples to train the original neural network. Thus, the period of time required to train the modified neural network can be significantly shorter than training the original neural network.
さらに、説明される技術は、決定ツリーの演算を実行するために、低コストのプログラム可能なハードウェアを使用して、コストを低減することができる。たとえば、計算システムは、乗算アキュムレータユニット（ＭＡＣ）の代わりに、マルチプレクサ（ＭＵＸ）ユニットを使用して、決定ツリーにおける演算を計算できる。当業者には、ＭＵＸユニットが、ＭＡＣユニットよりも、少ない電力およびスペースしか消費しないことが知られている。したがって、計算システムは、ＧＰＵまたはＴＰＵなどの高価なハードウェアアクセラレータではなく、変更されているニューラルネットワークに適したプログラム可能なハードウェアユニットのみを含むことができる。したがって、変更されているニューラルネットワークの推論計算を実行するためのハードウェアシステムを構築する総コストは、元のニューラルネットワークの場合よりもはるかに少ない。 Furthermore, the described technique can reduce costs by using low-cost programmable hardware to perform the operations of the decision tree. For example, the computing system can use multiplexer (MUX) units instead of multiply-accumulator units (MACs) to compute the operations in the decision tree. Those skilled in the art know that MUX units consume less power and space than MAC units. Thus, the computing system can include only programmable hardware units suitable for the modified neural network, rather than expensive hardware accelerators such as GPUs or TPUs. Thus, the total cost of building a hardware system to perform inference computations of the modified neural network is much less than that of the original neural network.
本明細書の主題の１つまたは複数の実施形態の詳細は、添付の図面および以下の説明に記載されている。主題の他の特徴、態様、および利点は、説明、図面、および特許請求の範囲から明らかになるであろう。 The details of one or more embodiments of the subject matter herein are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, drawings, and claims.
詳細な説明
様々な図面における同様の参照番号および指定は、同様の要素を示す。
DETAILED DESCRIPTION Like reference numbers and designations in the various drawings indicate like elements.
大規模なニューラルネットワークのトレーニングおよび計算における効率を高めるための１つの従来のアプローチは、各々の層の活性化入力および重みに対してそれぞれの疎行列を構築することである。しかしながら、疎行列を構築するアプローチは、新たな問題を引き起こす可能性がある。たとえば、疎行列を使用すると、計算システムが、時間期間中、同じメモリアドレスにアクセスできなくなる可能性があり、これは推論局所性の欠如とも呼ばれる。より具体的には、疎行列を構築する場合、計算システムは、元の行列の非ゼロ項を、物理的に互いに遠く離れた位置にあり得る異なるメモリアドレスに格納する。いくつかの状況では、疎行列における各々の項のメモリアドレスは、計算中に動的に変化することがある。たとえば、ゼロ項は、別の非ゼロ項と加算した後に非ゼロになる可能性がある。したがって、計算システムは、メモリに格納されているデータにアクセスする場合、メモリレイテンシ、キャッシュトラッシング、さらにはキャッシュ汚染にさえも悩まされる可能性があり、最終的には、大規模なニューラルネットワークを使用したトレーニングおよび入力処理の演算効率を下げる。 One conventional approach to increase efficiency in training and computing large neural networks is to build a respective sparse matrix for each layer's activation input and weight. However, the approach of building a sparse matrix can introduce new problems. For example, the use of a sparse matrix can prevent a computing system from accessing the same memory address during a period of time, also known as lack of inference locality. More specifically, when building a sparse matrix, the computing system stores the non-zero terms of the original matrix in different memory addresses that may be physically located far away from each other. In some situations, the memory address of each term in a sparse matrix may change dynamically during the computation. For example, a zero term may become non-zero after being added with another non-zero term. Thus, the computing system may suffer from memory latency, cache trashing, and even cache pollution when accessing data stored in memory, ultimately reducing the computational efficiency of training and input processing with large neural networks.
あるいは、別の従来技術によるアプローチは、重み付け行列を層ロジックに融合することである。たとえば、この融合重み付け技術を採用するシステムは、重み行列におけるゼロ項を決定できるため、この項に対して乗算および累積演算を実行しない。これを考慮すると、融合重み付け技術は、入力の１つがゼロであるため、出力がゼロになる計算を実行しないことにより、大規模なニューラルネットワークの計算コストを低減できる。しかしながら、融合重み付けは、代償を伴う。第１に、融合重み付け技術は、ニューラルネットワークがハードウェアアクセラレータにおいて展開されると、変更されてはならず、すなわち、重み行列におけるゼロ項は、計算中、ゼロのままでなければならない。しかしながら、実際には、展開されているニューラルネットワークの重み行列を、新たなトレーニングデータを使用して微調整する必要がある場合がある。第２に、疎重み行列におけるゼロ項が、たとえば、乗算後に加算、その後に別の乗算のような複数の演算によって共有されている場合、システムは、依然として、ゼロ項のための演算を実行する必要がある。 Alternatively, another prior art approach is to fuse the weighting matrix into the layer logic. For example, a system employing this fused weighting technique can determine the zero terms in the weighting matrix and not perform multiplication and accumulation operations on this term. With this in mind, the fused weighting technique can reduce the computational cost of large neural networks by not performing calculations where the output is zero because one of the inputs is zero. However, fused weighting comes at a price. First, the fused weighting technique must not be changed once the neural network is deployed in the hardware accelerator, i.e., the zero terms in the weighting matrix must remain zero during the calculation. However, in practice, the weighting matrix of the deployed neural network may need to be fine-tuned using new training data. Second, if the zero terms in the sparse weighting matrix are shared by multiple operations, such as, for example, a multiplication followed by an addition followed by another multiplication, the system still needs to perform operations for the zero terms.
以下、明細書で説明される技術は、上記の問題に対処することができる。より具体的には、説明される技術は、量子化および決定ツリーを使用して、大規模なニューラルネットワークのために１つまたは複数の推論計算を効率的に実行できる。一般に、説明される技術は、ニューラルネットワークの１つまたは複数の層を、それぞれの決定ツリーによって置き換えることに関し、決定ツリーへの入力および決定ツリーからの出力は、対応する層の入力および出力の量子化バージョンである。 The techniques described below can address the above problems. More specifically, the described techniques can use quantization and decision trees to efficiently perform one or more inference calculations for large neural networks. In general, the described techniques involve replacing one or more layers of a neural network with respective decision trees, where the inputs to and outputs from the decision trees are quantized versions of the inputs and outputs of the corresponding layers.
本明細書は、経済的なハードウェアを使用して、推論計算を実行するための計算コストを低減し、効率を向上させるために、ニューラルネットワークのネットワーク層の１つまたは複数のグループを、１つまたは複数の決定ツリーによって置き換えることによって、新たな機械学習モデルを生成する、１つまたは複数の場所にある１つまたは複数のコンピュータにおいてコンピュータプログラムとして実施されるシステムを説明する。 This specification describes a system, implemented as a computer program in one or more computers at one or more locations, that generates new machine learning models by replacing one or more groups of network layers of a neural network with one or more decision trees to reduce the computational cost and improve efficiency of performing inference calculations using economical hardware.
図１は、例示的なニューラルネットワーク変更エンジン１２０を含む例示的なニューラルネットワーク展開システム１００を示す。
Figure 1 shows an example neural
一般に、ニューラルネットワーク展開システム１００は、ニューラルネットワークを表すデータ１１０を入力として受け取り、トレーニングされている新たな機械学習モデル１８０を出力する。ニューラルネットワーク展開システム１００は、入力ニューラルネットワークモデルのための新たな機械学習モデル１３０を生成するためのニューラルネットワーク変更エンジン１２０を含む。新たな機械学習モデル１３０は、元の入力ニューラルネットワークモデルと、１つまたは複数の決定ツリーによって置き換えられる１つまたは複数のニューラルネットワーク層との混成である。新たな機械学習モデル１３０の生成の詳細が、以下に説明される。ニューラルネットワーク展開システム１００はまた、トレーニングデータ１５０を使用して新たな機械学習モデルをトレーニングするように構成されているトレーニングエンジン１４０と、トレーニングエンジン１４０のためのデータ（たとえば、機械学習モデルのためのトレーニングおよび出力データ、ならびに機械学習モデルを定義するデータ）を格納および提供するように構成されているメモリ１６０とを含んでいる。
In general, the neural
より具体的には、展開システム１００によって受け取られるニューラルネットワークを表すデータ１１０は、ニューラルネットワークの各々の層の演算、および各々のネットワーク層の重みなど、ニューラルネットワークを定義する情報を含むことができる。
More specifically, the
データ１１０は、ニューラルネットワークの他の態様を表すこともできる。たとえば、データ１１０は、ニューラルネットワーク内のネットワーク層の数と、各々の層におけるそれぞれのノードの数と、１つまたは複数のタイプの層間接続、たとえば要素ごとの接続または全接続を表すデータと、ニューラルネットワークにおける各々の層、たとえば、プーリング層、完全接続層、またはＳｏｆｔＭａｘ層などのタイプを表すデータとを含むことができる。
一般に、データ１１０は、複数のトレーニングデータ１５０に対してトレーニングされているトレーニング済ニューラルネットワーク、または、まだトレーニングされていないニューラルネットワークを表すことができる。
In general, the
ニューラルネットワーク展開システム１００は、受け取られているデータ１１０を、ニューラルネットワーク変更エンジン１２０に提供することができる。変更エンジン１２０は、ニューラルネットワークの層の１つまたは複数のグループを選択し、層の各々のグループを、それぞれの決定ツリーで置き換えて、新たな機械学習モデル１３０を出力することができる。層の１つまたは複数のグループの選択が、以下により詳細に説明される。
The neural
層のそれぞれのグループを置き換える各々の決定ツリーは、メモリ１６０に格納され得、変更エンジン１２０のためにアクセス可能である。より具体的には、決定ツリーを表し、メモリ１６０に格納されているデータは、ノード（たとえば、ルートおよび複数のリーフ）の総数と、ノード間の接続性（たとえば、リーフが１つまたは複数の他のリーフにどのように接続されているか）と、１つまたは複数のノード演算（たとえば、１つまたは複数のノードのための論理比較）とを指定するデータを含むことができる。
Each decision tree that replaces a respective group of layers may be stored in
変更エンジン１２０は、層のグループを置き換えるためのそれぞれの決定ツリーを自動的に決定することができる。あるいは、層のそれぞれのグループを置き換えるための決定ツリーのタイプは、ユーザによって、または変更エンジン１２０の外部の１つまたは複数のコンピュータによって実施されているコンピュータプログラムによって、事前に決定することができる。決定ツリーは、ＧｒａｄｉｅｎｔＢｏｏｓｔツリーまたはＡｄａＢｏｏｓｔツリーであることができる。
The
ＧｒａｄｉｅｎｔＢｏｏｓｔ（または、ｇｒａｄｉｅｎｔ ｂｏｏｓｔ）ツリーは、勾配ブーストを介して取得でき、勾配ブーストは、１つまたは複数の単純な予測モデル（たとえば、決定ツリー）の組合せ（たとえば、加重和）の形態でモデルを作り出す機械学習方法である。ＡｄａＢｏｏｓｔ（または、Ａｄａｐｔｉｖｅ Ｂｏｏｓｔｉｎｇ）は、１つまたは複数の単純な予測モデル（たとえば、決定ツリー）を適応的に組み合わせることで、トレーニング中に、パフォーマンスの低い単純な予測モデル（たとえば、誤った分類の尺度など）に大きな重みが割り当てられるようにし、ＡｄａＢｏｏｓｔによって生成されるトレーニングされているモデルは、特定の入力が与えられた場合に予測を正確に生成する可能性が高くなる。 GradientBoost (or gradient boost) trees can be obtained via gradient boosting, which is a machine learning method that produces models in the form of a combination (e.g., weighted sum) of one or more simple predictive models (e.g., decision trees). AdaBoost (or Adaptive Boosting) adaptively combines one or more simple predictive models (e.g., decision trees) so that during training, greater weights are assigned to poorly performing simple predictive models (e.g., misclassification measures, etc.), and the trained model produced by AdaBoost is more likely to produce accurate predictions given a particular input.
変更エンジン１２０はまた、データ１１０によって表されているニューラルネットワークの少なくとも一部を量子化することができる。たとえば、変更エンジン１２０は、決定ツリーによって置き換えられる層のグループの最初の層への入力、および層のグループの最後の層からの出力を量子化することができる。あるいは、変更エンジン１２０は、各々の層への入力および出力が量子化されているように、ニューラルネットワーク全体を量子化することができる。さらに、変更エンジン１２０は、トレーニングデータ１５０の少なくとも一部を量子化し、量子化されているトレーニングデータを使用して、それぞれの決定ツリー、または新たな機械学習モデル１３０の少なくとも一部、あるいはその両方をトレーニングすることができる。
The
量子化は、大きなセットの入力値を、小さなセットの出力値にマッピングするプロセスであり、通常は、丸めと切り捨てに使用される。より具体的には、量子化を使用して、数値の精度を下げることができる。たとえば、量子化は、浮動小数点数の精度を３２ビットから８ビットに下げることができる。ニューラルネットワークの文脈に関連して、変更エンジン１２０は、ニューラルネットワーク層の活性化テンソル、重みテンソル、または層出力を、８ビットから４ビット、さらには１ビット（たとえば、仮数部に１ビット）の精度で量子化することができる。量子化（たとえば、２値量子化および３値量子化）の詳細は、図２Ａおよび図２Ｂに関連して説明される。
Quantization is the process of mapping a large set of input values to a smaller set of output values, typically using rounding and truncation. More specifically, quantization can be used to reduce the precision of a number. For example, quantization can reduce the precision of a floating-point number from 32 bits to 8 bits. In the context of neural networks, the
変更エンジン１２０は、新たな機械学習モデル１３０をトレーニングエンジン１４０に提供することができる。その後、トレーニングエンジン１４０は、元のニューラルネットワーク１１０をトレーニングするために使用されているトレーニングデータ１５０に基づいて、新たな機械学習モデル１３０の少なくとも一部をトレーニングし、トレーニングされている新たな機械学習モデル１８０を、システム１００の出力として出力することができる。より具体的には、トレーニングエンジン１４０は、トレーニングデータ１５０を使用して、時間期間中、決定ツリーによって置き換えられていないニューラルネットワークの残りの層をトレーニングすることができる。あるいは、または加えて、トレーニングエンジン１４０は、各々の決定ツリーのそれぞれの勾配を仮定して、量子化されているトレーニングデータに基づいて、新たな機械学習モデル１３０全体をトレーニングすることができる。
The
新たな機械学習モデル１３０をトレーニングするための時間期間は、数分、数時間、または数日になる場合がある。あるいは、または加えて、時間期間は、新たな機械学習モデル１３０の少なくとも一部をトレーニングするために使用されているトレーニングデータ１５０のサイズに基づくことができる。たとえば、時間期間は、トレーニングデータ１５０の１００個のミニバッチまたは１０００個のミニバッチを使用して、新たな機械学習モデル１３０をトレーニング（たとえば、微調整）するために必要な時間によって決定することができる。
The time period for training the new machine learning model 130 may be minutes, hours, or days. Alternatively, or in addition, the time period may be based on the size of the
トレーニングエンジン１４０は、元のニューラルネットワークの１つまたは複数の層を置き換える決定ツリーをトレーニングすることができる。
The
より具体的には、トレーニングエンジン１４０は、元のニューラルネットワークのネットワーク層を決定ツリーで置き換える前に、決定ツリーをトレーニングする。トレーニングエンジン１４０は、上記で説明したように、新たな機械学習モデル１３０における決定ツリーを微調整することもできる。
More specifically, the
決定ツリーをトレーニングするために、トレーニングエンジン１４０は、元のニューラルネットワークをトレーニングするための、同じであるが量子化されているトレーニングデータ１５０の対応する部分を、トレーニングデータとして使用することができる。具体的には、トレーニングエンジン１４０は、それぞれのトレーニングサンプルを使用して決定ツリーをトレーニングする。それぞれのトレーニングの例は、層のグループの最初の層への層入力の量子化バージョンと、層のグループの最後の層からの層出力の量子化バージョンとを含む。層のグループは、決定ツリーによって置き換えられる。層入力および層出力の量子化バージョンは、元のニューラルネットワークにおける層のグループをトレーニングするために使用されている、対応するトレーニングデータ１５０の層入力および層出力に関連付けられる。
To train the decision tree, the
トレーニングエンジン１４０は、決定ツリーをトレーニングする場合、損失を定義し、損失が、特定の基準を下回るまで損失を減少させるようにノード演算を調整することができる。損失は、ラベルの誤りを示すヒンジ損失、またはエントロピ理論に基づく情報獲得を表す対数損失、または決定ツリーのトレーニングのために適した他の任意の損失である可能性がある。トレーニングエンジン１４０は、１つまたは複数のノードにおける論理比較演算のそれぞれの臨界値などのノード演算を調整することができる。いくつかの状況では、トレーニングエンジン１４０は、トレーニング中に決定ツリーを枝刈りすること、すなわち、１つまたは複数のリーフと、１つまたは複数のリーフに関連付けられているすべての分岐および子リーフを削除することによって、過剰適合を低減することができる。トレーニングエンジン１４０は、過剰適合を検出し改善するための検証セットとして、元のトレーニングデータセットの一部を残しておくことができる。
When training a decision tree, the
新たな機械学習モデル１３０の少なくとも一部をトレーニングするために使用されているトレーニングエンジン１４０は、中央処理ユニット（ＣＰＵ）、グラフィック処理ユニット（ＧＰＵ）、テンソル処理ユニット（ＴＰＵ）、またはニューラルネットワークの演算を実行するのに適した他の任意の計算ユニットを含むことができる。特に、新たな機械学習モデルは、線形演算（たとえば、主にテンソル演算）を含む、置き換えられていない層を依然として有することができ、したがって、トレーニングエンジン１４０は、トレーニングプロセスを容易にするために、ＣＰＵまたはＧＰＵよりも多くのＴＰＵを含むことができる。
The
トレーニングされている新たな機械学習モデル１８０は、入力が与えられた場合に、推論を効率的に生成するために使用することができる。トレーニングされている新たな機械学習モデル１８０は、より少ない計算能力を使用して、推論を生成するために使用することができる。たとえば、ニューラルネットワーク層の１つまたは複数の層は、より少ないノードを有する浅い決定ツリーによって置き換えられているため、トレーニングされている新たな機械学習モデル１８０は、格納するためのメモリサイズを、元のニューラルネットワークよりも少ないメモリサイズしか必要としない。別の例として、トレーニングされている新たな機械学習モデルは、より少ないビットを使用して表されている量子化されている入力および出力と互換性があるため、計算中のシステムメモリ帯域幅を低減させる。さらに、ＭＵＸユニットまたは算術論理（ＡＬＣ）ユニットなどの計算ユニットを使用して、決定ツリーにおける演算を実行できるため、新たな機械学習モデル１８０の決定ツリーの推論計算を実行するために、ニューラルネットワーク推論エンジンまたはシステムは、ＴＰＵまたはＧＰＵなどの大型で高価な計算ユニットを、１つまたは複数のＭＵＸユニットまたはＡＬＣユニットを有する、より小型で安価なプログラマブルコアで置き換えることができる。したがって、新たな機械学習モデルの推論を生成するための、デバイスの総コストおよび総サイズを低減することができる。 The new machine learning model 180 being trained can be used to efficiently generate inferences given an input. The new machine learning model 180 being trained can be used to generate inferences using less computational power. For example, the new machine learning model 180 being trained requires less memory size to store than the original neural network because one or more layers of the neural network layer have been replaced by a shallow decision tree with fewer nodes. As another example, the new machine learning model being trained is compatible with quantized inputs and outputs that are represented using fewer bits, thereby reducing system memory bandwidth during calculations. Furthermore, because computational units such as MUX units or arithmetic logic (ALC) units can be used to perform operations in the decision tree, the neural network inference engine or system can replace large and expensive computational units such as TPUs or GPUs with smaller and cheaper programmable cores having one or more MUX units or ALC units to perform inference calculations of the decision tree of the new machine learning model 180. Thus, the total cost and size of the device for generating inferences for the new machine learning model can be reduced.
図２Ａは、２値量子化出力２２５を有する決定ツリーを有する例示的な新たな機械学習モデル２９５の一部を例示する。
Figure 2A illustrates a portion of an exemplary new
図２Ａに示されているように、入力データ１１０によって表されている元のニューラルネットワーク２００の一部は、複数のネットワーク層を含んでいる。複数のネットワーク層は、対応する決定ツリーによって置き換えられるようにシステム１００によって決定されているネットワーク層のグループ２９０と、ネットワーク層のグループ２９０の最初の層に先行する、第１のネットワーク層２１０と、ネットワーク層のグループの最後の層に後続する、第２のネットワーク層２３０とを含むことができる。
As shown in FIG. 2A, the portion of the original neural network 200 represented by the
ニューラルネットワーク２００の一部における複数の層の各々の層は、各々が線形演算および非線形演算を表す複数のノードを有する。たとえば、ネットワーク層２１０は、ノード２１０ａ～ｆを含む。別の例として、ネットワーク層２３０は、ノード２３０ａ～２３０ｆを含む。また、ネットワーク層のグループ２９０の各々の層は、それぞれの数のノードを含む（図示せず）。
Each of the layers in the portion of neural network 200 has a number of nodes, each of which represents a linear operation and a nonlinear operation. For example, network layer 210 includes
ニューラルネットワーク２００の一部のネットワーク層は、ニューラルネットワークへの各々の入力について、先行する層が、層出力を生成し、その出力を、層活性化入力として後続する層に提供できるように、シーケンスに従って配置されている。たとえば、ネットワーク層のグループ２９０の最初の層は、ネットワーク層２１０から、層活性化入力２１３として受け取る。別の例として、ネットワーク層のグループ２９０の最後の層は、層出力２１７を、後続する層２３０に提供する。
Some of the network layers of neural network 200 are arranged in a sequence such that for each input to the neural network, the preceding layer generates a layer output and provides that output as a layer activation input to the subsequent layer. For example, the first layer of
入力および出力は、計算要件に従って、それぞれの精度を有することができる。たとえば、入力および出力は、３２ビット精度の浮動小数点形式を有することができる。別の例として、ニューラルネットワークの最初の数層の入力と、最後の数層からの出力とは、中間層よりも高い精度を有する可能性がある。 The inputs and outputs can have different precisions according to computational requirements. For example, the inputs and outputs can have floating-point format with 32-bit precision. As another example, the inputs for the first few layers and the outputs from the last few layers of a neural network can have higher precision than the intermediate layers.
いくつかの実施態様では、システム１００は、異なる量子化方法を使用して、ニューラルネットワークの１つまたは複数の層の入力および出力を量子化し、精度を低下させることができる。たとえば、活性化入力２１３および層出力２１７は、２値量子化または３値量子化を使用して量子化することができ、８ビット、またはさらには１ビットの精度も有することができる。あるいは、システム１００は、ネットワーク層について、重み入力または活性化入力の一部のみを量子化することができる。
In some implementations, the
上記で説明したように、量子化は、数値の精度を低下させる（たとえば、数値の符号、指数、および仮数を表すビット数を減少させる）プロセスである。２値量子化および３値量子化は、量子化プロセスの分岐である。 As explained above, quantization is the process of reducing the precision of a numeric value (e.g., reducing the number of bits that represent the sign, exponent, and mantissa of the number). Binary quantization and ternary quantization are branches of the quantization process.
２値量子化に関して、ニューラルネットワーク展開システム１００および図２Ａに関連して、システム１００は、浮動小数点数を２値セット｛１、－１｝に量子化することができる。２値量子化は、システム１００が、符号に１桁、指数累乗に０桁、および浮動小数点数の有効数に１桁だけを使用する特定の量子化プロセスと考えることができる。より具体的には、ほんの数例を挙げると、２値量子化を使用して、浮動小数点形式の重み０．７４４を、１として量子化でき、浮動小数点形式の活性化入力－０．２１を、－１として量子化できる。
Regarding binary quantization, and with reference to neural
同様に、３値量子化は、２値量子化の代替手段であり、モデルサイズは大きくなるが、精度を向上させることができる。図２Ｂに関連して、システム１００は、浮動小数点数を、３値セット｛１、０、－１｝に量子化することができる。より具体的には、０．６６より大きい浮動小数点形式の正規化重みは、１として量子化でき、０．６６より小さく－０．６６より大きい別の正規化重みは、０として量子化でき、－０．６６より小さい別の正規化重みは、－１として量子化できる。
Similarly, ternary quantization is an alternative to binary quantization that can improve accuracy at the expense of larger model size. With reference to FIG. 2B, the
システム１００は、各々の量子化入力または出力を、それぞれのスケーリング係数で乗算することによって、それぞれのスケーリング係数を、各々の量子化入力および出力に適用することができる。システム１００は、特定の入力を与えられると、量子化されているニューラルネットワークと、量子化されていない元のニューラルネットワークとの間の近似性（または類似性）の尺度に基づいて、それぞれのスケーリング係数を取得することができる。システム１００は、変更されているニューラルネットワークをトレーニングした後に、スケーリング係数を決定し、変更されているニューラルネットワークについて推論計算を実行する場合、スケーリング係数を定数として設定することができる。
The
図２Ａの参照に戻ると、ニューラルネットワーク変更エンジン１２０は、ニューラルネットワーク２００の元の部分のネットワーク層のグループ２９０を、ネットワーク層のグループ２９０における少なくとも層の数に基づくツリー深さを有する決定ツリー２２０によって置き換えることによって、新たな機械学習モデル２９５（または、その新たな部分）を生成することができる。決定ツリーは、ネットワーク層のグループを置き換えるのに適している限り、任意のタイプであることができる。たとえば、決定ツリーは、ＧｒａｄｉｅｎｔＢｏｏｓｔ決定ツリーまたはＡｄａＢｏｏｓｔ決定ツリーであることができる。
Returning to reference to FIG. 2A , the neural
システム１００は、少なくとも、ネットワーク層のグループ２９０の最初の層への入力２１３、およびネットワーク層のグループ２９０の最後の層からの出力２１７に対して２値量子化を実行することができる。あるいは、システム１００は、ニューラルネットワーク全体に対して２値量子化を実行する。
The
決定ツリー２００は、先行する層２１０から２値量子化入力２１５を受け取り、量子化出力２２５を、後続する層２３０に出力することができる。より具体的には、量子化入力２１５は、先行する層２１０のノード２１０ａ～ｆからの２値入力（１または－１のいずれか）を含んでいる。同様に、例示のみを目的として、量子化出力２２５は、｛１｝を表す出力２２５ａ、または｛－１｝を表す出力２２５ｂのいずれかであることができる。決定ツリーからの各々の出力は、１または－１のいずれかとして量子化され、後続する層２３０へ提供される。
The decision tree 200 may receive binary
ネットワーク層のグループ２９０を、決定ツリー２２０によって置き換える前に、システム１００は、トレーニング例の対応する部分の量子化バージョンを使用して、決定ツリー２２０をトレーニングすることができる。より具体的には、システム１００は、元のニューラルネットワークのトレーニング例が与えられた際の、層のグループ２９０の最初の層への入力の量子化バージョンと、層のグループ２９０の最後の層からの出力の量子化バージョンとを取得することができる。システム１００は、入力の量子化バージョンを、決定ツリー２２０への入力として設定し、出力の量子化バージョンを、決定ツリー２２０からの出力として設定し、入力および出力の量子化バージョンを使用して、決定２２０（ｔｈｅ ｄｅｃｉｓｉｏｎ ２２０）をトレーニングすることができる。
Before replacing the group of network layers 290 with the
図２Ｂは、３値量子化出力２７５を有する決定ツリーを有する、別の例示的な新たな機械学習モデル２５５の一部を示す。
Figure 2B shows a portion of another example new
図２Ａと同様に、システム１００は、ニューラルネットワーク２５０の一部における層の異なるグループ２８５を、異なる決定ツリー２７０によって置き換え、３値量子化を使用して、決定ツリーの入力および出力を量子化することができる。量子化入力２６５および量子化出力２７５は、値セット｛－１、０、１｝のうちの１つを含む。例示のみを目的として、量子化出力２７５は、｛１｝を表す出力２７５ａ、｛０｝を表す出力２７５ｂ、および｛－１｝を表す出力２７５ｃのうちの１つであることができる。
2A, the
例示を容易にするために、図２Ａではネットワーク層のグループの数が３であり、図２Ｂではその数は５であるが、決定ツリーによって置き換えられるネットワーク層のグループの数は、システム１００によって決定されている任意の適切な値とすることができる。たとえば、その数は１、１０、または５０であることができる。
For ease of illustration, in FIG. 2A the number of network layer groups is 3 and in FIG. 2B the number is 5, however, the number of network layer groups replaced by the decision tree can be any suitable value as determined by the
図３は、トレーニングされている新たな機械学習モデルを生成するための例示的なプロセス３００のフロー図である。便宜上、プロセス３００は、１つまたは複数の場所に配置されている１つまたは複数のコンピュータのシステムによって実行されているものとして説明される。たとえば、ニューラルネットワーク展開システム１００、たとえば図１に示され、本明細書に従って適切にプログラムされているシステム１００は、プロセス３００を実行することができる。
Figure 3 is a flow diagram of an exemplary process 300 for generating a new machine learning model that is trained. For convenience, process 300 is described as being performed by one or more computer systems located at one or more locations. For example, a neural
システムは、シーケンスに配置されている複数の層を備えたニューラルネットワークを表すデータを受け取る（３１０）。受け取られるデータは、ほんの数例を挙げると、ニューラルネットワークのネットワーク層の各々によって実行されている演算と、各々のネットワーク層の重みとを含むことができる。ネットワーク層は、先行する層からの層出力が、後続する層に層入力として提供されるように、シーケンスに従って配置されている。 The system receives (310) data representing a neural network with multiple layers arranged in a sequence. The received data can include operations being performed by each of the network layers of the neural network and the weights of each network layer, to name just a few. The network layers are arranged in a sequence such that layer outputs from a preceding layer are provided as layer inputs to a subsequent layer.
システムは、複数の層から、層の１つまたは複数のグループを選択し、層の各々のグループは、シーケンスにおいて互いに隣接する１つまたは複数の層を備えている（３２０）。たとえば、システムは、層の３つのグループを選択することができ、層の最初のグループは、１つの層のみを含み、層の２番目のグループは、３つの層を含み、層の最後のグループは、ニューラルネットワークの最後から２番目の層を有する５つの層を含んでいる。 The system selects one or more groups of layers from the plurality of layers, each group of layers comprising one or more layers adjacent to one another in sequence (320). For example, the system may select three groups of layers, with the first group of layers including only one layer, the second group of layers including three layers, and the last group of layers including five layers with the penultimate layer of the neural network.
システムは、選択されている層の１つまたは複数のグループの各々を、それぞれの決定ツリーによって置き換えることによって、ニューラルネットワークに対応する新たな機械学習モデルを生成する（３３０）。 The system generates a new machine learning model corresponding to the neural network by replacing each of the one or more groups of selected layers with a respective decision tree (330).
層のそれぞれのグループを置き換えるためのそれぞれの決定ツリーは、少なくとも層のグループにおける層の数に基づいたツリー深さを有することができる。たとえば、決定ツリーのツリー深さは、層のグループにおける層の数に等しい。別の例として、決定ツリーのツリー深さは３であり、決定ツリーによって置き換えられる層のグループは、５つのネットワーク層を有する。 Each decision tree for replacing a respective group of layers can have a tree depth based at least on the number of layers in the layer group. For example, the tree depth of a decision tree is equal to the number of layers in the layer group. As another example, the tree depth of a decision tree is 3 and the layer group replaced by the decision tree has 5 network layers.
決定ツリーは、任意の適切なツリータイプを含むことができる。たとえば、決定ツリーは、ＧｒａｄｉｅｎｔＢｏｏｓｔツリーまたはＡｄａＢｏｏｓｔツリーであることができる。 The decision tree may include any suitable tree type. For example, the decision tree may be a GradientBoost tree or an AdaBoost tree.
システムは、少なくとも、層のグループの最初の層への入力と、層のグループの最後の層からの出力とを量子化し、量子化されている入力および出力を、それぞれの決定ツリーに提供することができる。より具体的には、層の１つまたは複数のグループの各々について、それぞれの決定ツリーは、グループにおけるそれぞれの最初の層への入力の量子化バージョンを、入力として受け取り、グループにおけるそれぞれの最後の層の出力の量子化バージョンを、出力として生成する。 The system may quantize at least the input to a first layer in a group of layers and the output from a last layer in the group of layers, and provide the quantized inputs and outputs to a respective decision tree. More specifically, for each of one or more groups of layers, a respective decision tree receives as input a quantized version of the input to a respective first layer in the group and produces as output a quantized version of the output of a respective last layer in the group.
いくつかの実施態様では、システムは、それぞれのスケーリング係数に基づいて、それぞれの決定ツリーへの入力および出力の量子化バージョンを取得することができる。より具体的には、システムは、量子化されている入力に、それぞれのスケーリング係数を乗算することによって、決定ツリーへの入力の量子化バージョンを生成することができる。上記で説明したように、スケーリング係数は、量子化されている層と、元の層との間の類似性の尺度に少なくとも基づいて取得される。 In some implementations, the system may obtain quantized versions of the inputs and outputs to each decision tree based on the respective scaling factors. More specifically, the system may generate quantized versions of the inputs to the decision trees by multiplying the inputs being quantized by the respective scaling factors. As described above, the scaling factors are obtained based at least on a measure of similarity between the layer being quantized and the original layer.
システムは、元のニューラルネットワークのトレーニングデータに基づいて、新たな機械学習モデルをトレーニングする（３４０）。システムは、それぞれの決定ツリーによって置き換えられなかった元のニューラルネットワークにおける層の少なくとも一部をトレーニングする。いくつかの実施態様では、システムは、シーケンスにおけるニューラルネットワークの層の１つまたは複数のグループに続く、ニューラルネットワークの層をトレーニングする。 The system trains (340) a new machine learning model based on the training data of the original neural network. The system trains at least some of the layers in the original neural network that were not replaced by the respective decision trees. In some implementations, the system trains layers of the neural network that follow one or more groups of layers of the neural network in the sequence.
いくつかの状況では、システムは、元のニューラルネットワークのために、同じであるが、量子化されているトレーニングサンプルを使用して、新たな機械学習モデル全体をトレーニングできる。いくつかの実施態様では、システムは、トレーニング中、順方向伝搬のために、量子化されている入力および出力を使用し、逆方向伝搬中に、重みを更新するために浮動小数点型の入力および出力を使用できる。あるいは、システムは、新たな機械学習モデルにおけるそれぞれの決定ツリーのために、それぞれの勾配を表すデータを計算することもできる。 In some situations, the system can train an entire new machine learning model using the same, but quantized, training samples as for the original neural network. In some implementations, the system can use quantized inputs and outputs for forward propagation during training, and floating-point inputs and outputs to update the weights during backward propagation. Alternatively, the system can calculate data representing the respective gradients for each decision tree in the new machine learning model.
システムは、層のグループを選択し、層のグループを、それぞれの決定ツリーによって置き換えるために、任意の適切なアルゴリズムを選択することができる。いくつかの実施態様では、システムは、層のグループを繰り返し選択できる。より具体的には、層のグループを選択するための１つの例示的なアルゴリズムが、以下に説明される。 The system may select any suitable algorithm for selecting layer groups and replacing the layer groups with their respective decision trees. In some implementations, the system may iteratively select layer groups. More specifically, one exemplary algorithm for selecting layer groups is described below.
ニューラルネットワークが、Ｎ個のネットワーク層を含んでいると仮定すると、システムは、ｉ∈［０、１、２、・・・、Ｎ－１］である場合、層ｉとして、シーケンスに従って、ニューラルネットワークの各々の層をインデクス付けする。 Assuming that the neural network contains N network layers, the system indexes each layer of the neural network in sequence as layer i, where i ∈ [0, 1, 2, ..., N-1].
システムは、ニューラルネットワークのサイズ（Ｎ層）に等しい層のグループをそこから選択する、層の合計数（すべての層）を設定する。 The system sets the total number of layers (all layers) from which to select a group of layers equal to the size of the neural network (N layers).
システムは、すべての層の中から、層Ｌを、層のグループの最初の層としてランダムに選択する。いくつかの実施態様では、システムは、層シーケンスに従って、層のグループの最初の層を選択することができる。たとえば、システムは、層のグループの最初の層として層０から開始できる。
The system randomly selects layer L from among all layers as the first layer of the group of layers. In some implementations, the system may select the first layer of the group of layers according to the layer sequence. For example, the system may start with
層Ｌから始まる、最後の層Ｎまでのシーケンスにおける各々の層について、システムはまず、現在の層がすでに決定ツリーによって置き換えられているか、または決定ツリーに属しているかを確認する。 For each layer in the sequence, starting from layer L through the final layer N, the system first checks whether the current layer has already been replaced by or belongs to a decision tree.
現在の層が、決定ツリーに置き換えられていない、または決定ツリーに属していると判定すると、システムは、現在の層を、層のグループに追加する。 If the system determines that the current layer is not replaced by or belongs to a decision tree, it adds the current layer to the group of layers.
システムは、層のグループの初期層Ｌへの入力と、現在の層からの出力とに対して量子化を実行し、層Ｌから現在の層までの層を、それぞれの決定ツリーによって暫定的に置き換える。上記で説明したように、システムは、ニューラルネットワーク層への入力および出力に対して、２値量子化または３値量子化を実行できる。いくつかの実施態様では、システムは、ニューラルネットワークにおける各々の層のすべての出力を量子化し、アキュムレータのサイズを、特定数の決定ツリーに制限できるようにする。 The system performs quantization on the inputs to an initial layer L of the group of layers and the outputs from the current layer, and provisionally replaces layers L through the current layer with their respective decision trees. As described above, the system can perform binary or ternary quantization on the inputs and outputs to the neural network layers. In some implementations, the system quantizes all outputs of each layer in the neural network, allowing the size of the accumulator to be limited to a specific number of decision trees.
システムは、層のグループの層情報を更新し、変更されているネットワークのパフォーマンス（たとえば、推論精度）を測定する。層のグループの決定およびパフォーマンス測定の詳細は、図４に関連してさらに詳細に説明される。 The system updates the layer information for the layer groups and measures the performance (e.g., inference accuracy) of the modified network. Details of the layer group determination and performance measurement are described in further detail in connection with FIG. 4.
層のグループを決定した後、システムは、層のグループを、それぞれの決定ツリーで置き換える。 After determining the layer groups, the system replaces the layer groups with their respective decision trees.
システムは、グループの最後の層から始まる、ニューラルネットワークの最後の層までの残りの層をトレーニングする。より具体的には、システムは、元のニューラルネットワークをトレーニングするために使用されているものと同じトレーニング例の、対応する部分を使用して、残りの層の重みを微調整する。 The system trains the remaining layers, starting from the last layer in the group, up to the last layer of the neural network. More specifically, the system fine-tunes the weights of the remaining layers using the corresponding portions of the same training examples used to train the original neural network.
図４は、ニューラルネットワーク層の１つまたは複数のグループを選択するための例示的なプロセス４００のフロー図である。便宜上、プロセス４００は、１つまたは複数の場所に配置されている１つまたは複数のコンピュータのシステムによって実行されているものとして説明される。たとえば、本明細書に従って適切にプログラムされている、たとえば図１に示されているシステム１００であるニューラルネットワーク展開システム１００は、プロセス４００を実行することができる。
FIG. 4 is a flow diagram of an exemplary process 400 for selecting one or more groups of neural network layers. For convenience, process 400 is described as being performed by one or more computer systems located at one or more locations. For example, a neural
システムは、ニューラルネットワークにおけるそれぞれの初期層を選択する（４１０）。上記で説明されるように、システムは、層のグループの初期層をランダムに、または層のシーケンスに基づいて選択できる。 The system selects (410) each initial layer in the neural network. As described above, the system can select the initial layer of a group of layers randomly or based on the sequence of layers.
システムは、各々が、それぞれの初期層を、候補グループにおける最初の層として有する、それぞれの複数の候補グループを生成する（４２０）。より具体的には、システムは、同じ初期層を有する各々の候補グループに対して、それぞれの層の数を暫定的に設定する。たとえば、システムは、２つの層のみを有する第１の候補グループと、４つの層を有する第２の候補グループと、６つの層を有する第３の候補グループとを生成する。いくつかの実施態様では、候補グループは、連続した数の層を有することができる。たとえば、第１の候補グループは単一の層を含み、第２の候補グループは２つの層を含み、第３の候補グループは３つの層を含む。 The system generates (420) a plurality of respective candidate groups, each having a respective initial layer as the first layer in the candidate group. More specifically, the system provisionally sets the respective number of layers for each candidate group having the same initial layer. For example, the system generates a first candidate group having only two layers, a second candidate group having four layers, and a third candidate group having six layers. In some implementations, the candidate groups can have consecutive numbers of layers. For example, the first candidate group includes a single layer, the second candidate group includes two layers, and the third candidate group includes three layers.
それぞれの複数の候補グループの各々について、システムは、候補グループにおける層が、それぞれの決定ツリーによって置き換えられている、対応する新たな機械学習モデルのパフォーマンスを測定する候補グループのそれぞれのパフォーマンス尺度を判定する（４３０）。より具体的には、システムは、複数の新たな機械学習モデルを生成し、複数の新たな機械学習モデルの各々は、それぞれの決定ツリーによって置き換えられる層のそれぞれの候補グループを含んでいる。システムは、同じ入力データを使用して、新たな機械学習モデルの各々に対して推論計算を実行し、新たな機械学習モデルの各々について、それぞれのパフォーマンススコアを取得する。それぞれのパフォーマンスコア（ｐｅｒｆｏｒｍａｎｃｅｓ ｃｏｒｅｓ）は、推論精度に基づいて取得できる。 For each of the respective plurality of candidate groups, the system determines (430) a respective performance measure of the candidate group that measures the performance of a corresponding new machine learning model in which layers in the candidate group are replaced by the respective decision tree. More specifically, the system generates a plurality of new machine learning models, each of the plurality of new machine learning models including a respective candidate group of layers replaced by the respective decision tree. The system performs an inference calculation for each of the new machine learning models using the same input data, and obtains a respective performance score for each of the new machine learning models. Respective performance scores can be obtained based on the inference accuracy.
システムは、それぞれの複数の候補グループについて、それぞれのパフォーマンス尺度に基づいて、決定ツリーによって置き換えられる層のグループとして、層の候補グループのうちの１つを選択する（４４０）。いくつかの実施態様では、システムは、それぞれのパフォーマンス尺度の中から最大のパフォーマンス尺度を判定し、それぞれの複数の候補グループから、最大のパフォーマンス尺度に関連付けられている候補グループを、グループとして選択する。あるいは、システムは、パフォーマンススコアは比較的高いが、推論計算の実行が最も速い候補グループを選択する。 For each of the plurality of candidate groups, the system selects (440) one of the candidate groups of layers as the group of layers to be replaced by the decision tree based on the respective performance measure. In some implementations, the system determines a maximum performance measure among the respective performance measures and selects, from each of the plurality of candidate groups, the candidate group associated with the maximum performance measure as the group. Alternatively, the system selects the candidate group that has a relatively high performance score but is the fastest to perform the inference calculation.
図５は、決定ツリー５００の例を示す。
図５に示されているように、決定ツリー５００は、特定のツリー深さを有する複数のノードを含むことができる。ツリー深さは、ツリー層の総数に基づいて決定される。決定の各々の層（Ｅａｃｈ ｌａｙｅｒ ｏｆ ｔｈｅ ｄｅｃｉｓｉｏｎ）は、たとえば論理比較または他の適切な基準など、それぞれのノード演算を表すノードを含むことができる。
FIG. 5 shows an example of a
5, a
たとえば、図５に示されているように、決定ツリー５００は、ルートノード５１０と、異なるツリー層におけるそれぞれの非ルートノード５２０、５３０とを含む、複数のノードを有することができる。ルートノード５１０は、決定ツリー５００の開始点であり、親ノードを有しておらず、非ルートノード５２０および５３０は、各々が親ノードを有し、子ノードとも呼ばれる。一般に、ツリー層におけるリーフノードを除く各々のノードは、そのノードを、次のツリー層におけるそれぞれの子ノードに接続する１つまたは複数の分岐（図５における矢印）を有することができる。子ノードを有するノードは、非リーフノード、たとえばノード５１０、５２０ａ、および５２０ｂとも呼ばれる。
For example, as shown in FIG. 5, a
最深すなわち最後のツリー層におけるリーフノード（たとえば、ノード５３０ａ、５３０ｂ、５３０ｃ、および５３０ｄ）に対して、各々は、決定ツリー５００の推論出力を表すことができ、他の子ノードに接続する分岐を有していない。推論出力は、予測、たとえばリーフノード５３０ａの確率Ｐ１であることができる。
For leaf nodes in the deepest or final tree layer (e.g.,
図５を参照すると、システム（たとえば、図１におけるシステム１００）は、係数ｎｆ１、ｎｆ２、およびｎｆ３を表す入力データを用いて決定ツリーの推論演算を実行する場合、最深のツリー層におけるリーフノードを除く各々の非リーフノードは、それぞれのノード演算（たとえば、図５に示されているような論理比較）を有する。システムは、ルートノード５１０に対してノード演算を実行し、ノード演算から論理結果（たとえば、真または偽）を取得し、その結果に基づいて、それぞれの分岐に沿って対応する子ノードに近づくことができる。たとえば、結果が偽の場合、システムはノード５２０ａに近づく。いくつかの実施態様では、システムは、整数０および整数１を使用して、偽および真を表すことができる。最終的に、システムは、決定ツリーの最深層における特定のリーフノードに近づき、特定のリーフノードによって表されている推論を返す。
5, when a system (e.g.,
図６は、固定されている機能ハードウェア６００を使用する決定ツリーの例示的な実施態様を示す。
Figure 6 shows an exemplary implementation of a decision tree using fixed
図６に示されているように、システム（たとえば、図１に示されているシステム１００）は、固定されている機能ハードウェア６００を使用して、決定ツリーの推論計算を実行することができる。より具体的には、システムは、図５に示されている例示的な決定ツリーのために、入力ｘ６１０を受け取り、最終推論出力６２０を返すことができる。決定ツリー６００は、異なるノードから推論結果を取得し、決定ツリーにおける特定のリーフノードによって表されている最終的な推論出力６２０を生成するために、複数の計算ユニット６３０および６４０を含むことができる。
As shown in FIG. 6, a system (e.g.,
固定されている機能ハードウェア６００を使用するシステムは、各々の入力データを、対応するツリーノードに割り当て、それぞれの比較器およびマルチプレクサを使用して、各々のノードにおいてそれぞれの機能を実施することができる。図６に示されているように、システムは、入力データｘ∈［ｈ１，ｌ１］を、（図５におけるルートノード５１０に相当する）最上位ノードに割り当て、入力データｘ∈［ｈ２，ｌ２］を、（第２のツリー層における左側の非リーフノード５２０ａに相当する）左側ノード６３０ａに割り当て、入力データｘ∈［ｈ３，ｌ３］を、（第２のツリー層における右側の非リーフノード５２０ｂに相当する）右側ノード６３０ｂに割り当てる。システムは、それぞれの比較器６４０およびマルチプレクサ６３０を使用して、各々のノードにおいて演算を実施することができる。たとえば、システムは、比較器６３０ｃおよびマルチプレクサ６３０ｃを使用して、最上位ノードにおいて演算を実施する。より具体的には、システムは、実数値のベクトルであり得る入力ｘを受け取り、範囲ｘ∈［ｈ１，ｌ１］を有する入力の一部を選択して、最上位のルートノードに提供する。システムは、比較器６４０ｃを使用して、入力の一部を、基準ａ１と比較し、基準ａ１は、実数値スカラを表すことができる。入力データｘが、ａ１以上である場合、システムは、ノードに割り当てられるマルチプレクサ６３０ｃを使用して「真」を表す結果を出力し、たとえば、マルチプレクサは、「真」を表す整数１を出力することができる。システムは、現在のノードが、対応するツリーの分岐に沿ってリンクしている子ノードにおいて演算を実行し続けることができる。最後から２番目のツリー層における非リーフノードに対する演算を処理した後、システムは、決定ツリーにおける対応するリーフノードによって表されている値（たとえば、確率）に基づいて最終結果６２０を出力することができる。
A system using fixed
図７は、決定ツリーの推論計算を実行するための例示的なプログラマブルコア７００を例示している。
Figure 7 illustrates an exemplary
図７に示されているように、システム（たとえば、図１に示されているシステム１００）は、プログラマブルコア７００を使用して決定ツリーのための推論計算を実行することができる。プログラマブルコア７００は、ツリー入力７１０を受け取り、ツリー入力に対する推論出力７２０を生成することができる。プログラマブルコア７００は、複数の計算コンポーネント、たとえば、ＭＵＸユニット、ＡＬＵユニット、およびスタティックランダムアクセスメモリ（ＳＲＡＭ）を含むことができる。プログラマブルコア７００は、計算コンポーネントにおいて構成されている機能を追加、選択、および切り替えるだけでノード演算を実行することができる。したがって、プログラマブルコア７００は、決定ツリーの推論出力を生成するためのノード演算として、乗算および加算を実行するためのＭＡＣユニットを含む必要がない。
As shown in FIG. 7, a system (e.g.,
図７を参照すると、ルートノード（たとえば、図５におけるルートノード５１０）における演算を実施するために、システムは、入力７１０を受け取り、受け取った入力７１０を、結合ユニットによって、ルートノードに固有で、以前にキューに格納されている配列（ｘ、ａ、ｌ、ｈ、ｉ０、ｉ１）を用いて変更することができる。上記で説明されるように、ａは、ノード基準を表し、ｌおよびｈは、入力ｘの数値範囲を表し、インデクスｉ０、ｉ１は各々、現在のノードをそれぞれの子ノードに接続するツリー分岐を表すことができ、ノード演算に対して演算を実行した結果を表すために、それぞれの整数値に割り当てることができる。
7, to perform an operation at a root node (e.g.,
システムは、任意のユニットを使用して結合されている入力を選択し、ルートノードでノード演算を実施できる。たとえば、システムは、それぞれの比較器を使用して、入力ｘ∈［ｈ，ｌ］と基準ａ１との間の比較を実施できる。それに応じて、システムは、インデクスｉ０、ｉ１を、それぞれの整数値に割り当てて、比較の結果を表し、決定ツリーの計算を、対応する子ノードに指示することができる。たとえば、システムが、ｘ∈［ｈ，ｌ］が基準ａ１よりも大きいと判定した場合、システムは、ｉ０＝０、ｉ１＝１を割り当てることができるため、システムは、ｉ１によって表されているツリー分岐に沿って、対応する次の子ノードに対して演算を実行できる。 The system can use any unit to select the combined inputs and perform the node operation at the root node. For example, the system can use each comparator to perform a comparison between the input x∈[h,l] and the criterion a1 . Accordingly, the system can assign indexes i0 , i1 to respective integer values to represent the results of the comparison and direct the computation of the decision tree to the corresponding child node. For example, if the system determines that x∈[h,l] is greater than the criterion a1 , the system can assign i0 =0, i1 =1, so that the system can perform the operation on the corresponding next child node along the tree branch represented by i1 .
次の子ノードに対して演算を実施するために、システムはまず、切替ユニットを適用して、付番基準Ｎに基づいて、次の子ノードが、非リーフノードであるかリーフノードであるかを判定できる。判定するために、システムは、それぞれのタグＫ（たとえば、整数）を使用して、各々のノードに付番し、所定の付番基準に基づいて、次の子ノードのタイプを判定することができる。たとえば、図５に関連して、システムは、ノード５１０、５２０ａ、および５２０ｂを、ノード０、ノード１、およびノード２としてタグ付けし、リーフノード５３０ａ、５３０ｂ、５３０ｃ、および５３０ｄを、ノード３、ノード４、ノード５、およびノード６としてタグ付けすることができる。システムは、タグＫ＜３を有するノードが、非リーフノードであり、タグＫ＞＝３を有するノードが、リーフノードであるように、付番基準Ｎ＝３を設定できる。
To perform the operation on the next child node, the system can first apply the switching unit to determine whether the next child node is a non-leaf node or a leaf node based on the numbering criterion N. To do so, the system can number each node with a respective tag K (e.g., an integer) and determine the type of the next child node based on a predefined numbering criterion. For example, with reference to FIG. 5, the system can tag
次の子ノードが、非リーフノードであるとの判定に応じて、システムは、非リーフＳＲＡＭに格納されているデータから、次の子ノードのそれぞれの配列（ｘ、ａ、ｌ、ｈ、ｉ０、ｉ１）を更新できる。同様に、次の子ノードが、リーフノードであるとの判定に応じて、システムは、リーフＳＲＡＭに格納されているリーフノードに関連付けられている最終出力を提供することができる。 In response to determining that the next child node is a non-leaf node, the system may update the arrays (x, a, l, h, i0 , i1 ) of each of the next child nodes from data stored in the non-leaf SRAMs. Similarly, in response to determining that the next child node is a leaf node, the system may provide a final output associated with the leaf node that is stored in the leaf SRAMs.
並列計算のために、システムは、フォーク関数を採用して、キューにおけるノードの入力データの一部と、ＳＲＡＭにおけるノードの演算およびタイプを識別するそれぞれの配列とを格納できる。システムは、並列計算中に、各々の計算ユニットについて観察されているそれぞれのレイテンシに基づいて、１つまたは複数の結合ユニットを使用して、それぞれの配列を用いて、入力データをいつ変更するかを自動的に決定できる。 For parallel computation, the system can employ a fork function to store a portion of the node's input data in a queue and a respective array identifying the node's operation and type in SRAM. During parallel computation, the system can automatically determine when to modify the input data with the respective array using one or more join units based on the respective latencies observed for each computation unit.
本明細書で言及されている実施形態は、ニューラルネットワークをトレーニングするための改良されている方法を提供する。ニューラルネットワークは、任意の種類のデジタルデータ入力を受け取り、その入力に基づいて任意の種類のスコア、分類、または回帰出力を生成するように構成され得る。入力データ項目は、画像データ（ここではビデオデータを含む）、オーディオデータ、またはテキストデータ、たとえば自然言語の単語または単語部分（またはその表現、たとえば埋め込み）を備え得る。入力データ項目は、連続したデータ、たとえば、デジタル化されているオーディオを表すデータサンプルのシーケンス、または、ピクセルのシーケンスとして表されている画像、または画像のシーケンスによって表されているビデオ、または自然言語における単語のシーケンスを表すシーケンスを備え得る。本明細書において「画像」は、たとえば、ＬＩＤＡＲ画像を含む。 The embodiments referred to herein provide an improved method for training a neural network. A neural network may be configured to receive any type of digital data input and generate any type of score, classification, or regression output based on the input. The input data items may comprise image data (including video data here), audio data, or text data, e.g., words or word portions (or representations, e.g., embeddings) of a natural language. The input data items may comprise continuous data, e.g., a sequence of data samples representing digitized audio, or an image represented as a sequence of pixels, or a video represented by a sequence of images, or a sequence representing a sequence of words in a natural language. As used herein, "image" includes, for example, LIDAR images.
いくつかの実施態様では、ニューラルネットワーク出力は、特徴表現を備え得、特徴表現は、その後さらに処理されて、システム出力を生成し得る。たとえば、システム出力は、入力データ項目を、たとえば、画像、ビデオ、もしくはオーディオのカテゴリ（たとえば、入力データ項目または入力データ項目のオブジェクト／要素が、カテゴリに属する推定尤度を表すデータ）である複数のカテゴリのうちの１つへ分類するための分類出力、または、入力データ項目の領域を、たとえば、画像もしくはビデオで表されているオブジェクトもしくは動作にセグメント化するためのセグメンテーション出力を備え得る。または、システム出力は、強化学習システムにおける動作選択出力であり得る。 In some implementations, the neural network output may comprise feature representations, which may then be further processed to generate a system output. For example, the system output may comprise a classification output for classifying the input data items into one of a number of categories, e.g., image, video, or audio categories (e.g., data representing an estimated likelihood that the input data items or objects/elements of the input data items belong to a category), or a segmentation output for segmenting regions of the input data items into objects or actions, e.g., represented in an image or video. Or, the system output may be an action selection output in a reinforcement learning system.
他のいくつかの実施態様では、ネットワーク出力は、同じタイプまたは異なるタイプの別のデータ項目を備え得る。たとえば、入力データ項目は、画像、オーディオ、またはテキストであり得、出力データ項目は、画像、オーディオ、またはテキストの変更バージョンであり得、たとえば、入力データ項目、または入力データ項目内の１つもしくは複数のオブジェクトもしくは要素の、スタイル、コンテンツ、プロパティ、ポーズなどを変更するか、あるいは、入力データ項目の（欠落した）部分を埋めるか、あるいは、データ項目の別のバージョン、またはビデオもしくはオーディオデータ項目の拡張子を予測するか、あるいは、入力データ項目のアップサンプリングされた（またはダウンサンプリングされた）バージョンを提供する。たとえば、入力データ項目は、第１の言語でのテキストの表現であり得、出力データ項目は、テキストの別の言語への翻訳、またはテキストの別の言語への翻訳のためのスコアであり得る。別の例では、入力画像が、ビデオ、もしくはワイヤフレームモデル、もしくはＣＡＤモデルへ変換され得るか、または、２Ｄの入力画像が３Ｄに、またはその逆に変換され得る。または、入力データ項目は、口頭での発話もしくは口頭での発話のシーケンスから導出される特徴、またはそれらから導出される特徴を備え得、ネットワークシステム出力は、一連のテキストの部分の各々のスコアを備え得、各々のスコアは、テキストの一部が、特徴に基づいた正しい置き換えである推定尤度を表す。別の例では、入力データ項目は、画像、オーディオ、またはテキストであり得、出力データ項目は、異なる形式での入力データ項目の表現であり得る。たとえば、ニューラルネットワークは、（音声認識の場合）テキストを音声に、またはその逆に変換したり、または、（たとえば、キャプションのために）画像（または、ビデオ）をテキストに変換し得る。連続したデータを備える出力を生成する場合、ニューラルネットワークは、１つまたは複数の畳み込み、たとえば、拡張畳み込み層を含み得る。 In some other implementations, the network output may comprise another data item of the same or different type. For example, the input data item may be an image, audio, or text, and the output data item may be a modified version of the image, audio, or text, e.g., modifying the style, content, properties, pose, etc. of the input data item, or one or more objects or elements within the input data item, or filling in (missing) parts of the input data item, or predicting another version of the data item, or an extension of a video or audio data item, or providing an upsampled (or downsampled) version of the input data item. For example, the input data item may be a representation of a text in a first language, and the output data item may be a translation of the text into another language, or a score for the translation of the text into another language. In another example, the input image may be converted into a video, or a wireframe model, or a CAD model, or a 2D input image may be converted into 3D or vice versa. Or, the input data items may comprise features derived from or derived from a verbal utterance or sequence of verbal utterances, and the network system output may comprise a score for each of a series of text portions, each score representing an estimated likelihood that the portion of text is a correct replacement based on the features. In another example, the input data items may be images, audio, or text, and the output data items may be representations of the input data items in different formats. For example, a neural network may convert text to speech or vice versa (for speech recognition), or convert images (or videos) to text (e.g., for captioning). When generating an output comprising continuous data, the neural network may include one or more convolutions, e.g., dilated convolution layers.
いくつかの他の実施態様では、ネットワーク出力は、たとえば、現実世界環境または現実世界環境のシミュレーションのような環境におけるロボットまたは他の機械的エージェントなどのエージェントによって実行される動作を選択するための出力を備え得る。 In some other implementations, the network output may comprise an output for selecting an action to be performed by an agent, such as a robot or other mechanical agent, in an environment, such as a real-world environment or a simulation of a real-world environment.
いくつかの実施態様では、ニューラルネットワークは、入力データ項目を受け取り、ネットワークパラメータに従って入力データ項目の特徴表現を生成するために入力データ項目を処理するように構成されている。一般に、データ項目の特徴表現は、データ項目を、多次元特徴空間内の点として表す、順序付けられている数値の集合、たとえばベクトルである。言い換えれば、各々の特徴表現は、入力データ項目の複数の特徴の各々に対する数値を含み得る。前述したように、ニューラルネットワークは、任意の種類のデジタルデータ入力を入力として受け取り、その入力から特徴表現を生成するように構成できる。たとえば、ネットワーク入力とも呼ばれ得る入力データ項目は、画像、ドキュメントの一部、テキストシーケンス、オーディオデータ、医療データなどであることができる。 In some implementations, the neural network is configured to receive an input data item and process the input data item to generate a feature representation of the input data item according to network parameters. In general, the feature representation of a data item is an ordered collection of numerical values, e.g., a vector, that represents the data item as a point in a multidimensional feature space. In other words, each feature representation may include a numerical value for each of a number of features of the input data item. As previously mentioned, a neural network may be configured to receive any type of digital data input as an input and generate a feature representation from the input. For example, the input data item, which may be referred to as a network input, may be an image, a portion of a document, a text sequence, audio data, medical data, etc.
トレーニングされると、特徴表現は、たとえばネットワーク入力に対して機械学習タスクを実行する際に使用するために、別のシステムに入力を提供できる。例示的なタスクは、特徴ベースの検索、クラスタリング、準重複検出、検証、特徴マッチング、領域適応、ビデオベースの弱教師学習を含み得、ビデオの場合、たとえばビデオフレームにわたるオブジェクト追跡、ビデオに描かれているエンティティによって実行されているジェスチャのジェスチャ認識を含み得る。 Once trained, the feature representations can provide input to another system, e.g., for use in performing machine learning tasks on the network input. Exemplary tasks can include feature-based search, clustering, near-duplicate detection, validation, feature matching, domain adaptation, video-based weakly supervised learning, and in the case of video, e.g., object tracking across video frames, gesture recognition of gestures being performed by an entity depicted in the video.
ニューラルネットワークへの入力が、画像、または画像から抽出されている特徴である場合、所与の画像のためにニューラルネットワークによって生成される出力は、一連のオブジェクトカテゴリの各々のスコアであり得、各々のスコアは、画像が、カテゴリに属するオブジェクトの画像を含む推定尤度を表す。より具体的には、入力画像または入力画像から抽出される特徴の各々は、各々がそれぞれの強度値を有する１つまたは複数のピクセルを含むことができる。ニューラルネットワークは、入力画像のそれぞれの強度値、または画像から抽出される特徴を処理し、たとえば、画像分類、画像認識、または画像セグメンテーションなどの予測を生成するように構成されている。 If the input to the neural network is an image, or features extracted from the image, the output generated by the neural network for a given image may be a score for each of a set of object categories, each score representing an estimated likelihood that the image contains an image of an object belonging to the category. More specifically, each of the input image or features extracted from the input image may include one or more pixels, each having a respective intensity value. The neural network is configured to process each intensity value of the input image, or features extracted from the image, to generate a prediction, such as, for example, image classification, image recognition, or image segmentation.
別の例として、ニューラルネットワークへの入力が、インターネットリソース（たとえば、ウェブページ）、ドキュメント、もしくはドキュメントの一部であるか、またはインターネットリソース、ドキュメント、もしくはドキュメントの一部から抽出される特徴である場合、所与のインターネットリソース、ドキュメント、またはドキュメントの一部のために、ニューラルネットワークによって生成される出力は、一連のトピックの各々に対するスコアであり得、各々のスコアは、インターネットリソース、ドキュメント、またはドキュメントの一部がそのトピックに関している推定尤度を表す。 As another example, if the inputs to a neural network are Internet resources (e.g., web pages), documents, or portions of documents, or features extracted from Internet resources, documents, or portions of documents, the output generated by the neural network for a given Internet resource, document, or portion of a document may be a score for each of a set of topics, each score representing an estimated likelihood that the Internet resource, document, or portion of a document is related to that topic.
別の例として、ニューラルネットワークへの入力が、特定の広告のインプレッション文脈の特徴である場合、ニューラルネットワークによって生成される出力は、特定の広告がクリックされる推定尤度を表すスコアであり得る。 As another example, if the input to a neural network is features of the impression context of a particular ad, the output generated by the neural network may be a score representing the estimated likelihood that a particular ad will be clicked.
別の例として、ニューラルネットワークへの入力が、ユーザのために個人化されている推奨の特徴、たとえば、推奨の文脈を特徴付ける特徴、たとえば、ユーザによって講じられる、以前の動作を特徴付ける特徴である場合、ニューラルネットワークによって生成される出力は、一連のコンテンツ項目の各々に対するスコアであり得、各々のスコアは、推奨されているコンテンツ項目に対してユーザが肯定的に反応する推定尤度を表す。 As another example, if the inputs to the neural network are features of a recommendation that are personalized for a user, e.g., features that characterize the context of the recommendation, e.g., features that characterize previous actions taken by the user, the output generated by the neural network can be a score for each of a set of content items, each score representing an estimated likelihood that the user will respond positively to the content item being recommended.
別の例として、ニューラルネットワークへの入力が、１つの言語のテキストのシーケンスである場合、ニューラルネットワークによって生成される出力は、別の言語の一連のテキストの部分の各々に対するスコアであり得、各々のスコアは、他の言語のテキストの一部が、入力されるテキストを、他の言語に適切に翻訳したものである推定尤度を表す。 As another example, if the input to a neural network is a sequence of text in one language, the output generated by the neural network can be a score for each portion of the sequence of text in another language, each score representing an estimated likelihood that the portion of text in the other language is a good translation of the input text into the other language.
別の例として、ニューラルネットワークへの入力が、口頭での発話を表すシーケンスである場合、ニューラルネットワークによって生成される出力は、一連のテキストの部分の各々のスコアであり得、各々のスコアは、テキストの一部が、発話を正しく文字起こししたものである推定尤度を表す。 As another example, if the input to a neural network is a sequence representing a verbal utterance, the output generated by the neural network may be a score for each of the sequence of text portions, each score representing an estimated likelihood that the portion of text is a correct transcription of the utterance.
本明細書に説明される主題および機能演算の実施形態は、デジタル電子回路構成、有形に具現化されているコンピュータソフトウェアまたはファームウェア、本明細書に開示される構造およびそれらの構造的等価物を含むコンピュータハードウェア、またはそれらの１つまたは複数の組合せで実施することができる。本明細書で説明される主題の実施形態は、１つまたは複数のコンピュータプログラム、すなわち、データ処理装置によって実行される、またはデータ処理装置の演算を制御するために有形の非一時的記憶媒体にエンコードされるコンピュータプログラム命令の１つまたは複数のモジュールとして実施することができる。コンピュータ記憶媒体は、機械可読記憶デバイス、機械可読記憶基板、ランダムアクセスメモリデバイスもしくはシリアルアクセスメモリデバイス、またはそれらの１つまたは複数の組合せであってよい。あるいは、または加えて、プログラム命令は、データ処理装置による実行のために適切な受信装置への送信のための情報をエンコードするために生成される、人工的に生成される伝搬信号、たとえば、機械生成される電気信号、光信号、または電磁信号においてエンコードされ得る。 Embodiments of the subject matter and functional operations described herein may be implemented in digital electronic circuitry, tangibly embodied computer software or firmware, computer hardware including the structures disclosed herein and their structural equivalents, or one or more combinations thereof. Embodiments of the subject matter described herein may be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded in a tangible non-transitory storage medium for execution by or control of the operation of a data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random access memory device or a serial access memory device, or one or more combinations thereof. Alternatively, or in addition, the program instructions may be encoded in an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to a suitable receiving device for execution by the data processing apparatus.
「データ処理装置」という用語は、データ処理ハードウェアを指し、例として、プログラム可能なプロセッサ、コンピュータ、または複数のプロセッサもしくはコンピュータを含む、データを処理するための任意の種類の装置、デバイス、および機械を包含する。この装置はまた、たとえばＦＰＧＡ（フィールドプログラマブルゲートアレイ）またはＡＳＩＣ（特定用途向け集積回路）などの専用の論理回路構成であってもよく、またはさらにそれを含んでもよい。この装置は、ハードウェアに加えて、コンピュータプログラムのための実行環境を作成するコード、たとえば、プロセッサファームウェア、プロトコルスタック、データベース管理システム、オペレーティングシステム、またはそれらのうちの１つまたは複数の組合せを構成するコードを、任意選択的に含むことができる。 The term "data processing apparatus" refers to data processing hardware and encompasses any type of apparatus, device, and machine for processing data, including, by way of example, a programmable processor, computer, or multiple processors or computers. The apparatus may also be or include special-purpose logic circuitry, such as, for example, an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for computer programs, such as code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or one or more combinations thereof.
プログラム、ソフトウェア、ソフトウェアアプリケーション、アプリ、モジュール、ソフトウェアモジュール、スクリプト、もしくはコードとも呼ばれ得る、またはそう説明され得るコンピュータプログラムは、コンパイルもしくはインタープリタ型言語、または宣言型もしくは手続き型言語を含む任意の形態のプログラミング言語で記述することができ、スタンドアロンプログラムとして、または計算環境での使用に適したモジュール、コンポーネント、サブルーチン、もしくは他のユニットとしてなど、任意の形態で展開できる。プログラムは、ファイルシステムにおけるファイルに対応する場合があるが、対応している必要はない。プログラムは、たとえば、マークアップ言語ドキュメントに格納されている１つまたは複数のスクリプトである、他のプログラムもしくはデータを保持するファイルの一部に、対象のプログラムに専用の単一のファイルに、または、たとえば、１つもしくは複数のモジュール、サブプログラム、もしくはコードの一部を格納するファイルである、複数の調整済ファイルに格納できる。コンピュータプログラムは、１つのコンピュータにおいて、または１つのサイトに配置されるか、もしくは複数のサイトにわたって分散され、データ通信ネットワークによって相互接続されている複数のコンピュータにおいて実行されるように展開できる。 A computer program, which may also be referred to or described as a program, software, software application, app, module, software module, script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and can be deployed in any form, such as a standalone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may correspond to a file in a file system, but need not. A program can be stored in a single file dedicated to the program in question, in part of a file holding other programs or data, for example one or more scripts stored in a markup language document, or in multiple coordinated files, for example a file storing one or more modules, subprograms, or code portions. A computer program can be deployed to be executed in one computer, or in multiple computers located at one site or distributed across multiple sites and interconnected by a data communication network.
本明細書において、「データベース」という用語は、データの任意の集合を指すために広く使用され、データは、任意の特定の手法で構造化される必要はなく、またはまったく構造化される必要もなく、１つまたは複数の場所にある記憶デバイスに格納できる。したがって、たとえば、インデクスデータベースは、データの複数の集合を含むことができ、データの複数の集合の各々は、異なる方式で編成およびアクセスされ得る。 The term "database" is used broadly herein to refer to any collection of data, which need not be structured in any particular manner, or even at all, and which may be stored on storage devices in one or more locations. Thus, for example, an index database may contain multiple collections of data, each of which may be organized and accessed in a different manner.
同様に、本明細書において、「エンジン」という用語は、１つまたは複数の特定の機能を実行するようにプログラムされているソフトウェアベースのシステム、サブシステム、またはプロセスを指すために広く使用される。一般に、エンジンは、１つまたは複数のソフトウェアモジュールまたはコンポーネントとして実施され、１つまたは複数の場所にある１つまたは複数のコンピュータにインストールされる。いくつかの場合には、１つまたは複数のコンピュータが、特定のエンジン専用になり、他の場合には、複数のエンジンを、同じ１つまたは複数のコンピュータにインストールして実行することもできる。 Similarly, the term "engine" is used broadly herein to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. Generally, an engine is implemented as one or more software modules or components and installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine, and in other cases, multiple engines may be installed and run on the same computer or computers.
本明細書で説明されるプロセスおよび論理フローは、入力データに対して演算を行い、出力を生成することによって機能を実行する、１つまたは複数のコンピュータプログラムを実行する、１つまたは複数のプログラマブルコンピュータによって実行できる。プロセスおよび論理フローは、たとえば、ＦＰＧＡもしくはＡＳＩＣなどの専用の論理回路構成によって、または専用の論理回路構成と、１つもしくは複数のプログラムされているコンピュータとの組合せによって実行することもできる。 The processes and logic flows described herein may be performed by one or more programmable computers executing one or more computer programs that perform functions by performing operations on input data and generating output. The processes and logic flows may also be performed by special purpose logic circuitry, such as, for example, an FPGA or ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
コンピュータプログラムの実行に適したコンピュータは、汎用もしくは専用のマイクロプロセッサまたはその両方、または他の任意の種類の中央処理ユニットに基づくことができる。一般に、中央処理ユニットは、読取専用メモリ、またはランダムアクセスメモリ、またはその両方から、命令およびデータを受け取る。コンピュータの必須要素は、命令を実行または実施するための中央処理ユニットと、命令およびデータを格納するための１つまたは複数のメモリデバイスである。中央処理ユニットおよびメモリは、専用の論理回路構成によって補完されているか、または専用の論理回路構成に組み込まれ得る。一般に、コンピュータはまた、データを格納するための１つまたは複数の大容量記憶デバイス、たとえば、磁気ディスク、光磁気ディスク、もしくは光ディスクを含むか、またはそれらからデータを受け取るために、もしくはそれらにデータを転送するために、もしくはその両方のために、動作可能に結合されている。しかしながら、コンピュータは、そのようなデバイスを有している必要はない。さらに、コンピュータは、いくつか挙げると、たとえば、モバイル電話、携帯情報端末（ＰＤＡ）、モバイルオーディオもしくはビデオプレーヤ、ゲーム機、全地球測位システム（ＧＰＳ）受信機、または、たとえばユニバーサルシリアルバス（ＵＳＢ）フラッシュドライブなどのポータブル記憶デバイスなどの、別のデバイスに組み込むこともできる。 A computer suitable for executing a computer program may be based on a general-purpose or dedicated microprocessor or both, or any other type of central processing unit. Typically, the central processing unit receives instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or implementing instructions, and one or more memory devices for storing instructions and data. The central processing unit and memory may be supplemented by or incorporated in dedicated logic circuitry. Typically, a computer also includes one or more mass storage devices for storing data, e.g., magnetic disks, magneto-optical disks, or optical disks, or is operatively coupled to receive data from them, or to transfer data to them, or both. However, a computer need not have such devices. Furthermore, a computer may also be incorporated in another device, such as, for example, a mobile phone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a global positioning system (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name a few.
コンピュータプログラム命令およびデータを格納するのに適したコンピュータ可読媒体は、例として、たとえば、ＥＰＲＯＭ、ＥＥＰＲＯＭ、およびフラッシュメモリデバイスのような半導体メモリデバイスと、たとえば、内蔵ハードディスクまたはリムーバブルディスクのような磁気ディスクと、光磁気ディスクと、ＣＤ ＲＯＭディスクおよびＤＶＤ－ＲＯＭディスクとを含むあらゆる形態の不揮発性メモリ、媒体、およびメモリデバイスを含む。 Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and memory devices, including, by way of example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices, magnetic disks such as internal hard disks or removable disks, magneto-optical disks, CD ROM disks, and DVD-ROM disks.
ユーザとのインタラクションを提供するために、本明細書において説明される主題の実施形態は、ユーザに情報を表示するための、たとえばＣＲＴ（陰極線管）またはＬＣＤ（液晶ディスプレイ）モニタのようなディスプレイデバイスと、ユーザがコンピュータに入力を提供できる、キーボードおよび、たとえばマウスまたはトラックボールなどのポインティングデバイスとを有するコンピュータにおいて実施することができる。他の種類のデバイスを使用して、ユーザとのインタラクションを提供することもでき、たとえば、ユーザに提供されるフィードバックは、たとえば、視覚的フィードバック、聴覚的フィードバック、または触覚的フィードバックなど、任意の形態の感覚的フィードバックとすることができ、ユーザからの入力は、音響、音声、または触覚入力を含む、任意の形態で受け取ることができる。加えて、コンピュータは、ユーザによって使用されているデバイスにドキュメントを送信すること、およびデバイスからドキュメントを受信することにより、たとえば、ウェブブラウザから受け取られる要求に応じて、ユーザのデバイスにおけるウェブブラウザへ、ウェブページを送信することにより、ユーザとインタラクトできる。また、コンピュータは、テキストメッセージまたは他の形態のメッセージを、パーソナルデバイス、たとえば、メッセージングアプリケーションを実行しているスマートフォンなどに送信し、返信としてユーザから応答メッセージを受け取ることによって、ユーザとインタラクトすることもできる。 To provide interaction with a user, embodiments of the subject matter described herein can be implemented in a computer having a display device, such as, for example, a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user, and a keyboard and a pointing device, such as, for example, a mouse or trackball, through which the user can provide input to the computer. Other types of devices can also be used to provide interaction with the user, for example, the feedback provided to the user can be any form of sensory feedback, such as, for example, visual feedback, auditory feedback, or tactile feedback, and the input from the user can be received in any form, including acoustic, voice, or tactile input. In addition, the computer can interact with the user by sending documents to and receiving documents from a device being used by the user, for example, by sending a web page to a web browser on the user's device in response to a request received from the web browser. The computer can also interact with the user by sending text messages or other forms of messages to a personal device, such as, for example, a smartphone running a messaging application, and receiving a response message from the user in return.
機械学習モデルを実施するためのデータ処理装置はまた、たとえば、機械学習のトレーニングまたは生産の、すなわち、推論、作業負荷の、一般的で計算集約的な部分を処理するための専用のハードウェアアクセラレータユニットを含むこともできる。 A data processing device for implementing machine learning models may also include dedicated hardware accelerator units for handling typical, computationally intensive parts of, for example, machine learning training or production, i.e., inference, workloads.
機械学習モデルは、たとえばＴｅｎｓｏｒＦｌｏｗフレームワーク、Ｍｉｃｒｏｓｏｆｔ Ｃｏｇｎｉｔｉｖｅ Ｔｏｏｌｋｉｔフレームワーク、Ａｐａｃｈｅ Ｓｉｎｇａフレームワーク、またはＡｐａｃｈｅ ＭＸＮｅｔフレームワークなどの機械学習フレームワークを使用して実施および展開できる。 The machine learning model can be implemented and deployed using a machine learning framework, such as, for example, the TensorFlow framework, the Microsoft Cognitive Toolkit framework, the Apache Singa framework, or the Apache MXNet framework.
本明細書で説明される主題の実施形態は、たとえばデータサーバとして、バックエンドコンポーネントを含む、または、たとえばアプリケーションサーバであるミドルウェアコンポーネントを含む、または、たとえば、グラフィカルユーザインターフェース、ウェブブラウザ、もしくはユーザが本明細書で説明される主題の実施態様とそれを介してインタラクトできるアプリを有するクライアントコンピュータである、フロントエンドコンポーネントを含む、または、１つもしくは複数のそのようなバックエンド、ミドルウェア、もしくはフロントエンドのコンポーネントのうちの任意の組合せである、計算システムにおいて実施することができる。システムのコンポーネントは、たとえば通信ネットワークであるデジタルデータ通信の任意の形態または媒体によって相互接続できる。通信ネットワークの例は、ローカルエリアネットワーク（ＬＡＮ）と、たとえばインターネットであるワイドエリアネットワーク（ＷＡＮ）とを含む。 Embodiments of the subject matter described herein may be implemented in a computing system that includes a back-end component, e.g., as a data server, or includes a middleware component, e.g., an application server, or includes a front-end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an embodiment of the subject matter described herein, or any combination of one or more such back-end, middleware, or front-end components. The components of the system may be interconnected by any form or medium of digital data communication, e.g., a communications network. Examples of communications networks include local area networks (LANs) and wide area networks (WANs), e.g., the Internet.
計算システムは、クライアントおよびサーバを含むことができる。クライアントおよびサーバは、一般に、互いに離れており、通常は、通信ネットワークを介してインタラクトする。クライアントとサーバとの関係は、それぞれのコンピュータにおいて実行され、互いにクライアント－サーバの関係を有する、コンピュータプログラムによって生じる。いくつかの実施形態では、サーバは、たとえば、クライアントとして作動するデバイスとインタラクトするユーザにデータを表示し、ユーザからユーザ入力を受け取る目的で、データ、たとえばＨＴＭＬページを、ユーザデバイスに送信する。ユーザデバイスにおいて生成されているデータ、たとえばユーザインタラクションの結果は、デバイスから、サーバにおいて受け取られ得る。 A computing system may include clients and servers. Clients and servers are generally remote from each other and typically interact through a communications network. The relationship of clients and servers arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data, e.g., HTML pages, to a user device, e.g., for the purpose of displaying data to and receiving user input from a user interacting with the device acting as a client. Data being generated at the user device, e.g., results of user interaction, may be received at the server from the device.
本明細書は、多くの特定の実施態様の詳細を含んでいるが、これらは、任意の発明の範囲、または、特許請求され得る範囲に対する制限として解釈されるべきではなく、むしろ、特定の発明の特定の実施形態に特有であり得る特徴の説明として解釈されるべきである。本明細書において、別個の実施形態の文脈で説明される特定の特徴は、単一の実施形態において組み合わせて実施することもできる。逆に、単一の実施形態の文脈で説明される様々な特徴は、複数の実施形態において、個別に、または任意の適切な部分的な組合せで実施することもできる。さらに、特徴は、特定の組合せで作動するものとして上記に説明され、最初はそのように特許請求されていることもあるが、いくつかの場合には、特許請求されている組合せからの１つまたは複数の特徴は、組合せから除外され、特許請求されている組合せは、部分的な組合せ、または、部分的な組合せのバリエーションに向けられる場合がある。 Although the specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of a particular invention. Certain features described herein in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features described in the context of a single embodiment may also be implemented in multiple embodiments individually or in any suitable subcombination. Furthermore, although features may be described above as operating in a particular combination and may initially be claimed as such, in some cases one or more features from the claimed combination may be excluded from the combination and the claimed combination may be directed to a subcombination or a variation of the subcombination.
同様に、動作は、特定の順番で、図面に示され、特許請求の範囲に記載されているが、これは、所望の結果を達成するために、そのような動作が、示されている特定の順番で、もしくは連続した順番で実行されていること、または、例示されているすべての動作が実行されていることを必要とするものとして理解されるべきではない。特定の状況では、マルチタスクおよび並列処理が有利であり得る。さらに、上記で説明される実施形態における様々なシステムモジュールおよびコンポーネントの分離は、すべての実施形態においてそのような分離を必要とするものとして理解されるべきではなく、説明されるプログラムコンポーネントおよびシステムは、一般に、単一のソフトウェア製品に、共に統合できるか、または、複数のソフトウェア製品にパッケージできると理解されるべきである。 Similarly, although operations are shown in the figures and claimed in a particular order, this should not be understood as requiring such operations to be performed in the particular order shown, or in any sequential order, or that all of the illustrated operations be performed, to achieve desired results. In certain situations, multitasking and parallel processing may be advantageous. Furthermore, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the program components and systems described may generally be integrated together in a single software product or packaged in multiple software products.
主題の特定の実施形態が説明される。他の実施形態は、以下の特許請求の範囲内にある。たとえば、特許請求の範囲に記載されている動作は、異なる順番で実行されても、依然として所望の結果を達成することができる。１つの例として、添付の図面に示されているプロセスは、所望の結果を達成するために、必ずしも、示されている特定の順番、または連続した順番を必要とする訳ではない。いくつかの場合には、マルチタスクおよび並列処理が有利であり得る。 Specific embodiments of the subject matter are described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims may be performed in a different order and still achieve desirable results. As one example, the processes illustrated in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
ネットワーク層のグループ２９０を、決定ツリー２２０によって置き換える前に、システム１００は、トレーニング例の対応する部分の量子化バージョンを使用して、決定ツリー２２０をトレーニングすることができる。より具体的には、システム１００は、元のニューラルネットワークのトレーニング例が与えられた際の、層のグループ２９０の最初の層への入力の量子化バージョンと、層のグループ２９０の最後の層からの出力の量子化バージョンとを取得することができる。システム１００は、入力の量子化バージョンを、決定ツリー２２０への入力として設定し、出力の量子化バージョンを、決定ツリー２２０からの出力として設定し、入力および出力の量子化バージョンを使用して、決定ツリー２２０（ｔｈｅ ｄｅｃｉｓｉｏｎ ２２０）をトレーニングすることができる。
Before replacing the group of network layers 290 with the
それぞれの複数の候補グループの各々について、システムは、候補グループにおける層が、それぞれの決定ツリーによって置き換えられている、対応する新たな機械学習モデルのパフォーマンスを測定する候補グループのそれぞれのパフォーマンス尺度を判定する（４３０）。より具体的には、システムは、複数の新たな機械学習モデルを生成し、複数の新たな機械学習モデルの各々は、それぞれの決定ツリーによって置き換えられる層のそれぞれの候補グループを含んでいる。システムは、同じ入力データを使用して、新たな機械学習モデルの各々に対して推論計算を実行し、新たな機械学習モデルの各々について、それぞれのパフォーマンススコアを取得する。それぞれのパフォーマンススコア（ｐｅｒｆｏｒｍａｎｃｅｓ ｓｃｏｒｅｓ）は、推論精度に基づいて取得できる。 For each of the respective plurality of candidate groups, the system determines (430) a respective performance measure of the candidate group that measures the performance of a corresponding new machine learning model in which layers in the candidate group are replaced by the respective decision tree. More specifically, the system generates a plurality of new machine learning models, each of the plurality of new machine learning models including a respective candidate group of layers replaced by the respective decision tree. The system performs an inference calculation for each of the new machine learning models using the same input data, and obtains a respective performance score for each of the new machine learning models. The respective performance scores can be obtained based on the inference accuracy.
図５は、決定ツリー５００の例を示す。
図５に示されているように、決定ツリー５００は、特定のツリー深さを有する複数のノードを含むことができる。ツリー深さは、ツリー層の総数に基づいて決定される。決定ツリーの各々の層（Ｅａｃｈ ｌａｙｅｒ ｏｆ ｔｈｅ ｄｅｃｉｓｉｏｎ Ｔｒｅｅ）は、たとえば論理比較または他の適切な基準など、それぞれのノード演算を表すノードを含むことができる。
FIG. 5 shows an example of a
5, a
固定されている機能ハードウェア６００を使用するシステムは、各々の入力データを、対応するツリーノードに割り当て、それぞれの比較器およびマルチプレクサを使用して、各々のノードにおいてそれぞれの機能を実施することができる。図６に示されているように、システムは、入力データｘ∈［ｈ１，ｌ１］を、（図５におけるルートノード５１０に相当する）最上位ノードに割り当て、入力データｘ∈［ｈ２，ｌ２］を、（第２のツリー層における左側の非リーフノード５２０ａに相当する）左側ノード６３０ａに割り当て、入力データｘ∈［ｈ３，ｌ３］を、（第２のツリー層における右側の非リーフノード５２０ｂに相当する）右側ノード６３０ｂに割り当てる。システムは、それぞれの比較器６４０およびマルチプレクサ６３０を使用して、各々のノードにおいて演算を実施することができる。たとえば、システムは、比較器６４０ｃおよびマルチプレクサ６３０ｃを使用して、最上位ノードにおいて演算を実施する。より具体的には、システムは、実数値のベクトルであり得る入力ｘを受け取り、範囲ｘ∈［ｈ１，ｌ１］を有する入力の一部を選択して、最上位のルートノードに提供する。システムは、比較器６４０ｃを使用して、入力の一部を、基準ａ１と比較し、基準ａ１は、実数値スカラを表すことができる。入力データｘが、ａ１以上である場合、システムは、ノードに割り当てられるマルチプレクサ６３０ｃを使用して「真」を表す結果を出力し、たとえば、マルチプレクサは、「真」を表す整数１を出力することができる。システムは、現在のノードが、対応するツリーの分岐に沿ってリンクしている子ノードにおいて演算を実行し続けることができる。最後から２番目のツリー層における非リーフノードに対する演算を処理した後、システムは、決定ツリーにおける対応するリーフノードによって表されている値（たとえば、確率）に基づいて最終結果６２０を出力することができる。
A system using fixed
Claims (15)
シーケンスに配置されている複数の層を備えるニューラルネットワークを表すデータを受け取ることと、
前記複数の層から層の１つまたは複数のグループを選択することとを含み、
層の各々のグループは、前記シーケンスにおいて互いに隣接する１つまたは複数の層を備え、
前記方法は、
前記ニューラルネットワークに対応する新たな機械学習モデルを生成することをさらに含み、前記新たな機械学習モデルを生成することは、
層の各々のグループについて、層の前記グループを置き換えるそれぞれの決定ツリーを選択することを含み、それぞれの前記決定ツリーは、前記グループにおけるそれぞれの最初の層への入力の量子化バージョンを入力として受け取り、前記グループにおけるそれぞれの最後の層の出力の量子化バージョンを出力として生成し、それぞれの前記決定ツリーのツリー深さは、前記グループの層の数に少なくとも部分的に基づく、方法。 1. A method implemented by one or more computers, the method comprising:
receiving data representative of a neural network comprising a plurality of layers arranged in a sequence;
selecting one or more groups of layers from the plurality of layers;
each group of layers comprises one or more layers adjacent to one another in said sequence;
The method comprises:
and generating a new machine learning model corresponding to the neural network, the generating the new machine learning model comprising:
1. A method for computing a quantized decision tree for each group of layers, comprising: selecting, for each group of layers, a respective decision tree to replace the group of layers, each of the decision trees receiving as input a quantized version of an input to a respective first layer in the group and producing as output a quantized version of an output of a respective last layer in the group, a tree depth of each of the decision trees being based at least in part on a number of layers in the group.
前記シーケンスに従って、前記ニューラルネットワークの層の前記１つまたは複数のグループに後続する前記ニューラルネットワークの前記層をトレーニングすることを含む、請求項２に記載の方法。 Training at least a portion of the layers in the neural network that were not replaced by a respective decision tree includes:
The method of claim 2 , further comprising training the layer of the neural network subsequent to the one or more groups of layers of the neural network in accordance with the sequence.
前記ニューラルネットワークにおけるそれぞれの初期層を選択することと、
各々が、それぞれの前記初期層を、候補グループにおける最初の層として有する、それぞれの複数の前記候補グループを生成することと、
それぞれの前記複数の前記候補グループの各々について、それぞれの決定ツリーによって置き換えられる前記候補グループに前記層を有する対応する新たな機械学習モデルのパフォーマンスを測定する前記候補グループのそれぞれのパフォーマンス尺度を判定することと、
それぞれの前記複数の前記候補グループについて、それぞれのパフォーマンス尺度に基づいて、前記候補グループのうちの１つを前記グループとして選択することとを含む、請求項１～請求項３のいずれか１項に記載の方法。 Selecting each of the one or more groups of layers includes:
selecting a respective initial layer in the neural network;
generating a respective plurality of said candidate groups, each having a respective said initial layer as a first layer in the candidate group;
determining, for each of the respective plurality of candidate groups, a respective performance measure of the candidate group that measures the performance of a corresponding new machine learning model having the layer in the candidate group replaced by a respective decision tree;
and for each of said plurality of candidate groups, selecting one of said candidate groups as said group based on a respective performance measure.
それぞれの前記初期層を、ランダムプロセスによって、または、前記ニューラルネットワークの前記シーケンスに基づいて選択することを含む、請求項４に記載の方法。 Selecting each of the initial layers in the neural network to generate a respective group of layers includes:
The method of claim 4 , comprising selecting each of the initial layers by a random process or based on the sequence of the neural networks.
それぞれの前記パフォーマンス尺度の中から最大のパフォーマンス尺度を判定することと、
それぞれの前記複数の前記候補グループから、前記最大のパフォーマンス尺度に関連付けられている候補グループを、前記グループとして選択することとを含む、請求項４または請求項５に記載の方法。 Selecting one of the candidate groups as the group based on a respective performance measure includes:
determining a maximum performance measure from each of said performance measures;
and selecting as said group from each of said plurality of candidate groups the candidate group associated with said maximum performance measure.
層の前記１つまたは複数のグループにない前記ニューラルネットワークにおける各々の層について、前記層に関連付けられている重みの少なくとも一部を量子化することをさらに含む、請求項１～請求項８のいずれか１項に記載の方法。 Each layer has a respective set of weights, and the method comprises:
9. The method of claim 1, further comprising, for each layer in the neural network that is not in the one or more groups of layers, quantizing at least a portion of the weights associated with the layer.
それぞれの層のグループを置き換える各々の決定ツリーは、前記トレーニングデータセットのそれぞれの量子化バージョンの一部に基づいてトレーニングされ、
前記トレーニングデータセットの前記量子化バージョンの前記それぞれの一部の各々のトレーニングサンプルは、（ｉ）前記グループの前記最初の層への層入力の量子化バージョンと、（ｉｉ）前記グループの前記最後の層からの層出力の量子化バージョンとを備える、請求項１～請求項１１のいずれか１項に記載の方法。 the neural network represented by the received data is a neural network that has been initially trained with a training data set;
each decision tree replacing a respective group of layers is trained based on a portion of a respective quantized version of the training dataset;
12. The method of claim 1, wherein each training sample of the respective portion of the quantized version of the training dataset comprises (i) a quantized version of a layer input to the first layer of the group, and (ii) a quantized version of a layer output from the last layer of the group.
Publications (1)
Publication Number | Publication Date |
---|---|
JP2024519326A true JP2024519326A (en) | 2024-05-10 |
Family
ID=
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110520871B (en) | Training machine learning models using learning progress measurements | |
US11790212B2 (en) | Quantization-aware neural architecture search | |
WO2022057776A1 (en) | Model compression method and apparatus | |
KR20210029785A (en) | Neural network acceleration and embedding compression system and method including activation sparse | |
CN111783462A (en) | Chinese named entity recognition model and method based on dual neural network fusion | |
US11604960B2 (en) | Differential bit width neural architecture search | |
CN111368993A (en) | Data processing method and related equipment | |
CN113837370B (en) | Method and apparatus for training a model based on contrast learning | |
WO2022156561A1 (en) | Method and device for natural language processing | |
CN114564593A (en) | Completion method and device of multi-mode knowledge graph and electronic equipment | |
CN113632106A (en) | Hybrid precision training of artificial neural networks | |
US20230177097A1 (en) | Multi-phase training of machine learning models for search ranking | |
CN113761868A (en) | Text processing method and device, electronic equipment and readable storage medium | |
CN110569355B (en) | Viewpoint target extraction and target emotion classification combined method and system based on word blocks | |
CN115795065A (en) | Multimedia data cross-modal retrieval method and system based on weighted hash code | |
CN115879508A (en) | Data processing method and related device | |
CN115080749A (en) | Weak supervision text classification method, system and device based on self-supervision training | |
US11941360B2 (en) | Acronym definition network | |
US20230196128A1 (en) | Information processing method, apparatus, electronic device, storage medium and program product | |
CN111753995A (en) | Local interpretable method based on gradient lifting tree | |
JP2024519326A (en) | Embedding decision trees in neural networks | |
CN117371447A (en) | Named entity recognition model training method, device and storage medium | |
EP4315165A1 (en) | Incorporation of decision trees in a neural network | |
CN110059314B (en) | Relation extraction method based on reinforcement learning | |
CN113641790A (en) | Cross-modal retrieval model based on distinguishing representation depth hash |