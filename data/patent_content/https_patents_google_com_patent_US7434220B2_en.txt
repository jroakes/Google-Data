US7434220B2 - Distributed computing infrastructure including autonomous intelligent management system - Google Patents
Distributed computing infrastructure including autonomous intelligent management system Download PDFInfo
- Publication number
- US7434220B2 US7434220B2 US10/662,936 US66293603A US7434220B2 US 7434220 B2 US7434220 B2 US 7434220B2 US 66293603 A US66293603 A US 66293603A US 7434220 B2 US7434220 B2 US 7434220B2
- Authority
- US
- United States
- Prior art keywords
- computer
- message
- peer
- blade
- portable
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
- H04L67/104—Peer-to-peer [P2P] networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/07—Responding to the occurrence of a fault, e.g. fault tolerance
- G06F11/16—Error detection or correction of the data by redundancy in hardware
- G06F11/1658—Data re-synchronization of a redundant component, or initial sync of replacement, additional or spare unit
- G06F11/1662—Data re-synchronization of a redundant component, or initial sync of replacement, additional or spare unit the resynchronized component or unit being a persistent storage device
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/07—Responding to the occurrence of a fault, e.g. fault tolerance
- G06F11/16—Error detection or correction of the data by redundancy in hardware
- G06F11/20—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements
- G06F11/2097—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements maintaining the standby controller/processing unit updated
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/54—Interprogram communication
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L12/00—Data switching networks
- H04L12/02—Details
- H04L12/16—Arrangements for providing special services to substations
- H04L12/18—Arrangements for providing special services to substations for broadcast or conference, e.g. multicast
- H04L12/1813—Arrangements for providing special services to substations for broadcast or conference, e.g. multicast for computer conferences, e.g. chat rooms
- H04L12/1818—Conference organisation arrangements, e.g. handling schedules, setting up parameters needed by nodes to attend a conference, booking network resources, notifying involved parties
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/08—Configuration management of networks or network elements
- H04L41/0803—Configuration setting
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/08—Configuration management of networks or network elements
- H04L41/0803—Configuration setting
- H04L41/0813—Configuration setting characterised by the conditions triggering a change of settings
- H04L41/0816—Configuration setting characterised by the conditions triggering a change of settings the condition being an adaptation, e.g. in response to network events
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/08—Configuration management of networks or network elements
- H04L41/085—Retrieval of network configuration; Tracking network configuration history
- H04L41/0853—Retrieval of network configuration; Tracking network configuration history by actively collecting configuration information or by backing up configuration information
- H04L41/0856—Retrieval of network configuration; Tracking network configuration history by actively collecting configuration information or by backing up configuration information by backing up or archiving configuration information
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/08—Configuration management of networks or network elements
- H04L41/085—Retrieval of network configuration; Tracking network configuration history
- H04L41/0859—Retrieval of network configuration; Tracking network configuration history by keeping history of different configuration generations or by rolling back to previous configuration versions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/08—Configuration management of networks or network elements
- H04L41/085—Retrieval of network configuration; Tracking network configuration history
- H04L41/0859—Retrieval of network configuration; Tracking network configuration history by keeping history of different configuration generations or by rolling back to previous configuration versions
- H04L41/0863—Retrieval of network configuration; Tracking network configuration history by keeping history of different configuration generations or by rolling back to previous configuration versions by rolling back to previous configuration versions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/50—Network service management, e.g. ensuring proper service fulfilment according to agreements
- H04L41/5003—Managing SLA; Interaction between SLA and QoS
- H04L41/5009—Determining service level performance parameters or violations of service level contracts, e.g. violations of agreed response time or mean time between failures [MTBF]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/06—Message adaptation to terminal or network requirements
- H04L51/066—Format adaptation, e.g. format conversion or compression
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
- H04L67/104—Peer-to-peer [P2P] networks
- H04L67/1044—Group management mechanisms
- H04L67/1048—Departure or maintenance mechanisms
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
- H04L67/104—Peer-to-peer [P2P] networks
- H04L67/1061—Peer-to-peer [P2P] networks using node-based peer discovery mechanisms
- H04L67/1068—Discovery involving direct consultation or announcement among potential requesting and potential source peers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
- H04L67/1097—Protocols in which an application is distributed across nodes in the network for distributed storage of data in networks, e.g. transport arrangements for network file system [NFS], storage area networks [SAN] or network attached storage [NAS]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/133—Protocols for remote procedure calls [RPC]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/34—Network arrangements or protocols for supporting network services or applications involving the movement of software or configuration parameters
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/50—Network services
- H04L67/56—Provisioning of proxy services
- H04L67/565—Conversion or adaptation of application format or content
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/50—Network services
- H04L67/60—Scheduling or organising the servicing of application requests, e.g. requests for application data transmissions using the analysis and optimisation of the required network resources
- H04L67/62—Establishing a time schedule for servicing the requests
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/08—Protocols for interworking; Protocol conversion
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/30—Definitions, standards or architectural aspects of layered protocol stacks
- H04L69/32—Architecture of open systems interconnection [OSI] 7-layer type protocol stacks, e.g. the interfaces between the data link level and the physical level
- H04L69/322—Intralayer communication protocols among peer entities or protocol data unit [PDU] definitions
- H04L69/329—Intralayer communication protocols among peer entities or protocol data unit [PDU] definitions in the application layer [OSI layer 7]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L9/00—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols
- H04L9/40—Network security protocols
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/07—Responding to the occurrence of a fault, e.g. fault tolerance
- G06F11/16—Error detection or correction of the data by redundancy in hardware
- G06F11/20—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements
- G06F11/202—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements where processing functionality is redundant
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/07—Responding to the occurrence of a fault, e.g. fault tolerance
- G06F11/16—Error detection or correction of the data by redundancy in hardware
- G06F11/20—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements
- G06F11/202—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements where processing functionality is redundant
- G06F11/2023—Failover techniques
- G06F11/203—Failover techniques using migration
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/07—Responding to the occurrence of a fault, e.g. fault tolerance
- G06F11/16—Error detection or correction of the data by redundancy in hardware
- G06F11/20—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements
- G06F11/202—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements where processing functionality is redundant
- G06F11/2023—Failover techniques
- G06F11/2033—Failover techniques switching over of hardware resources
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/07—Responding to the occurrence of a fault, e.g. fault tolerance
- G06F11/16—Error detection or correction of the data by redundancy in hardware
- G06F11/20—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements
- G06F11/202—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements where processing functionality is redundant
- G06F11/2038—Error detection or correction of the data by redundancy in hardware using active fault-masking, e.g. by switching out faulty elements or by switching in spare elements where processing functionality is redundant with a single idle spare processing component
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/08—Configuration management of networks or network elements
- H04L41/0803—Configuration setting
- H04L41/0813—Configuration setting characterised by the conditions triggering a change of settings
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/08—Configuration management of networks or network elements
- H04L41/0876—Aspects of the degree of configuration automation
- H04L41/0879—Manual configuration through operator
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/08—Configuration management of networks or network elements
- H04L41/0876—Aspects of the degree of configuration automation
- H04L41/0886—Fully automatic configuration
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/22—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks comprising specially adapted graphical user interfaces [GUI]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L41/00—Arrangements for maintenance, administration or management of data switching networks, e.g. of packet switching networks
- H04L41/50—Network service management, e.g. ensuring proper service fulfilment according to agreements
- H04L41/5061—Network service management, e.g. ensuring proper service fulfilment according to agreements characterised by the interaction between service providers and their network customers, e.g. customer relationship management
- H04L41/5067—Customer-centric QoS measurements
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/02—Protocols based on web technology, e.g. hypertext transfer protocol [HTTP]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
- H04L67/1001—Protocols in which an application is distributed across nodes in the network for accessing one among a plurality of replicated servers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
- H04L67/1001—Protocols in which an application is distributed across nodes in the network for accessing one among a plurality of replicated servers
- H04L67/10015—Access to distributed or replicated servers, e.g. using brokers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/50—Network services
- H04L67/56—Provisioning of proxy services
- H04L67/568—Storing data temporarily at an intermediate stage, e.g. caching
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/30—Definitions, standards or architectural aspects of layered protocol stacks
- H04L69/32—Architecture of open systems interconnection [OSI] 7-layer type protocol stacks, e.g. the interfaces between the data link level and the physical level
- H04L69/322—Intralayer communication protocols among peer entities or protocol data unit [PDU] definitions
- H04L69/328—Intralayer communication protocols among peer entities or protocol data unit [PDU] definitions in the presentation layer [OSI layer 6]
Definitions
- the present invention relates generally to computer systems and specifically to distributed computing and storage mechanisms on networked computer systems.
- PCs personal computers
- each user of a PC in the enterprise has a networked PC at his/her desk or work area.
- the management of resources in the network may become increasingly complex and expensive.
- Some of the manageability issues involved in maintaining a large number of networked computer systems may include ease of installation and deployment, the topology and physical logistics of the network, asset management, scalability (the cost and effort involved in increasing the number of units), troubleshooting network or unit problems, support costs, software tracking and management, as well as the simple issue of physical space, be it floor space or room on the desktop, as well as security issues regarding physical assets, information protection, software control, and computer virus issues.
- a typical computer system has input/output (I/O) access to one or more volumes on a storage device such as a hard drive.
- a volume generally includes an amount of free (unused) storage space which varies over time. When the number of computers and respective volumes in the networked system becomes very large, the total amount of unused storage may become significant.
- the computer systems may be configured to access virtual network attached storage (referred to herein as a shared storage volume).
- the computer systems may include a first computer, having a first processor and a first storage medium coupled to the first processor, and a peripheral device, e.g., a keyboard, monitor, mouse, or other type of peripheral device.
- the computer systems may further include a second computer having a second processor and a second storage medium coupled to the second processor.
- the first computer and the second computer may each be configured on a respective computer blade, also referred to as a “computer on a card.”
- a communication link e.g., a bus, may couple the first computer to the second computer.
- a peripheral switch may be coupled to the first computer, the second computer, and the peripheral device. The peripheral switch may be configured to route signals between the peripheral device and the first computer.
- each computer may have some file server functionality.
- the file server functionality may include some I/O functionality, and may also include functionality for negotiating file write and read protocols. Communicating with the storage medium on the computer may be analogous to communicating with a standard file server attached memory.
- the computer may negotiate information writes similar to a file server, and order and prioritize transactions.
- the computer may also implement striping analogous to that used by RAID (Redundant Array of Inexpensive Disks).
- a Distributed Computing Infrastructure adds to these underlying capabilities by providing a software platform for creating, running, and managing distributed applications.
- the DCI software solution includes a framework that provides a variety of management and deployment functions for creating and running large-scale distributed applications.
- DCI may be implemented using a Java-based framework.
- the infrastructure is not dependent on the presence of a server and therefore eliminates a single-point bottleneck that often slows performance of distributed applications.
- the DCI solution is primarily designed to make it easy for domain specialists (biologists, mathematicians, etc.) to re-leverage existing code modules, scripts, and applications and adapt them to make use of the immense power of cluster solutions.
- DCI may include eXtensible Markup Language (XML) capable software applications on a peer-to-peer network.
- a source application on a first computer system may generate a message intended for a second computer system.
- the message may be translated from an original, internal, or other native format to a portable format (e.g., XML) on the first computer system, thereby generating a portable (e.g., XML) message.
- the portable message may include metadata which comprise identifying characteristics of the source application.
- the portable message may be sent from the first computer system to a second computer system using peer-to-peer message passing between the first computer system, the second computer system, and optionally one or more intermediary computer systems. After being received at the second computer system, the portable message may be routed using DCI to an appropriate target application based on the metadata.
- DCI may include small, network-unaware applications called “peerlets.”
- Peerlets may be suitable for applications including chat, shared whiteboard, and other collaborative applications.
- a peerlet on a first computer system may generate a message (including collaborative data such as chat text or whiteboard graphics) and send the message to the distributed computing infrastructure using an AN application programming interface (API).
- the DCI may translate the message from an original or native format to a portable format (e.g., XML), thereby generating a portable (e.g., XML) message, wherein the portable message comprises metadata which comprise identifying characteristics of the source peerlet.
- a portable format e.g., XML
- the portable message may then be sent from the first computer system to a second computer system using peer-to-peer message passing between the first computer system, the second computer system, and optionally one or more intermediary computer systems;.
- DCI may route the portable message to a target peerlet on the second computer system based on the metadata.
- the target peerlet is configured to communicate using the API to the distributed computing infrastructure API.
- DCI may include a system and method for creating complex distributed applications using pre-complied binaries or other functions on distributed computer systems.
- Instructions for performing a complex task may be sent a first computer system to one or more remote computer systems.
- the instructions for performing the task may comprise instructions for performing one or more subtasks with each of a plurality of applications.
- the instructions for performing the task may comprise a plurality of messages in a portable format (e.g., XML).
- the instructions for performing the task may be translated using DCI from the portable format to a format which is executable, thereby generating executable instructions for performing the plurality of subtasks.
- the executable instructions may be executed at the remote computer systems to perform the subtasks comprising the task.
- DCI may include a capability for multiple, independent collaborative sessions for distributed collaborative applications (e.g., chat, instant messaging, shared whiteboard, etc.).
- collaborative data may be sent from a first instance of the collaborative application on a first computer system to a second instance of the collaborative application on a second computer system.
- collaborative data may be sent from the second instance of the collaborative application to the first instance of the collaborative application.
- collaborative data may be sent from a third instance of the collaborative application on the first computer system to a fourth instance of the collaborative application on the second computer system, and from the fourth instance to the third instance.
- Each instance of the collaborative application may be associated with a globally unique ID (GUID) which distinguishes the respective instance from other instances in the networked computing environment.
- GUID globally unique ID
- the application type of the relevant collaborative application may be determined. If there is already an instance of the application type on the target computer system, the collaborative data may be sent to the existing instance. If not, a new instance may be instantiated and may receive the collaborative data.
- DCI may include a system and method for reducing interactions between users and applications to an archivable form.
- User input may be entered to a source application on a first computer system to request performance of a task.
- the task may be performed.
- a message may also be generated in response to the user input, wherein the message comprises one or more instructions which are computer-executable to perform the task.
- the message may include metadata which comprise identifying characteristics of the source application.
- the message may be translated from an original or native format (e.g., a internal format which is suitable for communication with an API, or a format which is suitable for execution) to a portable format (e.g., XML) on the first computer system, thereby generating a portable message.
- the portable message may be stored in a message log or message queue for retrieval and/or playback at a later time.
- the instructions in the message may be retrieved and re-executed on the same computer system or different computer systems.
- DCI may include an Autonomous Intelligent Management System (AIMS).
- AIMS may provide for the retrieval and playback of archived portable messages (e.g., XML messages) to perform a task on one or more target computer systems in a network.
- the target computer systems need not be the original computer system on which the instructions for the task were recorded.
- DCI may include a system and method for automatic software retrieval on a peer-to-peer network.
- Software may be sent from a first computer system to one or more remote computer systems along with instructions for automatically installing the software at the remote computer systems.
- the instructions for deploying the software may comprise one or more messages in a portable format (e.g., XML).
- a portable format e.g., XML
- the instructions for installing the software may be translayed from the portable format to an executable format at each of the one or more remote computer systems, thereby generating executable instructions.
- the executable instructions may then be executed to install the software at each of the one or more remote computer systems.
- FIG. 1 illustrates computer systems including peripheral devices coupled to computer blades in a cage, according to one embodiment
- FIG. 2 illustrates a computer blade pulled out of the cage, according to one embodiment
- FIG. 3 illustrates a computer blade having a power supply, hard drive, and motherboard, according to one embodiment
- FIGS. 4 a , 4 b , 4 c , and 4 d illustrate computer blade storage patterns, according to one embodiment
- FIG. 5 illustrates a failed computer blade restored onto a backup computer blade, according to one embodiment
- FIG. 6 illustrates an embodiment of a flowchart for restoring a failed computer by copying after a failure is detected
- FIG. 7 illustrates an embodiment of a flowchart for restoring a failed computer by copying before a failure is detected
- FIG. 8 illustrates an embodiment of a flowchart for restoring a failed computer by copying from a third storage medium
- FIG. 9 illustrates a login screen of a management console, according to an embodiment
- FIG. 10 illustrates a main screen in a web-based management interface, according to an embodiment
- FIG. 11 illustrates a configuration screen, according to an embodiment
- FIG. 12 illustrates a screen for a scheduling interface, according to an embodiment
- FIG. 13 illustrates an information recovery view, according to an embodiment
- FIG. 14 illustrates an archive view screen, according to an embodiment
- FIG. 15 illustrates a memory stack on a computer blade storing information from other computer blades, according to one embodiment
- FIG. 16 illustrates a memory stack for a computer blade separated by a PC region and a Server region, according to one embodiment
- FIG. 17 illustrates an enterprise switch and a storage network switch controlling attached PCs, according to one embodiment
- FIG. 18 illustrates a one-to-one move, according to two embodiments
- FIG. 19 illustrates a cascading move, according to one embodiment
- FIG. 20 illustrates a swap move, according to one embodiment
- FIG. 21 flowcharts an initiation and process of a move, according to one embodiment
- FIG. 22 is a block diagram illustrating a DCI architecture, according to one embodiment
- FIG. 23 is a flowchart illustrating a method for using DCI, according to one embodiment
- FIG. 24 illustrates a Virtual Network Attached Storage (VNAS) architecture, according to one embodiment
- FIG. 25 is a flowchart illustrating a method for using a VNAS system, according to one embodiment
- FIG. 26 is a screenshot that demonstrates a manner in which commands can be broadcasted to every node running the DCI platform, according to one embodiment.
- FIG. 27 is a block diagram illustrating an AIMS architecture, according to one embodiment.
- FIGS. 1 - 3 Elements of Computer Systems Used in Various Embodiments
- FIGS. 1-3 illustrate computer system components that may be used in various embodiments of the invention.
- the system may include a plurality of computer systems, where each computer system may include at least one peripheral device, e.g., comprised in a human interface, and a computer blade.
- a computer blade (or “blade”) may comprise a “computer on a card.”
- the computing system may be comprised on a circuit card which may include standard computing system components such as a central processing unit (CPU), memory, power supply, and network interface, as well as an extender, e.g., a Peripheral Component Interconnect (PCI) extender, for communicating with the remote human interface.
- PCI Peripheral Component Interconnect
- the computer system may include various components necessary for computer operations, such as, but not limited to, a processor and a storage medium.
- a processor and a storage medium.
- FIGS. 1-3 various embodiments of the present invention may be implemented using the systems of FIGS. 1-3 , where, for example, information from a first storage medium on a first computer blade may be copied to a second storage medium on a second computer blade.
- the information may be copied before, during, or after a fail-over condition is detected on the first computer blade.
- the term “fail-over condition” may refer to an impending failure of a component, to the component failure itself, or to a condition of a computer that requires attention.
- a computer that indicates a fail-over condition may be referred to as a “failed” computer, even if an actual component failure has not occurred.
- a fail-over condition may indicate failure of a processor, impending failure of a processor, or even that the performance, e.g., the processing capability, of the computer needs to be upgraded or modified.
- a fail-over condition may include, but is not limited to, a hard drive crash or a short circuit in a processor, or indications that such a crash or short circuit is imminent or likely.
- software executing on one or more of the computer blades may continually monitor the first computer blade for a fail-over condition.
- Other ways of detecting a fail-over condition on the first computer blade may also be within the scope of the invention.
- a peripheral switch may switch a first computer peripheral device over to a second computer blade.
- the peripheral switch may switch the human interface from the first (failing or failed) computer blade to the second (replacement) computer blade.
- the information from the first storage medium may be copied prior to the fail-over condition.
- the information may be copied directly to the second computer blade, or alternatively, the information may be copied onto a third storage medium on a third computer blade, where after the fail-over condition is detected, the information from the first storage medium (now stored on the third storage medium) may be copied onto the second storage medium of the second computer blade.
- a peripheral switch may then switch the first computer peripheral device over to the second computer blade, and the user of the first computer system may continue accessing the information from the first storage medium without losing user time.
- the peripheral switch may be further operable to switch a plurality of peripheral devices in the human interface, i.e., the number peripheral devices switched is not limited to one.
- a peripheral switch may not be needed in an embodiment where a backup component, such as, but not limited to a backup processor or a backup storage medium, is coupled to the first computer prior to or after a fail-over condition is detected in a corresponding component on the first computer.
- a backup component such as, but not limited to a backup processor or a backup storage medium
- a backup processor may be coupled to the first computer to take the place of the processor on the first computer with a fail-over condition.
- FIG. 1 Computer Blades and Respective Peripheral Devices
- FIG. 1 an embodiment of computer systems including peripheral devices coupled to computer blades in a cage is shown. While one embodiment may include computer blades, it is noted that other computer types and forms may also be within the scope of the invention. In other words, the embodiment shown in FIG. 1 is intended to be exemplary only, and is not intended to limit the types or number of computer systems used.
- connecting cables 151 , 153 , and 155 may connect computer blades 101 , 105 , and 109 to respective peripheral device groups through respective device ports or hubs, referred to herein as C-Ports, 157 , 159 , and 161 .
- each device port may comprise an extender device that may enable transmission of user interface signals (i.e., peripheral device signals) over distances generally not allowed by standard protocols such as Universal Serial Bus (USB).
- USB Universal Serial Bus
- the peripheral device groups may include a keyboard 117 , a pointing device, e.g., a mouse 119 , a display device, e.g., a computer monitor 121 , and/or other peripheral devices for human interface.
- the computer blade such as computer blade 105 , may communicate with the peripheral devices coupled to the computer blade 105 by sending and receiving encoded human interface signals transmitted over the connecting cable 151 .
- a cage 113 e.g., a metal cabinet or chassis, may have a plurality of slots, such as slots 103 , 107 , and 111 .
- the computer blades 101 , 105 , and 109 may be inserted into the slots 103 , 107 , and 111 , respectively.
- the cage 113 may also include cage connectors (not shown) to couple the computer blades 101 , 105 , and 109 to their respective connecting cables 155 , 153 , and 151 .
- the computer blades 101 , 105 , and 109 may be installed in the cage 113 at a central location, while the peripheral devices for each computer blade 101 , 105 , and 109 may be located remotely from the cage 113 , such as at respective work areas of the users of the computer blades 101 , 105 , and 109 .
- the separation of the peripheral device groups from the computer blades 101 , 105 , and 109 may allow easier software installation across a network, such as but not limited to downloading CD-ROMs, and provide a central location of multiple computers which may simplify both hardware and software maintenance.
- Each computer blade 101 , 105 , and 109 may also be coupled to a network 115 through an on-board network logic (not shown).
- the network 115 may be a Local Area Network (LAN) or a Wide Area Network (WAN), such as the Internet, although other networks are also contemplated.
- the computer blades 101 , 105 , and 109 may be inserted into respective slots 103 , 107 , and 111 of the cage 113 , and coupled to respective peripheral device groups through the cage connectors (not shown) and connecting cables 151 , 153 , and 155 .
- each computer blade 101 , 105 , and 109 may also be coupled to the network 115 through the cage connectors (not shown) and a network cable, such as Ethernet cables 163 , 165 , and 167 .
- FIG. 2 Computer Blade
- the computer blade 105 may include components such as but not limited to a slide drawer frame 205 , motherboard 207 , a power supply 210 , and a hard drive 208 , as shown.
- the motherboard 207 , the power supply 210 , and the hard drive 208 may be coupled to the slide drawer frame 205 .
- the slide drawer frame 205 may be three rack units high (or approximately 5.25 inches) to occupy a much smaller space than standard PC units, although other slide drawer frame 205 dimensions may also be within the scope of the invention.
- the motherboard 207 may be a printed circuit board with components such as but not limited to a central processing unit (CPU), memory, and LAN interface. Other types of motherboards and other types of motherboard components are also contemplated.
- the hard drive 208 may be a non-volatile memory such as but not limited to a hard drive, optical drive, and/or flash memory.
- the computer blade 105 may communicate with external systems such as but not limited to peripheral devices and networks, through an edge connector 209 .
- the edge connector 209 may transmit signals such as but not limited to network signals, input/output (I/O) signals, video signals, audio signals, and universal serial bus (USB) signals.
- the edge connector may communicate network signals to a network and encoded human interface signals to a group of peripheral devices.
- the computer blade 105 may further include power supply 210 mounted on the slide drawer frame 205 with an internal power source or coupled to an external power source (not shown) to provide power to the computer blade 105 .
- the power supply 210 may convert local main power to an appropriate voltage for the computer blade 105 . Because computer blade 105 has an individual power supply 210 , if the power supply 210 fails, computer blade 105 may be the only computer blade that fails.
- a single power supply located in the cage 113 may supply power to several computer blades such as computer blades 101 , 105 , and 109 (shown in FIG. 1 ). However, a single power supply for the cage 113 (shown in FIG.
- the cage 1 may be a single point of failure for the cage 113 . If the single power supply fails, multiple computer blades may also fail, requiring multiple replacement blades. In a system with a single power supply for a cage 113 , the computer blades 101 , 105 , and 109 may all require stand-by replacement blades connected to another power source. If the power supply for the cage 113 fails, information from the computer blades 101 , 105 , and 109 may be copied onto the replacement computer blades from other computer blades in the system to which information from the computer blades 101 , 105 , and 109 had been previously copied.
- cage 113 may have a plurality of slots, such as slot 107 , to house the computer blade 105 .
- the computer blade 105 may be inserted into one of the slots of the cage 113 , such as slot 107 .
- the cage 113 may include a cage connector (not shown) to couple to the edge connector 209 on the computer blade 105 .
- the cage connector may also include an external second connector (not shown) that is electrically coupled to the computer blade 105 when the computer blade 105 is inserted into the slot 107 .
- the external second connector may be further coupled to the connecting cables 151 , 153 , and 155 (shown in FIG.
- the use of the cage connectors (not shown) as an intermediate connection between computer blade 105 and the connecting cable 153 (shown in FIG. 1 ) may allow the removal and exchange of computer blade 105 without the need to disconnect the connecting cable 153 (shown in FIG. 1 ) from the cage 113 . If the computer blade 105 fails, the computer blade 105 may be removed and a new computer blade (not shown) inserted in a slot, e.g., slot 107 .
- the user's human interface e.g., one or more peripheral devices
- a replacement computer blade possibly in a manner that is transparent to the user
- FIG. 3 Computer Blade Components
- the computer blade 105 may include elements that make up a standard PC, such as, but not limited to, a motherboard 207 with various components such as but not limited to a processor, e.g., a CPU 306 , memory 304 , and interface logic 302 , which may include network logic 305 , I/O logic 307 , and human interface logic 303 , as well as other interface circuitry associated with a motherboard 207 , configured on a single card.
- a processor e.g., a CPU 306 , memory 304 , and interface logic 302 , which may include network logic 305 , I/O logic 307 , and human interface logic 303 , as well as other interface circuitry associated with a motherboard 207 , configured on a single card.
- the network logic 305 may include a LAN or WAN connection, such as but not limited to a IEEE803.2 (10/100 BaseT) Ethernet, and circuitry for connecting to peripheral devices coupled to the computer blade 105 .
- the computer blade 105 may be electrically coupled to the cage 113 (shown in FIG. 2 ) through the edge connector 209 or interfacing edge connector 309 that may face to the rear of the computer blade 105 .
- the computer blade 105 may slide into a slot 107 (shown in FIG. 2 ) of the cage 113 (shown in FIG. 2 ), making contact with the cage connector (not shown).
- the computer blade 105 may further include a network interface logic 305 included on a printed circuit board for interfacing to a network.
- the network logic 305 may encode network signals into a format suitable for transmission to the network.
- the network logic 305 may also receive encoded network signals from the network, and decode the encoded network signals.
- the motherboard 207 may further include logic supporting PCI slot-based feature cards.
- the components on the computer blade 105 may be arranged from front to back for thermal efficiency.
- the interface logic 302 may be located at the rear of the computer blade 105
- the power supply 210 and hard disk 208 may be located at the front of the computer blade 105 .
- the computer blade 105 may have different slide drawer frame shapes, such as but not limited to square, rectangle, cubic, and three-dimensional rectangular forms.
- the computer blade 105 may have components mounted on either side of the computer blade 105 .
- the computer blade 105 may also have components mounted on both sides of the computer blade 105 . If the slide drawer frame 205 has a three-dimensional shape, the components may be mounted on an inside surface and outside surface of the slide drawer frame 205 .
- FIGS. 4 a , 4 b , 4 c , and 4 d Computer Blade Storage Patterns
- FIGS. 4 a , 4 b , 4 c , and 4 d embodiments of computer blade storage patterns are shown for three computer blades 401 , 403 , and 405 . It is noted that the systems shown in FIGS. 4 a , 4 b , 4 c , and 4 d are meant to be exemplary and are not intended to limit the system or method to any particular number of computers. As shown in FIG. 4 a , in one embodiment, to prepare for a failure of computer blade 401 , the information from a storage medium on the computer blade 401 may be copied (i.e., backed up) onto a storage medium on the computer blade 403 .
- a first portion of the information on the computer blade 401 may be located in the random access memory (RAM) of the first computer blade 401 and a second portion of the information may be located on a hard drive of the computer blade 401 .
- RAM random access memory
- Other locations of information from the computer blade 401 may also be within the scope of the invention. If the computer blade 401 fails or indicates a fail-over condition, a peripheral device coupled to the computer blade 401 through a peripheral switch may be switched over to the computer blade 403 through the peripheral switch, thereby enabling a user of computer blade 401 to continue accessing the information (originally) from computer blade 401 (but now on computer blade 403 ).
- the information originating from computer blade 401 may also be copied onto the storage medium of computer blade 405 .
- the peripheral switch may also be operable to switch the peripheral device over to computer blade 405 in the event that both computer blades 401 and 403 fail.
- a backup processor may be switched over to the computer blade 401 to access and run off of the storage medium on the computer blade 401 .
- information from a backup computer blade may be copied over to a backup storage medium, and the components of the computer blade 401 may access the backup storage medium.
- the information on the storage medium of computer blade 403 may be copied onto the storage mediums of computer blade 401 and 405 .
- the information on the storage medium of computer 405 may be copied onto the storage mediums of computer blade 401 and 403 , as shown in FIG. 4 c .
- various of the computer blades may provide backup storage for one another.
- the computer blade 401 may detect and determine identities of one or more local computer blades, such as, but not limited to computer blades 403 and 405 , on a subnet. Once detected, the computer blade identities may be stored and cached for later access on the computer blade 401 .
- the detected computer blades to be used in backing up a computer blade may be assigned automatically or manually. In one embodiment, e.g., through an administration interface, computer blades may be assigned to each other at any time.
- a “replication factor” configuration value may define how many other computer blades to send copies of the information on the computer blade to be backed up.
- the computer blade 405 may detect the computer blades 401 and 403 and then choose the computer blade 401 and the computer blade 403 as computer blades to send backups to. If the computer blade 405 has a fail-over condition, either the computer blade 401 or the computer blade 403 may be used to restore information to the computer blade 405 , or used to send the information originating from the computer blade 405 to a replacement computer blade.
- the computer blades in one subnet may be backed up to computer blades on another subnet.
- the computer blades in one subnet may be able to detect the computer blades in another subnet, and then the configuration of which computer blades are used to back up other computer blades between the two subnets may be performed, e.g., by an administrator or automatically.
- a web based configuration interface may allow computer blades to be assigned or reassigned to remote computer blades, including blades on other subnets.
- Computer blades may backup with neighboring computer blades, computer blades in a same building, and/or computer blades in a remote location.
- computer blades may be backed up to both geographically local computer blades and geographically remote computer blades. The local computer blades may provide quicker restoration on a replacement computer blade and remote computer blades may provide increased security and reliability.
- backed up information can be copied to a replacement computer blade from the local computer blade without having to copy information from the remote computer blade. Copying information from the remote computer blade may take longer than from the local computer blade. If multiple computer blades at a site have a fail-over condition, (e.g., if both the first computer blade and the local computer blade with the backup fail) the remote computer blade may have a copy of the information from the first computer blade to copy to a replacement computer blade.
- several local computer blades may backup to a single remote computer blade.
- the local computer blades may also be backed up to other local computer blades. Having one remote computer blade to backup to may make the remote backups more manageable.
- the single remote computer blade handling multiple backups may be more powerful than a remote computer blade handling one backup.
- the remote computer blade may be managed through a web-based management interface. In one embodiment, the web-based management interface may be used by an administrator to schedule snapshots and manage configuration settings.
- the information on computer blade 401 may be backed up in a snapshot method in which all of the information to be copied is bulk copied at specified refresh times, where refresh times may indicate particular times to refresh (e.g., every hour on the hour, once per day at 3:00 a.m., etc.), or may indicate intervals between successive refreshes.
- a refresh time may be an hour such that the information is bulk copied from the first storage medium onto the second and/or third storage medium once per hour.
- a delta method may be used to copy the information from the first storage medium, where only a difference between a previous copy of the information copied to the second and/or third storage medium and the current information on the first storage medium is added to the second and/or third storage medium.
- the delta method may take less time to update but it is conceivable that space required on the storage medium of computer blade 403 may grow to very large proportions because the delta method may keep adding information without removing deleted information.
- the first time the delta method is used it may function analogously to the snapshot method because the initial copy may encompass all the information from the storage medium on the computer blade 401 .
- the growth of archived data is automatically managed by the “aging” feature which removes archives or deltas older than a user-defined time period.
- the computer blade 401 may continue to backup to the computer blade 403 . If the computer blade 401 is using a delta method, incremental backups on a computer blade may be tagged and archived. If the computer blade 405 becomes functional again, a peersync method may be used to update the last incremental backups from the computer blade 401 to the computer blade 405 . For example, if the computer blade 401 backs up information from the computer blade 401 onto the computer blade 405 using a delta method, the computer blade 405 may have missed several incremental backups (i.e.
- the computer blade 401 may send the computer blade 405 a catalog of past incremental backups.
- the computer blade 405 may compare the catalog to the incremental backups the computer blade 405 currently has and then query the computer blade 401 for the incremental backups the computer blade 405 needs to become current.
- an archive of the past incremental backups may be sent along with the catalog, and the computer blade 405 may not query the computer blade 401 for the incremental backups. Instead, the computer blade 405 may pull the needed incremental backups from the archive.
- information being written to the storage medium of the computer blade 401 may also be written to the computer blade 403 at substantially the same time. In other words, rather than backing up the information after it has been written to computer blade 401 , the information writes may be performed effectively in parallel, or at least in conjunction.
- the peripheral switch may switch the peripheral device over to the computer blade 403 .
- the information on the storage medium on computer blade 403 (which may mimic or replicate the information on the storage medium of the failed computer blade 401 ) may be copied onto the computer blade 405 .
- the peripheral switch may switch the peripheral device from the computer blade 401 over to the computer blade 405 .
- computer blade 405 is used as the replacement computer blade, and so the backed-up information is copied to computer blade 405 and the peripheral device switched from the failed computer blade 401 to the replacement computer blade 405 .
- the hard drives on the computer blades 401 , 403 , and 405 may share memory space using a virtual network storage space (VNAS) system incorporating a decentralized peer-to-peer sharing process.
- VNAS virtual network storage space
- Information stored on the computer blade 401 may also be stored on computer blades 403 and 405 . If the computer blade 401 fails, a peripheral switch may switch a peripheral device from computer blade 401 over to computer blade 403 . For example, computer blade 403 may then access the information originally stored on or associated with the computer blade 401 from the storage medium of computer blade 403 and the third storage medium of computer blade 405 .
- the information originally stored on the failed computer blade 401 may be distributed over the computer blades 403 and 405 , but may be accessible (to replacement computer blade 403 ) as if stored on a single (virtual) storage medium.
- the (backed-up) information stored on the computer blades 403 and the 405 may be organized (e.g., copied) onto the replacement computer blade 403 to have the information from the failed computer blade 401 collected onto one computer.
- the peripheral switch may then switch the peripheral device from the first computer over to the computer with the organized or collected copy.
- failure management software may execute to redirect information reads and information writes directed at the failed hard drive to an operating hard drive until a replacement computer or replacement hard drive is brought into service.
- the backup copy of the lost information may already be on a replacement disk.
- the entire process is transparent to the user, i.e., the user may not notice the failure of the computer blade.
- the computer blades 401 , 403 , and 405 may use failure information backup in a virtual network attached storage (VNAS) system.
- VNAS virtual network attached storage
- the information may exist in multiple locations across the VNAS system composed of computer blades 401 , 403 , and 405 , such that an occurrence of a failed computer blade 403 does not result in the loss of vital information from the failed computer blade 403 .
- Other information backup strategies may also be within the scope of the invention to ensure information redundancy. For example, other RAID (Redundant Array of Inexpensive Disks) levels may be used.
- an underlying distributed computer infrastructure may be used to distribute resources among the computer blades. DCI is further described with reference to FIGS. 23 and 24 .
- Each computer blade may be assigned a number of “peer” or neighbor computer blades that may be used to backup information from the storage medium of a computer blade.
- “Peer” computer blades such as, but not limited to, computer blades 403 and computer blade 405 , may be assigned to a nearby computer blade 401 .
- computer blades may be backed up onto computer blades at a remote location. For example, multiple groups of computer blades at multiple locations may be backed up to a one or more central locations, such as, but not limited to disaster recovery centers, with replacement computer blades.
- backups to the disaster recovery center may be scheduled so that multiple groups of computer blades can coordinate their backups.
- a disaster recovery system may provide a central location for a group of moving computer blades to use for initialization. For example, several moving field hospitals using computer blades with the same set of programs and user preferences may initialize their computer blades from a central location of computer blades.
- FIG. 5 Restoring a Failed Computer Blade Onto a Backup Computer Blade
- an embodiment of restoring or rebuilding a failed computer blade's functionality (e.g., computer blade 403 ) onto a backup computer blade (e.g., computer blade 501 ) is shown.
- the failed computer blade 403 may have a fail-over condition such as a fatal hard drive crash or a short-circuit on the motherboard 207 .
- rebuilding a desktop experience may include identifying a replacement computer blade 501 to use, switching via the connector cable 153 (shown in FIG. 1 ) to the user's desktop connector, and loading failed computer blade information from either the first computer blade 401 or (assuming a previous backup to computer blade 405 ) from third computer blade 405 .
- the fail-over condition of computer blade 403 may be detected and signaled automatically by computer blade 403 , or by computer blades 401 and/or 405 . Other signals and other sources may also be within the scope of the invention.
- the information originating from the computer blade 401 may include user preferences. Including the user preferences with the information to be copied to another computer blade 403 may enable a move manager application and/or a switch manager application (or equivalent) to seamlessly provide a replacement computer blade (e.g., computer blade 501 ) with a similar look, feel, and functionality as a computer blade that has a fail-over condition.
- the move manager as discussed below, may implement the transfer of information from one computer blade to another computer blade.
- the switch manager application may implement switching a first peripheral from a first computer blade to a replacement computer blade.
- the fail-over condition of computer blade 403 may be signaled manually, such as by a user calling a system administrator.
- reconnecting a user's peripheral devices e.g., keyboard 123 or 129 (see FIG. 1 ), mouse 125 or 131 , and monitor 127 or 133 , may include identifying replacement computer blade 501 , loading the failed computer blade 403 information onto the replacement computer blade 501 from either the first computer blade 401 or the third computer blade 405 , and establishing a connection between the user's peripheral devices and the replacement computer blade 501 , such as via a soft switch (not shown).
- a replacement computer blade 501 may have the standard operating system and applications already stored on it.
- the peripheral device for the user's computer blade may be switched over to the replacement computer blade and the user may begin using the applications already stored on the replacement computer blade.
- Backup information may be restored to the replacement computer blade in the background, and while the user uses applications already stored on the replacement computer blade, writes the user performs may be diverted to the replacement computer blade.
- first computer 401 may have a first processor and a first storage medium coupled to the first processor.
- the first storage medium may be a medium including but not limited to a random access memory and a hard disk drive.
- Second computer 403 may have a second processor and a second storage medium coupled to the second processor.
- a communication link e.g., a bus (not shown), may couple the first computer 401 to the second computer 403 .
- a peripheral switch may be coupled to the first computer 401 and the second computer 403 and may route signals from a human interface, such as but not limited to a first peripheral device coupled to the first computer 401 , to the first computer 401 through a peripheral device port coupled to the first computer 401 .
- the second storage medium on the second computer 403 may store program instructions executable by the second processor to detect a fail-over condition of the first computer 401 and copy information from the first storage medium onto the second storage medium, thereby making the information accessible by the second processor. Part of or substantially all of the information on the first storage medium may be copied onto the second storage medium.
- the peripheral switch may be configured to route signals between the first peripheral device and the second computer to make the second processor and the second storage device accessible by the first peripheral device.
- copying information from the first storage medium to the second storage medium and routing signals from the first peripheral device to the second processor may occur without user input to trigger the copying and routing, i.e., the data fail-over process may be performed programmatically.
- the copying and re-routing may be transparent to a user of the first computer 401 .
- the peripheral switch may switch the signals from the first peripheral device targeted to the first computer 401 over to the second computer 403 without a user input to trigger the switching.
- the first processor may simply access the second storage medium (e.g., of the second computer 403 ) instead of switching the first peripheral device to the second processor (e.g., the second computer blade 403 ).
- the second processor may simply access the first storage medium and the first peripheral switch may be switched over to the second processor.
- the storage medium on the second computer 403 may store program instructions executable by the second processor to copy information from the first storage medium onto the second storage medium to make the information accessible by the second processor before the second processor detects a fail-over condition of the first computer 401 .
- Part of or substantially all of the information on the first storage medium may be copied onto the second storage medium, where the information may be stored (and optionally updated) until a fail-over condition (of the first computer 401 ) is detected.
- the information may be stored on additional storage mediums, e.g., in case the second computer also has a failure, e.g., a hard disk crash.
- the information from the first storage medium may be repeatedly copied onto the second storage medium to keep the information on the second storage medium current with the current state of the first storage medium.
- the information on the first storage medium may be copied over the previously copied information from the first storage medium on the second storage medium.
- the information from the first storage medium may be initially copied onto the second storage medium, and then subsequent changes to the information on the first storage medium may be stored onto the second storage medium over time. The information on the first storage medium may then be reconstructed using the initial copy stored and the subsequent changes. Periodically, the initial copy and subsequent changes may be overwritten with a new copy of the information on the first storage medium, e.g., to prevent the stored subsequent changes from filling up the second storage medium.
- the first storage medium may also use space on the second storage medium to store information not stored on the first storage medium, e.g., may use the space for purposes other than backup.
- the first computer 401 may keep a record of the location of the extra information stored on the second storage medium.
- the information from the first computer 401 may also be stored on other storage mediums, e.g., on other computers. For example, if the first storage medium is running out of space, it may use space on the second storage medium or other storage mediums to save information to, thus using other computers' storage mediums for overflow.
- the information on or originating from the first storage medium may be striped onto other storage mediums.
- information from the second storage medium may be copied onto the first storage medium and/or other storage mediums in case the second computer fails or the second computer needs additional space to store its information.
- a third computer 405 including a third processor and a third storage medium coupled to the third processor may also be coupled to the communication link.
- the third storage medium may store program instructions executable by the third processor to copy (i.e., backup) information from the first storage medium onto the third storage medium.
- the program instructions may be further executable by the third processor to detect a fail-over condition of the first computer 401 , and to copy information from the third storage medium to the second storage medium so that the information is accessible by the second processor.
- the peripheral switch may then operate to route signals between the first peripheral device and the second processor if the first computer 401 fails.
- the third storage medium may not be comprised in the third computer, e.g., computer 405 , but may be coupled to the communication link as a substantially independent networked resource.
- the second computer 403 may copy information from the first storage medium (e.g., from the first computer 401 ) onto the third storage medium.
- the second computer 403 may backup information from the first computer 401 onto the third storage medium.
- information from the third storage medium (that was originally on the first computer 401 ) may be copied to the second storage medium to make the information accessible by the second processor, i.e., by the replacement computer 403 .
- the peripheral switch may then route the signals from the first peripheral device to the second computer 403 .
- FIGS. 6 - 8 Flowcharts of Methods For Responding to a Fail-Over Condition
- FIGS. 6 through 8 are flowchart diagrams of various methods for responding to a detected fail-over condition.
- a first computer including a first processor and a first storage medium coupled to the first processor, couples to a first peripheral device via a peripheral switch, where the peripheral switch may be configured to route signals between the first peripheral device and the first computer.
- a second computer including a second processor and a second storage medium coupled to the second processor, may be coupled to the first computer via a communication link, as described above.
- the storage medium of the first and/or the second computer may store program instructions executable by the first and/or second processor to implement various embodiments of the methods described below.
- other computers may also perform all or portions of the methods described herein. It should be noted that in various embodiments of the methods described below, one or more of the steps described may be performed concurrently, in a different order than shown, or may be omitted entirely. Other additional steps may also be performed as desired.
- a fail-over condition for the first computer may be detected, for example, regarding the first processor or the first storage medium, although the fail-over condition may relate to any other components or subsystems of the first computer.
- the fail-over condition may indicate a failure of the respective component, or may indicate that such a failure is likely or imminent, although in this particular embodiment, the fail-over condition does not include an actual failure of the first storage medium, since information is copied therefrom after detection of the fail-over condition.
- the detection process generally involves some type of monitoring of the first computer.
- various metrics may be determined that indicate a fail-over condition, such as, for example, read/write error rates, operating temperatures, and so forth, and these metrics may be monitored to detect the fail-over condition.
- the monitoring process may be performed by the first computer itself, or by any other computers coupled to the first computer, as desired.
- information stored on the first storage medium may be copied onto the second storage medium, thereby making the information accessible by the second processor.
- the second computer may access the first storage medium and copy at least a portion of its contents to the second storage medium, after which the second processor, i.e., the second computer, may access the information.
- the second computer may access the first storage medium and attempt to copy the desired portion of its contents to the second storage medium, after which the second processor may access the information, although it is noted that in some cases the storage medium may fail before all of the desired information has been copied.
- the peripheral switch may be configured to route signals between the first peripheral device and the second computer.
- the first peripheral device may then access the second processor and the second storage medium.
- a plurality of peripheral devices i.e., human interface devices composing a human interface, are coupled to the first computer, and are subsequently switched by the peripheral switch to the second computer.
- the peripheral switch may operate to switch the entire human interface of the first computer to the second computer.
- the information stored on the first computer may be copied to the replacement computer (i.e., the second computer) after detection of the fail-over condition. It is noted that this approach may make transparency of the process to the user problematic, in that there may be a noticeable delay between the detection of the fail-over condition and resumption of operations using the replacement computer.
- actions are taken prior to detection of the fail-over condition that may significantly improve, i.e., decrease, the time it takes to resume user operations with the replacement computer.
- the expected downtime for the user may be substantially reduced.
- information from the first storage medium may be copied onto the second storage medium to make the information from the first storage medium accessible by the second storage processor.
- information stored on the first storage medium may be backed-up onto the second storage medium.
- this backup operation may be performed by the first computer, by the second computer, or by another computer, e.g., the third computer.
- a fail-over condition may be detected for the first computer, e.g., related to the first processor and/or the first storage medium (or any other component of the first computer). It is noted that in various embodiments, the detection of the fail-over condition may be performed by various of the computers in the system, e.g., by the first, second, and/or third computers.
- the peripheral switch may operate to route signals between the first peripheral device and the second computer.
- the first peripheral device may then access the second processor and the second storage medium.
- the peripheral switch may switch the human interface of the first computer over to the second computer, as described above.
- the switchover from the first computer to the replacement computer may occur substantially immediately, i.e., with minimum delay.
- transparency to the user of the switchover process may not be difficult to achieve.
- the third computer including the third processor and third storage medium is coupled to the first and second computers via communication link.
- information from the first storage medium may be copied onto the third storage medium.
- information stored on the first storage medium may be backed-up onto the third storage medium.
- this backup operation may be performed by the first computer, second computer, the third computer, or yet another computer included in the system.
- a fail-over condition may be detected for the first computer, e.g., related to the first processor and/or the first storage medium (or any other component of the first computer).
- the detection of the fail-over condition may be performed by various of the computers in the system, e.g., by the first, second, and/or third computers (or others).
- the information from the first storage medium may be copied from the third storage medium onto the second computer, i.e., onto the second storage medium, to make the information accessible by the second processor.
- the information may be copied from the backup computer (the third computer) to the replacement computer (the second computer).
- the peripheral switch may be configured to route signals between the first peripheral device and the second computer (the replacement computer).
- the first peripheral device may then access the second processor and the second storage medium (and may be accessed by the second processor).
- this embodiment includes backing up the information stored on the first computer (to the third computer) prior to detection of the fail-over condition, and thus may facilitate transparency of the process from the user's perspective.
- a resource manager may be operable to manage the VNAS system in one embodiment.
- the resource manager may be operable to manage the plurality of computers and associated peripheral devices.
- the resource manager may be located on one of the computer blades.
- a copy of the resource manager may operate on each of the computer blades.
- the resource manager may be distributed across the plurality of the computer blades. In each of these embodiments, the resource manager, or resource managers, may operate to schedule efficient information storage among the plurality of computer blades, e.g., computer blades 401 , 403 , and 405 .
- the resource manager may operate to monitor resource usage for each of the plurality of computers.
- the resource manager may monitor performance metrics for each computer such as a total memory size, a used memory size, a virtual memory size, peripheral type, available ports, processor type, processor speed, type of installed applications, whether a user is logged in, frequency of login ins, percentage of usage of CPU, percentage of usage of hard disks, network hardware installed, network usage, usage of installed applications, video specifications, usage of CD-ROM, a variable imparted by the operating system, and a variable imparted by the Basic Input/Output System (BIOS), among others.
- BIOS Basic Input/Output System
- the resource manager may function both as an analyzer and a controller for the system.
- the resource manager may utilize information about the performance and use patterns of each of the plurality of computers. Based on the performance and use patterns, the resource manager may compute demand and usage metrics or issues (e.g., processor time, memory usage and demand, hard drive memory, and network information). The resource manager may also generate reports on applications and links used by the system. These patterns may be used to generate a map of the demands on the system's collective resources over time. The continually updated map may be used by the system administrator and/or the resource manager in order to perform predictive and proactive scheduling of resources to users. Other uses of the map may also be within the scope of the invention.
- demand and usage metrics or issues e.g., processor time, memory usage and demand, hard drive memory, and network information.
- the resource manager may also generate reports on applications and links used by the system. These patterns may be used to generate a map of the demands on the system's collective resources over time.
- the continually updated map may be used by the system administrator and/or the
- FIGS. 9 - 14 Screen Shot Embodiments of Various Interfaces
- software used to manage functions such as, but not limited to, assignment of computer blades to other computer blades for backup, detecting a fail-over condition in a computer blade, and managing the copy process from a backup computer blade to a replacement computer blade, may be installed and managed using a graphical installation program.
- installation may be performed by a computer blade local administrator.
- the installation program may be restricted to administrator access because the installation program may require system resources to which only the administrator may have access.
- other installers are also contemplated. While various embodiments of screens and interfaces are shown, it noted that other screens and interfaces may also be within the scope of the invention.
- FIG. 9 Screen Shot of a Login Screen
- FIG. 9 illustrates an embodiment of a login screen 901 of a management console, e.g., a web-based management interface.
- the login screen 901 may appear before a management session is started.
- a user may need domain administration rights.
- a login may be validated through a primary domain controller.
- a username 903 and password 905 may be registered with a network to validate users on the management console.
- the user may supply a username 903 , password 905 , and a domain name 907 , although other user inputs are also contemplated.
- the user may be automatically redirected to a management console main menu or screen, described below.
- FIG. 10 Screen Shot of an Auto-Discovery Screen
- FIG. 10 illustrates an embodiment of a main screen in a web-based management interface such as may be used in various embodiments of the resource manager.
- the main screen shown FIG. 10 illustrates an auto-discovery screen 1001 showing a list 1005 of computer blades on a local subnet.
- clicking on a name of a computer blade may load the management interface with information about that computer blade.
- an indicator 1003 in the top left hand corner of the main screen displays the name of the computer blade to which the administrator is currently connected (e.g., a computer blade named swlab1).
- management software is first installed on a computer blade, the administrator or user may click ‘Discover Nodes’ 1007 , and if no computer blade names appear, the administrator or user may click “Re-discover” 1009 to get a list of computer blade names.
- FIG. 11 Screen shot of a Configuration Screen
- FIG. 11 illustrates an embodiment of a configuration screen 1101 .
- a parameter indicating a location of a configuration file for a backup computer may have a standard location on that computer (e.g., the administrator may not need to determine the location of each configuration file on each computer when performing data fail-over).
- a root directory 1105 may list the location on a storage medium that may be considered as a starting point for a backup process, including, but not limited to, archiving.
- a subdirectory may be included in the root directory 1105 for the backup process.
- an optimum setting for the parameter may point to or indicate the location of preferences file (e.g.
- a replication factor 1111 may define how many peers to send local information to and may be used to automatically assign the appropriate number of other computer blades to the local computer blade.
- a directory exclusion list 1107 may list the names of directories that are not to be included in the backup process, even if they are subdirectories of the root directory. The directory exclusion list 1107 may explicitly exclude system or binary folders that may be unnecessary to backup.
- File exclusion 1109 may indicate a comma-delimited list of extensions (e.g., .exe, mpg, etc.) that may not be backed up. The comma-delimited list may include .mp3 or other rich media files that may not be important enough to warrant backup.
- FIG. 12 Screen Shot of a Scheduling Interface
- FIG. 12 illustrates an embodiment of a screen for a scheduling interface 1201 .
- a time-of-day drop down list 1203 and a frequency drop-down list 1205 may allow a time and frequency to be scheduled.
- An added time in the time-of-day drop down list 1203 may be added or removed.
- a “weekly” frequency schedule may also be entered. For example, if “6 am, weekly” is entered into the “weekly” frequency, the backup may be taken once a week at 6 am. Frequent backups may cause increased network activity. However, in one embodiment, if a delta method of backup is used, only files changed from the last backup may be archived and sent. In addition, in environments where files are not changing very frequently, very frequent snapshots may not be needed.
- FIG. 13 Screen Shot of an Information Recovery View
- FIG. 13 illustrates an embodiment of an information recovery view 1301 .
- three archives each representing a file or set of files in a backup, may be listed in archive contents listings 1303 , 1305 , and 1307 , along with sizes 1308 , names 1304 , and time of each archive 1306 .
- a “View Contents” button 1309 , 1313 , and 1315 may be placed next to each entry.
- the “View Contents” button 1309 , 1313 , and 1315 may be clicked to view contents of each archive.
- a next button may control sequential restoration.
- each archive may represent changes between two points in time (e.g., an archive created at 12 p.m.
- the backups may represent changes that occurred to files between the time of a last backup at 10 p.m. and 12 p.m.).
- the backups may be used to restore a computer blade to the last archived state of the computer blade, or the backups may be used for controlled rollbacks to earlier versions.
- the backups may be used for controlled rollbacks if addition of some information to the backup resulted in the corruption of important files or documents.
- the button when the “Recover Sequentially to this archive” button 1317 , 1319 , and 1321 is clicked for a particular archive in a backup, the button may cause a restoration of each archive up to and including the selected archive. For example, if archives A, B, C, D, E, and F, are displayed (not shown), clicking on archive D may result in A, B, C and D being restored to a replacement computer blade. However, if there are additional archives, E and F, they will not be restored. The administrator may return to this view and choose further restoration for E, or both E and F. In addition, in one embodiment, the restoration to a replacement computer blade may be non-destructive.
- the restoration may overwrite existing files when their names clash with files present in a restored archive
- the restoration may not delete files that do not exist at all in the restored archives. For example, if files X, Y and Z are present in a next archive to restore to a replacement computer blade in a sequential restoration, and prior to restoring the next archive, older versions of X and Y, and a completely different file, W, have already been copied over to the replacement computer blade from previous archives, the restore process may overwrite files X and Y with archived versions of X and Y, may create a new file Z, and may not alter file W.
- the button when the “Recover Sequentially to this archive” button 1317 , 1319 , and 1321 is clicked for a particular archive in a backup, the button may cause a restoration of each archive up to and including the selected archive. For example, if archives A, B, C, D, E, and F, are displayed (not shown), clicking on archive D may result in A, B, C and D being restored to a replacement computer blade. However, if there are additional archives, E and F, they will not be restored. The administrator may return to this view and choose further restoration for E, or both E and F. In addition, in one embodiment, the restoration to a replacement computer blade may be non-destructive.
- the restoration may overwrite existing files when their names clash with files present in a restored archive
- the restoration may not delete files that do not exist at all in the restored archives. For example, if files X, Y and Z are present in a next archive to restore to a replacement computer blade in a sequential restoration, and prior to restoring the next archive, older versions of X and Y, and a completely different file, W, have already been copied over to the replacement computer blade from previous archives, the restore process may overwrite files X and Y with archived versions of X and Y, may create a new file Z, and may not alter file W.
- files may be replicated throughout the network at various storage devices that participate in the VNAS cluster. If one of the underlying VNAS storage devices fails, any requests for data on the failed device may be redirected to a functioning machine having a copy of the requested data. This redirection may be handled in a way that is transparent to the user. Thus, the information recovery view 1301 may be little-used in many circumstances where VNAS is utilized and the replication factor is sufficient to provide adequate copies of data.
- FIG. 14 Screen Shot of an Archive View Screen
- FIG. 14 illustrates an embodiment of an archive view screen 1401 .
- each file 1403 , each file size 1405 , and each file's date of creation 1407 may be listed.
- Checkboxes, such as checkbox 1409 may be selected for partial restoration of an archive. Corrupted or accidentally deleted information from backups may be selected and restored.
- FIG. 15 Memory Stack for a Computer Blade
- FIG. 15 an embodiment of a memory stack 1501 for a computer blade storing information from other computer blades is shown.
- the user's computer blade e.g., computer blade 403
- two additional computer blades e.g., computer blades 401 and 405
- the memory spaces used by the blades include memory spaces 1503 , 1505 , and 1507 , although in other embodiments, other memory spaces may be defined and used.
- FIG. 15 indicates, there may be additional memory space 1509 available for use by a virtual network attached storage (VNAS) system 1509 .
- VNAS virtual network attached storage
- a storage network with a storage area network server may be coupled to the computer blade 401 and 405 .
- the storage network server may make the storage medium of computer blade 401 accessible by the processor of the computer blade 405 , and to make the storage medium of the computer blade 405 accessible by the processor of the computer blade 401 .
- the organization and manipulation of the user's computer blade memory space may be such that the blade memory space does not have a single point of failure, as described below in detail. By eliminating single points of failure, the computer blades 401 , 403 , and 405 together may be more reliable for use in such applications as e-commerce, trading floors, and repair call centers, among others.
- each computer blade 401 , 403 , and 405 may have some file server functionality.
- the file server functionality may include some I/O capabilities, and may also include functionality for negotiating file write and read protocols.
- Communicating with the computer memory on the computer blades 401 , 403 , and 405 may be analogous to communicating with a standard file server attached memory.
- the computer blades 401 , 403 , and 405 may negotiate information writes similar to a file server, and order and prioritize transactions.
- the computer blades 401 , 403 , and 405 may also implement striping analogous to one used by RAID (Redundant Array of Inexpensive Disks).
- a fail-forward hard drive may also utilize Network Attached Storage (NAS) and/or Storage Area Network (SAN) techniques.
- the computer blades 401 , 403 , and 405 may operate as a distributed NAS server.
- the computer blades 401 , 403 , and 405 may utilize unused memory space in a manner analogous to that used by NAS and SAN, and may also track the location of hardware and information in the system.
- a virtual NAS (VNAS) system may be implemented where the NAS server software is distributed across the peer computer blades 401 , 403 , and 405 (and/or other computer blades) in the network, thereby eliminating the NAS server as a point of failure.
- VNAS virtual NAS
- each of the computer blades 401 , 403 , and 405 may maintain a copy of the NAS server software.
- the computer blades 401 , 403 , and 405 may store the NAS server software and may be able to transfer a copy of the software to one of the remainder of the computer blades 401 , 403 , and 405 in the event of a failure of a computer blade 401 , 403 , or 405 .
- the computer blades 401 , 403 , and 405 may also use computer blades 401 , 403 , and 405 (i.e., each other) for other software storage, as desired.
- the VNAS system is described further with respect to FIGS. 24 and 25 .
- failure management software may execute to rebuild the hard drive contents on a replacement hard drive, and replace the failed computer blade in the network with a replacement computer blade.
- the failure management software may route information reads and information writes from and to the failed hard drive to the replacement computer blade such that the user may not be aware of a failure.
- the failure management software may execute on a central management server, optionally with a backup server in case of failure, although this approach may still present critical points of failure.
- the failure management software may be distributed over the computer blades 401 , 403 , and 405 , such that the entire storage management system is distributed, i.e., decentralized to eliminate single points of failure.
- the computer blades 401 , 403 , and 405 may not need a central server.
- the systems and methods described herein may be used to augment an existing NAS and SAN distributed hard drive system.
- the VNAS system may implement an algorithm for a data fail-over system.
- the VNAS system may be operable to couple computer blades 401 , 403 , and 405 to the VNAS system, and to configure a resource manager.
- the computer blades 401 , 403 , and 405 may also be coupled to an enterprise network.
- the resource manager may be operable to manage the VNAS system, including information writing and striping protocols.
- the resource manager may be located on one of the computer blades 401 , 403 , and 405 coupled to the VNAS system. In another embodiment, a copy of the resource manager may operate on each of the computer blades.
- the resource manager may be distributed across the plurality of the computer blades, e.g., computer blades 401 , 403 , and 405 .
- the information and other configuration information may be saved across computer blades 401 , 403 , and 405 in the VNAS system by the resource manager.
- Each computer blade 401 , 403 , and 405 coupled to the VNAS system may be involved in storing the information for the other computer blades 401 , 403 , and 405 .
- the VNAS system may check if the computer blades 401 , 403 , and 405 in the VNAS system are functioning properly, and if the VNAS system determines that one of the computer blades 401 , 403 , or 405 has failed, may provide a replacement computer blade 501 , as described above.
- the vital information on the failed computer blade 401 , 403 , and 405 may have been distributed across the computer blades 401 , 403 , and 405 in the VNAS system prior to the fail-over condition.
- the VNAS system may thus access the computer blades 401 , 403 , and 405 in the VNAS system to retrieve the vital information for the replacement computer blade 501 .
- the computer blades 401 , 403 , and 405 may schedule efficient information storage among themselves, e.g., over respective others of the computer blades 401 , 403 , and 405 .
- FIG. 16 Memory Stack with a PC Region and a Server Region
- each computer blade 1601 may have some server functionality.
- the server functionality may include some I/O functionality and the ability to negotiate file write and read rules, as mentioned above.
- the computer blade 1601 may negotiate writes similar to a file server, and order and prioritize transactions.
- the computer blade 1601 may also be coupled to an enterprise network 1609 and a VSAN network 1607 .
- server functionality for the system may effectively be distributed over the plurality of computer blades, thereby removing any single points of failure associated with the user of a central server.
- FIG. 17 Enterprise Switch and Storage Network Switch
- an embodiment of an enterprise network switch 1709 and a storage network switch 1707 controlling attached PCs 1701 , 1703 , and 1705 which in one embodiment, may be computer blades.
- a Network Attached Storage (NAS) device may be a group of hard disk drives that connect to a network, such as but not limited to an Ethernet.
- the NAS device may function like a server to implement file sharing.
- the NAS may allow more hard disk storage space to be added to the network without shutting down attached servers for maintenance and upgrades.
- a Storage Area Network may be a network of shared storage devices. The SAN may make the storage devices coupled to the SAN available to servers coupled to the SAN. As more storage devices are added to the SAN, the additional storage devices may be accessible from any server in the SAN.
- the NAS or the SAN may consist of multiple hard disks in a box with a system to serve the information out onto the network.
- the NAS or the SAN may use a central or limited distribution control and management node, e.g., a server, to keep track of file locations and to distribute files for storage.
- the computer blades 1701 , 1703 , 1705 may function as the server to form a VNAS environment 1711 .
- the computer blades 1701 , 1703 , and 1705 may negotiate file write rules, file reads, and order and prioritize transactions.
- Storage mediums on the computer blades 1701 , 1703 , and 1705 may function as a standard server attached memory.
- the computer blades 1701 , 1703 , and 1705 may have an internal index of files in the form of a location file stored on other computer blades 1701 , 1703 , and 1705 .
- the location file may indicate where information from various computer blades have been stored on other computer blades.
- the computer blades 1701 , 1703 , and 1705 may also store striping and write rules.
- Each file stored in the VNAS 1711 may have different striping rules that may be determined by the nature of the file and the expectations of a system administrator.
- the VNAS 1711 may use a transponder Routing Information Protocol (RIP) to disseminate files on the computer blades 1701 , 1703 , and 1705 .
- RIP transponder Routing Information Protocol
- the RIP may be a protocol defined by RFC 4038 that specifies how routers exchange routing table information, although other protocols may also be within the scope of the invention.
- computer blades 1701 , 1703 , and 1705 may periodically exchange entire routing tables.
- the RIP may broadcast the name, index, and rules for a memory domain of the computer blades 1701 , 1703 , and 1705 , where, for example, the broadcasts may occur in response to a change in the index, or to a lapse of a specified time period.
- the files may be moved to reorganize the storage space or moved based on the frequency of use.
- the file may be moved to a “closer” computer blade in which there are fewer intervening switches, e.g., Ethernet links, between the file and a user of the file.
- the computer blades 1701 , 1703 , and 1705 may be operable to query an index for a specific (instant) update.
- a computer blade e.g., computer blade 1701
- the computer blade 1701 may search the computer blade's internal index.
- the computer blade 1701 may also send a query to another computer blade, e.g., computer blade 1703 , that may be listed as a primary source of the file in the internal index.
- the computer blade 1701 may then access the file. If multiple computer blades attempt to access the file at the same time, the computer blade with the file may negotiate a multiple transaction session. After the computer blade 1701 accesses the file, the computer blade 1703 with the file may perform a backup according to read/write rules stored on the VNAS.
- the resource manager may function both as an analyzer and a controller when accessing the entire VNAS system 1711 .
- the resource manager may utilize information about the performance and use patterns of the entire VNAS system 1711 .
- the resource manager may compute demand and usage metrics or issues (e.g., processor time, memory usage and demand, hard drive memory, and network information) as well as generate reports on the applications and links used. These patterns may be used to generate a map of the demands on the system's collective resources over time.
- the continually updated map may be used by the system administrator and/or the resource manager in order to perform predictive and proactive scheduling of resources to users. Other uses of the map may also be within the scope of the invention.
- the user of the first computer may not notice any downtime.
- the fail-over process may be transparent to the user.
- the hard drives on the second and third computers may be the storage medium for the user through the VNAS so that a replacement processor only has to access the already copied information from the user's computer.
- the failed hard drive on the user's computer may be replaced with a new hard drive.
- the new hard drive may be brought into the computer system, i.e., the user's computer, independently and without intervention of the user.
- VNAS when the participating computers in a VNAS cluster are NAS servers, VNAS allows new storage capacity, in the form of a new NAS server to be added, or existing capacity to be removed, without affecting the uptime of the VNAS volume.
- VNAS running on NAS servers provides SAN level capabilities in the area of zero downtime while adding or removing storage, without any hardware modifications to existing NAS products.
- the computer blades may be additionally coupled to an external RAID system.
- the coupling to an external RAID system may give the computer blades more redundancy and reliability.
- the computer blades may also be coupled to separate NAS and SAN storage networks.
- a distributed VNAS storage management system may minimize or eliminate points of failure in the networked distributed computer system. At least a portion of the VNAS server software and the failure management software may be distributed over the computers in the network, reducing or removing central servers as a point of failure.
- the distributed computer system may include a plurality of centrally located computers with respective human interfaces located remotely from the computers.
- FIGS. 18 - 21 Move Manager Embodiments
- a move manager may provide the ability to undertake individual, workgroup, or department-level hardware moves. Move manager may be used when a fail-over condition is detected on a computer blade to restore the computer blade to a replacement computer blade. Move manager may also migrate an operating system, applications, and information from a user's old computer blade to a new computer blade without detecting a fail-over condition. In one embodiment, move manager may provide the ability to schedule moves to prevent network saturation during peak work hours.
- a scriptable interface may allow an administrator to control a move process and insert custom tasks or activities they would like to execute prior to initiating the move process, or upon the completion of a move. The move manager may also allow a system administrator to use a scriptable interface to add custom tasks to a move process.
- the move process may include one or more of a one-to-one move, a cascading move, and a swap move, as discussed below.
- FIG. 18 A One-to-One Move
- FIG. 18 illustrates a one-to-one move, according to two embodiments.
- a single user of an existing computer e.g., an old computer blade 1801
- a user of a legacy Whitebox PC 1803 may be moved or switched to new computer blade 1807 .
- the user may be moved for a hardware upgrade or unreliable performance of existing hardware.
- User preferences (such as desktop settings and icons) may be combined with other information and transferred over a network to the new computer blades 1805 and 1807 as a series of backup files (e.g., collected in archives or as “snapshots”).
- the backup files may be compressed packages for network transmission.
- a delta backup method as discussed above, may be used.
- a first computer may be switched to the second computer in a one to one move by copying at least a portion of the information from the first computer to the second computer and switching the first peripheral device over to the second computer using the peripheral switch.
- FIG. 19 Clicking Move
- a cascade move may be performed. For example, if a new powerful computer is added to the network, multiple users may be upgraded to computers more powerful than their currently assigned machines, e.g., based on computation needs, seniority, etc., where, for example, user A gets moved to the new computer, user B gets moved to user A's old computer, user C gets moved to user B's old computer, and so on.
- the information from the first computer may be copied to the second computer (user A's old computer) while the information from the second computer (user A's old computer) is copied onto a third computer (the new computer).
- a peripheral switch may switch the first peripheral (i.e., user B's human interface) over to the second computer and may switch the second peripheral (i.e., user A's human interface) over to the third computer.
- Other switches may also be within the scope of the invention.
- a single peripheral switch may provide switching for all of the human interfaces.
- the system may include a peripheral switch for each of a number of subsets of the computers/human interfaces.
- FIG. 19 illustrates an embodiment of a cascading move where pairs of old computers are upgraded, possibly in parallel.
- old computer blades 1903 and 1909 may be moved to new computer blades 1905 and 1911 , respectively.
- Computers older than old computer blades 1903 and 1909 such as computer blade 1901 and legacy Whitebox PC 1907 , may be moved onto the old computer blades 1903 and 1909 , respectively.
- Other computers may also be within the scope of the invention.
- a cascading move may be managed between a first computer, a second computer, and a third computer.
- a copy of at least a portion of the information from the first computer may be copied onto the storage medium of the second computer.
- Information from the second computer may be copied onto a third computer.
- the peripheral switch may route signals from the first peripheral device to the second computer and from the second peripheral device to the third computer.
- a cascading move may be performed for more than three computers.
- FIG. 20 Shown: Move
- a fail-over condition may include an indication of a need to swap the first computer with the second computer, e.g., to improve performance for a user, or to change environments (e.g., from Microsoft Windows to Apple MacOS). For example, if the second computer is a higher performing computer, and the user of the first computer needs more computational power than the user of the second computer, the computers assigned to each user may be swapped. In other words, the first computer (or another computer) may copy the information from the first computer over to the second computer, and the second computer (or another computer) may copy the information from the second computer onto the first computer.
- a peripheral switch may swap the human interfaces for the respective computers, e.g., by routing signals from a first peripheral device (originally routed to the first computer) to the second computer, and from a second peripheral device (originally routed to the second computer) to the first computer.
- FIG. 20 illustrates a swap move, according to one embodiment.
- a swap move may be used to equalize or adjust the use of resources in a network (e.g., to put more demanding users with faster computer blades).
- the computer blades may be switched by two users, such as computer blades 2001 and 2003 .
- Information such as, but not limited to, applications and settings from one computer blade 2001 , may be present on another computer blade 2003 , post move, and vice-versa.
- information from one of the computer blades 2005 and 2007 performing a switch may be stored in a temporary third location to preserve the target computer blade 2007 while the switching computer blade 2005 overwrites the target computer blade's information.
- an intermediate image server 2009 (based on Preboot Execution Environment (PXE) technology) may be used.
- PXE Preboot Execution Environment
- Large-scale moves may also be within the scope of the invention. In moving multiple computer blades, moves may be scheduled for Operating System settings, profiles, applications, and user information from old computer blades to new computer blades.
- a swap move at least a portion of the information from the storage medium of the first computer may be stored onto the storage medium of second computer, and at least a portion of the information from the storage medium of the second computer may be stored onto the storage medium of said first computer.
- the peripheral switch may switch the signal routing from the first peripheral device to the first computer to route to the second computer and the signal routing from the second peripheral device to the second computer to route to the first computer.
- FIG. 21 Flowchart of a Move Process
- FIG. 21 flowcharts an initiation and process of a single computer move, according to one embodiment. It should be noted that in various embodiments of the method described below, one or more of the steps described may be performed concurrently, in a different order than shown, or may be omitted entirely. Other additional steps may also be performed as desired.
- a move may be initiated, e.g., by a system administrator, or programmatically, i.e., automatically.
- the move may be initiated as a result of one or more operation rules, or the system administrator may initiate the move from a move manager web console or other user interface.
- source and target computer blades may be tagged.
- one or more source/target pairs may be specified where information from each source computer is to be moved to the respective target computer, and/or one or more respective peripheral devices are to be switched from each source computer to the respective target computer.
- a move may be scheduled.
- a schedule may be set to activate an image/backup process and move process at night to avoid any network saturation or other inconveniences during the work day.
- the scheduled move may be performed.
- at least a portion of the information from the source computer may be moved to the target computer, and any peripheral devices comprised in the human interface for the source computer may be switched to the target computer.
- any of the various moves described above may follow a similar process.
- FIGS. 22 and 23 Distributed Computing Infrastructure
- intelligent reconfiguration may be conducted for an enterprise-wide computing resources based on observed usage trends for individual computer blades.
- the latest and most powerful hardware is assigned to the newest employee simply as a function of the employee and the hardware arriving at roughly the same time. This may result in a disparate and inequitable assignment of resources across the enterprise.
- An engineer or developer with greater resource requirements could find himself equipped with an old machine, whereas a casual or non-power user could be using a more capable machine.
- the hardware platform shown in FIGS. 1 through 3 due to its dense configuration and increased manageability, may provide a highly effective clustering solution.
- a Distributed Computing Infrastructure adds to these underlying capabilities by providing a software platform for creating, running, and managing distributed applications.
- the DCI software solution includes a framework that provides a variety of management and deployment functions for creating and running large-scale distributed applications.
- DCI may be implemented using a Java-based framework.
- the infrastructure is not dependent on the presence of a server and therefore eliminates a single-point bottleneck that often slows performance of distributed applications.
- the DCI solution is primarily designed to make it easy for domain specialists (biologists, mathematicians, etc.) to re-leverage existing code modules, scripts, and applications and adapt them to make use of the immense power of cluster solutions.
- DCI may include socket-level communications services provided through a multi-threaded server.
- DCI may include embedded support for hypertext transport protocol (HTTP) communications provided through a multi-threaded server.
- DCI may include embedded support for free-form XML communications provided through a multi-threaded server.
- DCI may include Simple Object Access Protocol or Service Oriented Architecture Protocol (SOAP) support.
- SOAP Service Oriented Architecture Protocol
- DCI may include meta-data (XML) routing of messages in peer-to-peer fashion, with time-to-live (TTL) based expiry.
- DCI may include automated routing of incoming XML messages through to application tier with support for auto-invocation of recipient application based on message-type.
- DCI may include directory functionality to allow applications to conveniently monitor online and connected peer nodes.
- DCI may include an ability to embed existing applications as Peer-to-Peer services without modifying them in source or binary form.
- DCI may include use of XML to support cross-language bridging (language to language subroutine invocation) without modifying called pre-compiled application or scripts.
- DCI may include a virtual file system to allow connected peers to share exposed files and folders under a navigable and structured file system.
- DCI may include a distributed shell that exposes the underlying virtual file system in a command line environment, and also provides the ability to view, manage, and execute distributed applications.
- DCI may include application programming interface (API) support for unicast, multicast, and broadcast messaging.
- DCI may include automated application update capability to obtain, install, or update new distributed applications using a network such as the Internet.
- DCI may include graphical interface and desktop management functionality, to expose application interface in a consistent fashion to the end user.
- DCI may include auto-discovery capability to rapidly search hundreds of entries in a network address space to locate machines (nodes) running DCI.
- DCI may include cross-platform deployment capabilities including a Java-based infrastructure supported on a large number of platforms.
- DCI may provide a windowed, graphical interface that can help visualize computational results or monitor progress of multiple jobs in a single environment.
- applications may make use of the built in communications, directory, and XML routing capabilities of the underlying infrastructure. These applications would ordinarily be large and complex and would have to either utilize cryptic APIs such as Message Passing Interface (MPI) or contain implementations of sophisticated message passing and communications technology.
- MPI Message Passing Interface
- DCI eliminates most of this development and management overhead and provides a simple and consistent environment to develop, deploy, and manage distributed or cluster capable applications.
- FIG. 22 is a block diagram illustrating a DCI architecture according to one embodiment.
- Each of two or more computer blades 101 runs an operating system (OS) 2302 .
- the OS 2302 handles basic tasks like networking over Transmission Control Protocol/Internet Protocol (TCP/IP).
- Each DCI-enabled computer system on the network 115 may include a DCI stack.
- the DCI stack may include the core DCI framework 2304 , one or more peerlet APIs 2306 , and one or more peerlets 2308 .
- Peerlets 2308 are applications that provide functions on DCI-enabled computers. For example, chat, whiteboard, and other collaborative applications may be implemented as peerlets that can take advantage of DCI. In some cases, peerlets can implement functionality themselves. Peerlets may also be “shells” that are used to invoke functionality provided by other pieces of software. Specific peerlet APIs (Application Programming Interfaces) 2306 provide an interface between the core DCI framework 2304 and specific peerlets. In one embodiment, peerlets are not network-aware but rather pass requests for network activity to the DCI framework 2304 .
- DCI may enable applications that were not originally designed for distributed computing to be executed in a distributed manner.
- DCI may utilize an existing web browser without new integration code in the following manner.
- a user may request the retrieval of a web page from a remote web server (e.g., a news story from a news website), the combination of that web page with locally generated data (e.g., editorial comments from the user on the news story), and the storage of the combination on a database server.
- the user computer A may unicast a message to a web browser program on computer B to retrieve the web page.
- the output may be returned to computer A.
- Computer A may then combine the news story with the editorial comments in an XML message. This message may then be unicast to a command-line database program on computer C for storage in a database.
- DCI uses peer-to-peer message passing with no intermediary server.
- FIG. 23 is a flowchart illustrating a method for DCI message-passing according to one embodiment.
- a peerlet on computer A generates a message to be sent to computer B.
- a user may instruct a chat peerlet on computer A to request a chat session with a user on computer B or send a chat message in an ongoing, active chat session to a user on computer B.
- messages may include text and/or other data and metadata as well as requests to invoke the functionality of an application on another DCI-enabled computer.
- the DCI framework may transform the message to an XML (eXtensible Markup Language) format.
- the XML message may include the “payload” (i.e., the actual message to be delivered to the user on computer B) as well as a plurality of metadata elements.
- the metadata elements may include, for example, the type of application that send the message, the GUID (globally unique ID) of the instance of the application, and the sender.
- the DCI framework may log the XML message.
- all tasks that have been reduced to XML messages may be logged to a message queue in archivable form.
- the XML messages in the queue may be sorted by application type, sender, receiver, etc. Activities may be replayed through the use of the logged XML messages.
- the XML message may be sent over the network from computer A to computer B.
- the XML metadata may include a TTL value indicating the total number of allowable “hops” before the message is considered undeliverable. This value is decremented with each network hop; when the value reaches zero, DCI may consider the message to be expired.
- a value in excess of a maximum value (e.g., 9) may be reset to that maximum value.
- a DCI “listener” in the core DCI framework on computer B may receive the XML message.
- the DCI listener may utilize a User Datagram Protocol (UDP) server to listen for incoming packets over an IP-based network connection.
- UDP User Datagram Protocol
- the use of UDP rather than TCP may allow for the rapid shipment of packets without the overhead of TCP.
- the UDP server may be multi-threaded for increased scalability and improved response time.
- the actual communication between DCI-enabled computers may use a more reliable mechanism such as TCP.
- DCI may process the message, for example, to deliver its payload and/or invoke the requested functionality.
- the DCI framework 2304 B may determine the type of application that sent the incoming message using the XML metadata. If there is no application currently executing on computer B that can receive the message, the DCI framework 2304 B may invoke an instance of the relevant application (e.g., a peerlet 2308 B) on computer B. In one embodiment, DCI 2304 B may queue the incoming message until the relevant application has started up. DCI 2304 B may then transmit the queued message to the application.
- FIGS. 24 and 25 Virtual Storage Using Excess Distributed Storage Capacity
- VNAS Virtual Network Attached Storage
- a Virtual Network Attached Storage (VNAS) system may be used to aggregate storage capacity from a number of blades and re-expose the aggregated storage to the network as one or more virtual volumes.
- VNAS may therefore allow the construction of secure and scalable storage solutions. Not only may the mean time between failures (MTBF) for the overall solution be extended as a result of replication, but the ability to stripe data across drives may also result in increased speed of access.
- MTBF mean time between failures
- With storage pooling under VNAS unused network storage capacity may be salvaged, treated as a single volume, and provided to network-connected workstations.
- the VNAS solution can stripe a single file across multiple physical drives residing in multiple blades, thus enabling parallel access for a single file-level object, and thus reducing overall access times.
- the VNAS architecture is based on a collection of “store nodes” which may include ordinary desktops or workstations.
- the store nodes contribute their excess disk capacity to the VNAS volume.
- Mount points including computers such as desktop PCs, may allow a user to point to a single hostname (from a list of several) to “mount” and view the VNAS volume.
- VNAS supports industry-standard protocols (e.g., Distributed Authoring and Versioning (DAV)) which negate the need for any special software to be installed on a typical PC desktop in order for the user to browse the VNAS volume.
- DAV Distributed Authoring and Versioning
- mount points may also act as local caches. For example, if a user in one department requests a document that is stored on store nodes physically located three switches away, that access may result in a copy of the document being cached on the closest mount point cache.
- This “auto-caching” capability may result in frequently accessed data being replicated at local points of presence, therefore minimizing the load on store nodes, minimizing network traffic, and reducing latencies of access resulting from multiple switch traversals.
- pooled storage under VNAS may be exposed to the end-user as a simple folder, directory, volume, or other unit of storage. Users may interact with the VNAS folder (e.g., to drag and drop files) just like they would interact with a regular folder
- VNAS may be managed through a web-browser interface or other graphical user interface. Examples of browser-based interface screens are shown in FIGS. 9 through 14 . Management of VNAS may include storage usage policies, file finding, node maintenance, node removal, and virtual volume maintenance.
- FIG. 24 illustrates a VNAS architecture according to one embodiment.
- the DCI framework 2304 A discussed with reference to FIGS. 23 and 24 may run on top of an operating system 2302 A.
- VNAS 2320 may utilize the distributed computing infrastructure for communication with other blades.
- custom VNAS software need not be present on clients in one embodiment,
- VNAS uses industry-standard communications protocols 2322 such as DAV and HTTP to enable load balancing and scalability.
- FIG. 25 is a flowchart illustrating a method for using VNAS to assist in data restoration, according to one embodiment.
- the virtual network attached storage system may be configured for use. As discussed above, this configuration may include coupling the computers to the VNAS system and/or configuring a resource manager.
- the resource manager may manage the VNAS system, including write and striping rules.
- the resource manager may be located on one or more of the computers coupled to the VNAS system.
- the data and other configuration may be saved across one or more computers in the VNAS system according to the rules set up by the resource manager.
- Each one of the plurality of computers coupled to the VNAS system may be involved in storing the data for the other computers.
- the VNAS system may check if all of the computers comprised in the VNAS system are functioning properly.
- the VNAS system may determine one or more failed computers as well as one or more replacement computers.
- the vital data on the one or more failed computers is distributed across one or more computers comprised in the VNAS system.
- the VNAS system may access the one or more computers comprised in the VNAS system in order to retrieve the vital data for the one or more replacement computers.
- DCI With DCI in place, networked blades are capable of XML message exchange between distributed applications and application instances. This capability is leveraged by to provide one-to-one and one-to-many collaboration capabilities.
- DCI enables multiple concurrent conversations for collaborative applications such as chat and whiteboard applications. Every time a new instance of an application starts up, DCI assigns that instance a GUID (globally unique ID).
- GUID globally unique ID
- DCI can maintain security and other contextual boundaries between users on DCI-enabled computers.
- DCI particularly assists users and domain experts that wish to develop and deploy computationally complex applications
- application development through modular composition This capability allows existing modules, command line programs, perl scripts, or other implementations to be seamlessly tied together, coupled with a graphical interface, and almost instantly transformed into a cluster capable application.
- FIG. 26 is a screenshot that demonstrates the simple manner in which commands can be broadcasted to every node (Blade or PC) running the DCI platform.
- the dialog box 2650 on the upper right hand side allows commands and arguments to be entered, while the simple results screen 2660 on the left shows the output of the command as received from a particular node.
- management tasks such as distributed process listing across multiple operating systems, process deletion, or invocation, may be easy to implement and use.
- the Autonomous Intelligent Management System includes a collection of agents, applications, and tools built on top of the Distributed Computing Infrastructure.
- AIMS may augment the capabilities of human Information Technology (IT) resources and enable IT managers to easily manage much larger numbers of systems than would otherwise be possible.
- AIMS may use existing (i.e., logged) XML messages to “play back” tasks on DCI-enabled computers.
- AIMS may also use new (i.e., synthetic) XML messages to invoke functionality on DCI-enabled computers.
- Remote Management and Security Auditing may allow customers to sign up for remotely administered system configuration checks, security audits, license compliance, and similar administrative functions.
- the service may provide IT executives with reports of the exact status of each of their Blades, regardless of physical location.
- An enterprise-level report could include information on cluster of Blades in multiple geographical locations.
- AIMS may also be used to enhance enterprise security. Ongoing monitoring for infection may include the ability to scan all files on disk, probe incoming emails and scan outgoing emails to ensure that Trojan horses or virus code is not being transported. Compliance with corporate security guidelines may allow system administrators to ensure that any one of thousands of enterprise desktops is compliant with security policies. When machines are newly delivered to employees, they are typically compliant to policies that may dictate installation of a certain version of a program because other versions may contain backdoors or exploitable loopholes. As the employee's usage of a PC progresses, new programs and files are routinely installed and de-installed. As a result, the workstations in a typical large enterprise do not comply with security guidelines within a matter of weeks after initial deployment. Because it is often difficult to manually visit each workstation and ensure compliance, AIMS may allow administrators to centrally monitor compliance and run security-oriented queries.
- Patch deployment under AIMS may allow system administrators to remotely copy new patches and execute them remotely to modify existing programs that may have been outdated due to the discovery of security holes.
- Feature activation/deactivation under AIMS may allow fine-grained control over services such as web servers, database servers, or any other program that monitors incoming connections and thus may be prone to remote hacking.
- Service deactivation may allow system administrators to temporarily prevent remote entry while applying patches or taking other precautions.
- Installation and package management may allow utilities and applications to be copied remotely, installed, and deployed on corporate workstations. This will negate the need for IT staff to physically visit desktops and install applications in that cumbersome manner.
- Driver version management may allow administrators to query all systems for their installed drivers and driver versions. Mismatches, or version numbers not in compliance with manufacturer or enterprise guidelines will be tagged. Hardware or peripheral issues related to non-functioning drivers, or bad driver versions will be diagnosed and fixed remotely. This functionality will allow both individual machine level diagnostics and queries across the corporate network to ensure large-scale compliance with predefined guidelines.
- Patch deployment may allow OS and Application-related patches and fixes to be copied and installed remotely.
- Disk De-fragmentation may allow administrators to remotely, at will, or according to a predefined schedule, conduct hard drive de-fragmentation operations. This will ensure that the performance of workstations does not degrade over time and that users of workstations do not have to worry about conducting such maintenance on their own, or suffer the decreased performance due to fragmented data.
- Memory integrity tests may allow administrators to debug and fix memory related issues with workstations. Very often, unpredictable and random OS blue screening (i.e. crash) is caused by the presence of faulty or unreliable locations in memory. Writes or reads from these locations can result in the system crashing or suddenly rebooting. Memory integrity features will allow administrators to probe such issues remotely.
- AIMS may also support performance and usage monitoring.
- Syslog Queries may enable the OS (e.g., Windows) system logs to be queried remotely.
- OS e.g., Windows
- Windows 2 k does not provide capabilities where all logs of an entire network can be managed centrally, as each machine maintains its own logs.
- Syslog querying will allow queries to run on all machines on a network to monitor unique traffic, usage or performance related trends.
- Threshold based pro-active reporting may allow administrators to specify thresholds with respect to key system variables and attributes. Rather than a human manually monitoring these variables, AIMS may maintain continuous background monitoring and will report threshold violations to an IT manager. This capability, when implemented on large networks, will provide a very significant advantage to IT staff in terms of timesavings and pro-active impending problem notifications.
- Voice XML Voice XML
- standard interactive interfaces may be constructed that can be uniformly interpreted and used by any Voice XML compliant application (including browsers).
- Voice XML and speech processing technology may allow applications to be controlled by repeating pre-recorded voice prompts.
- FIG. 27 is a block diagram illustrating an AIMS architecture, according to one embodiment.
- Management modules 2332 may include security modules, application modules, etc. These modules may be added to the existing AIMS and DCI frameworks to suit the management needs of individual systems.
- the AIMS application 2330 and DCI framework 2304 may run on top of an operating system 2302 on all blades in the network.
- Suitable carrier media may include storage media or memory media such as magnetic or optical media, e.g., disk or CD-ROM.
- Suitable carrier media may also include transmission media or signals such as electrical, electromagnetic, or digital signals, conveyed via a communication medium such as a network and/or a wireless link.
Abstract
Description
Claims (21)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/662,936 US7434220B2 (en) | 2002-09-16 | 2003-09-15 | Distributed computing infrastructure including autonomous intelligent management system |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US41106602P | 2002-09-16 | 2002-09-16 | |
US10/662,936 US7434220B2 (en) | 2002-09-16 | 2003-09-15 | Distributed computing infrastructure including autonomous intelligent management system |
Publications (2)
Publication Number | Publication Date |
---|---|
US20040107420A1 US20040107420A1 (en) | 2004-06-03 |
US7434220B2 true US7434220B2 (en) | 2008-10-07 |
Family
ID=31994240
Family Applications (7)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/662,968 Abandoned US20040098458A1 (en) | 2002-09-16 | 2003-09-15 | Distributed computing infrastructure including multiple collaborative sessions |
US10/662,955 Abandoned US20040104927A1 (en) | 2002-09-16 | 2003-09-15 | System and method for automatic software retrieval on a peer-to-peer network |
US10/662,933 Active 2025-10-06 US7370336B2 (en) | 2002-09-16 | 2003-09-15 | Distributed computing infrastructure including small peer-to-peer applications |
US10/662,954 Abandoned US20040098728A1 (en) | 2002-09-16 | 2003-09-15 | System and method for multi-functional XML-capable software applications on a peer-to-peer network |
US10/662,932 Abandoned US20040098717A1 (en) | 2002-09-16 | 2003-09-15 | System and method for creating complex distributed applications |
US10/662,889 Active 2026-04-17 US7430616B2 (en) | 2002-09-16 | 2003-09-15 | System and method for reducing user-application interactions to archivable form |
US10/662,936 Active 2025-11-29 US7434220B2 (en) | 2002-09-16 | 2003-09-15 | Distributed computing infrastructure including autonomous intelligent management system |
Family Applications Before (6)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/662,968 Abandoned US20040098458A1 (en) | 2002-09-16 | 2003-09-15 | Distributed computing infrastructure including multiple collaborative sessions |
US10/662,955 Abandoned US20040104927A1 (en) | 2002-09-16 | 2003-09-15 | System and method for automatic software retrieval on a peer-to-peer network |
US10/662,933 Active 2025-10-06 US7370336B2 (en) | 2002-09-16 | 2003-09-15 | Distributed computing infrastructure including small peer-to-peer applications |
US10/662,954 Abandoned US20040098728A1 (en) | 2002-09-16 | 2003-09-15 | System and method for multi-functional XML-capable software applications on a peer-to-peer network |
US10/662,932 Abandoned US20040098717A1 (en) | 2002-09-16 | 2003-09-15 | System and method for creating complex distributed applications |
US10/662,889 Active 2026-04-17 US7430616B2 (en) | 2002-09-16 | 2003-09-15 | System and method for reducing user-application interactions to archivable form |
Country Status (3)
Country | Link |
---|---|
US (7) | US20040098458A1 (en) |
AU (1) | AU2003272404A1 (en) |
WO (1) | WO2004025466A2 (en) |
Cited By (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060075407A1 (en) * | 2004-10-06 | 2006-04-06 | Digipede Technologies, Llc | Distributed system interface |
US20080027948A1 (en) * | 2006-07-06 | 2008-01-31 | Richard Corley | Managing Application System Load |
US20090310531A1 (en) * | 2008-06-17 | 2009-12-17 | Raytheon Company | Airborne Communication Network |
US20100250298A1 (en) * | 2009-03-25 | 2010-09-30 | International Business Machines Corporation | Prioritization enablement for soa governance |
US20110078681A1 (en) * | 2009-09-30 | 2011-03-31 | International Business Machines Corporation | Method and system for running virtual machine image |
US20110082939A1 (en) * | 2009-10-02 | 2011-04-07 | Michael Peter Montemurro | Methods and apparatus to proxy discovery and negotiations between network entities to establish peer-to-peer communications |
US20110082940A1 (en) * | 2009-10-02 | 2011-04-07 | Michael Peter Montemurro | Methods and apparatus to establish peer-to-peer communications |
US9183560B2 (en) | 2010-05-28 | 2015-11-10 | Daniel H. Abelow | Reality alternate |
US9392060B1 (en) | 2013-02-08 | 2016-07-12 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
US9977721B2 (en) | 2007-12-20 | 2018-05-22 | Netapp, Inc. | Evaluating and predicting computer system performance using kneepoint analysis |
Families Citing this family (141)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6658423B1 (en) * | 2001-01-24 | 2003-12-02 | Google, Inc. | Detecting duplicate and near-duplicate files |
US7392375B2 (en) * | 2002-09-18 | 2008-06-24 | Colligo Networks, Inc. | Peer-to-peer authentication for real-time collaboration |
US8402001B1 (en) * | 2002-10-08 | 2013-03-19 | Symantec Operating Corporation | System and method for archiving data |
US20040103392A1 (en) * | 2002-11-26 | 2004-05-27 | Guimei Zhang | Saving and retrieving archive data |
US7660864B2 (en) * | 2003-05-27 | 2010-02-09 | Nokia Corporation | System and method for user notification |
US8046701B2 (en) * | 2003-08-07 | 2011-10-25 | Fuji Xerox Co., Ltd. | Peer to peer gesture based modular presentation system |
US20050102488A1 (en) * | 2003-11-07 | 2005-05-12 | Bullis George A. | Firmware description language for accessing firmware registers |
US7908208B2 (en) * | 2003-12-10 | 2011-03-15 | Alphacap Ventures Llc | Private entity profile network |
US20050132353A1 (en) * | 2003-12-11 | 2005-06-16 | Chueh-Hsin Chen | Method for installing networked attached storage |
US20050198152A1 (en) * | 2004-02-12 | 2005-09-08 | International Business Machines Corporation | Computer with a personal digital assistant |
US7454465B2 (en) * | 2004-03-26 | 2008-11-18 | Microsoft Corporation | Real-time collaboration and communication in a peer-to-peer networking infrastructure |
US8688803B2 (en) | 2004-03-26 | 2014-04-01 | Microsoft Corporation | Method for efficient content distribution using a peer-to-peer networking infrastructure |
US7515528B2 (en) * | 2004-03-31 | 2009-04-07 | Intel Corporation | Host fail-over switch presence detection compatible with existing protocol and host controllers |
US8108429B2 (en) * | 2004-05-07 | 2012-01-31 | Quest Software, Inc. | System for moving real-time data events across a plurality of devices in a network for simultaneous data protection, replication, and access services |
US7565661B2 (en) * | 2004-05-10 | 2009-07-21 | Siew Yong Sim-Tang | Method and system for real-time event journaling to provide enterprise data services |
US9357031B2 (en) | 2004-06-03 | 2016-05-31 | Microsoft Technology Licensing, Llc | Applications as a service |
US7908339B2 (en) * | 2004-06-03 | 2011-03-15 | Maxsp Corporation | Transaction based virtual file system optimized for high-latency network connections |
US8812613B2 (en) * | 2004-06-03 | 2014-08-19 | Maxsp Corporation | Virtual application manager |
US8074277B2 (en) * | 2004-06-07 | 2011-12-06 | Check Point Software Technologies, Inc. | System and methodology for intrusion detection and prevention |
US7698487B2 (en) * | 2004-06-30 | 2010-04-13 | Intel Corporation | Share resources and increase reliability in a server environment |
US8316088B2 (en) | 2004-07-06 | 2012-11-20 | Nokia Corporation | Peer-to-peer engine for object sharing in communication devices |
US7664834B2 (en) | 2004-07-09 | 2010-02-16 | Maxsp Corporation | Distributed operating system management |
US20060020455A1 (en) * | 2004-07-20 | 2006-01-26 | Motorola, Inc. | Adaptive plug-in architecture for mix-mode personal communication |
US7567974B2 (en) | 2004-09-09 | 2009-07-28 | Microsoft Corporation | Method, system, and apparatus for configuring a data protection system |
US8145601B2 (en) | 2004-09-09 | 2012-03-27 | Microsoft Corporation | Method, system, and apparatus for providing resilient data transfer in a data protection system |
US7979404B2 (en) | 2004-09-17 | 2011-07-12 | Quest Software, Inc. | Extracting data changes and storing data history to allow for instantaneous access to and reconstruction of any point-in-time data |
US7734753B2 (en) * | 2004-10-12 | 2010-06-08 | International Business Machines Corporation | Apparatus, system, and method for facilitating management of logical nodes through a single management module |
US20060080319A1 (en) * | 2004-10-12 | 2006-04-13 | Hickman John E | Apparatus, system, and method for facilitating storage management |
JP3864968B2 (en) * | 2004-10-19 | 2007-01-10 | コニカミノルタビジネステクノロジーズ株式会社 | Image processing system and control method therefor, image processing apparatus, and computer program |
US7756931B2 (en) * | 2004-10-28 | 2010-07-13 | International Business Machines Corporation | Method and apparatus for manager/agent communications |
US7904913B2 (en) | 2004-11-02 | 2011-03-08 | Bakbone Software, Inc. | Management interface for a system that provides automated, real-time, continuous data protection |
US7617481B2 (en) * | 2004-11-30 | 2009-11-10 | Avanade Holdings Llc | Prescriptive architecture for application development |
US8650259B2 (en) * | 2005-02-03 | 2014-02-11 | International Business Machines Corporation | Method and apparatus for increasing the search space or peer-to-peer networks using time-to-live boosting |
US8234238B2 (en) | 2005-03-04 | 2012-07-31 | Maxsp Corporation | Computer hardware and software diagnostic and report system |
US8589323B2 (en) | 2005-03-04 | 2013-11-19 | Maxsp Corporation | Computer hardware and software diagnostic and report system incorporating an expert system and agents |
US7624086B2 (en) * | 2005-03-04 | 2009-11-24 | Maxsp Corporation | Pre-install compliance system |
US7512584B2 (en) * | 2005-03-04 | 2009-03-31 | Maxsp Corporation | Computer hardware and software diagnostic and report system |
US20060218435A1 (en) * | 2005-03-24 | 2006-09-28 | Microsoft Corporation | Method and system for a consumer oriented backup |
US8117597B2 (en) * | 2005-05-16 | 2012-02-14 | Shia So-Ming Daniel | Method and system for specifying and developing application systems with dynamic behavior |
US20060277092A1 (en) * | 2005-06-03 | 2006-12-07 | Credigy Technologies, Inc. | System and method for a peer to peer exchange of consumer information |
US7802257B1 (en) * | 2005-06-20 | 2010-09-21 | Oracle America, Inc. | Mechanism for bridging a thread-oriented computing paradigm and a job-oriented computing paradigm |
US7689602B1 (en) * | 2005-07-20 | 2010-03-30 | Bakbone Software, Inc. | Method of creating hierarchical indices for a distributed object system |
US7788521B1 (en) * | 2005-07-20 | 2010-08-31 | Bakbone Software, Inc. | Method and system for virtual on-demand recovery for real-time, continuous data protection |
US7739314B2 (en) * | 2005-08-15 | 2010-06-15 | Google Inc. | Scalable user clustering based on set similarity |
US7558858B1 (en) * | 2005-08-31 | 2009-07-07 | At&T Intellectual Property Ii, L.P. | High availability infrastructure with active-active designs |
US8166547B2 (en) * | 2005-09-06 | 2012-04-24 | Fortinet, Inc. | Method, apparatus, signals, and medium for managing a transfer of data in a data network |
CN1928806A (en) * | 2005-09-09 | 2007-03-14 | 鸿富锦精密工业（深圳）有限公司 | Two-desktop remote control systems and method |
US8949364B2 (en) * | 2005-09-15 | 2015-02-03 | Ca, Inc. | Apparatus, method and system for rapid delivery of distributed applications |
US8479146B2 (en) * | 2005-09-23 | 2013-07-02 | Clearcube Technology, Inc. | Utility computing system having co-located computer systems for provision of computing resources |
US20070074187A1 (en) * | 2005-09-29 | 2007-03-29 | O'brien Thomas E | Method and apparatus for inserting code fixes into applications at runtime |
US7779157B2 (en) * | 2005-10-28 | 2010-08-17 | Yahoo! Inc. | Recovering a blade in scalable software blade architecture |
US7873696B2 (en) * | 2005-10-28 | 2011-01-18 | Yahoo! Inc. | Scalable software blade architecture |
US20070100979A1 (en) * | 2005-11-01 | 2007-05-03 | Savvis Communications Corporation | Virtualized utility service platform |
US8671133B2 (en) | 2005-11-29 | 2014-03-11 | The Boeing Company | System having an energy efficient network infrastructure for communication between distributed processing nodes |
US8560456B2 (en) * | 2005-12-02 | 2013-10-15 | Credigy Technologies, Inc. | System and method for an anonymous exchange of private data |
US20070162377A1 (en) * | 2005-12-23 | 2007-07-12 | Credigy Technologies, Inc. | System and method for an online exchange of private data |
US20070240164A1 (en) * | 2006-03-15 | 2007-10-11 | Microsoft Corporation | Command line pipelining |
GB0607294D0 (en) | 2006-04-11 | 2006-05-24 | Nokia Corp | A node |
US7797670B2 (en) * | 2006-04-14 | 2010-09-14 | Apple Inc. | Mirrored file system |
US8811396B2 (en) | 2006-05-24 | 2014-08-19 | Maxsp Corporation | System for and method of securing a network utilizing credentials |
US8898319B2 (en) | 2006-05-24 | 2014-11-25 | Maxsp Corporation | Applications and services as a bundle |
US9317506B2 (en) | 2006-09-22 | 2016-04-19 | Microsoft Technology Licensing, Llc | Accelerated data transfer using common prior data segments |
US7840514B2 (en) * | 2006-09-22 | 2010-11-23 | Maxsp Corporation | Secure virtual private network utilizing a diagnostics policy and diagnostics engine to establish a secure network connection |
US20100095009A1 (en) * | 2006-10-02 | 2010-04-15 | Nokia Corporation | Method, System, and Devices for Network Sharing or Searching Of Resources |
US7844686B1 (en) | 2006-12-21 | 2010-11-30 | Maxsp Corporation | Warm standby appliance |
US8423821B1 (en) | 2006-12-21 | 2013-04-16 | Maxsp Corporation | Virtual recovery server |
CN101212477B (en) * | 2006-12-30 | 2010-11-10 | 广达电脑股份有限公司 | Management interface between embedded systems of blade server |
US8285646B2 (en) * | 2007-03-19 | 2012-10-09 | Igt | Centralized licensing services |
US8131723B2 (en) | 2007-03-30 | 2012-03-06 | Quest Software, Inc. | Recovering a file system to any point-in-time in the past with guaranteed structure, content consistency and integrity |
US8702505B2 (en) | 2007-03-30 | 2014-04-22 | Uranus International Limited | Method, apparatus, system, medium, and signals for supporting game piece movement in a multiple-party communication |
US8060887B2 (en) | 2007-03-30 | 2011-11-15 | Uranus International Limited | Method, apparatus, system, and medium for supporting multiple-party communications |
US7950046B2 (en) | 2007-03-30 | 2011-05-24 | Uranus International Limited | Method, apparatus, system, medium, and signals for intercepting a multiple-party communication |
US7765261B2 (en) | 2007-03-30 | 2010-07-27 | Uranus International Limited | Method, apparatus, system, medium and signals for supporting a multiple-party communication on a plurality of computer servers |
US7765266B2 (en) | 2007-03-30 | 2010-07-27 | Uranus International Limited | Method, apparatus, system, medium, and signals for publishing content created during a communication |
US8627211B2 (en) | 2007-03-30 | 2014-01-07 | Uranus International Limited | Method, apparatus, system, medium, and signals for supporting pointer display in a multiple-party communication |
US8364648B1 (en) | 2007-04-09 | 2013-01-29 | Quest Software, Inc. | Recovering a database to any point-in-time in the past with guaranteed data consistency |
US8438136B2 (en) * | 2007-09-27 | 2013-05-07 | Symantec Corporation | Backup catalog recovery from replicated data |
US9531798B2 (en) * | 2007-10-03 | 2016-12-27 | Virtela Technology Services Incorporated | Pandemic remote access design |
US8175418B1 (en) | 2007-10-26 | 2012-05-08 | Maxsp Corporation | Method of and system for enhanced data storage |
US8307239B1 (en) | 2007-10-26 | 2012-11-06 | Maxsp Corporation | Disaster recovery appliance |
US8645515B2 (en) | 2007-10-26 | 2014-02-04 | Maxsp Corporation | Environment manager |
US8813062B1 (en) * | 2007-12-12 | 2014-08-19 | Genband Us Llc | Dynamically binding a logic component to a processing point in a software execution flow |
US20090172082A1 (en) * | 2007-12-31 | 2009-07-02 | Joaquin Sufuentes | Software as a service in a peer-to-peer environment |
US20090265586A1 (en) * | 2008-04-18 | 2009-10-22 | Sun Microsystems, Inc. | Method and system for installing software deliverables |
US8225292B2 (en) * | 2008-04-18 | 2012-07-17 | Oracle America, Inc. | Method and system for validating a knowledge package |
US8405727B2 (en) * | 2008-05-01 | 2013-03-26 | Apple Inc. | Apparatus and method for calibrating image capture devices |
US8219713B2 (en) * | 2008-07-01 | 2012-07-10 | Broadcom Corporation | Method and system for a network controller based pass-through communication mechanism between local host and management controller |
US8508671B2 (en) * | 2008-09-08 | 2013-08-13 | Apple Inc. | Projection systems and methods |
US8538084B2 (en) * | 2008-09-08 | 2013-09-17 | Apple Inc. | Method and apparatus for depth sensing keystoning |
US20100079653A1 (en) * | 2008-09-26 | 2010-04-01 | Apple Inc. | Portable computing system with a secondary image output |
US8527908B2 (en) | 2008-09-26 | 2013-09-03 | Apple Inc. | Computer user interface system and methods |
US20100079426A1 (en) * | 2008-09-26 | 2010-04-01 | Apple Inc. | Spatial ambient light profiling |
US7881603B2 (en) * | 2008-09-26 | 2011-02-01 | Apple Inc. | Dichroic aperture for electronic imaging device |
US8610726B2 (en) * | 2008-09-26 | 2013-12-17 | Apple Inc. | Computer systems and methods with projected display |
US20100095250A1 (en) * | 2008-10-15 | 2010-04-15 | Raytheon Company | Facilitating Interaction With An Application |
KR101310218B1 (en) * | 2008-10-28 | 2013-09-24 | 삼성전자주식회사 | Method for installing an integrated file and image forming apparatus for installing the integrated file thereby |
US8489721B1 (en) * | 2008-12-30 | 2013-07-16 | Symantec Corporation | Method and apparatus for providing high availabilty to service groups within a datacenter |
US8935366B2 (en) | 2009-04-24 | 2015-01-13 | Microsoft Corporation | Hybrid distributed and cloud backup architecture |
US8769049B2 (en) | 2009-04-24 | 2014-07-01 | Microsoft Corporation | Intelligent tiers of backup data |
US8769055B2 (en) | 2009-04-24 | 2014-07-01 | Microsoft Corporation | Distributed backup and versioning |
US8560639B2 (en) * | 2009-04-24 | 2013-10-15 | Microsoft Corporation | Dynamic placement of replica data |
US8502926B2 (en) * | 2009-09-30 | 2013-08-06 | Apple Inc. | Display system having coherent and incoherent light sources |
US8619128B2 (en) | 2009-09-30 | 2013-12-31 | Apple Inc. | Systems and methods for an imaging system using multiple image sensors |
US8645511B2 (en) * | 2009-10-13 | 2014-02-04 | Google Inc. | Pre-configuration of a cloud-based computer |
SG171492A1 (en) * | 2009-12-01 | 2011-06-29 | Creative Tech Ltd | An electronic book reader |
US20110137872A1 (en) * | 2009-12-04 | 2011-06-09 | International Business Machines Corporation | Model-driven data archival system having automated components |
US8260813B2 (en) | 2009-12-04 | 2012-09-04 | International Business Machines Corporation | Flexible data archival using a model-driven approach |
US8687070B2 (en) | 2009-12-22 | 2014-04-01 | Apple Inc. | Image capture device having tilt and/or perspective correction |
EP3508978B1 (en) * | 2010-03-12 | 2021-09-22 | BlackBerry Limited | Distributed catalog, data store, and indexing |
WO2011116087A2 (en) | 2010-03-16 | 2011-09-22 | Copiun, Inc. | Highly scalable and distributed data de-duplication |
US9357328B1 (en) | 2010-06-15 | 2016-05-31 | Thales Avionics, Inc. | Systems and methods for distributing content using attributes |
US8497897B2 (en) | 2010-08-17 | 2013-07-30 | Apple Inc. | Image capture using luminance and chrominance sensors |
US8538132B2 (en) | 2010-09-24 | 2013-09-17 | Apple Inc. | Component concentricity |
US10225335B2 (en) | 2011-02-09 | 2019-03-05 | Cisco Technology, Inc. | Apparatus, systems and methods for container based service deployment |
US10003672B2 (en) | 2011-02-09 | 2018-06-19 | Cisco Technology, Inc. | Apparatus, systems and methods for deployment of interactive desktop applications on distributed infrastructures |
US10678602B2 (en) | 2011-02-09 | 2020-06-09 | Cisco Technology, Inc. | Apparatus, systems and methods for dynamic adaptive metrics based application deployment on distributed infrastructures |
US8862933B2 (en) | 2011-02-09 | 2014-10-14 | Cliqr Technologies, Inc. | Apparatus, systems and methods for deployment and management of distributed computing systems and applications |
EP2756409A4 (en) * | 2011-09-12 | 2015-01-28 | Intel Corp | Metadata driven collaboration between applications and web services |
KR101942335B1 (en) * | 2011-09-30 | 2019-01-28 | 삼성전자 주식회사 | Method for integrated management of maintenance of electronic devices and system thereof |
US20130290855A1 (en) * | 2012-04-29 | 2013-10-31 | Britt C. Ashcraft | Virtual shared office bulletin board |
US9356061B2 (en) | 2013-08-05 | 2016-05-31 | Apple Inc. | Image sensor with buried light shield and vertical gate |
US9485099B2 (en) | 2013-10-25 | 2016-11-01 | Cliqr Technologies, Inc. | Apparatus, systems and methods for agile enablement of secure communications for cloud based applications |
US9608932B2 (en) * | 2013-12-10 | 2017-03-28 | International Business Machines Corporation | Software-defined networking single-source enterprise workload manager |
US10887165B2 (en) * | 2014-01-22 | 2021-01-05 | Zhenhua Li | Personal working system available for dynamic combination and adjustment |
US10256855B2 (en) * | 2014-01-31 | 2019-04-09 | Qualcomm Incorporated | Interference management information signaling |
US9430213B2 (en) | 2014-03-11 | 2016-08-30 | Cliqr Technologies, Inc. | Apparatus, systems and methods for cross-cloud software migration and deployment |
US9442803B2 (en) | 2014-06-24 | 2016-09-13 | International Business Machines Corporation | Method and system of distributed backup for computer devices in a network |
US9218407B1 (en) | 2014-06-25 | 2015-12-22 | Pure Storage, Inc. | Replication and intermediate read-write state for mediums |
US9811241B2 (en) | 2014-09-17 | 2017-11-07 | International Business Machines Corporation | Shared inter-operational control among multiple computing devices |
US20160092310A1 (en) * | 2014-09-30 | 2016-03-31 | Vivint, Inc. | Systems and methods for managing globally distributed remote storage devices |
US9575837B2 (en) * | 2015-02-03 | 2017-02-21 | Uber Technologies, Inc. | System and method for introducing functionality to an application for use with a network service |
US9961043B2 (en) * | 2015-02-20 | 2018-05-01 | Dell Products L.P. | Automatic discovery and configuration of stack ports |
US10212536B2 (en) | 2015-07-10 | 2019-02-19 | Uber Technologies, Inc. | Selecting a messaging protocol for transmitting data in connection with a location-based service |
US10158528B2 (en) | 2015-10-13 | 2018-12-18 | Uber Technologies, Inc. | Application service configuration system |
US11533226B2 (en) | 2015-10-13 | 2022-12-20 | Uber Technologies, Inc. | Application service configuration system |
US10785315B2 (en) * | 2015-10-30 | 2020-09-22 | Citrix Systems, Inc. | Method for resumption of an application session with a very dynamic and very large state in a standby intermediary device when the primary device fails |
JP6658157B2 (en) * | 2016-03-17 | 2020-03-04 | 株式会社リコー | Information processing apparatus, system, program, and processing method |
CN106302734B (en) * | 2016-08-16 | 2019-03-26 | 北京控制工程研究所 | A kind of autonomous evolution implementation method of satellite counting system |
US11223537B1 (en) | 2016-08-17 | 2022-01-11 | Veritas Technologies Llc | Executing custom scripts from the host during disaster recovery |
US10977105B2 (en) | 2018-12-14 | 2021-04-13 | Uber Technologies, Inc. | Memory crash prevention for a computing device |
US11089141B2 (en) * | 2020-01-08 | 2021-08-10 | Bank Of America Corporation | Method and system for data prioritization communication |
Citations (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5960404A (en) * | 1997-08-28 | 1999-09-28 | International Business Machines Corp. | Mechanism for heterogeneous, peer-to-peer, and disconnected workflow operation |
US5987376A (en) | 1997-07-16 | 1999-11-16 | Microsoft Corporation | System and method for the distribution and synchronization of data and state information between clients in a distributed processing system |
WO2000039713A1 (en) | 1998-12-28 | 2000-07-06 | Gemteq Software, Inc. | A method and system for performing electronic data-gathering across multiple data sources |
EP1043671A2 (en) | 1999-03-19 | 2000-10-11 | International Business Machines Corporation | Message broker providing a publish/subscribe service and method of processing messages in a publish/subscribe environment |
WO2001006365A2 (en) | 1999-07-19 | 2001-01-25 | Groove Networks, Inc. | Method and apparatus for activity-based collaboration by a computer system equipped with a communications manager |
US6353608B1 (en) | 1998-06-16 | 2002-03-05 | Mci Communications Corporation | Host connect gateway for communications between interactive voice response platforms and customer host computing applications |
WO2002032171A1 (en) | 2000-10-09 | 2002-04-18 | Telstra New Wave Pty Ltd | Message processing |
US6453356B1 (en) * | 1998-04-15 | 2002-09-17 | Adc Telecommunications, Inc. | Data exchange system and method |
US6564246B1 (en) | 1999-02-02 | 2003-05-13 | International Business Machines Corporation | Shared and independent views of shared workspace for real-time collaboration |
US6604104B1 (en) * | 2000-10-02 | 2003-08-05 | Sbi Scient Inc. | System and process for managing data within an operational data store |
US6643652B2 (en) | 2000-01-14 | 2003-11-04 | Saba Software, Inc. | Method and apparatus for managing data exchange among systems in a network |
US6658526B2 (en) | 1997-03-12 | 2003-12-02 | Storage Technology Corporation | Network attached virtual data storage subsystem |
US6671737B1 (en) * | 1999-09-24 | 2003-12-30 | Xerox Corporation | Decentralized network system |
US20040031037A1 (en) * | 1998-04-28 | 2004-02-12 | Mikio Ikoma | Apparatus and method for use in distributed computing environment for converting data format between program language-specific format used in respective computers and stream format used for communication among computers |
US20040254945A1 (en) * | 2003-05-16 | 2004-12-16 | Patrick Schmidt | Business process management for a message-based exchange infrastructure |
US6845507B2 (en) * | 2000-05-18 | 2005-01-18 | Ss & C Technologies, Inc. | Method and system for straight through processing |
US6879995B1 (en) * | 1999-08-13 | 2005-04-12 | Sun Microsystems, Inc. | Application server message logging |
US7024669B1 (en) * | 1999-02-26 | 2006-04-04 | International Business Machines Corporation | Managing workload within workflow-management-systems |
US7127613B2 (en) * | 2002-02-25 | 2006-10-24 | Sun Microsystems, Inc. | Secured peer-to-peer network data exchange |
US7168077B2 (en) * | 2003-01-31 | 2007-01-23 | Handysoft Corporation | System and method of executing and controlling workflow processes |
Family Cites Families (78)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4932868A (en) * | 1986-09-04 | 1990-06-12 | Vent-Plant Corporation | Submergible screw-type dental implant and method of utilization |
US5333266A (en) * | 1992-03-27 | 1994-07-26 | International Business Machines Corporation | Method and apparatus for message handling in computer systems |
US5805897A (en) * | 1992-07-31 | 1998-09-08 | International Business Machines Corporation | System and method for remote software configuration and distribution |
JP2519390B2 (en) * | 1992-09-11 | 1996-07-31 | インターナショナル・ビジネス・マシーンズ・コーポレイション | DATA COMMUNICATION METHOD AND DEVICE |
GB2272311A (en) * | 1992-11-10 | 1994-05-11 | Ibm | Call management in a collaborative working network. |
US6873627B1 (en) * | 1995-01-19 | 2005-03-29 | The Fantastic Corporation | System and method for sending packets over a computer network |
US5724508A (en) * | 1995-03-09 | 1998-03-03 | Insoft, Inc. | Apparatus for collaborative computing |
US5815793A (en) * | 1995-10-05 | 1998-09-29 | Microsoft Corporation | Parallel computer |
US5831975A (en) * | 1996-04-04 | 1998-11-03 | Lucent Technologies Inc. | System and method for hierarchical multicast routing in ATM networks |
US20020032803A1 (en) * | 1996-07-15 | 2002-03-14 | Paul Marcos | Method and apparatus for dynamically brokering object messages among object models |
GB9625019D0 (en) * | 1996-11-29 | 1997-01-15 | Northern Telecom Ltd | Network restoration routing optimisation |
DE59800113D1 (en) * | 1997-01-23 | 2000-05-11 | Werner Hermann | Clamping arrangements for orthopedic fixators and uses thereof |
US6710786B1 (en) * | 1997-02-03 | 2004-03-23 | Oracle International Corporation | Method and apparatus for incorporating state information into a URL |
US6247056B1 (en) * | 1997-02-03 | 2001-06-12 | Oracle Corporation | Method and apparatus for handling client request with a distributed web application server |
GB2324175B (en) * | 1997-04-10 | 2002-07-31 | Ibm | Personal conferencing system |
US6144992A (en) * | 1997-05-09 | 2000-11-07 | Altiris, Inc. | Method and system for client/server and peer-to-peer disk imaging |
IES77331B2 (en) * | 1997-06-03 | 1997-12-03 | Tecos Holdings Inc | Pluridirectional and modulable vertebral osteosynthesis device of small overall size |
US6247050B1 (en) * | 1997-09-12 | 2001-06-12 | Intel Corporation | System for collecting and displaying performance improvement information for a computer |
US6362836B1 (en) * | 1998-04-06 | 2002-03-26 | The Santa Cruz Operation, Inc. | Universal application server for providing applications on a variety of client devices in a client/server network |
US6484196B1 (en) * | 1998-03-20 | 2002-11-19 | Advanced Web Solutions | Internet messaging system and method for use in computer networks |
US6487583B1 (en) * | 1998-09-15 | 2002-11-26 | Ikimbo, Inc. | System and method for information and application distribution |
US5910142A (en) * | 1998-10-19 | 1999-06-08 | Bones Consulting, Llc | Polyaxial pedicle screw having a rod clamping split ferrule coupling element |
US6214012B1 (en) * | 1998-11-13 | 2001-04-10 | Harrington Arthritis Research Center | Method and apparatus for delivering material to a desired location |
US6463460B1 (en) * | 1999-04-23 | 2002-10-08 | The United States Of America As Represented By The Secretary Of The Navy | Interactive communication system permitting increased collaboration between users |
US6779184B1 (en) * | 1999-01-21 | 2004-08-17 | Oracle International Corporation | Method for loosely coupling object oriented and non-object oriented applications in a messaging-based communication infrastructure |
US6129730A (en) * | 1999-02-10 | 2000-10-10 | Depuy Acromed, Inc. | Bi-fed offset pitch bone screw |
US6584493B1 (en) * | 1999-03-02 | 2003-06-24 | Microsoft Corporation | Multiparty conferencing and collaboration system utilizing a per-host model command, control and communication structure |
US7007235B1 (en) * | 1999-04-02 | 2006-02-28 | Massachusetts Institute Of Technology | Collaborative agent interaction control and synchronization system |
US6446110B1 (en) * | 1999-04-05 | 2002-09-03 | International Business Machines Corporation | Method and apparatus for representing host datastream screen image information using markup languages |
US6629129B1 (en) * | 1999-06-16 | 2003-09-30 | Microsoft Corporation | Shared virtual meeting services among computer applications |
US6766298B1 (en) * | 1999-09-03 | 2004-07-20 | Cisco Technology, Inc. | Application server configured for dynamically generating web pages for voice enabled web applications |
GB2357226B (en) * | 1999-12-08 | 2003-07-16 | Hewlett Packard Co | Security protocol |
GB2357229B (en) * | 1999-12-08 | 2004-03-17 | Hewlett Packard Co | Security protocol |
GB2357227B (en) * | 1999-12-08 | 2003-12-17 | Hewlett Packard Co | Security protocol |
US7590644B2 (en) * | 1999-12-21 | 2009-09-15 | International Business Machine Corporation | Method and apparatus of streaming data transformation using code generator and translator |
US6877023B1 (en) * | 2000-01-28 | 2005-04-05 | Softwired, Inc. | Messaging system for delivering data in the form of portable message formats between message clients |
US6658596B1 (en) * | 2000-03-13 | 2003-12-02 | International Business Machines Corporation | Automated queue recovery using element- based journaling |
US7155455B2 (en) * | 2000-03-24 | 2006-12-26 | Inner Circle Logistics, Inc. | Method and system for business information networks |
JP2004514192A (en) * | 2000-04-03 | 2004-05-13 | スターク ジュールゲン | Method and system for performing content-controlled electronic message processing |
US7111076B2 (en) * | 2000-04-13 | 2006-09-19 | Intel Corporation | System using transform template and XML document type definition for transforming message and its reply |
US7065769B1 (en) * | 2000-06-30 | 2006-06-20 | Intel Corporation | Method for automatically installing and updating drivers |
US7024413B2 (en) * | 2000-07-26 | 2006-04-04 | International Business Machines Corporation | Method of externalizing legacy database in ASN.1-formatted data into XML format |
US6944662B2 (en) * | 2000-08-04 | 2005-09-13 | Vinestone Corporation | System and methods providing automatic distributed data retrieval, analysis and reporting services |
US6910216B2 (en) * | 2000-08-08 | 2005-06-21 | International Business Machines Corporation | IMS transaction messages metamodel |
US6775680B2 (en) * | 2000-08-08 | 2004-08-10 | International Business Machines Corporation | High level assembler metamodel |
US7130885B2 (en) * | 2000-09-05 | 2006-10-31 | Zaplet, Inc. | Methods and apparatus providing electronic messages that are linked and aggregated |
US20020032646A1 (en) * | 2000-09-08 | 2002-03-14 | Francis Sweeney | System and method of automated brokerage for risk management services and products |
US6948000B2 (en) * | 2000-09-22 | 2005-09-20 | Narad Networks, Inc. | System and method for mapping end user identifiers to access device identifiers |
US6551322B1 (en) * | 2000-10-05 | 2003-04-22 | The Cleveland Clinic Foundation | Apparatus for implantation into bone |
US6941560B1 (en) * | 2000-12-19 | 2005-09-06 | Novell, Inc. | XML-based integrated services event system |
US7130883B2 (en) * | 2000-12-29 | 2006-10-31 | Webex Communications, Inc. | Distributed network system architecture for collaborative computing |
US6839846B2 (en) * | 2001-01-03 | 2005-01-04 | Intel Corporation | Embedding digital signatures into digital payloads |
US20060036941A1 (en) * | 2001-01-09 | 2006-02-16 | Tim Neil | System and method for developing an application for extending access to local software of a wireless device |
WO2002057917A2 (en) * | 2001-01-22 | 2002-07-25 | Sun Microsystems, Inc. | Peer-to-peer network computing platform |
US7222156B2 (en) * | 2001-01-25 | 2007-05-22 | Microsoft Corporation | Integrating collaborative messaging into an electronic mail program |
WO2002065329A1 (en) * | 2001-02-14 | 2002-08-22 | The Escher Group, Ltd. | Peer-to peer enterprise storage |
US20040268344A1 (en) * | 2001-03-08 | 2004-12-30 | Sridhar Obilisetty | Centrally managed and distributed applications |
US6983326B1 (en) * | 2001-04-06 | 2006-01-03 | Networks Associates Technology, Inc. | System and method for distributed function discovery in a peer-to-peer network environment |
US6732095B1 (en) * | 2001-04-13 | 2004-05-04 | Siebel Systems, Inc. | Method and apparatus for mapping between XML and relational representations |
US7134075B2 (en) * | 2001-04-26 | 2006-11-07 | International Business Machines Corporation | Conversion of documents between XML and processor efficient MXML in content based routing networks |
US20030061279A1 (en) * | 2001-05-15 | 2003-03-27 | Scot Llewellyn | Application serving apparatus and method |
US6478798B1 (en) * | 2001-05-17 | 2002-11-12 | Robert S. Howland | Spinal fixation apparatus and methods for use |
US20030093471A1 (en) * | 2001-10-18 | 2003-05-15 | Mitch Upton | System and method using asynchronous messaging for application integration |
WO2003038548A2 (en) * | 2001-10-18 | 2003-05-08 | Vitria Technology, Inc. | Model driven collaborative business application development environment and collaborative applications developed therewith |
US20030088571A1 (en) * | 2001-11-08 | 2003-05-08 | Erik Ekkel | System and method for a peer-to peer data file service |
US7139809B2 (en) * | 2001-11-21 | 2006-11-21 | Clearcube Technology, Inc. | System and method for providing virtual network attached storage using excess distributed storage capacity |
US6978305B1 (en) * | 2001-12-19 | 2005-12-20 | Oracle International Corp. | Method and apparatus to facilitate access and propagation of messages in communication queues using a public network |
US7290267B2 (en) * | 2002-01-23 | 2007-10-30 | International Business Machines Corporation | Multi-protocol object distribution |
US7051102B2 (en) * | 2002-04-29 | 2006-05-23 | Microsoft Corporation | Peer-to-peer name resolution protocol (PNRP) security infrastructure and method |
US20030208534A1 (en) * | 2002-05-02 | 2003-11-06 | Dennis Carmichael | Enhanced productivity electronic meeting system |
US20030217094A1 (en) * | 2002-05-17 | 2003-11-20 | Anthony Dean Andrews | Correlation framework |
US7610404B2 (en) * | 2002-05-22 | 2009-10-27 | Cast Iron Systems, Inc. | Application network communication method and apparatus |
US7574488B2 (en) * | 2002-05-31 | 2009-08-11 | Hitachi, Ltd. | Method and apparatus for peer-to-peer file sharing |
US7376959B2 (en) * | 2002-06-27 | 2008-05-20 | Siebel Systems, Inc. | Method and system for outbound web services |
US7130893B2 (en) * | 2003-05-19 | 2006-10-31 | International Business Machines Corporation | System and method for representing MFS control blocks in XML for MFS-based IMS applications |
US20040068553A1 (en) * | 2002-10-07 | 2004-04-08 | International Business Machines Corporation | Dynamically selecting a Web service container for hosting remotely instantiated Web services |
US7363377B1 (en) * | 2002-10-09 | 2008-04-22 | Unisys Corporation | Method for protecting the program environment of a microsoft component object model (COM) client |
US20040103144A1 (en) * | 2002-11-26 | 2004-05-27 | Hussein Sallam | Systems and methods for communicating with devices as Web Services |
-
2003
- 2003-09-15 US US10/662,968 patent/US20040098458A1/en not_active Abandoned
- 2003-09-15 US US10/662,955 patent/US20040104927A1/en not_active Abandoned
- 2003-09-15 US US10/662,933 patent/US7370336B2/en active Active
- 2003-09-15 US US10/662,954 patent/US20040098728A1/en not_active Abandoned
- 2003-09-15 WO PCT/US2003/028918 patent/WO2004025466A2/en active Search and Examination
- 2003-09-15 US US10/662,932 patent/US20040098717A1/en not_active Abandoned
- 2003-09-15 US US10/662,889 patent/US7430616B2/en active Active
- 2003-09-15 US US10/662,936 patent/US7434220B2/en active Active
- 2003-09-15 AU AU2003272404A patent/AU2003272404A1/en not_active Abandoned
Patent Citations (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6658526B2 (en) | 1997-03-12 | 2003-12-02 | Storage Technology Corporation | Network attached virtual data storage subsystem |
US5987376A (en) | 1997-07-16 | 1999-11-16 | Microsoft Corporation | System and method for the distribution and synchronization of data and state information between clients in a distributed processing system |
US5960404A (en) * | 1997-08-28 | 1999-09-28 | International Business Machines Corp. | Mechanism for heterogeneous, peer-to-peer, and disconnected workflow operation |
US6453356B1 (en) * | 1998-04-15 | 2002-09-17 | Adc Telecommunications, Inc. | Data exchange system and method |
US20040031037A1 (en) * | 1998-04-28 | 2004-02-12 | Mikio Ikoma | Apparatus and method for use in distributed computing environment for converting data format between program language-specific format used in respective computers and stream format used for communication among computers |
US6353608B1 (en) | 1998-06-16 | 2002-03-05 | Mci Communications Corporation | Host connect gateway for communications between interactive voice response platforms and customer host computing applications |
WO2000039713A1 (en) | 1998-12-28 | 2000-07-06 | Gemteq Software, Inc. | A method and system for performing electronic data-gathering across multiple data sources |
US6564246B1 (en) | 1999-02-02 | 2003-05-13 | International Business Machines Corporation | Shared and independent views of shared workspace for real-time collaboration |
US7024669B1 (en) * | 1999-02-26 | 2006-04-04 | International Business Machines Corporation | Managing workload within workflow-management-systems |
EP1043671A2 (en) | 1999-03-19 | 2000-10-11 | International Business Machines Corporation | Message broker providing a publish/subscribe service and method of processing messages in a publish/subscribe environment |
WO2001006365A2 (en) | 1999-07-19 | 2001-01-25 | Groove Networks, Inc. | Method and apparatus for activity-based collaboration by a computer system equipped with a communications manager |
US6879995B1 (en) * | 1999-08-13 | 2005-04-12 | Sun Microsystems, Inc. | Application server message logging |
US6671737B1 (en) * | 1999-09-24 | 2003-12-30 | Xerox Corporation | Decentralized network system |
US6643652B2 (en) | 2000-01-14 | 2003-11-04 | Saba Software, Inc. | Method and apparatus for managing data exchange among systems in a network |
US6845507B2 (en) * | 2000-05-18 | 2005-01-18 | Ss & C Technologies, Inc. | Method and system for straight through processing |
US6604104B1 (en) * | 2000-10-02 | 2003-08-05 | Sbi Scient Inc. | System and process for managing data within an operational data store |
WO2002032171A1 (en) | 2000-10-09 | 2002-04-18 | Telstra New Wave Pty Ltd | Message processing |
US7127613B2 (en) * | 2002-02-25 | 2006-10-24 | Sun Microsystems, Inc. | Secured peer-to-peer network data exchange |
US7168077B2 (en) * | 2003-01-31 | 2007-01-23 | Handysoft Corporation | System and method of executing and controlling workflow processes |
US20040254945A1 (en) * | 2003-05-16 | 2004-12-16 | Patrick Schmidt | Business process management for a message-based exchange infrastructure |
Non-Patent Citations (2)
Title |
---|
International Search Report, International Application No. PCT/US03/28918, mailed Jun. 15, 2004. |
International Search Report, International Application No. PCT/US03/28918, mailed Oct. 22, 2004. |
Cited By (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060075407A1 (en) * | 2004-10-06 | 2006-04-06 | Digipede Technologies, Llc | Distributed system interface |
US20080027948A1 (en) * | 2006-07-06 | 2008-01-31 | Richard Corley | Managing Application System Load |
US9977721B2 (en) | 2007-12-20 | 2018-05-22 | Netapp, Inc. | Evaluating and predicting computer system performance using kneepoint analysis |
US8457034B2 (en) | 2008-06-17 | 2013-06-04 | Raytheon Company | Airborne communication network |
US20090310531A1 (en) * | 2008-06-17 | 2009-12-17 | Raytheon Company | Airborne Communication Network |
US20100250298A1 (en) * | 2009-03-25 | 2010-09-30 | International Business Machines Corporation | Prioritization enablement for soa governance |
US20110078681A1 (en) * | 2009-09-30 | 2011-03-31 | International Business Machines Corporation | Method and system for running virtual machine image |
US20110082940A1 (en) * | 2009-10-02 | 2011-04-07 | Michael Peter Montemurro | Methods and apparatus to establish peer-to-peer communications |
US9949305B2 (en) * | 2009-10-02 | 2018-04-17 | Blackberry Limited | Methods and apparatus for peer-to-peer communications in a wireless local area network |
US10681757B2 (en) | 2009-10-02 | 2020-06-09 | Blackberry Limited | Method and apparatus for peer-to-peer communications in a wireless local area network including the negotiation and establishment of a peer-to-peer connection between peers based on capability information |
US20110082939A1 (en) * | 2009-10-02 | 2011-04-07 | Michael Peter Montemurro | Methods and apparatus to proxy discovery and negotiations between network entities to establish peer-to-peer communications |
US9183560B2 (en) | 2010-05-28 | 2015-11-10 | Daniel H. Abelow | Reality alternate |
US11222298B2 (en) | 2010-05-28 | 2022-01-11 | Daniel H. Abelow | User-controlled digital environment across devices, places, and times with continuous, variable digital boundaries |
US9444889B1 (en) * | 2013-02-08 | 2016-09-13 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
US9753654B1 (en) | 2013-02-08 | 2017-09-05 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
US10019316B1 (en) | 2013-02-08 | 2018-07-10 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
US10067830B1 (en) * | 2013-02-08 | 2018-09-04 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
US10521301B1 (en) | 2013-02-08 | 2019-12-31 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
US9612906B1 (en) | 2013-02-08 | 2017-04-04 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
US10810081B1 (en) * | 2013-02-08 | 2020-10-20 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
US11093328B1 (en) | 2013-02-08 | 2021-08-17 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
US9392060B1 (en) | 2013-02-08 | 2016-07-12 | Quantcast Corporation | Managing distributed system performance using accelerated data retrieval operations |
Also Published As
Publication number | Publication date |
---|---|
US20050060432A1 (en) | 2005-03-17 |
US7430616B2 (en) | 2008-09-30 |
US20040098728A1 (en) | 2004-05-20 |
WO2004025466A3 (en) | 2004-12-16 |
US20040107420A1 (en) | 2004-06-03 |
AU2003272404A1 (en) | 2004-04-30 |
US20040098458A1 (en) | 2004-05-20 |
US20040098729A1 (en) | 2004-05-20 |
US20040104927A1 (en) | 2004-06-03 |
US20040098717A1 (en) | 2004-05-20 |
WO2004025466A2 (en) | 2004-03-25 |
US7370336B2 (en) | 2008-05-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US7434220B2 (en) | Distributed computing infrastructure including autonomous intelligent management system | |
US7139809B2 (en) | System and method for providing virtual network attached storage using excess distributed storage capacity | |
US7318095B2 (en) | Data fail-over for a multi-computer system | |
JP6514308B2 (en) | Failover and Recovery for Replicated Data Instances | |
US11816003B2 (en) | Methods for securely facilitating data protection workflows and devices thereof | |
US8655851B2 (en) | Method and system for performing a clean file lock recovery during a network filesystem server migration or failover | |
US8533171B2 (en) | Method and system for restarting file lock services at an adoptive node during a network filesystem server migration or failover | |
US7689862B1 (en) | Application failover in a cluster environment | |
US9311328B2 (en) | Reference volume for initial synchronization of a replicated volume group | |
JP5443613B2 (en) | Provision and manage replicated data instances | |
US9043391B2 (en) | Capturing and restoring session state of a machine without using memory images | |
US9992155B2 (en) | DNS alias synchronization in replication topology | |
WO2019152117A1 (en) | Systems and methods for synchronizing microservice data stores | |
US20120011509A1 (en) | Migrating Session State of a Machine Without Using Memory Images | |
US20100228819A1 (en) | System and method for performance acceleration, data protection, disaster recovery and on-demand scaling of computer applications | |
JP6279744B2 (en) | How to queue email web client notifications | |
US8683258B2 (en) | Fast I/O failure detection and cluster wide failover | |
US20160056996A1 (en) | System and Method for Implementing High Availability of Server in Cloud Environment | |
Stanek | Microsoft Exchange Server 2010 Administrator's Pocket Consultant | |
Youn et al. | The approaches for high available and fault-tolerant cluster systems | |
Vallath et al. | Testing for Availability | |
WO2013147784A1 (en) | Dns alias synchronization in replication topology |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: CLEARCUBE TECHNOLOGY, INC., TEXASFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HUSAIN, SYED MOHAMMAD AMIR;ENRIGHT, TODD JOHN;THORNTON, BARRY W.;REEL/FRAME:014869/0752;SIGNING DATES FROM 20030925 TO 20031013 |
|
AS | Assignment |
Owner name: CLEARCUBE TECHNOLOGY, INC., TEXASFree format text: CORRECTIVE TO CORRECT THE SERIAL NUMBER ON A DOCUMENT PREVIOUSLY RECORDED AT REEL 014869 FRAME 0752. (ASSIGNMENT OF ASSIGNOR'S INTEREST);ASSIGNORS:HUSAIN, SYED MOHAMMAD AMIR;ENRIGHT, TODD JOHN;THORNTON, BARRY W.;REEL/FRAME:015933/0035;SIGNING DATES FROM 20030925 TO 20031013 |
|
AS | Assignment |
Owner name: HORIZON TECHNOLOGY FUNDING COMPANY LLC, CONNECTICUFree format text: SECURITY AGREEMENT;ASSIGNOR:CLEARCUBE TECHNOLOGY, INC.;REEL/FRAME:016862/0048Effective date: 20050524 |
|
AS | Assignment |
Owner name: COMERICA BANK, CALIFORNIAFree format text: SECURITY AGREEMENT;ASSIGNOR:CLEARCUBE TECHNOLOGY, INC.;REEL/FRAME:016621/0707Effective date: 20050721 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: BRIDGE BANK, NATIONAL ASSOCIATION, CALIFORNIAFree format text: SECURITY INTEREST;ASSIGNOR:CLEARCUBE TECHNOLOGY, INC.;REEL/FRAME:021645/0719Effective date: 20080806 |
|
AS | Assignment |
Owner name: CLEARCUBE TECHNOLOGY INC., TEXASFree format text: RELEASE BY SECURED PARTY;ASSIGNOR:COMERICA BANK;REEL/FRAME:021679/0175Effective date: 20081003 |
|
AS | Assignment |
Owner name: HORIZON TECHNOLOGY FUNDING COMPANY LLC,CONNECTICUTFree format text: RELEASE;ASSIGNOR:CLEARCUBE TECHNOLOGY, INC.;REEL/FRAME:024358/0521Effective date: 20100427 |
|
AS | Assignment |
Owner name: ROOSTER ROW, LLC, NEW JERSEYFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:CLEARCUBE TECHNOLOGY INC.;REEL/FRAME:024927/0820Effective date: 20100526 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:ROOSTER ROW, LLC;REEL/FRAME:026867/0348Effective date: 20110825 |
|
AS | Assignment |
Owner name: CLEARCUBE TECHNOLOGY, INC., TEXASFree format text: RELEASE BY SECURED PARTY;ASSIGNOR:BRIDGE BANK, NATIONAL ASSOCIATION;REEL/FRAME:027135/0663Effective date: 20110921 |
|
FEPP | Fee payment procedure |
Free format text: PAYOR NUMBER ASSIGNED (ORIGINAL EVENT CODE: ASPN); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FEPP | Fee payment procedure |
Free format text: PAT HOLDER NO LONGER CLAIMS SMALL ENTITY STATUS, ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: STOL); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044127/0735Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 12 |