CN117136405A - Automated assistant response generation using large language models - Google Patents
Automated assistant response generation using large language models Download PDFInfo
- Publication number
- CN117136405A CN117136405A CN202180096665.5A CN202180096665A CN117136405A CN 117136405 A CN117136405 A CN 117136405A CN 202180096665 A CN202180096665 A CN 202180096665A CN 117136405 A CN117136405 A CN 117136405A
- Authority
- CN
- China
- Prior art keywords
- assistant
- outputs
- llm
- additional
- output
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000004044 response Effects 0.000 title claims abstract description 45
- 238000000034 method Methods 0.000 claims abstract description 171
- 238000012545 processing Methods 0.000 claims abstract description 102
- 230000015654 memory Effects 0.000 claims description 32
- 230000008569 process Effects 0.000 abstract description 51
- 230000004048 modification Effects 0.000 description 21
- 238000012986 modification Methods 0.000 description 21
- 238000010801 machine learning Methods 0.000 description 18
- 230000000007 visual effect Effects 0.000 description 17
- 238000009877 rendering Methods 0.000 description 10
- 238000004891 communication Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 230000000694 effects Effects 0.000 description 6
- 230000009471 action Effects 0.000 description 5
- 235000013305 food Nutrition 0.000 description 5
- 238000005516 engineering process Methods 0.000 description 4
- 230000007613 environmental effect Effects 0.000 description 4
- 235000012736 patent blue V Nutrition 0.000 description 4
- 230000026676 system process Effects 0.000 description 4
- 240000008574 Capsicum frutescens Species 0.000 description 3
- 241000282412 Homo Species 0.000 description 3
- 241001272720 Medialuna californiensis Species 0.000 description 3
- 238000001514 detection method Methods 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000002452 interceptive effect Effects 0.000 description 3
- 238000013507 mapping Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 230000033764 rhythmic process Effects 0.000 description 2
- 238000013518 transcription Methods 0.000 description 2
- 230000035897 transcription Effects 0.000 description 2
- 241000288105 Grus Species 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 230000005059 dormancy Effects 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 239000012530 fluid Substances 0.000 description 1
- 230000002496 gastric effect Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 230000011273 social behavior Effects 0.000 description 1
- 238000002604 ultrasonography Methods 0.000 description 1
Abstract
As part of a conversational session between a user and an automated assistant, embodiments may: receiving an audio data stream that captures a spoken utterance including an assistant query; determining a set of assistant outputs based on processing the audio data stream, each of the assistant outputs being predicted to be responsive to an assistant query; generating a modified set of assistant outputs using a Large Language Model (LLM) output to process the assistant outputs and the context of the dialog session; and causing a given modified assistant output from among the set of modified assistant outputs to be provided for presentation to the user in response to the spoken utterance. In some implementations, LLM output may be generated offline for subsequent use in an online manner. In additional or alternative implementations, the LLM output can be generated in an online manner when the spoken utterance is received.
Description
Background
Humans may conduct man-machine conversations with interactive software applications referred to herein as "automated assistants" (also referred to as "chat robots," "interactive personal assistants," "intelligent personal assistants," "personal voice assistants," "conversation agents," etc.). Automated assistants typically rely on a series of components to interpret and respond to spoken utterances. For example, an Automatic Speech Recognition (ASR) engine may process audio data corresponding to a spoken utterance of a user to generate an ASR output, such as ASR hypotheses (i.e., sequences of terms and/or other markers) of the spoken utterance. Further, a Natural Language Understanding (NLU) engine may process ASR output (or touch/enter input) to generate NLU output, such as a request (e.g., intent) expressed by a user in providing spoken utterances (or touch/enter input) and optionally slot values of parameters associated with the intent. Finally, the NLU output may be processed by various fulfillment components to generate fulfillment output, such as responsive content in response to the spoken utterance and/or one or more actions that may be performed in response to the spoken utterance.
In general, a dialog session with an automated assistant is initiated by a user providing a spoken utterance, and the automated assistant can respond to the spoken utterance using the aforementioned series of components. The user may continue the conversation session by providing additional spoken utterances, and the automated assistant may respond again to the additional spoken utterances using the series of components described previously. In other words, these conversation sessions are typically turn-based in that the user provides spoken utterances in turns in the conversation session, the automated assistant responds to spoken utterances in turns in the conversation session, the user provides additional spoken utterances in additional turns in the conversation session, the automated assistant responds to additional spoken utterances in additional turns in the conversation session, and so on. However, from the perspective of the user, these round-based dialog sessions may be unnatural, as they do not reflect how humans actually talk to each other.
For example, if a first person provides a spoken utterance during a conversation session to convey an initial idea to a second person (e.g., "I'm going to the beach today (I want to go to the beach today)"), the second person may formulate a response to the first person in view of the spoken utterance in the context of the conversation session (e.g., "sound fun, what are you going to do at the beach. Notably, the second person, in response to the first person, can provide a spoken utterance that causes the first person to continue the conversation session in a natural manner. In other words, during the conversation session, both the first person and the second person may provide spoken utterances to facilitate a natural conversation, and without one of the persons driving the conversation session.
However, if the second person is replaced by an automated assistant in the above example, the automated assistant may not be able to provide a response that causes the first person to continue the conversation session. For example, in response to a first person providing a spoken utterance of "I'm going to the beach today", the automated assistant may simply respond to "sound" or "nice" without providing any additional response to facilitate the conversation session, and while the automated assistant is able to perform some action and/or provide some response to facilitate the conversation session, such as actively asking the first person what they intend to do at the beach, actively looking for weather forecast for the beach that the first person frequently accesses, and including weather forecast in the response, actively making some inferences based on weather forecast, etc. Thus, the response provided by the automated assistant in response to the spoken utterance of the first person may not resonate with the first person because the response may not reflect natural conversation between multiple persons. Furthermore, the first person may have to provide additional spoken utterances to explicitly request certain information that the automated assistant may actively provide (e.g., weather forecast for the beach), thereby increasing the number of spoken utterances directed to the automated assistant and wasting computing resources for the client device that processes the spoken utterances.
Disclosure of Invention
Embodiments described herein relate to enabling an automated assistant to perform natural conversations with a user during a conversation session. Some implementations may receive an audio data stream that captures a spoken utterance of a user. The audio data stream may be generated by one or more microphones of the client device, and the spoken utterance may include an assistant query. Some implementations may further determine a set of assistant outputs based on processing the audio data stream, and process the set of assistant outputs and a context of the conversation session to generate a modified set of assistant outputs using one or more Large Language Model (LLM) outputs generated using LLM. Each of the one or more LLM outputs can be determined based on at least a portion of the context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs. Some implementations may also cause a given modified assistant output from among the set of modified assistant outputs to be provided for presentation to the user. Further, each of the one or more LLM outputs can include, for example, a probability distribution of a sequence of one or more words and/or phrases across the one or more words, and the one or more words and/or phrases in the sequence can be selected as the one or more LLM outputs based on the probability distribution. Further, the context of the conversation session may be determined based on one or more context signals including, for example, time of day, day of week, location of the client device, environmental noise detected in the environment of the client device, user profile data, software application data, environmental data regarding the known environment of the user of the client device, conversation history of the conversation session between the user and the automated assistant, and/or other context signals.
In some implementations, the set of assistant outputs may be determined based on processing the audio data stream using a streaming Automatic Speech Recognition (ASR) model to generate an ASR output stream, such as one or more recognized terms or phrases predicted to correspond to the spoken utterance, one or more phonemes predicted to correspond to the spoken utterance, one or more prediction metrics associated with each of the one or more recognized terms or phrases and/or the one or more predicted phonemes, and/or other ASR outputs. Further, the ASR output may be processed using a Natural Language Understanding (NLU) model to generate an NLU output stream, such as one or more predicted intentions of a user in providing the spoken utterance and one or more corresponding slot values of one or more parameters associated with each of the one or more predicted intentions. Further, the NLU data stream may be processed by one or more first party (1P) and/or third party (3P) systems to generate a set of assistant outputs. As used herein, one or more 1P systems include systems developed and/or maintained by the same entity (e.g., a common publisher) that developed and/or maintained the automated assistant described herein, while one or more 3P systems include systems developed and/or maintained by an entity that is different from the entity that developed and/or maintained the automated assistant described herein. Notably, the set of assistant outputs described herein includes assistant outputs that are generally considered for responding to spoken utterances. However, by using the claimed techniques, the set of assistant outputs generated in the manner described above can be further processed to generate a modified set of assistant outputs. In particular, one or more LLM outputs can be used to modify the set of assistant outputs, and a given modified assistant output can be selected from the set of modified assistant outputs to be provided for presentation to a user in response to receiving the spoken utterance.
For example, assume that a user of a client device provides a spoken utterance of "Hey Assistant, I'm thinking about going surfing today (Hey Assistant, I are considering surfing today)". In this example, the automated assistant can process the spoken utterance in the manner described above to generate a set of assistant outputs and a modified set of assistant outputs. In this example, the assistant outputs included in the set of assistant outputs may include, for example, "That sounds like fun-! (what Sounds interesting! (sound interesting |) "etc. Further, in this example, the assistant outputs included in the modified set of assistant outputs may include, for example, "That sounds like fun, how long have you been surfing? (what is interesting to sound, how long you surf) "," enter it, but you're going to Example Beach again, be prepared for some light showers (Enjoy it, but if you want to go to the example beach again, ready to answer a small gust of rain) ", etc. Notably, the assistant outputs included in the set of assistant outputs do not include any assistant outputs that drive the conversation session in a manner that further enables the user of the client device to conduct the conversation session, but the assistant outputs included in the modified set of assistant outputs include assistant outputs that drive the conversation session in a manner that further enables the user of the client device to conduct the conversation session by asking for contextually relevant questions (e.g., "how long have you been surfing.
In some implementations, the modified set of assistant responses can be generated using one or more LLM outputs generated in an online manner. For example, in response to receiving a spoken utterance, the automated assistant may cause a set of assistant outputs to be generated in the manner described above. Further, and in response to receiving the spoken utterance, the automated assistant can cause processing of the set of assistant outputs using the one or more LLMs, the context of the conversation session, and/or the assistant query included in the spoken utterance to generate a modified set of assistant outputs based on the one or more LLM outputs generated using the one or more LLMs.
In additional or alternative embodiments, the modified set of assistant responses may be generated using one or more LLM outputs generated in an offline manner. For example, before receiving the spoken utterance, the automated assistant may obtain a plurality of assistant queries and a corresponding context for a corresponding previous dialog session for each of the plurality of assistant queries from an assistant activity database (which may be a limited assistant activity of the user of the client device). Furthermore, the automated assistant may cause a set of assistant outputs to be generated for a given assistant query of the plurality of assistant queries in the manner described above and for the given assistant query. Further, the automated assistant can cause the one or more LLMs to process the set of assistant outputs, the corresponding context of the conversation session, and/or the given assistant query to generate a modified set of assistant outputs based on the one or more LLM outputs generated using the one or more LLMs. The process may be repeated for each of the plurality of queries and the corresponding context of the previous dialog session obtained by the automated assistant.
In addition, the automation assistant can index one or more LLM outputs in a memory accessible by the user's client device. In some implementations, the automated assistant can cause one or more LLMs to be indexed in memory based on one or more terms included in the plurality of assistant queries. In additional or alternative implementations, the automated assistant can generate a corresponding embedding (e.g., a word2vec embedding or another low-dimensional representation) for each of the plurality of assistant queries, and map each of the corresponding embedding to the assistant query embedding space to index one or more LLM outputs. In additional or alternative embodiments, the automated assistant can cause the one or more LLMs to be indexed in memory based on one or more context signals included in the corresponding previous context. In additional or alternative implementations, the automation assistant can generate corresponding embeddings for each of the corresponding contexts and map each of the corresponding embeddings to the context embedding space to index one or more LLM outputs. In additional or alternative implementations, the automated assistant can cause the one or more LLMs to be indexed in memory based on one or more terms or phrases of the assistant output included in the set of assistant outputs for each of the plurality of assistant queries. In additional or alternative implementations, the automated assistant can generate a corresponding embedding (e.g., word2vec embedding, or another low-dimensional representation) for each assistant output included in the set of assistant outputs, and map each corresponding embedding to the assistant output embedding space to index one or more LLM outputs.
Thus, when a spoken utterance is subsequently received at a client device of a user, an automated assistant may identify one or more LLM outputs that were previously generated based on a current assistant query corresponding to one or more assistant queries included in the plurality of queries, a current context corresponding to one or more previous contexts, and/or one or more current assistant outputs corresponding to one or more previous assistant outputs. For example, in embodiments that index one or more LLM outputs based on corresponding embeddings of previous assistant queries, the automated assistant may cause an embedment of the current assistant query to be generated and mapped to the assistant query embedment space. Further, the automated assistant may determine that the current assistant query corresponds to the previous assistant query based on a distance between an embedding of the current assistant query and a corresponding embedding of the previous assistant query in the query embedding space meeting a threshold. The automated assistant can obtain one or more LLM outputs from the memory that are generated based on processing the previous assistant query and utilize the one or more LLM outputs to generate a modified set of assistant outputs. Further, for example, in embodiments in which one or more LLMs are indexed based on one or more terms included in a plurality of assistant queries, an automated assistant can determine, for example, an edit distance between a current assistant query and a plurality of previous assistants to identify a previous assistant query corresponding to the current assistant query. Similarly, the automated assistant can obtain one or more LLM outputs from the memory that are generated based on processing the previous assistant query and utilize the one or more LLM outputs to generate a modified set of assistant outputs.
In some implementations, and in addition to one or more LLM outputs, additional assistant queries can be generated based on the context in which the assistant queries and/or conversation sessions are processed. For example, in processing the context of the assistant query and/or the dialog session, the automated assistant may determine intent associated with a given assistant query based on the NLU data stream. Further, the automated assistant can identify at least one relevant intent that is relevant to the intent associated with the assistant query based on the intent associated with the given assistant query (e.g., based on a mapping of the intent to at least one relevant intent in a database or memory accessible to the client device and/or based on processing the intent associated with the given assistant query using one or more Machine Learning (ML) models or heuristically defined rules). Further, the automated assistant can generate additional assistant queries based on the at least one related intent. For example, suppose that the assistant query indicates that the user is going to the beach (e.g., "Hey assistant, I'm going to the beach today (Hey assistant, I want to go to the beach today)"). In this example, the additional assistant query may correspond to, for example, "what s the weather at Example Beach? (how does the weather of the Example Beach), "for Example, to actively determine weather information of a Beach named Example Beach that is commonly accessed by a user. Notably, no additional assistant queries may be provided for presentation to the user of the client device.
Rather, in these embodiments, additional assistant outputs may be determined based on processing additional assistant queries. For example, the automated assistant may transmit a structured request to one or more 1P and/or 3P systems to obtain weather information as additional assistant output. Further assume that the weather information indicates that the example beach is expected to rain. In some versions of these embodiments, the automated assistant may further cause the additional assistant to be processed using the one or more LLM outputs and/or the one or more additional LLM outputs to generate an additional modified set of assistant outputs. Thus, in the initial example provided above, the given modified Assistant output from the initial set of modified Assistant outputs that was provided to the user in response to receiving the spoken utterance of "Hey Assistant, I'm thinking about going surfing today" may be "Enjoit" and the given additional modified Assistant output from the additional set of modified Assistant outputs may be "but if you're going to Example Beach again, be prepared for some light showers". In other words, automated assistant
In various implementations, when generating the modified set of assistant outputs, each of the utilized one or more LLM outputs can be generated using a corresponding set of parameters of the plurality of distinct sets of parameters. Each parameter of the plurality of distinct parameter sets may be associated with a distinct personality of the automated assistant. In some versions of those embodiments, a single LLM may be utilized to generate one or more corresponding LLM outputs using the corresponding parameter sets for each distinct personality, while in other versions of those embodiments, multiple LLMs may be utilized to generate one or more corresponding LLM outputs using the corresponding parameter sets for each distinct personality. Thus, when a given modified assistant output from the set of modified assistant outputs is provided for presentation to the user, it may reflect various dynamic contextual personalities via prosodic attributes of different personalities (e.g., intonation, pause, speed of speech, accent, rhythm, etc. of these different personalities).
Notably, these personality replies described herein reflect not only prosodic attributes of different personalities, but may also reflect different vocabularies of different personalities and/or different speaking styles of different personalities (e.g., lengthy speaking styles, succinct speaking styles, etc.). For example, a given modified assistant output provided for presentation to the user may be generated using a first set of parameters reflecting a first personality of the automated assistant in terms of a first vocabulary used by the automated assistant and/or a first set of prosodic attributes for providing the modified assistant output for audible presentation to the user. Alternatively, the modified assistant output provided for presentation to the user may be generated using a second set of parameters reflecting a second personality of the automated assistant in terms of a second vocabulary used by the automated assistant and/or a second set of prosodic attributes for providing the modified assistant output for audible presentation to the user.
Thus, the automated assistant may dynamically adjust the personality used in providing the modified assistant output for presentation to the user based on both the vocabulary used by the automated assistant and the prosodic attributes used in rendering the modified assistant output for audible presentation to the user. Notably, the automated assistant can dynamically adjust these personalities used in providing the modified assistant output based on the context of the conversation session, including the previous spoken utterances received from the user and the previous assistant output provided by the automated assistant and/or any other contextual signals described herein. As a result, the modified assistant output provided by the automated assistant may better resonate with the user of the client device. Furthermore, it should be noted that the personality used in a given dialog session may be dynamically adjusted as the context of the given dialog session is updated.
In some implementations, the automated assistant can rank the assistant outputs (i.e., not generated using one or more LLM outputs) and the modified assistant outputs (i.e., generated using one or more LLM outputs) included in the set of assistant outputs according to one or more ranking criteria. Thus, upon selecting a given assistant output to provide for presentation to a user, the automated assistant may select from both the set of assistant outputs and the modified set of assistant outputs. The one or more ranking criteria may include, for example: one or more prediction metrics (e.g., ASR metrics generated when generating an ASR output stream, NLU degrees generated when generating an NLU output stream, performance metrics generated when generating a set of assistant outputs) -indicating how responsive each assistant output included in the set of assistant outputs and the modified set of assistant outputs is predicted to an assistant query included in the spoken utterance; one or more intents included in the NLU output stream; and/or other ranking criteria. For example, if the intent of the user of the client device indicates that the user wants a factual answer (e.g., based on providing a spoken utterance of an assistant query that includes "why is the sky blue. However, if the intent of the user of the client device indicates that the user provided more open input (e.g., based on providing a spoken utterance of an assistant query that includes "what is time it.
In some implementations, and prior to generating the modified set of assistant outputs, the automated assistant can determine whether to even generate the modified set of assistant outputs. In some versions of those implementations, the automated assistant may determine whether to even generate the modified set of assistant outputs based on one or more predicted intentions of the user in providing the spoken utterance as indicated by the NLU data stream. For example, in embodiments in which the automated assistant determines that the spoken utterance requests the automated assistant to perform a search (e.g., assistant query "Why is the sky blue. In additional or alternative versions of those embodiments, the automated assistant may determine whether to even generate the modified set of assistant outputs based on one or more computational costs associated with modifying the one or more assistant outputs. The one or more computational costs associated with modifying the one or more assistant outputs may include, for example, one or more of the following: battery consumption associated with modifying one or more of the assistant outputs, processor consumption, or latency associated with modifying one or more of the assistant outputs. For example, if the client device is in a low power mode, the automated assistant may determine not to generate a modified set of assistant outputs to reduce battery consumption of the client device.
One or more technical advantages may be realized by using the techniques described herein. As one non-limiting example, the techniques described herein enable an automated assistant to conduct natural conversations with a user during a conversation session. For example, the automated assistant can use one or more LLM outputs that are more conversational in nature to generate a modified assistant output. Thus, the automated assistant can proactively provide contextual information associated with the dialog session that is not directly requested by the user (e.g., by generating additional assistant queries as described herein and by providing additional assistant outputs that are determined based on the additional assistant queries), such that the modified assistant outputs resonate with the user. Further, modified assistant outputs having various personalities may be generated from the vocabulary adjusted in context throughout the dialog session and from prosodic attributes for audibly rendering the modified assistant outputs, thereby causing the modified assistant outputs to resonate even further with the user. This brings various technical advantages of saving computing resources at the client device and may enable the conversation session to end in a faster and more efficient manner and/or reduce the number of conversation sessions. For example, the number of user inputs received at the client device may be reduced because the number of occurrences that a user must request information related to the conversational context may be reduced because it may be proactively provided by the automated assistant to be presented to the user. Further, for example, in embodiments in which one or more LLM outputs are generated offline and then used online, latency may be reduced at runtime.
As used herein, a "conversation session" may include a logically independent (logical-self-contained) exchange between a user and an automated assistant (and in some cases, other human participants). The automated assistant may differentiate between multiple conversations with the user based on various signals such as time lapse between the sessions, changes in user context (e.g., location, pre/during/after a predetermined meeting, etc.) between the sessions, detection of one or more intervening interactions between the user and the client device in addition to the conversation between the user and the automated assistant (e.g., user switching applications for a period of time, user leaving and then returning to a stand alone (stand alone) voice activated product), locking/dormancy of the client device between the sessions, changes in the client device for interfacing with the automated assistant, etc. Notably, during a given dialog session, a user may interact with the automated assistant using various input modalities, including, but not limited to, spoken input, typed input, and/or touch input.
The foregoing description is provided as an overview of only some of the embodiments disclosed herein. Those embodiments, as well as other embodiments, are described in more detail herein.
It should be appreciated that the techniques disclosed herein may be implemented locally on a client device, remotely by a server connected to the client device via one or more networks, and/or both.
Drawings
FIG. 1 depicts a block diagram of an example environment that demonstrates aspects of the present disclosure and in which embodiments disclosed herein may be implemented.
FIG. 2 depicts an example process flow for generating assistant output using a large language model, in accordance with various embodiments.
FIG. 3 depicts a flowchart that shows an example method of generating an assistant output in an offline manner for subsequent use in an online manner using a large language model, according to various embodiments.
FIG. 4 depicts a flowchart that shows an example method for generating an assistant output based on generating an assistant query using a large language model, in accordance with various embodiments.
FIG. 5 depicts a flowchart illustrating an example method for utilizing a large language model to generate assistant output based on generating assistant personality replies, in accordance with various embodiments.
FIG. 6 depicts a non-limiting example of a dialog session between a user and an automated assistant that generates assistant output using a large language model, in accordance with various embodiments.
FIG. 7 depicts an example architecture of a computing device according to various embodiments.
Detailed Description
Turning now to fig. 1, a block diagram of an example environment 100 is depicted, the example environment 100 demonstrating aspects of the present disclosure, and in which embodiments disclosed herein may be implemented. The example environment 100 includes a client device 110 and a natural conversation system 120. In some implementations, natural talking system 120 may be implemented locally at client device 110. In additional or alternative implementations, natural talking system 120 may be implemented remotely (e.g., at a remote server) from client device 110 as depicted in fig. 1. In these embodiments, client device 110 and natural conversation system 120 may be communicatively coupled to each other via one or more networks 199, such as one or more wired or wireless local area networks ("LANs" including Wi-Fi LANs, mesh networks, bluetooth, near field communications, etc.) or wide area networks ("WANs" including the internet).
Client device 110 may be, for example, one or more of the following: a desktop computer, a laptop computer, a tablet computer, a mobile phone, a computing device of a vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a stand alone interactive speaker (optionally with a display), a smart appliance such as a smart television, and/or a wearable device that includes a user of the computing device (e.g., a watch of a user with the computing device, glasses of a user with the computing device, a virtual or augmented reality computing device). Additional and/or alternative client devices may be provided.
The client device 110 may execute an automated assistant client 114. An example of the automated assistant client 114 may be an application separate from the operating system of the client device 110 (e.g., installed "on top" of the operating system) -or may alternatively be implemented directly by the operating system of the client device 110. The automated assistant client 114 may interact with a natural talking system 120 implemented locally at the client apparatus 110 or remotely and invoked via one or more networks 199, as depicted in fig. 1. The automated assistant client 114 (and optionally through its interaction with other remote systems (e.g., servers)) may form a logical instance that appears to the user to be an automated assistant 115 through which the user can conduct a human-machine conversation. An example of an automated assistant 115 is depicted in fig. 1 and is surrounded by a dashed line including an automated assistant client 114 of the client device 110 and the natural talking system 120. It should therefore be appreciated that a user engaged with an automated assistant client 114 executing on a client device 110 may actually engage with a logical instance of his or her own automated assistant 115 (or a logical instance of an automated assistant 115 shared between a home or other group of users). For brevity and simplicity, an automated assistant 115 as used herein will refer to an automated assistant client 114 executing locally on a client device 110 and/or executing remotely at one or more remote servers that may implement a natural conversation system 120.
In various implementations, the client device 110 may include a user input engine 111 configured to detect user input provided by a user of the client device 110 using one or more user interface input devices. For example, the client device 110 may be equipped with one or more microphones that capture audio data, such as audio data corresponding to a spoken utterance of a user or other sounds in the environment of the client device 110. Additionally or alternatively, the client device 110 may be equipped with one or more visual components configured to capture visual data corresponding to images and/or movements (e.g., gestures) detected in the field of view of the one or more visual components. Additionally or alternatively, the client device 110 may be equipped with one or more touch sensitive components (e.g., keyboard and mouse, stylus, touch screen, touch panel, one or more hardware buttons, etc.) configured to capture signals corresponding to touch inputs directed to the client device 110.
In various implementations, the client device 110 may include a rendering engine 112 configured to provide content for audible and/or visual presentation to a user of the client device 110 using one or more user interface output devices. For example, the client device 110 may be equipped with one or more speakers that enable content to be provided for audible presentation to a user via the client device 110. Additionally or alternatively, the client device 110 may be equipped with a display or projector that enables content to be provided for visual presentation to a user via the client device 110.
In various implementations, the client device 110 may include one or more presence sensors 113 configured to provide a signal indicative of detected presence, particularly human presence, upon approval from a corresponding user. In some of those implementations, the automated assistant 115 can identify the client device 110 (or another computing device associated with the user of the client device 110) based at least in part on the presence of the user at the client device 110 (or at another computing device associated with the user of the client device 110) to satisfy the spoken utterance. The spoken utterance may be satisfied by rendering the response content at the client device 110 and/or other computing devices associated with the user of the client device 110 (e.g., via the rendering engine 112), by causing the client device 110 and/or other computing devices associated with the user of the client device 110 to be controlled, and/or by causing the client device 110 and/or other computing devices associated with the user of the client device 110 to perform any other actions that satisfy the spoken utterance. As described herein, the automation assistant 115 may utilize data determined based on the presence sensor 113 to determine the client device 110 (or other computing device) based on where the user is nearby or was recently nearby, and provide corresponding commands only to the client device 110 (or those other computing devices). In some additional or alternative implementations, the automated assistant 115 may utilize the data determined based on the presence sensor 113 to determine whether any user (any user or a particular user) is currently in proximity to the client device 110 (or other computing device), and may optionally refrain from providing data to the client device 110 (or other computing device) and/or providing data from the client device 110 (or other computing device) based on the user in proximity to the client device 110 (or other computing device).
The presence sensor 113 may take various forms. For example, client device 110 may detect the presence of a user using one or more of the user interface input components described above with respect to user input engine 111. Additionally or alternatively, the client device 110 may be equipped with other types of light-based presence sensors 113, such as passive infrared ("PIR") sensors that measure infrared ("IR") light radiated from objects within its field of view.
Additionally or alternatively, in some implementations, the presence sensor 113 may be configured to detect other phenomena associated with human presence or device presence. For example, in some embodiments, client device 110 may be equipped with presence sensor 113 that detects various types of wireless signals (e.g., waves such as radio, ultrasound, electromagnetic, etc.) transmitted by other computing devices (e.g., mobile devices, wearable computing devices, etc.) and/or other computing devices that are carried/operated by, for example, a user. For example, the client device 110 may be configured to emit a human-imperceptible wave, such as an ultrasonic or infrared wave, that may be detected by other computing devices (e.g., via an ultrasonic/infrared receiver such as an ultrasonic-capable microphone).
Additionally or alternatively, the client device 110 may emit other types of human-imperceptible waves, such as radio waves (e.g., wi-Fi, bluetooth, cellular, etc.) that may be detected by other computing devices (e.g., mobile devices, wearable computing devices, etc.) carried/operated by the user and used to determine the particular location of the user. In some implementations, GPS and/or Wi-Fi triangulation may be used to detect the location of a person, for example, based on GPS and/or Wi-Fi signals to/from client device 110. In other implementations, other wireless signal characteristics, such as time of flight, signal strength, etc., may be used by client device 110 alone or in combination to determine the location of a particular person based on signals transmitted by other computing devices carried/operated by the user. Additionally or alternatively, in some implementations, the client device 110 may perform speaker recognition (SID) to recognize the user from the user's voice and/or facial recognition (FID) to recognize the user from visual data capturing the user's face.
In some implementations, the movement of the speaker may then be determined, for example, by the presence sensor 113 of the client device 110 (and optionally the GPS sensor, the Soli chip, and/or the accelerometer of the client device 110). In some implementations, based on such detected movement, the location of the user may be predicted, and when any content is caused to be rendered at the client device 110 and/or other computing device based at least in part on the proximity of the client device 110 and/or other computing device to the user's location, the location may be assumed to be the location of the user. In some embodiments, the user may simply be assumed to be in his or her last position of engagement with the automated assistant 115, especially if too much time has not elapsed since the last engagement.
Further, client device 110 and/or natural conversation system 120 can include one or more memories for storing data and/or software applications, one or more processors for accessing data and executing software applications, and/or other components that facilitate communication over one or more of networks 199. In some implementations, one or more of the software applications may be installed locally at the client device 110, while in other implementations, one or more of the software applications may be hosted remotely (e.g., through one or more servers) and accessible by the client device 110 through one or more of the networks 199.
In some implementations, the operations performed by the automated assistant 115 may be implemented locally at the client device 110 via the automated assistant client 114. As shown in fig. 1, the automated assistant client 114 may include an Automatic Speech Recognition (ASR) engine 130A1, a Natural Language Understanding (NLU) engine 140A1, a Large Language Model (LLM) engine 150A1, and a text-to-speech (TTS) engine 160A1. In some implementations, such as when natural conversation system 120 is implemented remotely at client device 110 as depicted in fig. 1, operations performed by automated assistant 115 may be distributed across multiple computer systems. In these implementations, the automated assistant 115 may additionally or alternatively utilize the ASR engine 130A2, NLU engine 140A2, LLM engine 150A2, and TTS engine 160A2 of the natural conversation system 120.
Each of these engines may be configured to perform one or more functions. For example, the ASR engines 130A1 and/or 130A2 may process an audio data stream that captures spoken utterances and is generated by a microphone of the client device 110 to generate an ASR output stream using a streaming ASR model (e.g., a Recurrent Neural Network (RNN) model, a transducer (transformer) model, and/or any other type of ML model capable of performing ASR) stored in a Machine Learning (ML) model database 115A. Notably, the streaming ASR model can be used to generate an ASR output stream when generating an audio data stream. Further, NLU engines 140A1 and/or 140A2 may process the ASR output streams to generate NLU output streams using NLU models (e.g., long Short Term Memory (LSTM), gated loop units (GRUs), and/or any other type of RNN or other ML model capable of executing NLUs) and/or grammar-based rules stored in ML model database 115A. Further, the automated assistant 115 may cause NLU output to be processed to generate a fulfillment data stream. For example, the automated assistant 115 may send one or more structured requests to one or more first party (1P) systems 191 and/or to one or more third party (3P) systems 192 over one or more networks 199 (or one or more Application Programming Interfaces (APIs)) and receive fulfillment data from one or more 1P systems 191 and/or 3P systems 192 to generate a fulfillment data stream. The one or more structured requests may include NLU data included in the fulfillment data stream, for example. The fulfillment data stream may correspond to, for example, a set of assistant outputs predicted to be responsive to assistant queries included in spoken utterances captured in the audio data stream processed by the ASR engines 130A1 and/or 130 A2.
Further, LLM engines 150A1 and/or 150A2 can process a set of assistant outputs predicted to be responsive to assistant queries included in spoken utterances captured in the audio data stream processed by ASR engines 130A1 and/or 130 A2. As described herein (e.g., with reference to fig. 2-6), in some implementations, LLM engines 150A1 and/or 150A2 can use one or more LLM outputs such that the set of assistant outputs is modified to generate a modified set of assistant outputs. In some versions of those implementations (e.g., and as described with respect to fig. 3), the automated assistant 115 can cause one or more LLM outputs to be generated in an offline manner (e.g., not responsive to spoken utterances received during a conversation session) and then used in an online manner (e.g., when spoken utterances are received during the conversation session) to generate a modified set of assistant outputs. In additional or alternative versions of those embodiments (e.g., and as described with respect to fig. 4 and 5), the automated assistant 115 can cause one or more LLM outputs to be generated in an online manner (e.g., when a spoken utterance is received during a conversation session). In these implementations, one or more LLM outputs may be generated based on processing a set of assistant outputs (e.g., fulfilling a data stream) using one or more LLMs (e.g., one or more transducer models, such as Meena, RNN, and/or any other LLM) stored in the model database 115A, a context of a conversation session in which the spoken utterance was received (e.g., based on one or more context signals stored in the context database 110A), recognition text corresponding to an assistant query included in the spoken utterance, and/or other information that the automated assistant 115 may utilize in generating the one or more LLM outputs. Each of the one or more LLM outputs may include, for example, a probability distribution over a sequence of one or more words and/or phrases across the one or more words, and the one or more words and/or phrases in the sequence may be selected as the one or more LLM outputs based on the probability distribution. In various implementations, one or more of the LLM outputs can be stored in the LLM output database 150A for later use in modifying one or more of the assistant outputs included in the set of assistant outputs.
Further, in some implementations, TTS engines 160A1 and/or 160A2 can process text data (e.g., text formulated by automated assistant 115) using TTS models stored in ML model database 115A to generate synthesized speech audio data that includes computer-generated synthesized speech. The text data may correspond to, for example, one or more of the set of assistant outputs included in the fulfillment data stream, one or more of the modified assistant outputs from the modified set of assistant outputs, and/or any other text data described herein. Notably, the ML model stored in ML model database 115A may be an on-device ML model stored locally at client device 110, or a shared ML model accessible to both client device 110 and/or a remote system when natural conversation system 120 is not implemented locally at client device 110. In additional or alternative implementations, audio data corresponding to one or more of the set of assistant outputs from the set of assistant outputs included in the fulfillment data stream, one or more of the modified assistant outputs from the set of modified assistant outputs, and/or any other text data described herein may be stored in memory or in one or more databases accessible by the client device 110 such that the automated assistant does not need to generate any synthesized speech audio data using TTS engines 160A1 and/or 160A2 when the audio data is caused to be provided for audible presentation to a user.
In various embodiments, the ASR output stream may include, for example: an ASR hypothesis stream (e.g., a vocabulary hypothesis and/or transcription hypothesis) predicted to correspond to a spoken utterance of a user captured in an audio data stream; one or more corresponding predicted values (e.g., probabilities, log-likelihoods, and/or other values) for each ASR hypothesis; a plurality of phonemes predicted to correspond to a spoken utterance of a user captured in an audio data stream; and/or other ASR outputs. In some versions of those implementations, the ASR engines 130A1 and/or 130A2 may select one or more ASR hypotheses as the recognized text corresponding to the spoken utterance (e.g., based on the corresponding predicted values).
In various implementations, the NLU output stream may include, for example, a annotated recognition text stream that includes one or more annotations of recognition text for one or more (e.g., all) terms of the recognition text. For example, NLU engines 140A1 and/or 140A2 may include a portion of a part-of-speech tagger (not depicted) configured to annotate a term with its grammatical role. Additionally or alternatively, NLU engines 140A1 and/or 140A2 may include entity markers (not depicted) configured to annotate entity references in one or more segments of recognized text, such as references to people (including, for example, literature personas, celebrities, public personas, etc.), organizations, places (real and fictional), and the like. In some implementations, data about an entity can be stored in one or more databases, such as in a knowledge graph (not depicted). In some implementations, the knowledge graph can include nodes representing known entities (and in some cases, entity attributes), and edges connecting the nodes and representing relationships between the entities. The entity marker may annotate references to entities at a high level of granularity (e.g., to enable identification of all references to entity classes such as people) and/or at a lower level of granularity (e.g., to enable identification of all references to particular entities such as particular people). The entity tagger may rely on the content of the natural language input to resolve a particular entity and/or may optionally communicate with a knowledge graph or other entity database to resolve a particular entity.
Additionally or alternatively, NLU engines 140A1 and/or 140A2 may include co-reference digesters (not depicted) configured to group or "cluster" references to the same entity based on one or more contextual cues. For example, based on the "the tickets" mentioned in the client device notification rendered immediately prior to receiving the input "buy they", the coreference resolution may be used to resolve the term "the m" as "buy theatre tickets" in the natural language input "buy tickets". In some implementations, one or more components of NLU engines 140A1 and/or 140A2 may rely on annotations from one or more other components of NLU engines 140A1 and/or 140 A2. For example, in some implementations, an entity annotator may rely on annotations from the coreference resolver to annotate all references to a particular entity. In addition, for example, in some implementations, the coreference resolver may rely on annotations from the entity labeler to cluster references to the same entity.
Although fig. 1 is described with respect to a single client device having a single user, it should be understood that this is for purposes of illustration and is not meant to be limiting. For example, one or more additional client devices of the user may also implement the techniques described herein. For example, client device 110, one or more additional client devices, and/or any other computing devices of a user may form an ecosystem of devices that may employ the techniques described herein. These additional client devices and/or computing devices may communicate with client device 110 (e.g., over network 199). As another example, a given client device may be used by multiple users in a shared setting (e.g., a group of users, a family).
As described herein, the automated assistant 115 can determine whether to modify the set of assistant responses using one or more LLM outputs and/or determine one or more modified sets of assistant outputs based on the one or more LLM outputs. The automated assistant 115 may make these determinations using the natural talking system 120. In various implementations and as depicted in fig. 1, natural talking system 120 may additionally or alternatively include offline output modification engine 170, online output modification engine 180, and/or ranking engine 190. Offline output modification engine 170 may include, for example, an assistant activity engine 171 and an index engine 172. Further, the online output modification engine 180 may include, for example, an assistant query engine 181 and an assistant personality engine 182. These various engines of natural talking system 120 will be described in more detail with reference to fig. 2-5.
Turning now to fig. 2, an example process flow 200 for generating assistant output using LLM is depicted. The audio data stream 201 generated by the one or more microphones of the client device 110 of fig. 1 may be processed by the ASR engines 130A1 and/or 130A2 to generate a stream of ASR output 203. Further, ASR output 203 may be processed by NLU engines 140A1 and/or 140A2 to generate a stream of NLU output 204. In some implementations, NLU engines 140A1 and/or 140A2 can process a context 202 of a conversation session between a user of client device 110 and an automated assistant 115 executing at least in part at the user's client device 110. In some versions of those implementations, the context 202 of the conversation session may be determined based on one or more context signals generated by the client device 110 (e.g., time of day, day of week, location of the client device 110, ambient noise detected in the environment of the client device 110, and/or other context signals generated by the client device 110). In additional or alternative versions of those embodiments, the context 202 of the conversation session may be determined based on one or more context signals (e.g., user profile data, software application data, environmental data regarding the known environment of the user of the client device 110, conversation history of an ongoing conversation session between the user and the automated assistant 115, and/or historical conversation history of one or more previous conversation sessions between the user and the automated assistant 115, and/or other context data stored in the context database 110A) stored in the context database 110A accessible at the client device 110. Further, the stream of NLU outputs 204 may be processed by one or more 1P systems 191 and/or 3P systems to generate a fulfillment data stream comprising one or more sets of assistant outputs 205, wherein each of the one or more assistant outputs included in the one or more sets of assistant outputs 205 is predicted to be responsive to a spoken utterance captured in the audio data stream 201.
In general, in turn-based conversation sessions that do not utilize LLM, the ranking engine 190 can process the one or more sets of assistant outputs 205 to rank each of the one or more assistant outputs included in the one or more sets of assistant outputs 205 according to one or more ranking criteria, and the automated assistant 115 can select one or more given assistant outputs 207 from the one or more sets of assistant outputs 205 to provide for presentation to a user of the client device 110 in response to receiving the spoken utterance. In some implementations, the selected one or more given assistant outputs 207 can be processed by TTS engines 160A1 and/or 160A2 to generate synthesized speech audio data that includes synthesized speech corresponding to the selected one or more given assistant outputs 207, and rendering engine 112 can cause the synthesized speech audio data to be audibly rendered by speakers of client device 110 for audible presentation to a user of client device 110. In additional or alternative implementations, the rendering engine 112 may cause text data corresponding to the selected one or more given assistant outputs 207 to be visually rendered by the display of the client device 110 for visual presentation to the user of the client device 110.
However, when using the claimed technology, the automated assistant 115 can also cause one or more sets of assistant outputs 205 to be processed by the LLM engines 150A1 and/or 150A2 to generate one or more modified sets of assistant outputs 206. In some implementations, one or more LLM outputs may be previously generated in an offline manner using the offline output modification engine 170 (e.g., prior to receiving the audio data stream 201 generated by the one or more microphones of the client device 110), and may be stored in the LLM output database 150A. As described with respect to fig. 3, one or more LLM outputs may be previously indexed in the LLM output database 150A based on the corresponding assistant query and/or the corresponding context of the corresponding dialog session in which the corresponding assistant query was received. Further, LLM engine 150A1 and/or 150A2 can determine that an assistant query included in a spoken utterance captured in audio data stream 201 matches a corresponding assistant query and/or that context 202 of a conversation session in which the assistant query was received matches a corresponding context of a corresponding conversation session in which the corresponding assistant query was received. The LLM engine 150A1 and/or 150A2 can obtain one or more LLM outputs indexed by the corresponding assistant query and/or the corresponding context that matches the assistant query to modify the set of one or more assistant outputs 205. Further, one or more sets of assistant outputs 205 can be modified based on the one or more LLM outputs, resulting in one or more modified sets of assistant outputs 206.
In additional or alternative implementations, the online output modification engine 180 may be used to generate one or more LLM outputs in an online manner (e.g., in response to receiving the audio data stream 201 generated by the one or more microphones of the client device 110). As described with reference to fig. 4 and 5, one or more LLM outputs may be generated based on processing the set of one or more assistant outputs 205 using one or more LLMs stored in the ML model database 115A, recognition text corresponding to assistant queries included in spoken utterances captured in the audio data stream 201 (e.g., included in a stream of ASR outputs 203), and/or a context 202 of a conversation session between a user of the client device 110 and the automated assistant 115 to generate the one or more LLM outputs. Further, one or more sets of assistant outputs 205 can be modified based on the one or more LLM outputs, resulting in one or more modified sets of assistant outputs 206. In other words, in these embodiments, LLM engine 150A1 and/or 150A2 can directly generate one or more modified sets of assistant outputs 206 using one or more LLMs stored in ML model database 115A.
In these embodiments, and in contrast to the typical round-based dialog session described above that does not utilize LLM, the ranking engine 190 can process the one or more sets of assistant outputs 205 and the one or more modified sets of assistant outputs 206 to rank each of the one or more assistant outputs included in both the one or more sets of assistant outputs 205 and the one or more modified sets of assistant outputs 206 according to one or more ranking criteria. Thus, upon selection of one or more given assistant outputs 207, the automated assistant 207 may select from among one or more sets of assistant outputs 205 and one or more modified sets of assistant outputs 206. Notably, the assistant outputs included in the one or more modified sets of assistant outputs 206 can be generated based on the one or more sets of assistant outputs 205 and convey the same or similar information, but also along with additional information related to the context 202 of the conversation (e.g., as described with respect to fig. 4) and/or more natural, fluent, and/or more consistent with the characteristics of the automated assistant, such that the one or more given assistant outputs 207 are better resonated with the user of the client device 110.
The one or more ranking criteria may include, for example: one or more prediction metrics (e.g., ASR metrics generated by ASR engines 130A1 and/or 130A2 in generating a stream of ASR outputs 203, NLU metrics generated by NLU engines 140A1 and/or 140A2 in generating a stream of NLU outputs 204, performance metrics generated by one or more of 1P system 191 and/or 3P system 192) -that indicate how responsive each assistant output included in one or more sets of assistant outputs 205 and one or more modified sets of assistant outputs 206 is to assistant queries included in a spoken utterance captured in audio data stream 201 is predicted; one or more intents included in the stream of NLU outputs 204; metrics derived from a classifier that processes each of the assistant outputs included in the set of one or more assistant outputs 205 and the set of one or more modified assistant outputs 206 to determine how natural, fluent, and/or consistent each assistant output is with the characteristics of an automated assistant when provided for presentation to a user; and/or other ranking criteria. For example, if the intent of the user of the client device 110 indicates that the user wants a factual answer (e.g., based on providing a spoken utterance of an assistant query that includes "Why is the sky blue. However, if the intent of the user of client device 110 indicates that the user provided more open input (e.g., based on providing a spoken utterance including an assistant query of "what time is it.
Although fig. 1 and 2 are described herein with respect to a voice-based conversation session, it should be understood that this is for purposes of illustration and is not meant to be limiting. Rather, it should be understood that the techniques described herein may be utilized regardless of the input modality of the user. For example, in some implementations in which a user provides typed and/or touch input as an assistant query, automated assistant 115 may process typed input using NLU engines 140A1 and/or 140A2 to generate a stream of NLU output 204 (e.g., skipping processing of audio data stream 201), and LLM engines 150A1 and/or 150A2 may utilize text input corresponding to the assistant query (e.g., derived from typed and/or touch input) to generate a set of one or more modified assistant outputs 206 in the same or similar manner as described above.
Turning now to FIG. 3, a flow diagram is depicted illustrating an example method 300 of generating an assistant output in an offline manner using a large language model for subsequent use in an online manner. For convenience, the operations of method 300 are described with reference to a system performing the operations from process flow 200 of fig. 2. The system of method 300 includes one or more processors, memory, and/or other components of a computing device (e.g., client device 110 of fig. 1, client device 610 of fig. 6, and/or computing device 710 of fig. 7, one or more servers, and/or other computing devices). Furthermore, although the operations of method 300 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 352, the system obtains a plurality of assistant queries directed to the automated assistant and a corresponding context for a corresponding previous dialog session for each of the plurality of assistant queries. For example, the system may cause the assistant activity engine 171 of the offline output modification engine of fig. 1 and 2 to obtain a plurality of assistant queries and corresponding contexts of previous dialog sessions, where the plurality of assistant queries are received from the assistant activity database 170A, such as depicted in fig. 1. In some implementations, the plurality of assistant queries and the corresponding contexts of the previous dialog session in which the plurality of assistant queries were received may be limited to those associated with a user of the client device (e.g., the user of the client device 110 of fig. 1). In other implementations, the plurality of assistant queries and the corresponding contexts of the previous dialog session in which the plurality of assistant queries were received may be limited to those associated with the plurality of users of the respective client devices (e.g., which may or may not include the user of client device 110 of fig. 1).
At block 354, the system processes a given assistant query of the plurality of assistant queries using the one or more LLMs to generate one or more corresponding LLM outputs, wherein each of the one or more corresponding LLM outputs is predicted to be responsive to the given assistant query. Each of the one or more corresponding LLM outputs may include, for example, a probability distribution over a sequence of one or more words and/or phrases across the one or more words, and the one or more words and/or phrases in the sequence may be selected as the one or more corresponding LLM outputs based on the probability distribution. In various implementations, upon generating one or more corresponding LLM outputs for a given assistant query, the system can process, using the one or more LLMs in conjunction with the assistant query, the corresponding context of the corresponding previous dialog session in which the given assistant query was received, and/or a set of assistant outputs predicted to be responsive to the given assistant query (e.g., based on a set of assistant outputs generated using one or more of ASR engines 130A1 and/or 130A2, NLU engines 140A1 and/or 140A2, and 1P systems 191 and/or 3P systems 192 as described with reference to fig. 2 to process audio data corresponding to the given assistant query). In some implementations, the system can process the recognition text corresponding to a given assistant query, while in additional or alternative implementations, the system can process capturing audio data that includes the spoken utterance of the given assistant query. In some implementations, the system can cause the LLM engine 150A1 to process a given assistant query using one or more LLMs local at a client device of a user (e.g., a user of the client device 110 of fig. 1), while in other implementations, the system can cause the LLM engine 150A2 to process a given assistant query using one or more LLMs remote at a client device of the user (e.g., at a remote server). As described herein, one or more corresponding LLM outputs may reflect a more natural conversational output than a typical assistant output that an automated assistant may provide, which enables the automated assistant to drive conversational sessions in a more fluid manner such that assistant outputs modified based on one or more corresponding LLM outputs are more likely to resonate with users perceiving the modified assistant outputs.
In some implementations, in addition to one or more corresponding LLM outputs, additional assistant queries can be generated using one or more LLM models based on processing a given assistant query and/or corresponding contexts of corresponding previous dialog sessions in which the given assistant query was received. For example, one or more LLMs may determine intent associated with a given assistant query (e.g., based on a stream of NLU outputs 204 generated using NLU engines 140A1 and/or 140A2 in fig. 2) when processing the given assistant query and/or a corresponding context of a corresponding previous dialog session in which the given assistant query was received. Further, the one or more LLMs can identify at least one relevant intent that is relevant to the intent associated with the given assistant query based on the intent associated with the given assistant query (e.g., based on a mapping of the intent to at least one relevant intent in a database or memory accessible to the client device 110 and/or based on processing the intent associated with the given assistant query using one or more Machine Learning (ML) models or heuristically defined rules). Further, one or more LLMs can generate additional assistant queries based on at least one related intent. For example, assume that the assistant query indicates that the user has not eaten dinner (e.g., a given assistant query "I'm feeling pretty hungry (I feel hungry)" received at the user's actual residence at night, as indicated by the corresponding context of the corresponding previous dialog session associated with the user intent indicating that he/she wants to eat). In this example, the additional assistant query may correspond to, for example, "what types of cuisine has the user indicated he/she preferences? (what type of food he/she has indicated? (which restaurants are open in the vicinity), (e.g., reflecting a related restaurant lookup intent associated with a user intent indicating that he/she wants to eat) and/or other additional assistant queries.
In these implementations, the additional assistant output may be determined based on processing the additional assistant query. Is "what types of cuisine has the user indicated he/she preferences" at the additional assistant query? In the above example of "user profile data from one or more 1P systems 191 stored locally at client device 110 may be utilized to determine that the user indicates that he/she prefers mediterranean and indian cuisine. Based on user profile data indicating that the user prefers Mediterranean and Indian cuisine, one or more corresponding LLM outputs may be modified to query the user as to whether the Mediterranean cuisine and/or Indian cuisine user sounds gastric (e.g., "how does Mediterranean cuisine or Indian cuisine sound for dinner (how evening meal Mediterranean cuisine or Indian cuisine sounds)").
Is "what restaurants nearby are open? "restaurant data from one or more of the 1P system 191 and/or 3P system 192 may be used to determine which restaurants are open in the vicinity of the user's primary residence (and optionally limited to restaurants that provide mediterranean and indian cuisine based on user profile data). Based on the results, one or more corresponding LLM outputs can be modified to provide the user with a list of one or more restaurants that are open to business in the vicinity of the user's primary residence (e.g., "Example Mediterranean Restaurant is open until 9:00PM and Example Indian Restaurant is open until 10:00PM (example Mediterranean restaurants open to 9:00pm and example indian restaurants open to 10:00 pm)"). Notably, additional assistant queries (e.g., "what types of cuisine has the user indicated he/she preferences. Instead, additional assistant outputs (e.g., "how does Mediterranean cuisine or Indian cuisine sound for dinner" and "Example Mediterranean Restaurant is open until 9:00PM and Example Indian Restaurant is open until 10:00PM") determined based on the additional assistant queries may be included in one or more corresponding LLM outputs, and thus may be provided for presentation to the user.
In additional or alternative embodiments, each of the one or more corresponding LLM outputs (and optionally additional assistant outputs determined based on additional assistant queries) may be generated using a corresponding set of parameters of the plurality of distinct sets of parameters of the one or more LLMs. Each parameter of the plurality of distinct parameter sets may be associated with a distinct personality of the automated assistant. In some versions of those embodiments, a single LLM may be utilized to generate one or more corresponding LLM outputs using the corresponding parameter sets for each distinct personality, while in other versions of those embodiments, multiple LLMs may be utilized to generate one or more corresponding LLM outputs using the corresponding parameter sets for each distinct personality. For example, a single LLM may be utilized to: generating a first LLM output using a first set of parameters reflecting a first personality (e.g., chef personality in the above example, wherein a given assistant query corresponds to "I'm feeling pretty hungry"); generating a second LLM output using a second set of parameters reflecting a second personality (e.g., a housekeeper personality in the above example, wherein a given assistant query corresponds to "I'm feeling pretty hungry"); for a number of other distinct personalities, and so on. Further, for example, the first LLM can be used to generate a first LLM output using a first set of parameters reflecting a first personality (e.g., chef personality in the above example, where a given assistant query corresponds to "I'm feeling pretty hungry"); the second LLM may be used to generate a second LLM output using a second set of parameters reflecting a second personality (e.g., a housekeeper personality in the above example, where a given assistant query corresponds to "I'm feeling pretty hungry"); for a number of other distinct personalities, and so on. Thus, when the corresponding LLM output is provided for presentation to the user, it can reflect various dynamic contextual personalities via prosodic attributes of the different personalities (e.g., intonation, pause, speed of speech, accent, rhythm, etc. of the different personalities). Additionally or alternatively, the user may define one or more personalities to be used by the automated assistant (e.g., via settings of an automated assistant application associated with the automated assistant described herein) in a persistent manner (e.g., always using a housekeeper personality) and/or in a contextual manner (e.g., using a housekeeper personality in the morning and evening, but a different personality in the afternoon).
Notably, these personality replies described herein reflect not only prosodic attributes of different personalities, but also vocabularies of different personalities and/or different speaking styles of different personalities (e.g., lengthy speaking styles, succinct speaking styles, friendly personalities, irony personalities, etc.). For example, the chef personality described above may have a particular chef vocabulary such that a probability distribution over one or more word and/or phrase sequences using one or more corresponding LLM outputs generated for a parameter set of the chef personality may facilitate other word and/or phrase sequences of words and/or phrases used by the chef over other personalities (e.g., scientist personalities, librarian personalities, etc.). Thus, when one or more corresponding LLM outputs are provided for presentation to a user, they may reflect not only the various dynamic contextual personalities in terms of prosodic attributes of the different personalities, but also the various dynamic contextual personalities in terms of accurate and realistic vocabulary of the different personalities, such that the one or more corresponding LLM outputs better resonate with the user in the different contextual scenarios. Furthermore, it should be appreciated that different personalised vocabulary and/or speaking styles may be defined with different degrees of granularity. Continuing with the above example, the chef personality may have a particular mediterranean chef vocabulary when a mediterranean food is queried based on an additional assistant query associated with a mediterranean food, a particular indian chef vocabulary when an indian food is queried based on an additional assistant query associated with an indian food, and so on.
At block 356, the system indexes one or more corresponding LLM outputs in a memory (e.g., LLM output database 150A of fig. 1) accessible at the client device based on the given assistant query and/or the corresponding context of the corresponding previous dialog session for the given assistant query. For example, the system can cause the index engine 172 of the offline output modification engine of fig. 1 and 2 to index one or more corresponding LLM outputs in the LLM output database 150A. In some implementations, the indexing engine 172 can index one or more corresponding LLM outputs based on one or more terms included in a given assistant query and/or one or more context signals included in a corresponding context of a corresponding previous dialog session in which the given assistant query was received. In additional or alternative embodiments, the indexing engine 172 may cause the generation of an embedding of a given assistant query (e.g., word2vec embedding or any other lower dimensional representation) and/or an embedding of one or more context signals included in a corresponding context of a corresponding previous dialog session in which the given assistant query was received, and one or more of these embeddings may be mapped to an embedding space (e.g., lower dimensional space). In these embodiments, one or more corresponding LLM outputs indexed in the LLM output database 150A may then be used by the online output modification engine 180 to modify the set of assistant outputs (e.g., as described below with respect to blocks 362, 364, and 366 of the method 300 of fig. 3). In various implementations, the system may additionally or alternatively generate one or more assistant outputs (e.g., one or more assistant outputs 205 as described with respect to fig. 2) that are predicted to be responsive to a given assistant query (but not generated using LLM engines 150A1 and/or 150 A2), and one or more corresponding LLM outputs may additionally or alternatively be indexed by one or more assistant outputs.
In some implementations, and as indicated at block 358, the system may optionally receive user input to review and/or modify one or more corresponding LLM outputs. For example, a human reviewer may analyze one or more corresponding LLM outputs generated using one or more LLM models and modify the one or more corresponding LLM outputs by changing one or more terms and/or phrases included in the one or more corresponding LLM outputs. In addition, for example, a human reviewer may re-index, discard, and/or otherwise modify the index of one or more corresponding LLM outputs. Thus, in these embodiments, one or more corresponding LLM outputs generated using one or more LLMs may be curated (currate) by a human reviewer to ensure the quality of the one or more corresponding LLM outputs. In addition, any undipped, re-indexed, and/or planned LLM output can be used to modify or re-train the LLM in an offline manner.
At block 360, the system determines whether there are additional assistant queries included in the plurality of assistant queries obtained at block 352 that have not been processed using one or more LLMs. If, at the iteration of block 360, the system determines that there are additional assistant queries included in the plurality of assistant queries obtained at block 352 that have not been processed using one or more LLMs, the system returns to block 354 and performs additional iterations of blocks 354 and 356, but with respect to additional assistants rather than the given assistant query. These operations may be repeated for each of the assistant queries included in the plurality of assistant queries obtained at block 352. In other words, the system can index one or more corresponding LLM outputs for each assistant query and/or a corresponding context of a corresponding previous dialog session in which a corresponding one of the plurality of assistant queries was received prior to causing utilization of the one or more corresponding LLM outputs.
If, at the iteration of block 360, the system determines that there are no additional assistant queries included in the plurality of assistant queries obtained at block 352 that have not been processed using one or more LLMs, the system may proceed to block 362. At block 362, the system may monitor an audio data stream generated by one or more microphones of the client device to determine whether the audio data stream captures a spoken utterance of a user of the client device directed to the automated assistant. For example, the system may monitor one or more particular words or phrases included in the audio data stream (e.g., monitor one or more particular words or phrases that invoke an automated assistant using a hot word detection model). Further, for example, the system may monitor speech directed to the client device, optionally in addition to one or more other signals (e.g., one or more gestures captured by a visual sensor of the client device, eye gaze directed to the client device, etc.). If, at the iteration of block 362, the system determines that the audio data stream did not capture a spoken utterance of the user of the client device directed to the automated assistant, the system may continue to monitor the audio data stream at block 362. If, at the iteration of block 362, the system determines that the audio data stream captures a spoken utterance of the user of the client device that is directed to the automated assistant, the system may proceed to block 364.
At block 364, the system determines, based on processing the audio data stream, that the spoken utterance includes a current assistant query corresponding to one of the plurality of assistant queries and/or that the spoken utterance is received in a current context of a current conversation session corresponding to a corresponding context of a corresponding previous conversation session for the one of the plurality of assistant queries. For example, the system may process an audio data stream (e.g., the audio data stream 201 of FIG. 2) using the ASR engines 130A1 and/or 130A2 to generate an ASR output stream (e.g., the stream of ASR output 203 of FIG. 2). Further, the system can process the ASR output stream using NLU engines 140A1 and/or 140A2 to generate an NLU output stream (e.g., a stream of NLU output 204 of fig. 2). Further, based on the ASR output stream and/or the NLU output stream, the system can identify a current assistant query. In some implementations, the system can also determine a set of one or more assistant outputs (e.g., one or more assistant outputs 205 of fig. 2) by causing one or more of the 1P system 191 and/or 3P system to process the ASR output stream and/or the NLU output stream.
In some implementations of the method 300 of fig. 3, the system may utilize the online output modification engine 180 to determine that one or more terms or phrases of the current assistant query corresponds to one or more terms or phrases of one of the plurality of assistant queries for which one or more corresponding LLM outputs are indexed (e.g., using any known technique to determine whether a term or phrase corresponds to one or other, such as an exact match technique, a soft match technique, an edit distance technique, a pronunciation similarity technique, an embedding technique, etc.). In response to determining that the one or more terms or phrases of the current assistant query correspond to the one or more terms or phrases of one of the plurality of assistant queries, the system may obtain (e.g., from LLM output database 150A) one or more corresponding LLM outputs indexed at iterations of block 356 and associated with the one of the plurality of assistant queries. For example, if the current query includes the phrase "I'm hunry," the system may obtain one or more corresponding LLM outputs generated in the above-described examples with respect to the given assistant query description. For example, the system may determine that both the current query and the given assistant query include the phrase "i starved" based on comparing edit distances between terms of the current query and terms of the given assistant query. In addition, for example, the system may generate an embedding of the current assistant query and map the embedding of the current query to the embedding space described above with respect to block 356. Further, the system may determine that the current assistant query corresponds to the given assistant query based on a distance between a generated embedding of the current query in the embedding space and a previously generated embedding of the given assistant query meeting a distance threshold.
In additional or alternative implementations of the method 300 of fig. 3, the system may utilize the online output modification engine 180 to determine that the one or more context signals detected when the current assistant query is received correspond to one or more corresponding context signals when one of the plurality of assistant queries is received (e.g., the same day of the week received, the same time of day, the same place, the same environmental noise present in the environment of the client device, received in a particular sequence of spoken utterances during a conversation session, etc.). In response to determining that the one or more context signals associated with the current assistant query correspond to one or more context signals of one of the plurality of assistant queries, the system may obtain (e.g., from LLM output database 150A) one or more corresponding LLM outputs indexed at the iterations of block 356 and associated with the one of the plurality of assistant queries. For example, if the current query is received at the user's primary residence at night, the system may obtain one or more corresponding LLM outputs generated in the above-described examples with respect to the given assistant query description. For example, the system may determine that the described current query and given assistant query are both associated with a time context signal of "evening" and a location context signal of "primary residence". In addition, for example, the system may generate an embedding of one or more context signals associated with the current assistant query and map the embedding of one or more context signals associated with the current query to the embedding space described above with respect to block 356. Further, the system may determine that the one or more context signals associated with the current assistant query correspond to the one or more context signals associated with the given assistant query based on a distance between a generated embedding of the one or more context signals associated with the current query in the embedding space and a previously generated embedding of the one or more context signals associated with the given assistant query meeting a distance threshold.
Notably, the system can utilize one or both of the current assistant query and a context (e.g., one or more detected context signals) of the dialog session in which the current assistant query was received to determine that one or more corresponding LLM outputs are to be used to generate one or more current assistant outputs to be provided for presentation to the user in response to the current assistant query. In various implementations, the system may additionally or alternatively utilize one or more assistant outputs generated for the current assistant query to determine one or more corresponding LLM outputs to be used in generating one or more current assistant outputs to be provided for presentation to the user in response to the current assistant query. For example, the system may additionally or alternatively generate one or more assistant outputs (e.g., one or more assistant outputs 205 as described with respect to fig. 2) that are predicted to be responsive to the current assistant query (but not generated using LLM engines 150A1 and/or 150 A2) and determine, using the various techniques described above, that the one or more assistant outputs that are predicted to be responsive to the current assistant query correspond to one or more previously generated assistant outputs for one of the plurality of assistant queries.
At block 366, the system causes the automated assistant to utilize the one or more corresponding LLM outputs to generate one or more current assistant outputs to be provided for presentation to a user of the client device. For example, the system can rank the one or more assistant outputs (e.g., one or more assistant outputs 205 as described with respect to fig. 2) and the one or more corresponding LLM outputs (e.g., one or more modified assistant outputs 206 as described with respect to fig. 2) according to one or more ranking criteria. Further, the system can select one or more current assistant outputs from among the one or more assistant outputs and the one or more corresponding LLM outputs. Further, the system may cause one or more current assistant outputs to be visually and/or audibly rendered for presentation to a user of the client device.
While embodiments of the method 300 of fig. 3 are described with one or more corresponding LLM outputs generated in an offline manner (e.g., by utilizing the offline output modification engine 170 to generate corresponding LLM outputs using one or more LLMs and indexing the one or more corresponding LLM outputs in the LLM output database 150A) and then utilizing the one or more corresponding LLM outputs in an online manner (e.g., by utilizing the online output modification engine 180 to determine utilization from the LLM output database 150A based on a current assistant query), it should be understood that this is for purposes of illustration and not meant to be limiting. For example, as described below with respect to fig. 4 and 5, in a preferred embodiment, the online output modification engine 180 may additionally or alternatively utilize the LLM engines 150A1 and/or 150A2 in an online manner.
Turning now to FIG. 4, a flow diagram is depicted illustrating an example method 400 of generating an assistant output based on generating an assistant query using a large language model. For convenience, the operations of method 400 are described with reference to a system performing the operations from process flow 200 of fig. 2. The system of method 400 includes one or more processors, memory, and/or other components of a computing device (e.g., client device 110 of fig. 1, client device 610 of fig. 6, and/or computing device 710 of fig. 7, one or more servers, and/or other computing devices). Furthermore, although the operations of method 400 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 452, the system receives an audio data stream that captures a spoken utterance of a user, the spoken utterance including an assistant query directed to an automated assistant, and the spoken utterance being received during a conversation session between the user and the automated assistant. In some implementations, the system can process the audio data stream to determine that it captures the assistant query only in response to determining that one or more conditions are met. For example, the system may monitor one or more particular words or phrases included in the audio data stream (e.g., monitor one or more particular words or phrases that invoke an automated assistant using a hot word detection model). Further, for example, the system may monitor speech directed to the client device, optionally in addition to one or more other signals (e.g., one or more gestures captured by a visual sensor of the client device, eye gaze directed to the client device, etc.).
At block 454, the system determines a set of assistant outputs based on processing the audio data stream, each of the assistant outputs included in the set being responsive to an assistant query included in the spoken utterance. For example, the system may process an audio data stream (e.g., audio data stream 201) using the ASR engines 130A1 and/or 130A2 to generate an ASR output stream (e.g., ASR output 203). Further, the system can process the ASR output stream (e.g., audio data stream 201) using NLU engines 140A1 and/or 140A2 to generate an NLU output stream (e.g., a stream of NLU outputs 204). Further, the system can cause one or more 1P systems 191 and/or 3P systems 192 to process NLU output streams (e.g., streams of NLU outputs 204) to generate a set of assistant outputs 205 (e.g., a set of assistant outputs). Notably, the set of assistant outputs may correspond to such one or more candidate assistant outputs: the automated assistant may consider these one or more candidate assistant outputs for use in responding to the spoken utterance without the techniques described herein (i.e., techniques that do not utilize LLM engine 150A1 and/or 150A2 in modifying the assistant outputs as described herein).
At block 456, the system processes the context of the dialog session and the set of assistant outputs to: (1) Generating a modified set of assistant outputs using one or more LLM outputs, each of the one or more LLM outputs determined based at least on a context of the conversation session and/or one or more assistant outputs included in the set of assistant outputs; and (2) generating additional assistant queries related to the spoken utterance based on at least a portion of the context of the conversation session and at least a portion of the assistant queries included in the spoken utterance. In various implementations, each LLM output can be determined further based on an assistant query included in the spoken utterance captured in the audio data stream. In some implementations, one or more LLM outputs may have been previously generated in an offline manner when the modified set of assistant outputs is generated (e.g., before the spoken utterance is received and the offline output modification engine 170 is used, as described above with reference to fig. 3). In these implementations, the system can determine that the assistant query included in the spoken utterance captured in the audio data stream corresponds to a previous assistant query for which one or more LLM outputs have been previously generated, that the context of the conversation session corresponds to a previous context of a previous conversation session in which the previous assistant query was received, and/or that the one or more assistant outputs included in the set of assistant outputs determined at block 454 correspond to the one or more previous assistant outputs determined based on the previous query. Further, the system can obtain one or more LLM outputs (e.g., using the online output modification engine 180) indexed based on previous assistant queries, previous contexts, and/or one or more previous assistant outputs as described with respect to the method 300 of fig. 3, respectively, corresponding to one or more assistant outputs in the set of assistant outputs, and utilize the one or more LLM outputs as a modified set of assistant outputs.
In additional or alternative implementations, upon generating the modified set of assistant outputs, the system can process at least the context of the conversation session and/or one or more assistant outputs included in the set of assistant outputs in an online manner (e.g., in response to receiving the spoken utterance and using the online output modification engine 180) to generate one or more LLM outputs. For example, the system can cause the LLM engines 150A1 and/or 150A2 to process the context of the conversation session, the assistant queries, and/or one or more assistant outputs included in the assistant output set using one or more LLMs to generate a modified assistant output set. The one or more LLM outputs may be generated in an online manner in the same or similar manner as described above with respect to generating the one or more LLM outputs in an offline manner with reference to block 354 of the method 300 of fig. 3, but in response to receiving the spoken utterance at the client device. For example, the system may process one or more of the assistant outputs included in the set of assistant outputs using one or more LLMs to generate one or more personality replies for each of the one or more assistant outputs included in the set of assistant outputs, as described above with respect to block 354 of method 300 of fig. 3. In other words, each assistant output included in the set of assistant outputs may have a limited vocabulary and consistent personality in terms of prosodic attributes associated with each assistant output. However, in processing each of the assistant outputs included in the set of assistant outputs to generate a set of modified assistant outputs, each modified assistant output may have a vocabulary that is much larger and has a greater variance in prosodic attributes associated with each modified assistant output according to the use of one or more LLMs to generate one or more modified assistant outputs. As a result, each modified assistant output may correspond to a contextually relevant assistant output that better resonates with a user in a conversational session with the automated assistant.
Similarly, in some implementations, additional assistants may have been previously generated in an offline manner when additional assistant queries were generated (e.g., before a spoken utterance was received and the engine 170 was modified using offline output, as described above with respect to fig. 3). In these embodiments, and similar to that described above with respect to obtaining one or more LLM outputs previously generated in an offline manner, the system can obtain additional assistant queries indexed based on previous assistant queries, previous contexts, and/or one or more previous assistant outputs that respectively correspond to assistant queries, contexts, and/or one or more assistant outputs included in the set of assistant outputs as described with respect to method 300, and utilize the previously generated additional assistant queries as additional assistant queries.
Also, similarly, in additional or alternative embodiments, upon generating the additional assistant query, the system may process at least the context of the conversation session and/or one or more assistant outputs included in the set of assistant outputs in an online manner (e.g., in response to receiving the spoken utterance and using online output modification engine 180) to generate the additional assistant query. For example, the system can cause the LLM engines 150A1 and/or 150A2 to process the context of the conversation session, the assistant query, and/or one or more assistant outputs included in the set of assistant outputs using one or more LLMs to generate additional assistant queries. The additional assistant query may be generated in an online manner in the same or similar manner as described above with respect to generating the additional assistant query in an offline manner with reference to block 354 of the method 300 of fig. 3, but in response to receiving the spoken utterance at the client device. In some implementations, one or more LLMs described herein may have multiple distinct layers dedicated to performing certain functions. For example, one or more first layers of one or more LLMs may be used to generate the personality replies described herein, and one or more second layers of one or more LLMs may be used to generate the additional assistant queries described herein. In additional or alternative embodiments, one or more LLMs may communicate with one or more additional layers not included in one or more LLMs in generating additional assistant queries described herein. For example, one or more layers of one or more LLMs may be used to generate personality replies described herein, and one or more additional layers of another ML model in communication with the one or more LLMs may be used to generate additional assistant queries described herein. Non-limiting examples of personality replies and additional assistant queries are described in more detail below with reference to FIG. 6.
At block 458, the system determines additional assistant outputs responsive to the additional assistant queries based on the additional assistant queries. In some implementations, the system can cause additional assistant queries to be processed by one or more of 1P system 191 and/or 3P system 192 in the same or similar manner as described with respect to the processing assistant queries in fig. 2 to generate additional assistant outputs. In some implementations, the additional assistant output may be a single additional assistant output, while in other implementations, the additional assistant output may be included in an additional set of assistant outputs (e.g., similar to the additional set of assistant outputs 205 of fig. 2). In additional or alternative implementations, the additional assistant query can be mapped directly to the additional assistant output based on, for example, user profile data of the user providing the spoken utterance and/or any other data accessible by the automated assistant.
At block 460, the system processes the modified set of assistant outputs based on the additional assistant outputs in response to the additional assistant queries to generate an additional modified set of assistant outputs. In some implementations, the system can append (prepended) or append (applied) additional assistant outputs to the one or more modified assistant outputs to each of the one or more assistant outputs comprising the modified set of assistant outputs generated at block 456. In additional or alternative implementations and as indicated at block 460A, the system can process the context of the additional assistant output and the conversation session to generate an additional modified set of assistant outputs using one or more LLM outputs used at block 456 and/or one or more additional LLM outputs generated based on at least a portion of the context of the conversation session and at least a portion of the additional assistant output in addition to the one or more LLM outputs used at block 456. The additional modified set of assistant outputs may be generated in the same or similar manner as described above with respect to generating the modified set of assistant outputs but based on additional assistant outputs instead of the set of assistant outputs (e.g., using one or more LLM outputs generated using LLM engines 150A1 and/or 150A2 in an offline manner and/or in an online manner).
At block 462, the system causes a given modified assistant output from the modified set of assistant outputs and/or a given additional modified assistant output from the additional modified set of assistant outputs to be provided for presentation to the user. In some implementations, the system can cause the ranking engine 190 to rank each of the one or more modified assistant outputs included in the modified set of assistant outputs (and optionally each of the one or more assistant outputs included in the set of assistant outputs) according to one or more ranking criteria and select a given modified assistant output from the modified set of assistant outputs (or select a given assistant output from the set of assistant outputs). Further, the system may also cause the ranking engine 190 to rank each of the one or more additional modified assistant outputs (and optionally additional assistant outputs) included in the additional modified set of assistant outputs according to one or more ranking criteria and select a given additional modified assistant output from the additional modified set of assistant outputs (or select an additional assistant output as a given additional assistant output). In these embodiments, the system may combine a given modified assistant output and a given additional assistant output and cause the given modified assistant output and the given additional assistant output to be provided for visual and/or audible presentation to a user of a client device in a conversational session with the automated assistant.
Turning now to fig. 5, a flow diagram is depicted illustrating an example method 500 of generating an assistant output based on generating an assistant personality reply using a large language model. For convenience, the operations of method 500 are described with reference to a system performing the operations from process flow 200 of fig. 2. The system of method 500 includes one or more processors, memory, and/or other components of a computing device (e.g., client device 110 of fig. 1, client device 610 of fig. 6, and/or computing device 710 of fig. 7, one or more servers, and/or other computing devices). Furthermore, although the operations of method 500 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 552, the system receives an audio data stream that captures a spoken utterance of a user, the spoken utterance including an assistant query directed to an automated assistant, and the spoken utterance being received during a conversation session between the user and the automated assistant. At block 554, the system determines a set of assistant outputs based on processing the audio data stream, each assistant output included in the set being responsive to an assistant query included in the spoken utterance. Operations of blocks 552 and 554 of method 500 of fig. 5 may be performed in the same or similar manner as described with respect to blocks 452 and 454, respectively, of method 400 of fig. 4.
At block 556, the system determines whether to modify one or more of the assistant outputs included in the set of assistant outputs. The system may determine whether to modify one or more of the assistant outputs based on, for example, a user's intent to provide a spoken utterance (e.g., included in a stream of NLU outputs 204), one or more of the assistant outputs included in a set of assistant outputs (e.g., set of assistant outputs 205), one or more computational costs (e.g., battery consumption, processor consumption, latency, etc.) associated with modifying one or more of the assistant outputs included in the set of assistant outputs, a duration of interaction with an automated assistant, and/or other considerations. For example, if the user's intent indicates that the user providing the spoken utterance desires a quick and/or factual answer (e.g., "Why is the sky blue. In addition, for example, if the user's client device is in a power saving mode, the system may determine not to modify one or more of the assistant outputs to save battery power. Additionally, for example, if the duration that the user has engaged in a conversation session exceeds a threshold duration (e.g., 30 seconds, 1 minute, etc.), the system may determine not to modify one or more assistant outputs in an attempt to end the conversation session in a faster and efficient manner.
If, at the iteration of block 556, the system determines that one or more of the assistant outputs included in the set of assistant outputs are not to be modified, the system may proceed to block 558. At block 558, the system causes a given assistant output from the set of assistant outputs to be provided for presentation to the user. For example, the system may cause the ranking engine 190 to rank each of the assistant outputs included in the set of assistant outputs according to one or more ranking criteria and select a given assistant output to provide for visual and/or audible presentation to the user based on the ranking.
If, at the iteration of block 556, the system determines to modify one or more of the assistant outputs included in the set of assistant outputs, the system may proceed to block 560. At block 560, the system processes the set of assistant outputs and the context of the conversation session to generate a modified set of assistant outputs using one or more LLM outputs, each of the one or more LLM outputs determined based on the context of the conversation session and/or one or more assistant outputs included in the set of assistant outputs, and each of the one or more LLM outputs reflecting a corresponding personality of the automated assistant from among a plurality of disparate personalities. As described with respect to block 354 of the method 300 of fig. 3, various distinct parameters may be used to generate one or more LLM outputs to reflect different personalities of an automated assistant. In various implementations, each LLM output can be determined further based on an assistant query included in the spoken utterance captured in the audio data stream. In some implementations, the system can process the context of the conversation session (and optionally the assistant query) and the set of assistant outputs to generate a modified set of assistant outputs using one or more of the LLM outputs previously generated in an offline manner as described herein, while in additional or alternative implementations, the system can process the context of the conversation session (and optionally the assistant query) and the set of assistant outputs to generate a modified set of assistant outputs in an online manner as described herein. As noted above with respect to method 400 of fig. 4, non-limiting examples of personality replies and additional assistant queries are described in more detail below with respect to fig. 6.
At block 562, the system causes a given assistant output from the set of assistant outputs to be provided for presentation to the user. In some implementations, the system can cause the ranking engine 190 to rank each of the one or more modified assistant outputs included in the modified set of assistant outputs (and optionally each of the one or more assistant outputs included in the set of assistant outputs) according to one or more ranking criteria and select a given modified assistant output from the modified set of assistant outputs (or select a given assistant output from the set of assistant outputs). Further, the system may cause a given modified assistant output to be provided for visual and/or audible presentation to the user.
Although FIG. 5 is not described with respect to generating any additional assistant queries, it should be understood that this is for purposes of illustration and is not meant to be limiting. Rather, it should be appreciated that generating additional assistant queries in the method 400 of FIG. 4 is based on determining that additional assistant outputs are contextually relevant, which may be provided for facilitating a conversational session and with respect to providing a more natural conversational experience for the user, such that any assistant output provided for presentation to the user better reflects the conversational session between humans, and such that the conversational session between the user and the automated assistant is better resonated with the user. Further, while fig. 3 and 4 are not described with respect to determining whether to modify the set of assistant outputs, it should also be understood that this is for purposes of example and is not meant to be limiting. Rather, it should be appreciated that determining whether to use one or more LLMs to trigger modification of the set of assistant responses may be performed in any of the example methods of fig. 3, 4, and 5.
Turning now to fig. 6, a non-limiting example of a dialog session between a user and an automated assistant is depicted, wherein the automated assistant utilizes one or more LLMs to generate assistant output. As described herein, in some implementations, an automated assistant can utilize one or more LLM outputs previously generated in an offline manner to generate a modified set of assistant outputs (e.g., as described above with respect to method 300 of fig. 3). For example, the automated assistant may determine that a previous assistant query for which one or more LLM outputs have been previously generated corresponds to an assistant query included in the spoken utterance, that a previous context of a previous dialog session corresponds to a context of a dialog session between the user and the automated assistant in which the spoken utterance was received, and/or that one or more previous assistant outputs correspond to one or more assistant outputs included in a set of assistant outputs for the assistant query included in the spoken utterance. Further, the automated assistant can obtain one or more LLM outputs (e.g., in LLM output database 150A) indexed according to previous assistant queries, previous contexts, and/or one or more previous assistant outputs included in the previous assistant output set, and utilize the one or more LLM outputs as a modified assistant output set. In additional or alternative implementations, the automated assistant can process the assistant query, the context of the conversation session, and/or one or more assistant outputs included in the set of assistant outputs using one or more LLMs to generate one or more LLM outputs to be used as a modified set of assistant outputs in an online manner (e.g., as described above with respect to method 400 of fig. 4 and method 500 of fig. 5). Thus, the non-limiting example of fig. 6 is provided to illustrate how utilization of LLM in accordance with the techniques described herein results in improved natural conversation between a user and an automated assistant.
Client device 610 (e.g., an instance of client device 110 of fig. 1) may include various user interface components including, for example: a microphone for generating audio data based on the spoken utterance and/or other audible input; a speaker 680 for audibly rendering synthesized speech and/or other audible output; and/or a display 680 for visually rendering visual output. Further, the display 680 of the client device 610 may include various system interface elements 681, 682, and 683 (e.g., hardware and or software interface elements) that may be interacted with by a user of the client device 610 to cause the client device 610 to perform one or more actions. The display 680 of the client device 610 enables a user to interact with content rendered on the display 680 by touch input (e.g., by directing user input to the display 680 or portions thereof (e.g., to a text input box (not depicted), to a keyboard (not depicted), or to other portions of the display 680)) and/or by spoken input (e.g., by selecting the microphone interface element 684 at the client device 610-or by speaking only without having to select the microphone interface element 684 (i.e., the automated assistant may monitor one or more terms or phrases, gestures, gaze, mouth movements, lip movements, and/or other conditions that activate spoken input)). Although the client device 610 depicted in fig. 6 is a mobile telephone, it should be understood that this is for illustration only and is not meant to be limiting. For example, the client device 610 may be a standalone speaker with a display, a standalone speaker without a display, a home automation device, an in-vehicle system, a laptop computer, a desktop computer, and/or any other device capable of executing an automated assistant to conduct a human-machine conversation session with a user of the client device 610.
For example, assume that the user of client device 610 provides "Hey Assistant, what time is it? (he assisted, now a few. In this example, the automated assistant can cause the use of the ASR engines 130A1 and/or 130A2 to process the audio data capturing the spoken utterance 652 to generate an ASR output stream. Furthermore, the automation assistant can cause the ASR output stream to be processed using NLU engines 140A1 and/or 140A2 to generate an NLU output stream. Further, the automated assistant can cause the NLU output streams to be processed by one or more of 1P system 191 and/or 3P system 192 to generate a set of one or more assistant outputs. The set of assistant outputs may include, for example, "8:30am (8:30 am)", "Good moving," it's 8:30am (Good in the morning, now 8:30 am) ", and/or any other output that conveys the current time to the user of the client device 610.
In the example of fig. 6, it is further assumed that the automated assistant determines to modify one or more of the assistant outputs included in the set of assistant outputs to generate a modified set of assistant outputs. For example, the automated assistant may determine that the assistant query included in the spoken utterance 652 requests that the automated assistant provide the current time for presentation to the user. The automated assistant may determine that an instance of a previous assistant query for which the automated assistant was requested to provide the current time to present to the user for which one or more LLM outputs have been previously generated corresponds to the assistant query included in the spoken utterance 652 of fig. 6, that a previous context of a previous conversation session corresponds to a context of a conversation session between the user and the automated assistant in fig. 6 (e.g., the user requested the automated assistant to provide the current time in the morning (and optionally make a request by initiating a conversation session), that the client device 610 is located at a particular location, and/or other contextual signals), and/or that one or more previous assistant outputs correspond to one or more assistant outputs included in a set of assistant outputs for the assistant query included in the spoken utterance 652 of fig. 6. Further, the automated assistant can obtain one or more LLM outputs indexed according to previous assistant queries, previous contexts, and/or one or more previous assistant outputs included in a previous set of assistant outputs (e.g., in LLM output database 150A), and utilize the one or more LLM outputs as a modified set of assistant outputs. In addition, for example, the automated assistant can cause one or more LLMs to be used to process the assistant query, the context of the conversation session, and/or one or more assistant outputs included in the set of assistant outputs to generate one or more LLM outputs in an online manner for use as a modified set of assistant outputs.
In the example of FIG. 6, further assume that the automated assistant determines to provide a modified assistant output 654"Good morning[User ] +|! It's 8:30AM.Any fun plans today? (morning good user | is now 8:30am what interesting plans are today. The modified assistant output 654 provided for presentation to the User is personalized or customized for the context of the User and conversation session of the client device 610, as the modified assistant output 654 calls the User with a contextually appropriate greeting (e.g., "Good turn") and names the User of the client device 610 (e.g., "User"). Notably, the one or more LLM outputs can include one or more corresponding placeholders (e.g., as indicated by "[ User ] ([ User ]) |" in the modified assistant output 654) that can be populated with User profile data accessible to the automated assistant. While the example of fig. 6 includes a corresponding placeholder for the name of the user of client device 610, it should be understood that this is for purposes of example and is not meant to be limiting. For example, the one or more corresponding placeholders may be populated with any data accessible to the automated assistant, such as a smart networking device identifier (e.g., a smart light, a smart TV, a smart appliance, a smart speaker, a smart door lock, etc.), a known location associated with the user of the client device 610 (e.g., a city, a state, a county, a region, a province, a country, a physical address of a work place, a business of the user, or a primary residence of the user of the client device 610, etc.), an entity reference (e.g., an introducer, a place, a thing, etc.), a software application accessible at the user's client device 610, and/or any other data accessible to the automated assistant.
Further, the modified assistant output 654 functions in responding to assistant queries (e.g., "It's 8:30am (now 8:30 am)") included in the spoken utterance 652. However, the modified assistant output 654 is not only personalized or customized to the user and plays a role in responding to the assistant query, but the modified assistant output 654 also helps drive the dialog session between the user and the automated assistant by further having the user conduct the dialog session (e.g., "Any fun plans today (what interesting plans are today. If the techniques described herein with respect to using one or more LLM outputs to modify the initially generated set of assistant outputs based on processing the spoken utterance 652 are not used, the automated assistant may simply reply to "It's 8:30am (now 8:30 a.m.") without providing any greetings (e.g., "Good turn" to the User of the client device 610), call the User of the client device 610 for a name (e.g., "User (User)") and do not further have the User of the client device 610 conduct a conversation session (e.g., "Any fun plans today (what is interesting today. Thus, the modified assistant output 654 may be better resonated with the user of the client device 610 than any assistant output included in the originally generated set of assistant outputs that do not utilize one or more LLM outputs.
In the example of fig. 6, further assume that the user of client device 610 provides a spoken utterance 656 of "Yes, I' mthinking about going to the beach (Yes, I are considering going to beach)". In this example, the automated assistant can cause processing of the audio data that captured the spoken utterance 656 to generate a set of assistant outputs that were generated without using one or more LLM outputs. Further, the automated assistant can cause processing of the assistant query, the set of assistant outputs, and/or the context of the conversation session included in the spoken utterance 656 to generate a modified set of assistant outputs (e.g., in an offline and/or online manner) that are determined using one or more LLM outputs, and optionally generate additional assistant queries based on the assistant query.
In this example, the assistant outputs included in the set of assistant outputs (generated without using one or more LLM outputs) may be limited because the assistant query included in the spoken utterance 656 does not request the automated assistant to perform any actions. For example, the assistant outputs included in the set of assistant outputs may include "sound fun-! (sound interesting |) "," Surf's up-! (surfing! (what sounds interesting |) "and/or other assistant outputs in response to spoken utterance 656, but without further enabling the user of client device 610 to conduct a conversation session. In other words, the assistant outputs included in the set of assistant outputs may be limited in terms of vocabulary changes because the assistant outputs are not generated using one or more LLM outputs as described herein. Nonetheless, the automated assistant can utilize the assistant outputs included in the set of assistant outputs to determine how to modify one or more assistant outputs using one or more LLM outputs.
Further, and as described with respect to fig. 3 and 4, the automated assistant can generate additional assistant queries based on the assistant queries and using one or more LLMs or different ML models in communication with the one or more LLMs. For example, in the example of fig. 6, the spoken utterance 656 provided by the user of the client device 610 indicates that the user is going to the beach. Based on recognizing that the intent associated with the spoken utterance 656 indicates that the user intends to go to the beach, the automated assistant may determine a related intent associated with weather for a beach (e.g., an example beach named "Half Moon Bay") that is frequently accessed by the user looking for the client device 610. Based on identifying the relevant intent, the automated assistant may generate "What's the weather" with the location parameter "Half Moon Bay"? (what does weather. In some implementations, the automated assistant can cause processing of the additional assistant output and/or the context of the conversation session to generate an additional modified set of assistant outputs determined using the one or more LLM outputs and/or the one or more additional LLM outputs.
In the example of fig. 6, the automated assistant may cause the assistant outputs included in the assistant output set and the modified assistant output set to rank according to one or more ranking criteria, and may select one or more assistant outputs based on the ranking (e.g., select a given assistant output "sound fun |)"). Further, the automated assistant may cause the assistant outputs and additional assistant outputs included in the additional modified set of assistant outputs to be ranked according to one or more ranking criteria, and may select one or more assistant outputs based on the ranking (e.g., select a given additional assistant output "But if you're going to Half Moon Bay again, expect rain and chilly temps. (But if you go to the half-moon bay again, it is expected to rain and overcast air temperature.)"). In addition, the automated assistant may combine the selected given assistant output and the selected given additional assistant output to produce a "sound fun-! The modified assistant output 658 of the But you're going to Half Moon Bay again, expect rain and chilly temps (which sounds interesting | But if you go to the half-moon bay again, is expected to rain and air temperature smoldering) and may cause the modified assistant output 658 to be provided for visual and/or audible presentation to the user of the client device 610. Thus, in this example, the automated assistant can process the spoken utterance 656 and provide additional contextual information associated with the spoken utterance (e.g., weather at the beach that the user of the client device 610 may access) to further enable the user of the client device 610 to conduct a conversation session. Without the techniques described herein, although an automated assistant is able to determine and provide weather information, a user of the client device 610 may be required to actively request weather information from the automated assistant, thereby increasing the number of user inputs and wasting computing resources at the client device 610 handling the increased number of user inputs and also increasing the cognitive load of the user of the client device 610.
In the example of fig. 6, further assume that the user of client device 610 provides spoken utterance 660 "ohno … thanks for the heads up, can you remind me to check the weather again in two hours? (do you … … thank you for a reminder, you can remind me to see the weather again after two hours. In this example, the automated assistant can cause processing of the audio data capturing the spoken utterance 660 to generate a set of assistant outputs that are generated without using one or more LLM outputs. Further, the automated assistant can cause processing of the context of the assistant query, the set of assistant outputs, and/or the dialog session included in the spoken utterance 660 to generate a modified set of assistant outputs (e.g., in an offline and/or online manner) that is determined using one or more LLM outputs. Based on processing the spoken utterance 660, the automated assistant may determine to set a reminder of 10:30 a.m. (e.g., two hours after a conversation session) that reminds the user of the client device 610 to check weather at "Half Moon Bay," or to actively provide weather at "Half Moon Bay" to the user of the client device 610 at 10:30 a.m. Further, the automated assistant may cause a modified assistant output 662"Sure thing,I set the reminder and hope the weather clears up for you (without problem, i set up reminders and hope that weather is you sunny) from a modified set of assistant outputs (i.e., generated using one or more LLM outputs) to be provided for visual and/or audible presentation to the user of the client device 610. Notably, the modified assistant output 662 in the example of fig. 6 is contextually relevant to the conversation session as it indicates to the user a weather wish to be sunny. Instead, the assistant outputs included in the set of assistant outputs (i.e., generated without using one or more LLM outputs) may only provide an indication to set the reminder, regardless of the context of the conversation session.
While fig. 6 is described with respect to generating a particular modified assistant output using one or more LLM outputs and based on a particular spoken utterance and context of a conversation session, and selecting a particular modified assistant output to provide for presentation to a user, it should be understood that this is for purposes of illustration and is not meant to be limiting. Rather, it should be understood that the techniques described herein may be used for any dialog session between any user and a corresponding instance of an automated assistant. Further, while transcription corresponding to a conversational session between the user and the automated assistant is depicted at the display 680 of the client device 610, it should also be understood that this is for purposes of illustration and is not meant to be limiting. For example, it should be appreciated that the conversation session may be performed at any device capable of executing an automated assistant, regardless of whether the client device includes a display.
Further, it should also be appreciated that providing the assistant output for presentation to the user in the conversation session of fig. 6 may include various personality replies described herein. For example, the modified assistant output 654 may be generated using a first set of parameters that reflect a first personality of the automated assistant in terms of a first vocabulary to be used by the automated assistant and/or a first set of prosodic attributes to be used to provide the modified assistant output 654 for audible presentation to the user. Further, the modified assistant output 658 can be generated using a second set of parameters reflecting a second personality of the automated assistant in terms of a second vocabulary to be used by the automated assistant and/or a second set of prosodic attributes to be used to provide the modified assistant output 658 for audible presentation to the user. In this example, the first personality may reflect, for example, a personality of a caretaker or a maid for providing a morning greeting, respond to the user requesting the current time, and ask the user if he/she has any plan for the day. Further, the second personality may reflect personalities such as weather forecasters, surfers, or life preservers (or a combination thereof) that indicate that going to the beach sounds interesting, but that the weather may not fit on the beach for one day. For example, "sound fun-! (interesting sound |) "section because the user indicates that he/she is planning to go to the beach, and the weather forecaster personality can be used to provide" But if you're going to Half Moon Bay again, expect rain and chilly temps (But if you go to the half-moon bay again, raining and cold air temperature is expected) "because the automated assistant is providing weather information to the user.
Thus, the automated assistant can dynamically adjust the personality for providing the modified assistant output for presentation to the user based on both the vocabulary used by the automated assistant and the prosodic attributes used in rendering the modified assistant output for audible presentation to the user. Notably, the automated assistant can dynamically adjust these personalities used in providing the modified assistant output based on the context of the conversation session, including the previous spoken utterances received from the user and the previous assistant output provided by the automated assistant and/or any other contextual signals described herein. As a result, the modified assistant output provided by the automated assistant may better resonate with the user of the client device.
Further, while FIG. 6 is described herein with respect to a user providing spoken utterances throughout a conversation session, it should be understood that this is for purposes of illustration and is not meant to be limiting. For example, the user may additionally or alternatively provide typed input and/or touch input throughout the conversation session. In these implementations, the automated assistant can process the typed input (e.g., using NLU engine 140A1 and/or 140 A2) to generate an NLU output stream (e.g., and optionally skip any processing using ASR engine 130A1 and/or 130 A2), and can process NLU data streams and text input corresponding to assistant queries derived from typed input and/or touch input (e.g., using LLM engine 150A1 and/or 150 A2) to generate a set of one or more modified assistant outputs in the same or similar manner as described above.
Turning now to fig. 7, a block diagram of an example computing device 710 is depicted that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client device, cloud-based automation assistant component, and/or other components may include one or more components of the example computing device 710.
Computing device 710 typically includes at least one processor 714 that communicates with a number of peripheral devices via bus subsystem 712. These peripheral devices may include a storage subsystem 724 (including, for example, a memory subsystem 725 and a file storage subsystem 726), a user interface output device 720, a user interface input device 722, and a network interface subsystem 716. Input and output devices allow users to interact with computing device 710. The network interface subsystem 716 provides an interface to external networks and couples to corresponding interface devices among other computing devices.
User interface input devices 722 may include a keyboard, a pointing device such as a mouse, trackball, touch pad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways of inputting information into computing device 710 or onto a communication network.
The user interface output device 720 may include a display subsystem, a printer, a facsimile machine, or a non-visual display, such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a viewable image. The display subsystem may also provide for non-visual display, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 710 to a user or another machine or computing device.
Storage subsystem 724 stores programming and data structures that provide functionality for some or all of the modules described herein. For example, the storage subsystem 724 may include logic to perform selected aspects of the methods disclosed herein and to implement the various components depicted in fig. 1 and 2.
These software modules are typically executed by processor 714 alone or in combination with other processors. The memory 725 used in the storage subsystem 724 may include a number of memories including a main Random Access Memory (RAM) 730 for storing instructions and data during program execution and a Read Only Memory (ROM) 732 in which fixed instructions are stored. File storage subsystem 726 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive, and associated removable media, CD-ROM drive, optical disk drive, or removable media cartridge. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 726 in storage subsystem 724, or in other machines accessible to processor 714.
Bus subsystem 712 provides a mechanism for allowing the various components and subsystems of computing device 710 to communicate with each other as intended. Although bus subsystem 712 is shown schematically as a single bus, alternative embodiments of bus subsystem 712 may use multiple buses.
Computing device 710 may be of different types including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 710 depicted in FIG. 7 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computing device 710 are possible with more or fewer components than the computing device depicted in fig. 7.
Where the system described herein collects or otherwise monitors personal information about a user or may utilize personal and/or monitored information, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social behavior or activity, profession, user preferences, or the user's current geographic location), or whether and/or how content that may be more relevant to the user is received from a content server. In addition, some data may be processed in one or more ways prior to storage or use such that personal identification information is deleted. For example, the identity of the user may be processed such that personal identity information of the user cannot be determined, or the geographic location of the user may be generalized (such as to a city, zip code, or state level) where geographic location information is obtained such that a particular geographic location of the user cannot be determined. Thus, the user may control how information about the user is collected and/or used.
In some implementations, a method implemented by one or more processors is provided, and includes: as part of a conversational session between a user of a client device and an automated assistant implemented by the client device: receiving an audio data stream that captures a spoken utterance of a user, the audio data stream generated by one or more microphones of a client device, and the spoken utterance including an assistant query; determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to an assistant query included in the spoken utterance; the processing assistant outputs the set and the context of the dialog session to: generating a modified set of assistant outputs using one or more language model (LLM) outputs generated using a large LLM, each of the one or more LLM outputs determined based on at least a portion of a context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs, and generating an additional assistant query related to the spoken utterance based on at least a portion of the context of the conversation session and at least a portion of the assistant query; determining additional assistant outputs responsive to the additional assistant queries based on the additional assistant queries; processing the additional assistant output and the context of the conversation session to generate an additional modified set of assistant outputs using one or more of LLM outputs generated with LLM or one or more additional LLM outputs, each of the additional LLM outputs determined based on at least a portion of the context of the conversation session and the additional assistant output; and causing a given modified assistant output from the modified set of assistant outputs and a given additional modified assistant output from the additional modified set of assistant outputs to be provided for presentation to the user.
These and other embodiments of the technology disclosed herein may optionally include one or more of the following features.
In some implementations, determining an assistant output responsive to an assistant query included in the spoken utterance based on processing the audio data stream can include: processing the audio data stream using an Automatic Speech Recognition (ASR) model to generate an ASR output stream; processing the ASR output stream using a Natural Language Understanding (NLU) model to generate an NLU data stream; and causing a set of assistant outputs to be determined based on the NLU stream.
In some versions of those implementations, processing the set of assistant outputs and the context of the conversation session to generate a modified set of assistant outputs using one or more of the LLM outputs generated with LLM may include: using the LLM processing assistant output sets and the context of the dialog session to generate one or more of the LLM outputs; and determining a modified set of assistant outputs based on one or more of the LLM outputs. In some further versions of those embodiments, using the LLM to process the set of assistant outputs and the context of the conversation session to generate one or more of the LLM outputs may include: the assistant output set and the context of the conversation session are processed using a first LLM parameter set of the plurality of distinct LLM parameter sets to determine one or more of the LLM outputs having a first personality of the plurality of distinct personalities. The modified set of assistant outputs may include one or more first personality assistant outputs reflecting the first personality. In yet another version of those embodiments, using the LLM to process the context of the dialog session and the set of assistant outputs to generate one or more of the LLM outputs may include: the assistant output set and the context of the conversation session are processed using a second LLM parameter set of the plurality of distinct LLM parameter sets to determine one or more of the LLM outputs having a second personality of the plurality of distinct personalities. The modified set of assistant outputs may include one or more second personality assistant outputs reflecting a second personality, and the second personality may be different from the (unique from) first personality. In yet further versions of those embodiments, a first vocabulary associated with the first personality may be used to determine one or more first personality assistant outputs included in the modified set of assistant outputs and reflecting the first personality, and a second vocabulary associated with the second personality may be used to determine one or more second personality assistant outputs included in the modified set of assistant outputs and reflecting the second personality, and wherein the second personality is different from the first personality based on the second vocabulary being different from the first vocabulary. In further additional or alternative versions of those embodiments, the one or more first personality assistant outputs included in the modified set of assistant outputs and reflecting the first personality may be associated with a first set of prosodic attributes for use in providing the given modified assistant output for audible presentation to the user, the one or more second personality assistant outputs included in the modified set of assistant outputs and reflecting the second personality may be associated with a second set of prosodic attributes for use in providing the given modified assistant output for audible presentation to the user, and the second personality may be different from the first personality based on the second set of prosodic attributes being different from the first set of prosodic attributes.
In some versions of those implementations, processing the set of assistant outputs and the context of the conversation session to generate a modified set of assistant outputs using one or more of the LLM outputs generated with LLM may include: identifying one or more of the LLM outputs that were previously generated using the LLM model based on one or more of the LLM outputs being previously generated based on previous assistant queries of previous dialog sessions that correspond to assistant queries of the dialog session and/or based on one or more of the LLM outputs being previously generated for previous contexts of previous dialog sessions that correspond to contexts of the dialog session; and causing modification of the set of assistant outputs with one or more of the LLM outputs to determine a modified set of assistant outputs. In some further versions of those embodiments, identifying one or more of the LLM outputs previously generated using the LLM model may include: one or more first LLM outputs of the one or more LLM outputs reflecting a first personality of the plurality of disparate personalities are identified. The modified set of assistant outputs includes one or more first personality assistant outputs reflecting the first personality. In further versions of those embodiments, identifying one or more of the LLM outputs previously generated using the LLM model may include: one or more second LLM outputs of the one or more LLM outputs reflecting a second personality of the plurality of disparate personalities are identified. The modified set of assistant outputs may include one or more second personality assistant outputs reflecting a second personality, and the second personality may be different from the first personality. In further additional or alternative versions of those embodiments, the method may further include: the method includes determining that a prior assistant query of a prior dialog session corresponds to an assistant query of the dialog session based on the ASR outputting one or more terms of the assistant query that include one or more terms of the prior assistant query of the prior dialog session. In further additional or alternative versions of these embodiments, the method may further comprise: generating an embedding of the assistant query based on one or more terms in the ASR output that correspond to the assistant query; and determining that the previous assistant query of the previous dialog session corresponds to the assistant query of the dialog session based on comparing the embedding of the assistant query with the previously generated embedding of the previous assistant query of the previous dialog session. In further additional or alternative versions of these embodiments, the method may further comprise: based on the one or more context signals of the conversation session corresponding to the one or more context signals of the previous conversation session, a previous context of the previous conversation session corresponding to the context of the conversation session is determined. In still other versions of those embodiments, the one or more context signals may include one or more of the following: time of day, day of the week, location of the client device, ambient noise in the environment of the client device. In further additional or alternative versions of these embodiments, the method may further comprise: generating an embedding of the context of the conversation session based on the context signal of the conversation session; and determining that the previous context of the previous dialog session corresponds to the context of the dialog session based on comparing the embedding of the one or more context signals with the previously generated embedding of the previous context of the previous dialog session.
In some versions of those implementations, processing the context of the conversation session and the set of assistant outputs to generate additional assistant queries related to the spoken utterance based on at least a portion of the context of the conversation session and at least a portion of the assistant queries may include: determining intent associated with the assistant query included in the spoken utterance based on the NLU output; identifying at least one relevant intent related to the intent associated with the assistant query included in the spoken utterance based on the intent associated with the assistant query included in the spoken utterance; and generating additional assistant queries related to the spoken utterance based on the at least one related intent. In some further versions of those implementations, determining additional assistant outputs responsive to the additional assistant queries based on the additional assistant queries may include: such that the additional assistant query is transmitted to the one or more first party systems via an Application Programming Interface (API) to generate an additional assistant output responsive to the additional assistant query. In some additional or alternative further versions of those implementations, determining additional assistant outputs responsive to the additional assistant queries based on the additional assistant queries may include: causing additional assistant queries to be transmitted to one or more third party systems over one or more networks; and receiving additional assistant output responsive to the additional assistant query in response to the additional assistant query being transmitted to one or more of the third party systems. In some additional or alternative further versions of those embodiments, processing the context of the additional assistant output and the conversation session to generate an additional modified set of assistant outputs using one or more of the LLM outputs determined with the LLM or one or more of the additional LLM outputs may include: processing the set of additional assistant outputs and the context of the conversation session using LLM to determine one or more of the additional LLM outputs; and determining an additional modified set of assistant outputs based on one or more of the additional LLM outputs. In some additional or alternative further versions of those embodiments, processing the context of the additional assistant output and the conversation session to generate an additional modified set of assistant outputs using one or more of the LLM outputs determined with the LLM or one or more of the additional LLM outputs may include: identifying one or more of the additional LLM outputs that were previously generated using the LLM model based on one or more of the additional LLM outputs being previously generated based on previous assistant queries of previous dialog sessions that correspond to the additional assistant queries of the dialog session and/or based on one or more of the additional LLM outputs being previously generated for previous contexts of the previous dialog session that correspond to the contexts of the dialog session; and causing modification of the additional set of assistant outputs with one or more of the additional LLM outputs to generate an additional modified set of assistant outputs.
In some embodiments, the method may further comprise: ranking a superset of assistant outputs based on one or more ranking criteria, the superset of assistant outputs including at least a set of assistant outputs and a modified set of assistant outputs; and selecting a given modified assistant output from the modified set of assistant outputs based on the ranking. In some versions of those embodiments, the method may further comprise: ranking a superset of additional assistant outputs based on one or more ranking criteria, the superset of assistant outputs including at least the additional assistant outputs and the additional modified set of assistant outputs; and selecting a given additional modified assistant output from the set of additional modified assistant outputs based on the ranking. In some further versions of those embodiments, causing the given modified assistant output and the given additional modified assistant output to be provided for presentation to the user may include: combining a given modified assistant output with a given additional modified assistant output; processing the given modified assistant output and the given additional modified assistant output using a text-to-speech (TTS) model to generate synthesized speech audio data, the synthesized speech audio data including synthesized speech capturing the given modified assistant output and the given additional modified assistant output; and causing the synthesized speech audio data to be audibly rendered for presentation to the user via a speaker of the client device.
In some embodiments, the method may further comprise: ranking a superset of assistant outputs based on one or more ranking criteria, the superset of assistant outputs comprising a set of assistant outputs, a modified set of assistant outputs, an additional set of assistant outputs, and an additional modified set of assistant outputs; and selecting a given modified assistant output from the set of modified assistant outputs and selecting a given additional modified assistant output from the set of additional modified assistant outputs based on the ranking. In some further versions of those embodiments, causing the given modified assistant output and the given additional modified assistant output to be provided for presentation to the user may include: processing the given modified assistant output and the given additional modified assistant output using a text-to-speech (TTS) model to generate synthesized speech audio data, the synthesized speech audio data including synthesized speech capturing the given modified assistant output and the given additional modified assistant output; and causing the synthesized speech audio data to be audibly rendered for presentation to the user via a speaker of the client device.
In some implementations, generating the modified set of assistant outputs using one or more of the LLM outputs can be further based on processing at least a portion of the assistant query included in the spoken utterance.
In some implementations, a method implemented by one or more processors is provided, and includes: as part of a conversational session between a user of a client device and an automated assistant implemented by the client device: receiving an audio data stream that captures a spoken utterance of a user, the audio data stream generated by one or more microphones of a client device, and the spoken utterance including an assistant query; determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to an assistant query included in the spoken utterance; the processing assistant outputs the set and the context of the dialog session to: generating a modified set of assistant outputs using one or more Large Language Model (LLM) outputs generated, each of the one or more LLM outputs determined based on at least a portion of a context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs, and generating an additional assistant query related to the spoken utterance based on at least a portion of the context of the conversation session and at least a portion of the assistant query; determining additional assistant outputs responsive to the additional assistant queries based on the additional assistant queries; processing the modified set of assistant outputs to generate an additional modified set of assistant outputs based on the additional assistant outputs in response to the additional assistant queries; and causing a given additional modified assistant output from among the additional modified set of assistant outputs to be provided for presentation to the user.
In some implementations, a method implemented by one or more processors is provided, and includes: as part of a conversational session between a user of a client device and an automated assistant implemented by the client device: receiving an audio data stream that captures a spoken utterance of a user, the audio data stream generated by one or more microphones of a client device, and the spoken utterance including an assistant query; determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to an assistant query included in the spoken utterance; the method includes processing a set of assistant outputs and a context of a conversation session using one or more language model (LLM) outputs generated using a large LLM to generate a modified set of assistant outputs, each of the one or more LLM outputs determined based on at least a portion of the context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs. Generating a modified set of assistant outputs using one or more of the LLM outputs includes: a first set of personality replies is generated based on (i) a set of assistant outputs, (ii) a context of the conversation session, and (iii) one or more of the one or more LLM outputs reflecting a first personality of the plurality of disparate personalities. The method also includes causing a given modified assistant output from among the modified set of assistant outputs to be provided for presentation to the user.
In some implementations, a method implemented by one or more processors is provided, and includes: as part of a conversational session between a user of a client device and an automated assistant implemented by the client device: receiving an audio data stream that captures a spoken utterance of a user, the audio data stream generated by one or more microphones of a client device, and the spoken utterance including an assistant query; determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to an assistant query included in the spoken utterance; processing a set of assistant outputs and a context of a conversation session using one or more language model (LLM) outputs generated using a large LLM to generate a modified set of assistant outputs, each of the one or more LLM outputs determined based on at least a portion of the context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs; and causing a given modified assistant output from among the modified set of assistant outputs to be provided for presentation to the user.
In some implementations, a method implemented by one or more processors is provided, and includes: as part of a conversational session between a user of a client device and an automated assistant implemented by the client device: receiving an audio data stream that captures a spoken utterance of a user, the audio data stream generated by one or more microphones of a client device, and the spoken utterance including an assistant query; determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to an assistant query included in the spoken utterance; determining whether to modify one or more of the assistant outputs included in the set of assistant outputs based on processing the spoken utterance; responsive to determining to modify one or more of the assistant outputs included in the set of assistant outputs: processing a set of assistant outputs and a context of a conversation session using one or more language model (LLM) outputs generated using a large LLM to generate a modified set of assistant outputs, each of the one or more LLM outputs determined based on at least a portion of the context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs; and causing a given modified assistant output from among the modified set of assistant outputs to be provided for presentation to the user.
These and other embodiments of the technology disclosed herein may optionally include one or more of the following features.
In some implementations, determining whether to modify one or more of the assistant outputs included in the set of assistant outputs based on processing the spoken utterance may include: processing the audio data stream using an Automatic Speech Recognition (ASR) model to generate an ASR output stream; processing the ASR output stream using a Natural Language Understanding (NLU) model to generate an NLU data stream; identifying, based on the NLU data stream, a user's intent to provide a spoken utterance; and determining whether to modify the assistant output based on the user's intent to provide the spoken utterance.
In some implementations, determining whether to modify one or more of the assistant outputs included in the set of assistant outputs may be further based on one or more computational costs associated with modifying one or more of the assistant outputs. In some versions of those embodiments, the one or more computational costs associated with modifying one or more of the assistant outputs may include one or more of: battery consumption associated with modifying one or more of the assistant outputs, processor consumption, or latency associated with modifying one or more of the assistant outputs.
In some implementations, a method implemented by one or more processors is provided, and includes: obtaining a plurality of assistant queries directed to the automated assistant and a corresponding context for a corresponding previous dialog session for each of the plurality of assistant queries; for each of a plurality of assistant queries: processing a given assistant query of the plurality of assistant queries using one or more Large Language Models (LLMs) to generate a corresponding LLM output responsive to the given assistant query; and indexing, in memory accessible at the client device, a corresponding LLM output based on the given assistant query and/or a corresponding context of a corresponding previous dialog session for the given assistant query; and after indexing the corresponding LLM output in memory accessible at the client device, and as part of a current dialog session between a user of the client device and an automated assistant implemented by the client device: receiving an audio data stream capturing a spoken utterance of a user, the audio data stream generated by one or more microphones of a client device; based on processing the audio data stream, determining that the spoken utterance includes a current assistant query corresponding to the given assistant query and/or that the spoken utterance is received in a current context of a current dialog session corresponding to a corresponding context of a previous dialog session corresponding to the given assistant query; and causing the automated assistant to utilize the corresponding LLM output to generate an assistant output to be provided for presentation to the user in response to the spoken utterance.
These and other embodiments of the technology disclosed herein may optionally include one or more of the following features.
In some implementations, multiple assistant queries directed to an automated assistant may have been previously submitted by a user via a client device. In some implementations, multiple assistant queries directed to an automated assistant may have been previously submitted via respective client devices by multiple additional users in addition to the users of the client devices.
In some implementations, indexing corresponding LLM outputs in memory accessible at the client device can be based on embedding of a given assistant query generated when processing the given assistant query. In some implementations, indexing corresponding LLM outputs in memory accessible at the client device can be based on one or more terms or phrases included in a given assistant query generated when the given assistant query is processed. In some implementations, indexing the corresponding LLM output in memory accessible at the client device can be based on embedding of the corresponding context for the corresponding previous dialog session for the given assistant query. In some implementations, indexing the corresponding LLM output in memory accessible at the client device can be based on one or more context signals included in a corresponding context of a corresponding previous dialog session for a given assistant query.
Additionally, some implementations include one or more processors (e.g., a Central Processing Unit (CPU), a Graphics Processing Unit (GPU), and/or a Tensor Processing Unit (TPU)) of the one or more computing devices, wherein the one or more processors are operable to execute the instructions stored in the associated memory, and wherein the instructions are configured to cause execution of any of the foregoing methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by the one or more processors to perform any of the foregoing methods. Some embodiments also include a computer program product comprising instructions executable by one or more processors to perform any of the foregoing methods.
Claims (42)
1. A method implemented by one or more processors, the method comprising:
as part of a conversational session between a user of a client device and an automated assistant implemented by the client device:
receiving an audio data stream that captures a spoken utterance of the user, the audio data stream generated by one or more microphones of the client device, and the spoken utterance comprising an assistant query;
Determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to the assistant query included in the spoken utterance;
processing the set of assistant outputs and the context of the conversation session to:
generating a modified set of assistant outputs using one or more Large Language Model (LLM) outputs generated using LLM, each of the one or more LLM outputs determined based on at least a portion of the context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs, and
generating additional assistant queries related to the spoken utterance based on at least a portion of the context of the dialog session and at least a portion of the assistant queries;
determining, based on the additional assistant query, an additional assistant output responsive to the additional assistant query;
processing the additional assistant output and the context of the conversation session to generate an additional modified set of assistant outputs using one or more of the LLM outputs generated with the LLM or one or more additional LLM outputs, each of the additional LLM outputs determined based on at least a portion of the context of the conversation session and the additional assistant output; and
Such that a given modified assistant output from the modified set of assistant outputs and a given additional modified assistant output from the additional modified set of assistant outputs are provided for presentation to the user.
2. The method of claim 1, wherein determining the assistant output responsive to the assistant query included in the spoken utterance based on processing the audio data stream comprises:
processing the audio data stream using an Automatic Speech Recognition (ASR) model to generate an ASR output stream;
processing the ASR output stream using a Natural Language Understanding (NLU) model to generate an NLU data stream; and
such that the set of assistant outputs is determined based on the NLU stream.
3. The method of claim 2, wherein processing the set of assistant outputs and the context of the conversation session to generate the modified set of assistant outputs using one or more of the LLM outputs generated with the LLM comprises:
processing the set of assistant outputs and the context of the dialog session using the LLM to generate one or more of the LLM outputs; and
The modified set of assistant outputs is determined based on one or more of the LLM outputs.
4. The method of claim 3, wherein processing the set of assistant outputs and the context of the conversation session using the LLM to generate one or more of the LLM outputs comprises:
processing the assistant output set and the context of the conversation session using a first LLM parameter set of a plurality of disparate LLM parameter sets to determine one or more of the LLM outputs having a first personality of a plurality of disparate personalities,
wherein the modified set of assistant outputs includes one or more first personality assistant outputs reflecting the first personality.
5. The method of claim 4, wherein processing the set of assistant outputs and the context of the conversation session using the LLM to generate one or more of the LLM outputs comprises:
processing the assistant output set and the context of the conversation session using a second LLM parameter set of the plurality of disparate LLM parameter sets to determine one or more of the LLM outputs having a second personality of the plurality of disparate personalities,
Wherein the modified set of assistant outputs includes one or more second personality assistant outputs reflecting the second personality, and
wherein the second personality is different from the first personality.
6. The method of claim 5, wherein the one or more first personality assistant outputs included in the modified set of assistant outputs and reflecting the first personality are determined using a first vocabulary associated with the first personality, wherein the one or more second personality assistant outputs included in the modified set of assistant outputs and reflecting the second personality are determined using a second vocabulary associated with the second personality, and wherein the second personality is different from the first personality based on the second vocabulary being different from the first vocabulary.
7. The method of claim 5, wherein the one or more first persona assistant outputs included in the modified set of assistant outputs and reflecting the first persona are associated with a first set of prosodic attributes utilized in providing the given modified assistant output for audible presentation to the user, wherein the one or more second persona assistant outputs included in the modified set of assistant outputs and reflecting the second persona are associated with a second set of prosodic attributes utilized in providing the given modified assistant output for audible presentation to the user, and wherein the second persona is different from the first persona based on a second set of prosodic attributes being different from the first set of prosodic attributes.
8. The method of any of claims 2-7, wherein processing the set of assistant outputs and the context of the conversation session to generate the modified set of assistant outputs using one or more of the LLM outputs generated with the LLM comprises:
identifying one or more of the LLM outputs that were previously generated using the LLM model based on one or more of the LLM outputs being previously generated based on a previous assistant query of a previous dialog session that corresponds to the assistant query of the dialog session and/or based on one or more of the LLM outputs being previously generated for a previous context of the previous dialog session that corresponds to the context of the dialog session; and
such that the set of assistant outputs is modified with one or more of the LLM outputs to determine the modified set of assistant outputs.
9. The method of claim 8, wherein identifying one or more of the LLM outputs previously generated using the LLM model comprises:
identifying one or more first LLM outputs of the one or more LLM outputs reflecting a first personality of a plurality of disparate personalities,
Wherein the modified set of assistant outputs includes one or more first personality assistant outputs reflecting the first personality.
10. The method of claim 9, wherein identifying one or more of the LLM outputs previously generated using the LLM model comprises:
identifying one or more second LLM outputs of the one or more LLM outputs reflecting a second personality of a plurality of disparate personalities,
wherein the modified set of assistant outputs includes one or more second personality assistant outputs reflecting the second personality, and
wherein the second personality is different from the first personality.
11. The method of any of claims 8 to 10, further comprising:
based on the ASR output including one or more terms of an assistant query corresponding to one or more terms of the prior assistant query of the prior dialog session, the prior assistant query of the prior dialog session is determined to correspond to the assistant query of the dialog session.
12. The method of any of claims 8 to 11, further comprising:
generating an embedding of the assistant query based on one or more terms in the ASR output corresponding to the assistant query; and
Based on comparing the embedding of the assistant query with a previously generated embedding of the previous assistant query of the previous dialog session, it is determined that the previous assistant query of the previous dialog session corresponds to the assistant query of the dialog session.
13. The method of any of claims 8 to 12, further comprising:
based on one or more context signals of the dialog session corresponding to one or more context signals of the previous dialog session, the previous context of the previous dialog session is determined to correspond to the context of the dialog session.
14. The method of claim 13, wherein the one or more context signals comprise one or more of: time of day, day of week, location of the client device, ambient noise in the environment of the client device.
15. The method of any of claims 8 to 14, further comprising:
generating an embedding of the context of the dialog session based on a context signal of the dialog session; and
based on comparing the embedding of the one or more context signals with a previously generated embedding of the previous context of the previous dialog session, it is determined that the previous context of the previous dialog session corresponds to the context of the dialog session.
16. The method of any of claims 2-15, wherein processing the set of assistant outputs and the context of the conversation session to generate the additional assistant query related to the spoken utterance based on at least a portion of the context of the conversation session and at least a portion of the assistant query comprises:
determining intent associated with the assistant query included in the spoken utterance based on the NLU output;
identifying at least one relevant intent related to the intent based on the intent associated with the assistant query included in the spoken utterance, the intent associated with the assistant query included in the spoken utterance; and
the additional assistant query related to the spoken utterance is generated based on the at least one related intent.
17. The method of claim 16, wherein determining the additional assistant output responsive to the additional assistant query based on the additional assistant query comprises:
such that the additional assistant query is transmitted to one or more first party systems via an Application Programming Interface (API) to generate the additional assistant output in response to the additional assistant query.
18. The method of claim 16, wherein determining the additional assistant output responsive to the additional assistant query based on the additional assistant query comprises:
causing the additional assistant query to be transmitted to one or more third party systems over one or more networks; and
in response to the additional assistant query being transmitted to one or more of the third party systems, the additional assistant output is received in response to the additional assistant query.
19. The method of any of claims 16-18, wherein processing the additional assistant output and the context of the conversation session to generate the additional modified set of assistant outputs using one or more of the LLM outputs or one or more of the additional LLM outputs determined with the LLM comprises:
processing the set of additional assistant outputs and the context of the conversation session using the LLM to determine one or more of the additional LLM outputs; and
the additional modified set of assistant outputs is determined based on one or more of the additional LLM outputs.
20. The method of any of claims 16-18, wherein processing the additional assistant output and the context of the conversation session to generate the additional modified set of assistant outputs using one or more of the LLM outputs or one or more of the additional LLM outputs determined with the LLM comprises:
identifying one or more of the additional LLM outputs based on one or more of the additional LLM outputs previously generated using the LLM model being previously generated based on a previous assistant query of a previous dialog session corresponding to the additional assistant query of the dialog session and/or based on one or more of the additional LLM outputs being previously generated for a previous context of the previous dialog session corresponding to the context of the dialog session; and
such that the additional set of assistant outputs is modified with one or more of the additional LLM outputs to generate the additional modified set of assistant outputs.
21. The method of any preceding claim, further comprising:
Ranking a superset of assistant outputs based on one or more ranking criteria, the superset of assistant outputs including at least the set of assistant outputs and the modified set of assistant outputs; and
based on the ranking, the given modified assistant output is selected from the modified set of assistant outputs.
22. The method of claim 21, further comprising:
ranking a superset of additional assistant outputs based on one or more of the ranking criteria, the superset of assistant outputs including at least the additional assistant output and the additional modified set of assistant outputs; and
based on the ranking, the given additional modified assistant output is selected from the set of additional modified assistant outputs.
23. The method of claim 22, wherein causing the given modified assistant output and the given additional modified assistant output to be provided for presentation to the user comprises:
combining the given modified assistant output and the given additional modified assistant output;
processing the given modified assistant output and the given additional modified assistant output using a text-to-speech (TTS) model to generate synthesized speech audio data, the synthesized speech audio data including synthesized speech capturing the given modified assistant output and the given additional modified assistant output; and
Such that the synthesized speech audio data is audibly rendered for presentation to the user via a speaker of the client device.
24. The method of any preceding claim, further comprising:
ranking a superset of assistant outputs based on one or more ranking criteria, the superset of assistant outputs comprising the set of assistant outputs, the modified set of assistant outputs, the additional assistant outputs, and the additional modified set of assistant outputs; and
based on the ranking, the given modified assistant output is selected from the set of modified assistant outputs and the given additional modified assistant output is selected from the set of additional modified assistant outputs.
25. The method of claim 24, wherein causing the given modified assistant output and the given additional modified assistant output to be provided for presentation to the user comprises:
processing the given modified assistant output and the given additional modified assistant output using a text-to-speech (TTS) model to generate synthesized speech audio data, the synthesized speech audio data including synthesized speech capturing the given modified assistant output and the given additional modified assistant output; and
Such that the synthesized speech audio data is audibly rendered for presentation to the user via a speaker of the client device.
26. The method of any preceding claim, wherein generating the modified set of assistant outputs using one or more of the LLM outputs is further based on processing at least a portion of the assistant query included in the spoken utterance.
27. A method implemented by one or more processors, the method comprising:
as part of a conversational session between a user of a client device and an automated assistant implemented by the client device:
receiving an audio data stream that captures a spoken utterance of the user, the audio data stream generated by one or more microphones of the client device, and the spoken utterance comprising an assistant query;
determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to the assistant query included in the spoken utterance;
processing the set of assistant outputs and the context of the conversation session to:
generating a modified set of assistant outputs using one or more Large Language Model (LLM) outputs generated using LLM, each of the one or more LLM outputs determined based on at least a portion of the context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs, and
Generating additional assistant queries related to the spoken utterance based on at least a portion of the context of the dialog session and at least a portion of the assistant queries;
determining, based on the additional assistant query, an additional assistant output responsive to the additional assistant query;
processing the modified set of assistant outputs to generate an additional modified set of assistant outputs based on the additional assistant outputs in response to the additional assistant queries; and
such that a given additional modified assistant output from the set of additional modified assistant outputs is provided for presentation to the user.
28. A method implemented by one or more processors, the method comprising:
as part of a conversational session between a user of a client device and an automated assistant implemented by the client device:
receiving an audio data stream that captures a spoken utterance of the user, the audio data stream generated by one or more microphones of the client device, and the spoken utterance comprising an assistant query;
determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to the assistant query included in the spoken utterance;
Processing the set of assistant outputs and the context of the conversation session using one or more language model (LLM) outputs generated using LLM to generate a set of modified assistant outputs, each of the one or more LLM outputs determined based on at least a portion of the context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs, wherein generating the set of modified assistant outputs using one or more of the LLM outputs comprises:
based on (i) the set of assistant outputs, (ii) the context of the conversation session, and (iii) one or more of the one or more LLM outputs reflecting a first personality of a plurality of disparate personalities,
generating a first personality reply set; and
such that a given modified assistant output from the modified set of assistant outputs is provided for presentation to the user.
29. A method implemented by one or more processors, the method comprising:
as part of a conversational session between a user of a client device and an automated assistant implemented by the client device:
Receiving an audio data stream that captures a spoken utterance of the user, the audio data stream generated by one or more microphones of the client device, and the spoken utterance comprising an assistant query;
determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to the assistant query included in the spoken utterance;
processing the set of assistant outputs and a context of the conversation session using a Large Language Model (LLM) generated LLM output to generate a modified set of assistant outputs, each LLM output of the one or more LLM outputs determined based on at least a portion of the context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs; and
such that a given modified assistant output from the modified set of assistant outputs is provided for presentation to the user.
30. A method implemented by one or more processors, the method comprising:
as part of a conversational session between a user of a client device and an automated assistant implemented by the client device:
Receiving an audio data stream that captures a spoken utterance of the user, the audio data stream generated by one or more microphones of the client device, and the spoken utterance comprising an assistant query;
determining a set of assistant outputs based on processing the audio data stream, each assistant output in the set of assistant outputs being responsive to the assistant query included in the spoken utterance;
determining, based on the processing of the spoken utterance, whether to modify one or more of the assistant outputs included in the set of assistant outputs;
responsive to determining to modify one or more of the assistant outputs included in the set of assistant outputs:
processing the set of assistant outputs and the context of the conversation session using one or more language model (LLM) outputs generated using LLM to generate a modified set of assistant outputs, each LLM output of the one or more LLM outputs determined based on at least a portion of the context of the conversation session and one or more of the assistant outputs included in the set of assistant outputs; and
such that a given modified assistant output from the modified set of assistant outputs is provided for presentation to the user.
31. The method of claim 30, wherein determining whether to modify one or more of the assistant outputs included in the set of assistant outputs based on the processing of the spoken utterance comprises:
processing the audio data stream using an Automatic Speech Recognition (ASR) model to generate an ASR output stream;
processing the ASR output stream using a Natural Language Understanding (NLU) model to generate an NLU data stream;
identifying, based on the NLU data stream, an intent of the user to provide the spoken utterance; and
a determination is made as to whether to modify the assistant output based on the intent of the user to provide the spoken utterance.
32. The method of claim 30 or claim 31, wherein determining whether to modify one or more of the assistant outputs included in the set of assistant outputs is further based on one or more computational costs associated with modifying one or more of the assistant outputs.
33. The method of claim 32, wherein the one or more computational costs associated with modifying one or more of the assistant outputs comprise one or more of: battery consumption associated with modifying one or more of the assistant outputs, processor consumption, or latency associated with modifying one or more of the assistant outputs.
34. A method implemented by one or more processors, the method comprising:
obtaining a corresponding context for a corresponding previous dialog session for each of a plurality of assistant queries directed to an automated assistant;
for each assistant query of the plurality of assistant queries:
processing a given assistant query of the plurality of assistant queries using one or more Large Language Models (LLMs) to generate a corresponding LLM output responsive to the given assistant query; and
indexing the corresponding LLM output in a memory accessible at a client device based on the given assistant query and/or a corresponding context of a corresponding previous dialog session for the given assistant query; and
after indexing the corresponding LLM output in the memory accessible at the client device, as part of a current conversation session between a user of the client device and an automated assistant implemented by the client device:
receiving an audio data stream capturing a spoken utterance of the user, the audio data stream generated by one or more microphones of the client device;
Based on processing the audio data stream, determining that the spoken utterance includes a current assistant query corresponding to the given assistant query and/or that the spoken utterance is received in a current context of the current dialog session with a corresponding context of a corresponding previous dialog session for the given assistant query; and
such that the automated assistant utilizes the corresponding LLM output to generate an assistant output that is provided for presentation to the user in response to the spoken utterance.
35. The method of claim 34, wherein the plurality of assistant queries directed to the automated assistant are previously submitted by the user via the client device.
36. The method of claim 34, wherein the plurality of assistant queries directed to the automated assistant are previously submitted via the respective client device by a plurality of additional users in addition to the user of the client device.
37. The method of any of claims 34-36, wherein indexing the corresponding LLM output in the memory accessible at the client device is based on embedding of the given assistant query generated in processing the given assistant query.
38. The method of any of claims 34-36, wherein indexing the corresponding LLM output in the memory accessible at the client device is based on one or more terms or phrases included in the given assistant query generated in processing the given assistant query.
39. The method of any of claims 34-36, wherein indexing the corresponding LLM output in the memory accessible at the client device is based on embedding of the corresponding context of the corresponding previous dialog session for the given assistant query.
40. The method of any of claims 34-36, wherein indexing the corresponding LLM output in the memory accessible at the client device is based on one or more context signals included in the corresponding context of the corresponding previous dialog session for the given assistant query.
41. A system, comprising:
at least one processor; and
a memory storing instructions that, when executed, cause the at least one processor to perform operations corresponding to any one of claims 1 to 40.
42. A non-transitory computer-readable storage medium storing instructions that, when executed, cause at least one processor to perform operations corresponding to any one of claims 1 to 40.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US63/241,232 | 2021-09-07 | ||
US17/532,794 US20230074406A1 (en) | 2021-09-07 | 2021-11-22 | Using large language model(s) in generating automated assistant response(s |
US17/532,794 | 2021-11-22 | ||
PCT/US2021/061214 WO2023038654A1 (en) | 2021-09-07 | 2021-11-30 | Using large language model(s) in generating automated assistant response(s) |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117136405A true CN117136405A (en) | 2023-11-28 |
Family
ID=88854963
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180096665.5A Pending CN117136405A (en) | 2021-09-07 | 2021-11-30 | Automated assistant response generation using large language models |
Country Status (1)
Country | Link |
---|---|
CN (1) | CN117136405A (en) |
-
2021
- 2021-11-30 CN CN202180096665.5A patent/CN117136405A/en active Pending
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR102357218B1 (en) | Natural assistant interaction | |
US11810578B2 (en) | Device arbitration for digital assistant-based intercom systems | |
JP7064018B2 (en) | Automated assistant dealing with multiple age groups and / or vocabulary levels | |
KR102180832B1 (en) | Detecting a trigger of a digital assistant | |
JP7418526B2 (en) | Dynamic and/or context-specific hotwords to trigger automated assistants | |
US20230074406A1 (en) | Using large language model(s) in generating automated assistant response(s | |
JP7243625B2 (en) | Information processing device and information processing method | |
KR102599607B1 (en) | Dynamic and/or context-specific hot words to invoke automated assistant | |
KR20240007261A (en) | Use large-scale language models to generate automated assistant response(s) | |
CN111556999B (en) | Method, computer device and computer readable storage medium for providing natural language dialogue by providing substantive answer in real time | |
US20240055003A1 (en) | Automated assistant interaction prediction using fusion of visual and audio input | |
US20230343324A1 (en) | Dynamically adapting given assistant output based on a given persona assigned to an automated assistant | |
CN117136405A (en) | Automated assistant response generation using large language models | |
US20230215422A1 (en) | Multimodal intent understanding for automated assistant | |
US20240031339A1 (en) | Method(s) and system(s) for utilizing an independent server to facilitate secure exchange of data | |
CN117121102A (en) | Collaborative ranking of interpretations of spoken utterances | |
CN111899739A (en) | Voice notification |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |