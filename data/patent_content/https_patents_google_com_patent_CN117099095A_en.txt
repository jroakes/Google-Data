CN117099095A - Intelligent advice for image zoom regions - Google Patents
Intelligent advice for image zoom regions Download PDFInfo
- Publication number
- CN117099095A CN117099095A CN202280026294.8A CN202280026294A CN117099095A CN 117099095 A CN117099095 A CN 117099095A CN 202280026294 A CN202280026294 A CN 202280026294A CN 117099095 A CN117099095 A CN 117099095A
- Authority
- CN
- China
- Prior art keywords
- image
- zoom region
- search
- zoom
- search query
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 65
- 230000004044 response Effects 0.000 claims description 28
- 238000013507 mapping Methods 0.000 claims description 10
- 230000015654 memory Effects 0.000 claims description 7
- 238000004590 computer program Methods 0.000 claims description 2
- 230000006399 behavior Effects 0.000 description 10
- 238000013528 artificial neural network Methods 0.000 description 9
- 238000013179 statistical model Methods 0.000 description 9
- 230000000007 visual effect Effects 0.000 description 9
- 230000008569 process Effects 0.000 description 6
- 238000012545 processing Methods 0.000 description 6
- 239000000463 material Substances 0.000 description 5
- 238000010079 rubber tapping Methods 0.000 description 5
- 238000004891 communication Methods 0.000 description 3
- 238000013527 convolutional neural network Methods 0.000 description 3
- 238000010586 diagram Methods 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 238000005065 mining Methods 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 238000003491 array Methods 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000004807 localization Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000013139 quantization Methods 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 238000011160 research Methods 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/532—Query formulation, e.g. graphical querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04845—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range for image manipulation, e.g. dragging, rotation, expansion or change of colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T3/00—Geometric image transformation in the plane of the image
- G06T3/40—Scaling the whole image or part thereof
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2203/00—Indexing scheme relating to G06F3/00 - G06F3/048
- G06F2203/048—Indexing scheme relating to G06F3/048
- G06F2203/04806—Zoom, i.e. interaction techniques or interactors for controlling the zooming operation
Abstract
Techniques for providing intelligent suggestions for image scaling regions are described herein. A method comprising: receiving a search query; performing a search using the search query to identify search results comprising image search results, the image search results comprising a plurality of images responsive to the search query; for a given image of a plurality of images included in the image search result, determining at least one zoom region in the given image; and providing search results including image search results, including providing the given image and an indication of at least one zoom region in the given image.
Description
Background
Individuals (also referred to herein as "users") typically conduct online research by submitting search queries that may return search results that include images. The user may also search for a particular image by submitting an image search query. The user may be interested in particular aspects of one or more images as the user is browsing search results that include the images or images returned in response to an image search query. For example, a user may be interested in a particular object of interest included within one or more images or a particular piece of information contained within one or more images, among other visual information. The specific object of interest may be, for example, a specific part or a specific toy in a vehicle parts drawing.
The user may browse search results that include images, images returned in response to an image search query, and/or collections of images on a mobile device or other computing device, the mobile device or other computing device having limited screen realism such that the images may not be presented at their native resolution. Thus, the user may zoom in on a particular area of the image to view finer details. By zooming in on the images, the user may be able to more clearly view a particular object of interest included within the one or more images or a piece of particular information contained within the one or more images. However, image scaling can be a time consuming process that requires full focus and flexibility by the user as the user fine-tunes the zoom position and zoom level, and can therefore result in excessive use of power resources and/or other computing resources of the client device that are used when manually performing image scaling.
Disclosure of Invention
Embodiments disclosed herein relate to providing intelligent suggestions for image scaling regions. Embodiments may automatically suggest one or more image zoom regions in one or more images, such as images returned in response to a search query. Each automatically suggested image zoom region may be indicated by a selectable target, and in response to selection of the target by a user (e.g., by tapping the target or by clicking on the target), the image may be automatically enlarged on the display at a location in the image associated with the selectable target. Thus, a particular portion of the image may be enlarged so that the user may view finer details in the particular portion of the image. By zooming in on the image, the user may be able to more clearly view a particular object of interest included within the image or a piece of particular information contained within the image.
In an embodiment, the image scaling region may be: areas within the image that are relevant to the search query, the image returned as a result of the search query; areas within the image that are typically enlarged by other users or similar to areas within the image that are typically enlarged by other users; and/or regions within the image that are frequently enlarged by the user or similar to regions within the image that are frequently enlarged by the user. For example, if the user has previously zoomed into an area of the image that includes the racing car, the image zoom region may be automatically determined based on the area of the image that includes the racing car.
In an example, if the user searches for "toy X," the user may be most interested in the portion of the image that includes toy X that is included in the search results. In an embodiment, an image zoom region including toy X may be automatically suggested because these portions of the image are most responsive to the "toy X" search query. In another example, if a user searches for "toy" based on a previous indicated interest in toy X by a particular user (e.g., repeated zooming in on portions of the image that include toy X by the particular user) embodiments may automatically suggest an image zoom region that includes toy X when an image is displayed for the particular user (e.g., as a search result, or as part of a collection of images that the user is browsing). In yet another example, if a particular user searches for "toy" based on the frequency with which other users zoom in on the image area that includes toy X, then embodiments may automatically suggest an image zoom region that includes toy X.
By selecting the target associated with the image zoom region, the user may quickly zoom in on a particular area of the image of interest to the user, thereby improving user convenience, saving time, and reducing use of power resources and/or other computing resources of the client device as compared to when the user manually performs image zooming.
Embodiments include a method and associated interface for providing image scaling suggestions for images returned in response to a user query or images included in a collection browsed by a user. These image scaling suggestions may be possibly based on a scaled region (e.g., determined based on behavior of other users), may be implied by the search query, or may be based on preferences and interests learned from the user issuing the query. In an embodiment, the image scaling suggestion may be displayed as an automatically suggested image scaling region. The automatically suggested image zoom region may be displayed as a highlight, a dim light, a box, an icon, or other graphical indication that is superimposed over some portion of one or more images in the image search result set or the image set being viewed by the user. In an embodiment, tapping or clicking on the automatically suggested image zoom region may cause the zoom level of the relevant image to be automatically adjusted (e.g., to a particular zoom level associated with the automatically suggested image zoom region), and the image may be automatically centered on the relevant zoom region. In an embodiment, a user may use a zoom control provided in the user interface to further adjust the zoom level of the image and/or navigate through the image such that different portions of the image are displayed on the user interface.
In some embodiments, image scaling suggestions may be provided when the user is browsing the user's own photos (e.g., a photo library comprising photos taken on the user's cell phone). For example, the automatically suggested image zoom region may be determined based on an area within a given image that is frequently enlarged by the user or similar to areas within other images that are frequently enlarged by the user (e.g., areas that include family members of the user). The system may determine the suggested image zoom region based on the user's historical browsing behavior either server-side or locally on the user device (e.g., as part of a photo viewer application).
In some embodiments, the system may wait until the user selects an image (e.g., by tapping or clicking on the image) before determining and/or providing an indication of one or more suggested image zoom regions (e.g., as a highlight, a dim light, a box, an icon, or other graphical indication superimposed on the image).
In various embodiments, a method implemented by one or more processors may include: receiving a search query; performing a search using the search query to identify search results comprising image search results, the image search results comprising a plurality of images responsive to the search query; for a given image of a plurality of images included in the image search result, determining at least one zoom region in the given image; and providing search results including image search results including providing the given image and an indication of at least one zoom region in the given image.
In some implementations, the search query is an image search query and the search results include only image search results. In some embodiments, the method further includes determining, for another image of the plurality of images included in the image search result, that no indication of the zoom region is provided. In some embodiments, providing an indication of at least one zoom region in the given image includes providing a graphical indication of a location of the zoom region within the given image for each of the at least one zoom region in the given image.
In some embodiments, determining at least one zoom region includes determining, for each of the at least one zoom region: the location of the zoom region, and the zoom level of the zoom region or the bounding box of the zoom region, and the method further comprises receiving user interface input indicative of the selected zoom region in the given image; and in response to receiving the user interface input, adjusting the zoom level of the given image to display a view of the given image magnified with the zoom level of the selected zoom region or magnified based on the bounding box of the selected zoom region, the magnified view centered at the location of the selected zoom region within the given image. In some embodiments, determining the at least one zoom region includes, for each of the at least one zoom region, determining a location of the zoom region by retrieving metadata associated with the given image, the metadata indicating a location of the zoom region in the given image. In some implementations, the metadata includes a mapping between the search query and a region in the given image, or a mapping between a semantic representation of the search query and a region in the given image.
In some implementations, at least one zoom region in a given image includes a zoom region associated with a search query. In some implementations, at least one zoom region in a given image is associated with a search query based on historical data reflecting previous zooms on the given image for past instances of the given image returned in response to the search query or related search queries.
In some implementations, at least one zoom region in a given image is further associated with a search query based on historical data of one or more other queries having at least a threshold similarity to the search query. In some implementations, determining at least one zoom region in a given image includes determining a location of the zoom region associated with a search query, but not determining a location of another zoom region associated with another search query.
In some implementations, at least one zoom region in a given image includes a zoom region that is not relevant to the search query. In some implementations, at least one zoom region in a given image includes a zoom region associated with user preferences.
In some additional or alternative embodiments, a computer program product may include one or more computer-readable storage media having program instructions stored collectively on the one or more computer-readable storage media. The program instructions may be executable to: receiving a search query; performing a search using the search query to identify search results comprising image search results, the image search results comprising a plurality of images responsive to the search query; providing search results including image search results, the providing including responding to each of a plurality of images of a search query; receiving user interface input indicating a selected image of a plurality of images responsive to a search query; determining a location of at least one zoom region in the selected image; and providing an indication of the location of each of the at least one zoom regions in the selected image.
In some embodiments, providing an indication of the location of each of the at least one zoom region in the selected image comprises providing a graphical indication of the location of the zoom region within the selected image for each of the at least one zoom region in the selected image. In some implementations, the program instructions are further executable to: receiving user interface input indicating a selected zoom region in a selected image; and in response to receiving the user interface input, adjusting a zoom level of the selected image to display a magnified view of the selected image centered on a location of the selected zoom region within the selected image. In some embodiments, determining the location of the at least one zoom region in the selected image includes retrieving metadata associated with the selected image, the data indicating the location of the at least one zoom region in the selected image.
In some additional or alternative embodiments, the system may include a processor, a computer readable memory, one or more computer readable storage media, and program instructions collectively stored on the one or more computer readable storage media. The program instructions may be executable to: receiving a search query; performing a search using the search query to identify search results comprising image search results, the image search results comprising a plurality of images responsive to the search query; for a given image of a plurality of images included in the image search result, determining a location of a zoom region in the given image; and providing search results including image search results, the providing including providing the given image at a zoom level adjusted to display a magnified view of the given image centered on a location of the zoom region within the given image for the given image.
In some implementations, the program instructions are further executable to: providing an indication to zoom in on a given image; and providing a user interface element to adjust the zoom level of the given image. In some implementations, the program instructions are further executable to determine a confidence level associated with the zoom region; and adjusting the zoom level to display an enlarged view of the given image in response to the confidence level associated with the zoom region meeting the confidence level threshold.
The above description is provided as an overview of some embodiments of the present disclosure. Further description of these and other embodiments are described in more detail below.
Various embodiments can include a non-transitory computer-readable storage medium storing instructions executable by one or more processors (e.g., a Central Processing Unit (CPU), a Graphics Processing Unit (GPU), a Digital Signal Processor (DSP), and/or a Tensor Processing Unit (TPU)) to perform a method, such as one or more methods described herein. Other embodiments can include a client device including a processor operable to execute stored instructions to perform methods such as one or more of the methods described herein. Other embodiments can include a system of one or more servers including one or more processors operable to execute stored instructions to perform methods such as one or more of the methods described herein.
Drawings
Fig. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, according to various embodiments.
Fig. 2, 3, and 4 depict flowcharts illustrating example methods of automatically suggesting image zoom regions, according to various embodiments.
Fig. 5A and 5B depict example applications of the techniques described herein, according to various embodiments.
FIG. 6 illustrates an example architecture of a computing device.
Detailed Description
Fig. 1 schematically depicts an example environment 100 in which selected aspects of the present disclosure may be implemented, according to various embodiments. Any computing device depicted in fig. 1 or elsewhere in the diagram may include logic, such as one or more microprocessors (e.g., central processing units or "CPUs," graphics processing units or "GPUs") executing computer-readable instructions stored in memory, or other types of logic, such as application specific integrated circuits ("ASICs"), field programmable gate arrays ("FPGAs"), and the like. Some of the systems depicted in fig. 1, such as user intent prediction system 110, may be implemented using one or more server computing devices forming what is sometimes referred to as a "cloud infrastructure," although this is not required.
In an embodiment, the example environment 100 includes one or more user interface input devices 102, one or more user interface output devices 104, and a search system 112. Although the search system 112 is illustrated in fig. 1 as being separate from the user interface input device 102 and the user interface output device 104, in some embodiments, all or aspects of the search system 112 may be implemented on a computing device that also includes the user interface input device 102 and/or the user interface output device 104. For example, all or aspects of the presentation engine 132 of the search system 112 may be implemented on a computing device. In some embodiments, all or aspects of the search system 112 may be implemented on a computing device separate and apart from the computing device containing the user interface input device 102 and/or the user interface output device 104 (e.g., all or aspects may be implemented "in the cloud"). In some of these embodiments, the aspects of the search system 112 may communicate with the computing device via one or more networks, such as a Local Area Network (LAN) and/or a Wide Area Network (WAN) (e.g., the internet). In some implementations, one or more aspects of the search system 112 and the user interface input/output devices 102, 104 may be facilitated by a virtual assistant or a component of a virtual assistant.
The user interface input device 102 may include, for example, a physical keyboard, a touch screen, a visual sensor such as a digital camera, an accelerometer (e.g., for capturing gestures), a fingerprint sensor, a radar sensor (e.g., visually detectable gestures), and/or a microphone, to name a few. The user interface output device 104 may include, for example, a display screen, a haptic feedback device, and/or a speaker. The user interface input and output devices 102, 104 may be incorporated on one or more computing devices of a user. For example, a user's mobile phone may include user interface input device 102 and user interface output device 104; or a separate personal virtual assistant hardware device may include user interface input device 102 and user interface output device 104; or the first computing device may include a user interface input device 102 and the separate computing device may include a user interface output device 104, etc. A "stand-alone personal virtual assistant hardware device" may be a device designed primarily or exclusively to allow a user to interact with a virtual assistant using free-form natural language input. These may take various forms, such as a separate "smart" speaker, smart display, etc.
As illustrated in fig. 1, a user provides a search query to a search system 112 via a user interface input device 102. The search system 112 provides a response output via the user interface output device 104 for presentation to the user. Each search query is a request for information. The search query can, for example, take a textual form and/or other form, such as, for example, take an audio form and/or an image form. For example, in some embodiments, the user interface input device 102 may include a keyboard that generates text input in response to user interface input directed to the keyboard. Also, for example, in some embodiments, the user interface input device 102 may include a microphone. In some such cases, a speech recognition module local to the search system 112 or with which the search system 112 may communicate may convert audio speech input received at the microphone into text input. Text input may then be provided to the search system 112. For simplicity, input is illustrated in FIG. 1 as being provided directly to the search system 112 by the user interface input device 102, and output is illustrated as being provided directly to the user interface output device 104 by the search system 112. However, it should be noted that in various embodiments, one or more middleware and/or hardware components may be functionally interposed between the search system 112 and the user decoding input device 102 and/or the user interface output device 104, and may optionally process inputs and/or outputs. Thus, in some embodiments, a plurality of client computing devices are used to communicate with all or aspects of the search system 112, which collectively form a coordinated "ecosystem" of computing devices.
The search system 112 may additionally include a search engine 142, a ranking engine 152, and a presentation engine 132. In some embodiments, one or more of the engines 142, 152, and/or 132 may be omitted, combined, and/or implemented in a component separate from the search system 112. For example, one or more of the engines 142, 152, and/or 132, or any operational portions thereof, may be implemented in components executed by a client computing device that includes the user interface input device 102 and/or the user interface output device 104 and is separate from the search system 112. Also, for example, the search engine 142 and/or the ranking engine 152 may be implemented in whole or in part by a system separate from the search system 112 (e.g., a separate search system in communication with the search system 112).
In some implementations, the search queries provided to the search system 112 via the user interface input device 102 are used by the search engine 142 during performance of searches of one or more search databases 175. In some implementations, once the search engine 142 generates search results for the search database 175 that are executed based on the search query, the results are provided to the ranking engine 152. Ranking engine 152 may rank the results according to the relevance of the resources associated with the search results to the search parameters of the search query.
In some embodiments, the ranked search results are then provided to presentation engine 132. The presentation engine 132 provides the ranked search results responsive to the search query to one or more user interface output devices 104 for presentation to the user in various output formats. In some embodiments, the ranked search results may include one or more images. In some implementations, the search query provided to the search system 112 via the user interface input device 102 can be an image search query, and the ranked search results can be a plurality of images returned in response to the image search query.
In some embodiments, the presentation engine 132 will also utilize the images provided to the one or more user interface output devices 104 to provide an indication of one or more automatically suggested image zoom regions included in the one or more images in the ranked search results. In some embodiments, the indication may be a visual indication, such as a highlight, a dim light, a box, an icon, or other graphical indication, that is superimposed over the images included in the ranked search results when those images are displayed on one or more user interface output devices 104. The visual indication may identify selectable targets in one or more images. In response to a user's selection of a target using the user interface input device 102 (e.g., by tapping the target or by clicking on the target), the image may be automatically magnified on one of the user interface output devices 104 (e.g., a display) at a location in the image associated with the selectable target. In some embodiments, the user may also manually manipulate the zoom level and/or scroll position within the image after selecting the target or without selecting the target by accessing controls in the user interface using the user interface input device 102, providing touch input, and the like. In some embodiments, the search system 112 may be provided with information regarding user selection of one or more automatically suggested image zoom regions and/or information regarding manually selected zoom levels and/or scroll positions within one or more images.
In some embodiments, the image scaling region may be an area within images determined by the search engine 142 to be relevant to the search query that are returned as a result of the search query.
In other embodiments, the image zoom region may be an area within the image that is typically enlarged by other users, as identified by the search engine 142 using historical data in the search database 175. In response to receiving information from a plurality of users regarding user selections of one or more automatically suggested image zoom regions and/or information regarding manually selected zoom levels and/or scroll positions within one or more images, the information stored in search database 175 regarding areas within the generally enlarged images may be updated. The search system 112 may anonymize and/or aggregate information regarding user selection of one or more automatically suggested image zoom regions and/or information regarding manually selected zoom levels and/or scroll positions within one or more images in order to preserve user privacy.
In other embodiments, the image scaling region may be a region within the image that is frequently enlarged by the user, or similar to a region within the image that is frequently enlarged by the user, as identified by the search engine 142 using historical data in the user preference database 185. In response to receiving information regarding user selection of one or more automatically suggested image zoom regions and/or information regarding manually selected zoom levels and/or scroll positions within one or more images, the information stored in the user preference database 185 regarding areas within the images that are frequently enlarged by the user may be updated.
In some embodiments, when the search system 112 performs an image search, the search system 112 documents (records) when/how the user zooms in on the image results, and trains and refines an offline model for automatically suggesting image zoom regions for which users query and use this information. In particular, for a given image, the search system 112 mines information about the zoom behavior from logs generated from past searches of the same or similar queries to identify common zoom regions that are query-specific and query-independent. Each exact zoom position and/or level may be slightly different, so the search system 112 may perform clustering and/or quantization to collapse a set of similar zooms from different users/sessions into one. Thus, the search system 112 may determine a common zoom region for a general image (across different queries) and a common zoom region given a particular query (or cluster of queries). For example, any search that shows a particular image as a result will indicate a query-independent zoom, where the user has similar zoom behavior.
In some embodiments, the mining process described above is used to build a statistical model that enables the search system 112 to identify common user behavior regarding scaling based on log data given a particular image and/or query. Alternatively or additionally, in some embodiments, the mining process described above is used to train a machine learning model (e.g., a deep neural network) that is capable of generalizing common scaling behaviors more broadly across different images. The model may be a variant of a convolutional neural network. In particular, the model may be an object localization model, such as a single-shot detector (SSD), a Recurrent Convolutional Neural Network (RCNN), or the like. The input of the model may be an image and the output may be a set of potential bounding boxes. In some embodiments, the machine learning model may be multimodal and may employ side input of the query for biasing the results toward what the user explicitly mentions as part of his text request. The query parameters of the model may be optional. If no query parameters are specified, the model may fall back to suggesting general regions of interest that are not query-specific.
In some embodiments, a combination of the statistical model and the neural network may be used to determine the suggested scaling region. In some embodiments, the system uses previously observed behavior for a particular query while generalizing more broadly on images.
In some embodiments, for each set of search results that includes an image, a zoom region may be identified and displayed to a user. The search system 112 may identify the query-independent zoom region offline as a static process as associated metadata for each image. Once the system has a query for each candidate image, the search system 112 may search for and/or calculate query-specific zoom regions online at query time.
In some embodiments, the search system 112 may use image search results having smaller sizes or that are cropped versions of larger images as a learning source. For example, if a user typically taps or clicks on a smaller size image next to a larger image, the indication is based on the query and the user's preferences.
In some embodiments, the image zoom region may also be personalized based on the interests of the user. For example, the search system 112 may determine that the user is interested in racing based on a user profile stored in the user preference database 185. The search system 112 may also infer the user's interests based on the user's previous image scaling history and/or other signals such as web searches, video views, purchase history, travel history, and the like. In this arrangement, the search system 112 may learn a separate or expanded model that allows the search system 112 to adjust the convolutional neural network used to determine the suggested zoom region according to such preferences provided as another side input. In other embodiments, the search system 112 may use a generic model that issues bounding boxes and related classes. The search system 112 may intersect the output of the generic model with the interests of the user. For example, if one of the bounding boxes issued by the generic model is a bounding box of a racing car, the search system 112 may select that bounding box of the racing car as a suggested image zoom area provided to the user based on a user profile stored in a user preference database 185 that identifies the user's interest in the racing car.
FIG. 2 depicts a flowchart illustrating an example method 200 of automatically suggesting an image zoom region. For convenience, the operations of method 200 are described with reference to a system performing the operations. The system of method 200 includes one or more processors and/or other components of various computer systems. Furthermore, although the operations of method 200 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 210, the system receives a search query. In some embodiments, a search application or search web page presented by a web browser may be displayed on one or more of the upper user interface output devices 104. A user may input a search query using one or more user interface input devices 102, which may be transmitted to the search system 112. The search query may be a text query, an image query, or any other type of search query.
At block 220, the system performs a search using the search query to identify search results that include image search results that include a plurality of images responsive to the search query. In some embodiments, the search engine 142 of the search system 112 performs a search of one or more search databases 175 using the search query received by the search system 112 at block 210. As a result of the search, the search engine 142 identifies search results that include image search results that include a plurality of images that are responsive to the search query. In some implementations, the search query received at block 210 is an image search query and the search results include only image search results. In other embodiments, the search results include images and other types of search results (e.g., web pages, text, etc.).
At block 230, for a given image of a plurality of images included in the image search results, the system determines at least one zoom region in the given image. In some embodiments, search engine 142 provides the search results identified at block 220 to ranking engine 152, which may rank the search results according to the relevance of the resources associated with the results to the search parameters of the search query. In some embodiments, ranking engine 152 then provides the ranked search results to presentation engine 132. In some embodiments, the search engine 142, ranking engine 152, and/or presentation engine 132 determine at least one zoom region in a given image.
Still referring to block 230, in some embodiments, determining at least one zoom region includes, for each of the at least one zoom region, determining a location of the zoom region and determining a zoom level of the zoom region or a bounding box of the zoom region.
Still referring to block 230, in some embodiments, determining the at least one zoom region includes, for each of the at least one zoom region, determining a location of the zoom region by retrieving metadata associated with the given image, the metadata indicating a location of the zoom region in the given image. In some implementations, the metadata includes a mapping between the search query and a region in the given image, or a mapping between a semantic representation of the search query and a region in the given image.
Still referring to block 230, in some implementations, at least one zoom region in a given image includes a zoom region associated with a search query. In some implementations, at least one zoom region in a given image is associated with a search query based on historical data reflecting previous zooms on the given image for past instances of the given image returned in response to the search query or related search queries. For example, the related search query may be a search query that is similar to or the same as the search query received at block 210. In other implementations, at least one zoom region in a given image is further associated with a search query based on historical data of one or more other queries having at least a threshold similarity to the search query. The historical data may be stored in search database 175 or any other location. In some implementations, determining at least one zoom region in a given image includes determining a location of the zoom region associated with a search query, but not determining a location of another zoom region associated with another search query.
Still referring to block 230, in some implementations, at least one zoom region in a given image includes a zoom region that is not related to a search query. In some implementations, at least one zoom region in a given image includes a zoom region associated with user preferences. For example, the image zoom region may be an area within a given image that is frequently enlarged by the user or similar to areas within other images that are frequently enlarged by the user (e.g., areas that include family members of the user), as identified by the search engine 142 using historical data in the user preference database 185.
Still referring to block 230, in some embodiments, the search system 112 uses a statistical model, a neural network, or a combination of a statistical model and a neural network to determine the suggested image scaling region. In some embodiments, the system uses previously observed behavior for a particular query while generalizing more broadly on images.
Still referring to block 230, in some embodiments, for another image of the plurality of images included in the image search results, the system determines that an indication of a zoom region is not provided.
At block 240, the system provides search results including image search results, including providing an indication of the given image and at least one zoom region in the given image. In some embodiments, the presentation engine 132 provides search results to the one or more user interface output devices 104 for presentation to the user in various output formats, the search results including the image search results identified at block 220 and an indication of at least one zoom region in the given image determined at block 230. For example, the search results may be displayed in a search application or search web page presented by a web browser.
Still referring to block 240, in some embodiments, providing an indication of at least one zoom region in the given image includes providing a graphical indication of a location of the zoom region within the given image for each of the at least one zoom region in the given image. In some embodiments, the graphical indication may be a visual indication, such as a highlight, a dim light, a box, an icon, etc., that is superimposed over a given image when the given image is displayed on one or more user interface output devices 104.
At block 250, the system receives user interface input indicating a selected zoom region in a given image. In some implementations, the user interface input is provided via the user interface input device 102. For example, the user may tap or click on an indication (e.g., highlight, dim light, box, icon, etc.) of the location of the zoom region in the given image provided at block 240.
In response to receiving the user interface input at block 250, the system adjusts the zoom level of the given image to display a view of the given image of the selected zoom region that is zoomed in on the zoom level or zoomed in on a bounding box of the selected zoom region, the zoomed-in view centered on the location of the selected zoom region within the given image. In some embodiments, the zoom level and position of a given image displayed on one or more user interface output devices 104 is automatically adjusted based on the zoom level and position associated with the zoom region selected at block 250.
Still referring to block 260, in some embodiments, after adjusting the zoom level and position (e.g., by zooming in on the bounding box), the system may emphasize a particular object in the bounding box and/or crop a particular object in the bounding box from a larger image. In some embodiments, this is done by highlighting a particular object and/or by retaining a particular object but applying a level of transparency to surrounding areas in the image. For example, if the bounding box surrounds a bird appearing in the image depicting the landscape, the system may emphasize a heat map corresponding to the bird (e.g., highlighting bird objects against the background), or may perform a "soft" cropping of the bird from the surrounding image by applying transparency to the area around the bird.
In other embodiments, the operations of method 200 may be performed with respect to a video, which the system may consider as a collection of images. For example, the system may pre-process the video to identify and/or extract 10 key frames, which may be indexed and processed according to the operations of method 200. These key frames may be included in the search results at block 220, and the system may determine a zoom region in the key frames at block 230. The video and/or key frames from the video may be included in the image search results at block 240 and may be selected at block 250 and the zoom level of the key frame image or the entire video may be adjusted at block 260.
FIG. 3 depicts a flowchart illustrating an example method 300 of automatically suggesting an image zoom region. For convenience, the operations of method 300 are described with reference to a system performing the operations. The system of method 300 includes one or more processors and/or other components of various computer systems. Furthermore, although the operations of method 300 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 310, the system receives a search query. In some embodiments, a search application or search web page presented by a web browser may be displayed on one or more user interface output devices 104. A user may input a search query using one or more user interface input devices 102, which may be transmitted to the search system 112. The search query may be a text query, an image query, or any other type of search query.
At block 320, the system performs a search using the search query to identify search results that include image search results that include a plurality of images responsive to the search query. In some embodiments, search engine 142 of search system 112 performs a search of one or more search databases 175 using the search query received by search system 112 at block 310. As a result of the search, the search engine 142 identifies search results that include image search results that include a plurality of images that are responsive to the search query. In some implementations, the search query received at block 310 is an image search query and the search results include only image search results. In other embodiments, the search results include images and other types of search results (e.g., web pages, text, etc.).
At block 330, the system provides search results including image search results that include each of the plurality of images responsive to the search query. In some embodiments, presentation engine 132 provides search results, including the image search results identified at block 320, to one or more user interface output devices 104 for presentation to a user in various output formats. For example, the search results may be displayed in a search application or search web page presented by a web browser.
At block 340, the system receives a user interface input indicating a selected image of a plurality of images responsive to a search query. In some implementations, the user interface input is provided to the search system 112 via the user interface input device 102. For example, the user may scroll through the plurality of images provided at block 330 and then tap or click on the selected image.
At block 350, the system determines the location of at least one zoom region in the selected image. In some implementations, in response to receiving user input at block 340, the search engine 142, ranking engine 152, and/or presentation engine 132 of the search system 112 determines the location of at least one zoom region in the image selected at block 340. In some embodiments, determining the location of the at least one zoom region in the selected image includes retrieving metadata associated with the selected image, the metadata indicating the location of the at least one zoom region in the selected image. In some implementations, the metadata includes a mapping between the search query and a region in the selected image, or a mapping between a semantic representation of the search query and a region in the selected image.
Still referring to block 350, in some implementations, at least one zoom region in the selected image includes a zoom region associated with the search query. In some implementations, at least one zoom region in the selected image is associated with the search query based on historical data reflecting previous zooms on the selected image for past instances of the given image returned in response to the search query. In other implementations, at least one zoom region in the selected image is further associated with the search query based on historical data of one or more other queries having at least a threshold similarity to the search query. The historical data may be stored in search database 175 or any other location. In some implementations, determining the location of at least one zoom region in the selected image includes determining the location of the zoom region associated with the search query, but not determining the location of another zoom region associated with another search query.
Still referring to block 350, in some implementations, at least one zoom region in the selected image includes a zoom region that is not related to the search query. In some implementations, the at least one zoom region in the selected image includes a zoom region associated with a user preference. For example, the image zoom region may be an area within the selected image that is frequently enlarged by the user or similar to an area within other images that are frequently enlarged by the user (e.g., an area that includes family members of the user), as identified by the search engine 142 using historical data in the user preference database 185.
Still referring to block 350, in some embodiments, the search system 112 uses a statistical model, a neural network, or a combination of a statistical model and a neural network to determine the suggested image scaling region. In some embodiments, the system uses previously observed behavior for a particular query while generalizing more broadly on images.
At block 360, the system provides an indication of the location of each of the at least one zoom regions in the selected image. In some implementations, the presentation engine 132 provides an indication of the location of the at least one zoom region in the selected image determined at block 350 to the one or more user interface output devices 104 for presentation to the user, for example, within a search application or search web page presented by a web browser.
Still referring to block 360, in some implementations, providing an indication of the location of the at least one zoom region in the selected image includes providing a graphical indication of the location of the zoom region within the selected image for each of the at least one zoom region in the selected image. In some embodiments, the graphical indication may be a visual indication, such as a highlight, a dim light, a box, an icon, etc., that is superimposed over the selected image when the given image is displayed on the one or more user interface output devices 104.
At block 370, the system receives a user interface input indicating a selected zoom region in the selected image. In some implementations, the user interface input is provided via the user interface input device 102. For example, the user may tap or click on an indication (e.g., highlight, dim light, box, icon, etc.) of the location of the zoom region in the selected image provided at block 360.
In response to receiving the user interface input at block 370, the system adjusts the zoom level of the selected image to display a magnified view of the selected image centered at the location of the selected zoom region within the selected image at block 380. In some implementations, the zoom level and position of the selected image displayed on the one or more user interface output devices 104 is automatically adjusted based on the zoom level and zoom position associated with the zoom region selected at block 370.
FIG. 4 depicts a flowchart illustrating an example method 400 of automatically suggesting an image zoom region. For convenience, the operations of method 400 are described with reference to a system performing the operations. The system of method 400 includes one or more processors and/or other components of various computer systems. Furthermore, although the operations of method 400 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 410, the system receives a search query. In some embodiments, a search application or search web page presented by a web browser may be displayed on one or more user interface output devices 104. A user may input a search query using one or more user interface input devices 102, which may be transmitted to the search system 112. The search query may be a text query, an image query, or any other type of search query.
At block 420, the system performs a search using the search query to identify search results that include image search results that include a plurality of images responsive to the search query. In some embodiments, the search engine 142 of the search system 112 performs searches of one or more search databases 175 using the search query received by the search system 112 at block 410. As a result of the search, the search engine 142 identifies search results that include image search results that include a plurality of images that are responsive to the search query. In some implementations, the search query received at block 410 is an image search query and the search results include only image search results. In other embodiments, the search results include images and other types of search results (e.g., web pages, text, etc.).
At block 430, for a given image of the plurality of images included in the image search results, the system determines a location of the zoom region in the given image. In some embodiments, search engine 142 provides the search results identified at block 420 to ranking engine 152, which may rank the search results according to the relevance of the resources associated with the results to the search parameters of the search query. In some embodiments, ranking engine 152 then provides the ranked search results to presentation engine 132. In some embodiments, the search engine 142, ranking engine 152, and/or presentation engine 132 determine the location of the zoom region in a given image.
Still referring to block 430, in some embodiments, determining the location of the zoom region in the given image includes retrieving metadata associated with the given image, the metadata indicating the location of the zoom region in the given image. In some implementations, the metadata includes a mapping between the search query and a region in the given image, or a mapping between a semantic representation of the search query and a region in the given image.
Still referring to block 430, in some implementations, the zoom region in the given image includes a zoom region associated with the search query. In some implementations, the zoom region in a given image is associated with a search query based on historical data reflecting previous zooms on the given image for past instances of the given image returned in response to the search query or related search queries. For example, the related search query may be a search query that is similar to or the same as the search query received at block 410. In other implementations, the zoom region in a given image is further associated with the search query based on historical data of one or more other queries having at least a threshold similarity to the search query. The historical data may be stored in search database 175 or any other location. In some implementations, determining the location of the zoom region in the given image includes determining the location of the zoom region associated with the search query, but not determining the location of another zoom region associated with another search query.
Still referring to block 430, in some implementations, the zoom region in the given image includes a zoom region that is not related to the search query. In some implementations, the zoom region in a given image includes a zoom region associated with user preferences. For example, the zoom region may be an area within a given image that is frequently enlarged by the user or similar to areas within other images that are frequently enlarged by the user (e.g., areas that include family members of the user), as identified by the search engine 142 using historical data in the user preference database 185.
Still referring to block 430, in some embodiments, the search system 112 uses a statistical model, a neural network, or a combination of a statistical model and a neural network to determine the zoom region. In some embodiments, the system uses previously observed behavior for a particular query while generalizing more broadly on images.
At block 440, the system provides search results including image search results, including for the given image, providing the given image at a zoom level adjusted to display a magnified view of the given image centered on the location of the zoom region within the given image determined at block 430. In some embodiments, the presentation engine 132 provides search results to the one or more user interface output devices 104 for presentation to the user in various output formats, the search results including the image search results identified at block 420 and the given image with a zoom level automatically adjusted to display a magnified view of the given image. For example, the search results may be displayed in a search application or in a search web page presented by a web browser. In some implementations, the zoom level of a given image is automatically adjusted without requiring the user to first select a zoom region in the given image.
Still referring to block 440, in some embodiments, the system provides an indication to zoom in on a given image and provides a user interface element to adjust the zoom level of the given image. The indication may be a graphical element (e.g., an icon) or any other visual indication displayed in the user interface.
Still referring to block 440, in some embodiments, the system determines a confidence level associated with the zoom region. The confidence level may be based, for example, on historical data reflecting previous scaling on a given image in a scaling region associated with the search query received at block 410 and/or similar search queries. In some embodiments, the confidence level may be determined based on the output of the statistical model and/or the neural network. In some embodiments, in response to the confidence level associated within the zoom region meeting a confidence level threshold, the system adjusts the zoom level to display an enlarged view of the given image.
Fig. 5A and 5B depict examples of how an image zoom region and a magnified image may be displayed on user interface output device 104 (e.g., by a search application or by a web browser). The scenario of fig. 5A and 5B is for illustration purposes only. In fig. 5A and 5B, the user interface output device 104 displays a web page 500. As illustrated in fig. 5A, web page 500 includes a search box 510 in which a user provides a search query "bird"; and search result images 520-1, 520-2, which are provided by search system 112 in response to a search query. Search result image 520-1 includes an indication 530 of the location of image scaling area 540. In response to receiving user interface input (e.g., a user tapping or electrically clicking on the image zoom region 540) via the user interface input device 102, the zoom level of the search result image 520-1 is adjusted to display an enlarged view of the portion of the search result image 520-1 that is the zoom region 540, as illustrated in fig. 5B. The user may further adjust the zoom level using zoom control 550.
FIG. 6 is a block diagram of an example computing device 610 that may optionally be used to perform one or more aspects of the techniques described herein. The computing device 610 typically includes at least one processor 614 in communication with a plurality of peripheral devices via a bus subsystem 612. These peripheral devices may include: storage subsystem 624, including, for example, memory subsystem 625 and file storage subsystem 626; a user interface output 620; a user interface input device 622; and a network interface subsystem 616. The input and output devices allow user interaction with the computing device 610. The network interface subsystem 616 provides an interface to external networks and couples to corresponding interface devices among other computing devices.
User interface input devices 622 may include a keyboard, a pointing device such as a mouse, trackball, touch pad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 610 or onto a communication network.
The user interface output device 620 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for producing a viewable image. The display subsystem may also provide for a non-visual display, for example, via an audio output device. In general, use of the term "output device" is intended to include the various possible types of devices and ways to output information from computing device 610 to a user or another machine or computing device.
Storage subsystem 624 stores programming and data structures that provide the functionality of some or all of the modules described herein. For example, storage subsystem 624 may include logic for performing selected aspects of the methods of fig. 2, 3, and 4, as well as implementing the various components depicted in fig. 1.
These software modules are typically executed by processor 614 alone or in combination with other processors. The memory subsystem 625 included in the storage subsystem 624 can include a number of memories, including a main Random Access Memory (RAM) 630 for storing instructions and data during program execution and a Read Only Memory (ROM) 632 for storing fixed instructions. File storage subsystem 626 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive, and associated removable media, CD-ROM drive, optical disk drive, or removable media cartridge. Modules implementing the functionality of certain embodiments may be stored in storage subsystem 624 by file storage subsystem 626, or in other machines accessible by processor 614.
Bus subsystem 612 provides a mechanism for allowing the various components and subsystems of computing device 610 to communicate with one another as intended. Although bus subsystem 612 is shown schematically as a single bus, alternative embodiments of the bus subsystem may use multiple buses.
Computing device 610 may be of various types including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Because of the ever-changing nature of computers and networks, the description of computing device 610 depicted in FIG. 6 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computing device 610 are possible with more or fewer components than the computing device depicted in FIG. 6.
Although several embodiments have been described and illustrated herein, various other ways and/or structures for performing functions and/or obtaining results and/or one or more advantages described herein may be used and each of these variations and/or modifications is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure relate to each individual feature, system, article, material, kit, and/or method described herein. In addition, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, any combination of two or more such features, systems, articles, materials, kits, and/or methods is included within the scope of the present disclosure.
Claims (23)
1. A method implemented by one or more processors, the method comprising:
receiving a search query;
performing a search using the search query to identify search results comprising image search results, the image search results comprising a plurality of images responsive to the search query;
for a given image of the plurality of images included in the image search results, determining at least one zoom region in the given image; and
providing the search results including the image search results includes providing the given image and an indication of the at least one zoom region in the given image.
2. The method of claim 1, wherein the search query is an image search query and the search results include only the image search results.
3. The method of claim 1 or 2, further comprising determining, for another image of the plurality of images included in the image search result, that no indication of a zoom region is provided.
4. The method of any of the preceding claims, wherein providing the indication of the at least one zoom region in the given image comprises, for each of the at least one zoom region in the given image, providing a graphical indication of a position of the zoom region within the given image.
5. The method of any of the preceding claims, wherein determining the at least one zoom region comprises, for each of the at least one zoom region, determining:
the location of the zoom region, and
a zoom level of the zoom region or a bounding box of the zoom region, and
further comprises:
receiving user interface input indicating a selected zoom region in the given image, an
In response to receiving the user interface input, a zoom level of the given image is adjusted to display a view of the given image zoomed in on the zoom level of the selected zoom region or based on the bounding box of the selected zoom region, the zoomed-in view centered on a location of the selected zoom region within the given image.
6. The method of any of claims 1-4, wherein determining the at least one zoom region comprises, for each of the at least one zoom region, determining a location of the zoom region by retrieving metadata associated with the given image, the metadata indicating a location of the zoom region in the given image.
7. The method of claim 6, wherein the metadata comprises a mapping between a search query and a region in the given image or a mapping between a semantic representation of a search query and a region in the given image.
8. The method of any of the preceding claims, wherein the at least one zoom region in the given image comprises a zoom region associated with the search query.
9. The method of claim 8, wherein the at least one zoom region in the given image is associated with the search query based on historical data reflecting previous zooms on the given image for past instances of the given image returned in response to the search query or a related search query.
10. The method of claim 9, wherein the at least one zoom region in the given image is further associated with the search query based on historical data of one or more other queries having at least a threshold similarity to the search query.
11. The method of any of the preceding claims, wherein determining the at least one zoom region in the given image comprises determining a location of a zoom region associated with the search query, but not another zoom region associated with another search query.
12. The method of any of the preceding claims, wherein the at least one zoom region in the given image comprises a zoom region that is independent of the search query.
13. The method of any of the preceding claims, wherein the at least one zoom region in the given image comprises a zoom region associated with user preferences.
14. A method implemented by one or more processors, the method comprising:
receiving a search query;
performing a search using the search query to identify search results comprising image search results, the image search results comprising a plurality of images responsive to the search query;
providing the search results including the image search results, including each of the plurality of images responsive to the search query;
receiving user interface input indicative of a selected image of the plurality of images responsive to the search query;
determining a location of at least one zoom region in the selected image; and
an indication of the location of each of the at least one zoom region in the selected image is provided.
15. The method of claim 14, wherein providing the indication of the location of each of at least one zoom region in the selected image comprises, for each of the at least one zoom region in the selected image, providing a graphical indication of the location of the zoom region within the selected image.
16. The method of claim 14 or 15, further comprising:
receiving user interface input indicating a selected zoom region in the selected image; and
in response to receiving the user interface input, a zoom level of the selected image is adjusted to display a magnified view of the selected image centered at the location of the selected zoom region within the selected image.
17. The method of any of claims 14-16, wherein determining the location of the at least one zoom region in the selected image comprises retrieving metadata associated with the selected image, the metadata indicating the location of the at least one zoom region in the selected image.
18. A method implemented by one or more processors, the method comprising:
receiving a search query;
performing a search using the search query to identify search results comprising image search results, the image search results comprising a plurality of images responsive to the search query;
determining, for a given image of the plurality of images included in the image search result, a location of a zoom region in the given image; and
Providing the search results including the image search results includes providing the given image at a zoom level for the given image that adjusts to display a magnified view of the given image centered at the location of the zoom region within the given image.
19. The method of claim 18, further comprising:
providing an indication that the given image is enlarged; and
a user interface element is provided to adjust the zoom level of the given image.
20. The method of claim 18 or 19, further comprising determining a confidence level associated with the zoom region,
wherein the zoom level is adjusted to display the magnified view of the given image in response to the confidence level associated within the zoom region meeting a confidence level threshold.
21. A computer program product comprising instructions which, when executed by one or more processors, cause the one or more processors to carry out the method according to any one of claims 1 to 20.
22. A computer-readable storage medium comprising instructions that, when executed by one or more processors, cause the one or more processors to carry out the method of any one of claims 1 to 20.
23. A system comprising a processor, a computer readable memory, one or more computer readable storage media, and program instructions collectively stored on the one or more computer readable storage media, the program instructions being executable to carry out the method of any one of claims 1 to 20.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/336,000 US20220382802A1 (en) | 2021-06-01 | 2021-06-01 | Smart suggestions for image zoom regions |
US17/336,000 | 2021-06-01 | ||
PCT/US2022/031596 WO2022256321A1 (en) | 2021-06-01 | 2022-05-31 | Smart suggestions for image zoom regions |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117099095A true CN117099095A (en) | 2023-11-21 |
Family
ID=82156788
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280026294.8A Pending CN117099095A (en) | 2021-06-01 | 2022-05-31 | Intelligent advice for image zoom regions |
Country Status (4)
Country | Link |
---|---|
US (1) | US20220382802A1 (en) |
EP (1) | EP4291995A1 (en) |
CN (1) | CN117099095A (en) |
WO (1) | WO2022256321A1 (en) |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8423902B2 (en) * | 2010-04-21 | 2013-04-16 | Microsoft Corporation | Representation of overlapping visual entities |
US20150170333A1 (en) * | 2011-08-31 | 2015-06-18 | Google Inc. | Grouping And Presenting Images |
US20130132867A1 (en) * | 2011-11-21 | 2013-05-23 | Bradley Edward Morris | Systems and Methods for Image Navigation Using Zoom Operations |
US9916328B1 (en) * | 2014-07-11 | 2018-03-13 | Google Llc | Providing user assistance from interaction understanding |
CN104765809A (en) * | 2015-04-02 | 2015-07-08 | 北京奇虎科技有限公司 | Preview method and device of search pictures of mobile terminal |
US9870623B2 (en) * | 2016-05-14 | 2018-01-16 | Google Llc | Segmenting content displayed on a computing device into regions based on pixels of a screenshot image that captures the content |
US20180173692A1 (en) * | 2016-12-19 | 2018-06-21 | Google Inc. | Iconographic symbol predictions for a conversation |
US11915115B2 (en) * | 2019-12-31 | 2024-02-27 | Google Llc | Lane selection using machine learning |
-
2021
- 2021-06-01 US US17/336,000 patent/US20220382802A1/en active Pending
-
2022
- 2022-05-31 CN CN202280026294.8A patent/CN117099095A/en active Pending
- 2022-05-31 EP EP22732841.6A patent/EP4291995A1/en active Pending
- 2022-05-31 WO PCT/US2022/031596 patent/WO2022256321A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
EP4291995A1 (en) | 2023-12-20 |
US20220382802A1 (en) | 2022-12-01 |
WO2022256321A1 (en) | 2022-12-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9361318B2 (en) | Adjacent search results exploration | |
US11417092B2 (en) | Systems, methods, and apparatus for image-responsive automated assistants | |
US9569547B2 (en) | Generating a news timeline | |
US10540055B2 (en) | Generating interactive content items based on content displayed on a computing device | |
US20210382934A1 (en) | Dynamic search control invocation and visual search | |
US20230229695A1 (en) | Dynamic search input selection | |
KR20160138440A (en) | Adjusting serp presentation based on query intent | |
EP3513328A1 (en) | Method and apparatus for ranking electronic information by similarity association | |
US9619519B1 (en) | Determining user interest from non-explicit cues | |
CN107003829B (en) | Request-related result regions within and outside of view for each result category | |
JP2023162232A (en) | Intelligent systems and methods for visual search queries | |
WO2015101945A1 (en) | Generating a news timeline and recommended news editions | |
US20240086490A1 (en) | Systems and methods for pre-loading object models | |
US20220382802A1 (en) | Smart suggestions for image zoom regions | |
JP2012048474A (en) | Information processor, information processing method and program | |
US20210382905A1 (en) | Dynamic injection of related content in search results | |
US11687581B2 (en) | Information extraction, enrichment, and caching framework for augmented reality applications |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |