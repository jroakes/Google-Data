CN116783589A - Generating augmented reality pre-rendering using template images - Google Patents
Generating augmented reality pre-rendering using template images Download PDFInfo
- Publication number
- CN116783589A CN116783589A CN202280010102.4A CN202280010102A CN116783589A CN 116783589 A CN116783589 A CN 116783589A CN 202280010102 A CN202280010102 A CN 202280010102A CN 116783589 A CN116783589 A CN 116783589A
- Authority
- CN
- China
- Prior art keywords
- images
- rendered
- template
- augmented reality
- rendering
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000003190 augmentative effect Effects 0.000 title claims abstract description 136
- 238000009877 rendering Methods 0.000 title claims abstract description 115
- 238000000034 method Methods 0.000 claims abstract description 108
- 230000008569 process Effects 0.000 claims description 49
- 238000012545 processing Methods 0.000 claims description 31
- 239000002537 cosmetic Substances 0.000 claims description 12
- 230000008447 perception Effects 0.000 claims description 5
- 230000008901 benefit Effects 0.000 abstract description 6
- 238000010801 machine learning Methods 0.000 description 49
- 230000000694 effects Effects 0.000 description 19
- 238000010586 diagram Methods 0.000 description 16
- 238000012549 training Methods 0.000 description 16
- 238000013528 artificial neural network Methods 0.000 description 12
- 230000000007 visual effect Effects 0.000 description 10
- 239000008186 active pharmaceutical agent Substances 0.000 description 7
- 239000003086 colorant Substances 0.000 description 6
- 230000018109 developmental process Effects 0.000 description 6
- 230000006870 function Effects 0.000 description 5
- 230000011218 segmentation Effects 0.000 description 5
- 238000004891 communication Methods 0.000 description 4
- 238000005034 decoration Methods 0.000 description 4
- 238000012986 modification Methods 0.000 description 4
- 230000004048 modification Effects 0.000 description 4
- 238000005259 measurement Methods 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 238000003709 image segmentation Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 239000003973 paint Substances 0.000 description 2
- 238000012360 testing method Methods 0.000 description 2
- 238000007792 addition Methods 0.000 description 1
- -1 automotive (3D) Substances 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000013144 data compression Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000005286 illumination Methods 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000004806 packaging method and process Methods 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/44—Browsing; Visualisation therefor
- G06F16/444—Spatial browsing, e.g. 2D maps, 3D or virtual spaces
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
- G06T11/60—Editing figures and text; Combining figures or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T17/00—Three dimensional [3D] modelling, e.g. data description of 3D objects
Abstract
Systems and methods for generating augmented reality pre-rendering may provide the benefits of augmented reality rendering without the need to use user data. Template images may be used in place of user data to protect the privacy of the user while enabling the user to view objects or products rendered onto a preferred template image or onto various template images.
Description
RELATED APPLICATIONS
The present application claims priority and benefit from U.S. non-provisional patent application Ser. No. 17/153,263 filed on day 1 and 20 of 2021. U.S. non-provisional patent application No. 17/153,263 is incorporated by reference herein in its entirety.
Technical Field
The present disclosure relates generally to prerendering images. More particularly, the present disclosure relates to pre-rendering augmented reality images from a set of template images, for example, to enable a limited augmented reality experience from the template images as an alternative to real-time personalized augmented reality experiences.
Background
Augmented Reality (AR) may refer to the creation and execution of an interactive experience of a real-world environment, where objects residing in the real world are augmented by computer-generated sensory information. As one example, an AR experience may include enhancing a scene captured by a camera by inserting virtual objects into the scene and/or modifying the appearance of real world objects included in the scene.
When searching for products with strong aesthetic aspects, such as cosmetics, it is often not sufficient to see the packaging of the product or even the product itself. To address this problem, efforts have been made to digitize cosmetics and other products in Augmented Reality (AR) to allow consumers to visualize the products on their own or in their personal environment. However, many users may not wish to try-on using AR due to natural conflicts. For example, the user may not be in a location where it is practical to use the camera, the user may not feel that they are looking at the best and are reluctant to turn on the camera, and/or they may simply not want to grant the camera rights.
Furthermore, a live AR experience may require a significant amount of data bandwidth and processing power.
Disclosure of Invention
Aspects and advantages of embodiments of the disclosure will be set forth in part in the description which follows, or may be learned from the description, or may be learned by practice of the embodiments.
One example aspect of the present disclosure relates to a computer-implemented method for providing pre-rendered enhanced images. The method may include obtaining, by a computing device, a plurality of template images. The method may include processing, by the computing device, the plurality of template images with the augmented reality rendering model to generate a plurality of pre-rendered images. In some implementations, the method can include receiving, by a computing device, a request and a preference for a result image. The method may include providing, by the computing device, a pre-rendering result based at least in part on the request and the preference. In some implementations, the pre-rendering result can be a pre-rendered image from a plurality of pre-rendered images.
Another example aspect of the present disclosure relates to a computing system. The computer system may include one or more processors and one or more non-transitory computer-readable media collectively storing instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operations may include obtaining an augmented reality asset. In some implementations, the augmented reality asset can include digitized parameters. The operations may include obtaining a plurality of template images. The operations may include processing the plurality of template images with the augmented reality model to generate a plurality of pre-rendered images based at least in part on the digitized parameters and storing the plurality of pre-rendered images.
Another example aspect of the disclosure relates to one or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause a computing system to perform operations. The operations may include obtaining an augmented reality asset. The augmented reality asset may include digitized parameters. In some implementations, the operations may include obtaining a plurality of template images. The operations may include processing the plurality of template images with the augmented reality model to generate a plurality of pre-rendered images based at least in part on the digitized parameters. The operations may include storing a plurality of pre-rendered images on a server and receiving a search query that may include one or more search terms. One or more search terms may be related to a product. In some implementations, the operations may include providing search results. The search results may include pre-rendered images from a plurality of pre-rendered images retrieved from a server. The pre-rendered image may include a rendering of the product.
Other aspects of the disclosure relate to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the principles of interest.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification with reference to the accompanying drawings, in which:
FIG. 1A depicts a block diagram of an example computing system performing prerendering, according to an example embodiment of the present disclosure.
FIG. 1B depicts a block diagram of an example computing device performing prerendering in accordance with an example embodiment of the present disclosure.
FIG. 1C depicts a block diagram of an example computing device performing prerendering, according to an example embodiment of the present disclosure.
FIG. 2 depicts a block diagram of an example display of a pre-rendered image, according to an example embodiment of the present disclosure.
FIG. 3 depicts a block diagram of an example display of a pre-rendered image, according to an example embodiment of the present disclosure.
Fig. 4 depicts a block diagram of an example display of a pre-rendered image, according to an example embodiment of the present disclosure.
Fig. 5 depicts a block diagram of an example pre-rendering system, according to an example embodiment of the present disclosure.
Fig. 6 depicts a flowchart of an example method for performing prerendering, according to an example embodiment of the present disclosure.
Fig. 7 depicts a flowchart of an example method for performing prerendering, according to an example embodiment of the present disclosure.
Fig. 8 depicts a flowchart of an example method for performing prerendering, according to an example embodiment of the present disclosure.
Fig. 9 depicts a block diagram of an example pre-rendering system, according to an example embodiment of the present disclosure.
The repeated reference characters across the several figures are intended to identify identical features in the various embodiments.
Detailed Description
SUMMARY
In general, the present disclosure is directed to systems and methods for pre-rendering an augmented reality template image, for example, to achieve a limited augmented reality experience from the template image as an alternative to a real-time personalized augmented reality experience using a specific image (image) of a user. In some implementations, the systems and methods may be implemented as a platform for generating pre-rendered augmented reality images from a set of template images. The system may include obtaining an augmented reality asset and a set of template images. The template image may be a set of different images showing people or settings having different characteristics or features, such as, for example, different eye colors or other visual characteristics. The set of template images may be processed by an augmented model or an augmented reality model parameterized by the obtained augmented reality asset. The augmented reality model may output a set of pre-rendered images, and the pre-rendered images may be stored on a server. In some implementations, the system can receive a request for a result image and a user preference. For example, the request may be a search query in the form of one or more search terms. The system may process the request and preferences to determine a resulting image. The resulting image may be one or more pre-rendered images from the set of pre-rendered images. For example, the user may select (or pre-select) one or more preferences, which may guide the selection of one or more pre-rendered images as the resulting image. The results may be provided to the user. In this way, one or more pre-rendered images (e.g., pre-rendered images generated from templates having characteristics that match characteristics of the user) that satisfy the user's preferences may be provided to the user. Thus, in some cases, the user may see how the product would look on a person or setting similar to themselves, but need not be in a location where it is practically feasible to use the camera and/or provide an actual image of themselves or their surroundings. In some implementations, the system may also provide the user with a link to a real-time augmented reality experience, e.g., so that the user may still pursue a full real-time AR experience if desired.
In some implementations, pre-rendering of the augmented reality image may be facilitated by the platform. The platform may be used to collect augmented reality data assets made using the platform, or alternatively, augmented reality assets generated external to the platform may be collected. The platform may collect augmented reality assets from third party companies that provide products for sale (e.g., cosmetics (e.g., lipstick, eye shadow, etc.), furniture or other household items (e.g., electronic devices, cookware, glassware, ornaments, plants, etc.), apparel, paint, automobiles, various electronic products, etc.). Further, the platform may use the collected augmented reality assets to render rendering effects into a template image to generate a pre-rendered image. The template image may be stored locally or obtained from outside the platform. The pre-rendered image may then be stored for later display to the user. For example, the user computing device may send a request for one or more pre-rendered images. The request may be a search query, a user selection, or an automatic request to react to a user action. The platform may process the request and the user's preferences to provide one or more pre-rendered images related to the request and preferences.
The systems and methods may include obtaining a plurality of template images. The plurality of template images may be a set of images depicting similar focus but with variations (e.g., each of the template images depicts a living room, but each picture depicts a different color scheme for the living room, with furniture and decorations having varying themes and colors). The systems and methods may process a plurality of template images using an augmented reality rendering model to generate a plurality of pre-rendered images. In some implementations, the augmented reality rendering model may include object tracking and rendering. The plurality of pre-rendered images may be a set of enhanced images, wherein each template image may be enhanced to include an augmented reality rendering effect. The augmented reality rendering effect may be a product sold by a third party, where the consumer may virtually "try in" the product (e.g., an item of furniture rendered into a template room image) in a different template scene. In some implementations, the systems and methods can include receiving a request and a preference for a result image. The request may be from a user computing system where the user has indicated a desire to view the pre-rendered image. In some implementations, the request can include search terms entered into a search engine. The preferences may include selections made at the time of request or previously stored, which may indicate a preferred template type or a preferred template. The systems and methods may then provide a pre-rendering result based at least in part on the request and the preference, wherein the pre-rendering result may be a pre-rendered image from the plurality of pre-rendered images. In some implementations, the pre-rendering result can be a pre-rendered image that matches a preference, where the preference includes a template selected by a user from a plurality of template images. In some implementations, the systems and methods may provide a link to an augmented reality rendering experience for live fitting.
In some implementations, the systems and methods may obtain an augmented reality asset that may be stored for use with an augmented reality model or an augmented model. The augmented reality asset may include digitized parameters. The digitized parameters may enable the augmented reality model or the augmented model to render a particular rendering effect. In some implementations, the augmented reality asset can be used by an augmented reality model to process a set of template images to generate a set of pre-rendered images. The set of pre-rendered images may then be stored for later retrieval. In some implementations, the set of pre-rendered images may be stored on a server.
In some implementations, the set of template images may be processed by object tracking computer vision algorithms and computer rendering operations to generate an augmented reality pre-rendered image that simulates the appearance of a product. The product may be applied to the depicted face in the template image or otherwise inserted into the depicted images in the set of template images. Rendering operations may be affected by the CPU or GPU algorithms and corresponding parameters. The corresponding parameter may be a parameter that truly captures the appearance of the product inserted into the template image under the depicted lighting conditions.
In some implementations, systems and methods can include receiving a search query that includes one or more search terms, where the one or more search terms are related to a product. Further, the systems and methods may include providing search results. The search results may include pre-rendered images from a plurality of pre-rendered images. Further, the pre-rendered image may include a rendering of the product.
In some implementations, the set of template images may be processed to generate a set of template models that can be processed by an augmented reality model or an augmented model. In some implementations, the template model may be modified prior to processing. The systems and methods may receive an input to modify a template model of a plurality of template models. The systems and methods may modify the template model based at least in part on the input. In some implementations, the systems and methods may include providing a template model of a plurality of template models for display.
In some implementations, the augmented reality model may include a perceived subgraph and a rendered subgraph. The perceptual subgraphs may be unified (unified) throughout the system. The perceptual subgraph may be used with a variety of different rendered subgraphs. The render sub-graph may be constructed by a third party to generate rendering effects to provide to the user. The rendered sub-graph may be constructed and then used by an augmented reality pre-rendering platform that stores the perceived sub-graph. Rendering subgraphs may vary depending on the rendering effect and the third party. In some implementations, a single perceptual subgraph may be used with multiple rendering subgraphs to render multiple renderings in an enhanced image or video. For example, pictures or videos of a face may be processed to generate an augmented reality rendering of lipstick, eye shadow, and mascara on the face. The process may include a single perceived subgraph, but a rendered subgraph for each respective product (i.e., lipstick, eye shadow, and mascara).
The systems and methods disclosed herein may be applicable to a variety of augmented reality experiences (e.g., household, cosmetics, automotive (3D), glasses, jewelry, clothing, and hair cutting). For example, the systems and methods disclosed herein may be used to generate a set of pre-rendered images to provide a consumer with a rendering of a product in various environments or applications. In some implementations, the template image can be various room images of varying settings, color schemes, and decorations. The desired rendering effect may be a sofa. The system may acquire the set of template images and the augmented reality asset associated with the sofa and may generate a set of pre-rendered images to provide to the user. The pre-rendered image may include an augmented reality rendering of the desired sofa in each of the template images to allow a user to see how the sofa would look in various room types with various color schemes, decorations, and layouts.
In some implementations, the plurality of template images may include a set of facial images having different characteristics or features (such as, for example, different eye colors or other visual characteristics), wherein the consumer may select and/or store a preference template that may be most similar to his (her) people. The rendering effect may be cosmetic rendering (e.g., lipstick, eye shadow, mascara, foundation, etc.). The augmented reality model may process the augmented reality asset and a set of template images to generate a pre-rendered cosmetic image, wherein each of the plurality of template images may be augmented to include a cosmetic product. For example, a lipstick of a certain color (shade) may be rendered into each of the template images.
In some implementations, a set of template images may be manually or automatically selected from a large corpus of images to provide a representative grouping of template images that indicate representative images of varying scenes or features for a given topic. For example, the theme may be a room, a neighborhood, a face, etc.
The systems and methods disclosed herein may be used to provide personalized advertisements to a user. For example, the user's preferences may be stored. The stored preferences may customize the provided product advertisements to render the advertised product on the user's preferred template image.
In some implementations, the augmented reality asset may be managed, generated, and/or rendered by the product brand. The digitised parameters may be taken from a third party company. In some implementations, the digitized parameters may be extracted from a third party rendering engine or derived from a provided template for augmented reality rendering effect generation.
Furthermore, the perceptual model may be manually or automatically adjusted to provide an optimized mesh. The adjustment may react to illumination or varying image quality. The platform may provide previews to help modify the augmented reality model. In some implementations, the platform may include a pipeline for retrieving images.
The pre-rendered image may include a label to index the rendered product.
In some implementations, indexing the products may include a unified naming process for the products and product colors to obtain better search results. The platform may include data structures that may relate the augmented reality assets to certain semantic or lexical (lexoggraphic) entities. The data structure may help understand the search query to create a product map.
In some embodiments, the pre-rendered image may be provided as a dial (carousels) for the user to scroll through. Alternatively, a changing pre-rendered image with a different product rendering but the same template image may be provided in the carousel for personalized previews. Further, in some implementations, the carousel may include a virtual "try-on" panel for providing the user with an augmented reality rendering experience that processes the user's data to provide the user-enhanced image or video.
In some implementations, the augmented reality platform may retrieve data assets for rendering the augmented reality effect via systems and methods for data asset acquisition. Systems and methods for data asset acquisition may involve one or more systems or devices. The first computing device may be a server, a facilitating computing device, or an intermediary computing device. The second computing device may be a third party computing device. The third party may be a video game company, a product manufacturer, or a product brand. The first computing device and the second computing device may exchange data to generate an augmented reality rendering experience for the user. The augmented reality rendering experience may include rendering an augmented reality view including one or more products or items. The product may be a cosmetic (e.g., lipstick, eye shadow, etc.), furniture or other household (e.g., electronic equipment, cookware, glassware, ornaments, plants, etc.), apparel, paint, automotive, various electronic products, or any other item.
The data asset acquisition may include the first computing device sending a software development kit to the second computing device. The software development kit may include templates for building rendering effect shaders. The software development kit may include example effects, tools for building rendering effects, and preview modes for helping build augmented reality renderings. The second computing device may be used to build the rendering effect, and once the rendering effect is built, the second computing device may export the built rendering effect data into a renderable compressed file (e.g., a ZIP file), which may include the data assets required to recreate the rendering effect. The data asset may then be transmitted to the first computing device. The first computing device, upon receiving the data asset, may store the data asset for use in an augmented reality rendering experience provided to the user. The provided augmented reality rendering experience may be provided to a user, where the user may input their user data for processing, and the output may be augmented user data including rendering effects built on the second computing device. The user data may be image data or video data captured by the user device. In some implementations, the user data can be a live camera feed.
Further, in some embodiments, the systems and methods may be used as a visual compatibility calculator. For example, the system and method may be used to ensure that a certain product or component will fit in a desired space or location. The system and method may be used to virtually test the measurement/size of a product using virtual reality. The third party may provide a data asset, which may include data describing the measurement of the product. The data asset may then be used to provide an augmented reality rendering experience to the user, wherein the product is rendered according to the measurements provided by the third party. This aspect may allow a consumer to "try on" a product to visualize the space that the product may occupy.
The systems and methods of the present disclosure provide a number of technical effects and benefits. As one example, the systems and methods may use a set of template images and an augmented reality experience to generate a set of pre-rendered images. The system and method may also be used to provide a consumer with pre-rendered images of a product on a varying template. Further, the system and method may enable a consumer to view pre-rendered augmented reality renderings on templates that satisfy consumer preferences.
Another technical benefit of the systems and methods of the present disclosure is the ability to provide "try-on" images that match consumer shopping preferences when an augmented reality real-time try-on experience is not ideal.
Referring now to the drawings, example embodiments of the present disclosure will be discussed in more detail.
Example devices and systems
FIG. 1A depicts a block diagram of an example computing system 100 performing prerendering, according to an example embodiment of the present disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled by a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., a laptop computer or desktop computer), a mobile computing device (e.g., a smart phone or tablet computer), a game console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor or multiple processors operatively connected. Memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, and combinations thereof. The memory 114 may store instructions 118 and data 116 that are executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 may store or include one or more augmented reality models 120 or augmented reality models. For example, the augmented reality model 120 may be or may otherwise include various machine learning models, such as a neural network (e.g., deep neural network) or other types of machine learning models, including non-linear models and/or linear models. The neural network may include a feed forward neural network, a recurrent neural network (e.g., a long and short memory recurrent neural network), a convolutional neural network, or other form of neural network. An example augmented reality rendering model 120 is discussed with reference to fig. 5-9.
In some implementations, one or more augmented reality rendering models 120 may be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 may implement multiple parallel instances of a single augmented reality rendering model 120.
More specifically, the augmented reality rendering model may utilize the perception model and the rendering model to render the augmented reality rendering into the template image. The perceptual model may be a model stored on a platform that is adapted to various rendering models. The rendering model may be generated by a third party using a software development kit. The rendering model may be sent to the platform after being built by a third party. In some implementations, the platform can receive data assets for rendering the model from a third party.
The template image may be processed by an augmented reality rendering model, wherein the perception model generates a mesh and a segmentation mask based on the processing of the template image, and the rendering model may process the template image, the mesh and the segmentation mask to generate a pre-rendered image.
Additionally or alternatively, one or more augmented reality rendering models 140 may be included in the server computing system 130 or otherwise stored and implemented by the server computing system 130, the server computing system 130 in communication with the user computing device 102 according to a client-server relationship. For example, the augmented reality rendering model 140 may be implemented by the server computing system 140 as part of a web service (e.g., a pre-rendering try-in service). Accordingly, one or more models 120 may be stored and implemented at the user computing device 102 and/or one or more models 140 may be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other means by which a user may provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor or multiple processors operatively connected. Memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, and combinations thereof. Memory 134 may store instructions 138 and data 136 that are executed by processor 132 to cause server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances where the server computing system 130 includes multiple server computing devices, such server computing devices may operate in accordance with a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include one or more machine-learned augmented reality rendering models 140. For example, model 140 may be or may otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layer nonlinear models. Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. An example model 140 is discussed with reference to fig. 5-9.
The user computing device 102 and/or the server computing system 130 may train the models 120 and/or 140 via interaction with a training computing system 150 communicatively coupled via a network 180. The training computing system 150 may be separate from the server computing system 130 or may be part of the server computing system 130.
The training computing system 150 includes one or more processors 152 and memory 154. The one or more processors 152 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor or multiple processors operatively connected. The memory 154 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, and combinations thereof. Memory 154 may store instructions 158 and data 156 that are executed by processor 152 to cause training computing system 150 to perform operations. In some implementations, the training computing system 150 includes or is otherwise implemented by one or more server computing devices.
Training computing system 150 may include a model trainer 160, model trainer 160 using various training or learning techniques (such as, for example, back propagation of errors) to train machine learning models 120 and/or 140 stored at user computing device 102 and/or server computing system 130. For example, the loss function may be back-propagated through the model(s) to update one or more parameters of the model(s) (e.g., a gradient based on the loss function). Various loss functions may be used, such as mean square error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. Gradient descent techniques may be used to iteratively update parameters over multiple training iterations.
In some implementations, performing back-propagation of the error may include performing back-propagation of the truncated transit time. Model trainer 160 may perform several generalization techniques (e.g., weight decay, discard (dropout), etc.) to improve the generalization ability of the model being trained.
In particular, model trainer 160 may train augmented reality models 120 and/or 140 based on a set of training data 162. The training data 162 may include, for example, shaders built by third parties using a software development kit that the third parties receive from the facilitating computing device or the server computing system 130. The third party may have generated shaders and data assets by building and testing the augmented reality experience with a software development kit.
In some implementations, the training examples can be provided by the user computing device 102 if the user has provided consent. Thus, in such embodiments, the model 120 provided to the user computing device 102 may be trained by the training computing system 150 on user-specific data received from the user computing device 102. In some cases, this process may be referred to as personalizing the model.
In some implementations, the template image can be used to train a perception model and a rendering model of the augmented reality model. The template images may be images from a corpus of template images provided to the user as pre-rendered images, or in some embodiments, the training template images may be a different set of template images used only for training.
Model trainer 160 includes computer logic that is used to provide the desired functionality. Model trainer 160 may be implemented in hardware, firmware, and/or software controlling a general purpose processor. For example, in some embodiments, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium, such as a RAM hard disk or an optical or magnetic medium.
The network 180 may be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof, and may include any number of wired or wireless links. In general, communications over network 180 may be carried via any type of wired and/or wireless connection using a variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), coding or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
The machine learning model described in this specification may be used in a variety of tasks, applications, and/or use cases.
In some implementations, the input to the machine learning model(s) of the present disclosure can be image data. The machine learning model(s) may process the image data to generate an output. As an example, the machine learning model(s) may process the image data to generate an image recognition output (e.g., recognition of the image data, potential embedding of the image data, encoded representation of the image data, hashing of the image data, etc.). As another example, the machine learning model(s) may process the image data to generate an image segmentation output. As another example, the machine learning model(s) may process the image data to generate an image classification output. As another example, the machine learning model(s) may process the image data to generate an image data modification output (e.g., a change in the image data, etc.). As another example, the machine learning model(s) may process the image data to generate an encoded image data output (e.g., an encoded and/or compressed representation of the image data, etc.). As another example, the machine learning model(s) may process the image data to generate an enlarged (upscale) image data output. As another example, the machine learning model(s) may process the image data to generate a prediction output.
In some implementations, the input to the machine learning model(s) of the present disclosure can be text or natural language data. The machine learning model(s) may process text or natural language data to generate an output. As an example, the machine learning model(s) may process natural language data to generate a language encoded output. As another example, the machine learning model(s) may process text or natural language data to generate a potential text-embedded output. As another example, the machine learning model(s) may process text or natural language data to generate a translation output. As another example, the machine learning model(s) may process text or natural language data to generate a classification output. As another example, the machine learning model(s) may process text or natural language data to generate text segmentation output. As another example, the machine learning model(s) may process text or natural language data to generate semantic intent output. As another example, the machine learning model(s) may process text or natural language data to generate an enlarged text or natural language output (e.g., text or natural language data of higher quality than the input text or natural language, etc.). As another example, the machine learning model(s) may process text or natural language data to generate a predictive output.
In some implementations, the input to the machine learning model(s) of the present disclosure can be potentially encoded data (e.g., a potential spatial representation of the input, etc.). The machine learning model(s) may process the potentially encoded data to generate an output. As an example, the machine learning model(s) may process the potentially encoded data to generate an identification output. As another example, the machine learning model(s) may process the potentially encoded data to generate a reconstructed output. As another example, the machine learning model(s) may process the potentially encoded data to generate a search output. As another example, the machine learning model(s) may process the potentially encoded data to generate a reclustering output. As another example, the machine learning model(s) may process the potentially encoded data to generate a prediction output.
In some implementations, the input to the machine learning model(s) of the present disclosure can be sensor data. The machine learning model(s) may process the sensor data to generate an output. As an example, the machine learning model(s) may process the sensor data to generate an identification output. As another example, the machine learning model(s) may process the sensor data to generate a prediction output. As another example, the machine learning model(s) may process the sensor data to generate classification output. As another example, the machine learning model(s) may process the sensor data to generate a segmented output. As another example, the machine learning model(s) may process the sensor data to generate a segmented output. As another example, the machine learning model(s) may process the sensor data to generate a visual output. As another example, the machine learning model(s) may process the sensor data to generate diagnostic output. As another example, the machine learning model(s) may process the sensor data to generate a detection output.
In some cases, the machine learning model(s) may be configured to perform tasks that include encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding). In another example, the input includes visual data (e.g., one or more images or videos), the output includes compressed visual data, and the task is a visual data compression task. In another example, the task may include generating an embedding of the input data (e.g., visual data).
In some cases, the input includes visual data and the task is a computer visual task. In some cases, pixel data including one or more images is input, and the task is an image processing task. For example, an image processing task may be an image classification, where the output is a set of scores, each score corresponding to a different object class and representing the likelihood that one or more images depict an object belonging to that object class. The image processing task may be object detection, wherein the image processing output identifies one or more regions in the one or more images and a likelihood that the region depicts the object of interest for each region. As another example, the image processing task may be image segmentation, wherein the image processing output defines a respective likelihood for each of a predetermined set of categories for each pixel in the one or more images. For example, the set of categories may be foreground and background. As another example, the set of classes may be object classes. As another example, the image processing task may be depth estimation, wherein the image processing output defines a respective depth value for each pixel in the one or more images. As another example, the image processing task may be motion estimation, wherein the network input includes a plurality of images, and the image processing output defines, for each pixel of one of the input images, a motion of a scene depicted at a pixel between the images in the network input.
FIG. 1A illustrates one example computing system that may be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such implementations, the model 120 may be trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 may implement the model trainer 160 to personalize the model 120 based on user-specific data.
FIG. 1B depicts a block diagram of an example computing device 10, performed in accordance with an example embodiment of the present disclosure. Computing device 10 may be a user computing device or a server computing device.
Computing device 10 includes several applications (e.g., application 1 through application N). Each application contains its own machine learning library and machine learning model(s). For example, each application may include a machine learning model. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like.
As shown in fig. 1B, each application may communicate with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., public API). In some implementations, the APIs used by each application are specific to that application.
Fig. 1C depicts a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
Computing device 50 includes several applications (e.g., application 1 through application N). Each application communicates with a central intelligent layer. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like. In some implementations, each application can communicate with the central intelligence layer (and model(s) stored therein) using an API (e.g., a public API across all applications).
The central intelligence layer includes a number of machine learning models. For example, as shown in fig. 1C, a respective machine learning model (e.g., model) may be provided for each application and managed by a central intelligent layer. In other implementations, two or more applications may share a single machine learning model. For example, in some implementations, the central intelligence layer can provide a single model (e.g., a single model) for all applications. In some implementations, the central intelligence layer is included within or otherwise implemented by the operating system of computing device 50.
The central intelligence layer may communicate with the central device data layer. The central device data layer may be a centralized repository for data of computing devices 50. As shown in fig. 1C, the central device data layer may communicate with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Example model arrangement
Fig. 2 depicts a block diagram of an example implementation 200 according to an example embodiment of the present disclosure. In some implementations, the example implementation 200 includes a search results list 204 that describes the results of the search query, and as a result of receiving the search results list 204, a pre-rendered image 208 is provided as an augmented reality pre-rendered image that includes a rendering of the resulting product from one or more of the results 206 of the search results list 204. Thus, in some implementations, the example embodiment 200 may include a search query input area 202 operable to receive search query inputs.
The example embodiment of fig. 2 depicts a search engine web service 210. The search engine input area 202 may obtain a search query that may include one or more search terms. The platform may retrieve and process the search query to provide a search result list 204. One or more of the search terms may relate to a product to be purchased. In these implementations, the search result list 204 can be one or more products related to the search term. In some implementations, one or more of the results 206 from the search results list 204 can include one or more pre-rendered images 208 associated with the results 206. In some implementations, the pre-rendered image 208 may include a rendering of the corresponding result 206. In some implementations, the user may further select a virtual live try-on augmented reality experience.
Fig. 3 depicts a block diagram of an example implementation 300 according to an example embodiment of the present disclosure. Example embodiment 300 is similar to example embodiment 200 of fig. 2, except example embodiment 300 further depicts example results.
FIG. 3 depicts an example embodiment 300 as a furniture fitting experience. For example, as shown in embodiment 300, product 302 may be rendered into a template environment to generate a pre-rendered image. In this example, the love seat 304, television 306, and carpet 308 are part of a template image of an example room. The template image may be one of a plurality of template images that may be processed to generate a plurality of pre-rendered images. In this example, the plurality of pre-rendered images may include a rendering of the product 302 in each of the respective template images. Template images may have varying sizes, themes, and configurations. For example, the example pre-rendered image includes a television 306 opposite a double sofa 304 with a carpet 308 therebetween. In some implementations, the platform may allow a user to view products rendered at various locations in the template image.
Fig. 4 depicts a block diagram of an example implementation 400 according to an example embodiment of the present disclosure. Example embodiment 400 is similar to example embodiment 200 of fig. 2, except that example embodiment 400 further depicts an example embodiment in a mobile application.
In the example implementation depicted in fig. 4, the platform is accessed through a user interface in a mobile application on the mobile device 402. A user may use a mobile device to access a mobile application, where the user may store preferences and access a library of pre-rendered images. For example, the user may have a preference for a certain living room template image 408 selected by the user. The user may have selected the template image 408 as the closest match to the user's living room. The template image 408 may include a similar sofa 404 and a similar light 406 as found in a user's home. Template image 408, as well as other multiple template images, may have been processed to generate various pre-rendered images for various furniture and decorative products. The platform, with the aid of the mobile application, can provide various decorative or furniture items that are rendered into the user's preferred template image 408 to assist the user in their shopping experience. For example, a user may use a mobile application to see how a carpet will look under a sofa 404 and lights 406 that are similar to the settings in their living room.
Fig. 5 depicts a block diagram of an example platform 500, according to an example embodiment of the present disclosure. In some implementations, the platform 500 is trained to receive preferences 506 requesting and describing user-specific preferences, and as a result of receiving the preferences 506, provide output data including an augmented reality rendered pre-rendered image on a template image associated with the user-specific preferences 506. Thus, in some implementations, the platform 500 may include a directory 510, the directory 510 being operable to store template images, augmented reality data assets, and pre-rendered images.
The example platform of fig. 5 includes a catalog 510 of pre-rendered images and a user 502. The user may have settings 504 selected by the user, which may include preferences 506. In some implementations, the preferences 506 can include selected preferences related to the template image. When a request is made, preferences 506 may be used to determine what pre-rendered images may be provided to the user from among the plurality of pre-rendered images. For example, preferences related to the first template image may cause the platform to provide a pre-rendered image that includes a product that is rendered into the first template image.
The catalog 510 may store template images, augmented reality assets, and pre-rendered images. The template image may be a template image for various environments including, but not limited to, rooms, patios, driveways, and faces. Augmented reality assets can be used to render a variety of objects and products including, but not limited to, furniture, decorations, plants, electronics, automobiles, and cosmetics. In some implementations, the pre-rendered image can include a product that is rendered into the template image, where the product can be rendered based on the augmented reality asset.
In some implementations, the platform may store pre-rendered images of various different products in various environments. For example, the data set 1 may include various pre-rendered images 512 of lights in various living rooms. The data set 2 may include a plurality of pre-rendered images 514 of lipstick on the face. Dataset 3 may include a plurality of pre-rendered images 516 of a tree in a yard. In some implementations, a user may access the pre-rendered image to facilitate shopping using a user interface provided by a web service, mobile application, or self-service terminal.
Fig. 9 depicts a block diagram of an example enhancement platform 900, according to an example embodiment of the present disclosure. In some implementations, the enhanced platform 900 is trained to receive a set of input data describing a user request and, as a result of receiving the input data, provide output data including a pre-rendered rendering in a template image. Thus, in some implementations, the augmented platform 900 may include an augmented reality pre-rendering platform 920 operable to interact with a user device and implement an augmented reality pre-rendering experience.
The augmented reality pre-rendering platform 920 depicted in fig. 9 includes a user interface 922 for allowing user interaction, a template library 924 for processing, a rendering engine 926 for processing template images, and a pre-rendering library 928 storing pre-rendered images generated by processing template images.
In some implementations, the augmented reality pre-rendering platform 920 can receive user preferences 912 from the user computing device 910. User preferences 912 may be used to determine which of a plurality of pre-rendered images to provide to user computing device 910 from pre-rendering library 928.
In some implementations, the augmented reality pre-rendering platform 920 can provide options for an augmented reality live try-on experience. The experience may involve processing, by rendering engine 926, the user media data or user camera feed 914 to generate a rendering in the user-provided data.
Example method
Fig. 6 depicts a flowchart of an example method for performing in accordance with an example embodiment of the present disclosure. Although fig. 6 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular order or arrangement shown. The various steps of method 600 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 602, a computing system may obtain a plurality of template images. In some implementations, the template image may include a room image, a face image, or a yard image. The plurality of template images may include images of varying environments. For example, the plurality of template images may include a variety of different room sizes, configurations, themes, lighting, or colors.
At 604, the computing system may process the plurality of template images with an augmented reality rendering model. The augmented reality rendering model may include object tracking and rendering. Processing the template image with the augmented reality rendering model may generate a plurality of pre-rendered images. The plurality of pre-rendered images may include an augmented reality rendering that is rendered within each of the plurality of template images. Augmented reality rendering may describe a product. The augmented reality rendering model may be based at least in part on data assets provided by a third party, wherein the data assets are generated by the third party by building an augmented reality rendering experience that renders the product in images and video. For example, the product being rendered may be furniture (e.g., a sofa, chair, table, etc.). The plurality of pre-rendered images may include a particular sofa rendered into a plurality of different template images depicting various different rooms having different sizes, colors, themes, and configurations. In another example, the product may be a cosmetic product (e.g., lipstick, mascara, foundation, eyeliner, eye shadow, etc.). In this embodiment, the plurality of pre-rendered images may include a rendering of a cosmetic product (such as lipstick). In this embodiment, lipstick may be rendered on multiple template face images.
In some implementations, processing with the augmented reality rendering model may include processing the plurality of template images with the perception model to generate a mesh and a segmentation mask, and then processing the mesh, the segmentation mask, and the template images with the rendering model to generate a plurality of pre-rendered images.
At 606, the computing system may receive a request and preferences for a resulting image. The request may include search terms entered into a search engine. In some implementations, the preferences may include pre-selected templates.
At 608, the computing system may provide the pre-rendering results. The prerendered results may be based at least in part on the request and the preference. The pre-rendering result may be a pre-rendered image from a plurality of pre-rendered images. In some implementations, the pre-rendering result can be a pre-rendered image that matches the preference, where the preference can include a template selected by the user from a plurality of template images.
In some implementations, a computer system may provide an augmented reality experience. When a selection is received to an augmented reality experience, an augmented reality experience may be provided.
Fig. 7 depicts a flowchart of an example method for performing in accordance with an example embodiment of the present disclosure. Although fig. 7 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular order or arrangement shown. The various steps of method 700 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 702, a computing system may obtain an augmented reality asset. The augmented reality asset may include digitized parameters.
At 704, the computing system may obtain a plurality of template images.
At 706, the computing system may process the plurality of template images with the augmented reality model to generate a plurality of pre-rendered images. A plurality of pre-rendered images may be generated based at least in part on the digitized parameters. In some implementations, the computer system can generate a plurality of template models based at least in part on the plurality of template images. The augmented reality model may then process the plurality of template models to generate a plurality of pre-rendered images.
In some implementations, a computing system may include receiving input to modify a template model of a plurality of template models. The template model may be modified based at least in part on the template. In some implementations, a template model may be provided for display.
At 708, the computing system may store a plurality of pre-rendered images. In some implementations, the plurality of pre-rendered images may be stored on a server.
The computing system may provide the stored pre-rendered image to the user. In some implementations, a computing system may receive a search query that includes one or more search terms, where the one or more search terms are related to a product. The computing system may then provide search results, where the search results include pre-rendered images from the plurality of pre-rendered images. In some implementations, the pre-rendered image can include a rendering of the product.
In some implementations, the computing system may provide a link to a real-time augmented reality experience.
Fig. 8 depicts a flowchart of an example method for performing in accordance with an example embodiment of the present disclosure. Although fig. 8 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular order or arrangement shown. The various steps of method 800 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 802, a computing system may obtain an augmented reality asset. The augmented reality asset may include digitized parameters.
At 804, the computing system may obtain a plurality of template images.
At 806, the computing system may process the plurality of template images with the augmented reality model to generate a plurality of pre-rendered images. The augmented reality model may generate a plurality of pre-rendered images based at least in part on the digitized parameters.
At 808, the computing system may store the plurality of pre-rendered images.
At 810, a computing system may receive a search query. The search query may include one or more search terms, wherein the one or more search terms are related to the product.
At 812, the computing system may provide the search results. The search results may include a pre-rendered image from a plurality of pre-rendered images retrieved from a server, where the pre-rendered image may include a rendering of the product.
Additional disclosure
The technology discussed herein refers to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between and among components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components working in combination. The database and applications may be implemented on a single system or distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation and not limitation of the present disclosure. Modifications, variations and equivalents to these embodiments may readily occur to those skilled in the art after having obtained an understanding of the foregoing. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For example, features illustrated or described as part of one embodiment can be used with another embodiment to yield still a further embodiment. Accordingly, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (20)
1. A computer-implemented method for providing a pre-rendered enhanced image, the method comprising:
obtaining, by a computing device, a plurality of template images;
processing, by the computing device, the plurality of template images with the augmented reality rendering model to generate a plurality of pre-rendered images;
receiving, by the computing device, a request and a preference for a result image; and
a pre-rendering result is provided by the computing device based at least in part on the request and the preference, wherein the pre-rendering result is a pre-rendered image from the plurality of pre-rendered images.
2. The computer-implemented method of any preceding claim, wherein the plurality of template images comprises a plurality of room images.
3. The computer-implemented method of any preceding claim, wherein the plurality of pre-rendered images comprises a plurality of rendered furniture images.
4. The computer-implemented method of any preceding claim, wherein the plurality of template images comprises a plurality of face images.
5. The computer-implemented method of any preceding claim, wherein the plurality of pre-rendered images comprises a plurality of rendered cosmetic images.
6. The computer-implemented method of any preceding claim, wherein the augmented reality rendering model comprises a perception model and a rendering model.
7. The computer-implemented method of any preceding claim, wherein the request includes a search term input into a search engine.
8. The computer-implemented method of any preceding claim, wherein the preferences comprise pre-selected templates.
9. The computer-implemented method of any preceding claim, wherein the pre-rendering result is a pre-rendered image that matches a preference, wherein the preference comprises a template selected by a user from a plurality of template images.
10. The computer-implemented method of any preceding claim, further comprising: an augmented reality rendering experience is provided by a computing device.
11. A computing system, comprising:
one or more processors;
one or more non-transitory computer-readable media collectively storing instructions that, when executed by the one or more processors, cause a computing system to perform operations comprising:
obtaining an augmented reality asset, wherein the augmented reality asset comprises digitized parameters;
obtaining a plurality of template images;
processing the plurality of template images with an augmented reality model to generate a plurality of pre-rendered images based at least in part on the digitized parameters; and
A plurality of pre-rendered images is stored.
12. The computing system of any preceding claim, further comprising: a plurality of template models is generated based at least in part on the plurality of template images, wherein the augmented reality model processes the plurality of template models.
13. The computing system of any preceding claim, further comprising:
receiving input to modify a template model of the plurality of template models; and
the template model is modified based at least in part on the input.
14. The computing system of any preceding claim, further comprising: a template model of the plurality of template models is provided for display.
15. The computing system of any preceding claim, wherein to store the plurality of pre-rendered images comprises to store the plurality of pre-rendered images on a server.
16. The computing system of any preceding claim, further comprising: a pre-rendered image from the plurality of pre-rendered images is provided to a user.
17. The computing system of any preceding claim, further comprising: a link to a real-time augmented reality experience is provided.
18. The computing system of any preceding claim, further comprising:
Receiving a search query comprising one or more search terms, wherein the one or more search terms are related to a product; and
providing a search result, wherein the search result comprises a pre-rendered image from the plurality of pre-rendered images, wherein the pre-rendered image comprises a rendering of a product.
19. The computing system of any preceding claim, wherein the augmented reality asset comprises data describing a product.
20. One or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause a computing system to perform operations comprising:
obtaining an augmented reality asset, wherein the augmented reality asset comprises digitized parameters;
obtaining a plurality of template images;
processing the plurality of template images with an augmented reality model to generate a plurality of pre-rendered images based at least in part on the digitized parameters;
storing the plurality of pre-rendered images on a server;
receiving a search query comprising one or more search terms, wherein the one or more search terms are related to a product; and
providing a search result, wherein the search result comprises a pre-rendered image from the plurality of pre-rendered images retrieved from a server, wherein the pre-rendered image comprises a rendering of a product.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/153,263 | 2021-01-20 | ||
US17/153,263 US11263821B1 (en) | 2021-01-20 | 2021-01-20 | Generating augmented reality prerenderings using template images |
PCT/US2022/011626 WO2022159283A1 (en) | 2021-01-20 | 2022-01-07 | Generating augmented reality prerenderings using template images |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116783589A true CN116783589A (en) | 2023-09-19 |
Family
ID=80119559
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280010102.4A Pending CN116783589A (en) | 2021-01-20 | 2022-01-07 | Generating augmented reality pre-rendering using template images |
Country Status (6)
Country | Link |
---|---|
US (3) | US11263821B1 (en) |
EP (1) | EP4252121A1 (en) |
JP (1) | JP2024502495A (en) |
KR (1) | KR20230124091A (en) |
CN (1) | CN116783589A (en) |
WO (1) | WO2022159283A1 (en) |
Families Citing this family (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220245898A1 (en) * | 2021-02-02 | 2022-08-04 | Unisys Corporation | Augmented reality based on diagrams and videos |
US20220308720A1 (en) * | 2021-03-26 | 2022-09-29 | MashApp, Inc. | Data augmentation and interface for controllable partitioned sections |
US11670059B2 (en) | 2021-09-01 | 2023-06-06 | Snap Inc. | Controlling interactive fashion based on body gestures |
US11673054B2 (en) | 2021-09-07 | 2023-06-13 | Snap Inc. | Controlling AR games on fashion items |
US11900506B2 (en) * | 2021-09-09 | 2024-02-13 | Snap Inc. | Controlling interactive fashion based on facial expressions |
US11734866B2 (en) | 2021-09-13 | 2023-08-22 | Snap Inc. | Controlling interactive fashion based on voice |
US11636662B2 (en) | 2021-09-30 | 2023-04-25 | Snap Inc. | Body normal network light and rendering control |
US11651572B2 (en) | 2021-10-11 | 2023-05-16 | Snap Inc. | Light and rendering of garments |
US20230368527A1 (en) * | 2022-05-10 | 2023-11-16 | Google Llc | Object Filtering and Information Display in an Augmented-Reality Experience |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104067274A (en) * | 2012-01-19 | 2014-09-24 | 谷歌公司 | System and method for improving access to search results |
CN110766780A (en) * | 2019-11-06 | 2020-02-07 | 北京无限光场科技有限公司 | Method and device for rendering room image, electronic equipment and computer readable medium |
US20200183969A1 (en) * | 2016-08-10 | 2020-06-11 | Zeekit Online Shopping Ltd. | Method, System, and Device of Virtual Dressing Utilizing Image Processing, Machine Learning, and Computer Vision |
Family Cites Families (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10970843B1 (en) * | 2015-06-24 | 2021-04-06 | Amazon Technologies, Inc. | Generating interactive content using a media universe database |
US10235810B2 (en) * | 2015-09-22 | 2019-03-19 | 3D Product Imaging Inc. | Augmented reality e-commerce for in-store retail |
US10049500B2 (en) * | 2015-09-22 | 2018-08-14 | 3D Product Imaging Inc. | Augmented reality e-commerce for home improvement |
US10311614B2 (en) * | 2016-09-07 | 2019-06-04 | Microsoft Technology Licensing, Llc | Customized realty renovation visualization |
CN206585648U (en) * | 2017-01-25 | 2017-10-24 | 米菲多媒体有限公司 | The electronic installation and electronic system of augmented reality image can be generated |
GB201709199D0 (en) * | 2017-06-09 | 2017-07-26 | Delamont Dean Lindsay | IR mixed reality and augmented reality gaming system |
WO2018227290A1 (en) * | 2017-06-14 | 2018-12-20 | Roborep Inc. | Telepresence management |
US11138255B2 (en) * | 2017-09-27 | 2021-10-05 | Facebook, Inc. | Providing combinations of pre-generated and dynamic media effects to create customized media communications |
GB2567203B (en) * | 2017-10-05 | 2022-02-23 | Displaylink Uk Ltd | System and method for locally generating data |
US10726633B2 (en) * | 2017-12-29 | 2020-07-28 | Facebook, Inc. | Systems and methods for generating and displaying artificial environments based on real-world environments |
US11050752B2 (en) * | 2018-06-07 | 2021-06-29 | Ebay Inc. | Virtual reality authentication |
US11727656B2 (en) * | 2018-06-12 | 2023-08-15 | Ebay Inc. | Reconstruction of 3D model with immersive experience |
US11386474B2 (en) * | 2018-10-09 | 2022-07-12 | Rovi Guides, Inc. | System and method for generating a product recommendation in a virtual try-on session |
US10936909B2 (en) * | 2018-11-12 | 2021-03-02 | Adobe Inc. | Learning to estimate high-dynamic range outdoor lighting parameters |
US10665037B1 (en) * | 2018-11-28 | 2020-05-26 | Seek Llc | Systems and methods for generating and intelligently distributing forms of extended reality content |
US11132056B2 (en) * | 2019-12-04 | 2021-09-28 | Facebook Technologies, Llc | Predictive eye tracking systems and methods for foveated rendering for electronic displays |
CN114730231A (en) * | 2020-06-05 | 2022-07-08 | M·塔什吉安 | Techniques for virtual try-on of an item |
US20220101417A1 (en) * | 2020-09-28 | 2022-03-31 | Snap Inc. | Providing augmented reality-based clothing in a messaging system |
-
2021
- 2021-01-20 US US17/153,263 patent/US11263821B1/en active Active
-
2022
- 2022-01-07 EP EP22702080.7A patent/EP4252121A1/en active Pending
- 2022-01-07 WO PCT/US2022/011626 patent/WO2022159283A1/en active Application Filing
- 2022-01-07 JP JP2023543428A patent/JP2024502495A/en active Pending
- 2022-01-07 KR KR1020237026767A patent/KR20230124091A/en not_active Application Discontinuation
- 2022-01-07 CN CN202280010102.4A patent/CN116783589A/en active Pending
- 2022-02-14 US US17/670,576 patent/US11670061B2/en active Active
-
2023
- 2023-04-25 US US18/306,454 patent/US20230260229A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104067274A (en) * | 2012-01-19 | 2014-09-24 | 谷歌公司 | System and method for improving access to search results |
US20200183969A1 (en) * | 2016-08-10 | 2020-06-11 | Zeekit Online Shopping Ltd. | Method, System, and Device of Virtual Dressing Utilizing Image Processing, Machine Learning, and Computer Vision |
CN110766780A (en) * | 2019-11-06 | 2020-02-07 | 北京无限光场科技有限公司 | Method and device for rendering room image, electronic equipment and computer readable medium |
Also Published As
Publication number | Publication date |
---|---|
KR20230124091A (en) | 2023-08-24 |
US11263821B1 (en) | 2022-03-01 |
US20230260229A1 (en) | 2023-08-17 |
JP2024502495A (en) | 2024-01-19 |
EP4252121A1 (en) | 2023-10-04 |
US20220230401A1 (en) | 2022-07-21 |
WO2022159283A1 (en) | 2022-07-28 |
US11670061B2 (en) | 2023-06-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11670061B2 (en) | Generating augmented reality prerenderings using template images | |
US10679061B2 (en) | Tagging virtualized content | |
CN107680165B (en) | HoloLens-based computer console holographic display and natural interaction application method | |
US10475103B2 (en) | Method, medium, and system for product recommendations based on augmented reality viewpoints | |
US8989440B2 (en) | System and method of room decoration for use with a mobile device | |
JP2022508674A (en) | Systems and methods for 3D scene expansion and reconstruction | |
WO2013120851A1 (en) | Method for sharing emotions through the creation of three-dimensional avatars and their interaction through a cloud-based platform | |
US11520949B2 (en) | Digital design of an area | |
US10957108B2 (en) | Augmented reality image retrieval systems and methods | |
US20230092068A1 (en) | Computing Platform for Facilitating Augmented Reality Experiences with Third Party Assets | |
US20230090253A1 (en) | Systems and methods for authoring and managing extended reality (xr) avatars | |
US11869136B1 (en) | User-context aware rendering dataset selection | |
JP2024061750A (en) | Computing platform for facilitating augmented reality experiences with third party assets | |
CN117876579A (en) | Platform for enabling multiple users to generate and use neural radiation field models | |
Liu | [Retracted] Ming-Style Furniture Display Design Based on Immersive 5G Virtual Reality | |
CN117934690A (en) | Household soft management method, device, equipment and storage medium | |
Sorokin et al. | Deep learning in tasks of interior objects recognition and 3D reconstruction | |
TW202236217A (en) | Deep relightable appearance models for animatable face avatars | |
Matsubara et al. | PLS-based approach for Kansei analysis | |
Joseph | AUGMENTED REALITY IN E-COMMERCE | |
Tang | Research on the Concept and Development of Contemporary Animation Design Based on Big Data Technology | |
Nikitha et al. | INTERIOR DESIGN USING AUGMENTED REALITY | |
Xie et al. | Research on the application of intelligent virtual simulation technology in the display of art works | |
José et al. | Graphically Speaking |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |