CN115271093A - Neural network based multimodal transformer for multitasking user interface modeling - Google Patents
Neural network based multimodal transformer for multitasking user interface modeling Download PDFInfo
- Publication number
- CN115271093A CN115271093A CN202210834687.3A CN202210834687A CN115271093A CN 115271093 A CN115271093 A CN 115271093A CN 202210834687 A CN202210834687 A CN 202210834687A CN 115271093 A CN115271093 A CN 115271093A
- Authority
- CN
- China
- Prior art keywords
- task
- computing device
- neural network
- user interface
- computer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
- G06N3/0455—Auto-encoder networks; Encoder-decoder networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/60—Type of objects
- G06V20/62—Text, e.g. of license plates, overlay texts or captions on TV images
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
- G06N5/041—Abduction
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V2201/00—Indexing scheme relating to image or video recognition or understanding
- G06V2201/02—Recognising information on displays, dials, clocks
Abstract
A method includes receiving, via a computing device, a screenshot of a display provided by a graphical user interface of the computing device. The method also includes generating, by an image-structure transformer of the neural network, a representation by fusing the first embedding based on the screen capture and the second embedding based on the layout of the virtual objects in the screen capture. The method also includes predicting, by the neural network and based on the generated representation, a modeled task output associated with the graphical user interface. The method also includes providing, by the computing device, the predicted modeling task output.
Description
Cross reference to related applications
This application claims priority from U.S. provisional patent application No. 63/221,677, filed on 7/14/2021, the entire contents of which are incorporated herein by reference.
Background
The neural network may be trained to predict aspects of the modeling task related to the graphical user interface, such as content, functionality, layout, and so forth. Modern graphical user interfaces open a rich problem space for modeling where the input is inherently multimodal and consists of several different types of data. Based on the graphical user interface, a large number of modeling tasks can directly enhance the end user experience and promote the development of the intelligent user interface.
Disclosure of Invention
In one aspect, a computer-implemented method is provided. The method includes receiving, via a computing device, a screenshot of a display provided by a graphical user interface of the computing device. The method also includes generating, by an image-structure transformer of the neural network, a representation by fusing the first embedding based on the screen capture and the second embedding based on the layout of the virtual objects in the screen capture. The method also includes predicting, by the neural network and based on the generated representation, a modeled task output associated with the graphical user interface. The method also includes providing, by the computing device, the predicted modeling task output.
In another aspect, a computing device is provided. The computing device includes one or more processors and data storage. The data storage has stored thereon computer-executable instructions that, when executed by one or more processors, cause the computing device to perform functions. These functions include: receiving, via a computing device, a screenshot of a display provided by a graphical user interface of the computing device; generating, by an image-structure transformer of a neural network, a representation by fusing a first embedding based on a screen capture and a second embedding based on a layout of virtual objects in the screen capture; predicting, by the neural network and based on the generated representation, a modeled task output associated with the graphical user interface; and providing, by the computing device, the predicted modeling task output.
In another aspect, a computer program is provided. The computer program includes instructions which, when executed by a computer, cause the computer to perform functions. These functions include: receiving, via a computing device, a screenshot of a display provided by a graphical user interface of the computing device; generating, by an image-structure transformer of a neural network, a representation by fusing a first embedding based on a screen capture and a second embedding based on a layout of virtual objects in the screen capture; predicting, by the neural network and based on the generated representation, a modeled task output associated with the graphical user interface; and providing, by the computing device, the predicted modeling task output.
In another aspect, an article is provided. The article of manufacture includes one or more computer-readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to perform functions. These functions include: receiving, via a computing device, a screenshot of a display provided by a graphical user interface of the computing device; generating, by an image-structure transformer of a neural network, a representation by fusing a first embedding based on a screen capture and a second embedding based on a layout of virtual objects in the screen capture; predicting, by the neural network and based on the generated representation, a modeling task output associated with the graphical user interface; and providing, by the computing device, the predicted modeling task output.
In another aspect, a computing device is provided. The computing device includes means for receiving, via the computing device, a screenshot of a display provided by a graphical user interface of the computing device; means for generating, by an image-structure transformer of a neural network, a representation by fusing a first embedding based on a screen capture and a second embedding based on a layout of virtual objects in the screen capture; means for predicting, by the neural network and based on the generated representation, a modeling task output associated with the graphical user interface; and means for providing, by the computing device, the predicted modeling task output.
The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects, embodiments, and features described above, further aspects, embodiments, and features will become apparent by reference to the drawings and the following detailed description and drawings.
Drawings
Fig. 1 is a diagram illustrating an example neural network, according to an example embodiment.
FIG. 2A illustrates example predicted results for a UI object detection task, according to an example embodiment.
FIG. 2B illustrates additional example prediction results for a UI object detection task, according to an example embodiment.
FIG. 3 shows an example of a language command detection task, according to an example embodiment.
FIG. 4 shows an example of a screen summary task, according to an example embodiment.
FIG. 5 illustrates an example of a widget-plus-text task, according to an example embodiment.
FIG. 6 illustrates an example of a flickability prediction task according to an example embodiment.
Fig. 7 is a diagram illustrating the training and reasoning phases of a machine learning model according to an example embodiment.
FIG. 8 depicts a distributed computing architecture in accordance with an example embodiment.
Fig. 9 is a block diagram of a computing device, according to an example embodiment.
Fig. 10 depicts a network of computing clusters arranged as a cloud-based server system, according to an example embodiment.
Fig. 11 is a flow chart of a method according to an example embodiment.
Detailed Description
The present application relates to a transformer architecture based neural network that can accept multi-modal input and can accomplish multiple modeling tasks for a graphical user interface simultaneously. Tasks may include, for example, UI object detection, natural language command landing (grouping), widget-plus-text (captioning), screen summarization (rendering), and UI flickability (tap) prediction. The model may be configured to process three types of data: images, structures (e.g., view hierarchies), and natural language.
The flexible architecture of the transducer makes it a "swiss saber" that addresses various problems. In addition to success in dealing with various areas of handling homogeneous inputs or outputs (e.g., natural language and vision), transformer architectures have recently shown promising results in dealing with problems involving multi-modal inputs, multi-tasking outputs, or both.
Described herein are tasks that model graphical user interfaces, which are important media to support almost every aspect of daily human activity. Modern graphical user interfaces open a rich problem space for modeling where the input is inherently multimodal, consisting of several different types of data. User interface screens exist in both visual form (i.e., screen shots) and structural representation (i.e., tree view hierarchy). Based on graphical user interfaces, there are a number of modeling tasks that will directly enhance the end user experience and advance the development of intelligent user interfaces. For example, existing approaches develop models and data sets for landing language commands on executable UI actions, generating language descriptions for accessibility on mobile devices, and understanding the usability of user interfaces, or identifying on-screen objects. Previous work also began learning efficient representations of user interface screens, which could potentially be beneficial for downstream tasks.
A universal user interface converter (VUT) is described that can handle three types of data: images, structure (view hierarchy), and language, and can perform a variety of different tasks, such as UI object detection, natural language command landing, widget-plus-text, screen summary, and UI flickability prediction.
The VUT can perform different tasks simultaneously. Often, using different models for different tasks results in a large amount of computing resources, including memory resources, processing resources, and/or power resources. This is particularly challenging when tasks have to be performed on a mobile device, such as a mobile phone. Thus, using one model to perform all the different tasks can greatly reduce the amount of computing resources required.
VUT is a multimodal model for multitasking modeling of graphical user interfaces, completing a wide range of tasks through one model to enhance the mobile user experience.
VUT can be based on a two-tower converter architecture, one for image structure and the other for language, where each converter is used to encode and decode its own modality, with cross-tower attention (attention).
The image structure converter can be used as both encoder and decoder. The VUT's image structure converter can perform early fusion across modes. Instead of operating across language and image regions, the VUT's image structure converter operates on the entire screenshot image and view level hierarchy. This can improve the efficiency and accuracy of task execution. The image structure converter of VUT is used not only for representation learning, but also for object detection when no view level information is present in the input, e.g. for object detection tasks.
The image structure converter of VUT is a single-tower architecture, where both the image and object queries are input to the same converter, i.e. early fusion, rather than the encoder-decoder architecture used in the traditional model.
VUT's question-answer converter is designed based on an auto-regressive architecture, where questions or commands are input as prefixes into the model and responses are decoded tag-by-tag (token).
For language (command) floor tasks, rather than generating a language response as in existing models, the model's final hidden state is used to retrieve on-screen UI objects to complete the command.
Using multiple different heads (heads) based on the same neural network layer increases efficiency and accuracy, and also enables efficient individual and/or joint training of one or more tasks.
Fig. 1 is a diagram illustrating an example neural network 100, according to an example embodiment. The graphical user interface contains a collection of UI elements for completing a coherent set of tasks. There may be various types of data involved in formulating a UI task: < S, V, T, Q, A >. S is a screenshot image 102 describing the visual appearance of the UI screen 102 a. V is a view hierarchy 104 tree representing the underlying structure of the UI screen 102. T is a target object 106 (UI element) to be operated on or queried in the view hierarchy 104. Q is a natural language description of the task, which may be something like "what is the text (caption) of an element? "an open question 130, such as" does the object appear clickable? "yes or no question, or a command such as" click next button ", etc. When the response form of the task is natural language, answer a 134 is a natural language answer to question Q130.
In some embodiments, the method includes receiving, via a computing device, a screenshot 102 of a display provided by a graphical user interface of the computing device.
The method further includes generating, by the image-structure transformer 108 of the neural network 100, the representation 120 by fusing the first embedding based on the screen capture and the second embedding based on the layout of the virtual object in the screen capture 102. The image structure model 108 encodes the entire screenshot 102 of the UI and its view hierarchy tree 104, with the two modalities fused early.
In general, the image-to-structure converter 108 is a bimodal model that takes an image, such as the screen shot 102, and the corresponding view level 104, and outputs a hidden representation 120 of each node in the image 102 and view level 104. For the image modality, the content of the screenshot 102 is embedded 114CSMay be computed by image embedder 110 and content embedding 114 of view level 104 may be computed by structure embedder 112. Such embedding may be associated with mode and position encoding 116PSCombined and input into the image-structure transformer 108. The transformer 118 generally refers to a transformer layer of the neural network 100, wherein the transformer layer includes the image-structure transformer 108 and the question-answer transformer 126.
The screen shots may be randomly sized for image enhancement and thus may be of different sizes. In some embodiments, a binary non-fill mask S of screenshot S102 may be usedmask. Further, tensor shaping and/or broadcasting may be applied, for example. Content embedding 114CSCan be determined as:
CS＝Dense(ResNet(S))；PS＝PE(Smask) (equation 1)
Wherein the content of the first and second substances,
In some embodiments, the method includes predicting, by the neural network 100, a layout of the virtual object in the screenshot 102. When there is no view level 104 in the input, the image-structure model 108 predicts the UI object. For view-level modalities, when view level 104 is not present, as in object detection task 124, the content of the view-level modality is embedded 114vIs set to zero. In some embodiments, the position encoding 116P of the view level modalityvMay be a learned embedded vector for each query location. When a view level 104 is present in the input, each object in the view level tree, view level 104, is embedded within the context of the entire structure.
A User Interface (UI) object may include a set of properties including type, whether clickable, location of bounding box, order in Document (DOM) location, text content, and whether the object is a target. Attributes may be embedded separately into the same dimension and then combined by addition to form an embedding for each element.
These attributes can be embedded separately into the same dimension and then combined by addition to form each element E0The embedding of (2). Note that for the command landing task 138, information whether the object is a target may not be present in the input, since T will be predicted by the model. C for calculating nodes in VvAnd PvThe method may be similar to the prior art. Whether or not V is present in the input, the embedding of view level modalities may be two tensors:
the method then includes determining, by the neural network 100, content embedding 114 and location embedding 116 for each of the screen capture 102 and the view hierarchy 104. Generating the representation 120 through fusion may include generating the representation 120 based on content embedding 114 and location embedding 116. Since the embedding from both modalities (image modality and view level modality) will jointly participate in the self-attention of the transducer encoder, it may be advantageous to have their position encoding global rather than local to each modality. To this end, a learnable modality embedding may be added to each of these modality-specific location codes. The embeddings from the two modalities may be connected along a first dimension (conjugate) to form an input to the image-to-structure transformer 108 to output the representation 120.
For example, PsAnd PvMay be a position code within each modality. In general, the embedding from both modalities can jointly participate in the self-attention of the transducer encoder. In some embodiments, the positional embedding 116 of the screenshot 102 and the positional embedding 116 of the view level 104 may be global embedding corresponding to the entire screenshot 102. For example, the position encoding may be global, rather than local to each modality. To this end, a learnable modality embedding may be added to each of these modality-specific position codes, as follows:
P′s＝Ps+Es；P′v＝Pv+Ev(equation 2)
Wherein the content of the first and second substances,
C＝Concat1st_dim[Cs,Cv]；P＝Concat1st_dim[P′s,P′v](equation 3)
Wherein, the first and the second end of the pipe are connected with each other,
H＝TransformerEncoder(C, P) (Eq. 4)
Hidden representation
Hs＝H[:M]；Hv＝H[M:](equation 5)
the image-structure model 108 performs an object detection task 124. For example, the object output layer 122 may be configured to output a UI object detection task 124 based on the representation 120. In some embodiments, the method includes receiving, via a computing device, a view hierarchy 104 indicating a layout of virtual objects in a screenshot 102.
The method also includes predicting, by the neural network 100 and based on the generated representation 120, a modeled task output associated with the graphical user interface. The term "modeling task output" as used herein generally refers to any task associated with a graphical user interface. For example, the graphical user interface may include virtual objects, lists, images, videos, icons, user-selectable icons, user-adjustable controls (e.g., sliders to adjust image characteristics, sound characteristics, etc.), application interfaces, application icons, and input fields for text, images, speech, and so forth. The graphical user interface may also include various relationships between these objects, tasks that may be performed, hierarchies, and display characteristics including color, hue, resolution, and the like. Thus, the modeling task output may be the output of any task that identifies various elements, attributes, functions, designs, layouts, etc. for a graphical user interface. Such tasks may include the output of natural language command-to-land tasks, widget-to-text tasks, screen summarization tasks, object detection tasks, flickability prediction tasks, and the like.
In some embodiments, the modeling of the graphical user interface includes multi-tasking modeling, and wherein the neural network 100 includes dual transformers, wherein the image-structure transformer 108 predicts the modeling task output of the image-structure task and the question-answer transformer 126 predicts the modeling task output of the natural language task. The question-answer model 126 encodes the question and uses the encoding from the image-structure model 108 to predict the answer. The image-to-structure converter 108 and the question-to-answer converter 126 are configured with a cross tower (cross tower) attention (e.g., encoder-decoder attention 146).
In some embodiments, the question-to-answer transformer 126 may be a language model that encodes the question Q130 and decodes the answer a 134. The process may begin at "start" 128 and end at "EOS"132. The input to the model may be X = X1:tWhere t represents the length of the sequence. The input may be a sequence of tokens for the question Q130 of the command-to-ground task 138, or when the answer a 134 is a linguistic answer to be generated, a concatenation of Q130 with the decoded answer a'. In some embodiments, during training with teaching, a = a' is forced. In the autoregressive inference process, A' is the predicted marker sequence dependent on step size:
g1:t＝TransformerDecoder(E(x1:t),PE(1:t)；Hs,Hv(equation 6)
Wherein x isiIs the ith marker (1 ≦ i ≦ t) in the sequence, E () and PE (), calculating the content embedding and position encoding of each marker in the sequence. HsAnd HvAccessible through the encoder-decoder attention 146 in the transformer decoder. The sequence of the hidden states is such that,command floor task 138.
In some embodiments, the modeling task output is for one or more of the following tasks: an object detection task 124, a natural language command landing task 138, a widget-to-text task 140, a screen summarization task 142, or a flick prediction task 144. When the task output is language, the question-answer model 126 directly implements tasks, such as a widget-plus-text task 140, a screen summary task 142, and a flickability prediction task 144. For the command-to-ground task 138, the representation 118 of the image-to-structure converter 108 may instead be used to locate the UI element to act upon.
In some embodiments, neural network 100 includes an object detection head, a text head, and a pointer head. These headers are based on the hidden representation 120. For example, an object detection head such as object output layer 122 may be used for UI object detection task 124, where HvMay be used as an input layer. Then, the user can use the device to perform the operation,
wherein the content of the first and second substances,
The text header may be used for tasks with text response components, e.g., answer A134, softmax layer may be applied to decoder hidden state g1:tTop of (as determined in equation 6) to generate each answer token:
ai＝argmax(Softmax(g|Q|+i-1Wtxt) (equation 8)
Wherein, ajIs the ith label in the answer sequence a and | Q | is the length of the question. In addition to this, the present invention is,
An indicator head or floor indicator 136 may be used to command the floor task 138, and the last hidden state from the transformer decoder may be used as an "indicator" to match all objects in the view level 104 based on their hidden representations 120 using the following dot product similarity:
wherein h isjIs HvLine j in (1), HvIs a hidden representation of the jth object in the view level 104. The term
In some embodiments, the modeling task output is for an object detection task 124, and the method includes detecting, by the neural network 100, one or more types of container objects indicative of a layout hierarchy of the screenshot 102. Given the screen shot image S102, the object detection task 124 is to detect each UI element on the screen. In some embodiments, the modeling task output is for an object detection task 124, and the method includes detecting one or more of a text field, a toggle button, or an image view through the neural network 100. This task is challenging because it requires the detection of different types of container objects that determine the layout level of the screen. In some embodiments, the layout hierarchy includes one of a linear layout, a frame layout, or a list. When the view hierarchy 104 is not available, detection of these objects is an important step towards providing accessibility features or reconstructing or adapting the UI. As a screen understanding task, this task may be beneficial for improving other UI modeling tasks. The task is expressed as:
v = F (S) (Eq.10)
In some embodiments, the object detection task 124 may be implemented based on a single tower image-to-structure converter and does not rely on a question-answer model.
In some embodiments, the modeling task output is for a widget-plus-text task 140, and the method includes predicting, by the neural network 100 and for the screen shot 102, a natural language description of a function of a predicted virtual object in the graphical user interface. In general, generating natural language descriptions for user interface elements is an important task for accessibility and language-based interactions. In some embodiments, given the UI view hierarchy 104, the screenshot image S102, and the target element T106, the model predicts a natural language phrase a 134 describing the functionality of the object. This relationship can be described as:
a = F (S, V, T, Q) (equation 11)
The model uses the information of S102 and view level 104 through the image-structure model 108. An example of a question Q130 in the widget-plus-text task 140 may include "what is the text of the element? "or" what most describes the object? ". Examples of answer a 134 may include "forward" or "shopping cart". The widget-plus-text task 140 extends the technology related to the classic image-plus-text task to the UI field.
The method also includes providing, via the computing device, the predicted modeling task output.
In some embodiments, the modeling task output is for a widget-plus-text task 140, and the method includes predicting, by the neural network 100 and for the screen shot 102, a natural language description of a function of a predicted virtual object in the graphical user interface.
In some embodiments, the modeling task output is for a screen summarization task 142, and the method includes predicting a summary of the screenshot 102 of the graphical user interface screen summarization task 142 over the neural network 100, which is considered to be the task of generating a summary describing the entire screen, as determined by equation 3 below, rather than focusing on a single element as a widget plus text task 140.
A = F (S, V, Q) (equation 12)
Some examples of question Q130 for screen summarization task 142 are "what is the description of the screen? "or" what best summarizes the UI? ". This task is broadly related to the multi-modal summarization task in existing approaches, but is specific to the user interface domain.
A useful feature of modern smartphone interfaces is the interpretation of a user's natural language commands into executable actions, such as voice control. In the language command floor task 138, given the UIs, S and V, and the language command Q, the model needs to predict which object on the screen can complete the language command. This can be determined as:
t = F (S, V, Q) (equation 13)
Accordingly, the method also includes associating the target virtual object with the natural language command. The method also includes providing the natural language command through a graphical user interface. Note that this task locates target object T106 on the screen, rather than generating a natural language response as with widget-plus-text task 140 and screen summarization task 142. The likelihood of Q130 may be unlimited, it may be any phrase that the user enters in order to manipulate the UI. Some example questions Q130 may be "go to next screen" or "tap the checkout button". The command may also indirectly refer to an object, such as "click on the icon to the right of the search box".
In some embodiments, providing the natural language command includes displaying the natural language command at or near the target virtual object. In some embodiments, providing the natural language command includes providing the natural language command as a voice command in response to a user interaction with the target virtual object. An important feature of modern smartphone interfaces is the interpretation of a user's natural language commands into executable actions, such as voice control.
In some embodiments, the modeled task output is for the flickeability prediction task 144, and the method involves: for a graphical user interface, a mismatch between a developer-designed flickable feature and a user-perceived flickable feature is identified. Whether a user perceives a UI object as clickable can be an important usability issue. Mismatches between the user perceived flickability and flickability desired by the designer or developer can adversely affect the mobile user experience. In the flickability prediction task 144, given the UI S102 and the view hierarchy 104, the target T106 of the expanded query, and the ask question Q130, the model provides a yes or no answer a 134. This can be determined as:
a = F (S, V, T, Q) (equation 14)
The method also involves providing a suggestion to a developer of the graphical user interface to compensate (offset) the identified mismatch. In general, the tasks share the image-structure transformer 108. In addition to the UI object detection task 124, other tasks also share a question-answer transformer 126. As a result, the natural language input Q is a task indicator for such tasks. Q also carries the actual task details for the floor task in order to find the object on the UI.
For a flickability prediction task, a synthetic yes or no question may be generated based on the following regular expression pattern. The model is trained to decode "yes" or "no" as the answer to the question: "[ object | element | widget | control ] [ clickable | flickable ]? ". In some embodiments, an example of a problem generated based on a regular expression is, for example, "is object clickable? "," is the widget clickable? "," is the element clickable? ", and the like.
For the widget-plus-text task 140 and the screen summary task 142, the model will need to generate open answers. In some embodiments, the following regular expressions may be used to generate questions for these tasks. The VUT may be trained to decode screen summaries or widget scripts that follow the following problems: what is [ summary | description ] of "[ screen | UI ]? "or" what is the [ text | description ] of the [ object | element | widget | ]? ". Some of the problems with regular expression based generation may be: "what is the summary of the screen? ", what is the description of the UI? "," what is the text of the widget? "," what is the description of the object? ", and the like.
For the language command landing task 138, commands relating to a particular object in the screen may be fed to the model, through which the model is trained to locate the object in question. Example commands may be generated by a human annotator for target UI objects displayed on a screen. For example, a human annotator may be required to put different commands for each highlighted target object. Commands such as "click on notification bar above status option", "press back arrow button", "select icon above clock option", "slide notification bar down" may be generated by a human annotator.
The method also involves training the neural network 100 to receive input screenshots displayed by the particular graphical user interface and predicting modeling task output associated with modeling of the particular graphical user interface. For the UI object detection task, RICO, a common corpus of mobile user interfaces containing 64462 unique Android screens from 9362 different applications, can be used for training. Each screen includes RGB screenshots and a corresponding view hierarchy. The view level is a tree structure with 21 nodes of unique type that can be merged from the Android view class properties in the original dataset. The nodes in the tree correspond to UI elements on the screen or container elements that manage the layout of their children nodes. In some embodiments, the view level may have a maximum of 128 nodes in the dataset. For example, the data may be divided into a training set (54611), a validation set (2518), and a test set (2627). Additional and/or alternative data sets are possible, with various distributions for the training set, validation set, and test set.
For widget-to-text tasks, a common data set may be used. The published data set includes more than 200000 manual annotations for more than 46000 unique UI objects from 17000 RICO screens. Annotated UI elements may be split for training (39951), verification (3436), and testing (3531). In some embodiments, a data set may be split by application such that screens of the same application may only appear in one split.
A screen summary dataset for 22301 unique Android screens was collected. Based on the displayed UI screen, the human worker is required to generate 3-5 summaries for the screen. The maximum length of the summary is 10 words. In some embodiments, the data set may be divided into a training set (17569 screens), a validation set (2298), and a test set (2434).
The tappable prediction dataset comprises tappable annotations of more than 20000 UI elements from 3218 Android screens. In data collection, given a target UI element highlighted on the screen, the human raters are asked to answer "yes" or "no" as to whether the target object is clickable to them. In some embodiments, the data set may be divided into training (14783), validation (1854), and testing (2029).
The language landing dataset includes 10000 human annotations for manipulating UI objects from 1432 unique screens of 26 Android built-in applications (e.g. settings). The human evaluator generates a command such as "click a button under the battery information", and the maximum length of the command phrase is 20 words. In some embodiments, the data set may be divided into training (7822), validation (1024), and testing (987).
When separating each data set into a training set, a validation set, and a test set, it may be desirable to ensure that there is no application (or screen) overlap between the training sets and any test sets for different tasks. This may be important because in the multitask learning case, the VUT learns from all training sets. Thus, it is preferable that the union of applications and screens across all training sets does not overlap with any test set.
In some embodiments, the training may be performed at a computing device.
In some embodiments, the predictive modeling task output involves: obtaining, at a computing device, a trained neural network; and applying the obtained trained neural network to model a prediction of task output.
In some embodiments, the predictive modeling task output involves: a request for a predictive modeling task output is determined by a computing device. The method also involves: a request for a predictive modeling task output is sent from a computing device to a second computing device that includes a training version of a neural network. After sending the request, the method involves the computing device receiving a predicted modeling task output from the second computing device.
Some example model parameters are provided for illustrative purposes and are not to be construed as limiting the scope of the claims. For UI object detection tasks, the VUT may be configured with a 12-layer transformer encoder as an image-structure model that reaches 48000000 trainable parameters, which is slightly less than 50000000 trainable parameters of a DETR with a 6-layer encoder and a 6-layer decoder. For the remaining tasks, the VUT can be configured with a 6-layer transformer encoder for the image-structure model and a 6-layer transformer decoder for the question-answer model. When all tasks are jointly trained, 64000000 parameters exist. Head (head) and word block embedding and projection for a particular task are major factors in parameter size growth. When only a subset of these tasks are involved in training, e.g., widget plus text and object detection, there may be fewer trainable parameters involved because only a portion of the complete model is used. All VUT variants use the following configuration: # Attention _ Heads =8 (# Attention _ header = 8), hidden _ Dimension =2568 (Hidden _ size = 2568), transformer _ MLP _ Dimension =20488 (Transformer _ MLP _ size = 20488), and Transformer _ QKV _ Dimension =256 (Transformer _ QKV _ size = 256).
All tasks except UI object detection require a model to encode the view level. To this end, each object in the view hierarchy is represented as a content-embedded Cv and a position code Pv. Content embedding embeds attributes of objects such as type, textual content, and clickable attributes. For textual content, it can treat all word piece tokens (word piece tokens) owned by an object as a "package of words". Each marker may be assigned a learnable embedding, and then maximum pooling may be performed on the set of embeddings to obtain a fixed-length embedding vector to represent the textual content of the object. The embedding of each content attribute may be added to form a content embedding of the object.
Because the flattened view hierarchy is fed to transformer 118, the desired position encoding is configured to capture both the spatial and structural positions of the object. The spatial location includes four coordinate values of the bounding box of the object, i.e., [ up, left, down, right ], and the structural location includes three DOM position attributes, including the index position of the object in the preceding and following traversals of the hierarchy, and the depth of the object in the hierarchy. Each type of position may be encoded using a sinusoidal representation. Note that in the image-structure model 108, a position code 116 is added to the input of each layer of the transformer 118. This is in contrast to the question-answer model 126, where the position code for each marker is only added to the input of the first layer in the question-answer model 126. The learned embedding for position coding is used in the question-answer model 126. During training, a 10% discard (dropout) may be used for attention and MLP loss in the question-answer transformer 126, and a 20% discard may be applied to the encoding from the image-structure model 108 before the cross attention 146. During 5-task joint learning, the attention and MLP discard rate may be 20% for the image-structure transformer. During the autoregressive decoding of the interference, the maximum decoding length may be 30, which covers the total length of the question and answer.
Phrases may be tagged into sequences of word blocks in a manner similar to that used in BERT, which results in a vocabulary of 28536. The maximum size of the screenshot image may be 1080 × 1080. Each image may be randomly resized for image magnification. The maximum number of UI objects and containers per screen may be limited to 128. The VUT can be implemented based on a library JAX2 for machine learning. In some embodiments, each VUT model may be trained with a batch size of 64 screens/example, which is parallelized on 64 TPU v3 cores.
Example applications
In some embodiments, user interface modeling is described. In one example implementation, a cloud-based developer tool may be provided. For example, a developer may be provided with a platform for designing and/or improving a GUI. As described herein, the neural network 100 may output predicted tasks and may provide interactive developer tools for developers.
For example, the flickability of an on-screen object may be identified and a mismatch between a flickability feature designed by a developer and a flickability feature perceived by a user may be determined. The cloud-based developer tool may then provide such information to the developer to enable the improvement of the flickable feature. Further, for example, in some embodiments, such a cloud-based developer tool may be a substantially real-time developer tool that models the GUI, predicts the modeling task output, and provides recommendations in substantially real-time.
In another example implementation, the neural network 100 may predict modeled task outputs that may be used to improve the user experience of an end user of the mobile device. For example, landing a language command into an executable UI action is described. This may enable an improved user experience.
Further, for example, generating a language description for accessibility on a mobile device is described. As another example, a summary of the GUI may be performed and provided to the user. These features may also enable an enhanced user experience, particularly utilizing text and/or voice commands to assist a user in navigating the GUI and/or multiple screens of the GUI.
In some embodiments, understanding the usability of a user interface is described, along with identifying on-screen objects.
Additional and/or alternative applications are possible. For example, one or more features may be made available to a developer to assist in developing tasks for a user platform via a GUI. For example, widget-literals may enable a developer to use the output of a widget-literals task, rather than requiring manual annotation of the widget.
Further, for example, the object detection task may provide the developer with a layout hierarchy of objects in the GUI, an index of objects, their functionality, and so forth. Such a predictive output of the neural network 100 can significantly reduce the time and resources allocated to the development task to be performed by the developer, and also enhance the accuracy of the development task. This may be an important application for the development of mobile platforms, such as Android-based systems, for example.
These tasks may be performed by a single neural network model that may be jointly trained to perform all tasks, jointly trained to perform a particular subset of all tasks, and/or independently trained to perform tasks. Such selection may depend on the platform (e.g., mobile or cloud-based platform), the available resource allocation for processors, memory, power, network bandwidth, etc., and may depend on the target audience (e.g., end user, developer, etc.).
One or more such example features may be provided via a cloud-based platform, as an interactive feature, as a platform as a service (PaaS) platform, a software as a service (SaaS) platform, and/or a machine learning as a service (MLaaS) platform. As described herein, an application may enhance the user experience of a user of a mobile phone, provide ease-of-use features to an end user of a GUI, assist a developer in designing an application based on an operating system of a mobile platform, such as a mobile device, assist a developer in addressing various aspects of a GUI, and so forth.
Fig. 2A and 2B illustrate example predicted results of a UI object detection task according to example embodiments. An example of the prediction and benchmark truth for each task on test data is shown, as achieved by a single model of the VUT as it learns all tasks jointly. Referring to fig. 2A, image 205 shows a baseline portrait image for a user interface for a search function, and image 210 shows a predictive image for the same user interface. Referring to fig. 2B, image 215 shows a baseline portrait image of a user interface for network landing page functionality, and image 220 shows a predicted image for the same user interface.
FIG. 3 shows an example of a language command detection task, according to an example embodiment. In each screenshot, the objects located by the model are highlighted with a bounding box having dashed borders. For example, in the image 305, a search page having a search box and a voice icon is displayed. A bounding box 305a surrounding the phonetic icon is highlighted and the command detection task may predict the command "tap the phonetic icon". As another example, in image 310, a page with a list of applications is displayed. A bounding box 310a (appearing below the calculator icon) around the calendar icon is highlighted, and the command detection task may predict that the command "press the icon below the calculator icon". Further, for example, in the image 315, a home page with weather notification is displayed. The bounding box 315a around the temperature display is highlighted and the command detection task may predict the command "select weather text under notification bar".
FIG. 4 shows an example of a screen summary task, according to an example embodiment. The references (baseline truth summaries) created by the human annotators for each screen are displayed together with the predictions from the neural network. For example, in image 405, a location-based search page is displayed. The human annotator may have created a baseline truth summary, such as "page showing search box in application," while the screen summary task may predict that the screen provides "search bar to search location. As another example, in image 410, a page from a media playback application is displayed. The human annotator may have created a baseline truth summary such as "page shows music playing on the application" and the screen summarization task may predict that the screen provides "a page showing a music track in a music application". Further, for example, in the image 415, a page for account setup is displayed. The human annotator may have created a baseline truth summary such as "pop-up display to set up account details" while the screen summary task may predict that the screen provides "pop-up display to create an account".
FIG. 5 illustrates an example of a widget-plus-text task, according to an example embodiment. The target element is highlighted by a bounding box with dashed borders. One of three references (baseline proper glyphs) created by a human annotator for each target element is displayed. For example, in image 505, a page for translating words and/or phrases from one language to another language is displayed, with a text entry portion 505a, a French word list in portion 505b, and a "copy" widget having a bounding box 505 c. The baseline true photo text may be a "copy to clipboard option," and the widget lettering task may display a bounding box 505c around the "copy" widget and predict that the widget text is "copy text. As another example, in image 510, a page is displayed with a emoji application displaying a plurality of emoji (emoji). The baseline true pictograph may be "select emoji 8" and the widget-to-text task may display a bounding box 510c around the "emoji 8" widget and predict the widget text as "select emoji". Further, for example, in the image 515, a page having a user profile is displayed. The reference true word may be "enter and confirm password", and the widget-plus-word task may display a bounding box 515a around the "confirm password" input field widget and predict the widget word as "enter password".
FIG. 6 shows an example of a flickability prediction task according to an example embodiment. The element of interest is highlighted with a bounding box with dashed borders. For example, in image 605, the flickability prediction task may be a flickability of an icon that predicts an image of the user. The reference truth may indicate that the icon is flickable, and the model may predict that the icon is flickable and place a bounding box 605a around the icon. As another example, in image 610, the flickability prediction task may be to predict flickability of an icon with the text "more about your information". The reference true may indicate that the icon is not flickable, and the model may predict that the icon is not flickable and place a bounding box 610a around the icon. Further, for example, in image 615, the flickeability prediction task may be predicting flickeability of a download icon for "widget 3". The reference true may indicate that the icon is flickable, and the model may predict that the icon is flickable and place a bounding box 615a around the downloaded icon of "widget 3".
These and other example applications are contemplated within the scope of the present disclosure.
Results of the experiment
Comparison of VUT and DETR
The image-structure model 108 described herein shares some aspects with the existing transformer-based model (DETR) for end-to-end object detection architectures. Thus, the two models can be compared for the UI object detection task. In this experiment, the det can be configured to use a 6-layer transformer encoder and a 6-layer transformer decoder, and with a similar number of parameters in the model, the VUT image structure can be configured to use a 12-layer transformer encoder in this experiment. Deta (50000000 parameters) has slightly more parameters than VUT (48000000 parameters) due to the weights associated with the encoder-decoder attention 146. Experiments have shown that the image-structure model significantly outperforms the encoder-decoder architecture of the DETR. Indeed, the DETR experiment found that more coding layers can significantly improve accuracy. However, in current experiments, the image-structure model of VUT uses an encoder-only architecture and also achieves better accuracy. Experiments show that the multi-modal coding method provided by the invention can well complete the target detection task.
Single task training with VUT
To understand how well a VUT performs when jointly learning multiple tasks, a baseline can be established by training the VUT on each dataset separately. Each model may be trained until it converges. For the UI object detection task, the model can be trained using default settings for DETR, using a batch size of 64 for 300000 iterations. The learning rate plan includes steps from 1e at 200000 steps-4To 1e-5One learning rate of (1) decays. In thatIn this experiment, a 6-layer image-structure encoder was used in VUT, with 8-head attention and a hidden size of 256. The present model achieves AP =37.0, AP50=47.6 and AP75=38.8. Note that this accuracy is lower than previously reported results using centret (central net) on different UI datasets. However, these results cannot be directly compared. The task of the VUT model is more challenging because the VUT model is trained to detect 21 different UI object types that include several container elements, rather than the 12 objects in the previous work. Furthermore, previous work used data sets that were manually labeled by a person, and heavy post-processing was also employed to improve the predictions.
For the widget-to-text task, both a 6-layer image-structure model and a 6-layer question-answer model may be used in addition to the text header. Similarly, the model may be trained with a batch size of 64 until it converges, which may require 45000 steps. The VUT model achieves comparable accuracy to existing models, although the model architecture of VUT differs significantly from previous work. Table 1 below provides the results of the widget-plus-text task:
TABLE 1
For the screen summarization task, the same settings can be used as for training the model for widgets plus text, and the model converges in 50000 steps. The accuracy achieved by VUT is shown in table 2:
TABLE 2
The language command floor task uses a similar model setup as the widget plus text and screen summary task, except that it uses a floor header instead of a text header. The model may require approximately 27000 steps to converge at a batch size of 64 (see results in table 3 below). To train the model for each of these tasks, the learning rate may beFrom 1e once in steps of 15000-4Decay to 1e-5。
TABLE 3
To learn the flickable prediction task alone, the same model setup is used as for the two text-related tasks (summary and text). We have found that despite the use of a large rejection rate, the model is very susceptible to overfitting. Therefore, we train the model with batch size 64, stopping ahead. Although we used a very different model architecture here, the accuracy (table 4 below) is comparable to the previously published results.
TABLE 4
Joint training VUT for multiple tasks
The execution of the VUT on multiple tasks simultaneously can be evaluated. In this experiment, both the 6-layer image-structure transformer and the 6-layer question-answer transformer can be used with all task heads. Each task head and model component is only used when it is needed by a particular task. The entire model can be implemented based on Jax.
Because the UI object detection task requires more iterations than other tasks, multitask learning may be initiated by training the VUT for the UI object detection task and then jointly training the VUT for all tasks by alternating batches from each data set. This learning strategy is reasonable because by learning from the UI object task, the model can learn useful information about how to encode the screen pixels. As consistently shown in experiments, joint learning involving object detection can often facilitate learning of the other four tasks.
Based on such a multi-task learning strategy, the VUT model can first be trained for approximately 300k steps for UI object detection. The model can then be trained for approximately an additional 200k steps for all tasks that are learned together. In training, the model may alternate between 5 datasets and tasks, and one batch from one dataset may be used at a time. As shown in tables 1, 2, 3, and 4 above, multitask learning, while more challenging than single task learning, can generally be performed on an equal level as single task learning. For widget-plus-text, screen summarization, and flickeability prediction tasks, multitask learning seems to be consistently superior to single task learning. The accuracy of the grounding task 138 may be reduced when text generation related tasks are involved. This is consistent with the model architecture, as the floor task relies on the last hidden state of the question-answer model and may compete with the three text generation tasks by "pulling" the hidden representation of the question-answer model in different directions. However, it appears that having the object detection task in multitasking learning is often superior to configurations that do not involve object detection. For the object detection task itself, accuracy may be degraded when a batch change for multitask learning starts. However, this gradually restores its accuracy, especially after the learning rate decays. The accuracy of object detection was restored to AP =32.5, AP50=44.2 and AP75=33.7. Accuracy may be further improved by careful learning rate scheduling and adjustment.
Additional results of the experiment
As indicated previously, the VUT model may be trained first for the UI object detection task, which helps the model to gain understanding of the screenshot image and learn the representative pixels before it is further trained with other tasks. As more tasks participate in training, the accuracy of the model on the UI object detection task may be affected. TABLE 5
TABLE 5
As indicated in table 5, the accuracy of the VUT on the object detection task is largely preserved when adding the additional task of multi-task learning. When all tasks are added to the training, i.e. the last row in the table, there is a more significant drop in object detection accuracy. Fine-tuning the learning rate scheduling and dropping (dropout) rate for different parts of the model can potentially bring the accuracy to its original level. For example, at all 5 tasks*In (c), with smaller discards in the image structure transformer, no attention discard and 10% MLP discard rate, as it seems that larger discards would impair the UI object detection task. Also, in this experiment, the MLP and attention-drop rate can be increased to 20% in the question-answer transformer to avoid overfitting to other tasks. In this setting, the accuracy of UI object detection is better recovered and seems to have a minor impact on the model accuracy of other tasks. These experiments show that object detection is not considered as an independent pre-training task, but is feasible as part of multi-task learning, where the VUT accomplishes all tasks through a single model.
Training machine learning model for generating inferences/predictions
Fig. 7 shows a diagram 700 illustrating a training phase 702 and an inference phase 704 of a trained machine learning model(s) 732, according to an example embodiment. Some machine learning techniques involve training one or more machine learning algorithms on an input training data set to recognize patterns in the training data and provide output inferences and/or predictions about (the patterns in) the training data. The resulting trained machine learning algorithm may be referred to as a trained machine learning model. For example, fig. 7 illustrates a training phase 702 in which one or more machine learning algorithms 720 are being trained on training data 710 to become trained machine learning model(s) 732. Then, during the inference stage 704, the trained machine learning model(s) 732 may receive input data 730 and one or more inference/prediction requests 740 (possibly as part of the input data 730), and responsively provide one or more inferences and/or predictions 750 as output.
As such, the trained machine learning model(s) 732 may include one or more models of one or more machine learning algorithms 720. Machine learning algorithm(s) 720 may include, but are not limited to, artificial neural networks (e.g., convolutional neural networks, cyclic neural networks, bayesian networks, hidden markov models, markov decision processes, logistic regression functions, support vector machines, suitable statistical machine learning algorithms, and/or heuristic machine learning systems described herein). Machine learning algorithm(s) 720 may be supervised or unsupervised, and any suitable combination of online and offline learning may be implemented.
In some examples, the machine learning algorithm(s) 720 and/or the trained machine learning model(s) 732 may be accelerated using an on-device co-processor such as a Graphics Processing Unit (GPU), tensor Processing Unit (TPU), digital Signal Processor (DSP), and/or Application Specific Integrated Circuit (ASIC). Such on-device co-processors may be used to accelerate the machine learning algorithm(s) 720 and/or the trained machine learning model(s) 732. In some examples, the trained machine learning model(s) 732 may be trained, resident, and executed to provide inferences on a particular computing device, and/or may otherwise make inferences for a particular computing device.
During the training phase 702, the machine learning algorithm(s) 720 may be trained by providing at least the training data 710 as training input using unsupervised, supervised, semi-supervised and/or reinforcement learning techniques. Unsupervised learning involves providing part (or all) of the training data 710 to the machine learning algorithm(s) 720, and the machine learning algorithm(s) 720 determines one or more output inferences based on the provided part (or all) of the training data 710. Supervised learning involves providing a portion of training data 710 to machine learning algorithm(s) 720, where machine learning algorithm(s) 720 determine one or more output inferences based on the provided portion of training data 710 and accept or correct the output inference(s) based on correct results associated with training data 710. In some examples, supervised learning of machine learning algorithm(s) 720 may be governed by a set of rules and/or a set of labels for training inputs, and the set of rules and/or the set of labels may be used to correct the inference of machine learning algorithm(s) 720.
Semi-supervised learning involves having some, but not all, of the correct results of the training data 710. During semi-supervised learning, supervised learning is used for a portion of the training data 710 with correct results, while unsupervised learning is used for a portion of the training data 710 with incorrect results. Reinforcement learning involves machine learning algorithm(s) 720 receiving a reward signal, which may be a numerical value, with respect to a previous inference. During reinforcement learning, machine learning algorithm(s) 720 may output a reward signal that is inferred and received in response, where machine learning algorithm(s) 720 is configured to attempt to maximize the value of the reward signal. In some examples, reinforcement learning also utilizes a value function that provides a value representing an expected sum of values provided by the reward signal over time. In some examples, machine learning algorithm(s) 720 and/or trained machine learning model(s) 732 may be trained using other machine learning techniques, including but not limited to incremental learning and curriculum learning.
In some examples, machine learning algorithm(s) 720 and/or trained machine learning model(s) 732 can use a migration learning technique. For example, the migration learning technique may involve a trained machine learning model(s) 732 pre-trained on a set of data and additionally trained using training data 710. More specifically, machine learning algorithm(s) 720 may be pre-trained on data from one or more computing devices and the resulting trained machine learning model provided to computing device CD1, where CD1 is intended to execute the trained machine learning model during inference stage 704. Then, during the training phase 702, the pre-trained machine learning model may be additionally trained using the training data 710, where the training data 710 may be derived from both kernel and uncore data of the computing device CD 1. Further training of machine learning algorithm(s) 720 and/or pre-trained machine learning models using training data 710 of CD1 data may be performed using supervised or unsupervised learning. Once machine learning algorithm(s) 720 and/or the pre-trained machine learning model have been trained on at least training data 710, training phase 702 may be completed. The trained resulting machine learning model may be used as at least one of the trained machine learning model(s) 732.
In particular, once the training phase 702 has been completed, the trained machine learning model(s) 732 can be provided to the computing device, if not already on the computing device. The inference stage 704 may begin after the trained machine learning model(s) 732 are provided to the computing device CD 1.
During the inference stage 704, the trained machine learning model(s) 732 may receive input data 730 and generate and output one or more corresponding inferences and/or predictions 750 regarding the input data 730. As such, the input data 730 may be used as input to the trained machine learning model(s) 732 for providing corresponding inference and/or prediction(s) 750 to the kernel component and the non-kernel component. For example, the trained machine learning model(s) 732 may generate inference and/or prediction(s) 750 in response to one or more inference/prediction requests 740. In some examples, the trained machine learning model(s) 732 may be executed by a portion of other software. For example, the trained machine learning model(s) 732 can be executed by an inference or prediction daemon to readily provide inferences and/or predictions upon request. The input data 730 may include data from a computing device CD1 executing the trained machine learning model(s) 732 and/or input data from one or more computing devices other than CD 1.
Inference(s) and/or prediction(s) 750 can include task outputs, numerical values, and/or other output data produced by trained machine learning model(s) 732 operating on input data 730 (and training data 710). In some examples, the trained machine learning model(s) 732 may use feedback 760 that outputs inference(s) and/or prediction(s) 750 as inputs. The trained machine learning model(s) 732 may also rely on past inferences as input for generating new inferences.
After training, the trained version of the neural network may be an example of the trained machine learning model(s) 732. In this approach, one or more examples of inference/prediction request 740 may be a request for a modeled task output of a prediction input screenshot, and a corresponding example of inference and/or prediction(s) 750 may be a predicted task output.
In some examples, one computing device CD _ SOLO may include a trained version of the neural network, possibly after training. The computing device CD _ SOLO may then receive a request for a predictive modeling task output and use the trained version of the neural network to predict the modeling task output.
In some examples, two or more computing devices CD _ CLI and CD _ SRV may be used to provide an output image; for example, a first computing device CD _ CLI may generate and send a request for a predictive modeling task output to a second computing device CD _ SRV. The CD _ SRV may then use the trained version of the neural network to predict modeled task outputs and respond to requests from the CD _ CLI for output classes. The CD CLI may then provide the requested output (e.g., using a user interface and/or display, printing a copy, electronic communication, etc.) upon receiving a response to the request.
Example data network
Fig. 8 depicts a distributed computing architecture 800 in accordance with an example embodiment. The distributed computing architecture 800 includes server devices 808, 810 configured to communicate with programmable devices 804a, 804b, 804c, 804d, 804e via a network 806. Network 806 may correspond to a Local Area Network (LAN), a Wide Area Network (WAN), a WLAN, a WWAN, an intranet, the public internet, or any other type of network configured to provide a communication path between networked computing devices. Network 806 may also correspond to a combination of one or more LANs, WANs, intranets, and/or the public Internet.
Although fig. 8 shows only five programmable devices, the distributed application architecture may serve tens, hundreds, or thousands of programmable devices. Further, the programmable devices 804a, 804b, 804c, 804d, 804e (or any additional programmable devices) may be any kind of computing device, such as a mobile computing device, desktop computer, wearable computing device, head-mounted device (HMD), network terminal, mobile computing device, and so forth. In some examples, the programmable devices may be directly connected to the network 806, such as illustrated by the programmable devices 804a, 804b, 804c, 804 e. In other examples, such as illustrated by programmable device 804d, the programmable device may be indirectly connected to network 806 via an associated computing device, such as programmable device 804 c. In this example, programmable device 804c may act as an associated computing device to pass electronic communications between programmable device 804d and network 806. In other examples, such as illustrated by programmable device 804e, the computing device may be part of and/or internal to a vehicle, such as a car, truck, bus, boat or ship, aircraft, or the like. In other examples not shown in fig. 8, the programmable device may be directly and indirectly connected to network 806.
The server devices 808, 810 may be configured to perform one or more services requested by the programmable devices 804a-804 e. For example, server devices 808 and/or 810 may provide content to programmable devices 804a-804 e. The content may include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video. The content may include compressed and/or uncompressed content. The content may be encrypted and/or unencrypted. Other types of content are also possible.
As another example, server devices 808 and/or 810 may provide programmable devices 804a-804e with access to software for database, search, computing, graphics, audio, video, web/internet applications, and/or other functionality. Many other examples of server devices are possible.
Computing device architecture
Fig. 9 is a block diagram of an example computing device 900, according to an example embodiment. In particular, the computing device 900 shown in fig. 9 may be configured to perform at least one function of the neural network 100 and/or the method 1100 and/or to perform at least one function related to the neural network 100.
Computing device 900 may include a user interface module 901, a network communication module 902, one or more processors 903, data storage 904, one or more cameras 918, one or more sensors 920, and a power system 922, all of which may be linked together via a system bus, network, or other connection mechanism 905.
The user interface module 901 may be operable to transmit data to and/or receive data from an external user input/output device. For example, the user interface module 901 may be configured to transmit data to and/or receive data from a user input device such as a touch screen, a computer mouse, a keyboard, a keypad, a touchpad, a trackball, a joystick, a voice recognition module, and/or other similar devices. User interface module 901 may also be configured to provide output to a user display device such as one or more Cathode Ray Tubes (CRTs), liquid crystal displays, light emitting diodes (leds), displays using Digital Light Processing (DLP) technology, printers, light bulbs, and/or other similar devices now known or later developed. User interface module 901 may also be configured to generate aural outputs using devices such as speakers, speaker jacks, audio output ports, audio output devices, headphones, and/or other similar devices. User interface module 901 may also be configured with one or more haptic devices that may generate haptic outputs, such as vibrations and/or other outputs detectable through touch and/or physical contact with computing device 900. In some examples, user interface module 901 may be used to provide a Graphical User Interface (GUI) for utilizing computing device 900, such as, for example, a graphical user interface of a mobile phone device.
The network communication module 902 may include one or more devices that provide one or more wireless interfaces 907 and/or one or more wired interfaces 908, which may be configured to communicate via a network. Wireless interface(s) 907 may include one or more wireless transmitters, receivers, and/or transceivers, such as BluetoothTMTransceiver, zigbee@Transceiver, wi-FiTMTransceiver, wiMAXTMTransceiver, LTETMA transceiver and/or other types of wireless transceivers that may be configured to communicate via a wireless network. The wired interface(s) 908 may include one or more wired transmitters, receivers, and/or transceivers, such as an ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceivers that may be configured to communicate via twisted pair, coaxial cable, fiber optic link, or similar physical connection to a wired network.
In some examples, the network communication module 902 may be configured to provide reliable, secure, and/or authenticated communication. For each communication described herein, information for facilitating reliable communication (e.g., guaranteed messaging) may be provided, possibly as part of a message header and/or trailer (e.g., packet/message ordering information, encapsulation header and/or trailer, size/time information, and transmission verification information such as Cyclic Redundancy Check (CRC) and/or parity values). Communications may be secured (e.g., encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, data Encryption Standard (DES), advanced Encryption Standard (AES), rivest-Shamir-Adelman (RSA) algorithm, diffie-Hellman (Diffie-Hellman) algorithm, secure socket protocol such as Secure Socket Layer (SSL) or Transport Layer Security (TLS), and/or Digital Signature Algorithm (DSA). Other cryptographic protocols and/or algorithms may also be used, or may be used in addition to those listed herein, to secure (and then decrypt/decode) communications.
The one or more processors 903 may include one or more general-purpose processors, and/or one or more special-purpose processors (e.g., digital signal processors, tensor Processing Units (TPUs), graphics Processing Units (GPUs), application specific integrated circuits, etc.). The one or more processors 903 may be configured to execute computer-readable instructions 906 contained in the data storage device 904 and/or other instructions described herein.
The data storage 904 may include one or more non-transitory computer-readable storage media that may be read and/or accessed by at least one of the one or more processors 903. The one or more computer-readable storage media may include volatile and/or nonvolatile storage components, such as optical, magnetic, organic, or other memory or disk storage, which may be integrated in whole or in part with at least one of the one or more processors 903. In some examples, data storage 904 may be implemented using a single physical device (e.g., one optical, magnetic, organic, or other memory or disk storage unit), while in other examples, data storage 904 may be implemented using two or more physical devices.
Data storage 904 may include computer readable instructions 906 and possibly additional data. In some examples, data storage 904 may include storage needed to perform at least a portion of the methods, scenarios, and techniques described herein and/or at least a portion of the functionality of the devices and networks described herein. In some examples, the data store 904 may include storage for a trained neural network model 912 (e.g., a model of a trained neural network, such as the neural network 100). In particular in these examples, the computer-readable instructions 906 may include instructions that, when executed by the one or more processors 903, enable the computing device 900 to provide some or all of the functionality of the trained neural network model 912.
In some examples, computing device 900 may include one or more cameras 918. Camera(s) 918 may include one or more image capture devices, such as still and/or video cameras, equipped to capture light and record the captured light in one or more images; that is, the camera(s) 918 may generate image(s) of captured light. The one or more images may be one or more still images and/or one or more images used in video imagery. The camera(s) 918 may capture light and/or electromagnetic radiation as visible light, infrared radiation, ultraviolet light, and/or as light emissions at one or more other frequencies.
In some examples, computing device 900 may include one or more sensors 920. The sensors 920 may be configured to measure conditions within the computing device 900 and/or conditions in the environment of the computing device 900 and provide data regarding such conditions. For example, sensors 920 may include one or more of (i) sensors for obtaining data about computing device 900, such as, but not limited to, thermometers for measuring a temperature of computing device 900, battery sensors for measuring power of one or more batteries of power system 922, and/or other sensors that measure conditions of computing device 900; (ii) An identification sensor for identifying other objects and/or devices, such as, but not limited to, a Radio Frequency Identification (RFID) reader, a proximity sensor, a one-dimensional barcode reader, a two-dimensional barcode (e.g., quick Response (QR) code) reader, and a laser tracker, wherein the identification sensor may be configured to read an identifier (such as an RFID tag, a barcode, a QR code) and/or other devices and/or objects configured to be read and provide at least identification information; (iii) Sensors for measuring position and/or motion of computing device 900, such as, but not limited to, tilt sensors, gyroscopes, accelerometers, doppler sensors, GPS devices, sonar sensors, radar devices, laser displacement sensors, and compasses; (iv) An environmental sensor, such as, but not limited to, an infrared sensor, an optical sensor, a light sensor, a biosensor, a capacitive sensor, a touch sensor, a temperature sensor, a wireless sensor, a radio sensor, a motion sensor, a microphone, a sound sensor, an ultrasonic sensor, and/or a smoke sensor, for obtaining data indicative of the environment of computing device 900; and/or (v) force sensors for measuring one or more forces (e.g., inertial and/or gravitational forces) acting around computing device 900, such as, but not limited to, one or more sensors measuring one or more dimensional forces, torques, ground forces, friction, and/or ZMP sensors identifying a Zero Moment Point (ZMP) and/or a position of a ZMP. Many other examples of sensors 920 are possible.
The power system 922 may include one or more batteries 924 and/or one or more external power interfaces 926 for providing power to the computing device 900. Each of the one or more batteries 924, when electrically coupled to the computing device 900, can serve as a source of stored power for the computing device 900. One or more batteries 924 of the power system 922 may be configured to be portable. Some or all of the one or more batteries 924 may be readily removable from the computing device 900. In other examples, some or all of the one or more batteries 924 may be internal to the computing device 900 and thus may not be readily removable from the computing device 900. Some or all of the one or more batteries 924 may be rechargeable. For example, rechargeable batteries may be recharged via a wired connection between the battery and another power source, such as through one or more power sources external to computing device 900 and connected to computing device 900 via one or more external power interfaces. In other examples, some or all of the one or more batteries 924 may be non-rechargeable batteries.
The one or more external power interfaces 926 of the power system 922 may include one or more wired power interfaces, such as a USB cable and/or a power cord, that enable a wired power connection to one or more power sources external to the computing device 900. The one or more external power interfaces 926 may include one or more wireless power interfaces, such as a Qi wireless charger, that enables wireless power connection to one or more external power sources, such as via the Qi wireless charger. Once a power connection to an external power source is established using one or more external power interfaces 926, computing device 900 may draw power from the external power source over the established power connection. In some examples, the power system 922 may include related sensors, such as battery sensors associated with one or more batteries or other types of power sensors.
Cloud-based server
Fig. 10 depicts a cloud-based server system, according to an example embodiment. In fig. 10, the functionality of the neural network and/or computing device may be distributed among computing clusters 1009a, 1009b, 1009 c. The computing cluster 1009a may include one or more computing devices 1000a, cluster storage arrays 1010a, and cluster routers 1011a connected by a local cluster network 1012 a. Similarly, a computing cluster 1009b may include one or more computing devices 1000b, cluster storage arrays 1010b, and cluster routers 1011b connected by a local cluster network 1012 b. Likewise, computing cluster 1009c may include one or more computing devices 1000c, cluster storage arrays 1010c, and cluster routers 1011c connected by a local cluster network 1012 c.
In some embodiments, the computing clusters 1009a, 1009b, 1009c may be a single computing device residing at a single computing center. In other embodiments, the computing clusters 1009a, 1009b, 1009c may include multiple computing devices in a single computing center, or even multiple computing centers located in different geographic locations. For example, fig. 10 depicts each of the compute clusters 1009a, 1009b, 1009c residing at different physical locations.
In some embodiments, the data and services at the computing clusters 1009a, 1009b, 1009c may be encoded as computer-readable information stored in a non-transitory tangible computer-readable medium (or computer-readable storage medium) and accessible by other computing devices. In some embodiments, the computing clusters 1009a, 1009b, 1009c may be stored on a single disk drive or other tangible storage medium, or may be implemented on multiple disk drives or other tangible storage media located in one or more different geographic locations.
In some embodiments, each of the computing clusters 1009a, 1009b, and 1009c may have the same number of computing devices, the same number of cluster storage arrays, and the same number of cluster routers. However, in other embodiments, each computing cluster may have a different number of computing devices, a different number of cluster storage arrays, and a different number of cluster routers. The number of computing devices, cluster storage arrays, and cluster routers in each computing cluster may depend on the one or more computing tasks assigned to each computing cluster.
For example, in computing cluster 1009a, computing device 1000a may be configured to perform various computing tasks of a conditioned, axial (axial) based self-attention neural network and/or computing device. In one embodiment, various functions of the neural network and/or the computing device may be distributed among one or more of the computing devices 1000a, 1000b, 1000 c. Computing devices 1000b and 1000c in respective computing clusters 1009b and 1009c may be configured similarly to computing device 1000a in computing cluster 1009 a. On the other hand, in some embodiments, computing devices 1000a, 1000b, and 1000c may be configured to perform different functions.
In some embodiments, the computing tasks and stored data associated with the neural network and/or computing devices may be distributed across the computing devices 1000a, 1000b, and 1000c based at least in part on the processing requirements of the neural network and/or computing devices, the processing power of the computing devices 1000a, 1000b, 1000c, the latency of network links between computing devices in each computing cluster and between the computing clusters themselves, and/or other factors that may contribute to cost, speed, fault tolerance, resiliency, efficiency, and/or other design goals of the overall system architecture.
The cluster storage arrays 1010a, 1010b, 1010c of the computing clusters 1009a, 1009b, 1009c may be data storage arrays comprising disk array controllers configured to manage read-write access to groups of hard disk drives. The disk array controllers, alone or in conjunction with their respective computing devices, may also be configured to manage backup or redundant copies of data stored in the clustered storage arrays to prevent disk drive or other clustered storage array failures and/or network failures that prevent one or more computing devices from accessing one or more clustered storage arrays.
Similar to the way that the functionality of the conditioned, axial self-attention based neural network and/or computing device may be distributed across the computing devices 1000a, 1000b, 1000c of the computing clusters 1009a, 1009b, 1009c, various active and/or backup portions of these components may be distributed across the cluster storage arrays 1010a, 1010b, 1010 c. For example, some of the clustered storage arrays may be configured to store a first layer of the neural network and/or a portion of the computing device's data, while other clustered storage arrays may store a second layer of the neural network and/or other portion(s) of the computing device's data. Further, for example, some cluster storage arrays may be configured to store data for an encoder of a neural network, while other cluster storage arrays may store data for a decoder of the neural network. In addition, some cluster storage arrays may be configured to store backup versions of data stored in other cluster storage arrays.
The cluster routers 1011a, 1011b, 1011c in the computing clusters 1009a, 1009b, 1009c may comprise networking equipment configured to provide internal and external communications for the computing clusters. For example, cluster router 1011a in computing cluster 1009a may include one or more internet switching and routing devices configured to provide (I) local area network communications between computing cluster 1000a and cluster storage array 1010a via local cluster network 1012a, and (ii) wide area network communications between computing cluster 1009a and computing clusters 1009b and 1009c via wide area network link 1013a to network 806. Cluster routers 1011b and 1011c may comprise network equipment similar to cluster router 1011a, and cluster routers 1011b and 1011c may perform networking functions for computing clusters 1009b and 1009b similar to that performed by cluster router 1011a for computing cluster 1009 a.
In some embodiments, the configuration of the cluster routers 1011a, 1011b, 1011c may be based at least in part on the data communications requirements of the computing devices and the cluster storage arrays, the data communications capabilities of the network equipment in the cluster routers 1011a, 1011b, 1011c, the latency and throughput of the local cluster networks 1012a, 1012b, 1012c, the latency, throughput and cost of the wide area network links 1013a, 1013b, 1013c, and/or other factors that may contribute to cost, speed, fault tolerance, resiliency, efficiency, and/or other design criteria of a reasonable system architecture.
Example methods of operation
Fig. 11 is a flow diagram of a method 1100 according to an example embodiment. Method 1100 may be performed by a computing device, such as computing device 900. Method 1100 may begin at block 1110, where a computing device receives a screenshot of a display provided by a graphical user interface of the computing device.
At block 1120, the computing device generates, by an image-structure transformer of the neural network, a representation by fusing the first embedding based on the screenshot and the second embedding based on the layout of the virtual object in the screenshot.
At block 1130, the computing device predicts, over the neural network and based on the generated representation, a modeled task output associated with the graphical user interface.
At block 1140, the computing device provides the predicted modeling task output via the computing device.
Some embodiments relate to predicting the layout of virtual objects in a screenshot through a neural network.
Some embodiments relate to receiving, via a computing device, a view hierarchy indicating a layout of virtual objects in a screenshot. Such embodiments involve determining content embedding and location embedding by the neural network for each of the screen capture and view hierarchies. Generating the representation through fusing includes generating the representation based on content embedding and location embedding. In such embodiments, the positional embedding of the screenshot and the positional embedding of the view level may be global embedding corresponding to the entire screenshot.
In some embodiments, the modeling of the graphical user interface involves multi-tasking modeling, and wherein the neural network comprises dual transformers, wherein the image-structure transformer predicts a modeling task output for the image structure task, the question-answer transformer predicts a modeling task output for the natural language task, and wherein the image-structure transformer and the question-answer transformer are configured to have cross-tower attention.
In some embodiments, the modeled task output may be used for one or more of the following tasks: an object detection task, a natural language command floor task, a widget-plus-text task, a screen summary task, or a flick prediction task.
In some embodiments, the modeling task output may be for a natural language command ground task. The embodiments relate to predicting, through a neural network and based on the representation, a target virtual object in a graphical user interface. Such embodiments also relate to associating a target virtual object with a natural language command. Embodiments are also directed to providing natural language commands via a graphical user interface.
In some embodiments, providing the natural language command involves displaying the natural language command at or near the target virtual object.
In some embodiments, providing the natural language command involves providing the natural language command as a voice command in response to a user interaction with the target virtual object.
In some embodiments, the modeling task output may be used for an object detection task. Such embodiments involve detecting, by the neural network, one or more types of container objects indicative of a layout hierarchy of the screenshot. In such embodiments, the layout hierarchy may include one of a linear layout, a frame layout, or a list.
In some embodiments, the modeling task output may be used for an object detection task. Such embodiments involve detecting one or more of a text field, a toggle button, or an image view through a neural network.
In some embodiments, the modeling task output may be for a widget plus text task. Such embodiments relate to predicting, through a neural network and for screenshots, a natural language description of the function of a predicted virtual object in a graphical user interface.
In some embodiments, the modeled task output may be used for a flickability prediction task. Such embodiments relate to identifying a mismatch between a developer-designed flickable feature and a user-perceived flickable feature for a graphical user interface. Such embodiments also relate to providing suggestions to developers of graphical user interfaces to compensate for identified mismatches.
In some embodiments, the neural network may include an object detection head, a text head, and an indicator head.
Some embodiments relate to training a neural network to receive input screenshots displayed by a particular graphical user interface and to predict a modeled task output associated with modeling of the particular graphical user interface.
In some embodiments, the training is performed at a computing device.
In some embodiments, the predictive modeling task output involves obtaining a trained neural network at the computing device. Such embodiments also involve applying the obtained trained neural network to the prediction of the modeled task output.
In some embodiments, the predictive modeling task output relates to a request to determine the predictive modeling task output by the computing device. Such embodiments involve sending a request for a predictive modeling task output from a computing device to a second computing device, the second computing device comprising a trained version of a neural network. Such embodiments also involve, after sending the request, the computing device receiving the predicted modeling task output from the second computing device.
The present disclosure is not limited to the particular embodiments described in this application, which are intended to be illustrative of various aspects. It will be apparent to those skilled in the art that many modifications and variations can be made without departing from the spirit and scope of the invention. Functionally equivalent methods and apparatuses within the scope of the invention, in addition to those enumerated herein, will be apparent to those skilled in the art from the foregoing description. Such modifications and variations are intended to fall within the scope of the appended claims.
The above detailed description describes various features and functions of the disclosed systems, devices, and methods with reference to the accompanying drawings. In the drawings, like numerals generally refer to like elements unless context indicates otherwise. The illustrative embodiments described in the detailed description, drawings, and claims are not meant to be limiting. Other embodiments may be utilized, and other changes may be made, without departing from the spirit or scope of the subject matter presented here. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.
With respect to any or all of the ladder diagrams, scenarios, and flow diagrams in the figures, and as discussed herein, each block and/or communication may represent processing of information and/or transmission of information, according to example embodiments. Alternate embodiments are included within the scope of these example embodiments. In such alternative embodiments, for example, functions described as blocks, transmissions, communications, requests, responses, and/or messages may be performed out of the order illustrated or discussed (including substantially concurrently or in reverse order) depending on the functionality involved. Further, more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow diagrams discussed herein, and these ladder diagrams, scenarios, and flow diagrams may be combined with one another, in part or in whole.
The blocks representing information processing may correspond to circuitry that may be configured to perform particular logical functions of the methods or techniques described herein. Alternatively or additionally, a block representing processing of information may correspond to a module, segment, or portion of program code (including associated data). The program code may include one or more instructions executable by a processor to implement specific logical functions or actions in a method or technique. The program code and/or associated data may be stored on any type of computer-readable medium, such as a storage device including a disk or hard drive or other storage medium.
The computer readable medium may also include non-transitory computer readable media, such as non-transitory computer readable media that store data for short periods of time, such as register memory, processor cache, and Random Access Memory (RAM). The computer readable medium may also include a non-transitory computer readable medium that stores program code and/or data for a long term, such as a secondary or permanent long term storage device, e.g., read Only Memory (ROM), optical or magnetic disks, compact disk read only memory (CD-ROM). The computer readable medium may also be any other volatile or non-volatile storage system. For example, the computer-readable medium may be considered a computer-readable storage medium, or a tangible storage device.
Further, a block representing one or more transfers of information may correspond to a transfer of information between software and/or hardware modules in the same physical device. However, other information transfers may occur between software modules and/or hardware modules in different physical devices.
As described herein, a multimodal transformer VUT for multitask modeling of a user interface is described. The model may be configured to accept three types of data, namely UI screenshots, view hierarchies, and natural language questions. Experiments based on 5 data sets show that the VUT simultaneously realizes 5 types of UI tasks and shows a prospect of providing unified modeling for the user interface field. The VUT model supports multi-modal multi-task learning of several reference UI tasks, and finally mobile interaction and user experience can be facilitated.
Although the example tasks described herein address the UI modeling problem, they may be generalized to different tasks. For example, the input and output modalities are based on common data types. The input includes images, view levels, and language. The output header is equipped with the ability to generate view hierarchies, object references, and linguistic responses. Thus, many tasks based on these input and output modalities can potentially be learned with this model. For example, UI layout generation may be processed by a question-answer model to generate a sequence of markup at a linearized view level.
While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are provided for purposes of illustration and are not intended to be limiting, with the true scope being indicated by the following claims.
Claims (21)
1. A computer-implemented method, comprising:
receiving, via a computing device, a screenshot of a display provided by a graphical user interface of the computing device;
generating, by an image-structure transformer of a neural network, a representation by fusing a first embedding based on a screen capture and a second embedding based on a layout of virtual objects in the screen capture;
predicting, by the neural network and based on the generated representation, a modeled task output associated with the graphical user interface; and
providing, by the computing device, the predicted modeling task output.
2. The computer-implemented method of claim 1, further comprising:
the layout of the virtual objects in the screenshot is predicted by the neural network.
3. The computer-implemented method of claim 1, further comprising:
receiving, via a computing device, a view hierarchy indicating a layout of virtual objects in a screenshot; and
content embedding and location embedding are determined by the neural network and for each of the screen capture and view hierarchies,
wherein generating the representation through fusing comprises generating the representation based on content embedding and location embedding.
4. The computer-implemented method of claim 3, wherein the positional embedding of the screenshot and the positional embedding of the view level are global embedding corresponding to the entire screenshot.
5. The computer-implemented method of claim 1, wherein the modeling of the graphical user interface comprises multi-tasking modeling, and wherein the neural network comprises dual transformers, wherein the image-structure transformer predicts modeling task outputs for a image-structure task and the question-answer transformer predicts modeling task outputs for a natural language task, and wherein the image-structure transformer and question-answer transformer are configured to have cross-tower attention.
6. The computer-implemented method of claim 1, wherein the modeling task output is for one or more of: an object detection task, a natural language command floor task, a widget-plus-text task, a screen summary task, or a flick prediction task.
7. The computer-implemented method of claim 1, wherein the modeling task output is for a natural language command floor task, and the method further comprises:
predicting, by the neural network and based on the representation, a target virtual object in the graphical user interface;
associating the target virtual object with the natural language command; and
the natural language commands are provided via a graphical user interface.
8. The computer-implemented method of claim 1, wherein providing a natural language command comprises displaying the natural language command at or near a target virtual object.
9. The computer-implemented method of claim 1, wherein providing a natural language command comprises providing the natural language command as a voice command in response to a user interaction with a target virtual object.
10. The computer-implemented method of claim 1, wherein the modeling task output is for an object detection task, and the method further comprises:
one or more types of container objects indicative of a layout hierarchy of the screenshot are detected by the neural network.
11. The computer-implemented method of claim 10, wherein the layout hierarchy comprises one of a linear layout, a frame layout, or a list.
12. The computer-implemented method of claim 1, wherein the modeling task output is for an object detection task, and the method further comprises:
one or more of a text field, a toggle button, or an image view is detected by the neural network.
13. The computer-implemented method of claim 1, wherein the modeling task output is for a widget-plus-text task, and the method further comprises:
predicting, by the neural network and for the screenshot, a natural language description of a function of the virtual object predicted in the graphical user interface.
14. The computer-implemented method of claim 1, wherein the modeled task output is for a flickability prediction task, and the method further comprises:
for a graphical user interface, identifying a mismatch between a developer-designed flickable feature and a user-perceived flickable feature; and
suggestions are provided to a developer of the graphical user interface to compensate for the identified mismatches.
15. The computer-implemented method of claim 1, wherein the neural network comprises an object detection head, a text head, and an indicator head.
16. The computer-implemented method of claim 1, further comprising:
the neural network is trained to receive input screenshots displayed by a particular graphical user interface and to predict modeling task outputs associated with modeling of the particular graphical user interface.
17. The computer-implemented method of claim 16, wherein the training is performed at the computing device.
18. The computer-implemented method of claim 1, wherein the predictive modeling task output further comprises:
obtaining, at a computing device, a trained neural network; and
the obtained trained neural network is applied to model predictions of task outputs.
19. The computer-implemented method of claim 1, wherein the predictive modeling task output comprises:
determining, by a computing device, a request for a predictive modeling task output;
sending, from the computing device to a second computing device, a request for a predictive modeling task output, the second computing device comprising a trained version of a neural network; and
after sending the request, the computing device receives a predicted modeling task output from the second computing device.
20. A computing device, comprising:
one or more processors; and
data storage, wherein the data storage has stored thereon computer-executable instructions that, when executed by the one or more processors, cause the computing device to perform functions comprising:
receiving a screenshot of a display provided by a graphical user interface of a computing device;
generating, by an image-structure transformer of a neural network, a representation by fusing a first embedding based on the screenshot and a second embedding based on a layout of the virtual object in the screenshot;
predicting, by the neural network and based on the generated representation, a modeled task output associated with the graphical user interface; and
providing a predicted modeling task output.
21. An article of manufacture comprising one or more non-transitory computer-readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to perform functions comprising:
receiving a screenshot of a display provided by a graphical user interface of a computing device;
generating, by an image-structure transformer of a neural network, a representation by fusing a first embedding based on the screenshot and a second embedding based on a layout of the virtual object in the screenshot;
predicting, by the neural network and based on the generated representation, a modeled task output associated with the graphical user interface; and
providing a predicted modeling task output.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163221677P | 2021-07-14 | 2021-07-14 | |
US63/221,677 | 2021-07-14 | ||
US17/812,208 | 2022-07-13 | ||
US17/812,208 US20230031702A1 (en) | 2021-07-14 | 2022-07-13 | Neural Networks based Multimodal Transformer for Multi-Task User Interface Modeling |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115271093A true CN115271093A (en) | 2022-11-01 |
Family
ID=83764519
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202210834687.3A Pending CN115271093A (en) | 2021-07-14 | 2022-07-14 | Neural network based multimodal transformer for multitasking user interface modeling |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230031702A1 (en) |
CN (1) | CN115271093A (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116048708A (en) * | 2023-03-31 | 2023-05-02 | 成都大前研软件开发有限公司 | Software window adjusting method, system, equipment and medium based on artificial intelligence |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11915471B2 (en) * | 2021-02-05 | 2024-02-27 | Salesforce, Inc. | Exceeding the limits of visual-linguistic multi-task learning |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6229287B2 (en) * | 2013-04-03 | 2017-11-15 | ソニー株式会社 | Information processing apparatus, information processing method, and computer program |
US10191889B2 (en) * | 2014-07-29 | 2019-01-29 | Board Of Regents, The University Of Texas System | Systems, apparatuses and methods for generating a user interface by performing computer vision and optical character recognition on a graphical representation |
US11645467B2 (en) * | 2018-08-06 | 2023-05-09 | Functionize, Inc. | Training a system to perform a task with multiple specific steps given a general natural language command |
GB2586265B (en) * | 2019-08-15 | 2023-02-15 | Vision Semantics Ltd | Text based image search |
CN113076441A (en) * | 2020-01-06 | 2021-07-06 | 北京三星通信技术研究有限公司 | Keyword extraction method and device, electronic equipment and computer readable storage medium |
US11741943B2 (en) * | 2020-04-27 | 2023-08-29 | SoundHound, Inc | Method and system for acoustic model conditioning on non-phoneme information features |
US11704559B2 (en) * | 2020-06-17 | 2023-07-18 | Adobe Inc. | Learning to search user experience designs based on structural similarity |
US11386114B2 (en) * | 2020-10-21 | 2022-07-12 | Adobe Inc. | Structure-based transformers with localization and encoding for chart question answering |
EP4262121A4 (en) * | 2020-12-31 | 2024-03-13 | Huawei Tech Co Ltd | Neural network training method and related apparatus |
-
2022
- 2022-07-13 US US17/812,208 patent/US20230031702A1/en active Pending
- 2022-07-14 CN CN202210834687.3A patent/CN115271093A/en active Pending
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116048708A (en) * | 2023-03-31 | 2023-05-02 | 成都大前研软件开发有限公司 | Software window adjusting method, system, equipment and medium based on artificial intelligence |
CN116048708B (en) * | 2023-03-31 | 2024-02-23 | 成都齐之之知识产权运营有限公司 | Software window adjusting method, system, equipment and medium based on artificial intelligence |
Also Published As
Publication number | Publication date |
---|---|
US20230031702A1 (en) | 2023-02-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Sarkar et al. | Hands-On Transfer Learning with Python: Implement advanced deep learning and neural network models using TensorFlow and Keras | |
AU2019200270B2 (en) | Concept mask: large-scale segmentation from semantic concepts | |
KR20210040319A (en) | Method, apparatus, device, storage medium and computer program for entity linking | |
KR20230128492A (en) | Explainable Transducers Transducers | |
CN111755078A (en) | Drug molecule attribute determination method, device and storage medium | |
US11740879B2 (en) | Creating user interface using machine learning | |
CN115271093A (en) | Neural network based multimodal transformer for multitasking user interface modeling | |
CN113762052A (en) | Video cover extraction method, device, equipment and computer readable storage medium | |
Rivas | Deep Learning for Beginners: A beginner's guide to getting up and running with deep learning from scratch using Python | |
EP3903182B1 (en) | Natural solution language | |
CN114443899A (en) | Video classification method, device, equipment and medium | |
US11532387B2 (en) | Identifying information in plain text narratives EMRs | |
CN115455171A (en) | Method, device, equipment and medium for mutual retrieval and model training of text videos | |
Kalin | Generative Adversarial Networks Cookbook: Over 100 Recipes to Build Generative Models Using Python, TensorFlow, and Keras | |
Ferlitsch | Deep Learning Patterns and Practices | |
US20230153335A1 (en) | Searchable data structure for electronic documents | |
CN115759262A (en) | Visual common sense reasoning method and system based on knowledge perception attention network | |
CN115249361A (en) | Instructional text positioning model training, apparatus, device, and medium | |
Newnham | Machine Learning with Core ML: An iOS developer's guide to implementing machine learning in mobile apps | |
Wooton | Facilitating visual to parametric interaction with deep contrastive learning | |
US20240070456A1 (en) | Corrective Reward Optimization for Sequential Labeling | |
US20230230406A1 (en) | Facilitating identification of fillable regions in a form | |
US11727215B2 (en) | Searchable data structure for electronic documents | |
Dibia et al. | Theme Article: Visual Data Science | |
Yun | Analysis of machine learning classifier performance in adding custom gestures to the Leap Motion |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |