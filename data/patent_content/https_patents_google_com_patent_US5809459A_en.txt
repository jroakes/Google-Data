US5809459A - Method and apparatus for speech excitation waveform coding using multiple error waveforms - Google Patents
Method and apparatus for speech excitation waveform coding using multiple error waveforms Download PDFInfo
- Publication number
- US5809459A US5809459A US08/651,172 US65117296A US5809459A US 5809459 A US5809459 A US 5809459A US 65117296 A US65117296 A US 65117296A US 5809459 A US5809459 A US 5809459A
- Authority
- US
- United States
- Prior art keywords
- error
- waveform
- excitation
- segment
- target
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Lifetime
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/04—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis using predictive techniques
- G10L19/08—Determination or coding of the excitation function; Determination or coding of the long-term prediction parameters
- G10L19/12—Determination or coding of the excitation function; Determination or coding of the long-term prediction parameters the excitation function being a code excitation, e.g. in code excited linear prediction [CELP] vocoders
- G10L19/125—Pitch excitation, e.g. pitch synchronous innovation CELP [PSI-CELP]
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/04—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis using predictive techniques
- G10L19/16—Vocoder architecture
- G10L19/18—Vocoders using multiple modes
- G10L19/24—Variable rate codecs, e.g. for generating different qualities using a scalable representation such as hierarchical encoding or layered encoding
Definitions
- the present invention relates generally to speech compression, and more specifically to speech compression using multiple error components derived from the excitation waveform.
- Prior-art speech compression techniques use modeling methods that cannot converge to original speech quality regardless of bandwidth or processing effort. Such prior-art methods rely heavily on classification and over-simplified modeling methodologies, resulting in poor performance and low speech quality.
- Over-simplified modeling techniques can introduce significant error in fundamental speech and excitation parameters, causing audible distortion in the synthesized speech waveform. Such algorithms can also fail to function properly in the face of classification errors, especially in the presence of interference.
- prior-art, speech coding algorithms often implement fragile, non-robust parameter extraction techniques. These prior-art speech compression methods are typically inflexible, making it difficult to adapt them to multiple data rates.
- FIG. 1 illustrates a voice coding apparatus in accordance with a preferred embodiment of the present invention
- FIG. 2 illustrates a multi-layer perceptron classifier structure in accordance with a preferred embodiment of the present invention
- FIG. 3 illustrates a method for calculating the degree of periodicity in accordance with a preferred embodiment of the present invention
- FIG. 4 illustrates a method for calculating pitch in accordance with a preferred embodiment of the present invention
- FIG. 5 illustrates a candidate correlation set for a highly periodic frame derived in accordance with a preferred embodiment of the present invention
- FIG. 6 illustrates a candidate correlation set for a transition frame derived in accordance with a preferred embodiment of the present invention
- FIG. 7 illustrates a method for estimating transmitter epoch locations using a three stage analysis in accordance with a preferred embodiment of the present invention
- FIG. 8 illustrates first stage epoch locations determined from filtered speech derived in accordance with a preferred embodiment of the present invention
- FIG. 9 illustrates third stage epoch locations determined from the excitation waveform derived in accordance with a preferred embodiment of the present invention.
- FIG. 10 illustrates a method for estimating receiver epoch locations in accordance with a preferred embodiment of the present invention
- FIG. 11 illustrates a method for computation of target in accordance with a preferred embodiment of the present invention
- FIG. 12 illustrates a typical target derived using an ensemble-mean approach in accordance with a preferred embodiment of the present invention
- FIG. 13 illustrates a typical target derived using a matched-filter approach in accordance with a preferred embodiment of the present invention
- FIG. 14 illustrates a method for encoding the calculated target in accordance with a preferred embodiment of the present invention
- FIG. 15 illustrates a target which has been cyclically shifted in accordance with a preferred embodiment of the present invention
- FIG. 16 illustrates the target inphase and quadrature components prior to a cyclical transform in accordance with a preferred embodiment of the present invention
- FIG. 17 illustrates the target inphase and quadrature components after a cyclical transform in accordance with a preferred embodiment of the present invention
- FIG. 18 illustrates a method for computation of orthogonal error components in accordance with a preferred embodiment of the present invention
- FIG. 19 illustrates a typical length-normalized, energy-denormalized periodic model derived from the ensemble-mean target, and a typical ensemble error waveform in accordance with a preferred embodiment of the present invention
- FIG. 20 illustrates a typical periodic model derived from the match-filtered target, and the typical ensemble error waveform derived from the match-filtered reference in accordance with a preferred embodiment of the present invention
- FIG. 21 illustrates a typical standard deviation error waveform derived in accordance with a preferred embodiment of the present invention
- FIG. 22 illustrates a typical mean error waveform derived in accordance with a preferred embodiment of the present invention
- FIG. 23 illustrates a typical phase error waveform derived in accordance with a preferred embodiment of the present invention
- FIG. 24 illustrates a method for encoding the ensemble modeling error in accordance with a preferred embodiment of the present invention
- FIG. 25 illustrates ensemble error characterization filtering of pitch normalized data derived in accordance with a preferred embodiment of the present invention
- FIG. 26 illustrates ensemble error characterization using a spectral model derived in accordance with a preferred embodiment of the present invention
- FIG. 27 illustrates a method for encoding standard deviation error, mean error, or phase error in accordance with a preferred embodiment of the present invention
- FIG. 28 illustrates a method for decoding standard deviation error, mean error, or phase error in accordance with a preferred embodiment of the present invention
- FIG. 29 illustrates a method for computing receiver epoch locations in accordance with a preferred embodiment of the present invention
- FIG. 30 illustrates a method for decoding target data in accordance with a preferred embodiment of the present invention
- FIG. 31 illustrates a method for decoding ensemble error in accordance with a preferred embodiment of the present invention
- FIG. 32 illustrates error component reconstruction using modulo-F cyclic repetition derived in accordance with a preferred embodiment of the present invention
- FIG. 33 illustrates error component reconstruction using modulo-F cyclic repetition plus noise derived in accordance with a preferred embodiment of the present invention.
- FIG. 34 illustrates a method for computing an excitation estimate in accordance with a preferred embodiment of the present invention.
- the method and apparatus of the present invention provides class-insensitive speech compression methods which encode a fundamental model and multiple orthogonal components which represent errors introduced by the fundamental model.
- the method and apparatus of the present invention further provides robust parameter extraction techniques and flexible modeling methodologies which enable operation at multiple data rates.
- the method and apparatus of the present invention provides improvement over prior art methods via convergence to an identity system (i.e., a system where output speech sounds identical to input speech) given sufficient bandwidth, reduced reliance on classification, reduced sensitivity to interference, robust parameter extraction techniques, and simple adaptation to multiple data rates.
- identity system i.e., a system where output speech sounds identical to input speech
- Prior-art, class-based interpolative speech coding methods cannot converge to perfect speech due to the simplicity of underlying models. Such simple models are unable to capture the underlying components of the excitation. These simplistic models are subject to a quality plateau, where perceptual speech quality fails to improve regardless of bandwidth or processing effort.
- the method and apparatus of the present invention achieves transparent speech output given sufficient bandwidth by means of new and novel error component modeling methods which orthogonalize and capture the fundamental basis elements of the excitation waveform.
- a periodic waveform model is incorporated along with four orthogonal error waveforms which together form a complete description of the excitation. These error waveforms represent those portions of the excitation which are not represented by the purely periodic model. By orthogonalizing the error components, the perceptual effect of each element is isolated from the composite set, and can thus be encoded separately.
- the identity-system capability and low complexity of this invention make it ideal for use in variable-rate applications. Such applications can be easily derived from the baseline algorithm without changing underlying modeling methods.
- the method and apparatus of the present invention provide improvements over prior art methods via convergence to an identity system given sufficient bandwidth, reduced reliance on classification, reduced sensitivity to interference, robust parameter extraction techniques, and simple adaptation to multiple data rates.
- FIG. 1 illustrates a voice coding apparatus in accordance with a preferred embodiment of the present invention.
- the voice coding apparatus includes Analysis Processor 100 and Synthesis Processor 900.
- Analysis Processor 100 encodes input speech which can either originate from a human speaker or be retrieved from a memory device (not shown).
- the encoded speech is sent to Synthesis Processor 900 over Channel 475.
- Channel 475 can be a hard-wired connection, a PSTN network, a radio frequency (RF) link, an optical or optical fiber link, a satellite system, or any combination thereof.
- RF radio frequency
- Synthesis Processor 900 decodes the received, encoded speech, resulting in synthesized speech. Synthesis Processor 900 can then output the synthesized speech to Audio Output Device 901, which can be, for example, a speaker, producing output speech audio Alternatively, Synthesis Processor 900 can store the synthesized speech in a memory device (not shown).
- FIG. 1 illustrates speech data being sent in one direction only (i.e., from Analysis Processor 100 to Synthesis Processor 900.
- This provides "simplex" (i.e., one-way) communication.
- "duplex" i.e., two-way) communication can be provided.
- another encoding device (not shown) can be co-located with Synthesis Processor 900.
- the other encoding device could encode speech data and send the encoded speech data to another decoding device (not shown) co-located with Analysis Processor 100.
- terminals that include both an encoding device and a decoding device could both send and receive speech data.
- input speech is first processed by Analog Input Device 102 which converts the input speech into an electrical analog signal.
- A/D Converter 10 then converts the electrical analog signal into a stream of digital samples.
- the digital samples are operated upon by Pre-processing Means 20, which performs functions such as high-pass filtering, adaptive filtering, and removal of spectral tilt.
- Pre-processing Means 20 performs functions such as high-pass filtering, adaptive filtering, and removal of spectral tilt.
- Frame-Synchronous LPC Means 25 performs linear predictive coding analysis and inverse filter operating on a frame of input speech to produce a frame-synchronous excitation waveform corresponding to the frame of speech under analysis.
- This first spectral model is optional, and, in an alternate embodiment, could be replaced by a somewhat modified algorithm structure which reduces computational complexity.
- Frame Synchronous LPC Means 25 is followed by Degree Of Periodicity Calculation Means 30, which computes a discrete degree of periodicity for the frame of speech under analysis.
- a low-level multi-layer perceptron classifier is used to calculate degree of periodicity and direct codebook selection for the coded parameters.
- the neural network classifier is used to direct the algorithm toward either "more random” or “more periodic” codebooks for those parameters which can benefit from classification. Since the classifier primarily directs codebook selection, and does not impact the underlying modeling methods, the speech coding algorithm is relatively insensitive to mis-classification.
- FIG. 2 illustrates Multi-Layer Perceptron (MLP) Classifier structure 35 in accordance with a preferred embodiment of the present invention.
- MLP Classifier 35 provides excellent class discrimination, is easily modified to support alternate feature sets and speech databases, and provides much more consistent results over prior-art, threshold-based methods.
- MLP Classifier 35 derived neural weights in an off-line backpropagation process well known to those of skill in the art
- the classifier uses a four-element feature vector, normalized to unit variance and zero mean, implemented on a two-subframe basis to provide a total of eight input features to the neural network.
- subframe features provide improved discrimination capability at class transition boundaries and further improves performance by providing a simple form of feature context.
- improved discrimination against "near-silence" conditions is obtained by including a very low level, zero-mean gaussian component prior to a feature calculation step.
- biases the features in low-energy conditions and provides for rejection of inaudible sinusoidal signal components which could be interpreted as class periodic.
- a preferred embodiment of the classifier was trained on a large, labeled database in excess of 10,000 speech frames in order to ensure good performance over a wide range of input speech data. Testing using a 5000 frame database outside the training set indicates a consistent accuracy rate of approximately 99.8%.
- FIG. 3 illustrates a method for calculating the degree of periodicity in accordance with a preferred embodiment of the present invention, which is performed by Degree of Periodicity Calculation Means 30.
- the method begins with Compute Features step 31, which computes the four classifier features.
- Compute Features step 31 is followed by Load Weights step 32, which loads the weights from memory which were calculated in the off-line backpropagation process.
- Compute MLP Output step 33 uses the weights and computed features to compute the output of the multi-layer perceptron classifier.
- Compute Degree of Periodicity step 34 then scalar quantizes the output of Compute MLP Output step 33 to one of multiple degree-of-periodicity levels.
- Degree of Periodicity Calculation Means 30 is followed by Pitch Calculation Means 70.
- Prior-art excitation-based methods based on processing the LPC residual for pitch determination have long proven to be unreliable for certain portions of voiced speech, especially for speech which is readily predicted by an all-pole model.
- a pitch detection technique accurately determines pitch directly from the speech waveform, eliminating problems associated with prior-art, excitation-based pitch detection methods.
- An accurate estimate of pitch is computed directly from subframe autocorrelation (e.g., 15 ms subframe segments) of low-pass filtered speech (e.g., 5 pole, low-pass Chebyshev, 0.1 dB ripple, 1000 Hz cutoff).
- Consistent pitch estimates are computed using this technique.
- Half-frame forward and backward subframe correlations are especially useful for onset and offset situations, in that they reduce the random bias introduced by the presence of nonperiodic transition data.
- FIG. 4 illustrates a method for calculating pitch in accordance with a preferred embodiment of the present invention, which is performed by Pitch Calculation Means 70.
- the method begins with Bandpass Filter Speech step 71, wherein the input speech is filtered using a bandpass filter (e.g., with cutoffs at 100 Hz and 1000 Hz).
- a bandpass filter e.g., with cutoffs at 100 Hz and 1000 Hz.
- Compute Multiple Subframe Autocorrelations step 72 computes a family of correlation sets using multiple subframe segments (e.g., two or more) of the segment of speech under analysis.
- Select Maximum Correlation Subset step 73 searches each of the subframe correlation sets and selects the set encompassing the maximum correlation coefficient ⁇ max .
- FIG. 5 illustrates a candidate correlation set for a highly periodic speech frame (i.e., highly-voiced) derived in accordance with a preferred embodiment of the present invention.
- FIG. 6 illustrates a candidate correlation set for a transition frame derived in accordance with a preferred embodiment of the present invention.
- onset and offset speech correlations maintain a useful harmonic pattern, which is augmented by the subframe analysis.
- an initial pitch estimate is selected in Select Initial Pitch Estimate step 74, which corresponds to the offset lag corresponding to ⁇ max .
- Search For All Possible Harmonics step 75 examines the correlation data for evidence of N possible harmonic patterns, each aligned with the maximum positive correlation. Naturally, a limited amplitude and lag variance relative to the peak correlation is tolerated.
- candidate harmonics are identified only if:
- Select Minimum Harmonic step 76 sets the pitch equal to a minimum identified harmonic lag. Pitch contour smoothing can be implemented later if necessary as a companion post process.
- Pitch Calculation Means 70 is followed by Transmitter (Tx) Epoch Location Estimation Means 110, which uses the input speech from Pre-processing Means 20, frame-synchronous excitation from Frame-Synchronous LPC Means 25, and pitch period determined from Pitch Calculation Means 70 to determine excitation epoch locations, wherein an epoch refers to a pitch synchronous segment of excitation corresponding to the pitch period.
- Tx Transmitter
- Epoch Location Estimation Means 110 uses the input speech from Pre-processing Means 20, frame-synchronous excitation from Frame-Synchronous LPC Means 25, and pitch period determined from Pitch Calculation Means 70 to determine excitation epoch locations, wherein an epoch refers to a pitch synchronous segment of excitation corresponding to the pitch period.
- a three-stage epoch position detection algorithm is used, whereby low-pass filtered speech, unfiltered speech, and preliminary excitation waveform are searched in a sequential fashion.
- the staged approach determines speech epoch indices directly from the filtered and unfiltered speech waveforms, and refines the estimate by using those indices as a mapping into the excitation waveform, where the index is finalized via a localized search.
- the algorithm first determines a dominant "sense", either positive or negative, and rectifies the waveform to preserve the identified sense.
- FIG. 7 illustrates a method for estimating transmitter epoch locations using a three stage analysis in accordance with a preferred embodiment of the present invention performed by Tx Epoch Location Estimation Means 110.
- the method begins with Lowpass Filter Speech step 111, where a lowpass filter is applied to the input speech waveform to produce a filtered speech waveform.
- Lowpass Filter Speech step 111 includes storing the original speech to memory for later reference.
- Determine Waveform Sense step 112 searches the speech waveform, the lowpass filtered speech waveform, and the excitation waveform for the dominant sense of each waveform, wherein sense refers to the primary sign of the waveforms under analysis.
- a preferred embodiment of this method searches for the maximum positive or negative extent for each waveform and assigns the sign of the extent to the sense for each waveform.
- Dominant Sense step 113 multiplies the excitation waveform, the speech waveform, and the filtered speech waveform by the sense associated with each waveform. Then, the excitation waveform, speech waveform, and filtered speech waveform is rectified in Rectify Waveforms step 114.
- Set Deviation Factors step 115 sets the appropriate pitch search factor for each waveform, where each factor represents the range of pitch period over which waveform peaks are to be determined. For example, the pitch search range factor for filtered speech could be set at 0.5, the search range factor for the speech waveform could be set at 0.3, and the search range factor for the excitation waveform could be set at 0.1. Hence, the search range of each subsequent stage is narrowed in order to restrict the peak search for that stage. Furthermore, Set Deviation Factors step 115 can take into account the degree-of-periodicity when assigning range factors by restricting search range for aperiodic data.
- Set Start Index step 116 sets the starting index for the peak search.
- a starting index is desirably assigned to be the ending index of the prior frame, minus the frame length.
- Search Filtered Speech step 117 searches the filtered speech for peaks at pitch intervals from the starting index over the range of samples determined by the search range factor assigned in Set Deviation Factors step 115, producing indices corresponding to filtered speech epoch locations.
- Search Unfiltered Speech step 118 uses the indices determined in Search Filtered Speech step 117 as start indices, searching the unfiltered speech for peaks over a range of samples determined by the second search range factor, producing indices corresponding to unfiltered epoch locations.
- Search Excitation step 119 uses the indices determined in Search Unfiltered Speech step 118 as start indices, searching the excitation for peaks over the range of samples determined by the third search range factor, producing excitation epoch locations.
- Assign Offset step 120 applies a desired offset to each of the excitation epoch peak locations to produce epoch locations, for example 0.5 * pitch, although other offsets could also be appropriate.
- FIG. 8 illustrates epoch peak locations of filtered speech
- FIG. 9 illustrates excitation epoch peak locations derived using a preferred embodiment of the method. Note from the figures that the staged method works well to provide an accurate index from the speech waveform into the corresponding excitation portion.
- Tx Epoch Location Estimation Means 110 produces an estimate of the number of epochs within the segment under analysis.
- Epoch Aligned LPC Means 150 uses the estimated epoch locations to compute second LPC parameters corresponding to a segment of speech aligned with the estimated epoch locations. In this manner, the excitation model corresponds directly with the spectral model for the segment of speech under analysis. Epoch Aligned LPC Means 150 produces line spectral frequencies corresponding to the segment of speech under analysis, although other representations could also be appropriate (e.g., reflection coefficients).
- Spectrum Encoding Means 155 encodes the spectral parameters corresponding to the segment of speech under analysis, producing a code index and quantized spectral parameters.
- Spectrum Encoding Means 155 can use vector quantization or multi-stage vector quantization techniques well-known to those of skill in the art. In a preferred embodiment of the invention, Spectrum Encoding Means 155 selects from codebooks corresponding to each of the discrete degrees-of-periodicity produced by Degree of Periodicity Calculation Means 30, although a non-class-based approach could also be appropriate.
- Excitation Computation Means 160 uses an inverse filter described by the quantized spectral parameters to compute a second excitation waveform.
- Target Location Selection Means 165 uses the second excitation waveform computed in Excitation Computation Means 160 to determine a target location corresponding to a representative segment of excitation consisting of a single pitch period.
- Target Location Selection Means 165 produces an integer representing the sample index.
- a problem associated with periodic excitation models is the introduction of error to the reconstructed excitation waveform.
- error can be manifest in the form of significant envelope/energy deviations relative to the waveform being modeled.
- Interpolation error variance can be larger than necessary due to random selection of interpolation target, leading to audible envelope/energy modulation artifacts in the absence of error modeling.
- Deterministic target selection in accordance with the present invention can correct problems associated with random target selection.
- Possible methods for example, could include a variance analysis of each candidate target epoch energy relative to the previously determined source energy, or analysis of the source-target peak interpolation envelope relative to the original peak excitation envelope. Proper selection of target position minimizes variance of the modeled error waveforms, and hence improves error quantization efficiency.
- Target Location Encoding Means 170 encodes the target location using a scalar quantizer, producing a code index and quantized target location.
- Frequency Encoding Means 175 scalar quantizes the number of epochs determined in Tx Epoch Location Estimation Means 110, and produces a code index and quantized number of epochs.
- Receiver (Rx) Epoch Location Estimation Means 180 uses the quantized target location from Target Location Encoding Means 170, and the quantized number of epochs from Frequency Encoding Means 175, to estimate the epoch locations which will be derived at the receiver, producing a sequence of epoch locations with an effective normalized pitch for each epoch within one sample of the average pitch.
- FIG. 10 illustrates a method for estimating receiver epoch locations in accordance with a preferred embodiment of the present invention performed by Rx Epoch Location Estimation Means 180.
- the method begins with Load Target Index step 181, which loads from memory the target index, ti, determined in Target Location Encoding Means 170.
- Load Source Index step 182 loads from memory the previous target index, and subtracts the frame length to form the source index, si.
- Set First Location L step 184 sets the location pointer, L, to the first epoch location, corresponding to the source index, si.
- Increment L by P step 185 increments the location pointer by the pitch estimate, P, forming a next location estimate.
- the location estimate, L is rounded to the nearest integer to reflect a proper sample index in Round L to Nearest Integer step 186.
- the location index is then stored to memory in Store Location L step 187.
- a determination is then made, in step 188, whether all locations have been computed. If not, the procedure branches as illustrated in FIG. 10. If so, the procedure then ends.
- Target Computation Means 220 computes a representative pitch-synchronous excitation segment, or target, to represent the entire frame.
- FIG. 11 illustrates a method for computation of target in accordance with a preferred embodiment of the present invention performed by Target Computation Means 220. The method begins with Load Target Length step 221, which sets the target length equal to the final epoch segment length determined by Rx Epoch Location Estimation Means 180. Load First Epoch step 222 then copies the epoch corresponding to the first excitation epoch into a temporary buffer, and optionally normalizes the epoch to a uniform length via linear or non-linear interpolation.
- Load Next Epoch step 223 repeats the procedure performed by Load First Epoch step 222 for the subsequent epoch, placing the epoch into the temporary buffer.
- Correlate N & N-1 step 224 correlates the current epoch (i.e., epoch N) in the temporary buffer with the adjacent epoch in the temporary buffer (i.e., epoch N-1), resulting in an array of correlation coefficients.
- Align Epoch step 225 then cyclically shifts epoch N by the lag corresponding to the maximum correlation in order to ensemble align epoch N with epoch N-1.
- Compute Representative Epoch step 227 performs an ensemble process which results in a single target epoch representative of the epochs within the frame.
- Compute Representative Epoch step 227 could compute an ensemble filter, an ensemble mean, or a synchronous matched-filter average which preserves the periodic portion of the segment under analysis.
- FIG. 12 illustrates a typical target derived using an ensemble-mean approach in accordance with a preferred embodiment of the present invention.
- Optimal filtering methods include a class of filters called "matched" filters. Such filters maximize the output signal-to-noise ratio over a symbol interval, T. The correlation characteristics inherent to this type of filter are commonly applied toward optimum detection of signals.
- matched filters One type of optimal matched filter that assumes white noise is defined by:
- FIG. 13 illustrates a typical target derived using a matched-filter approach in accordance with a preferred embodiment of the present invention.
- Target Computation Means 220 is followed by Target Standard Deviation Encoding Means 260, which scalar quantizes the target standard deviation, producing a code index and the quantized standard deviation value.
- Target Standard Deviation Encoding Means 260 selects from codebooks corresponding to each of the discrete degrees-of-periodicity produced by Degree of Periodicity Calculation Means 30, although a non-class-based approach could also be appropriate.
- Target Standard Deviation Encoding Means 260 is followed by Target Mean Encoding Means 265, which scalar quantizes the target mean, producing a code index and the quantized mean value.
- Target Mean Encoding Means 265 selects from codebooks corresponding to each of the discrete degrees-of-periodicity produced by Degree of Periodicity Calculation Means 30, although a non-class-based approach could also be appropriate.
- Target Encoding Means 270 uses the quantized standard deviation from Target Standard Deviation Encoding Means 260, the quantized mean from Target Mean Encoding Means 265, and the target from Target Computation Means 220 to encode the target vector, producing one or more code indices and the quantized target vector.
- FIG. 14 illustrates a method for encoding the calculated target in accordance with a preferred embodiment of the present invention which is performed by Target Encoding Means 270.
- the method begins by determining whether Target Period>M 271 in step 271.
- a check is made whether the target is greater than M samples in length, where M corresponds to the Fast Fourier Transform (FFT) size used for characterization of the target, typically a power of two.
- FFT Fast Fourier Transform
- Downsample step 272 is performed, which downsamples the representative target epoch to M samples. If the target length does not exceed FFT size M, or after Downsample step 272 is performed, Energy Normalize Target step 273 subtracts the target mean from each target sample and divides each target sample by the target standard deviation, producing a zero-mean, unit-variance target vector.
- Cyclic Transform step 274 pre-processes the target vector prior to frequency domain transformation in order to minimize frequency domain variance.
- FIG. 15 illustrates a target which has been cyclically shifted in accordance with a preferred embodiment of the present invention.
- FIG. 16 illustrates the target inphase and quadrature components prior to a cyclical transform in accordance with a preferred embodiment of the present invention.
- FIG. 17 illustrates the target inphase and quadrature components after a cyclical transform in accordance with a preferred embodiment of the present invention. Note from FIG. 17 that the variance of the cyclically shifted inphase and quadrature is reduced, improving subsequent quantization performance.
- FFT step 275 performs an M point FFT on the vector produced by Cyclic Transform step 274, producing inphase and quadrature frequency domain vectors.
- Select Codebook Subset step 276 uses the degree of periodicity from Degree of Periodicity Calculation Means 30 to select the codebook subset corresponding to the identified class.
- Encode Inphase step 277 quantizes, at most, M/2+1 samples of the inphase data using appropriate quantization methods, such as vector quantization (VQ), split VQ, multi-stage VQ, wavelet VQ, Trellis-Coded Quantization (TCQ), or wavelet TCQ quantizers, producing at least one code index and a quantized inphase vector.
- VQ vector quantization
- TCQ Trellis-Coded Quantization
- Encode Inphase step 277 can also perform linear or nonlinear downsampling on the inphase vector in order to increase the bandwidth-per-sample.
- Encode Quadrature step 278 quantizes, at most, M/2+1 samples of the quadrature data using appropriate quantization methods, such as VQ, split VQ, multi-stage VQ, wavelet VQ, TCQ, or wavelet TCQ quantizers, producing at least one code index and a quantized quadrature vector. Note that quadrature data will be zero for matched-filter methods. Encode Quadrature step 278 can also perform linear or nonlinear downsampling on the inphase vector in order to increase the bandwidth-per-sample.
- appropriate quantization methods such as VQ, split VQ, multi-stage VQ, wavelet VQ, TCQ, or wavelet TCQ quantizers, producing at least one code index and a quantized quadrature vector. Note that quadrature data will be zero for matched-filter methods.
- Encode Quadrature step 278 can also perform linear or nonlinear downsampling on the inphase vector in order to increase the bandwidth-per-sample.
- Compute Conjugate Spectrum step 279 uses the quantized inphase vector and quantized quadrature vector to produce the conjugate FFT spectrum, which will be familiar to those of skill in the art.
- the reconstructed inphase and quadrature vectors are then used in Inverse FFT step 280 to produce a quantized, energy-normalized, cyclically-shifted target vector.
- Inverse Cyclic Transform step 281 performs an inverse cyclic shift to return the vector to its original position.
- step 282 A determination is then made, in step 282, whether Target Period>M. If not, the procedure ends. If so, Upsample step 283 upsamples the target vector to the original vector length using methods well known to those of skill in the art, producing a quantized, energy-normalized target vector. The procedure then ends.
- Target Encoding Means 270 encodes inphase and quadrature vectors, alternate embodiments could also be appropriate, such as magnitude and phase representations.
- Target Encoding Means 270 can be used in conjunction with the ensemble mean, ensemble filter, and matched filter methods discussed above.
- Target Encoding Means 270 is followed by Error Component Computation Means 310.
- a preferred embodiment of the present invention implements an ensemble source-to-target interpolation process at the receiver in order to reconstruct an estimate of elided excitation components.
- a preferred embodiment of the invention extracts and encodes one or more error components.
- Error components represent those excitation components that are not modeled by source-to-target ensemble interpolation.
- the interpolated waveform and modeling error waveform produce periodic and pseudo-random functions, respectively, which correspond directly to the current analysis segment and spectral model.
- the ensemble interpolation operation results in an effective low-pass type of process.
- the receiver by combining a representation of the interpolation error and one or more of the remaining orthogonal error components with the interpolated model natural-sounding, high-quality speech can be achieved.
- FIG. 18 illustrates a method for computation of orthogonal error components in accordance with a preferred embodiment of the present invention which is performed by Error Component Computation Means 310.
- Orthogonal error components include phase error (step 319), ensemble error (step 321), standard deviation error (step 323), and mean error (step 324). These error components which are intrinsically orthogonal (i.e., uncorrellated) are represented as waveforms because they depict errors over a segment of speech samples or a period of time.
- the method can be said to orthogonalize the error components because specific components which are intrinsically uncorrellated are computed.
- the method begins with Pitch Normalize Quantized Target step 311.
- Pitch Normalize Quantized Target step 311 can upsample or downsample the segment to a second "global" normalizing length (i.e., a common pitch length for all frames), producing a unit variance, zero mean target vector with a normalized length. Upsampling of segments to an arbitrarily large value in this fashion has proven to be of value in epoch-to-epoch alignment and error computation, although downsampling to a smaller length could also be of value.
- the normalized target is correlated against the normalized target from the previous frame, or the "source”, which was stored to memory in an earlier operation.
- Correlate Target with Source step 312 hence produces an optimal alignment offset which is used by Align Target step 313 to cyclically shift the target epoch in order to maximize ensemble correlation, producing a quantized, normalized, shifted target.
- the reference excitation waveform is computed an placed in a temporary buffer.
- the first reference epoch computed by Excitation Computation Means 160, is loaded into the buffer in Load Reference Epoch step 314.
- Energy Normalize step 315 computes the mean and standard deviation of the reference epoch, and normalizes the reference epoch by subtracting the mean and dividing by the standard deviation for each epoch sample, producing a unit-variance, zero-mean reference epoch.
- Energy Normalize step 315 also stores the standard deviation and mean value for the current epoch to a second and third reference sequence, respectively, for later use.
- Pitch Normalize step 316 length normalizes the reference epoch in a manner identical with the method of Pitch Normalize Quantized Target step 311, producing an energy normalized, length normalized reference epoch.
- step 317 A determination is made, in step 317, whether all reference epochs have been computed. If not, the procedure iterates as shown in FIG. 18, where the next contiguous epoch segment is energy normalized, length normalized, and reference epochs have been placed in the buffer. If all reference epochs have been computed, Ensemble Interpolate step 318 produces an ensemble interpolated excitation waveform (i.e., the interpolation model) by ensemble point-to-point interpolation from the source to the shifted target, producing intervening epochs corresponding to the number of epochs in the analysis segment, minus one (i.e., the target).
- the interpolation model an ensemble interpolated excitation waveform
- Compute Phase Error step 319 computes the alignment offset for each epoch in the reference buffer relative to each epoch in the interpolation model, producing a sequence of integer offset values.
- Compute Phase Error step 319 uses correlation to compute the optimal offset for each reference epoch, although any method could be used which aligns the reference and model to minimize epoch-to-epoch distance.
- Align Model and Reference step 320 then cyclically shifts each epoch in the reference by the respective optimal offset in order to optimally align the reference and interpolation model prior to error computation, producing an aligned excitation reference.
- Compute Ensemble Error step 321 then subtracts the aligned excitation reference epoch on a point-to-point basis, producing a sequence of values representing the ensemble model error, or ensemble interpolation error for the segment of excitation being modeled.
- Restore Ensemble Error Phase step 322 cyclically shifts each epoch-synchronous error segment in the error waveform to it's pre-aligned, unshifted state for later characterization and encoding.
- Compute Standard Deviation Error step 323 then computes a model of standard deviation behavior across the segment of elided standard deviation values, and subtracts the model from the second reference sequence to produce a standard deviation error sequence.
- the model and reference could be used to form a fractional error factor.
- a linear interpolation model is used, whereby the source standard deviation is interpolated linearly to the standard deviation of the target, producing interpolated values of standard deviation for each elided epoch in the frame.
- Other non-linear standard deviation models could also be appropriate.
- a preferred embodiment employs an absolute minimum limit that the interpolated standard deviation is allowed to achieve.
- Compute Mean Error step 324 then linearly interpolates from the source mean to the target mean, producing interpolated values of mean for each elided epoch in the frame, and subtracts the model from the third reference sequence to produce a mean error sequence.
- an alternate embodiment could use the model and reference to compute a fraction error factor.
- a preferred embodiment employs an absolute minimum limit that the interpolated mean value is allowed to achieve.
- Store Normalized Target step 325 stores the quantized, energy-normalized, pitch-normalized target to memory, along with the quantized mean and quantized standard deviation, effectively producing the source target, source mean, and source standard deviation. The procedure then ends.
- FIG. 19 illustrates a typical length-normalized, energy-denormalized periodic model derived from the ensemble-mean target, and a typical ensemble error waveform in accordance with a preferred embodiment of the present invention.
- FIG. 20 illustrates a typical periodic model derived from the match-filtered target, and the typical ensemble error waveform derived from the match-filtered reference in accordance with a preferred embodiment of the present invention.
- FIG. 21 illustrates a typical standard deviation error waveform derived in accordance with a preferred embodiment of the present invention, expressed in fractional units relative to the standard deviation interpolative model, although other units could also be appropriate.
- FIG. 20 illustrates a typical periodic model derived from the match-filtered target, and the typical ensemble error waveform derived from the match-filtered reference in accordance with a preferred embodiment of the present invention.
- FIG. 21 illustrates a typical standard deviation error waveform derived in accordance with a preferred embodiment of the present invention, expressed in fractional units relative to the standard deviation interpolative model, although other units could also be appropriate
- FIG. 22 illustrates a typical mean error waveform derived in accordance with a preferred embodiment of the present invention, expressed in fractional units relative to the interpolative mean model, although other units could also be appropriate.
- FIG. 23 illustrates a typical phase error waveform derived in accordance with a preferred embodiment of the present invention, with each value expressed as an absolute offset, although other units could also be appropriate.
- Error Component Computation Means 310 can be used in conjunction with the ensemble mean, ensemble filter, and matched filter methods discussed above.
- Ensemble Error Encoding Means 350 characterizes the error sequence and encodes it for transmission or storage. Characterization and encoding of the ensemble error component can include baseband I and Q representations, although other representations could also be appropriate.
- error component characterization includes lowpass filtering of the normalized error component prior to excitation reconstruction and speech synthesis. Filter cutoffs on the order of 0.1 bandwidth (BW) or less can be imposed upon the normalized, length-expanded error components, resulting in a significant reduction in high-frequency energy.
- BW bandwidth
- this pre-filtering operation introduces little, if any, perceptual distortion for both male and female speakers upon reconstruction of the speech waveform, indicating that the error component need not be accurately represented at low bit rates.
- an alternate embodiment uses a form of indirect characterization via spectral modeling of the error waveform.
- a multi-pole LPC analysis and inverse filter is used to generate parameters describing the error waveform spectral envelope and corresponding "excitation", each of which can be encoded separately.
- FIG. 24 illustrates a method for encoding the ensemble modeling error in accordance with a preferred embodiment of the present invention and performed by Ensemble Error Encoding Means 350.
- the method begins with Characterize Ensemble Error step 351, wherein the length and energy normalized error waveform is first lowpass filtered, producing a lowpass-filtered ensemble error waveform.
- Ensemble Error step 351 By characterizing the ensemble error waveform in the pitch-normalized domain, a harmonic-aligned, fixed length vector is produced which is ideal for quantization.
- FIG. 25 shows ensemble error characterization filtering of pitch normalized data derived in accordance with a preferred embodiment of the present invention (i.e., the magnitude spectrum of two normalized representative periodic waveforms with different pitch).
- the typical error waveform spectrum is much less periodic.
- a latent periodic component can often be present since the error component is derived on a length-normalized epoch synchronous basis.
- the harmonics of the length-normalized waveforms are automatically aligned with each other, simplifying quantization of the baseband representation.
- quantization could be performed on harmonic-aligned, fixed-length vectors, (i.e., inphase and quadrature) notably improving quantization performance and subsequent speech quality.
- an effective error characterization filter has been experimentally shown to require approximately 64 frequency domain samples in order to preserve speech quality, corresponding to four harmonics of the error waveform, although more or fewer samples could also be appropriate. Note that a larger vector length preserves more harmonics of the error waveform.
- FIG. 26 shows ensemble error characterization using a spectral model derived in accordance with a preferred embodiment of the present invention (i.e., the lowpass filtered error waveform, and the four-pole error model residual of the error waveform). Note a sigma reduction from 0.244 in the filtered waveform to 0.004 in the four-pole residual, which represents a drop of over 35 dB. A bandwidth versus speech quality tradeoff optimizes the bandwidth allocated to the four pole model and the corresponding residual.
- a spectral model derived in accordance with a preferred embodiment of the present invention i.e., the lowpass filtered error waveform, and the four-pole error model residual of the error waveform. Note a sigma reduction from 0.244 in the filtered waveform to 0.004 in the four-pole residual, which represents a drop of over 35 dB.
- a bandwidth versus speech quality tradeoff optimizes the bandwidth allocated to the four pole model and the corresponding residual.
- H-Pole LPC step 353 performs an LPC analysis on the characterized error waveform, producing spectral error model parameters.
- Encode Spectrum step 354 then encodes the spectral parameters using quantization methods such as VQ, split VQ, multi-stage VQ, wavelet VQ, TCQ, and wavelet TCQ implementations.
- Encode Spectrum step 354 encodes H line spectral frequencies using a multi-stage vector quantizer, producing at least one code index and quantized spectral error model parameters, although other coding methods are also appropriate.
- the quantized error model parameters and characterized ensemble error are used to generate a spectral error model excitation waveform in Inverse Filter step 355 using methods well known to those of skill in the art.
- Select Codebook Subset step 357 uses the degree-of-periodicity computed by Degree of Periodicity Calculation Means 30 to select the codebook set which corresponds to the identified class for the speech segment under analysis.
- Encode Inphase step 358 then encodes the inphase component computed by FFT step 356 using the codebook subset identified by Select Codebook Subset step 357.
- Encode Inphase step 358 encodes the inphase data using quantization methods such as VQ, split VQ, multi-stage VQ, wavelet VQ, TCQ, and wavelet TCQ implementations, producing one or more code indices.
- Encode Quadrature step 359 then encodes the quadrature component computed by FFT step 356 using the codebook subset identified by Select Codebook Subset step 357.
- Encode Quadrature step 359 encodes the quadrature data using quantization methods such as VQ, split VQ, multi-stage VQ, TCQ, and wavelet TCQ implementations, producing one or more code indices. While a preferred embodiment of Ensemble Error Encoding Means 350 encodes inphase and quadrature vectors, alternate embodiments could also be appropriate, such as magnitude and phase representations.
- Ensemble Error Encoding Means 350 is followed by Standard Deviation Error Encoding Means 390, Mean Error Encoding Means 420, and Phase Error Encoding Means 460, each of which produces at least one code index corresponding to the standard deviation error, mean error, and phase error, respectively, computed by Error Component Computation Means 310.
- FIG. 27 illustrates a method for encoding standard deviation error, mean error, or phase error in accordance with a preferred embodiment of the present invention.
- the method is performed by Standard Deviation Error Encoding Means 390, Mean Error Encoding Means 420, and Phase Error Encoding Means 460.
- the method begins by determining, in step 391, whether Numepoch>1, where Numepoch corresponds to the number of epochs in the current frame under analysis, as calculated by Tx Epoch Location Estimation Means 110.
- Upsample step 392 upsamples the error vector to a common vector length.
- Upsample step 392 upsamples the error vector, which initially has Numepoch samples, to a common length equal to the maximum number of epochs allowed per frame (e.g., twelve, although other normalizing lengths could also be appropriate).
- Select Codebook Subset step 393 is performed, which uses the degree-of-periodicity computed by Degree of Periodicity Calculation Means 30 to select the codebook set which corresponds to the identified class for the speech segment under analysis. If the number of epochs is equal to one, the codebook subset can also include a scalar quantizer corresponding to the single standard deviation error value, mean error value, or phase error value.
- Encode Vector step 394 then encodes the standard deviation error vector, mean error vector, or phase error vector, or scalar values corresponding to those error vectors using quantization methods such as VQ, split VQ, multi-stage VQ, wavelet VQ, TCQ, and wavelet TCQ implementations, producing one or more code indices. The procedure then ends.
- Phase Error Encoding Means 460 is followed by Degree of Periodicity Encoding Means 465, which scalar quantizes the degree of periodicity produced by Degree of Periodicity Calculation Means 30, producing a code index.
- Degree of Periodicity Encoding Means 465 is followed by Modulation and Channel Interface Means 470, which constructs a modulated bitstream corresponding to the encoded data using methods well known to those of skill in the art.
- the modulated data bitstream is transmitted via Modulation and Channel Interface Means 470 to Channel 475, where the channel can be any communication medium, including fiber, RF, or coaxial cable, although other media are also appropriate.
- Synthesis Processor 900 receives the modulated, transmitted bitstream via Channel 475, and demodulates the data using Channel Interface and Demodulation Means 480 using techniques well known to those of skill in the art, producing code indices corresponding to the code indices generated by Analysis Processor 100.
- Channel Interface and Demodulation Means 480 is followed by several decoding steps which utilize code indices produced by Channel Interface and Demodulation Means 480.
- Target Location Decoding Means 485 decodes the target location, producing an integer target location.
- Frequency Decoding Means 490 then decodes the number of epochs, producing an integer number of epochs.
- Degree of Periodicity Decoding Means 495 decodes the degree of periodicity, producing a discrete degree of periodicity class.
- Target Standard Deviation Decoding Means 500 then decodes the target standard deviation, producing a target standard deviation value.
- Target Standard Deviation Decoding Means 500 selects from codebooks corresponding to each of the discrete degrees-of-periodicity produced by Degree of Periodicity Decoding Means 495, although a non-class-based approach could also be appropriate.
- Target Standard Deviation Decoding Means 500 is followed by Target Mean Decoding Means 505, producing a target mean value.
- Target Mean Decoding Means 505 selects from codebooks corresponding to each of the discrete degrees-of-periodicity produced by Degree of Periodicity Decoding Means 495, although a non-class-based approach could also be appropriate.
- Decode Target Mean 505 is followed by Standard Deviation Error Decoding Means 510, Mean Error Decoding Means 550, and Phase Error Decoding Means 590. Each of these Decoding Means 510, 550, and 590 perform a similar decoding process.
- Channel Interface and Demodulation Means 480 produces at least one code index which is decoded by Standard Deviation Error Decoding Means 510, Mean Error Decoding Means 550, and Phase Error Decoding Means 590 to produce a standard deviation error, mean error, and phase error, respectively.
- FIG. 28 illustrates a method for decoding standard deviation error, mean error, and phase error in accordance with a preferred embodiment of the present invention as performed by Standard Deviation Error Decoding Means 510.
- the method begins with Select Codebook Subset step 511, which uses the degree-of-periodicity produced by Degree of Periodicity Decoding Means 495 to select the codebook set which corresponds to the identified class for the speech segment being synthesized.
- Decode Vector step 512 decodes the error vectors or scalar values using the companion codebook subset of Standard Deviation Error Encoding Means 390, Mean Error Encoding Means 420, and Phase Error Encoding Means 460, respectively.
- decoding is performed using quantization methods such as VQ, split VQ, multi-stage VQ, wavelet VQ, TCQ, and wavelet TCQ implementations, producing the decoded, length-normalized standard deviation error vector, mean error vector, phase error vector, or scalar values for these errors.
- step 513 A determination is made, in step 513, whether the number of epochs produced by Frequency Decoding Means 490 exceeds one. If not, the procedure ends. If so, Downsample step 514 is performed, which downsamples each error vector to a number of samples equal to the number of epochs within the frame to be synthesized using methods well known to those of skill in the art, producing downsampled error vectors. The procedure then ends.
- Phase Error Decoding Means 590 is followed by Rx Epoch Location Computation Means 630, which uses the decoded target location from Target Location Decoding Means 485, and the decoded number of epochs from Frequency Decoding Means 490, to calculate the synthesized epoch locations, producing a sequence of epoch locations with an effective normalized pitch for each epoch within one sample of the average pitch.
- FIG. 29 illustrates a method for computing receiver epoch locations in accordance with a preferred embodiment of the present invention as performed by Rx Epoch Location Computation Means 630.
- the method begins with Load Target Index step 631, which loads from memory the target index, ti, determined by Target Location Decoding Means 485.
- Load Source Index step 632 loads from memory the previous target index, and subtracts the frame length to form the source index, si.
- Store First Location step 634 sets a location pointer, L, to the first epoch location, corresponding to the source index, si.
- Increment Location by Pitch step 635 increments the location pointer by the pitch estimate, P, forming a next location estimate.
- the location estimate, L is rounded to the nearest integer to reflect a proper sample index in Round Location to Nearest Integer step 636.
- the location index is stored to memory by Store Location step 637. A determination is then made whether all locations have been computed. If not, the procedure iterates as shown in FIG. 29. If all locations have been computed and stored to memory, the procedure ends.
- Rx Epoch Location Computation Means 630 is followed by Target Decoding Means 670, which uses the one or more codebook indices produced by Channel Interface and Demodulation Means 480 along with the degree of periodicity class produced by Degree of Periodicity Decoding Means 495, to produce an energy normalized target.
- FIG. 30 illustrates a method for decoding target data in accordance with a preferred embodiment of the present invention as performed by Target Decoding Means 670.
- the method begins with Select Codebook Subset step 671, which uses the degree-of-periodicity produced by Degree of Periodicity Decoding Means 495 to select the codebook set which corresponds to the identified class for the speech segment being synthesized.
- Decode Inphase step 672 uses the codebook subset for inphase vectors along with one or more code indices to produce the decoded target inphase vector.
- Decode Quadrature step 673 then uses the codebook subset for quadrature vectors along with one or more code indices to produce the decoded target quadrature vector.
- both steps 672 and 673 use quantization methods such as VQ, split VQ, multi-stage VQ, wavelet VQ, TCQ, and wavelet TCQ methods.
- Compute Conjugate Spectrum step 674 uses the quantized inphase vector and quantized quadrature vector to produce the conjugate FFT spectrum, which will be familiar to those of skill in the art.
- Compute Conjugate Spectrum step 674 produces M Inphase samples and M quadrature samples, where M corresponds to the FFT size used by Target Encoding Means 270.
- Inverse FFT step 675 performs an inverse M-point FFT on the M-point inphase and M-point quadrature vectors to produce the cyclically-shifted, energy-normalized target vector.
- Inverse Cyclic Transform step 676 then applies the inverse process described in conjunction with Cyclic Transform step 274 (FIG. 18) which was applied by Target Encoding Means 270, producing an energy-normalized target vector in the inphase data array.
- step 677 A determination is then made, in step 677, whether the actual target length, Targlength, exceeds M samples. If so, Upsample step 678 is performed, which upsamples the M sample target to the desired target length. After Upsample step 678, or if the actual target length does not exceed M samples, the procedure ends.
- Target Decoding Means 670 docodes inphase and quadrature vectors, alternate embodiments could also be appropriate, such as magnitude and phase representations.
- Target Decoding Means 670 is followed by Ensemble Error Decoding Means 710.
- FIG. 31 illustrates a method for decoding ensemble error in accordance with a preferred embodiment of the present invention as performed by Ensemble Error Decoding Means 710.
- the method begins with Select Codebook Subset step 711, which uses the degree-of-periodicity produced by Degree of Periodicity Decoding Means 495 to select the codebook set which corresponds to the identified class for the speech segment being synthesized.
- Decode Inphase step 712 then uses the codebook subset for inphase error vectors along with one or more code indices to produce the decoded error inphase vector.
- Decode Quadrature step 713 uses the codebook subset for error quadrature vectors along with one or more code indices to produce the decoded error quadrature vector.
- both steps 712 and 713 use quantization methods such as VQ, split VQ, multi-stage VQ, wavelet VQ, TCQ, and wavelet TCQ methods.
- Lowpass characterization filtering of the ensemble error preserves a relatively high level speech quality and speaker recognition. However, characterization filtering discards the ensemble error waveform's high frequency components, which can contribute to perceived quality.
- post-processing methods can be introduced which enhance speech quality without sacrificing bandwidth.
- perceived quality is improved in the face of error waveform filtering by simulating high frequency Inphase and Quadrature components which were discarded at the transmitter. Since Inphase and Quadrature vectors are reconstructed at the receiver, little computation effort is required for such methods.
- Modulo-F Cyclic Repetition step 715 represents a post-process which ultimately improves synthesized speech quality without the use of additional transmission bandwidth.
- FIG. 32 illustrates error component reconstruction using modulo-F cyclic repetition derived in accordance with a preferred embodiment of the present invention as performed by Modulo-F Cyclic Repetition step 715.
- the filtered ensemble error is cyclically repeated at modulo-F intervals, where F represents the characterization filter cutoff.
- F represents the characterization filter cutoff.
- sign is changed with each successive cycle.
- a linear trapezoidal weighting is applied across the synthesized upper frequencies in order to reduce high frequency energy. This technique provides an improvement in quality which is manifest in an apparent "brightening" of the synthesized speech.
- Quadrature data is modified in the same manner as the inphase data of FIG. 32.
- FIG. 33 illustrates error component reconstruction using modulo-F cyclic repetition plus noise derived in accordance with an alternate embodiment of the present invention, which incorporates modulo-F cyclic repetition plus scaled gaussian noise.
- This technique provides the greatest speech quality improvement for aperiodic speech, determined by the degree-of-periodicity class.
- Noise power can be proportional to the baseband energy, although other noise power levels could also be appropriate.
- the noise power can be proportional to the degree of periodicity class produced by Degree of Periodicity Decoding Means 495.
- this method relies upon classification to perform optimally, classification errors do not significantly impact the synthesized result. Since baseband error information is always preserved, high classification accuracy is not critical to success of the method. Hence, the method can be used with or without degree-of-periodicity class control.
- Compute Conjugate Spectrum step 716 is performed, which uses the inphase vector and quadrature vector to produce the conjugate FFT spectrum, which will be familiar to those of skill in the art.
- Compute Conjugate Spectrum step 716 produces the same number of inphase samples and quadrature samples used to transform the ensemble error component by Ensemble Error Encoding Means 350 (FIG. 1).
- Inverse FFT step 717 performs an inverse FFT on the inphase and quadrature components, producing a time domain, pitch-normalized ensemble error vector. A determination is made, in step 718 whether the error waveform spectral model embodiment is used. If not, the procedure ends.
- step 718 determines that an error waveform spectral model is used
- the procedure would branch to Compute Conjugate Spectrum step.
- Compute Conjugate Spectrum step 716 and Inverse FFT step 717 are computed as described above, producing the spectral error model excitation waveform.
- step 718 determines that an error waveform spectral model is used
- Decode Spectrum step 719 is performed, which decodes the spectral error model parameters using companion codebooks to those implemented in conjunction with Encode Spectrum step 354 (FIG. 20) of Ensemble Error Encoding Means 350.
- Prediction Filter step 720 uses the spectral error model parameters produced by Decode Spectrum step 719 and the spectral error model excitation waveform produced by Inverse FFT step 717 to produce the ensemble error waveform, using prediction filter methods well known to those of skill in the art.
- FFT step 721 transforms the ensemble error model to produce error waveform inphase and quadrature components.
- Compute Conjugate Spectrum step 723 computes the negative spectral components in the same manner as Compute Conjugate Spectrum step 716 discussed above, producing modified inphase and quadrature components.
- Inverse FFT step 724 then performs an inverse FFT upon the modified inphase and quadrature components, producing a time domain, pitch-normalized ensemble error vector. While a preferred embodiment of Ensemble Error Decoding Means 710 decodes inphase and quadrature vectors, alternate embodiments could also be appropriate, such as magnitude and phase representations.
- Spectrum Decoding Means 750 uses the one or more code indices produced by Channel Interface and Demodulation Means 480 and the companion codebooks to Spectrum Encoding Means 155 to produce quantized spectral parameters.
- Spectrum Decoding Means 750 selects from codebooks corresponding to each of the discrete degrees-of-periodicity produced by Degree of Periodicity Decoding Means 495, although a non-class-based approach could also be appropriate.
- Spectrum Decoding Means 750 is followed by Excitation Estimate Computation Means 760, which uses the decoded excitation model and model error components to produce a complete excitation estimate for the speech segment being synthesized.
- FIG. 34 illustrates a method for computing an excitation estimate in accordance with a preferred embodiment of the present invention as performed by Excitation Estimate Computation Means 760.
- the method begins with Pitch Normalize Target step 761.
- the effective "local" pitch length i.e., the pitch for the current frame
- Pitch Normalize Target step 761 can upsample or downsample the segment to a second "global" normalizing length (i.e., a common pitch length for all frames), producing a unit variance, zero mean target vector with a normalized length.
- Upsampling of segments to an arbitrarily large value in this fashion has proven to be of value in epoch-to-epoch alignment and error computation, although downsampling to a smaller length could also be of value.
- Pitch Normalize Target step 761 the normalized target is correlated against the normalized target from the previous frame, or the "source”, which was stored to memory in an earlier operation.
- Correlate Source-Target step 762 hence produces an optimal alignment offset and cyclically shifts the target epoch in order to maximize ensemble correlation, producing a quantized, normalized, shifted target.
- Phase Shift step 763 cyclically shifts each epoch in the ensemble error waveform produced by Ensemble Error Decoding Means 710 (FIG. 1), by the respective optimal offset produced by Phase Error Decoding Means 590 (FIG. 1), in order to align the epoch synchronous error components and interpolation model, producing an aligned ensemble error waveform.
- Ensemble Interpolate step 764 then produces an ensemble interpolated excitation waveform (i.e., the interpolation model) by ensemble point-to-point interpolation from the source to the shifted target, producing intervening epochs corresponding to the number of epochs in the analysis segment, minus one (i.e., the target).
- Ensemble Error step 765 adds the aligned ensemble error waveform to the ensemble interpolated waveform to produce a pitch-normalized, energy-normalized, shifted excitation waveform.
- Interpolate Standard Deviation step 766 then computes a standard deviation model for the speech segment to be synthesized.
- a linear interpolation model is used, whereby the source standard deviation, stored in an earlier operation, is interpolated linearly to the standard deviation of the target, produced by Target Standard Deviation Decoding Means 500 (FIG. 1), producing interpolated values of standard deviation for each elided epoch in the frame.
- Other non-linear standard deviation models could also be appropriate.
- Standard Deviation Error step 767 combines the standard deviation model with the decoded standard deviation error produced by Standard Deviation Error Decoding Means 510 (FIG. 1), producing an epoch-synchronous standard deviation estimate.
- Standard deviation error can be applied in either an additive fashion or multiplicative fashion, depending on the method implemented by Error Component Computation Means 310 (FIG. 1).
- Interpolate Mean step 768 computes a mean model for the speech segment to be synthesized.
- a linear interpolation model is used, whereby the source mean, stored in an earlier operation, is interpolated linearly to the mean of the target, produced by Target Mean Decoding Means 505 (FIG. 1), producing interpolated values of mean for each elided epoch in the frame.
- Other non-linear standard deviation models could also be appropriate.
- Apply Mean Error step 769 is then performed, which combines the mean model with the decoded mean error produced by Mean Error Decoding Means 550, producing an epoch-synchronous mean estimate.
- Mean error could be applied in either an additive fashion or multiplicative fashion, depending on the method implemented by Error Component Computation Means 310 (FIG. 1).
- Store Target step 770 stores the target epoch to memory, first shifting it to the original target position, effectively producing the "source” epoch for later use.
- Store Target step 770 also stores the target mean and standard deviation to memory, effectively forming the "source” mean and "source” standard deviation.
- Phase Shift step 771 then cyclically shifts each epoch-synchronous excitation segment to it's pre-aligned, unshifted state prior to speech synthesis, using the inverse phase operation to that employed by the earlier-described Phase Shift step 763, producing an energy-normalized, pitch-normalized excitation waveform.
- Denormalize Pitch step 772 epoch-synchronously downsamples each epoch in the energy-normalized, pitch-normalized excitation waveform to the epoch lengths defined by Rx Epoch Location Computation Means 630 (FIG. 1), producing an energy-normalized excitation waveform.
- Denormalize Energy step 773 then multiplies each epoch by the epoch-synchronous standard deviation estimate produced by Apply Standard Deviation Error step 767, and adds the epoch-synchronous mean estimate produced by Apply Mean Error step 769 to produce an excitation estimate. The procedure then ends.
- an alternate embodiment can also include computation of the excitation estimate using a standard-deviation-normalized model and corresponding error waveforms.
- Speech Synthesis Means 800 uses the excitation estimate to reconstruct high-quality speech.
- Speech Synthesis Means 800 can consist of direct form or lattice synthesis filters which implement the reconstructed excitation waveform and LPC prediction coefficients or reflection coefficients.
- Post Processing Means 810 consists of signal post processing methods well known to those of skill in the art, including adaptive post filtering techniques and spectral tilt re-introduction. Reconstructed, post-processed digitally-sampled speech from Post Processing Means 810 can then be converted to an analog signal via D/A Converter 811 and output to Audio Output Device 901, producing output speech audio. Alternatively, the digital signal or analog signal can be stored to an appropriate storage medium.
- variable rate embodiments can be developed from a preferred embodiment via a simple change of codebooks. In this fashion, the same algorithm is used across multiple data rates.
- variable-rate implementation of the invention simplifies hardware and software requirements in systems that require multiple data rates, improves performance in environments with widely varying interference conditions, and provides for improved bandwidth utilization in multi-channel applications.
- VQ, split VQ, wavelet VQ, TCQ, wavelet TCQ, or multi-stage VQ codebooks can be developed with varying bit allocations at each desired level of bandwidth.
- multi-stage vector quantizers can be developed which incorporate multiple stages that correspond to higher levels of bandwidth. In this manner, low-level stages can be omitted at low bit rates, with a corresponding drop in speech quality. Higher bit rate implementations would use more of the multi-stage VQ stages to achieve higher speech quality. Hence, multi-stage VQ implementations would provide for rapid change in data rate.
- variable rate vocoder should achieve near transparent speech quality by full application of codebooks of all modeled parameters.
- codebook allocations can be reduced, or specific non-critical parameters can be discarded to meet system bandwidth requirements. In this manner, the bandwidth formerly allocated to those parameters can be used for other purposes.
- the invention can be used to open multiple channels in a fixed bandwidth by reducing the bandwidth allocated to each channel.
- the multi-rate embodiment would also be useful in high interference environments, whereby more channel bandwidth is allocated toward FEC in order to preserve intelligibility.
Abstract
Description
ρ.sub.i >ρ.sub.max *α, where α=0.9, for example.
(EQ 1) H.sub.opt (ω)=KX*(ω)e.sup.-jαT
(EQ 2) h.sub.opt (t)=Kx*(T-t)
Claims (64)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US08/651,172 US5809459A (en) | 1996-05-21 | 1996-05-21 | Method and apparatus for speech excitation waveform coding using multiple error waveforms |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US08/651,172 US5809459A (en) | 1996-05-21 | 1996-05-21 | Method and apparatus for speech excitation waveform coding using multiple error waveforms |
Publications (1)
Publication Number | Publication Date |
---|---|
US5809459A true US5809459A (en) | 1998-09-15 |
Family
ID=24611854
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US08/651,172 Expired - Lifetime US5809459A (en) | 1996-05-21 | 1996-05-21 | Method and apparatus for speech excitation waveform coding using multiple error waveforms |
Country Status (1)
Country | Link |
---|---|
US (1) | US5809459A (en) |
Cited By (31)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5905814A (en) * | 1996-07-29 | 1999-05-18 | Matsushita Electric Industrial Co., Ltd. | One-dimensional time series data compression method, one-dimensional time series data decompression method |
US5960387A (en) * | 1997-06-12 | 1999-09-28 | Motorola, Inc. | Method and apparatus for compressing and decompressing a voice message in a voice messaging system |
US6012025A (en) * | 1998-01-28 | 2000-01-04 | Nokia Mobile Phones Limited | Audio coding method and apparatus using backward adaptive prediction |
WO2000038177A1 (en) * | 1998-12-21 | 2000-06-29 | Qualcomm Incorporated | Periodic speech coding |
US6098037A (en) * | 1998-05-19 | 2000-08-01 | Texas Instruments Incorporated | Formant weighted vector quantization of LPC excitation harmonic spectral amplitudes |
US6199041B1 (en) * | 1998-11-20 | 2001-03-06 | International Business Machines Corporation | System and method for sampling rate transformation in speech recognition |
US6292777B1 (en) * | 1998-02-06 | 2001-09-18 | Sony Corporation | Phase quantization method and apparatus |
US6327564B1 (en) * | 1999-03-05 | 2001-12-04 | Matsushita Electric Corporation Of America | Speech detection using stochastic confidence measures on the frequency spectrum |
US6396421B1 (en) * | 2001-07-31 | 2002-05-28 | Wind River Systems, Inc. | Method and system for sampling rate conversion in digital audio applications |
US6453283B1 (en) * | 1998-05-11 | 2002-09-17 | Koninklijke Philips Electronics N.V. | Speech coding based on determining a noise contribution from a phase change |
US20020193987A1 (en) * | 2001-01-12 | 2002-12-19 | Sandra Hutchins | Variable rate speech data compression |
US20030055633A1 (en) * | 2001-06-21 | 2003-03-20 | Heikkinen Ari P. | Method and device for coding speech in analysis-by-synthesis speech coders |
US6556967B1 (en) * | 1999-03-12 | 2003-04-29 | The United States Of America As Represented By The National Security Agency | Voice activity detector |
US20040008279A1 (en) * | 2002-07-12 | 2004-01-15 | Samsung Electronics Co., Ltd. | Apparatus and method for signal processing in digital video system |
US6721282B2 (en) | 2001-01-12 | 2004-04-13 | Telecompression Technologies, Inc. | Telecommunication data compression apparatus and method |
US20040093205A1 (en) * | 2002-11-08 | 2004-05-13 | Ashley James P. | Method and apparatus for coding gain information in a speech coding system |
US20040181399A1 (en) * | 2003-03-15 | 2004-09-16 | Mindspeed Technologies, Inc. | Signal decomposition of voiced speech for CELP speech coding |
US6826526B1 (en) * | 1996-07-01 | 2004-11-30 | Matsushita Electric Industrial Co., Ltd. | Audio signal coding method, decoding method, audio signal coding apparatus, and decoding apparatus where first vector quantization is performed on a signal and second vector quantization is performed on an error component resulting from the first vector quantization |
US20050049855A1 (en) * | 2003-08-14 | 2005-03-03 | Dilithium Holdings, Inc. | Method and apparatus for frame classification and rate determination in voice transcoders for telecommunications |
US20050060147A1 (en) * | 1996-07-01 | 2005-03-17 | Takeshi Norimatsu | Multistage inverse quantization having the plurality of frequency bands |
US6895379B2 (en) | 2002-03-27 | 2005-05-17 | Sony Corporation | Method of and apparatus for configuring and controlling home entertainment systems through natural language and spoken commands using a natural language server |
US20050273319A1 (en) * | 2004-05-07 | 2005-12-08 | Christian Dittmar | Device and method for analyzing an information signal |
US20070027680A1 (en) * | 2005-07-27 | 2007-02-01 | Ashley James P | Method and apparatus for coding an information signal using pitch delay contour adjustment |
US20070162236A1 (en) * | 2004-01-30 | 2007-07-12 | France Telecom | Dimensional vector and variable resolution quantization |
US20080312917A1 (en) * | 2000-04-24 | 2008-12-18 | Qualcomm Incorporated | Method and apparatus for predictively quantizing voiced speech |
US20150238091A1 (en) * | 2014-02-24 | 2015-08-27 | Covidien Lp | Photoacoustic monitoring technique with noise reduction |
US20170110132A1 (en) * | 2014-07-01 | 2017-04-20 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Audio processor and method for processing an audio signal using vertical phase correction |
US9692774B2 (en) * | 2010-10-12 | 2017-06-27 | Sonus Networks, Inc. | Real-time network attack detection and mitigation infrastructure |
US9917706B1 (en) * | 2015-05-27 | 2018-03-13 | Hewlett-Packard Development Company, L.P. | Recovering independent waveforms from input bitstream data |
US11581001B2 (en) | 2006-12-12 | 2023-02-14 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Encoder, decoder and methods for encoding and decoding data segments representing a time-domain data stream |
US11669667B2 (en) * | 2020-02-21 | 2023-06-06 | Synopsys, Inc. | Automatic test pattern generation (ATPG) for parametric faults |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH05265499A (en) * | 1992-03-18 | 1993-10-15 | Sony Corp | High-efficiency encoding method |
JPH05265496A (en) * | 1992-03-18 | 1993-10-15 | Hitachi Ltd | Speech encoding method with plural code books |
JPH06222797A (en) * | 1993-01-22 | 1994-08-12 | Nec Corp | Voice encoding system |
US5479559A (en) * | 1993-05-28 | 1995-12-26 | Motorola, Inc. | Excitation synchronous time encoding vocoder and method |
US5504834A (en) * | 1993-05-28 | 1996-04-02 | Motrola, Inc. | Pitch epoch synchronous linear predictive coding vocoder and method |
US5596676A (en) * | 1992-06-01 | 1997-01-21 | Hughes Electronics | Mode-specific method and apparatus for encoding signals containing speech |
US5602959A (en) * | 1994-12-05 | 1997-02-11 | Motorola, Inc. | Method and apparatus for characterization and reconstruction of speech excitation waveforms |
-
1996
- 1996-05-21 US US08/651,172 patent/US5809459A/en not_active Expired - Lifetime
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH05265499A (en) * | 1992-03-18 | 1993-10-15 | Sony Corp | High-efficiency encoding method |
JPH05265496A (en) * | 1992-03-18 | 1993-10-15 | Hitachi Ltd | Speech encoding method with plural code books |
US5596676A (en) * | 1992-06-01 | 1997-01-21 | Hughes Electronics | Mode-specific method and apparatus for encoding signals containing speech |
JPH06222797A (en) * | 1993-01-22 | 1994-08-12 | Nec Corp | Voice encoding system |
US5479559A (en) * | 1993-05-28 | 1995-12-26 | Motorola, Inc. | Excitation synchronous time encoding vocoder and method |
US5504834A (en) * | 1993-05-28 | 1996-04-02 | Motrola, Inc. | Pitch epoch synchronous linear predictive coding vocoder and method |
US5579437A (en) * | 1993-05-28 | 1996-11-26 | Motorola, Inc. | Pitch epoch synchronous linear predictive coding vocoder and method |
US5623575A (en) * | 1993-05-28 | 1997-04-22 | Motorola, Inc. | Excitation synchronous time encoding vocoder and method |
US5602959A (en) * | 1994-12-05 | 1997-02-11 | Motorola, Inc. | Method and apparatus for characterization and reconstruction of speech excitation waveforms |
Non-Patent Citations (14)
Title |
---|
Allen Gersho, "Advances in Speech and Audio Compression," Proc. IEEE, pp. 900-918, Jun. 1994. |
Allen Gersho, Advances in Speech and Audio Compression, Proc. IEEE, pp. 900 918, Jun. 1994. * |
An article entitled "Numerical Recipes In C" by William H. Press et al., Cambridge University Press, copyright 1988, (87-33844). |
An article entitled "Probability, Random Variables, And Random Signal Principles" by Peyton Z. Peebles, Jr., Ph.D., McGraw-Hill Book Company, Second Edition, Copyright 1987, 1980, (ISBN 0-07-049210-0). |
An article entitled "Statistical Theory Of Communication" by Y. W. Lee, John Wiley & Sons, Inc., Second Printing, Dec. 1961, (60-10318). |
An article entitled "Transformation and Decomposition of the Speech Signal For Coding", by W. Kleign and J. Haagen from IEEE Signal Processing Letters, vol. 1, No. 9, Sep. 1994. |
An article entitled Numerical Recipes In C by William H. Press et al., Cambridge University Press, copyright 1988, (87 33844). * |
An article entitled Probability, Random Variables, And Random Signal Principles by Peyton Z. Peebles, Jr., Ph.D., McGraw Hill Book Company, Second Edition, Copyright 1987, 1980, (ISBN 0 07 049210 0). * |
An article entitled Statistical Theory Of Communication by Y. W. Lee, John Wiley & Sons, Inc., Second Printing, Dec. 1961, (60 10318). * |
An article entitled Transformation and Decomposition of the Speech Signal For Coding , by W. Kleign and J. Haagen from IEEE Signal Processing Letters, vol. 1, No. 9, Sep. 1994. * |
Andreas S. Spanias, "Speech Coding: A Tutorial Review," Proc. IEEE, pp. 1541-1582, Oct. 1994. |
Andreas S. Spanias, Speech Coding: A Tutorial Review, Proc. IEEE, pp. 1541 1582, Oct. 1994. * |
Peter Noll, "Digital Audio Coding for Visual Communications," IEEE, pp. 925-943, Jun. 1995. |
Peter Noll, Digital Audio Coding for Visual Communications, IEEE, pp. 925 943, Jun. 1995. * |
Cited By (59)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6904404B1 (en) * | 1996-07-01 | 2005-06-07 | Matsushita Electric Industrial Co., Ltd. | Multistage inverse quantization having the plurality of frequency bands |
US6826526B1 (en) * | 1996-07-01 | 2004-11-30 | Matsushita Electric Industrial Co., Ltd. | Audio signal coding method, decoding method, audio signal coding apparatus, and decoding apparatus where first vector quantization is performed on a signal and second vector quantization is performed on an error component resulting from the first vector quantization |
US20050060147A1 (en) * | 1996-07-01 | 2005-03-17 | Takeshi Norimatsu | Multistage inverse quantization having the plurality of frequency bands |
US7243061B2 (en) | 1996-07-01 | 2007-07-10 | Matsushita Electric Industrial Co., Ltd. | Multistage inverse quantization having a plurality of frequency bands |
US5905814A (en) * | 1996-07-29 | 1999-05-18 | Matsushita Electric Industrial Co., Ltd. | One-dimensional time series data compression method, one-dimensional time series data decompression method |
US5960387A (en) * | 1997-06-12 | 1999-09-28 | Motorola, Inc. | Method and apparatus for compressing and decompressing a voice message in a voice messaging system |
US6012025A (en) * | 1998-01-28 | 2000-01-04 | Nokia Mobile Phones Limited | Audio coding method and apparatus using backward adaptive prediction |
US6292777B1 (en) * | 1998-02-06 | 2001-09-18 | Sony Corporation | Phase quantization method and apparatus |
US6453283B1 (en) * | 1998-05-11 | 2002-09-17 | Koninklijke Philips Electronics N.V. | Speech coding based on determining a noise contribution from a phase change |
US6098037A (en) * | 1998-05-19 | 2000-08-01 | Texas Instruments Incorporated | Formant weighted vector quantization of LPC excitation harmonic spectral amplitudes |
US6199041B1 (en) * | 1998-11-20 | 2001-03-06 | International Business Machines Corporation | System and method for sampling rate transformation in speech recognition |
US6456964B2 (en) | 1998-12-21 | 2002-09-24 | Qualcomm, Incorporated | Encoding of periodic speech using prototype waveforms |
WO2000038177A1 (en) * | 1998-12-21 | 2000-06-29 | Qualcomm Incorporated | Periodic speech coding |
US6327564B1 (en) * | 1999-03-05 | 2001-12-04 | Matsushita Electric Corporation Of America | Speech detection using stochastic confidence measures on the frequency spectrum |
US6556967B1 (en) * | 1999-03-12 | 2003-04-29 | The United States Of America As Represented By The National Security Agency | Voice activity detector |
US8660840B2 (en) * | 2000-04-24 | 2014-02-25 | Qualcomm Incorporated | Method and apparatus for predictively quantizing voiced speech |
US20080312917A1 (en) * | 2000-04-24 | 2008-12-18 | Qualcomm Incorporated | Method and apparatus for predictively quantizing voiced speech |
US20020193987A1 (en) * | 2001-01-12 | 2002-12-19 | Sandra Hutchins | Variable rate speech data compression |
US6721282B2 (en) | 2001-01-12 | 2004-04-13 | Telecompression Technologies, Inc. | Telecommunication data compression apparatus and method |
US6952669B2 (en) * | 2001-01-12 | 2005-10-04 | Telecompression Technologies, Inc. | Variable rate speech data compression |
US20030055633A1 (en) * | 2001-06-21 | 2003-03-20 | Heikkinen Ari P. | Method and device for coding speech in analysis-by-synthesis speech coders |
US7089180B2 (en) * | 2001-06-21 | 2006-08-08 | Nokia Corporation | Method and device for coding speech in analysis-by-synthesis speech coders |
US6509850B1 (en) | 2001-07-31 | 2003-01-21 | Wind River Systems, Inc. | Method and system for sampling rate conversion in digital audio applications |
US6661357B2 (en) | 2001-07-31 | 2003-12-09 | Wind River Systems, Inc. | System for sampling rate conversion in digital audio applications |
US6396421B1 (en) * | 2001-07-31 | 2002-05-28 | Wind River Systems, Inc. | Method and system for sampling rate conversion in digital audio applications |
US6895379B2 (en) | 2002-03-27 | 2005-05-17 | Sony Corporation | Method of and apparatus for configuring and controlling home entertainment systems through natural language and spoken commands using a natural language server |
US20040008279A1 (en) * | 2002-07-12 | 2004-01-15 | Samsung Electronics Co., Ltd. | Apparatus and method for signal processing in digital video system |
US7148931B2 (en) * | 2002-07-12 | 2006-12-12 | Samsung Electronics Co., Ltd. | Apparatus and method for signal processing in digital video system |
US7047188B2 (en) | 2002-11-08 | 2006-05-16 | Motorola, Inc. | Method and apparatus for improvement coding of the subframe gain in a speech coding system |
US20040093205A1 (en) * | 2002-11-08 | 2004-05-13 | Ashley James P. | Method and apparatus for coding gain information in a speech coding system |
WO2004044892A1 (en) * | 2002-11-08 | 2004-05-27 | Motorola, Inc. | Method and apparatus for coding gain information in a speech coding system |
US20040181399A1 (en) * | 2003-03-15 | 2004-09-16 | Mindspeed Technologies, Inc. | Signal decomposition of voiced speech for CELP speech coding |
US7529664B2 (en) * | 2003-03-15 | 2009-05-05 | Mindspeed Technologies, Inc. | Signal decomposition of voiced speech for CELP speech coding |
US7469209B2 (en) * | 2003-08-14 | 2008-12-23 | Dilithium Networks Pty Ltd. | Method and apparatus for frame classification and rate determination in voice transcoders for telecommunications |
US20050049855A1 (en) * | 2003-08-14 | 2005-03-03 | Dilithium Holdings, Inc. | Method and apparatus for frame classification and rate determination in voice transcoders for telecommunications |
US20070162236A1 (en) * | 2004-01-30 | 2007-07-12 | France Telecom | Dimensional vector and variable resolution quantization |
US7680670B2 (en) * | 2004-01-30 | 2010-03-16 | France Telecom | Dimensional vector and variable resolution quantization |
US7565213B2 (en) * | 2004-05-07 | 2009-07-21 | Gracenote, Inc. | Device and method for analyzing an information signal |
US20090265024A1 (en) * | 2004-05-07 | 2009-10-22 | Gracenote, Inc., | Device and method for analyzing an information signal |
US8175730B2 (en) | 2004-05-07 | 2012-05-08 | Sony Corporation | Device and method for analyzing an information signal |
US20050273319A1 (en) * | 2004-05-07 | 2005-12-08 | Christian Dittmar | Device and method for analyzing an information signal |
US20070027680A1 (en) * | 2005-07-27 | 2007-02-01 | Ashley James P | Method and apparatus for coding an information signal using pitch delay contour adjustment |
US9058812B2 (en) * | 2005-07-27 | 2015-06-16 | Google Technology Holdings LLC | Method and system for coding an information signal using pitch delay contour adjustment |
US11961530B2 (en) | 2006-12-12 | 2024-04-16 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E. V. | Encoder, decoder and methods for encoding and decoding data segments representing a time-domain data stream |
US11581001B2 (en) | 2006-12-12 | 2023-02-14 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Encoder, decoder and methods for encoding and decoding data segments representing a time-domain data stream |
US9692774B2 (en) * | 2010-10-12 | 2017-06-27 | Sonus Networks, Inc. | Real-time network attack detection and mitigation infrastructure |
US20150238091A1 (en) * | 2014-02-24 | 2015-08-27 | Covidien Lp | Photoacoustic monitoring technique with noise reduction |
TWI587289B (en) * | 2014-07-01 | 2017-06-11 | 弗勞恩霍夫爾協會 | Calculator and method for determining phase correction data for an audio signal |
US10140997B2 (en) | 2014-07-01 | 2018-11-27 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Decoder and method for decoding an audio signal, encoder and method for encoding an audio signal |
US10192561B2 (en) | 2014-07-01 | 2019-01-29 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Audio processor and method for processing an audio signal using horizontal phase correction |
US20190108849A1 (en) * | 2014-07-01 | 2019-04-11 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Audio processor and method for processing an audio signal using vertical phase correction |
US10283130B2 (en) | 2014-07-01 | 2019-05-07 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Audio processor and method for processing an audio signal using vertical phase correction |
US10529346B2 (en) | 2014-07-01 | 2020-01-07 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Calculator and method for determining phase correction data for an audio signal |
US10770083B2 (en) | 2014-07-01 | 2020-09-08 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Audio processor and method for processing an audio signal using vertical phase correction |
US10930292B2 (en) | 2014-07-01 | 2021-02-23 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Audio processor and method for processing an audio signal using horizontal phase correction |
CN106663438A (en) * | 2014-07-01 | 2017-05-10 | 弗劳恩霍夫应用研究促进协会 | Audio processor and method for processing audio signal by using vertical phase correction |
US20170110132A1 (en) * | 2014-07-01 | 2017-04-20 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Audio processor and method for processing an audio signal using vertical phase correction |
US9917706B1 (en) * | 2015-05-27 | 2018-03-13 | Hewlett-Packard Development Company, L.P. | Recovering independent waveforms from input bitstream data |
US11669667B2 (en) * | 2020-02-21 | 2023-06-06 | Synopsys, Inc. | Automatic test pattern generation (ATPG) for parametric faults |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US5809459A (en) | Method and apparatus for speech excitation waveform coding using multiple error waveforms | |
RU2718425C1 (en) | Speech decoder, speech coder, speech decoding method, speech encoding method, speech decoding program and speech coding program | |
US6708145B1 (en) | Enhancing perceptual performance of sbr and related hfr coding methods by adaptive noise-floor addition and noise substitution limiting | |
US7801733B2 (en) | High-band speech coding apparatus and high-band speech decoding apparatus in wide-band speech coding/decoding system and high-band speech coding and decoding method performed by the apparatuses | |
EP0673014B1 (en) | Acoustic signal transform coding method and decoding method | |
US5890108A (en) | Low bit-rate speech coding system and method using voicing probability determination | |
US6526376B1 (en) | Split band linear prediction vocoder with pitch extraction | |
JP4861196B2 (en) | Method and device for low frequency enhancement during audio compression based on ACELP / TCX | |
US5794185A (en) | Method and apparatus for speech coding using ensemble statistics | |
US6067511A (en) | LPC speech synthesis using harmonic excitation generator with phase modulator for voiced speech | |
US7805314B2 (en) | Method and apparatus to quantize/dequantize frequency amplitude data and method and apparatus to audio encode/decode using the method and apparatus to quantize/dequantize frequency amplitude data | |
US7498959B2 (en) | Apparatus and method of wideband decoding to synthesize a decoded excitation signal with a generated high frequency band signal | |
US6119082A (en) | Speech coding system and method including harmonic generator having an adaptive phase off-setter | |
EP0910067A1 (en) | Audio signal coding and decoding methods and audio signal coder and decoder | |
US6138092A (en) | CELP speech synthesizer with epoch-adaptive harmonic generator for pitch harmonics below voicing cutoff frequency | |
US20070040709A1 (en) | Scalable audio encoding and/or decoding method and apparatus | |
KR19990077753A (en) | Audio signal coding apparatus, audio signal decoding apparatus, and audio signal coding and decoding apparatus | |
JP3541680B2 (en) | Audio music signal encoding device and decoding device | |
JPH11510274A (en) | Method and apparatus for generating and encoding line spectral square root | |
JP3344944B2 (en) | Audio signal encoding device, audio signal decoding device, audio signal encoding method, and audio signal decoding method | |
US6535847B1 (en) | Audio signal processing | |
JP2000132194A (en) | Signal encoding device and method therefor, and signal decoding device and method therefor | |
JP3237178B2 (en) | Encoding method and decoding method | |
JP3218679B2 (en) | High efficiency coding method | |
JP2000132193A (en) | Signal encoding device and method therefor, and signal decoding device and method therefor |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: MOTOROLA, INC., ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BERGSTROM, CHAD SCOTT;GIFFORD, CARL STEVEN;PATTISON, RICHARD JAMES;AND OTHERS;REEL/FRAME:008036/0175Effective date: 19960521 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
FPAY | Fee payment |
Year of fee payment: 12 |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY, INC, ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA, INC;REEL/FRAME:025673/0558Effective date: 20100731 |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY LLC, ILLINOISFree format text: CHANGE OF NAME;ASSIGNOR:MOTOROLA MOBILITY, INC.;REEL/FRAME:029216/0282Effective date: 20120622 |
|
AS | Assignment |
Owner name: GOOGLE TECHNOLOGY HOLDINGS LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA MOBILITY LLC;REEL/FRAME:034475/0001Effective date: 20141028 |