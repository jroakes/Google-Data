US8639644B1 - Shared robot knowledge base for use with cloud computing system - Google Patents
Shared robot knowledge base for use with cloud computing system Download PDFInfo
- Publication number
- US8639644B1 US8639644B1 US13/464,699 US201213464699A US8639644B1 US 8639644 B1 US8639644 B1 US 8639644B1 US 201213464699 A US201213464699 A US 201213464699A US 8639644 B1 US8639644 B1 US 8639644B1
- Authority
- US
- United States
- Prior art keywords
- robot
- data
- query
- knowledge base
- shared
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000003993 interaction Effects 0.000 claims abstract description 10
- 238000000034 method Methods 0.000 claims description 51
- 238000012545 processing Methods 0.000 claims description 49
- 230000004044 response Effects 0.000 claims description 36
- 238000004891 communication Methods 0.000 claims description 28
- 238000004519 manufacturing process Methods 0.000 claims description 8
- 239000000463 material Substances 0.000 claims description 5
- 239000000203 mixture Substances 0.000 claims description 4
- 230000005540 biological transmission Effects 0.000 claims description 3
- 230000000977 initiatory effect Effects 0.000 claims 2
- 230000008901 benefit Effects 0.000 abstract description 10
- 238000003860 storage Methods 0.000 description 22
- 235000013305 food Nutrition 0.000 description 17
- 239000007788 liquid Substances 0.000 description 15
- 230000006870 function Effects 0.000 description 14
- 210000000078 claw Anatomy 0.000 description 13
- 241000220225 Malus Species 0.000 description 9
- CDBYLPFSWZWCQE-UHFFFAOYSA-L Sodium Carbonate Chemical compound [Na+].[Na+].[O-]C([O-])=O CDBYLPFSWZWCQE-UHFFFAOYSA-L 0.000 description 9
- 238000002360 preparation method Methods 0.000 description 9
- 230000001413 cellular effect Effects 0.000 description 8
- 239000000047 product Substances 0.000 description 8
- 239000011664 nicotinic acid Substances 0.000 description 7
- 238000010586 diagram Methods 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 6
- 235000021016 apples Nutrition 0.000 description 5
- 210000003811 finger Anatomy 0.000 description 5
- 210000004247 hand Anatomy 0.000 description 4
- 235000015243 ice cream Nutrition 0.000 description 4
- 230000033001 locomotion Effects 0.000 description 4
- 235000012054 meals Nutrition 0.000 description 4
- 230000007246 mechanism Effects 0.000 description 4
- 230000008569 process Effects 0.000 description 4
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 4
- 241000282412 Homo Species 0.000 description 3
- 230000003213 activating effect Effects 0.000 description 3
- 235000021443 coca cola Nutrition 0.000 description 3
- 230000004927 fusion Effects 0.000 description 3
- 244000300264 Spinacia oleracea Species 0.000 description 2
- 235000009337 Spinacia oleracea Nutrition 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 2
- 210000004556 brain Anatomy 0.000 description 2
- 239000000571 coke Substances 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 238000003306 harvesting Methods 0.000 description 2
- 238000007726 management method Methods 0.000 description 2
- 235000012149 noodles Nutrition 0.000 description 2
- 238000005096 rolling process Methods 0.000 description 2
- 240000008415 Lactuca sativa Species 0.000 description 1
- 235000007688 Lycopersicon esculentum Nutrition 0.000 description 1
- 241000203475 Neopanax arboreus Species 0.000 description 1
- 240000007594 Oryza sativa Species 0.000 description 1
- 235000007164 Oryza sativa Nutrition 0.000 description 1
- 240000003768 Solanum lycopersicum Species 0.000 description 1
- 244000061456 Solanum tuberosum Species 0.000 description 1
- 235000002595 Solanum tuberosum Nutrition 0.000 description 1
- 229920006328 Styrofoam Polymers 0.000 description 1
- 235000010724 Wisteria floribunda Nutrition 0.000 description 1
- 230000001133 acceleration Effects 0.000 description 1
- 230000002776 aggregation Effects 0.000 description 1
- 238000004220 aggregation Methods 0.000 description 1
- 235000012019 baked potatoes Nutrition 0.000 description 1
- 235000015278 beef Nutrition 0.000 description 1
- 235000008429 bread Nutrition 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000010267 cellular communication Effects 0.000 description 1
- 239000000919 ceramic Substances 0.000 description 1
- 235000013339 cereals Nutrition 0.000 description 1
- 235000013351 cheese Nutrition 0.000 description 1
- 235000014510 cooky Nutrition 0.000 description 1
- 238000013523 data management Methods 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 235000021186 dishes Nutrition 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 235000013601 eggs Nutrition 0.000 description 1
- 230000003203 everyday effect Effects 0.000 description 1
- 230000004438 eyesight Effects 0.000 description 1
- 235000013312 flour Nutrition 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000010191 image analysis Methods 0.000 description 1
- 238000009434 installation Methods 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000008267 milk Substances 0.000 description 1
- 235000013336 milk Nutrition 0.000 description 1
- 210000004080 milk Anatomy 0.000 description 1
- 230000004297 night vision Effects 0.000 description 1
- 235000016709 nutrition Nutrition 0.000 description 1
- 238000004806 packaging method and process Methods 0.000 description 1
- 239000000123 paper Substances 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 230000037361 pathway Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000010399 physical interaction Effects 0.000 description 1
- 239000004033 plastic Substances 0.000 description 1
- 235000012015 potatoes Nutrition 0.000 description 1
- 238000004064 recycling Methods 0.000 description 1
- 235000009566 rice Nutrition 0.000 description 1
- 235000012045 salad Nutrition 0.000 description 1
- 229910052709 silver Inorganic materials 0.000 description 1
- 239000004332 silver Substances 0.000 description 1
- 239000000779 smoke Substances 0.000 description 1
- 235000014214 soft drink Nutrition 0.000 description 1
- 229910001220 stainless steel Inorganic materials 0.000 description 1
- 239000010935 stainless steel Substances 0.000 description 1
- 239000008261 styrofoam Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 238000001931 thermography Methods 0.000 description 1
- 210000003813 thumb Anatomy 0.000 description 1
- 238000012384 transportation and delivery Methods 0.000 description 1
- 235000013311 vegetables Nutrition 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05B—CONTROL OR REGULATING SYSTEMS IN GENERAL; FUNCTIONAL ELEMENTS OF SUCH SYSTEMS; MONITORING OR TESTING ARRANGEMENTS FOR SUCH SYSTEMS OR ELEMENTS
- G05B19/00—Programme-control systems
- G05B19/02—Programme-control systems electric
- G05B19/04—Programme control other than numerical control, i.e. in sequence controllers or logic controllers
- G05B19/042—Programme control other than numerical control, i.e. in sequence controllers or logic controllers using digital processors
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/004—Artificial life, i.e. computing arrangements simulating life
- G06N3/008—Artificial life, i.e. computing arrangements simulating life based on physical entities controlled by simulated intelligence so as to replicate intelligent life forms, e.g. based on robots replicating pets or humans in their appearance or behaviour
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05B—CONTROL OR REGULATING SYSTEMS IN GENERAL; FUNCTIONAL ELEMENTS OF SUCH SYSTEMS; MONITORING OR TESTING ARRANGEMENTS FOR SUCH SYSTEMS OR ELEMENTS
- G05B2219/00—Program-control systems
- G05B2219/30—Nc systems
- G05B2219/31—From computer integrated manufacturing till monitoring
- G05B2219/31205—Remote transmission of measured values from site, local to host
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05B—CONTROL OR REGULATING SYSTEMS IN GENERAL; FUNCTIONAL ELEMENTS OF SUCH SYSTEMS; MONITORING OR TESTING ARRANGEMENTS FOR SUCH SYSTEMS OR ELEMENTS
- G05B2219/00—Program-control systems
- G05B2219/30—Nc systems
- G05B2219/39—Robotics, robotics to robotics hand
- G05B2219/39393—Camera detects projected image, compare with reference image, position end effector
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05B—CONTROL OR REGULATING SYSTEMS IN GENERAL; FUNCTIONAL ELEMENTS OF SUCH SYSTEMS; MONITORING OR TESTING ARRANGEMENTS FOR SUCH SYSTEMS OR ELEMENTS
- G05B2219/00—Program-control systems
- G05B2219/30—Nc systems
- G05B2219/45—Nc applications
- G05B2219/45084—Service robot
Definitions
- Cloud computing refers to the provision of computational resources via a computer network.
- both data and software are fully contained on a user's computer.
- the user's computer may contain relatively little software or data (perhaps just a minimal operating system and web browser, for example), and may serve as a display terminal for processes occurring on a network of computers.
- One common shorthand term used to describe a cloud computing system or service (or even an aggregation of cloud services) is “the cloud.”
- Cloud computing is sometimes referred to as “client-server computing.”
- client-server computing may include a distributed application structure that partitions tasks or workloads between providers of a resource or service (e.g., servers), and service requesters (e.g., clients).
- client-server computing generally involves a one-to-one relationship between the server and the client, whereas cloud computing includes generic services that can be accessed by generic clients such that a one-to-one relationship or connection may not be required.
- cloud computing generally includes client-server computing along with additional services and functionality.
- cloud computing may free users from certain hardware and software installation and maintenance tasks through the use of simplified hardware on the user's computer.
- the user's computer can access a vast network of computing resources (e.g., processors, disk drives, etc.), the user is not limited just to the computing and storage power of his or her local computer.
- the sharing of computing resources across many users may reduce computing costs to individuals. For example, multiple computers connected to the cloud may be able to share the same pool of computing power, applications, and files. Users can store and access personal files such as music, pictures, videos, and bookmarks or play games or use productivity applications on a remote server rather than physically carrying around a storage medium, such as a DVD or thumb drive.
- a user may open a web browser and connect to a host of web servers that run user interface software configured to collect commands from the user and interpret the commands into commands on the servers.
- the servers may handle the computing, and can either store or retrieve information from database servers or file servers and display an updated page to the user.
- cloud computing data across multiple servers can be synchronized around the world allowing for collaborative work on one file or project, from multiple users around the world, for example.
- the present application discloses various embodiments of a shared robot knowledge base for use with a cloud computing system and methods for using shared robot knowledge bases with a cloud computing system.
- any of the methods described herein may be implemented in the form of instructions stored on a non-transitory, computer readable media. When executed by a computing device, the instructions may cause the computing device to perform functions of the disclosed method. Further examples may also include articles of manufacture including tangible computer-readable media that have computer-readable instructions encoded thereon, and the instructions may comprise instructions to perform functions of the methods described herein.
- the computer readable media may include non-transitory computer readable media, such as computer-readable media that stores data for short periods of time like register memory, processor cache, and Random Access Memory (RAM).
- the computer readable media may also include non-transitory media, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, or compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage system.
- the computer readable media may be considered a computer readable storage media, for example, or a tangible storage media.
- circuitry configured to perform logical functions in any of the processes or methods described herein.
- many types of devices may be used or configured to perform logical functions in any of the processes or methods described herein.
- many types of devices and/or components or sub-components of the devices) may be used or configured as means for performing functions of any of the methods described herein (or any portions of the methods described herein).
- a cloud computing system may receive a query from a robot.
- the query may include identification information associated with a particular object.
- the identification information associated with the object may be any of (i) a name of the object in a sound clip or text excerpt, (ii) image data associated with the object, e.g., a digital image, video clip, point cloud, etc., (iii) information read from a tag or beacon associated with the object, e.g., an RFID tag, an RF or optical beacon, etc., (iv) information read from a bar code or similar code on the surface of the object, e.g., a QR code, a bar code, etc., and/or (v) text or product logos on the surface of the object.
- the query from the robot may include other identification information associated with the object as well.
- the cloud computing system may in turn use the identification information to query a shared robot knowledge base to determine the identity of the object.
- the shared robot knowledge base contains data related to an expansive inventory of objects, robot tasks, maps, robot applications, etc.
- the shared robot knowledge base includes data about a vast number of various objects that robots may encounter and/or interact with in their everyday environments.
- the shared robot knowledge base may also include robot instructions for interacting with the objects.
- the shared robot knowledge base may have data related to various types of furniture, appliances, household objects, utensils, tools, electronic devices, food, supplies, etc. Data about any object that that a robot might encounter or interact with may be included in the shared robot knowledge base.
- the data associated with a particular object may vary from object to object.
- most objects in the shared robot knowledge base have at least a baseline set of data that may include one or more of the object's (i) physical dimensions, weight, material composition, (ii) manufacturer (and possibly model number), (iii) ordering information (if the object is consumable), (iv) relevant tasks that a robot may perform with the object, (v) location of the object, etc.
- the shared object knowledge base may have additional types of data about the objects as well.
- Data about objects, robot tasks, maps, and robot applications may be entered and/or updated in the shared robot knowledge base by human operators, computing systems, and/or robots.
- human operators may enter data into the shared robot knowledge base, e.g., data about objects, robot tasks, maps, and robot applications.
- manufacturers of objects may provide data to the robot knowledge base, e.g., data about objects they manufacture, data about robot equipment they manufacture, etc.
- robots may also enter data into the shared robot knowledge base, e.g., data about objects they encounter, data about tasks they perform, data for maps associated with their environments, and data about robot applications.
- the robot may collect information about the object and send the collected data to the shared robot knowledge base for storage and future retrieval by any other robot authorized to access the uploaded information.
- the robot may ask a human one or more questions about the object. In this manner, information about an object that one robot learns from a human can be shared with other robots authorized to access information about that object. Robots may interact with humans to learn information about maps, robot tasks, and applications in a similar fashion.
- the cloud computing system may be configured to query the shared robot knowledge base to both (i) identify the object corresponding to the identification information received from the robot and (ii) retrieve at least some of the data associated with the identified object.
- the cloud computing system may send the identity of the object and at least some of the data associated with the identified object to the robot that sent the query.
- the robot may use the data associated with the object to interact with the object, e.g., pick up the object, move the object, power on/off the object, open/close doors, drawers, and/or compartments of the object, etc. If, while interacting with the object, the robot determines that some aspect of the received data associated with the object is inaccurate (e.g., the object weighs more or less than indicated, the force required to open/close a door is greater or less than indicated, etc.), then the robot may send feedback to the cloud computing system. In response, the cloud computing system may analyze the feedback, and in some instances, update the data associated with the object in the shared robot knowledge base.
- the cloud computing system may analyze the feedback, and in some instances, update the data associated with the object in the shared robot knowledge base.
- FIG. 1 is a high-level conceptual diagram showing components of a cloud computing system according to some embodiments of the disclosed systems and methods.
- FIG. 2A illustrates a high-level block diagram of a robot according to some embodiments of the disclosed systems and methods.
- FIG. 2B shows a graphical depiction of an example robot according to some embodiments of the disclosed systems and methods.
- FIG. 2C shows a graphical depiction of another example robot according to some embodiments of the disclosed systems and methods.
- FIG. 3 shows an example of multiple robots interacting with a cloud computing system having a shared robot knowledge base according to some embodiments of the disclosed systems and methods.
- FIG. 4 shows a message flow diagram between multiple robots and a cloud computing system having a shared robot knowledge base according to some embodiments of the disclosed systems and methods.
- FIG. 5 shows an example method of using a shared robot knowledge base according to some embodiments of the disclosed systems and methods.
- FIG. 6 shows another example of using a shared robot knowledge base according to some embodiments of the disclosed systems and methods.
- cloud based computing generally refers to networked computer architectures where application execution and storage may be divided, to some extent, between robots, client devices, and cloud computing systems.
- a robot may be any device that has a computing ability and interacts with its surroundings with an actuation capability (e.g., electromechanical capabilities).
- actuation capability e.g., electromechanical capabilities
- a robot may also be configured with various sensors and devices in the form of modules, where different modules may be added or removed from a robot depending on requirements.
- a robot may be configured to receive a computing device, such as mobile phone, a smartphone, a laptop computer, and/or a tablet computer, any of which may be configured to function as an accessory of the robot or even as a “brain” of the robot.
- a computing device such as mobile phone, a smartphone, a laptop computer, and/or a tablet computer, any of which may be configured to function as an accessory of the robot or even as a “brain” of the robot.
- a robot may interact with the cloud computing system to perform any number of actions, such as to share information with other cloud computing devices or to share information with other robots.
- a robot may interact with the cloud computing system to facilitate object recognition, to perform a mapping function, or to perform navigational functions (i.e., receive a map/navigation pathway previously traversed by another robot).
- a robot may interact with the cloud computing system to develop a map of objects in an area, to inventory objects in the area, and/or to perform voice recognition by and/or control of a robot.
- many of the embodiments described herein enable robots to store and access data from a shared robot knowledge base configured for use with the cloud computing system.
- the cloud computing system may be configured to (i) send and receive data and queries to and from multiple robots, (ii) send data retrieved from the shared robot knowledge base to robots, and (iii) update data in the shared robot knowledge base based on data received from robots.
- FIG. 1 is a high-level conceptual diagram showing components of a cloud computing system 102 according to some embodiments of the disclosed systems and methods.
- Cloud based computing generally refers to networked computer architectures where application execution, service provision, and data storage may be divided, to some extent, between clients and cloud computing devices.
- the “cloud” may refer to a service or a group of services accessible over a network (e.g., the Internet) by clients, server devices, and cloud computing systems, for example.
- cloud computing enables a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be provisioned and released with minimal management effort or interaction by the cloud service provider.
- configurable computing resources e.g., networks, servers, storage, applications, and services
- a cloud-based application may store copies of data and/or executable program code in the cloud computing system, while allowing client devices to download at least some of this data and program code as needed for execution at the client devices.
- downloaded data and program code can be tailored to the capabilities of specific client devices (e.g., a personal computer, tablet computer, mobile phone, smartphone, and/or robot) accessing the cloud based application. Additionally, dividing application execution and storage between client devices and the cloud computing system allows more processing to be performed by the cloud computing system, thereby taking advantage of the cloud computing system's processing power and capability, for example.
- Cloud-based computing can also refer to distributed computing architectures where data and program code for cloud-based applications are shared between one or more client devices and/or cloud computing devices on a near real-time basis. Portions of this data and program code may be dynamically delivered, as needed or otherwise, to various clients accessing the cloud-based application. Details of the cloud based computing architecture may be largely transparent to users of client devices. Thus, a PC user or a robot client device accessing a cloud-based application may not be aware that the PC or robot downloads program logic and/or data from the cloud computing system, or that the PC or robot offloads processing or storage functions to the cloud computing system, for example.
- the cloud computing system 102 includes one or more cloud services 104 , one or more cloud platforms 106 , cloud infrastructure 108 components, and cloud knowledge bases 110 .
- the cloud computing system 102 may include more of fewer components, and each of the cloud services 104 , the cloud platforms 106 , the cloud infrastructure components 108 , and the cloud knowledge bases 110 may comprise multiple computing and storage elements as well.
- one or more of the described functions of the cloud computing system 102 may be divided into additional functional or physical components, or combined into fewer functional or physical components.
- the example cloud computing system 102 shown in FIG. 1 is a networked computing architecture.
- the cloud services 104 may represent queues for handling requests from client devices.
- the cloud platforms 106 may include client-interface frontends for the cloud computing system 102 .
- the cloud platforms 106 may be coupled to the cloud services 104 to perform functions for interacting with client devices.
- the cloud platforms 106 may include applications for accessing the cloud computing system 102 via user interfaces, such as a web browser.
- the cloud platforms 106 may also include robot interfaces configured to exchange data with robot clients.
- the cloud infrastructure 108 may include service, billing, and other operational and infrastructure components of the cloud computing system 102 .
- the cloud knowledge bases 110 are configured to store data for use by the cloud computing system 102 , and thus, the cloud knowledge bases 110 may be accessed by any of the cloud services 104 , the cloud platforms 106 , and/or the cloud infrastructure components 108 .
- client devices may be configured to communicate with components of the cloud computing system 102 for the purpose of accessing data and executing applications provided by the cloud computing system 102 .
- a computer 112 a mobile device 114 , a host 116 , and a robot client 118 are shown as examples of the types of client devices that may be configured to communicate with the cloud computing system 102 .
- more or fewer client devices may communicate with the cloud computing system 102 .
- other types of client devices may also be configured to communicate with the cloud computing system 102 as well.
- the computer 112 shown in FIG. 1 may be any type of computing device (e.g., PC, laptop computer, tablet computer, etc.), and the mobile device 114 may be any type of mobile computing device (e.g., laptop, smartphone, mobile telephone, cellular telephone, tablet computer, etc.) configured to transmit and/or receive data to/from the cloud computing system 102 .
- the host 116 may be any type of computing device with a transmitter/receiver including a laptop computer, a mobile telephone, a smartphone, a tablet computer etc., which is configured to transmit/receive data to/from the cloud computing system 102 .
- the robot client 118 may include any type of computing device that is configured to communicate with the cloud computing system 102 and has an actuation capability (e.g., electromechanical capabilities) for moving about its environment and/or interacting with objects in its environment.
- the robot client 118 may include various combinations of computing devices, sensors, and electromechanical actuation elements.
- the robot client 118 may collect data via one or more sensors, and upload the data to the cloud computing system 102 via one or more communications interfaces.
- the cloud computing system 102 may be configured to analyze data received from the robot client 118 , and return processed data to the robot client 118 .
- a robot client 118 may be configured to send and receive data to a remote host 116 via the cloud computing system 102 .
- the robot client 118 may be configured to send/receive data to/from another client device via the cloud computing system 102 .
- the robot client may be configured to send/receive information to/from the computer 112 , the mobile device 114 , and/or even other robots either directly, indirectly via the cloud computing system 102 , or indirectly via other network systems.
- the robot client 118 may include one or more sensors, such as a gyroscope or an accelerometer to measure movement of the robot client 118 .
- Other sensors may further include Global Positioning System (GPS) receivers, infrared sensors, sonar, optical sensors, biosensors, Radio Frequency identification (RFID) systems, Near Field Communication (NFC) chip sensors, wireless sensors, and/or compasses, among others, for example.
- GPS Global Positioning System
- RFID Radio Frequency identification
- NFC Near Field Communication
- any of the client devices may also include a user-interface (UI) configured to allow a user to interact with the client device.
- the robot client 118 may include various buttons and/or a touchscreen interface configured to receive commands from a human or provide output information to a human.
- the robot client 118 may also include a microphone configured to receive voice commands from a human.
- the robot client 118 may also include one or more interfaces that allow various types of user-interface devices to be connected to the robot client 118 .
- the mobile device 114 , the computer 112 , and/or the host 116 may be configured to run a user-interface for sending and receiving information to/from the robot client 118 or otherwise configuring and controlling the robot client 118 .
- communication links between client devices and the cloud 102 may include wired connections, such as a serial or parallel bus, Ethernet, optical connections, or other type of wired connection.
- Communication links may also be wireless links, such as Bluetooth, IEEE 802.11 (IEEE 802.11 may refer to IEEE 802.11-2007, IEEE 802.11n-2009, or any other IEEE 802.11 revision), CDMA, 3G, GSM, WiMAX, or other wireless based data communication links.
- IEEE 802.11 may refer to IEEE 802.11-2007, IEEE 802.11n-2009, or any other IEEE 802.11 revision
- CDMA Code Division Multiple Access
- 3G Third Generation
- GSM Global System for Mobile communications
- WiMAX Worldwide Interoperability for Microwave Access
- the client devices may be configured to communicate with the cloud computing system 102 via wireless access points.
- Access points may take various forms.
- an access point may take the form of a wireless access point (WAP) or wireless router.
- WAP wireless access point
- an access point may be a base station in a cellular network that provides Internet connectivity via the cellular network.
- the client devices may include a wired or wireless network interface through which the client devices can connect to the cloud computing system 102 directly or via access points.
- the client devices may be configured to use one or more protocols such as 802.11, 802.16 (WiMAX), LTE, GSM, GPRS, CDMA, EV-DO, and/or HSPDA, among others.
- the client devices may be configured to use multiple wired and/or wireless protocols, such as “3G” or “4G” data connectivity using a cellular communication protocol (e.g., CDMA, GSM, or WiMAX, as well as for “WiFi” connectivity using 802.11).
- Other types of communications interfaces and protocols could be used as well.
- FIG. 2A illustrates a high-level block diagram of a robot 200 according to some embodiments of the disclosed systems and methods.
- the robot 200 includes a processor 202 , memory or storage 204 , sensors 206 , and electromechanical actuation devices 208 .
- one or more of the robot components may be custom designed for a specific robot or for a particular model or type of robot.
- one or more of the robot components may be generic to many different robots and/or types of robots.
- the robot 200 may have one or communications interfaces (not shown) for communicating with the cloud computing system 102 (as shown in FIG. 1 ).
- the communications interfaces may include wired or wireless interfaces.
- wired interfaces include, for example, a parallel interface, a Universal Serial Bus (USB) interface, an Ethernet interface, an optical interface, or any other type of wired communications interface now known or later developed.
- wireless interface include, for example, a Bluetooth interface, an IEEE 802.11 (and its variants) interface, a cellular (such as GSM, CDMA, UMTS, EV-DO, WiMAX, or LTE) interface, a Zigbee interface, or any other type of wireless communications interface now known or later developed.
- the storage 204 may be used for storing data from various sensors 206 of the robot 200 .
- the storage 204 may also be used for storing program instructions for execution by the processor 202 .
- the processor 202 may include one or more processors or other data processing sub-systems.
- the processor 202 may be coupled to the storage 204 and may be configured to control the robot 200 based on the program instructions stored at least partially in the storage 204 .
- the processor 202 may also be configured to receive and interpret data from the various sensors 206 on the robot 200 .
- sensors 206 that may be configured for use on the robot 200 include, for example, smoke sensors, light sensors, radio sensors, infrared sensors, microphones, gyroscopes, accelerometers, cameras, radars, capacitive sensors, touch sensors, or any other type of sensor now known or later developed.
- the robot 200 may also have electromechanical actuation devices 208 configured to enable the robot 200 to move about its environment or interact with objects in its environment.
- the robot 200 may have one or more electromechanical devices 208 , such as motors, wheels, movable arms, electromagnets, hands, grasping claws, tool attachments, etc., that enable the robot 200 to move about its environment, interact with objects located in its environment, and/or perform tasks with objects in its environment.
- the various sensors 206 and electromechanical devices 208 on the robot 200 may be modular in nature.
- different modules may be added to or removed from the robot 200 depending on particular requirements.
- the robot 200 may remove or perhaps power down one or more modules to reduce power usage.
- the robot 200 may add one or more additional modular electromechanical devices 208 as required.
- a robot may remove a modular “grasping claw” electromechanical device from its arm, and replace the “grasping claw” electromechanical device with a “tool interface” mechanism configured to accept various tool implements such as a screwdriver, bit driver, knife, wrench, or other tool, etc. to enable the robot to perform a specific task.
- a robot may remove a small-wheeled drive mechanism configured for indoor use, and replace it with a large-wheeled drive mechanism configured for outdoor use. From a sensor standpoint, the robot may remove a camera interface configured for daytime use, and replace it with a night-vision enabled interface configured for use in dark or unlit areas.
- Other types of modular electromechanical devices 208 and/or modular sensors 206 could be used as well. Robots with modular sensors 206 and electromechanical devices 208 may be advantageous in situations where the robot 200 may need to adapt to different situations and environments and/or use different tools and/or attachments to accomplish various tasks.
- the processor 202 , storage 204 , and sensors 206 of the robot 200 may optionally be components of a removable device 210 indicated by the dashed line shown in the FIG. 2A .
- the robot 200 may have a number of mechanical actuation devices 208 (e.g., a movable base, one or more grasping claws, robot hands, etc.), and the robot 200 may be configured to receive a removable device 210 to function as the “brains” or control unit of the robot 200 .
- the removable device 210 may correspond to a mobile telephone, a smartphone, a tablet computer, a laptop computer, etc.
- the device 210 may be a smartphone configured to plug in to the robot.
- the smartphone When plugged in to the robot, the smartphone may form an interactive display configured to receive commands from a human, for example.
- the smartphone may also provide a robot with sensors (e.g., a camera, compass, GPS receiver, accelerometer, etc.), one or more wireless communications interfaces, and processing capabilities, for example.
- the smartphone may allow a human user to download new routines or “robot applications” (or “robot apps”) from the cloud computing system 102 for execution by the robot 200 .
- robot applications or “robot apps”
- a user may download a laundry folding “robot app” from the cloud computing system 102 onto the smartphone, for example.
- the robot 200 can execute the laundry folding “robot app” via the smartphone to perform the functions specified in the “robot app,” such as folding laundry in this example.
- the robot 200 may be able to leverage the sensor and communications capabilities of neighboring client devices to supplement or augment its own sensor and communications capabilities.
- the robot 200 may access a separate smartphone via a Bluetooth connection, and use the smartphone's cellular data network interface to access the cloud computing system 102 shown in FIG. 1 .
- the robot 200 may be configured to connect to other sensors to obtain information.
- the robot 200 may be configured to connect to security system cameras or other sensors positioned near the entrance of a warehouse to monitor the shipments into and out of the warehouse, for example.
- FIG. 2B shows a graphical depiction of an example robot 212 according to some embodiments of the disclosed systems and methods.
- the robot 212 is shown as a mechanical form of a person including arms, legs, and a head.
- the robot 212 may be configured to receive any number of various modules or components, such a smartphone or tablet computer, which may be configured to operate or control the robot 212 , or otherwise be used by the robot 212 for sensor, processing, and/or communications capabilities.
- a smartphone e.g., removable device 210 of FIG. 2A
- the robot 212 may have its own communications interfaces, processors 202 , sensors 206 , and storage 204 (shown in FIG. 2A ) without having to rely on the capabilities of a smartphone or other detachable computing device.
- the robot 212 may include some combination of its own communications interfaces, processors 202 , sensors 206 , and storage 204 in cooperation with communications interfaces, processors 202 , sensors 206 , and storage 204 of a smartphone, tablet computer, laptop, or other removable or detachable communication-enabled computing device.
- FIG. 2C shows a graphical depiction of another example robot 214 according to some embodiments of the disclosed systems and methods.
- the robot 214 includes a computing device 216 , sensors 218 , and a mechanical actuator 220 .
- the computing device 216 may be a laptop or other type of portable computing device which may be configured to receive data from the sensors 218 .
- the sensors 218 may include a camera, infrared sensors, and other motion sensing or computer vision elements.
- the mechanical actuator 220 may include an arm with a grasping claw or other type of attachment, a base with wheels, and a motor upon which the computing device 216 and the sensors 218 can be positioned, for example.
- Any of the robots illustrated in FIGS. 2A-2C may be configured to operate according to a robot operating system, e.g., an operating system designed for specific functions of the robot.
- a robot operating system may provide libraries and tools (e.g., hardware abstraction, device drivers, visualizers, message-passing, package management, etc.) to enable robots to execute robot applications.
- robot operating systems include open source software such as ROS (robot operating system), DROS, or ARCOS (advanced robotics control operating system); proprietary software such as the robotic development platform ESRP from Evolution Robotics® and MRDS (Microsoft® Robotics Developer Studio), and other examples such as ROSJAVA.
- a robot operating system may include publish and subscribe functionality, and may also include functionality to control components of the robot, such as head tracking, base movement (e.g., velocity control, navigation framework), etc.
- FIG. 3 shows an example of multiple robots 301 , 302 , 303 interacting with a cloud computing system 304 having a shared robot knowledge base 306 according to some embodiments of the disclosed systems and methods.
- the robots 301 , 302 , and 303 shown in the example of FIG. 3 may be similar to any of the robots shown and described herein with respect to FIGS. 1 and 2 A-C.
- the cloud computing system 304 includes a cloud processing engine 305 and a shared robot knowledge base 306 .
- the cloud computing system 304 may have additional components as well.
- individual robots such as robots 301 , 302 , and 303 , may send queries to the cloud computing system 304 .
- the cloud computing system 304 may retrieve data from the shared robot knowledge base 306 and send the retrieved data to the robots in response to their queries.
- the cloud computing system 304 may also receive data from the robots, and update information in the shared robot knowledge base 306 based on the data received from the robots.
- the shared robot knowledge base 306 may include multiple component and/or sub-component knowledge bases.
- the shared robot knowledge base 306 may contain additional or fewer knowledge bases than the ones shown in FIG. 3 .
- the distribution of the information across the various component knowledge bases shown FIG. 3 is merely one example of one embodiment.
- the contents of the knowledge bases may be combined into a single common knowledge base or distributed across multiple different knowledge bases in a different fashion than described herein.
- the functionality of the cloud processing engine 305 may be combined with the functionality of the shared robot knowledge base 306 and/or other components of the cloud computing system 304 . Additional systems and sub-systems that are not illustrated in FIG. 3 may be utilized to queue and process queries from robots, return data to the robots in response to the received queries, receive data from robots, and update information in the shared robot knowledge base 306 based on the received data.
- One important feature of the shared robot knowledge base 306 is that many different robots may access the shared robot knowledge base 306 , download information from the shared robot knowledge base 306 , and upload information to the shared robot knowledge base 306 . Because multiple robots can share the information in the shared robot knowledge base 306 , information learned about a particular object by one robot, such as robot 301 , can be shared with another robot authorized to access information about that particular object, such as robot 302 . As a result, robot 302 benefits from the knowledge added to the shared robot knowledge base 306 by robot 301 .
- robot 301 can update the information about the weight of the hammer that is stored in the shared robot knowledge base 306 . If robot 302 is authorized to access information about hammers, then the next time robot 302 queries the cloud computing system 204 for information about the same type of hammer, then robot 302 will know that the hammer weighs 3 pounds (without having to independently weigh the hammer) based on the data about the type of hammer that was previously uploaded to the shared robot knowledge base 306 by robot 301 .
- a shared robot knowledge base 306 The advantages of a shared robot knowledge base 306 multiply as robots are deployed around the world. If information about a particular object is “public” and therefore accessible by all robots, then information that a single robot learns about that object and uploads to the shared robot knowledge base 306 may be accessible by other robots all around the world. In operation, many millions or even many billions of robots may ultimately access the shared robot knowledge base 306 . As a result, the shared robot knowledge base 306 enables robots to benefit from the collective information learned by many other robots.
- the example shared robot knowledge base 306 illustrated in FIG. 3 includes: (i) a unique object knowledge base 307 ; (ii) a general object knowledge base 308 ; (iii) a task knowledge base 309 ; (iv) a map knowledge base 310 ; and (v) an application knowledge base 311 .
- the shared robot knowledge base 306 may include a unique object knowledge base 307 .
- the unique object knowledge base 307 is configured to store data about “unique” objects that a robot may encounter or interact with in its environment.
- a “unique” object is an object that can be identified with specificity as being a specific, individual object.
- a “unique” object might be a specific refrigerator that belongs to a specific person, i.e. John Smith's specific refrigerator.
- a “unique” object can also be classified as a particular “type” of object such as a particular make and model of refrigerator.
- a specific individual refrigerator can be identified as a “unique” object in part because it has an individual manufacturer-assigned serial number or other type of unique identifier that can be used to distinguish it from all other refrigerators of the same “type,” e.g., same make and model for example.
- a “non-unique” object is something that may not be distinguishable from all other objects of its “type.” For example, an apple, a flower, a box of cereal, a coffee mug, a pen, a notepad, or other similar objects may not have a unique identifier associated with it.
- a “non-unique” object should not have an entry in the “unique” object knowledge base 307 , but it may have an entry in the general object knowledge base 308 , as described later.
- an individual unique object identifier may have the following corresponding data: (i) a general object identifier (GID); (ii) image data (IMG); (iii) location data (LOC); and (iv) serial number data (SER).
- Various embodiments of the unique object knowledge base 307 may contain more or less data corresponding to a particular unique object identifier (UID).
- the unique object knowledge base 307 may have many more additional categories of data than the categories shown in conceptual data structure 313 .
- the unique object knowledge base 307 may store more or less data for some unique objects as compared to the data stored for other unique objects.
- the unique object identifier is an identifier corresponding to a specific, “unique” object. Because each unique object identifier (UID) corresponds to a specific, “unique” object, the unique object knowledge base 307 may contain many billions of unique object identifiers (UID).
- the unique object knowledge base 307 may be indexed by unique object identifiers (UID). However, the unique object knowledge base 307 could alternatively be indexed by other identifiers or attributes as well. In operation, two “unique” objects should not have the same unique object identifier (UID); however, two “unique” objects may have the same corresponding general object identifier (GID). Individual unique object identifiers (UID) may have a corresponding general object identifier (GID) that can be cross-referenced in the general object knowledge base 308 described later.
- UID unique object identifiers
- GID general object identifier
- separating data corresponding to “unique” objects in the unique object knowledge base 307 from data corresponding to “non-unique” objects in the general object knowledge base 308 may have advantages in some embodiments in terms of data management and system performance. Separating data corresponding to “unique” objects and “non-unique” objects into separate knowledge bases may also have advantages from an information security standpoint in embodiments where it might be desirable to limit or restrict access to data about “unique” objects to certain robots while granting access to data about “non-unique” objects to all robots.
- an individual unique object identifier may have a corresponding general object identifier (GID).
- the general object identifier defines a “type” of object.
- the general object identifier defines the “type” of “unique” object that corresponds to the unique object identifier (UID).
- the distinction between a unique object identifier (UID) and its corresponding general object identifier (GID) can be illustrated with the following example of two Motorola®DROID X smartphones.
- the first DROID X smartphone may have a unique object identifier (UID) that is different than the unique object identifier (UID) of the second DROID X smartphone because each of the first and second DROID X smartphones can be uniquely identified.
- both of the two DROID X smartphones may have the same corresponding general object identifier (GID) because both of the two smartphones are the same “type” of object, i.e., a Motorola® DROID X smartphone.
- a unique object identifier (UID) and its corresponding general object identifier (GID) may be further illustrated by considering a third, different smartphone: a Motorola® DROID BIONIC smartphone.
- a specific DROID BIONIC smartphone will have a different corresponding general object identifier (GID) than the general object identifier (GID) of the two DROID X smartphones because the DROID BIONIC smartphone is a different “type” of object than the DROID X smartphones, i.e., one is a DROID BIONIC smartphone whereas the other two are DROID X smartphones.
- each of the DROID X and the two DROID BIONIC smartphones will have its own unique object identifier (UID) because each of the three smartphones is a “unique” object that can be uniquely and specifically identified.
- UID unique object identifier
- each of the three smartphones may be uniquely and specifically identified by any of their corresponding Electronic Serial Numbers (ESN), model serial numbers, WiFi MAC addresses, or Bluetooth addresses, for example.
- ESN Electronic Serial Numbers
- model serial numbers WiFi MAC addresses
- Bluetooth addresses for example.
- Different types of objects may be uniquely identifiable by other numbers, identifiers, or methods.
- An individual unique object identifier (UID) in the example conceptual data structure 313 of the unique object knowledge base 307 shown in FIG. 3 may also have corresponding image data (IMG).
- a particular unique object identifier's (UID) corresponding image data (IMG) may include one or more images, video clips, 3D models, point clouds, or other graphical representations of the unique object.
- the image data (IMG) may be used in connection with object recognition algorithms executed by the cloud processing engine 305 to identify a “unique” object in response to a query from a robot.
- a robot may send an image (or other graphical representation) of an object to the cloud processing engine 305 with a request or query to identify the object in the image.
- the cloud processing engine 305 may analyze the image and find any matching images (or other matching image data) from the image data (IMG) in the unique object knowledge base 307 to determine whether the object in the image data received from the robot corresponds to a unique object identifier (UID) in the unique object knowledge base 307 .
- UID unique object identifier
- the cloud processing engine 305 may send the robot a response to the query.
- the response may include (i) the identity of the “unique” object and (ii) data corresponding to the “unique” object.
- a robot may also add to the image data (IMG) for a corresponding unique object identifier (UID). For example, if a person tells a robot that a particular laptop computer is his or her laptop (e.g., “Hey, robot, this is my laptop.”), the robot may capture an image of the person's laptop and upload the captured image to the shared robot knowledge base 306 . Because the person's laptop is a “unique” object, the image data (IMG) along with other information that the robot might learn about the laptop may be stored in the unique object knowledge base 307 with a unique object identifier (UID).
- UID unique object identifier
- An individual unique object identifier (UID) in the example conceptual data structure 313 of the unique object knowledge base 307 shown in FIG. 3 may also have corresponding location data (LOC).
- the location data (LOC) corresponding to a particular unique object identifier (UID) in the unique object knowledge base 307 corresponds to the location of the unique object.
- the location data (LOC) may include GPS coordinates and/or other location-based information, e.g., an association with (or a link to) one or more maps stored in the map knowledge base 310 .
- maps in the map knowledge base 310 may include information about a particular building or house, a particular location in the building or house, a location in a room of the building or house, and a location in a drawer, shelf, cabinet, closet, box, or trunk, etc. in the room of the building or house.
- the location data (LOC) in the unique object knowledge base 307 may be as general or as specific as desired for the particular unique object.
- location data (LOC) corresponding to a “unique” car may include GPS coordinates indicating the last recorded location of the car.
- the location data (LOC) may also include links or other associations to one or more maps in the map knowledge base 310 that show the car located in a particular parking spot on a particular floor of a particular parking garage located at a particular address.
- location data (LOC) corresponding to a “unique” musical instrument may include GPS coordinates and links to map information in the map knowledge base 310 that describe the “unique” instrument's location in a case on a particular shelf of a closet in a particular room in a particular house, for example.
- Other types of location data (LOC) corresponding to “unique” objects may be contained in the unique object knowledge base 307 as well.
- An individual unique object identifier (UID) in the conceptual data structure 313 of the unique object knowledge base 307 shown in FIG. 3 may also have corresponding serial number data (SER).
- the serial number data (SER) corresponding to a particular unique object identifier (UID) in the unique object knowledge base 307 may include a manufacturer-assigned serial number of the “unique” object corresponding to the particular unique object identifier (UID).
- Most objects of any significant economic value have a manufacturer-assigned serial number designed to distinguish a particular object from other similar objects of the same type.
- two refrigerators of the same type i.e., same manufacturer, same model, etc.
- two cars of the same type i.e., same manufacturer, same model, etc.
- VIN manufacturer-assigned vehicle identification numbers
- the two cars may also have different license plate numbers that could additionally or alternatively be used to identify each car as a specific, unique car.
- Another assigned number or identifier could be used instead.
- a person could affix an RFID tag to a “unique” antique rocking chair so that the identification information read from the RFID tag in effect becomes the de facto serial number of the “unique” antique rocking chair.
- Other types of unique identifiers could be assigned to or otherwise associated with “unique” objects that may not have a manufacturer-assigned serial number.
- the shared robot knowledge base 306 may include a general object knowledge base 308 .
- the general object knowledge base 308 is configured to store data about different “types” of objects that a robot may encounter or interact with in its environment, including different “types” of “unique” and “non-unique” objects.
- a “unique” object is an object that can be identified with specificity as being a specific, individual object.
- a “non-unique” object is something that may not be distinguishable (at least in any practical manner) from all other objects of its same “type.”
- a can of soda may be a “non-unique” object.
- the can of soda can easily be identified as a particular “type” of object based on, for example, a Universal Product Code (UPC) on the soda can, there may be no practical way to distinguish one can of Coca-Cola® from any other can of Coca-Cola®.
- Another “non-unique” object may be a coffee mug in an office. The office may have hundreds of largely identical coffee mugs on multiple floors of the office building or even multiple buildings in an office campus. Each coffee mug can be easily identified as a particular “type” of mug based on its color, shape, and perhaps company logo on the exterior of the mug. However, there may be no practical way to distinguish one mug from any other mug.
- individual cans of soda or individual coffee mugs could, in some embodiments, be equipped with unique identifiers, e.g., RFID tags, unique QR codes, or other unique identifiers. If so, data about each “unique” can of soda or each “unique” coffee mug could be stored in the unique object knowledge base 307 . However, it may not be practical or even desirable to collect and maintain data about hundreds (or even billions in the soda can example) of largely identical items.
- an individual general object identifier may have the following corresponding data: (i) language data (LNG); (ii) image data (IMG); (iii) manufacturer data (MFG) (if applicable); (iv) model number data (MOD) (if applicable); (v) specification data (SPC); and (vi) task data (TASKS).
- LNG language data
- IMG image data
- MFG manufacturer data
- MOD model number data
- SPC specification data
- TSKS task data
- the conceptual data structure 314 of the general object knowledge base 308 may contain more or less information than the example shown in FIG. 3 .
- a general object identifier is a unique identifier corresponding to a particular “type” of object. Because each general object identifier (GID) corresponds to a particular “type” of object, the general object knowledge base 308 may contain many billions of different general object identifiers (GID).
- a particular “type” of smartphone e.g., a Motorola® DROID X smartphone
- the DROID X smartphone would have a different general object identifier (GID) than a Motorola® DROID BIONIC smartphone because the DROID X is a different “type” of smartphone than the DROID BIONIC.
- a particular “type” of car e.g., a Ford® Fusion, would have a specific (and preferably unique) general object identifier (GID).
- the Ford® Fusion would have a different general object identifier (GID) than a Ford® Mustang because the Fusion is a different “type” of car than the Mustang.
- GID general object identifier
- Other less-complex “types” of objects may have a specific (and preferably unique) general object identifier (GID) as well, including objects such as particular “types” of cups, glasses, mugs, dishes, utensils, tools, pencils, etc.
- GID general object identifier
- a coffee mug designed to hold 16 ounces of liquid would have a different general object identifier (GID) than a coffee mug designed to hold 12 ounces of liquid because the 16 ounce mug is a different “type” of mug than the 12 ounce mug.
- a short, wide coffee mug designed to hold 16 ounces of liquid may have a different general object identifier (GID) than a tall, narrow coffee mug also designed to hold 16 ounces of liquid because the short, wide mug is a different “type” of mug than the tall, narrow mug.
- GID general object identifier
- certain “types” of foods may have a specific (and preferably unique) general object identifier (GID).
- GID general identifiers
- an individual general object identifier (GID) in the conceptual data structure 314 of the general object knowledge base 308 shown in FIG. 3 may have corresponding language data (LNG).
- the language data (LNG) in the general object knowledge base 308 may include one or more words, names, terms, and/or synonyms associated with the particular type of object corresponding to the general object identifier (GID).
- the language data (LNG) may be used in connection with speech and/or text recognition algorithms executed by the cloud processing engine 305 when trying to identify a particular type of object in response to a query from a robot. For example, a robot may send a query to the cloud computing system 304 .
- the query may include a recorded sound clip referring to an object (e.g., a sound recording corresponding to instructions or commands received from a human) or a text excerpt (e.g., text read from packaging on an object or a sign associated with the object).
- the cloud processing engine 305 may analyze the sound clip or text excerpt received from the robot, and then find matching (or at least substantially similar) language data (LNG) in the general object knowledge base 308 .
- LNG language data
- the general object identifier (GID) associated with a particular type of refrigerator may have corresponding language data (LNG) that includes the terms “refrigerator,” “freezer,” “fridge,” “icebox,” etc.
- the general object identifier (GID) associated with a Motorola® DROID X smartphone may have corresponding language data (LNG) that includes the terms “phone,” “telephone,” “cell,” “cellphone,” “cellular phone,” “cellular telephone,” “mobile,” “mobile phone,” “mobile telephone,” “smartphone,” “DROID,” “DROID phone,” “DROID X,” “Android,” “Android phone,” etc.
- the language data (LNG) may have terms that account for differences in regional dialects.
- the general object identifier (GID) associated with can of Pepsi® may have corresponding language data (LNG) that includes the terms “Pepsi®,” “soda,” “pop,” “soda pop,” “soft drink” and “coke.” Even though a Pepsi® is not a Coca-Cola®, people in some areas may colloquially refer to a Pepsi® as a “coke” in general.
- the language data (LNG) may also have terms that account for different languages.
- some general object identifiers (GID) may have more or less corresponding language data (LNG) that other general object identifiers (GID).
- An individual general object identifier (GID) in the conceptual data structure 314 of the general object knowledge base 308 shown in FIG. 3 may also have corresponding image data (IMG).
- the image data (IMG) in the general object knowledge base 308 may include one or more images, 3D models, point clouds, or other graphical representations associated with the particular type of object corresponding to the general object identifier (GID).
- the image data (IMG) may also be used by object recognition algorithms.
- the image data (IMG) may be used in connection with image recognition algorithms executed by the cloud processing engine 305 to identify a particular object in response to a query from a robot.
- a robot may send an image of an object (or other image data corresponding to the object) to the cloud processing engine 305 with a request or query to identify the object in the image.
- the cloud processing engine 305 may analyze the image data received from the robot and find matching (or at least substantially similar) images (or other matching graphical data) from the image data (IMG) in the general object knowledge base 308 to determine whether the object in the image data received from the robot corresponds to a general object identifier (GID) in the general object knowledge base 308 .
- IMG image data
- GID general object identifier
- the cloud processing engine 305 may send the robot a response to the query.
- the response may include (i) an identification of the “type” of object and (ii) data corresponding to the “type” of object.
- An individual general object identifier (GID) in the conceptual data structure 314 of the general object knowledge base 308 shown in FIG. 3 may also have corresponding manufacturer data (MFG) and model data (MOD) if applicable.
- the manufacturer data (MFG) may include the manufacturer, maker, builder, or producer associated with the particular type of object corresponding to the general object identifier (GID).
- the manufacturer data (MFG) may also include information about the object's manufacturer, maker, builder, producer, etc. (e.g., manufacturer contact information, website, etc.).
- the model data (MOD) may include a model number, variant, or other useful designation assigned to a particular type of object by the object's manufacturer, maker, builder, producer, etc.
- the manufacturer data (MFG) corresponding to the general object identifier (GID) for a Ford® Mustang car may include data associated with the Ford Motor Company, e.g., “Ford,” “http://www.ford.com/”, or other similar data.
- the model data (MOD) corresponding to the general object identifier (GID) for a Ford® Mustang car may include data associated with a particular model, e.g., “2012,” “Boss 302 ,” a factory model designation, or other similar data associated with the particular model of car.
- the manufacturer data (MFG) corresponding to the general object identifier (GID) for a paper coffee cup manufactured by Georgia- Pacific may include data associated with Georgia- Pacific, e.g., “Georgia- Pacific”, “http://www.gp.com/, or other similar data.
- the model data (MOD) corresponding to the general object identifier (GID) for the paper coffee cup may include data associated with the particular product number of the cup.
- some manufacturers may even choose to provide the shared robot knowledge base 306 with a link to a proprietary or private database of information about a particular product so that any updates that the manufacturer might make to the specifications of the product can be accessed by robots via the shared robot knowledge base 304 .
- Some “types” of items may not have a manufacturer or model number.
- the different general object identifiers (GID) for the different “types” of apples described above may not have corresponding manufacturer data (MFG) or model data (MOD).
- a coded sticker affixed to the apple may contain information associated with a particular apple farm, e.g., “Happy Time Apple Farms,” and a harvest date, “Oct. 15, 2011.”
- a general object identifier (GID) for apples from “Happy Time Apple Farms” may have manufacturer data (MFG) associated with “Happy Time Apple Farms” and model data (MOD) associated with the “Oct. 15, 2011” harvest date.
- MFG manufacturer data
- MOD model data
- other apples without coded stickers may not have corresponding manufacturer (MFG) or model data (MOD).
- An individual general object identifier (GID) in the conceptual data structure 314 of the general object knowledge base 308 shown in FIG. 3 may also have corresponding specification data (SPC).
- the specification data (SPC) may include physical attributes associated with the particular “type” of object corresponding to the general object identifier (GID).
- specification data (SPC) may include many different physical attributes of a particular “type” of object, e.g., physical dimensions (height, width, depth, shape, volume, area, etc.), weight, mass, color, texture, material composition, etc.
- a particular “type” of coffee mug may have a height (6 inches), width (4 inches), shape (cylindrical), volume (16 ounces), weight (1 ⁇ 4 pound), color (silver), texture (smooth), and material composition (stainless steel), etc.
- Other more complex types of objects may have additional specification data (SPC) above and beyond the basic physical attributes described above.
- SPC additional specification data
- a car may additionally have more complex specification data (SPC) including miles per gallon, acceleration, top speed, gas tank capacity, trunk capacity, seating capacity, tire air pressure, etc.
- the specification data (SPC) associated with a particular “type” of apple may include average weight, shape, nutritional information, etc.
- the specification data (SPC) corresponding to some general object identifiers (GID) may be quite extensive (particularly for complex objects), whereas the specification data (SPC) corresponding to other general object identifiers (GID) may be fairly limited (particularly for less complex objects).
- An individual general object identifier (GID) in the conceptual data structure 314 of the general object knowledge base 308 shown in FIG. 3 may also have corresponding task data (TASKS).
- the task data (TASKS) may correspond to tasks that can be performed with the object corresponding to the general object identifier (GID).
- the task data (TASKS) associated with a particular general object identifier (GID) defines the types of interactions that a robot may execute with the physical object corresponding to the general object identifier (GID).
- a general object identifier (GID) associated with a type of paper cup may have task data (TASKS) corresponding to tasks such as (i) grasping the cup, (ii) filling the cup with liquid, (iii) pouring liquid from the cup, (iv) transporting the cup, (v) setting the cup on a surface, (vi) crushing the cup, and (vii) dropping the cup into a trashcan, for example.
- GID general object identifier
- TSKS task data
- tasks such as (i) grasping the cup, (ii) filling the cup with liquid, (iii) pouring liquid from the cup, (iv) transporting the cup, (v) setting the cup on a surface, (vi) crushing the cup, and (vii) dropping the cup into a trashcan, for example.
- a general object identifier (GID) associated with a refrigerator may have entirely different task data (TASKS) corresponding to tasks such as (i) opening the refrigerator door, (ii) opening the freezer door, (iii) opening a drawer inside the refrigerator, (iv) closing a drawer inside the refrigerator, (v) closing the freezer door, (vi) closing the refrigerator door, (vii) activating/deactivating the ice dispenser, and (viii) activating/deactivating the water dispenser, for example.
- TSKS task data
- a general object identifier (GID) associated with a refrigerator that does not have a water or ice dispenser may not have corresponding task data (TASKS) corresponding to tasks associated with water or ice dispensers, such as activating/deactivating the ice dispenser or water dispenser, for example.
- GID general object identifier
- TSKS task data
- Particularly complex objects such a car, may even have corresponding task data (TASKS) that includes robot instructions for driving the car.
- TSKS task data
- a robot when a robot (with the assistance of the cloud computing system 304 ) identifies a particular car, the robot may also be able to access instructions for driving that particular type of car.
- task data (TASKS) for driving a car may include physical movements, e.g., grasping and turning a steering wheel, grasping and moving a transmission controller to put the gar in gear, etc.
- the robot may be able to simply connect to the car via a wireless or wired communications link, and pilot the car electronically rather than having to manually control a steering wheel, gas pedal, or break pedal.
- TAS task data
- a robot could control a communication-enabled television, microwave oven or stove, clothes washer/drier, dish washer, home lighting system, home security system, or any other type of communications-enable device.
- the task data may include instruction code for controlling and operating grasping claws, robot hands, tool/utensil attachments, rolling bases, or other similar types of electromechanical actuation device configured for use with a robot.
- the task data (TASKS) may also include instruction code for controlling and operating various sensors on a robot, such as any of the sensors described herein.
- the shared robot knowledge base 304 may not require a separate task knowledge base 309 .
- the task data may include (i) task identifiers (TID) corresponding to instruction code that can be looked up in a separate task knowledge base 309 and (ii) task values corresponding to a particular task identifier (TID).
- TID task identifier
- the task identifier (TID) may correspond to a “generic” task (e.g., “grasping”), while the task values may “customize” the generic task to a particular object.
- the tasks data may include (i) a task identifier (TID) corresponding to a generic grasping task and (ii) task values for using the generic grasping task with the particular paper cup.
- TID task identifier
- the instructions for the “grasping” task may retrieved from the task knowledge base 309 based on the task identifier (TID) for the “grasping” task stored in the general object knowledge base 308 for the general object identifier (GID) associated with the paper cup.
- TID task identifier
- GID general object identifier
- the general “grasping” task can be customized for use with the paper cup by executing the “grasping” task with the task values that are specific to the paper cup.
- the task values specific to the paper cup may include values for variables in the generic “grasping” task instructions retrieved from the task knowledge base 309 .
- These task values may define the position and attitude of a grasping claw when approaching the paper cup, how wide to open the grasping claw to position the fingers of the claw around the paper cup, the amount of force to apply when closing the fingers of the grasping claw around the paper cup, and how much to close the fingers of the grasping claw around the paper cup, for example.
- the cloud computing system may be configured to synthesize or generalize task data (TASKS) associated with a particular object based on its similarity to other objects stored in the general object knowledge base 308 .
- TSKS task data
- a first cup designed to hold 16 ounces of liquid would have a different general object identifier (GID) than a second cup designed to hold 12 ounces of liquid.
- GID general object identifier
- the two types of cups may also have slightly different task data (TASKS) as well.
- the robot may collect specification data (SPC) about the cup for uploading to the shared robot knowledge base 306 where the new third cup can be associated with a new general object identifier (GID). Based on the degree of similarity between the new third cup and the two known cups (the 16 ounce cup and the 12 ounce cup), the cloud computing system 304 may generate estimated task data (TASKS) for the new third cup. For example, “grasping” the new third cup is probably similar to grasping the two known cups. The next time that a robot encounters a cup of the same type as the new third cup and queries the cloud computing system 304 , the cloud computing system 304 should be able to identify the cup and send data about the cup to the robot that sent the query.
- SPC specification data
- GID general object identifier
- the shared robot knowledge base 306 may include a task knowledge base 309 .
- the task knowledge base 309 may be configured to store data and task instruction code associated with different tasks that a robot may perform to interact with objects in its environment.
- an individual task identifier may have the following corresponding data: (i) language data (LNG); (ii) equipment data (EQP); and (iii) instruction data (INST).
- LNG language data
- EQP equipment data
- INDT instruction data
- the conceptual data structure 315 of the task knowledge base 309 may contain more or less information than the example shown in FIG. 3 .
- a task identifier corresponds to a particular task that a robot may perform to interact with an object in its environment.
- tasks may include grasping an object, placing an object on a surface, opening a door, closing a door, opening a drawer, closing a drawer, pushing an object, pulling an object, lifting an object, rotating an object, throwing an object, catching an object, traveling to an object, etc.
- Tasks may also include non-physical interactions with an object, such as establishing a communications channel with an object, sending data to an object, retrieving data from an object, etc.
- the task knowledge base 309 may have many billions of task identifiers (TID) corresponding to tasks that different types of robots equipped with different types of electromechanical devices may perform or execute with the various objects in the unique object knowledge base 307 and the general object knowledge base 308 .
- an individual task identifier (TID) in the conceptual data structure 315 of the task knowledge base 309 shown in FIG. 3 may have corresponding language data (LNG).
- the language data (LNG) in the task knowledge base 309 may include one or more words, names, terms, and/or synonyms associated with the particular task corresponding to the task identifier (TID).
- the language data (LNG) may be used in connection with speech and/or text recognition algorithms executed by the cloud processing engine 305 when trying to identify a particular task received in a query from a robot. For example, a robot may send a query to the cloud processing engine 305 .
- the query may include a recorded sound clip referring to a task (e.g., a sound recording corresponding to instructions or commands received from a human) or a text excerpt (e.g., text read from a command).
- the sound processing engine 305 may analyze the sound clip or text excerpt received from the robot, and then find matching (or at least substantially similar) language data (LNG) in the task knowledge base 309 to identify the corresponding task.
- LNG language data
- An individual task identifier (TID) in the conceptual data structure 315 of the task knowledge base 309 shown in FIG. 3 may also have corresponding instruction data (INST).
- the instruction data (INST) may include robot executable instruction code for controlling and operating grasping claws, robot hands, tool/utensil attachments, rolling bases, or other similar types of electromechanical actuation device configured for use with a robot.
- the instruction data (INST) may also include instruction code for controlling and operating various sensors on a robot, such as any of the sensors described herein.
- Task instruction data may be specific to particular robot modules.
- different electromechanical actuation devices may execute different instruction code depending on the type of electromechanical actuation device. For example, the instruction code executed by a two-finger claw to “grasp” a cup may be different than the instruction code executed by a five-finger robot hand to “grasp” a cup because of the differences between the features and electromechanical capabilities of the two devices. However, both devices may be capable of performing a “grasp” task.
- individual task identifiers (TID) in the conceptual data structure 315 of the task knowledge base 309 shown in FIG. 3 may also have corresponding equipment data (EQP).
- the equipment data (EQ) may include information about the type of electromechanical actuation device that can execute the instruction data (INST) corresponding to a particular task identifier (TID) in the task knowledge base.
- the shared robot knowledge base 306 may also have a map knowledge base 310 .
- the map knowledge base 310 may include inventories of objects located in areas corresponding to different maps.
- the conceptual data structure 316 for the map knowledge base 310 shown in FIG. 3 includes: (i) map identifiers (MID); location data (LOC) and (ii) inventory data (INV).
- the conceptual data structure 316 of the map knowledge base 310 may contain more or less information than the example shown in FIG. 3 .
- An individual map identifier may correspond to a unique map, which may correspond to a defined area.
- an area may be defined by a set of GPS coordinates or other similar location determination mechanisms.
- one map may correspond to a particular apartment.
- the map of the apartment may have related higher-level and lower-level maps.
- the map of the apartment may have a related higher-level map for the floor of an apartment building.
- the map of the apartment may have multiple related lower-level maps, such as a map for the kitchen of the apartment, a map for the living room of the apartment, and a map for a bedroom of the apartment.
- the map of the kitchen may also have related lower level maps, such as individual maps for individual cabinets and drawers in the kitchen.
- a cabinet may have an even lower-level related map for a box located inside the cabinet or a table in the kitchen, for example.
- Individual map identifiers may also have corresponding location data (LOC).
- the location data may correspond to GPS coordinates associated with the particular map. Location coordinates other than GPS could be used as well.
- Individual map identifiers may also have corresponding inventory data (INV).
- the inventory data (INV) may correspond to inventories of objects associated with the map.
- a cabinet in a kitchen may have a collection of plates, saucers, and cups.
- the inventory data (INV) may include information associated with the number and type of each plate, saucer, and cup in the particular cabinet.
- Objects such as the particular plate, the particular saucer, and the particular cup may have corresponding general object identifiers (GID) in the general object knowledge base 308 .
- GID general object identifiers
- the inventory data (INV) associated with the map of the cabinet may include quantities of each of the particular general object identifiers (GID) corresponding the plates, saucers, and cups. Because information about the plates, saucers, and cups is contained in the general object knowledge base 308 , object-specific information does not need to be reproduced in the map knowledge base 310 . Instead, detailed information about the objects in the cabinet can be obtained from the general object knowledge base 308 . Similarly, detailed information about how to interact with the objects can be obtained from the task knowledge base 309 .
- GID general object identifiers
- access to certain maps and/or inventory data associated with those maps may be limited or restricted to certain robots while other maps and their corresponding inventory data may be freely accessible by all robots.
- a person may restrict access to maps and inventory data associated with his or her apartment to all robots except for a small number of robots that the person owns or regularly uses.
- a store owner may grant access to maps and inventory data associated with public areas of his or her store to all robots but restrict access to maps and inventory data associated with non-public areas of the store to a limited number of trusted robots.
- the cloud computing system 304 can be configured to anonymize the data stored in the map knowledge base 310 to provide generalized statistics to facilitate robot learning and operation. For example, if an individual robot is unsure about where to store a carton of ice cream, a query to the map knowledge base 310 may conclude that most people store ice cream in a freezer based on anonymized inventory data (INV) across many maps.
- ISV anonymized inventory data
- the cloud computing system 304 can still provide useful instructions to the robot (i.e., store the ice cream in the freezer) based on anonymized inventory data (INV) in the map knowledge base 310 .
- ISV anonymized inventory data
- the shared robot knowledge base 306 may also include an application knowledge base 311 .
- the application knowledge base 311 may be one component of a robot application store from where robot users may purchase and download robot applications for installation and/or execution by on their robots. In other embodiments, the application knowledge base 311 may be separate from the robot application store.
- the application knowledge base 311 may include different robot applications that can be performed by a robot.
- some applications in the application knowledge base 311 may include a collection of tasks (and/or customized versions thereof) from the task knowledge base 316 to be performed with certain objects from the general object knowledge base 308 or the unique object knowledge base 307 .
- Some applications in the application knowledge base 311 may even include a collection of other application (and/or customized versions thereof) from the application knowledge base 311 .
- the applications in the application knowledge base 311 may not be limited only to collections of (i) tasks stored in the task knowledge base 309 , (ii) objects stored in the unique object knowledge base 307 and/or the general object knowledge base 308 , and/or (iii) other applications stored in the application knowledge base 311 .
- applications that rely on objects, tasks, and applications stored elsewhere in the shared robot knowledge base 306 may benefit from updates to object, task, and application information by humans and/or robots, as described herein.
- One example of an application that may reside in the application knowledge base 311 might be an application named “Groceries” that includes robot instructions for removing groceries from shopping bags and storing the groceries in their appropriate places in a person's home.
- a robot programmer may write the “Groceries” application, and then publish the application in the application knowledge base 311 .
- a robot may download the “Groceries” application from the application knowledge base 311 (or alternatively execute the “Groceries” application via the cloud computing system 304 ).
- the “Groceries” application may have been written by a robot programmer, a person may “coach” the robot as it executes the “Groceries” application. For example, if the robot cannot identify a particular grocery item, the robot may ask the person what the grocery item is and where the grocery item should be stored. Based on the response from the person, the robot may update information in the object knowledge base 308 regarding the identity of the grocery item. The robot may also update information in the map knowledge base 310 with the location of the grocery item.
- the cloud computing system 304 may be configured to synthesize certain rules for robots to follow when executing the “Groceries” application.
- some rules that could be synthesized or generalized from a large number of robot experiences might include (i) unidentifiable frozen items should be stored in a freezer, (ii) identifiable cold items should be stored in a refrigerator, (iii) certain types of vegetables should be stored in a certain refrigerator drawer, (iv) items should not be placed on top of bread in a pantry, etc.
- robot experiences including “coaching” from humans
- the “Fetch” application may include robot instructions for playing fetch with a dog.
- the “Fetch” application may be created by a human with limited or novice-level robot programming skills.
- the person could instruct the robot to record a series of tasks (similar to a macro) including (i) grasping a ball from the ground, (ii) throwing the ball, (iii) waiting for a dog to return with the ball, (iv) praising the dog, and (v) repeating tasks (i)-(iv).
- the robot executes the “Fetch” application the person may adjust or further configure certain aspects of the individual tasks via a robot user interface on a smartphone, tablet computer, etc. to “fine tune” the application.
- the person may increase or decrease the force with which the robot throws the ball or vary the praise given to the dog upon returning with the ball, e.g., “Good dog!” Creating the “Fetch” application in this manner is akin to “teaching” the robot a new skill.
- the person may upload the application (or perhaps instruct the robot to upload the application) to the application knowledge base 311 .
- the application knowledge base 311 Once uploaded to the application knowledge base 311 , other robots can download and/or execute the “Fetch” application.
- the skill i.e., playing fetch with a dog
- the skill learned by a single robot can be shared with all other robots via the application knowledge base 311 so that each robot does not have to learn the skill on its own.
- some applications in the application knowledge base 311 may build upon data from other knowledge bases.
- the “Fetch” application may rely on a “grasp” task from the task knowledge base 309 and a “ball” object from the general object knowledge base 308 .
- the individual robots can update data in the shared robot knowledge base 306 based on their individual experiences.
- the cloud computing system 304 can optimize the “Fetch” application based on the collective experiences of many different robots.
- that robot gains the benefit of the collective experiences of many other robots that have executed the “Fetch” application in the past.
- a robot executing the “Make Dinner” application might first determine the inventory of food items (e.g., flour, milk, eggs, rice, soda, cookies, spinach, ground beef, cheese, tomatoes, potatoes, etc.) available in a person's house by either or both (i) identifying items in the person's refrigerator and pantries and/or (ii) querying the map knowledge base 310 to determine the inventory of available food.
- food items e.g., flour, milk, eggs, rice, soda, cookies, spinach, ground beef, cheese, tomatoes, potatoes, etc.
- the robot may also determine the inventory of available food preparation tools (i.e., pots, pans, oven, kitchen tools, utensils, etc.) available in the person's house by either or both (i) identifying the available food preparation tools in the person's kitchen and/or (ii) querying the map knowledge base 310 to determine the inventory of available food preparation tools.
- available food preparation tools i.e., pots, pans, oven, kitchen tools, utensils, etc.
- the inventory can be compared to requirements for various food preparation applications in the application knowledge base 311 , e.g., “Make Lasagna,” “Make Pot Roast,” “Make Spinach Salad,” “Make Baked Potatoes,” etc.
- This type of comparison is an example of one application in the application knowledge base 311 , i.e., “Make Dinner,” leveraging other applications in the application knowledge base 311 .
- the robot may suggest to the person that the robot prepare a particular meal.
- the suggested meal would be a meal for which the person has all of the required food items and food preparation tools based on the inventory of food items and food preparation tools from the map knowledge base 310 .
- the robot might suggest to the human that it could prepare a particular meal, e.g., spaghetti, if one or more additional items were available, e.g., noodles.
- the robot could also access map data about local grocery stores from the map knowledge base 310 to identify the closest grocery store that has noodles in stock.
- FIG. 4 shows a message flow diagram between multiple robots 404 , 413 and a cloud computing system 401 having a shared robot knowledge base 403 according to some embodiments of the disclosed systems and methods.
- the example cloud computing system 401 shown in FIG. 4 includes a cloud processing engine 402 and a shared robot knowledge base 403 .
- the robots 404 , 413 , the cloud computing system 401 , the cloud processing engine 402 , and the shared robot knowledge base 403 may similar to the robots, cloud computing systems, cloud processing engines, and shared robot knowledge bases described herein with respect to FIGS. 1-3 .
- the horizontal lines between the robots 404 , 413 , the cloud processing engine 402 , and the shared robot knowledge base 403 illustrate message flows between the different entities.
- the robot 404 obtains identification information associated with an object 405 via one or more sensors associated with the robot 404 .
- the robot 404 obtains image data (e.g., an image, a 3D image, a point cloud, etc.) associated with the object 405 via optical sensors.
- the robot 404 may use other types of sensors to obtain other types of identification information associated with the object 405 , e.g., information from an RFID tag affixed to or associated with the object 404 , etc. as described elsewhere herein.
- the robot 404 sends an identification query 406 to the cloud processing engine 402 .
- the identification query 406 may include at least some of the image data associated with the object 405 .
- the cloud processing engine 402 may analyze the image data. The analysis may include extracting meaningful information from the image via digital image processing techniques. For example, the cloud processing engine 402 may extract certain features from the image data, e.g., the outline, texture, and/or color of the object 405 .
- the cloud processing engine 402 may also extract text, bar codes, QR codes, and/or product logos from the image of the object 405 for analysis.
- the cloud processing engine 402 may extract the star-shaped product logo appearing on the object 405 .
- the cloud processing engine 402 may use the results of the image analysis to send a query 407 the shared robot knowledge base 403 .
- the cloud processing engine 401 may query an object knowledge base component of the shared robot knowledge base 403 similar to the general object knowledge base 308 or the unique object knowledge base 307 shown and described herein with respect to FIG. 3 .
- the cloud processing engine 402 may receive a response 408 from the shared object knowledge base 403 in response the query 407 .
- the response 408 may include a list of items that appear to match the object 405 , and the cloud processing engine 402 may select the most-likely match.
- the response 408 from the shared robot knowledge base 403 may simply include the most-likely match.
- cloud computing system 401 may conclude that the object 405 is a cup of coffee.
- the conclusion that the object 405 is a cup of coffee may be based on the size and shape of the cup, color of the liquid in the cup, and the star-shaped product logo on the cup. If the robot 404 is equipped with a thermal imaging sensor, it could also determine the temperature of the liquid within the cup, or at least determine whether the liquid is hot or cold.
- the response 408 may also include additional information associated with the cup of coffee that may be stored in the shared robot knowledge base 403 .
- the response 408 may include specifications of the cup of coffee, such as, (i) the dimensions (height, circumference) of the cup, (ii) the weight of the cup when empty, (iii) the weight of cup when filled with liquid, (iv) the material that the cup is made from (e.g., paper, plastic, Styrofoam, ceramic, etc.), or any other similar object data similar to the object data shown and described herein with respect to the general object knowledge base 308 of FIG. 3 .
- the response 408 may also include task information associated with the cup, such as, (i) how to grasp the cup, (ii) how to pick up the cup, (iii) how to carry the cup, or any other task data similar to the task data shown and described herein with respect to the task knowledge base 309 of FIG. 3 .
- the response 408 need not return all the information stored about the cup in the shared robot knowledge base 403 . Instead, the response 408 may contain just the information about the cup that the cloud processing engine 402 requested from the shared robot knowledge base 403 .
- the cloud processing engine 402 may determine which information to request from the shared robot knowledge base 403 based on a particular application or task that that the robot may be performing at the time.
- the cloud computing system 401 can send object data 409 associated with the cup 405 to the robot 404 in response to the identification query 406 received from the robot 404 .
- the object data 409 may include both the identity of the cup 405 and instructions for interacting with the cup 405 based on the application or task that the robot 404 has been instructed to execute or perform.
- the robot 404 may be performing an application that includes moving the cup 405 from one location to another location. To move the cup 405 , the robot 404 must first grasp the cup 405 with its robot hand.
- the instructions for grasping the cup 405 may include instructions for positioning the robot hand around the cup 405 , the amount of force to use when grasping the cup 405 , and how tightly to close the fingers of the robot hand around the cup 405 .
- the robot 404 can execute the instructions to grasp the cup 405 .
- the robot 404 grasps the cup 405 according to the instructions it received in the object data 409 from the cloud computing system 401 , the robot 404 crushes the cup 405 and causes the coffee to spill 410 .
- the grasping force specified in the instructions was too high, and as a result, the robot crushed the cup and caused a mess.
- a human can “coach” the robot on how much force to use by, for example, manually controlling the robot's hand to grasp a new cup.
- the robot can capture the grasping force that it used to successfully grasp the cup under the manual control of the human, and then send feedback 411 to the cloud processing system 402 with the modified grasping force.
- the cloud processing system 402 can then update 412 task and object data in the shared robot knowledge base 403 to improve how the “grasp” task is applied this particular “cup” object.
- a second robot 413 may encounter an object 414 while performing the same (or a similar) application that requires grasping the same type of cup.
- the second robot 413 may send image data associated with the object 414 in an identification query 415 to the cloud computing system 401 .
- the cloud processing engine 402 may analyze the image data in the identification query 415 from the robot 413 , send a query 416 to the share robot knowledge base 403 , and receive a response 417 from the shared robot knowledge base 403 .
- the cloud computing system 401 can send object data 418 associated with the cup 414 to the robot 413 in response to the identification query 415 received from the robot 413 .
- the object data 418 may include both the identity of the cup 414 and instructions for interacting with the cup 414 based on the application or task that the robot 413 has been instructed to execute or perform.
- the robot 413 may be performing a task that requires the robot 413 to grasp the cup 414 .
- the instructions for interacting with the cup 414 may include instructions for positioning the robot hand around the cup 414 , the amount of force to use when grasping the cup 414 , and how tightly to close the fingers of the robot hand around the cup 414 .
- the cloud computing system 401 updated the amount of force to use when grasping the particular type of cup that the robot 413 must now grasp (i.e., cup 414 ) based on the feedback 411 previously received from robot 404 regarding the same type of cup.
- the robot 413 when the robot 413 applies the “grasp” task to the type of “cup” object here (i.e., when robot 413 grasps cup 414 ), the robot 413 successfully grasps the cup without crushing the cup 419 .
- the second robot 413 has in effect learned from the experience of the earlier robot 404 .
- the robot 413 may send feedback 420 to the cloud processing engine 402 that confirms the accuracy of the instructions that the robot 413 received in the object data 418 from the cloud computing system 401 .
- FIG. 5 shows an example method 500 of using a shared robot knowledge base according to some embodiments of the disclosed systems and methods.
- the shared robot knowledge base may be similar to any of the shared robot knowledge bases shown and described herein with respect to FIGS. 1-4 .
- the method begins at block 501 when a cloud computing system receives an object query from a robot.
- the query received from the robot may similar to the other robot-generated queries described elsewhere herein.
- the cloud computing system may determine that the object in the query from the robot is a “new” object that cannot be found in the shared robot knowledge base.
- the cloud computing system may create a new object identifier in the shared robot knowledge base for the new object.
- the cloud computing system may create a new general object identifier (GID) for the new object in an object knowledge base component of the robot knowledge base, such as the general object knowledge base 308 shown and described herein with respect to FIG. 3 .
- GID general object identifier
- the cloud computing system may instruct the robot to collect additional information about the new object, such as the weight, dimensions, or other type of information.
- the robot may collect the requested information about the new object, and then send the requested information to the cloud computing system.
- the cloud computing system may store the received information about the new object in the shared robot knowledge base.
- the cloud computing system may store the received information as specification data (SPC) associated with the new general object identifier (GID) in the general object knowledge base 308 shown and described herein with respect to FIG. 3 .
- SPC specification data
- GID new general object identifier
- the cloud computing system may generate additional data about the new object based on the similarities between the new object and other objects already stored in the shared robot knowledge base. For example, if the new object is a new type of cup, then the cloud computing system may generate task information about the new type of cup based on information about other similar cups, such as how to grasp the new cup, how to fill the new cup with liquid, how to carry the new cup when the new cup is filled with liquid, how to set the cup on a surface, how to pour liquid from the new cup, how to crush the cup, how to dispose of the cup in a recycling bin, or other types of tasks or information about the new cup.
- task information about the new type of cup based on information about other similar cups, such as how to grasp the new cup, how to fill the new cup with liquid, how to carry the new cup when the new cup is filled with liquid, how to set the cup on a surface, how to pour liquid from the new cup, how to crush the cup, how to dispose of the cup in a recycling bin, or other types of tasks or information about the new
- the cloud computing system can identify the new cup in the shared robot knowledge base and provide the other robot with instructions for how to use or otherwise interact with the cup. In this manner, later robots benefit from the information about the cup that was collected and sent to the shared robot knowledge base by the earlier robot.
- the cloud computing system can further leverage information collected about objects by robots and enhance and/or augment robot learning by analyzing similarities between a newly-discovered object (i.e., newly-discovered by robots at least) and known objects (i.e., objects already stored in the shared robot knowledge base) to predict how robots should interact with the newly-discovered object.
- FIG. 6 shows another example method 600 of using a shared robot knowledge base according to some embodiments of the disclosed systems and methods.
- the shared robot knowledge base may be similar to any of the shared robot knowledge bases shown and described herein with respect to FIGS. 1-4 .
- the method 600 begins at block 601 where a cloud computing system receives a first query from a first robot.
- the first robot may be similar to any of the robots shown and described herein with respect to FIGS. 1-4 .
- the first query may include identification information associated with an object.
- the identification information may include any type of object identification described herein, e.g., image data, textual data, sound data, etc.
- the cloud computing system accesses the shared robot knowledge base based on the information contained in the first query.
- the shared robot knowledge base may be accessed for the purpose of (i) recognizing and/or identifying the object of the first query, (ii) obtaining information about the object of the first query, and/or (iii) obtaining instructions for interacting with the object of the first query.
- the cloud computing system sends data associated with the object of the first query to the first robot in response to the first query received from the first robot at block 601 .
- the data sent to the first robot at block 603 may include (i) a name or identity of the object of the first query (if the name or identity was not already a component of the first query at block 501 ), (ii) one or more attributes of the object of the first query, and/or (iii) instructions for interacting with the object of the first query.
- the data associated with the object of the first query that is sent to the first robot in block 603 may be any of the types of object-related data (or other types of data) that may be stored in the shared robot knowledge base as described herein.
- the data may include any of the data or types of data about objects (specifications, manufacturer, etc.) or otherwise associated with objects (task data, map data, etc.) shown and described herein with respect to FIGS. 1-4 .
- the cloud computing system receives feedback from the first robot.
- the feedback may be based on the result of a comparison between (i) an attribute of the object sent to the first robot at block 503 and (ii) the first robot's separate determination (independently or via human coaching) of the attribute. For example, if the data sent to the first robot at block 603 specified the weight of the object of the first query as being 1 pound, but the first robot independently determined that the weight of the object of the first query was 2 pounds, then the feedback received at block 604 may be an indication of the determined discrepancy.
- the feedback may additionally or alternatively include a revision to instructions for interacting with the object of the first query based on the robot's experience with following the instructions sent at block 603 .
- the feedback received at block 504 may be an indication of the determined discrepancy.
- the feedback may be based on coaching received by the robot from a human, such as the coaching described elsewhere herein, such as the coaching shown and described herein with respect to FIG. 4 .
- the cloud computing system may revise the data in the shared robot knowledge base based on the feedback received at block 604 .
- the cloud computing system receives a second query from a second robot.
- the second query may contain identification information associated with an object that the second robot needs to interact with.
- the object that the second robot needs to interact with may be the same type of object as the object of the first query described above.
- the type of information in the second query may be similar to the type of information in the first query received at block 601 even if the contents of the second query are not identical to the contents of the first query.
- the cloud computing system accesses the shared robot knowledge base for the purpose of (i) recognizing and/or identifying the object of the second query, (ii) obtaining information about the object of the second query, and/or (iii) obtaining instructions for interacting with the object of the second query.
- the cloud computing system sends data associated with the object of the second query to the second robot in response to the second query received from the second robot at block 606 .
- the data may include (i) a name or identity of the object of the second query (if the name or identity was not already a component of the second query at block 606 ), (ii) one or more attributes of the object of the second query, and/or (iii) instructions for interacting with the object of the second query.
- the data associated with the object of the second query sent to the second robot at block 608 may include the revised data from block 605 that was based on the feedback from the first robot at block 604 .
- the second robot receives the revised data from block 605 that was based on the feedback from the first robot at block 604 , the second robot has effectively learned from the previous experience of the first robot even though the first robot and the second robot may not have ever directly communicated with one another.
Abstract
Description
Claims (24)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/464,699 US8639644B1 (en) | 2011-05-06 | 2012-05-04 | Shared robot knowledge base for use with cloud computing system |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201161483291P | 2011-05-06 | 2011-05-06 | |
US201261588944P | 2012-01-20 | 2012-01-20 | |
US13/464,699 US8639644B1 (en) | 2011-05-06 | 2012-05-04 | Shared robot knowledge base for use with cloud computing system |
Publications (1)
Publication Number | Publication Date |
---|---|
US8639644B1 true US8639644B1 (en) | 2014-01-28 |
Family
ID=49958045
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/464,699 Active US8639644B1 (en) | 2011-05-06 | 2012-05-04 | Shared robot knowledge base for use with cloud computing system |
Country Status (1)
Country | Link |
---|---|
US (1) | US8639644B1 (en) |
Cited By (106)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120022689A1 (en) * | 2010-07-23 | 2012-01-26 | Agile Planet, Inc. | System and Method for Robot Safety and Collision Avoidance |
US20130345873A1 (en) * | 2012-06-21 | 2013-12-26 | Rethink Robotics, Inc. | Training and operating industrial robots |
US20140012415A1 (en) * | 2012-07-09 | 2014-01-09 | Technion Research & Development Foundation Limited | Natural machine interface system |
US20140032033A1 (en) * | 2012-07-27 | 2014-01-30 | Honda Research Institute Europe Gmbh | Trainable autonomous lawn mower |
US20140180479A1 (en) * | 2012-12-20 | 2014-06-26 | Wal-Mart Stores, Inc. | Bagging With Robotic Arm |
US20140207281A1 (en) * | 2013-01-18 | 2014-07-24 | Irobot Corporation | Environmental Management Systems Including Mobile Robots and Methods Using Same |
US20140316636A1 (en) * | 2013-04-23 | 2014-10-23 | Samsung Electronics Co., Ltd. | Moving robot, user terminal apparatus and control method thereof |
US8984136B1 (en) * | 2011-05-06 | 2015-03-17 | Google Inc. | Systems and methods for object recognition |
US20150094852A1 (en) * | 2013-09-27 | 2015-04-02 | Brain Corporation | Robotic control arbitration apparatus and methods |
US20150224648A1 (en) * | 2014-02-13 | 2015-08-13 | GM Global Technology Operations LLC | Robotic system with 3d box location functionality |
US20150258684A1 (en) * | 2014-03-14 | 2015-09-17 | Seiko Epson Corporation | Robot, robot system, and control device |
US9144905B1 (en) * | 2013-03-13 | 2015-09-29 | Hrl Laboratories, Llc | Device and method to identify functional parts of tools for robotic manipulation |
US9216509B2 (en) | 2014-04-10 | 2015-12-22 | Smartvue Corporation | Systems and methods for automated cloud-based analytics for security and/or surveillance |
US20150381949A1 (en) * | 2014-04-10 | 2015-12-31 | Smartvue Corporation | Systems and Methods for an Automated Cloud-Based Video Surveillance System |
US9233472B2 (en) | 2013-01-18 | 2016-01-12 | Irobot Corporation | Mobile robot providing environmental mapping for household environmental control |
US9242372B2 (en) | 2013-05-31 | 2016-01-26 | Brain Corporation | Adaptive robotic interface apparatus and methods |
WO2016014774A1 (en) * | 2014-07-24 | 2016-01-28 | Google Inc. | Methods and systems for generating instructions for a robotic system to carry out a task |
US9248569B2 (en) | 2013-11-22 | 2016-02-02 | Brain Corporation | Discrepancy detection apparatus and methods for machine learning |
US9314924B1 (en) | 2013-06-14 | 2016-04-19 | Brain Corporation | Predictive robotic controller apparatus and methods |
US20160129592A1 (en) * | 2014-11-11 | 2016-05-12 | Google Inc. | Dynamically Maintaining A Map Of A Fleet Of Robotic Devices In An Environment To Facilitate Robotic Action |
WO2016073103A1 (en) * | 2014-11-03 | 2016-05-12 | Qualcomm Incorporated | Communicating configurable instruction sets to robots for controlling robot behavior |
US9346167B2 (en) | 2014-04-29 | 2016-05-24 | Brain Corporation | Trainable convolutional network apparatus and methods for operating a robotic vehicle |
US9358685B2 (en) | 2014-02-03 | 2016-06-07 | Brain Corporation | Apparatus and methods for control of robot actions based on corrective user inputs |
US9380922B2 (en) | 2013-01-18 | 2016-07-05 | Irobot Corporation | Environmental management systems including mobile robots and methods using same |
WO2016055159A3 (en) * | 2014-10-11 | 2016-07-14 | Audi Ag | Method for operating an automatically driven, driverless motor vehicle and monitoring system |
KR20160087168A (en) | 2015-01-13 | 2016-07-21 | 서울시립대학교 산학협력단 | System and method for control robot based cloud knowledge sharing |
US9405979B2 (en) | 2014-04-10 | 2016-08-02 | Smartvue Corporation | Systems and methods for automated cloud-based analytics and 3-dimensional (3D) display for surveillance systems |
US9407879B2 (en) | 2014-04-10 | 2016-08-02 | Smartvue Corporation | Systems and methods for automated cloud-based analytics and 3-dimensional (3D) playback for surveillance systems |
US9407880B2 (en) | 2014-04-10 | 2016-08-02 | Smartvue Corporation | Systems and methods for automated 3-dimensional (3D) cloud-based analytics for security surveillance in operation areas |
US9420238B2 (en) | 2014-04-10 | 2016-08-16 | Smartvue Corporation | Systems and methods for automated cloud-based 3-dimensional (3D) analytics for surveillance systems |
US9426428B2 (en) | 2014-04-10 | 2016-08-23 | Smartvue Corporation | Systems and methods for automated cloud-based analytics and 3-dimensional (3D) display for surveillance systems in retail stores |
US20160246297A1 (en) * | 2015-02-24 | 2016-08-25 | Siemens Corporation | Cloud-based control system for unmanned aerial vehicles |
US9463571B2 (en) | 2013-11-01 | 2016-10-11 | Brian Corporation | Apparatus and methods for online training of robots |
US9486922B2 (en) * | 2012-02-07 | 2016-11-08 | X Development Llc | Systems and methods for determining a status of a component of a device |
US20160327925A1 (en) * | 2015-05-08 | 2016-11-10 | Ciambella Ltd. | Method and apparatus for modifying behavior of code for a controller-based device |
US9566710B2 (en) | 2011-06-02 | 2017-02-14 | Brain Corporation | Apparatus and methods for operating robotic devices using selective state space training |
US9579789B2 (en) | 2013-09-27 | 2017-02-28 | Brain Corporation | Apparatus and methods for training of robotic control arbitration |
US9597797B2 (en) | 2013-11-01 | 2017-03-21 | Brain Corporation | Apparatus and methods for haptic training of robots |
US9604359B1 (en) | 2014-10-02 | 2017-03-28 | Brain Corporation | Apparatus and methods for training path navigation by robots |
US9616568B1 (en) | 2015-08-25 | 2017-04-11 | X Development Llc | Generating a grasp affordance for an object based on a thermal image of the object that is captured following human manipulation of the object |
CN106577345A (en) * | 2016-10-27 | 2017-04-26 | 重庆掌中花园科技有限公司 | Intelligent pet interactive system |
US9686514B2 (en) | 2014-04-10 | 2017-06-20 | Kip Smrt P1 Lp | Systems and methods for an automated cloud-based video surveillance system |
US9717387B1 (en) | 2015-02-26 | 2017-08-01 | Brain Corporation | Apparatus and methods for programming and training of robotic household appliances |
US9764468B2 (en) | 2013-03-15 | 2017-09-19 | Brain Corporation | Adaptive predictor apparatus and methods |
US9773413B1 (en) * | 2014-09-16 | 2017-09-26 | Knighscope, Inc. | Autonomous parking monitor |
US9792546B2 (en) | 2013-06-14 | 2017-10-17 | Brain Corporation | Hierarchical robotic controller apparatus and methods |
US9798321B2 (en) | 2016-02-18 | 2017-10-24 | Elwha Llc | Package management system for robotic vehicles |
US9804602B2 (en) * | 2016-02-18 | 2017-10-31 | Elwha Llc | Package management system for robotic vehicles |
US9827683B1 (en) | 2016-07-28 | 2017-11-28 | X Development Llc | Collaborative inventory monitoring |
US9875440B1 (en) | 2010-10-26 | 2018-01-23 | Michael Lamport Commons | Intelligent control with hierarchical stacked neural networks |
US9921583B2 (en) | 2016-02-18 | 2018-03-20 | Elwha Llc | Package management system for robotic vehicles |
US9987752B2 (en) | 2016-06-10 | 2018-06-05 | Brain Corporation | Systems and methods for automatic detection of spills |
US10001780B2 (en) | 2016-11-02 | 2018-06-19 | Brain Corporation | Systems and methods for dynamic route planning in autonomous navigation |
US10016896B2 (en) | 2016-06-30 | 2018-07-10 | Brain Corporation | Systems and methods for robotic behavior around moving bodies |
US20180200884A1 (en) * | 2017-01-16 | 2018-07-19 | Ants Technology (Hk) Limited | Robot apparatus, methods and computer products |
US10032137B2 (en) | 2015-08-31 | 2018-07-24 | Avaya Inc. | Communication systems for multi-source robot control |
US10040201B2 (en) * | 2015-08-31 | 2018-08-07 | Avaya Inc. | Service robot communication systems and system self-configuration |
US20180314887A1 (en) * | 2017-04-28 | 2018-11-01 | Intel Corporation | Learning though projection method and apparatus |
US10124491B2 (en) | 2015-08-31 | 2018-11-13 | Avaya Inc. | Operational parameters |
US20180354129A1 (en) * | 2017-06-09 | 2018-12-13 | Honda Motor Co., Ltd. | Service providing system, database, and service providing device |
WO2018160267A3 (en) * | 2017-02-16 | 2018-12-13 | Indiana University Research And Technology Corporation | Cloud based robotic control systems and methods |
US10169058B2 (en) * | 2015-09-24 | 2019-01-01 | Voodoo Robotics, Inc. | Scripting language for robotic storage and retrieval design for warehouses |
US10173320B1 (en) | 2016-06-08 | 2019-01-08 | X Development Llc | Robot task optimization based on historical task and location correlated durations |
US10217003B2 (en) | 2014-04-10 | 2019-02-26 | Sensormatic Electronics, LLC | Systems and methods for automated analytics for security surveillance in operation areas |
US10241514B2 (en) | 2016-05-11 | 2019-03-26 | Brain Corporation | Systems and methods for initializing a robot to autonomously travel a trained route |
US10259117B2 (en) * | 2016-08-02 | 2019-04-16 | At&T Intellectual Property I, L.P. | On-demand robot virtualization |
US10274325B2 (en) | 2016-11-01 | 2019-04-30 | Brain Corporation | Systems and methods for robotic mapping |
US10282849B2 (en) | 2016-06-17 | 2019-05-07 | Brain Corporation | Systems and methods for predictive/reconstructive visual object tracker |
US10293485B2 (en) | 2017-03-30 | 2019-05-21 | Brain Corporation | Systems and methods for robotic path planning |
US10311731B1 (en) | 2014-09-16 | 2019-06-04 | Knightscope, Inc. | Autonomous parking monitor |
US10350757B2 (en) | 2015-08-31 | 2019-07-16 | Avaya Inc. | Service robot assessment and operation |
US20190244421A1 (en) * | 2015-07-21 | 2019-08-08 | IAM Robotics, LLC | Three dimensional scanning and data extraction systems and processes for supply chain piece automation |
US10377040B2 (en) | 2017-02-02 | 2019-08-13 | Brain Corporation | Systems and methods for assisting a robotic apparatus |
US10409562B2 (en) | 2017-03-14 | 2019-09-10 | Ciambella Ltd. | Method and apparatus for automatically generating and incorporating code in development environments |
US10410176B2 (en) | 2015-05-28 | 2019-09-10 | Hds Mercury, Inc. | Product and equipment location and automation system and method |
WO2019217885A1 (en) * | 2018-05-11 | 2019-11-14 | Brown University | Augmented reality interface to robots |
US10510000B1 (en) | 2010-10-26 | 2019-12-17 | Michael Lamport Commons | Intelligent control with hierarchical stacked neural networks |
US20200039070A1 (en) * | 2018-07-31 | 2020-02-06 | At&T Intellectual Property I, L.P. | Providing logistical support for robots |
US20200058001A1 (en) * | 2017-05-15 | 2020-02-20 | Daikin Industries, Ltd. | Product information preparation system |
US10639797B1 (en) * | 2015-10-05 | 2020-05-05 | X Development Llc | Selectively uploading operational data generated by robot based on physical communication link attribute |
CN111251308A (en) * | 2020-05-07 | 2020-06-09 | 北京云迹科技有限公司 | Method, device and system for docking robot |
US20200225673A1 (en) * | 2016-02-29 | 2020-07-16 | AI Incorporated | Obstacle recognition method for autonomous robots |
US10723018B2 (en) | 2016-11-28 | 2020-07-28 | Brain Corporation | Systems and methods for remote operating and/or monitoring of a robot |
US10852730B2 (en) | 2017-02-08 | 2020-12-01 | Brain Corporation | Systems and methods for robotic mobile platforms |
US10877475B2 (en) * | 2015-05-22 | 2020-12-29 | Fujifilm Corporation | Robot device and method of controlling movement of robot device |
US10894318B2 (en) | 2016-02-23 | 2021-01-19 | Abb Schweiz Ag | Robot controller system and method therefor |
US10963597B2 (en) * | 2017-07-14 | 2021-03-30 | Beihang University | Method and apparatus for adaptively constructing three-dimensional indoor scenario |
US11040446B2 (en) * | 2018-03-14 | 2021-06-22 | Kabushiki Kaisha Toshiba | Transporter, transport system, and controller |
US20210197374A1 (en) * | 2019-12-30 | 2021-07-01 | X Development Llc | Composability framework for robotic control system |
US11093545B2 (en) | 2014-04-10 | 2021-08-17 | Sensormatic Electronics, LLC | Systems and methods for an automated cloud-based video surveillance system |
US11110603B2 (en) | 2018-10-02 | 2021-09-07 | Toyota Research Institute, Inc. | Systems and methods for naïve physics for contact and contact-awareness in robotic teleoperation |
US11120274B2 (en) | 2014-04-10 | 2021-09-14 | Sensormatic Electronics, LLC | Systems and methods for automated analytics for security surveillance in operation areas |
US11195024B1 (en) | 2020-07-10 | 2021-12-07 | International Business Machines Corporation | Context-aware action recognition by dual attention networks |
US11192242B2 (en) * | 2018-05-18 | 2021-12-07 | Toyota Jidosha Kabushiki Kaisha | Holding apparatus, container provided with tag, object holding program and object holding method |
US11253998B2 (en) * | 2017-10-04 | 2022-02-22 | Telefonaktiebolaget Lm Ericsson (Publ) | Method and arrangement for determination of a trajectory for a robot device in a cloud |
US11281873B2 (en) | 2015-05-28 | 2022-03-22 | Hds Mercury, Inc. | Product and equipment location and automation system and method |
CN114745217A (en) * | 2015-09-25 | 2022-07-12 | 英特尔公司 | Method and apparatus to facilitate end-user defined policy management |
US11389962B2 (en) * | 2010-05-24 | 2022-07-19 | Teladoc Health, Inc. | Telepresence robot system that can be accessed by a cellular phone |
US11443141B2 (en) | 2020-02-27 | 2022-09-13 | International Business Machines Corporation | Using video tracking technology to create machine learning datasets for tasks |
US11449061B2 (en) | 2016-02-29 | 2022-09-20 | AI Incorporated | Obstacle recognition method for autonomous robots |
CN115134348A (en) * | 2022-05-25 | 2022-09-30 | 阿里巴巴（中国）有限公司 | Sharing method of cloud application instance, system, equipment and storage medium thereof |
US11550321B1 (en) * | 2015-07-21 | 2023-01-10 | Hrl Laboratories, Llc | System and method for classifying agents based on agent movement patterns |
US11645444B2 (en) | 2016-05-10 | 2023-05-09 | Trustees Of Tufts College | Systems and methods enabling online one-shot learning and generalization by intelligent systems of task-relevant features and transfer to a cohort of intelligent systems |
US11693413B1 (en) | 2016-02-29 | 2023-07-04 | AI Incorporated | Obstacle recognition method for autonomous robots |
US11693403B2 (en) | 2019-06-04 | 2023-07-04 | Seegrid Corporation | Dynamic allocation and coordination of auto-navigating vehicles and selectors |
US11927965B2 (en) | 2016-02-29 | 2024-03-12 | AI Incorporated | Obstacle recognition method for autonomous robots |
Citations (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040019406A1 (en) | 2002-07-25 | 2004-01-29 | Yulun Wang | Medical tele-robotic system |
US20040045204A1 (en) | 2002-09-06 | 2004-03-11 | Rosemary Miano | Holographic product labeling method |
US20040068351A1 (en) * | 2002-04-22 | 2004-04-08 | Neal Solomon | System, methods and apparatus for integrating behavior-based approach into hybrid control model for use with mobile robotic vehicles |
US20040118916A1 (en) | 2002-12-18 | 2004-06-24 | Duanfeng He | System and method for verifying RFID reads |
JP2004326437A (en) | 2003-04-24 | 2004-11-18 | Matsushita Electric Ind Co Ltd | Article managing system and method |
JP2005088140A (en) | 2003-09-18 | 2005-04-07 | National Institute Of Advanced Industrial & Technology | Object processing system, object processing method and robot |
US7209803B2 (en) | 2003-02-17 | 2007-04-24 | Matsushita Electric Industrial Co., Ltd. | Article handling system and method and article management system and method |
US7269479B2 (en) | 2004-08-02 | 2007-09-11 | Matsushita Electric Industrial Co., Ltd. | Article transporting robot |
US7515990B2 (en) | 2003-05-21 | 2009-04-07 | Panasonic Corporation | Article control system, article control server, article control method |
US20090254217A1 (en) * | 2008-04-02 | 2009-10-08 | Irobot Corporation | Robotics Systems |
US7693757B2 (en) * | 2006-09-21 | 2010-04-06 | International Business Machines Corporation | System and method for performing inventory using a mobile inventory robot |
US7693753B1 (en) | 2002-01-31 | 2010-04-06 | Karen Purdy | System and method for presenting items and obtaining user selections of those items |
US20100241260A1 (en) * | 2009-03-17 | 2010-09-23 | Comau, Inc. | Industrial communication system and method |
US20110071676A1 (en) * | 2009-09-22 | 2011-03-24 | Gm Global Technology Operations, Inc. | Interactive robot control system and method of use |
US20110074923A1 (en) | 2009-09-25 | 2011-03-31 | Samsung Electronics Co., Ltd. | Image transmission system of network-based robot and method thereof |
US20110075915A1 (en) | 2009-09-25 | 2011-03-31 | Samsung Electronics Co., Ltd. | Image processing apparatus of robot system and method and computer-readable medium thereof |
US20110288684A1 (en) * | 2010-05-20 | 2011-11-24 | Irobot Corporation | Mobile Robot System |
US8095238B2 (en) * | 2006-11-29 | 2012-01-10 | Irobot Corporation | Robot development platform |
US8112176B2 (en) * | 2002-08-21 | 2012-02-07 | Neal Solomon | System for self-organizing mobile robotic collectives |
US8214079B2 (en) * | 2007-03-30 | 2012-07-03 | Sungkyunkwan University Foundation For Corporate Collaboration | Central information processing system and method for service robot having layered information structure according to recognition and reasoning level |
-
2012
- 2012-05-04 US US13/464,699 patent/US8639644B1/en active Active
Patent Citations (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7693753B1 (en) | 2002-01-31 | 2010-04-06 | Karen Purdy | System and method for presenting items and obtaining user selections of those items |
US20040068351A1 (en) * | 2002-04-22 | 2004-04-08 | Neal Solomon | System, methods and apparatus for integrating behavior-based approach into hybrid control model for use with mobile robotic vehicles |
US20040019406A1 (en) | 2002-07-25 | 2004-01-29 | Yulun Wang | Medical tele-robotic system |
US8112176B2 (en) * | 2002-08-21 | 2012-02-07 | Neal Solomon | System for self-organizing mobile robotic collectives |
US20040045204A1 (en) | 2002-09-06 | 2004-03-11 | Rosemary Miano | Holographic product labeling method |
US20040118916A1 (en) | 2002-12-18 | 2004-06-24 | Duanfeng He | System and method for verifying RFID reads |
US7209803B2 (en) | 2003-02-17 | 2007-04-24 | Matsushita Electric Industrial Co., Ltd. | Article handling system and method and article management system and method |
JP2004326437A (en) | 2003-04-24 | 2004-11-18 | Matsushita Electric Ind Co Ltd | Article managing system and method |
US7515990B2 (en) | 2003-05-21 | 2009-04-07 | Panasonic Corporation | Article control system, article control server, article control method |
JP2005088140A (en) | 2003-09-18 | 2005-04-07 | National Institute Of Advanced Industrial & Technology | Object processing system, object processing method and robot |
US7269479B2 (en) | 2004-08-02 | 2007-09-11 | Matsushita Electric Industrial Co., Ltd. | Article transporting robot |
US7693757B2 (en) * | 2006-09-21 | 2010-04-06 | International Business Machines Corporation | System and method for performing inventory using a mobile inventory robot |
US8095238B2 (en) * | 2006-11-29 | 2012-01-10 | Irobot Corporation | Robot development platform |
US8214079B2 (en) * | 2007-03-30 | 2012-07-03 | Sungkyunkwan University Foundation For Corporate Collaboration | Central information processing system and method for service robot having layered information structure according to recognition and reasoning level |
US20090254217A1 (en) * | 2008-04-02 | 2009-10-08 | Irobot Corporation | Robotics Systems |
US20100241260A1 (en) * | 2009-03-17 | 2010-09-23 | Comau, Inc. | Industrial communication system and method |
US20110071676A1 (en) * | 2009-09-22 | 2011-03-24 | Gm Global Technology Operations, Inc. | Interactive robot control system and method of use |
US20110074923A1 (en) | 2009-09-25 | 2011-03-31 | Samsung Electronics Co., Ltd. | Image transmission system of network-based robot and method thereof |
US20110075915A1 (en) | 2009-09-25 | 2011-03-31 | Samsung Electronics Co., Ltd. | Image processing apparatus of robot system and method and computer-readable medium thereof |
US20110288684A1 (en) * | 2010-05-20 | 2011-11-24 | Irobot Corporation | Mobile Robot System |
Non-Patent Citations (19)
Title |
---|
12.O. Zweigle, M.J.G. van de Molengraft, R. d'Andrea and K. Häussermann, RoboEarth: connecting robots worldwide, in ICIS '09: Proc. of the 2nd Int. Conf. on Interaction Sciences, 2009, pp. 184-191. * |
Angelis, G.; Van Pelt, C.; Van Den Molengraft, M.J.G. "Open innovation to further develop people-friendly robot technology: The RoboEarth Project." Gerontechnology 2010;9(2):76-77. * |
Arumugam, R.; Enti, V.R.; Liu Bingbing; Wu Xiaojun; Baskaran, K.; Foong Foo Kong; Kumar, A.S.; Kang Dee Meng; Goh Wai Kit; , "DAvinCi: A cloud computing framework for service robots," Robotics and Automation (ICRA), 2010 IEEE International Conference on , vol., No., pp. 3084-3089, May 3-7, 2010. * |
Bong Keun Kim; Miyazaki, M.; Ohba, K.; Hirai, S.; Tanie, K., "Web Services Based Robot Control Platform for Ubiquitous Functions," Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on , vol., No., pp. 691,696, Apr. 18-22, 2005. * |
Bong Keun Kim; Tomokuni, N.; Ohara, K.; Tanikawa, T.; Ohba, K.; Hirai, S., "Ubiquitous Localization and Mapping for Robots with Ambient Intelligence," Intelligent Robots and Systems, 2006 IEEE/RSJ International Conference on , vol., No., pp. 4809,4814, Oct. 9-15, 2006. * |
Burgard, W.; Moors, M.; Stachniss, C.; Schneider, F.E.; , "Coordinated multi-robot exploration," Robotics, IEEE Transactions on , vol. 21, No. 3, pp. 376-386, Jun. 2005. * |
Emanuele Menegatti, Matteo Danieletto, Marco Mina, Alberto Pretto, Andrea Bardella, Andrea Zanella, Pietro Zanuttigh. "Discovery, Localization and Recognition of Smart Objects by a Mobile Robot" Simulation, Modeling, and Programming for Autonomous Robots Lecture Notes in Computer Science, vol. 6472, 2010, pp. 436-448. * |
Guizzo, Erico. "Robots With Their Heads in the Clouds." IEEE Spectrum Mar. 2011 p. 16-18. * |
Hans-W. Gellersen, Michael Beigl, Holger Krull. "The MediaCup: Awareness Technology Embedded in an Everyday Object" Handheld and Ubiquitous Computing Lecture Notes in Computer Science, vol. 1707, 1999, pp. 308-310. * |
Hubel et al., "Learning and adaptation in Dynamic Systems: A Literature Survey", ICT Call 4 RoboEarth Project 2010-248942, http://www.roboearth.org, Apr. 1, 2010. * |
Jae-Han Park; Baeg, M.H.; Jaehan Koh; Kyung-Wook Park; Moon-Hong Baeg, "A new object recognition system for service robots in the smart environment," Control, Automation and Systems, 2007. ICCAS '07. International Conference on , vol., No., pp. 1083,1087, Oct. 17-20, 2007. * |
Luigi Atzori, Antonio Iera, Giacomo Morabito, The Internet of Things: A survey, Computer Networks, vol. 54, Issue 15, Oct. 28, 2010, pp. 2787-2805. * |
Luis Roalter, Matthias Kranz, Andreas Moller. "A Middleware for Intelligent Environments and the Internet of Things" Ubiquitous Intelligence and Computing Lecture Notes in Computer Science, vol. 6406, 2010, pp. 267-281. * |
Menegatti, E.; Danieletto, M.; Mina, M.; Pretto, A.; Bardella, A.; Zanconato, S.; Zanuttigh, P.; Zanella, A., "Autonomous discovery, localization and recognition of smart objects through WSN and image features," GLOBECOM Workshops (GC Wkshps), 2010 IEEE , vol., No., pp. 1653,1657, Dec. 6-10, 2010. * |
Narita, M.; Shimamura, M.; , "A report on RSi (robot services initiative) activities," Advanced Robotics and its Social Impacts, 2005. IEEE Workshop on , vol., No., pp. 265-268, Jun. 12-15, 2005. * |
Vasiliu, L.; Sakpota, B.; Hong-Gee Kim; , "A semantic Web services driven application on humanoid robots," Software Technologies for Future Embedded and Ubiquitous Systems, 2006 and the 2006 Second International Workshop on Collaborative Computing, Integration, and Assurance. SEUS 2006/WCCIA 2006. The Fourth IEEE Workshop on , vol., No., pp. 6 pp. * |
Waibel, Markus. "RoboEarth: A World Wide Web for Robots." IEEE Spectrum Feb. 5, 2011. * |
Yinong Chen; Zhihui Du; Garcia-Acosta, M.; , "Robot as a Service in Cloud Computing," Service Oriented System Engineering (SOSE), 2010 Fifth IEEE International Symposium on , vol., No., pp. 151-158, Jun. 4-5, 2010. * |
Zweigle, Oliver; Haussermann, Kai; Käppeler, Uwe-Philipp; Levi Paul: Extended TA Algorithm for adapting a Situation Ontology; Progress in Robotics, FIRA RoboWorld Congress 2009, Incheon, South Korea, p. 364-372. * |
Cited By (184)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11389962B2 (en) * | 2010-05-24 | 2022-07-19 | Teladoc Health, Inc. | Telepresence robot system that can be accessed by a cellular phone |
US8855812B2 (en) * | 2010-07-23 | 2014-10-07 | Chetan Kapoor | System and method for robot safety and collision avoidance |
US20120022689A1 (en) * | 2010-07-23 | 2012-01-26 | Agile Planet, Inc. | System and Method for Robot Safety and Collision Avoidance |
US11514305B1 (en) | 2010-10-26 | 2022-11-29 | Michael Lamport Commons | Intelligent control with hierarchical stacked neural networks |
US9875440B1 (en) | 2010-10-26 | 2018-01-23 | Michael Lamport Commons | Intelligent control with hierarchical stacked neural networks |
US10510000B1 (en) | 2010-10-26 | 2019-12-17 | Michael Lamport Commons | Intelligent control with hierarchical stacked neural networks |
US8984136B1 (en) * | 2011-05-06 | 2015-03-17 | Google Inc. | Systems and methods for object recognition |
US9566710B2 (en) | 2011-06-02 | 2017-02-14 | Brain Corporation | Apparatus and methods for operating robotic devices using selective state space training |
US9486922B2 (en) * | 2012-02-07 | 2016-11-08 | X Development Llc | Systems and methods for determining a status of a component of a device |
US9701015B2 (en) | 2012-06-21 | 2017-07-11 | Rethink Robotics, Inc. | Vision-guided robots and methods of training them |
US8958912B2 (en) * | 2012-06-21 | 2015-02-17 | Rethink Robotics, Inc. | Training and operating industrial robots |
US20130345873A1 (en) * | 2012-06-21 | 2013-12-26 | Rethink Robotics, Inc. | Training and operating industrial robots |
US8996174B2 (en) | 2012-06-21 | 2015-03-31 | Rethink Robotics, Inc. | User interfaces for robot training |
US8996175B2 (en) | 2012-06-21 | 2015-03-31 | Rethink Robotics, Inc. | Training and operating industrial robots |
US8965580B2 (en) | 2012-06-21 | 2015-02-24 | Rethink Robotics, Inc. | Training and operating industrial robots |
US9092698B2 (en) | 2012-06-21 | 2015-07-28 | Rethink Robotics, Inc. | Vision-guided robots and methods of training them |
US8965576B2 (en) | 2012-06-21 | 2015-02-24 | Rethink Robotics, Inc. | User interfaces for robot training |
US9669544B2 (en) | 2012-06-21 | 2017-06-06 | Rethink Robotics, Inc. | Vision-guided robots and methods of training them |
US9434072B2 (en) | 2012-06-21 | 2016-09-06 | Rethink Robotics, Inc. | Vision-guided robots and methods of training them |
US9753453B2 (en) * | 2012-07-09 | 2017-09-05 | Deep Learning Robotics Ltd. | Natural machine interface system |
US20140012415A1 (en) * | 2012-07-09 | 2014-01-09 | Technion Research & Development Foundation Limited | Natural machine interface system |
US10571896B2 (en) | 2012-07-09 | 2020-02-25 | Deep Learning Robotics Ltd. | Natural machine interface system |
US9137943B2 (en) * | 2012-07-27 | 2015-09-22 | Honda Research Institute Europe Gmbh | Trainable autonomous lawn mower |
US20140032033A1 (en) * | 2012-07-27 | 2014-01-30 | Honda Research Institute Europe Gmbh | Trainable autonomous lawn mower |
US20140180479A1 (en) * | 2012-12-20 | 2014-06-26 | Wal-Mart Stores, Inc. | Bagging With Robotic Arm |
US9233472B2 (en) | 2013-01-18 | 2016-01-12 | Irobot Corporation | Mobile robot providing environmental mapping for household environmental control |
US10391638B2 (en) | 2013-01-18 | 2019-08-27 | Irobot Corporation | Mobile robot providing environmental mapping for household environmental control |
US10488857B2 (en) | 2013-01-18 | 2019-11-26 | Irobot Corporation | Environmental management systems including mobile robots and methods using same |
US11648685B2 (en) | 2013-01-18 | 2023-05-16 | Irobot Corporation | Mobile robot providing environmental mapping for household environmental control |
US9874873B2 (en) | 2013-01-18 | 2018-01-23 | Irobot Corporation | Environmental management systems including mobile robots and methods using same |
US9802322B2 (en) | 2013-01-18 | 2017-10-31 | Irobot Corporation | Mobile robot providing environmental mapping for household environmental control |
US20140207281A1 (en) * | 2013-01-18 | 2014-07-24 | Irobot Corporation | Environmental Management Systems Including Mobile Robots and Methods Using Same |
US9375847B2 (en) * | 2013-01-18 | 2016-06-28 | Irobot Corporation | Environmental management systems including mobile robots and methods using same |
US9380922B2 (en) | 2013-01-18 | 2016-07-05 | Irobot Corporation | Environmental management systems including mobile robots and methods using same |
US9144905B1 (en) * | 2013-03-13 | 2015-09-29 | Hrl Laboratories, Llc | Device and method to identify functional parts of tools for robotic manipulation |
US10155310B2 (en) | 2013-03-15 | 2018-12-18 | Brain Corporation | Adaptive predictor apparatus and methods |
US9764468B2 (en) | 2013-03-15 | 2017-09-19 | Brain Corporation | Adaptive predictor apparatus and methods |
US9983592B2 (en) * | 2013-04-23 | 2018-05-29 | Samsung Electronics Co., Ltd. | Moving robot, user terminal apparatus and control method thereof |
US20140316636A1 (en) * | 2013-04-23 | 2014-10-23 | Samsung Electronics Co., Ltd. | Moving robot, user terminal apparatus and control method thereof |
US9242372B2 (en) | 2013-05-31 | 2016-01-26 | Brain Corporation | Adaptive robotic interface apparatus and methods |
US9821457B1 (en) | 2013-05-31 | 2017-11-21 | Brain Corporation | Adaptive robotic interface apparatus and methods |
US9314924B1 (en) | 2013-06-14 | 2016-04-19 | Brain Corporation | Predictive robotic controller apparatus and methods |
US9950426B2 (en) | 2013-06-14 | 2018-04-24 | Brain Corporation | Predictive robotic controller apparatus and methods |
US9792546B2 (en) | 2013-06-14 | 2017-10-17 | Brain Corporation | Hierarchical robotic controller apparatus and methods |
US9296101B2 (en) * | 2013-09-27 | 2016-03-29 | Brain Corporation | Robotic control arbitration apparatus and methods |
US20150094852A1 (en) * | 2013-09-27 | 2015-04-02 | Brain Corporation | Robotic control arbitration apparatus and methods |
US9579789B2 (en) | 2013-09-27 | 2017-02-28 | Brain Corporation | Apparatus and methods for training of robotic control arbitration |
US9844873B2 (en) | 2013-11-01 | 2017-12-19 | Brain Corporation | Apparatus and methods for haptic training of robots |
US9463571B2 (en) | 2013-11-01 | 2016-10-11 | Brian Corporation | Apparatus and methods for online training of robots |
US9597797B2 (en) | 2013-11-01 | 2017-03-21 | Brain Corporation | Apparatus and methods for haptic training of robots |
US9248569B2 (en) | 2013-11-22 | 2016-02-02 | Brain Corporation | Discrepancy detection apparatus and methods for machine learning |
US9358685B2 (en) | 2014-02-03 | 2016-06-07 | Brain Corporation | Apparatus and methods for control of robot actions based on corrective user inputs |
US10322507B2 (en) | 2014-02-03 | 2019-06-18 | Brain Corporation | Apparatus and methods for control of robot actions based on corrective user inputs |
US9789605B2 (en) | 2014-02-03 | 2017-10-17 | Brain Corporation | Apparatus and methods for control of robot actions based on corrective user inputs |
US9233469B2 (en) * | 2014-02-13 | 2016-01-12 | GM Global Technology Operations LLC | Robotic system with 3D box location functionality |
US20150224648A1 (en) * | 2014-02-13 | 2015-08-13 | GM Global Technology Operations LLC | Robotic system with 3d box location functionality |
US20150258684A1 (en) * | 2014-03-14 | 2015-09-17 | Seiko Epson Corporation | Robot, robot system, and control device |
US9216509B2 (en) | 2014-04-10 | 2015-12-22 | Smartvue Corporation | Systems and methods for automated cloud-based analytics for security and/or surveillance |
US11128838B2 (en) | 2014-04-10 | 2021-09-21 | Sensormatic Electronics, LLC | Systems and methods for automated cloud-based analytics for security and/or surveillance |
US9686514B2 (en) | 2014-04-10 | 2017-06-20 | Kip Smrt P1 Lp | Systems and methods for an automated cloud-based video surveillance system |
US10084995B2 (en) * | 2014-04-10 | 2018-09-25 | Sensormatic Electronics, LLC | Systems and methods for an automated cloud-based video surveillance system |
US10594985B2 (en) | 2014-04-10 | 2020-03-17 | Sensormatic Electronics, LLC | Systems and methods for automated cloud-based analytics for security and/or surveillance |
US11093545B2 (en) | 2014-04-10 | 2021-08-17 | Sensormatic Electronics, LLC | Systems and methods for an automated cloud-based video surveillance system |
US11120274B2 (en) | 2014-04-10 | 2021-09-14 | Sensormatic Electronics, LLC | Systems and methods for automated analytics for security surveillance in operation areas |
US9438865B2 (en) | 2014-04-10 | 2016-09-06 | Smartvue Corporation | Systems and methods for automated cloud-based analytics for security surveillance systems with mobile input capture devices |
US10057546B2 (en) | 2014-04-10 | 2018-08-21 | Sensormatic Electronics, LLC | Systems and methods for automated cloud-based analytics for security and/or surveillance |
US9405979B2 (en) | 2014-04-10 | 2016-08-02 | Smartvue Corporation | Systems and methods for automated cloud-based analytics and 3-dimensional (3D) display for surveillance systems |
US9426428B2 (en) | 2014-04-10 | 2016-08-23 | Smartvue Corporation | Systems and methods for automated cloud-based analytics and 3-dimensional (3D) display for surveillance systems in retail stores |
US9420238B2 (en) | 2014-04-10 | 2016-08-16 | Smartvue Corporation | Systems and methods for automated cloud-based 3-dimensional (3D) analytics for surveillance systems |
US9407880B2 (en) | 2014-04-10 | 2016-08-02 | Smartvue Corporation | Systems and methods for automated 3-dimensional (3D) cloud-based analytics for security surveillance in operation areas |
US20150381949A1 (en) * | 2014-04-10 | 2015-12-31 | Smartvue Corporation | Systems and Methods for an Automated Cloud-Based Video Surveillance System |
US9403277B2 (en) | 2014-04-10 | 2016-08-02 | Smartvue Corporation | Systems and methods for automated cloud-based analytics for security and/or surveillance |
US9407879B2 (en) | 2014-04-10 | 2016-08-02 | Smartvue Corporation | Systems and methods for automated cloud-based analytics and 3-dimensional (3D) playback for surveillance systems |
US10217003B2 (en) | 2014-04-10 | 2019-02-26 | Sensormatic Electronics, LLC | Systems and methods for automated analytics for security surveillance in operation areas |
US9346167B2 (en) | 2014-04-29 | 2016-05-24 | Brain Corporation | Trainable convolutional network apparatus and methods for operating a robotic vehicle |
CN106796665B (en) * | 2014-07-24 | 2019-06-14 | X开发有限责任公司 | Method and system for generating instructions for a robotic system to perform a task |
KR20190014586A (en) * | 2014-07-24 | 2019-02-12 | 엑스 디벨롭먼트 엘엘씨 | Methods and systems for generating instructions for a robotic system to carry out a task |
US9802309B2 (en) | 2014-07-24 | 2017-10-31 | X Development Llc | Methods and systems for generating instructions for a robotic system to carry out a task |
WO2016014774A1 (en) * | 2014-07-24 | 2016-01-28 | Google Inc. | Methods and systems for generating instructions for a robotic system to carry out a task |
CN106796665A (en) * | 2014-07-24 | 2017-05-31 | X开发有限责任公司 | Method and system for generating instructions for a robotic system to perform a task |
US10507577B2 (en) | 2014-07-24 | 2019-12-17 | X Development Llc | Methods and systems for generating instructions for a robotic system to carry out a task |
US11417210B1 (en) | 2014-09-16 | 2022-08-16 | Knightscope, Inc. | Autonomous parking monitor |
US10311731B1 (en) | 2014-09-16 | 2019-06-04 | Knightscope, Inc. | Autonomous parking monitor |
US9773413B1 (en) * | 2014-09-16 | 2017-09-26 | Knighscope, Inc. | Autonomous parking monitor |
US10105841B1 (en) | 2014-10-02 | 2018-10-23 | Brain Corporation | Apparatus and methods for programming and training of robotic devices |
US9687984B2 (en) | 2014-10-02 | 2017-06-27 | Brain Corporation | Apparatus and methods for training of robots |
US9902062B2 (en) | 2014-10-02 | 2018-02-27 | Brain Corporation | Apparatus and methods for training path navigation by robots |
US9604359B1 (en) | 2014-10-02 | 2017-03-28 | Brain Corporation | Apparatus and methods for training path navigation by robots |
US9630318B2 (en) | 2014-10-02 | 2017-04-25 | Brain Corporation | Feature detection apparatus and methods for training of robotic navigation |
US10131052B1 (en) | 2014-10-02 | 2018-11-20 | Brain Corporation | Persistent predictor apparatus and methods for task switching |
WO2016055159A3 (en) * | 2014-10-11 | 2016-07-14 | Audi Ag | Method for operating an automatically driven, driverless motor vehicle and monitoring system |
CN106794874A (en) * | 2014-10-11 | 2017-05-31 | 奥迪股份公司 | Method and monitoring system for running the unpiloted motor vehicle of automatic guiding |
US10338598B2 (en) | 2014-10-11 | 2019-07-02 | Audi Ag | Method for operating an automatically driven, driverless motor vehicle and monitoring system |
CN106794874B (en) * | 2014-10-11 | 2021-05-18 | 奥迪股份公司 | Method and monitoring system for operating an automatically guided unmanned motor vehicle |
WO2016073103A1 (en) * | 2014-11-03 | 2016-05-12 | Qualcomm Incorporated | Communicating configurable instruction sets to robots for controlling robot behavior |
US20160129592A1 (en) * | 2014-11-11 | 2016-05-12 | Google Inc. | Dynamically Maintaining A Map Of A Fleet Of Robotic Devices In An Environment To Facilitate Robotic Action |
US10022867B2 (en) * | 2014-11-11 | 2018-07-17 | X Development Llc | Dynamically maintaining a map of a fleet of robotic devices in an environment to facilitate robotic action |
US10296995B2 (en) * | 2014-11-11 | 2019-05-21 | X Development Llc | Dynamically maintaining a map of a fleet of robotic devices in an environment to facilitate robotic action |
KR20160087168A (en) | 2015-01-13 | 2016-07-21 | 서울시립대학교 산학협력단 | System and method for control robot based cloud knowledge sharing |
US20160246297A1 (en) * | 2015-02-24 | 2016-08-25 | Siemens Corporation | Cloud-based control system for unmanned aerial vehicles |
US9717387B1 (en) | 2015-02-26 | 2017-08-01 | Brain Corporation | Apparatus and methods for programming and training of robotic household appliances |
US10376117B2 (en) | 2015-02-26 | 2019-08-13 | Brain Corporation | Apparatus and methods for programming and training of robotic household appliances |
US20160327925A1 (en) * | 2015-05-08 | 2016-11-10 | Ciambella Ltd. | Method and apparatus for modifying behavior of code for a controller-based device |
US10067490B2 (en) * | 2015-05-08 | 2018-09-04 | Ciambella Ltd. | Method and apparatus for modifying behavior of code for a controller-based device |
US10877475B2 (en) * | 2015-05-22 | 2020-12-29 | Fujifilm Corporation | Robot device and method of controlling movement of robot device |
US11281873B2 (en) | 2015-05-28 | 2022-03-22 | Hds Mercury, Inc. | Product and equipment location and automation system and method |
US10410176B2 (en) | 2015-05-28 | 2019-09-10 | Hds Mercury, Inc. | Product and equipment location and automation system and method |
US11308689B2 (en) * | 2015-07-21 | 2022-04-19 | IAM Robotics, LLC | Three dimensional scanning and data extraction systems and processes for supply chain piece automation |
US20190244421A1 (en) * | 2015-07-21 | 2019-08-08 | IAM Robotics, LLC | Three dimensional scanning and data extraction systems and processes for supply chain piece automation |
US11550321B1 (en) * | 2015-07-21 | 2023-01-10 | Hrl Laboratories, Llc | System and method for classifying agents based on agent movement patterns |
US9616568B1 (en) | 2015-08-25 | 2017-04-11 | X Development Llc | Generating a grasp affordance for an object based on a thermal image of the object that is captured following human manipulation of the object |
US10350757B2 (en) | 2015-08-31 | 2019-07-16 | Avaya Inc. | Service robot assessment and operation |
US11120410B2 (en) | 2015-08-31 | 2021-09-14 | Avaya Inc. | Communication systems for multi-source robot control |
US10032137B2 (en) | 2015-08-31 | 2018-07-24 | Avaya Inc. | Communication systems for multi-source robot control |
US10124491B2 (en) | 2015-08-31 | 2018-11-13 | Avaya Inc. | Operational parameters |
US10040201B2 (en) * | 2015-08-31 | 2018-08-07 | Avaya Inc. | Service robot communication systems and system self-configuration |
US10169058B2 (en) * | 2015-09-24 | 2019-01-01 | Voodoo Robotics, Inc. | Scripting language for robotic storage and retrieval design for warehouses |
US20220329632A1 (en) * | 2015-09-25 | 2022-10-13 | Intel Corporation | Methods and apparatus to facilitate end-user defined policy management |
US11888903B2 (en) * | 2015-09-25 | 2024-01-30 | Intel Corporation | Methods and apparatus to facilitate end-user defined policy management |
CN114745217B (en) * | 2015-09-25 | 2024-01-02 | 英特尔公司 | Method and apparatus for facilitating end user defined policy management |
CN114745217A (en) * | 2015-09-25 | 2022-07-12 | 英特尔公司 | Method and apparatus to facilitate end-user defined policy management |
US10639797B1 (en) * | 2015-10-05 | 2020-05-05 | X Development Llc | Selectively uploading operational data generated by robot based on physical communication link attribute |
US9921583B2 (en) | 2016-02-18 | 2018-03-20 | Elwha Llc | Package management system for robotic vehicles |
US9798321B2 (en) | 2016-02-18 | 2017-10-24 | Elwha Llc | Package management system for robotic vehicles |
US9804602B2 (en) * | 2016-02-18 | 2017-10-31 | Elwha Llc | Package management system for robotic vehicles |
US10894318B2 (en) | 2016-02-23 | 2021-01-19 | Abb Schweiz Ag | Robot controller system and method therefor |
US11467587B2 (en) * | 2016-02-29 | 2022-10-11 | AI Incorporated | Obstacle recognition method for autonomous robots |
US11449061B2 (en) | 2016-02-29 | 2022-09-20 | AI Incorporated | Obstacle recognition method for autonomous robots |
US10788836B2 (en) * | 2016-02-29 | 2020-09-29 | AI Incorporated | Obstacle recognition method for autonomous robots |
US11927965B2 (en) | 2016-02-29 | 2024-03-12 | AI Incorporated | Obstacle recognition method for autonomous robots |
US11693413B1 (en) | 2016-02-29 | 2023-07-04 | AI Incorporated | Obstacle recognition method for autonomous robots |
US20200225673A1 (en) * | 2016-02-29 | 2020-07-16 | AI Incorporated | Obstacle recognition method for autonomous robots |
US11645444B2 (en) | 2016-05-10 | 2023-05-09 | Trustees Of Tufts College | Systems and methods enabling online one-shot learning and generalization by intelligent systems of task-relevant features and transfer to a cohort of intelligent systems |
US10241514B2 (en) | 2016-05-11 | 2019-03-26 | Brain Corporation | Systems and methods for initializing a robot to autonomously travel a trained route |
US11235464B1 (en) | 2016-06-08 | 2022-02-01 | X Development Llc | Robot task optimization based on historical task and location correlated durations |
US10173320B1 (en) | 2016-06-08 | 2019-01-08 | X Development Llc | Robot task optimization based on historical task and location correlated durations |
US11878425B1 (en) | 2016-06-08 | 2024-01-23 | Google Llc | Robot task optimization based on historical task and location correlated durations |
US9987752B2 (en) | 2016-06-10 | 2018-06-05 | Brain Corporation | Systems and methods for automatic detection of spills |
US10282849B2 (en) | 2016-06-17 | 2019-05-07 | Brain Corporation | Systems and methods for predictive/reconstructive visual object tracker |
US10016896B2 (en) | 2016-06-30 | 2018-07-10 | Brain Corporation | Systems and methods for robotic behavior around moving bodies |
US10265871B2 (en) | 2016-07-28 | 2019-04-23 | X Development Llc | Collaborative inventory monitoring |
WO2018022264A1 (en) * | 2016-07-28 | 2018-02-01 | X Development Llc | Collaborative inventory monitoring |
US10099391B2 (en) | 2016-07-28 | 2018-10-16 | X Development Llc | Collaborative inventory monitoring |
US9827683B1 (en) | 2016-07-28 | 2017-11-28 | X Development Llc | Collaborative inventory monitoring |
US10259117B2 (en) * | 2016-08-02 | 2019-04-16 | At&T Intellectual Property I, L.P. | On-demand robot virtualization |
US11103995B2 (en) | 2016-08-02 | 2021-08-31 | At&T Intellectual Property I, L.P. | On-demand robot virtualization |
CN106577345B (en) * | 2016-10-27 | 2023-08-29 | 重庆掌中花园科技有限公司 | Intelligent pet interaction system |
CN106577345A (en) * | 2016-10-27 | 2017-04-26 | 重庆掌中花园科技有限公司 | Intelligent pet interactive system |
US10274325B2 (en) | 2016-11-01 | 2019-04-30 | Brain Corporation | Systems and methods for robotic mapping |
US10001780B2 (en) | 2016-11-02 | 2018-06-19 | Brain Corporation | Systems and methods for dynamic route planning in autonomous navigation |
US10723018B2 (en) | 2016-11-28 | 2020-07-28 | Brain Corporation | Systems and methods for remote operating and/or monitoring of a robot |
US10661438B2 (en) * | 2017-01-16 | 2020-05-26 | Ants Technology (Hk) Limited | Robot apparatus, methods and computer products |
US20180200884A1 (en) * | 2017-01-16 | 2018-07-19 | Ants Technology (Hk) Limited | Robot apparatus, methods and computer products |
US10377040B2 (en) | 2017-02-02 | 2019-08-13 | Brain Corporation | Systems and methods for assisting a robotic apparatus |
US10852730B2 (en) | 2017-02-08 | 2020-12-01 | Brain Corporation | Systems and methods for robotic mobile platforms |
WO2018160267A3 (en) * | 2017-02-16 | 2018-12-13 | Indiana University Research And Technology Corporation | Cloud based robotic control systems and methods |
US11460861B2 (en) | 2017-02-16 | 2022-10-04 | Indiana University Research And Technology Corporation | Cloud based robotic control systems and methods |
US10409562B2 (en) | 2017-03-14 | 2019-09-10 | Ciambella Ltd. | Method and apparatus for automatically generating and incorporating code in development environments |
US10293485B2 (en) | 2017-03-30 | 2019-05-21 | Brain Corporation | Systems and methods for robotic path planning |
US10860853B2 (en) * | 2017-04-28 | 2020-12-08 | Intel Corporation | Learning though projection method and apparatus |
US20180314887A1 (en) * | 2017-04-28 | 2018-11-01 | Intel Corporation | Learning though projection method and apparatus |
US20200058001A1 (en) * | 2017-05-15 | 2020-02-20 | Daikin Industries, Ltd. | Product information preparation system |
CN109144046A (en) * | 2017-06-09 | 2019-01-04 | 本田技研工业株式会社 | Service provider system, database and service providing apparatus |
US10759049B2 (en) * | 2017-06-09 | 2020-09-01 | Honda Motor Co., Ltd. | Service providing system, database, and service providing device |
US20180354129A1 (en) * | 2017-06-09 | 2018-12-13 | Honda Motor Co., Ltd. | Service providing system, database, and service providing device |
US10963597B2 (en) * | 2017-07-14 | 2021-03-30 | Beihang University | Method and apparatus for adaptively constructing three-dimensional indoor scenario |
US11253998B2 (en) * | 2017-10-04 | 2022-02-22 | Telefonaktiebolaget Lm Ericsson (Publ) | Method and arrangement for determination of a trajectory for a robot device in a cloud |
US11040446B2 (en) * | 2018-03-14 | 2021-06-22 | Kabushiki Kaisha Toshiba | Transporter, transport system, and controller |
WO2019217885A1 (en) * | 2018-05-11 | 2019-11-14 | Brown University | Augmented reality interface to robots |
US11090813B2 (en) | 2018-05-11 | 2021-08-17 | Brown University | Augmented reality interface to robots |
US11192242B2 (en) * | 2018-05-18 | 2021-12-07 | Toyota Jidosha Kabushiki Kaisha | Holding apparatus, container provided with tag, object holding program and object holding method |
US11890757B2 (en) * | 2018-07-31 | 2024-02-06 | Hyundai Motor Company | Providing logistical support for robots |
US20200039070A1 (en) * | 2018-07-31 | 2020-02-06 | At&T Intellectual Property I, L.P. | Providing logistical support for robots |
US10953541B2 (en) * | 2018-07-31 | 2021-03-23 | At&T Intellectual Property I, L.P. | Providing logistical support for robots |
US20210205991A1 (en) * | 2018-07-31 | 2021-07-08 | At&T Intellectual Property I, L.P. | Providing logistical support for robots |
US11110603B2 (en) | 2018-10-02 | 2021-09-07 | Toyota Research Institute, Inc. | Systems and methods for naïve physics for contact and contact-awareness in robotic teleoperation |
US11693403B2 (en) | 2019-06-04 | 2023-07-04 | Seegrid Corporation | Dynamic allocation and coordination of auto-navigating vehicles and selectors |
US11498211B2 (en) * | 2019-12-30 | 2022-11-15 | Intrinsic Innovation Llc | Composability framework for robotic control system |
US20210197374A1 (en) * | 2019-12-30 | 2021-07-01 | X Development Llc | Composability framework for robotic control system |
US11443141B2 (en) | 2020-02-27 | 2022-09-13 | International Business Machines Corporation | Using video tracking technology to create machine learning datasets for tasks |
CN111251308A (en) * | 2020-05-07 | 2020-06-09 | 北京云迹科技有限公司 | Method, device and system for docking robot |
US11195024B1 (en) | 2020-07-10 | 2021-12-07 | International Business Machines Corporation | Context-aware action recognition by dual attention networks |
CN115134348A (en) * | 2022-05-25 | 2022-09-30 | 阿里巴巴（中国）有限公司 | Sharing method of cloud application instance, system, equipment and storage medium thereof |
CN115134348B (en) * | 2022-05-25 | 2024-05-03 | 阿里巴巴（中国）有限公司 | Sharing method of cloud application instance, system, equipment and storage medium thereof |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8639644B1 (en) | Shared robot knowledge base for use with cloud computing system | |
US8447863B1 (en) | Systems and methods for object recognition | |
US8386078B1 (en) | Methods and systems for providing a data library for robotic devices | |
US9895802B1 (en) | Projection of interactive map data | |
US20210196081A1 (en) | Systems and methods for automated cooking | |
US10131051B1 (en) | Anticipation-based robotic object grasping | |
US9205886B1 (en) | Systems and methods for inventorying objects | |
US9399294B1 (en) | Displaying estimated image data in a user interface | |
US8428777B1 (en) | Methods and systems for distributing tasks among robotic devices | |
US8380652B1 (en) | Methods and systems for autonomous robotic decision making | |
US20180053140A1 (en) | Food storage method and apparatus with sensors | |
US20170348854A1 (en) | Robotic manipulation methods and systems for executing a domain-specific application in an instrumented environment with containers and electronic minimanipulation libraries | |
US9026248B1 (en) | Methods and systems for multirobotic management | |
US8380349B1 (en) | Methods and systems for providing instructions to a robotic device | |
US8886829B1 (en) | Methods and systems for robot cloud computing using slug trails | |
US9740895B1 (en) | Method and system for identifying and tracking tagged, physical objects | |
US8525836B1 (en) | Systems and methods for representing information associated with objects in an area | |
US20170018042A1 (en) | Method and system for enhanced smart automation management facilitating social cookery | |
WO2004106009A1 (en) | Article operating system and method, and article managing system and method | |
WO2016167918A1 (en) | Dynamic nutrition tracking utensils | |
JP2008254167A (en) | Central information processing system and method of mobile service robot having indoor information of hierarchical structure according to level of recognition/reasoning | |
EP3494513A1 (en) | Selectively downloading targeted object recognition modules | |
JP2007111854A (en) | Article handling system and article handling server | |
CN107302589A (en) | Intelligent refrigerator storage system | |
JP2019138493A (en) | Refrigerator, commodity order placement method and commodity order placement system |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HICKMAN, RYAN;KUFFNER, JR., JAMES J.;BRUCE, JAMES R.;AND OTHERS;SIGNING DATES FROM 20120503 TO 20120504;REEL/FRAME:031100/0018 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: X DEVELOPMENT LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GOOGLE INC.;REEL/FRAME:039900/0610Effective date: 20160901 |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:X DEVELOPMENT LLC;REEL/FRAME:064658/0001Effective date: 20230401 |