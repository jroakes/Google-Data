US8923626B1 - Image retrieval - Google Patents
Image retrieval Download PDFInfo
- Publication number
- US8923626B1 US8923626B1 US13/531,912 US201213531912A US8923626B1 US 8923626 B1 US8923626 B1 US 8923626B1 US 201213531912 A US201213531912 A US 201213531912A US 8923626 B1 US8923626 B1 US 8923626B1
- Authority
- US
- United States
- Prior art keywords
- collection
- images
- visual words
- image
- duplicate images
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/46—Descriptors for shape, contour or point-related descriptors, e.g. scale invariant feature transform [SIFT] or bags of words [BoW]; Salient regional features
- G06V10/462—Salient features, e.g. scale invariant feature transforms [SIFT]
- G06V10/464—Salient features, e.g. scale invariant feature transforms [SIFT] using a plurality of salient features, e.g. bag-of-words [BoW] representations
Definitions
- This specification relates to information retrieval.
- Conventional information retrieval systems are used to identify a wide variety of resources, for example, images, audio files, web pages, or documents, e.g., news articles. Additionally, search results presented to a user that identify particular resources responsive to a query are typically ranked according to particular criteria.
- Image search systems can assign visual words to images; in this context, images may be said to have visual words, and one may refer to the visual words of an image.
- Image search systems can identify features of images and compute a feature vector for each of one or more regions of an image.
- Image search systems can quantize each computed feature vector into one or more corresponding visual words.
- An image search system can improve image retrieval by clustering near-duplicate images into collections of near-duplicate images and indexing images by collections of near-duplicate images, rather than indexing the images individually.
- a collection of near-duplicate images can be indexed by visual words in a union of the visual words of all images in the collection.
- An image search system can then identify collections having a particular number of visual words in common with a query image as collections that match the query image.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of aligning each image in a collection of near-duplicate images to a common coordinate frame; determining one or more visual words in the common coordinate frame for each of the images in the collection of near-duplicate images; determining a union of visual words, wherein the union of visual words comprises the one or more visual words determined in the common coordinate frame; and adding a posting list item that identifies the collection of near-duplicate images to one or more corresponding posting lists, wherein each posting list corresponds to one of the determined visual words, and wherein each posting list identifies one or more collections of near-duplicate images whose union of determined visual words have the particular visual word.
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- the foregoing and other embodiments can each optionally include one or more of the following features, alone or in combination.
- the actions include receiving a query image; determining one or more visual words for the query image; determining that the collection of near-duplicate images has at least a first threshold number of visual words in common with the query image; and computing a score for one or more images in the collection of near-duplicate images.
- Computing a score for one or more images in the collection of images comprises computing a similarity score using visual words of the query image and matching visual words of the union of determined visual words; and assigning the similarity score to each image in the collection of near-duplicate images.
- the first threshold number of visual words increases as the size of a candidate collection of near-duplicate images increases.
- the actions include determining that a second collection of near-duplicate images has at least a second different threshold number of visual words in common with the query image; and computing a score for one or more images in the second collection of near-duplicate images.
- Computing a score comprises computing a first score for each matching visual word between the query image and the collection of near-duplicate images based on a geometric mapping between visual words of the query image and visual words of the collection of near-duplicate images.
- Aligning each image in a collection of near-duplicate images to a common coordinate frame comprises selecting a reference image in the collection of near-duplicate images as the basis for the common coordinate frame; and determining an affine transformation between the reference image and each image of the collection of near-duplicate images.
- Determining one or more visual words in the common coordinate frame for each of the images in the collection of near-duplicate images comprises applying a corresponding affine transformation to each image in the collection of near-duplicate images; and determining visual words of each image according to coordinates in the common coordinate frame.
- the actions include clustering a plurality of images into one or more collections of near-duplicate images, wherein each cluster is no greater than a maximum size.
- Indexing collections of near-duplicate images rather than individual images improves image retrieval performance without increasing retrieval time. Indexing by collections of near-duplicate images also takes advantage of the combined visual words of near-duplicate images, reducing the chance that a near-duplicate of a query image is not identified or that two near-duplicate images are treated inconsistently. Indexing by collections of near-duplicate images can also improve retrieval time by making posting lists shorter.
- FIG. 1 illustrates the input and output of an example image search system.
- FIG. 2 illustrates an example image search system.
- FIG. 3A is a diagram of determining a union of visual words for a collection of near-duplicate images.
- FIG. 3B is a diagram of multiple example posting lists.
- FIG. 4 is a flow chart of an example process for indexing collections of near-duplicate images.
- FIG. 1 illustrates the input and output of an example image search system 110 .
- the search system 110 takes as input a query image 120 and provides as output one or more image search results, e.g., results that each identify a corresponding result image 130 , 140 , 150 in response to the query image.
- image search results e.g., results that each identify a corresponding result image 130 , 140 , 150 in response to the query image.
- multiple search results will be presented together in a user interface presentation, e.g., a web page, each search result will include a thumbnail of the corresponding result image that the search result identifies as well as a link, e.g., a hyperlink, to the result image.
- the image search system 110 can order the image search results for presentation or other purposes by a measure of visual similarity to the query image 120 .
- the image search system 110 can identify images that are visually similar to the query image 120 .
- the system orders the images 130 , 140 , and 150 by visual similarity to the query image 120 .
- Image 130 is a resized version of image 120 and is most visually similar to the query image 120 of the three provided images.
- Image 140 is an image of the same bridge depicted in image 130 , but from a perspective different from that of the query image 120 . Therefore, the image search system 110 determines that image 140 is less similar to query image 120 than image 130 .
- the image search system 110 similarly determines that image 140 is more similar to the query image 120 than image 150 .
- the image search system 110 can compute a measure of similarity between a query image and other images by using data that characterizes feature regions identified in the images.
- the search system 110 identifies elliptical regions in each image as feature regions. For example, the system 110 can identify elliptical regions 122 , 124 , and 126 in the query image 120 as feature regions. Similarly, the system 110 can identify as feature regions elliptical regions 132 , 134 , and 136 of image 130 ; 142 , 144 , and 146 of image 140 ; and 152 , 154 , and 156 of image 150 .
- the system 110 can compute a feature vector from each feature region.
- a feature vector can be a vector where each element of the vector is a quantity that represents a feature value of a feature of the corresponding feature region.
- the system 110 can compute a similarity score between two images by computing a similarity measure between feature vectors computed from each image. The system 110 can then determine that images having feature vectors that are more similar according to the computed similarity measure are more visually similar than images having feature vectors that are less similar according to the computed similarity measure.
- the search system 110 can quantize the feature space of feature vectors into a finite number of cells, which can be referred to as “visual words.” Then, for a given feature vector, the system 110 can determine to which of one or more visual words the feature vector should be assigned. The system 110 can determine to which of the cells of the feature space a particular feature vector belongs using an appropriate distance metric.
- the system can then quantize each feature vector for a particular image into a corresponding visual word and assign the visual words to the particular image.
- the system can also associate with each visual word geometry information of the corresponding feature region.
- the geometry information can include a position in the image, e.g., pixel coordinates, of the corresponding feature region and a scale value indicating a size of the feature region.
- the system 110 can make a preliminary determination of image similarity between two images by computing how many visual words the two images have in common.
- the system 110 can thus save computation resources by computing a similarity score between two images only if the two images have a threshold number of visual words in common.
- the system 110 can determine that a feature vector computed from region 122 is assigned to visual word A, that the feature vector computed from region 124 is assigned to visual word B, and that the feature vector computed from region 126 is assigned to visual word C.
- the system 110 can similarly determine that feature vectors from feature regions 132 , 134 , and 136 of image 130 are also assigned to visual words A, B, and C, and can therefore determine that query image 120 and image 130 have three visual words in common.
- the system 110 can determine that feature vectors computed from image 140 are assigned to visual words A, B, and D and determine that query image 120 and image 140 have two visual words in common.
- the system 110 can determine that feature vectors computed from image 150 are assigned to visual words A, E, and F and determine that query image 120 and image 150 have only one visual word in common.
- the system 110 can index images in multiple posting lists. For each visual word in the feature space, the system can maintain a posting list of images that have been assigned the visual word at least once. The system can then scan posting lists for the visual words of a received query image in order to identify indexed images that have at least a threshold number of visual words in common with the query image.
- FIG. 2 illustrates an example image search system 230 .
- the image search system 230 is an example of an information retrieval system in which the systems, components, and techniques described below can be implemented.
- a user device 210 can be coupled to the image search system 230 through a data communication network 220 .
- the user device 210 transmits an image query 214 over the network 220 to the image search system 230 .
- the image query 214 specifies a particular image, for example, by an image file or a resource locator, e.g., a uniform resource locators (URL), provided by the user device 210 .
- the image search system 230 identifies images that satisfy the image query 214 and generates image search results 216 .
- the image search system 230 transmits the image search results 216 over the network 220 back to the user device 210 for presentation to a user.
- the user is a person; but in certain cases, the user can be a software agent.
- the user device 210 can be any appropriate type of computing device, e.g., mobile phone, tablet computer, notebook computer, music player, e-book reader, laptop or desktop computer, PDA (personal digital assistant), smart phone, a server, or other stationary or portable device, that includes one or more processors, e.g., processor 208 , for executing program instructions, and random access memory, e.g., RAM 206 .
- the user device 210 can include computer readable media that store software applications, e.g., a browser or layout engine, an input device, e.g., a keyboard or mouse, a communication interface, and a display device.
- the network 220 can be, for example, a wireless cellular network, a wireless local area network (WLAN) or Wi-Fi network, a Third Generation (3G) or Fourth Generation (4G) mobile telecommunications network, a wired Ethernet network, a private network such as an intranet, a public network such as the Internet, or any appropriate combination of such networks.
- WLAN wireless local area network
- Wi-Fi Wireless Fidelity
- 4G Fourth Generation
- the image search system 230 can be implemented as computer programs installed on one or more computers in one or more locations that are coupled to each other through a network, e.g., network 220 .
- the image search system 230 includes a search engine 240 , an image collection 250 and an index database 260 .
- the index database 260 contains one or more indices of images in the image collection 250 .
- the “indexed images” are images in the image collection 250 that are indexed by any of the indices in the image database 250 .
- a search engine 240 identifies resources that satisfy the query 214 .
- the image search system 230 identifies images in the image collection 250 that have a highest similarity score for an image specified by the image query 214 .
- the search engine 240 will generally include an indexing engine 242 that indexes images.
- the indexing engine maintains multiple posting lists in the index database 260 .
- Each posting list is a list of images in the image collection 250 that have a same particular visual word.
- the search engine 240 can identify resources that satisfy the image query 214 .
- the search engine 240 includes a ranking engine 244 that can rank identified resources.
- the ranking engine 244 can identify indexed images that have at least a threshold number of visual words in common with the image specified by image query 214 .
- the ranking engine 214 can then rank the identified images, e.g., by a computed similarity score.
- the image search system 230 can respond to the image query 214 by generating image search results 216 , which the system can transmit over the network 220 to the user device 210 in a form that can be presented on the user device 210 , e.g., in a form that can be displayed in a web browser on the user device 210 .
- the image search results 216 can be presented in a markup language document, e.g., a HyperText Markup Language or eXtensible Markup Language document.
- the user device 210 renders the received form of the image search results 216 , e.g., by rendering a markup language document using a web browser, in order to present the image search results 216 on a display device coupled to the user device 210 .
- Multiple image search results 216 are generally presented at the same time; although on a small display device the results may be presented one at a time.
- Each of the presented image search results can include titles, text snippets, images, links, or other information.
- Each image search result is linked to a particular resource, e.g., Internet addressable document. Selection of an image search result can cause a display program running on the user device 210 to request the resource associated with the image search result.
- FIG. 3A is a diagram of determining a union of visual words for a collection of near-duplicate images.
- an image search system can improve image retrieval by computing a union of visual words from each image in a collection of near-duplicate images and indexing the collection of near-duplicate images by the union of visual words.
- a union of visual words from each image in a collection of near-duplicate images includes all distinct visual words from images in the collection.
- the images 302 , 304 , and 306 are considered near-duplicate images.
- Images in a collection of images can be different versions of the same image. In other words, the images are different versions of the same original pixels.
- An image can be a resized or rescaled version of another image.
- image 306 is a smaller version of image 302 .
- An image can also be a compressed version of another image, e.g., a compressed JPEG image computed from an uncompressed bitmap image.
- An image may be a cropped version of another image.
- image 304 is a cropped version of image 302 .
- An image may also have minor text or other minor alterations added.
- images in a collection of images can be images of the same object or scene, e.g., different images of the Golden Gate Bridge or the Eiffel Tower that, while captured at different times by different cameras, a search system considers to be near-duplicates.
- An image search system can organize such near-duplicate images into multiple collections of near duplicate images.
- the collections of near duplicate images can be determined in various ways, e.g., by conventional clustering algorithms that can cluster images according to image similarity.
- Visual words computed from near-duplicate images may not always be the same.
- image 302 has visual words Q and Y
- image 306 has visual words A, Q, and Y, even though visual word 306 is merely a smaller version of image 302 .
- image 304 a cropped version of image 302 , has visual words A and Q.
- Image 304 has only one visual word in common with image 302 .
- the three near-duplicate images 302 , 304 , and 306 may not be identified or may be treated inconsistently. For example, if the system requires three visual words for an image to be considered a matching image, only image 306 will be identified, even though images 302 and 304 are near-duplicates of image 306 . Similarly, images 302 , 304 , and 306 may in fact be near-duplicates of the query image, but the search system may not retrieve any of the images, for example, if the retrieval threshold is four or more visual words.
- the image search system can improve the retrieval performance by taking advantage of the collective information of near-duplicate images.
- the search system can combine visual words of the near-duplicate images into a union of visual words and treat the near-duplicate images as a single collection of near-duplicate images 315 .
- the image search system can treat the collection of near-duplicate images 315 as a single unit with visual words A, Q, and, Y.
- treating the collection of near-duplicate images 315 as a single unit reduces the possibility that an image that is similar to the query image is not retrieved or that near-duplicate images are treated inconsistently.
- the system enforces a size restriction when generating clusters of near-duplicate images.
- the system can divide the cluster into two smaller clusters. The system can cluster the images so that each image in a large collection of images to assigned to exactly one cluster. Some clusters may have only a single image. The system can then treat each final cluster as a collection of near-duplicate images.
- the image search system can return all images in the collection of near-duplicate images as image search results.
- the system can compute a similarity score using visual words of the collection that match visual words of the query image, assigning the same similarity score to each image in the collection.
- the image search system can rank images in the collection by another measure, such as size, for example, if a user wants to find multiple sizes of a particular image.
- the image search system can return only a single representative image from the collection of near-duplicate images.
- the system can select a representative image in various ways, for example, selecting the largest image, or an image that is selected often by users.
- the image search system can also perform an additional, more refined scoring step by computing a similarity score between the query image and each image in the collection of near-duplicate images.
- the system computes the more-refined similarity score using only visual words computed from each image, as opposed to all matching visual words for the collection of near-duplicate images.
- the image search system can index each collection of near-duplicate images according to the collection's union of visual words, rather than indexing images individually.
- FIG. 3B is a diagram of multiple example posting lists.
- Each posting list 320 , 330 , 340 shown in FIG. 3B includes a list of collections of near-duplicate images.
- Each collection of near-duplicate images indexed into a posting list has the same visual word among the union of visual words of the collection.
- An example query image 310 has three visual words, A, Q, and Y.
- the example collection of near-duplicate images 315 also has visual words, A, Q, and Y, the union of the visual words of the images in the collection 315 .
- the image search system has indexed the collection 315 by visual words in multiple posting lists, one for each visual word of the collection. The image search system traverses its posting lists to find collections of near-duplicate images that have at least a threshold number of visual words in common with the query image.
- Each posting list 320 , 330 , 340 corresponds to exactly one visual word as defined by the system, e.g., as defined by a quantizer.
- the word A posting list 320 is a list of collections of near-duplicate images that have visual word A.
- the word Q posting list 330 is a list of collections of near-duplicate images that have visual word Q.
- the word Y posting list 340 is a list of collections of near-duplicate images that have visual word Y.
- collection identifiers In the posting lists, collections of near-duplicate images are identified by collection identifiers.
- the collection identifiers can be, for example, values of a database key to a database, e.g., an index database, or file system identifiers.
- Images in a collection are further identified by document identifiers, e.g., document identifiers 20 , 67 , and 93 , which can, for example, identify electronic image documents in a database, on a file system, or on the Internet.
- item 321 of posting list 320 includes collection identifier 321 a with a value of “1”, indicating that the collection of near-duplicate images identified by collection identifier “1” is a collection of near-duplicate images that has visual word A.
- items 322 , 323 , 324 , and 325 of the posting list 320 include collection identifiers “3”, “4”, “7”, and “9”, respectively, which indicates that corresponding collections also have visual word A.
- a search system can maintain the posting lists in a sorted order by collection identifiers. Items in posting list 320 are sorted by increasing order of document identifiers, “1”, “3”, “4”, “6”, and “9”. The posting lists can be maintained in a decreasing order of collection identifiers, or the posting lists can be sorted in some other way.
- a search system can traverse the posting lists that correspond to visual words of the query image 310 to identify collections of near-duplicate images that have at least a threshold number of visual words in common with query image 310 . For example, the system can determine that the collection 315 , identified by the collection identifier “7,” has at least three visual words in common with query image 310 .
- the system can compare the number of matching visual words to a numerical threshold. If the number of matching visual words does not satisfy the threshold, the system can continue traversing the posting lists. If the number of matching visual words satisfies the threshold, the system can determine that the collection is a matching collection and stop the traversing.
- the system can use a dynamic threshold based on the size of the image collection under consideration. As the size of a particular collection of near-duplicate images increases, the number of the visual words in the union of visual words also tends to increase. Thus, a collection with many near-duplicate images and many visual words is more likely to match a query image than a collection with fewer images and fewer visual words.
- the image search system can use different thresholds when determining whether a collection of near-duplicate images is a match.
- the system can require five matching visual words for a singleton collection, e.g., a collection with only one image, and the system can require eight matching visual words for collections with multiple images.
- a search system can compute a score for a collection of near-duplicate images upon determining that the collection matches a query image.
- the system can either compute a score for the entire collection of images using the union of visual words, or the system can compute a respective score for each of the images in the collection, using only the visual words of each individual image.
- the procedure for scoring based on visual words, described below, can be used for either scoring technique.
- the search system can store further information in the posting lists to support scoring while traversing the posting list.
- Each posting list item for a visual word can, for example, include geometry data, e.g., a position and a scale value associated with the region or regions with which the visual word is associated in the image or images of the collection that have the visual word.
- item 321 of posting list 320 can include geometry data 321 b , which can specify a position and a scale value for visual word A assigned to the collection identified by collection identifier “1”.
- the value “p” of geometry data 321 b shown in FIG. 3 merely illustrates that the geometry data is different from geometry data of other items in the posting lists.
- the geometry data for visual words of the collection of near-duplicate images can be computed according to a common coordinate frame for images in the collection of near-duplicate images. Computing the common coordinate frame for images in a collection of near-duplicate images is described in more detail below with reference to FIG. 4 .
- the computed similarity score for a collection can be based on a measure of how well the visual words of the query image align with matched visual words of the matching collection under a particular geometric mapping.
- the system can define a variety of geometric mappings, including translation, scaling, rotation, skewing, in addition to other transformations.
- the geometric mapping transforms coordinates of visual words from the query image into transformed coordinates. Matched visual words of the matching collection will be said to align with visual words of the query image if the coordinates of the matched visual words are close to the respective transformed coordinates or within a certain error threshold. Thus, visual words that are more closely aligned will result in a higher score than visual words that are less closely aligned.
- the system can thus determine a particular geometric mapping by determining values for a, b, c, and d that optimize the alignment between the visual words of the query image and the visual words of the collection or individual image, as the case may be.
- the system uses a Random Sample Consensus (RANSAC) algorithm to determine values for a, b, c, and d.
- RANSAC Random Sample Consensus
- the system can compute a distance between coordinates of each matched visual word and each corresponding set of transformed coordinates. For each visual word for which the distance is within a particular error threshold the system can increase the similarity score for the collection by a particular amount, e.g., 1. For example, the system can assign a point to each aligned visual word of a matched collection. If three out of four matched visual words align with visual words of the query image, the matched collection can be assigned a similarity score of 3.
- the system can also adjust the score based on how closely the visual words of the query and target are aligned, giving a higher score for visual words that are more closely aligned.
- the system assigns a value between 0 and 1 for each visual word based on the computed distance.
- the system can assign 1 to a perfectly aligned visual word, 0 to a visual word with a computed distance beyond the error threshold, and a proportional value between 0 and 1 for distances between 0 and the error threshold.
- the error threshold depends on the scale value associated with the visual word, with visual words of larger regions having a larger error threshold than visual words of smaller regions.
- an example query image has three visual words, each having particular example coordinates in the image as follows:
- the system can now compute a similarity score for the matched collection using the geometry data.
- the system can search for a geometric mapping between the visual words of the query image 310 and the matched collection, or:
- the system can compute distances between the transformed coordinates and the coordinates of visual words of the matched collection and assign scores based on the alignment of visual words. If the error threshold is 10 pixels and the matched collection has three aligned visual words, the system can, for example, assign a score of 3. In contrast, if the error threshold is 5 pixels, the matched collection would have only two aligned visual words.
- the computed distances can also affect the score for each aligned visual word.
- visual word A m can be said to be perfectly aligned with the transformed coordinates because the computed distance is zero.
- the system can assign a maximum score to visual word A m , e.g., 1.
- the computed distance for visual word Q m is 6, and thus the system can assign a lower score to visual word Q m , e.g., 0.4, than to visual word A m . If the error threshold were 5 pixels, the system could instead assign a score of 0 to visual word Q m .
- FIG. 4 is a flow chart of an example process 400 for indexing collections of near-duplicate images.
- the process 400 can be performed by a component of a search system, for example, indexing engine 242 or ranking engine 244 ( FIG. 2 ), implemented as one or more computer programs installed on one or more computers in one or more locations.
- the process 400 will be described as being performed by a system of one or more computers.
- the system aligns images in a collection of near-duplicate images to a common coordinate frame ( 410 ). Because near-duplicate images can be cropped or rescaled versions of another image, corresponding visual words, e.g., visual words that represent the same image features, can have different pixel coordinates. The system can thus align the images to a common coordinate frame to ensure that visual words representing the same image features have corresponding pixel coordinates. Therefore, the system can first define a common coordinate frame and align all images in the collection of near-duplicate images to the common coordinate frame. The system can therefore ensure that corresponding visual words have corresponding pixel coordinates in the common coordinate frame. In some implementations, the system selects a reference image in the collection of images as the basis for the common coordinate frame. The system can then align each image in the collection by determining an affine transformation between the image and the reference image. The affine transformation for each image in the collection maps the coordinate frame of the collection to the features of the image.
- the system determines visual words in the common coordinate frame ( 420 ).
- the pixel coordinates of each non-reference image of the collection can be transformed into coordinates in the common coordinate frame by applying a corresponding affine transformation, e.g., the affine transformation that aligned the image with the reference image.
- the system can then compute visual words for each image using pixel coordinates of the common coordinate frame.
- the system determines a union of visual words in the common coordinate frame ( 430 ).
- the system adds a posting list item that identifies the collection to one or more posting lists ( 440 ).
- the posting list item includes a collection identifier for the collection of near-duplicate images.
- the posting list item can also include geometry data and other data, as described above.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory program carrier for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- special purpose logic circuitry e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
f(x,y)=[ax+b,cy+d],
where (x, y) are coordinates of visual words in the query image, and [ax+b, cy+d] are transformed coordinates. The system can thus determine a particular geometric mapping by determining values for a, b, c, and d that optimize the alignment between the visual words of the query image and the visual words of the collection or individual image, as the case may be. In some implementations, the system uses a Random Sample Consensus (RANSAC) algorithm to determine values for a, b, c, and d.
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/531,912 US8923626B1 (en) | 2012-06-25 | 2012-06-25 | Image retrieval |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/531,912 US8923626B1 (en) | 2012-06-25 | 2012-06-25 | Image retrieval |
Publications (1)
Publication Number | Publication Date |
---|---|
US8923626B1 true US8923626B1 (en) | 2014-12-30 |
Family
ID=52112561
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/531,912 Active 2033-06-22 US8923626B1 (en) | 2012-06-25 | 2012-06-25 | Image retrieval |
Country Status (1)
Country | Link |
---|---|
US (1) | US8923626B1 (en) |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140203947A1 (en) * | 2013-01-20 | 2014-07-24 | Alan Haddy | Storage and recall of buried asset data over communications networks for damage avoidance and mapping |
US9535996B1 (en) * | 2012-08-30 | 2017-01-03 | deviantArt, Inc. | Selecting content objects for recommendation based on content object collections |
WO2017105450A1 (en) * | 2015-12-16 | 2017-06-22 | Assa Abloy Ab | Optical authentication of objects based on latent structural characteristics |
US9747274B2 (en) * | 2014-08-19 | 2017-08-29 | International Business Machines Corporation | String comparison results for character strings using frequency data |
CN110297611A (en) * | 2018-03-22 | 2019-10-01 | 富士施乐株式会社 | System and method for tracking the duplication for the printing material that right holder possesses |
US11461360B2 (en) * | 2018-03-30 | 2022-10-04 | AVAST Software s.r.o. | Efficiently initializing distributed clustering on large data sets |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7991232B2 (en) | 2004-03-03 | 2011-08-02 | Nec Corporation | Image similarity calculation system, image search system, image similarity calculation method, and image similarity calculation program |
US8031914B2 (en) | 2006-10-11 | 2011-10-04 | Hewlett-Packard Development Company, L.P. | Face-based image clustering |
US8073818B2 (en) * | 2008-10-03 | 2011-12-06 | Microsoft Corporation | Co-location visual pattern mining for near-duplicate image retrieval |
US20120054600A1 (en) | 2010-08-31 | 2012-03-01 | Picaboo Corporation | Image clustering and page layout system and method |
US20120321193A1 (en) | 2010-12-30 | 2012-12-20 | Nokia Corporation | Method, apparatus, and computer program product for image clustering |
US8352465B1 (en) | 2009-09-03 | 2013-01-08 | Google Inc. | Grouping of image search results |
US8452059B2 (en) | 2008-09-25 | 2013-05-28 | Cyberlink Corp. | Systems and methods for performing image clustering |
US20130251266A1 (en) | 2012-03-21 | 2013-09-26 | Casio Computer Co., Ltd. | Image search system, image search apparatus, image search method and computer-readable storage medium |
US8625907B2 (en) | 2010-06-10 | 2014-01-07 | Microsoft Corporation | Image clustering |
US20140105509A1 (en) * | 2012-10-15 | 2014-04-17 | Canon Kabushiki Kaisha | Systems and methods for comparing images |
-
2012
- 2012-06-25 US US13/531,912 patent/US8923626B1/en active Active
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7991232B2 (en) | 2004-03-03 | 2011-08-02 | Nec Corporation | Image similarity calculation system, image search system, image similarity calculation method, and image similarity calculation program |
US8031914B2 (en) | 2006-10-11 | 2011-10-04 | Hewlett-Packard Development Company, L.P. | Face-based image clustering |
US8452059B2 (en) | 2008-09-25 | 2013-05-28 | Cyberlink Corp. | Systems and methods for performing image clustering |
US8073818B2 (en) * | 2008-10-03 | 2011-12-06 | Microsoft Corporation | Co-location visual pattern mining for near-duplicate image retrieval |
US8352465B1 (en) | 2009-09-03 | 2013-01-08 | Google Inc. | Grouping of image search results |
US8625907B2 (en) | 2010-06-10 | 2014-01-07 | Microsoft Corporation | Image clustering |
US20120054600A1 (en) | 2010-08-31 | 2012-03-01 | Picaboo Corporation | Image clustering and page layout system and method |
US20120321193A1 (en) | 2010-12-30 | 2012-12-20 | Nokia Corporation | Method, apparatus, and computer program product for image clustering |
US20130251266A1 (en) | 2012-03-21 | 2013-09-26 | Casio Computer Co., Ltd. | Image search system, image search apparatus, image search method and computer-readable storage medium |
US20140105509A1 (en) * | 2012-10-15 | 2014-04-17 | Canon Kabushiki Kaisha | Systems and methods for comparing images |
Cited By (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9535996B1 (en) * | 2012-08-30 | 2017-01-03 | deviantArt, Inc. | Selecting content objects for recommendation based on content object collections |
US10417294B2 (en) | 2012-08-30 | 2019-09-17 | Wix.Com Ltd. | Selecting content objects for recommendation based on content object collections |
US11301528B2 (en) | 2012-08-30 | 2022-04-12 | Wix.Com Ltd. | Selecting content objects for recommendation based on content object collections |
US20140203947A1 (en) * | 2013-01-20 | 2014-07-24 | Alan Haddy | Storage and recall of buried asset data over communications networks for damage avoidance and mapping |
US9747274B2 (en) * | 2014-08-19 | 2017-08-29 | International Business Machines Corporation | String comparison results for character strings using frequency data |
US9747273B2 (en) * | 2014-08-19 | 2017-08-29 | International Business Machines Corporation | String comparison results for character strings using frequency data |
WO2017105450A1 (en) * | 2015-12-16 | 2017-06-22 | Assa Abloy Ab | Optical authentication of objects based on latent structural characteristics |
EP3375132A4 (en) * | 2015-12-16 | 2019-07-10 | Assa Abloy AB | Optical authentication of objects based on latent structural characteristics |
US11030453B2 (en) | 2015-12-16 | 2021-06-08 | Assa Abloy Ab | Optical authentication of objects based on latent structural characteristics |
CN110297611A (en) * | 2018-03-22 | 2019-10-01 | 富士施乐株式会社 | System and method for tracking the duplication for the printing material that right holder possesses |
US11461360B2 (en) * | 2018-03-30 | 2022-10-04 | AVAST Software s.r.o. | Efficiently initializing distributed clustering on large data sets |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8429173B1 (en) | Method, system, and computer readable medium for identifying result images based on an image query | |
US9002831B1 (en) | Query image search | |
US8209330B1 (en) | Ordering image search results | |
US8924372B2 (en) | Dynamic image display area and image display within web search results | |
US11442946B2 (en) | Distance based search ranking demotion | |
US10210181B2 (en) | Searching and annotating within images | |
US9396413B2 (en) | Choosing image labels | |
US20140105505A1 (en) | Near duplicate images | |
US8923626B1 (en) | Image retrieval | |
US10503803B2 (en) | Animated snippets for search results | |
US9158857B2 (en) | Identifying landing pages for images | |
WO2012030514A1 (en) | Sketch-based image search | |
US20150169740A1 (en) | Similar image retrieval | |
US9218366B1 (en) | Query image model | |
US9507805B1 (en) | Drawing based search queries | |
US20200159765A1 (en) | Performing image search using content labels | |
US11055335B2 (en) | Contextual based image search results | |
KR101931859B1 (en) | Method for selecting headword of electronic document, method for providing electronic document, and computing system performing the same | |
US20160342860A1 (en) | Method and system for searching images | |
US9183251B1 (en) | Showing prominent users for information retrieval requests | |
WO2020247960A1 (en) | Method and apparatus for cosmetic product recommendation | |
US20230153338A1 (en) | Sparse embedding index for search |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:STEWENIUS, HENRIK C.;REEL/FRAME:028687/0865Effective date: 20120621 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |