CN118043818A - Self-attention-based neural network for processing network metrics from multiple modalities - Google Patents
Self-attention-based neural network for processing network metrics from multiple modalities Download PDFInfo
- Publication number
- CN118043818A CN118043818A CN202280064882.0A CN202280064882A CN118043818A CN 118043818 A CN118043818 A CN 118043818A CN 202280064882 A CN202280064882 A CN 202280064882A CN 118043818 A CN118043818 A CN 118043818A
- Authority
- CN
- China
- Prior art keywords
- neural network
- network
- input
- modality
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 230
- 238000012545 processing Methods 0.000 title claims description 37
- 238000012549 training Methods 0.000 claims abstract description 127
- 238000000034 method Methods 0.000 claims abstract description 96
- 230000008569 process Effects 0.000 claims description 71
- 238000010801 machine learning Methods 0.000 claims description 47
- 230000007246 mechanism Effects 0.000 claims description 27
- 238000004590 computer program Methods 0.000 abstract description 15
- 239000013598 vector Substances 0.000 description 13
- 230000009471 action Effects 0.000 description 7
- 239000011159 matrix material Substances 0.000 description 6
- 238000004891 communication Methods 0.000 description 5
- 230000008901 benefit Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 230000004044 response Effects 0.000 description 4
- 230000009466 transformation Effects 0.000 description 4
- 230000003993 interaction Effects 0.000 description 3
- 238000005070 sampling Methods 0.000 description 3
- 230000026676 system process Effects 0.000 description 3
- 238000013527 convolutional neural network Methods 0.000 description 2
- 206010027175 memory impairment Diseases 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000005192 partition Methods 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000013526 transfer learning Methods 0.000 description 2
- TVYLLZQTGLZFBW-ZBFHGGJFSA-N (R,R)-tramadol Chemical compound COC1=CC=CC([C@]2(O)[C@H](CCCC2)CN(C)C)=C1 TVYLLZQTGLZFBW-ZBFHGGJFSA-N 0.000 description 1
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 1
- 241000282326 Felis catus Species 0.000 description 1
- 241000009334 Singa Species 0.000 description 1
- 239000000654 additive Substances 0.000 description 1
- 230000000996 additive effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 230000001186 cumulative effect Effects 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000010365 information processing Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000014616 translation Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for performing and training a multi-modal multi-tasking self-attention neural network.
Description
Cross Reference to Related Applications
The present application claims the benefit of priority from U.S. provisional application No.63/252,593 filed on 5/10/2021, the entire contents of which are incorporated herein by reference.
Technical Field
The present description relates to processing inputs using neural networks.
Background
Neural networks are machine-learning models that employ one or more layers of nonlinear elements to predict the output of a received input. In addition to the output layer, some neural networks include one or more hidden layers. The output of each hidden layer is used as an input to the next layer (i.e., the next hidden layer or output layer) in the network. Each layer of the network generates an output from the received input according to the current value of the corresponding parameter set.
Disclosure of Invention
The present specification describes a system implemented as a computer program on one or more computers in one or more locations that uses a multi-modal, multi-tasking neural network to generate network outputs for received network inputs.
That is, the neural network can be used to perform a plurality of different machine learning tasks for inputs from a plurality of different modalities.
In particular, the neural network includes a shared self-attention layer set that is shared between tasks and modalities. The neural network can also include a modality-specific layer, a task-specific layer, or both.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
Using the techniques described in this specification, a system is able to train and execute a single neural network configured to process respective input sequences corresponding to a plurality of different modalities using self-attention. By co-training the neural network using machine learning tasks corresponding to different modalities, the training system is able to achieve higher performance on each task than training the neural network using a single task or a single modality.
By processing the respective inputs corresponding to the different modalities, the neural network is able to learn to generate a feature representation of the inputs generalized across multiple domains. As a specific example, by processing an input representing an image, the neural network can learn to generate a feature representation that is also useful in processing an input representing audio data.
Training the same neural network for different modalities can further improve the efficiency of training. For example, the number of training steps required to train a multi-modal neural network can be significantly less than the total number of training steps required to train a corresponding neural network of different tasks and modalities, such that the time and computational cost required to train a neural network is less than the time and computational cost required to train a plurality of corresponding neural networks.
As another example, training a single neural network requires less overall network parameters to be learned than training a separate neural network for each task corresponding to each modality. As a specific example, in some implementations in which the neural network is configured to perform n different tasks, the techniques described in this specification can reduce the number of network parameters that need to be learned to approximate the factor n. For example, the neural network can be deployed on an edge device (e.g., a mobile phone or tablet computer) that has limited computing and memory resources that would otherwise make it impractical to deploy n different trained neural networks on the edge device.
A neural network configured to process input sequences from multiple modalities can further have improved time, memory, and/or computational efficiency at inference time as compared to a neural network trained for a single task or modality. For example, in some embodiments, for an input sequence corresponding to any given task and/or modality, the neural network can activate only a strict subset of its network parameters to process the input sequence. As a particular example, as described above, the neural network can include one or more task-specific network blocks and/or one or more modality-specific network blocks. Thus, the neural network is able to perform less computation for each input, as only a subset of the network blocks are used. These efficiency gains can be particularly important on edge devices that may have limited computing resources available.
In some embodiments described in this specification, the system is able to train the neural network without any additional hyper-parameter adjustment relative to the single-task, single-mode neural network, thereby further improving training efficiency.
As described in this specification, a self-attention-based neural network configured to process input from a respective modality (e.g., input comprising one or more images, video, and/or audio sequences) can require less computation to achieve the same performance as prior art convolutional neural networks. That is, self-attention based neural networks perform better than convolutional neural networks for a fixed computational budget. This is because applying self-attention is generally more efficient than convolving the kernel across the entire sequence, because the self-attention mechanism can pay attention to different regions of the sequence with less computation than convolution.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example neural network system.
FIG. 2 is a flow chart of an example process for training a neural network.
Fig. 3 illustrates different schemes for training a neural network over multiple tasks.
FIG. 4 is a flow chart of an example process for generating a network output using a neural network.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
The specification describes systems implemented as computer programs on one or more computers in one or more locations that are capable of performing a number of different tasks using a self-attention based neural network.
Fig. 1 illustrates an example neural network system 100. The neural network system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.
The system 100 executes a self-attention-based neural network that has been configured through training to process network inputs from one of a plurality of different modalities and generate network outputs that characterize the network inputs.
A self-attention-based neural network is a neural network that includes one or more self-attention neural network layers. The self-attention neural network layer receives as input a sequence of layer input elements and applies an attention mechanism to the sequence of layer input elements to generate a sequence of layer output elements. In particular, for each input element, the self-attention neural network layer applies an attention mechanism on the sequence of layer input elements using one or more queries derived from the input element to generate a corresponding output element. Some of the self-care neural network layers are multiheaded self-care neural network layers. The multi-headed self-focusing neural network layer applies h different focusing mechanisms in parallel to generate a corresponding sequence of output elements, and then combines the multiple sequences of output elements to generate a final sequence of output elements.
The self-attention mechanism is described in more detail below.
The self-attention based neural network is configured to process a respective network input 102 corresponding to each of a plurality of modalities.
In this specification, a "modality" characterizes a pattern by which data can be represented, and can define a classification of network inputs, each representing data according to the pattern.
For example, the plurality of modalities can include an "image" modality, wherein the network input corresponding to the image modality includes or represents one or more images.
As another example, the plurality of modalities can include a "video" modality, wherein the network input corresponding to the video modality includes or represents one or more videos.
As another example, the plurality of modalities can include an "audio" modality, wherein the network input corresponding to the audio modality includes or represents one or more audio samples.
For each modality, the self-attention based neural network can be configured to generate and process an input sequence corresponding to the modality to perform one or more machine learning tasks corresponding to the modality.
For example, the one or more machine learning tasks corresponding to each modality may include a classification task.
For example, where the modality includes an image modality, the system may process pixels of an image included in or represented by a network input corresponding to the image modality to generate a sequence of inputs corresponding to the image modality. The system may process the input sequence to generate a predicted network output comprising an image classification output comprising a respective score corresponding to each of the plurality of categories.
Where the modality includes a video modality, the system may process pixels of one or more image frames included in a network input corresponding to the video modality to generate an input sequence corresponding to the video modality. The system may process the input sequence to generate a predicted network output, which may include a video classification output or a video frame classification output. The video (or video frame) classification output may include a respective score corresponding to each of a plurality of categories.
Where the modality includes an audio modality, the system may process audio samples included in network inputs corresponding to the audio modality to generate an input sequence corresponding to the audio modality. The system may process the input sequence to generate a predicted network output comprising an audio classification output comprising a respective score corresponding to each of the plurality of categories.
In some examples, the modality includes one of an image modality, a video modality, or an audio modality, and the one or more machine learning tasks include a classification task of the modality. In some examples, the modalities include two of an image modality, a video modality, or an audio modality, and the one or more machine learning tasks include a classification task for each of the two modalities (e.g., the two modalities may include a video modality and an audio modality). As another particular example, the modalities may include an image modality, a video modality, and an audio modality. In this case, the one or more machine learning tasks may include an image classification task, a video (or video frame) classification task, and an audio classification task.
Other machine learning tasks are also possible for the various modalities. For example, for an input sequence representing one or more images, the self-attention based neural network can process the input sequence to perform one or more image processing machine learning tasks.
Example machine learning tasks are discussed further below.
The self-attention based neural network processes each input sequence using one or more shared neural network layers 120, regardless of the modality or machine learning task corresponding to the input sequence.
The shared neural network layer 120 includes one or more self-attention neural network layers. For example, the self-attention-based neural network can include a series of self-attention network blocks (also referred to as "transducer layers"), each self-attention network block including one or more self-attention neural network layers, wherein the input of each self-attention network block is the output of the previous block in the sequence. In the example of fig. 1, the shared layer 120 includes a sequence of L transducer layers.
Typically, the self-attention based neural network further includes one or more modality-specific neural network layers configured to process only input sequences corresponding to specific modalities of the plurality of modalities (or intermediate representations of input sequences generated by previous neural network layers in the self-attention based neural network).
In some implementations, the self-attention based neural network can include one or more task-specific neural network layers configured to process only input sequences (or intermediate representations thereof) corresponding to specific tasks of specific ones of the plurality of modalities.
That is, the neural network can include a shared layer 120, one or more modality-specific layers, and one or more task-specific layers.
In particular, before and/or after the sequence of self-attention network blocks and for each of the plurality of modalities, the self-attention based neural network can include one or more modality-specific network blocks including one or more modality-specific neural network layers of the modality.
The modality-specific network block of each modality can be at the same location in the architecture of the self-attention-based neural network such that when a preceding neural network layer in the architecture generates a layer output corresponding to an input sequence having a specific modality, the self-attention-based neural network can determine to provide the layer output to the modality-specific network block corresponding to the specific modality. Then, when the modality-specific network block generates a block output, the self-attention based neural network can provide the block output to the same subsequent neural network layer regardless of the specific modality.
Similarly, before and/or after the sequence of self-care network blocks and for each machine learning task corresponding to each modality, the self-care neural network can include one or more task-specific network blocks including one or more task-specific neural network layers for the task. The task-specific network blocks of each task of each modality can be at the same location in the architecture of the self-attention based neural network, as described above.
As a particular example, the modality-specific network block can include an input network block 110 configured to generate and process an input sequence, and the task-specific network block can be an output network block 130 configured to generate a network output of the input sequence.
That is, the neural network can include a respective input network block 110 (also referred to as a "word analyzer (tokenizer)") for each modality, and can include a respective output network block 130 (also referred to as a "task head") for each task.
In some such embodiments, most of the network parameters of the self-attention based neural network are shared across modalities, i.e., in the shared neural network layer 120. As a specific example, more than 50%, more than 60%, more than 70%, more than 80%, or more than 90% of the network parameters of the self-attention-based neural network can be shared across all modalities.
Optionally, the neural network can also include one or more modality-specific self-attention layer blocks that receive as input a sequence generated by the lemma analyzer for the modality and generate an output that is consumed by the first shared self-attention block as input.
Thus, self-attention based neural networks can significantly benefit from cross-modal co-training because shared network parameters can learn to extract meaningful information from network inputs corresponding to any modality.
Given an input data object corresponding to a particular modality, a self-attention based neural network can generate an input sequence 112 using a modal's word element analyzer 110, and then process the input sequence 112 using a shared neural network layer 120 to generate an output sequence 122.
The task header 130 corresponding to the task to be performed on the input data object can then process the output sequence 122 to generate an output of the task. For example, each task header 130 can include one or more fully connected layers configured to process the output sequence 122, or more specifically, to process one or more embeddings of the output sequence 122, to generate the output of the task.
In particular, given a particular modality of input data object, the modal's lemma analyzer 110 is capable of generating an input sequence 112 by determining a plurality of partitions (patches) of the input data object, wherein each partition includes a different subset of the data object's elements.
For example, given an input image corresponding to an image modality, a self-attention based neural network can determine respective image tiles that each include a subset of pixels of the image.
As another example, given an input video corresponding to a video modality, a self-attention-based neural network can generate video tiles, each video tile comprising: one or more frames of video, a subset of pixels of a single frame of video, or a time slice of video that includes a subset of pixels from each of a plurality of frames of video.
As another example, given input audio samples corresponding to an audio modality, a self-attention-based neural network may be capable of each including a respective audio block of a subset of time steps of the audio samples.
The lemma analyzer 110 is then capable of processing the chunks of the input data object to generate an input sequence comprising a respective input element at each of the plurality of input positions. Each of the one or more input locations can correspond to a respective different chunk of the input data object.
For example, the lemma analyzer 110 can generate, for each chunk of the data object, a one-dimensional input element that includes the elements of the chunk. As a specific example, if each tile is an image tile having a dimension of lxwxc, where C represents the number of channels of the image (e.g., c=3 for an RGB image), the system is able to generate an input element having a dimension of 1× (l·w·c) in the input sequence.
As another example, the lemma analyzer 110 can generate a one-dimensional initial input element as described above for each chunk of the data object. The word element analyzer 110 can then use an embedded neural network that processes the initial input elements to generate an embedding of the initial input elements. For example, embedding the neural network can include projecting the initial input into one or more feedforward neural network layers in the embedding space.
In some implementations, the lemma analyzer 110 can determine the embedding of the initial input element as the input element in the input sequence 112.
In some other implementations, the lemma analyzer 110 can embed combine the initial input element with the position of the initial input element to generate an input element in the input sequence. The location embedment represents a location within the data object corresponding to the chunk of the initial input element. In some implementations, the location embedding corresponding to each chunk of the data object can be an integer. For example, a first image tile at the top left of the input image can have a position embedding of '1', a second image tile immediately to the right of the first image tile has a position embedding of '2', and so on. In some other implementations, the location embedding is machine-learned. For example, during training of the self-attention based neural network, the training system can learn the location embedding simultaneously by back-propagating errors of the self-attention based neural network through the self-attention based neural network and to the location embedding.
In some implementations, one or more of the input elements in the input sequence 112 do not correspond to any chunks of the data object. As a particular example, the input sequence 112 can include one machine-learned input element (also referred to as a "classification" lemma); for example, the first input element and/or the last input element of the input sequence can be machine-learned input elements. For example, during training of the self-attention based neural network, the training system can concurrently learn one or more machine learning input elements by back-propagating errors of the self-attention based neural network through the self-attention based neural network and to the machine learning input elements. In embodiments where the input elements corresponding to the respective tiles include location embeddings, the lemma analyzer 110 is also capable of adding location embeddings to the machine-learned input elements, e.g., machine-learned location embeddings or all-zero location embeddings. When the input sequence 112 includes a categorized word, the output header can process the embedding corresponding to the categorized word in the output sequence to generate an output of the task.
After generating the input sequence 112 corresponding to the respective data object, the self-attention based neural network can process the input sequence 112 using the shared layer 120 to generate the output sequence 112, and then process the output sequence 122 using the header 130 corresponding to the appropriate task to generate a network output of the respective data object characterizing the task.
In the example of fig. 1, the neural network is configured to process inputs of three different modalities: image, audio, and video. More specifically, the neural network is configured to perform task 1, task 2, and task 3 on an image, tasks 4 and 5 on a video, and tasks 6 and 7 on an audio.
However, more generally, the neural network can be configured to perform any suitable set of machine learning tasks corresponding to any suitable set of modalities.
Some specific examples are as follows.
For example, at least one of the machine learning tasks may be a speech recognition task, wherein the neural network is configured to process the representation of the audio waveform to generate an output that characterizes a sequence of phonemes, characters, or words corresponding to the audio waveform.
As another example, at least one of the machine learning tasks may be a video analysis task in which the neural network is configured to process a sequence of video frames to generate an output that characterizes the video frames, for example, by characterizing whether the video frames depict a person performing a particular action.
As another example, at least one of the machine learning tasks may be a natural language processing task in which the neural network is configured to process a portion of text to generate an output that characterizes the portion of text, for example, by characterizing translations of the portion of text as different natural languages.
As another example, at least one of the machine learning tasks may be an image processing task, wherein the neural network is configured to process an input comprising an image to generate a corresponding output, such as a classification output, a regression output, or a combination thereof.
The neural network can be configured to process any suitable type of image, such as RGB images, LIDAR images (e.g., point clouds), and the like. The neural network can be configured to process the image to perform any suitable image processing task, such as a classification task, a regression task, or a combination thereof.
In particular, in this specification, processing an image refers to processing intensity values of pixels of the image.
As a particular example, the neural network can be configured to generate a classification output that includes a respective score corresponding to each of a plurality of categories. The score of a category indicates the likelihood that the image belongs to that category. In some cases, the category may be a classification of the object (e.g., dog, cat, person, etc.), and if the image depicts an object included in the object classification corresponding to the category, the image may belong to the category. In some cases, a category may represent a global image attribute (e.g., whether an image depicts a day or night scene, or whether an image depicts a summer or winter scene), and an image may belong to a category if the image has a global attribute that corresponds to the category.
As another particular example, the neural network can be configured to generate an element-level classification output (e.g., a pixel-level classification output of an RGB image or a point-level classification output of a LIDAR image) that includes, for each element in the image, a respective score corresponding to each of a plurality of categories. For a given element (e.g., for a given pixel or point), the score for the category indicates the likelihood that the element belongs to the category. In some cases, a category may be a classification of an object, and an element may belong to the category if the element is part of an object included in the object classification corresponding to the category. That is, the element-level classification output may be a semantic segmentation output.
As another particular example, the neural network can be configured to generate a regression output that estimates one or more continuous variables that characterize the image (i.e., can assume an infinite number of possible values). In a particular example, the regression output may estimate coordinates of a bounding box surrounding a corresponding object depicted in the image. The coordinates of the bounding box may be defined by the (x, y) coordinates of the vertices of the bounding box.
In some implementations, the neural network can be configured to process multiple images, such as multiple frames of video. For example, the neural network can receive a plurality of images that are video frames of a video, and can process each video frame as described above to generate an output that characterizes the video frame, e.g., by characterizing whether the video frame depicts a person performing a particular action.
In some such embodiments, the neural network processes each video frame at a respective different point in time to generate a respective network output of each video frame that characterizes the prediction of the video frame. For example, the neural network can generate a network output that predicts a classification of the video frame. In some such implementations, the neural network combines a plurality of network outputs corresponding to respective video frames to generate a final network output characterizing the video. For example, the neural network can use a downstream neural network (e.g., a recurrent neural network) to process the corresponding network output.
In some other implementations, the neural network processes each video frame in parallel to generate a single network output that characterizes the video. As a particular example, the system can generate one or more respective input elements in the input sequence for each video frame.
Before using the neural network to perform one or more tasks, the system 100 or another training system trains the neural network on training data for the tasks, i.e., enables the neural network to efficiently perform multiple tasks on data from multiple different modalities.
The training neural network will be described below with reference to fig. 2 and 3.
After training, the self-attention based neural network can be deployed in any suitable setting.
In some implementations, when deploying the trained self-attention-based neural network, the self-attention-based neural network is configured to process an input sequence corresponding to only a single particular modality of a plurality of modalities for which the self-attention-based neural network is trained. For example, modality-specific network blocks corresponding to other modalities and/or task-specific network blocks corresponding to respective tasks of other modalities can be removed from the architecture of the self-attention-based neural network, leaving only shared network blocks and optionally modality-specific network blocks and task-specific network blocks corresponding to specific modalities. In some such embodiments, the deployed self-attention-based neural network is configured to perform only a single task of the plurality of tasks corresponding to the particular modality for which the self-attention-based neural network is trained.
In some other implementations, the deployed self-attention based neural network is configured to process a respective input sequence corresponding to each of a plurality of modalities. That is, the self-attention-based neural network can be deployed in an environment, such as in a data center or on an edge device, where the self-attention-based neural network is to receive a respective input sequence corresponding to each modality, and can perform each of one or more machine learning tasks corresponding to the modality for which the self-attention-based neural network is trained.
As a particular example, after training, the client device can interact with the system 100 through Application Programming Inference (API) (e.g., web-based API). In particular, the client device can submit an API call that includes or identifies network input to be analyzed, and the system 100 can be provided in response to data identifying the network output of the input. For example, the system 100 may format the object detection output into a specified format, e.g., as a JavaScript object notation (JSON) file or as another type of data exchange format, and provide the file in response to an API call.
FIG. 2 is a flow chart of an example process 200 for training a neural network. For convenience, process 200 will be described as being performed by a system of one or more computers located in one or more locations. For example, a suitably programmed neural network system (e.g., the neural network system 100 of fig. 1) can perform the process 200.
The system can repeatedly perform iterations of process 200 for different batches of training examples to update parameters of the neural network, i.e., parameters of the word element analyzer, shared layer, and neural network head.
That is, at each iteration of process 200, the system obtains a batch of one or more training examples, for example, by sampling the batch from a larger training data set, and updates parameters of the neural network system using the batch of one or more training examples.
The system can continue to perform iterations of process 200 until termination criteria for training of the neural network have been met, for example, until the parameters have converged, until a threshold amount of wall clock time has elapsed, or until a threshold number of iterations of process 200 have been performed.
Each training example includes a training input of a corresponding modality and a target output of one of a plurality of tasks to be performed on the input of the corresponding modality.
In some implementations, the system selects the batches such that each training input in the batch has the same modality and each training example is for the same task.
In some other implementations, the system selects a lot such that different examples can be used for different tasks of the plurality of tasks.
Example techniques for selecting a batch training example from a training dataset that includes training examples for a plurality of different tasks are described below with reference to fig. 3.
At each iteration of process 200, the system performs steps 202-210 for each training example in the batch.
The system obtains a network input corresponding to a particular modality and having a plurality of elements (step 202).
The system determines a plurality of tiles of the network input (step 204). Typically, each chunk includes a different subset of elements of the network input.
The system processes the plurality of tiles to generate an input sequence having respective input elements at each of the plurality of input locations, wherein some or all of the input elements correspond to respective different tiles (step 206). In particular, the system can process the chunks using input network chunks of the neural network ("word element analyzer") to generate input sequences.
The system processes the input sequence using the neural network to generate a respective predicted network output for at least one of the one or more machine learning tasks corresponding to the particular modality (step 208).
As described above, the neural network has one or more self-attention neural network layers, each configured to apply a self-attention mechanism to an input sequence or an intermediate representation of the input sequence. Additionally, at least a subset of the self-attention neural network layer is shared across multiple modalities. For example, the neural network can include only shared self-attention layer blocks or one or more modality-specific self-attention layer blocks, followed by a shared set of self-attention blocks that are shared among all modalities.
The system then determines updates to the plurality of parameters of the neural network based on the errors in the corresponding predicted network outputs (step 210).
When a task requires classification output, the error can be cross entropy loss or other suitable classification loss.
When a task requires regression output, the error can be a mean square error loss or other suitable regression loss.
In particular, the system can calculate a gradient of the error relative to the parameters of the neural network for each training example in the batch, and can combine, for example, the average or the sum to determine a combined gradient. The system can then apply an optimizer to the current values of the combined gradient and parameters to generate updated values of the parameters.
Generally, for any given input, the gradient will be only non-zero for the modality corresponding to the input, the shared network block, and the components corresponding to the task performed on the input.
In other words, for each of the plurality of modalities, the training system is capable of processing the training network input corresponding to the modality to generate a respective predicted network output for each of the one or more machine learning tasks corresponding to the modality. The training system can then determine errors in the corresponding predicted network outputs of the machine learning task and determine updates to the network parameters of the self-attention-based neural network from the errors, for example, using back propagation and gradient descent. Typically, for a given machine learning task and a given modality, the training system only determines updates to network parameters that process the input sequence corresponding to the given task and the given modality. For example, if the self-attention-based neural network includes a respective modality-specific network block for each modality, the training system can determine only updates to parameters of the modality-specific network block corresponding to the modality of the input sequence (and network parameters shared across all modalities, e.g., parameters of the self-attention network block).
Upon determining a parameter update corresponding to a particular modality or particular task, the training system can identify a predetermined training hyper-parameter set corresponding to the particular modality or particular task. For example, for each modality or for each task, the training system can identify a respective different learning rate, number of warm-up steps, initialization of network parameters, training steps, momentum, adam super-parameters, and so forth. In other words, throughout the training of the self-attention based neural network, the training system is able to change the hyper-parameters according to the modality or task for which the training system is processing the training network input. Furthermore, the system may not need to perform a hyper-parameter search to determine the values of these hyper-parameters, but rather can reuse the values of the hyper-parameters used in training the corresponding single-tasked neural networks, i.e., the values of the hyper-parameters generated as an output of the hyper-parameter search prior to training of the corresponding single-tasked neural networks.
FIG. 3 illustrates an example scenario for selecting batches of training examples from a training dataset.
Specifically, fig. 3 shows an example scenario for selecting a batch of training examples when the training dataset includes training examples for three tasks: an example of three lots from task #1, an example of five lots from task #2, and an example of seven lots from task # 3. That is, the training data set is a "large" training data set consisting of separate training data sets for each of the three tasks.
In the example of fig. 3, each task corresponds to a different modality. However, in some other examples, one or more modalities can have more than one corresponding task.
At each iteration of the process 200 described above, the system selects a batch of training examples from a larger training dataset and trains the neural network using the batch, i.e., updates parameters of the neural network.
The system can select batches of training examples for each iteration of process 200 from a larger training dataset in any of a variety of ways.
In some implementations, during training, the system can sample batches according to the size of the corresponding training data set from which the batch was generated (i.e., the respective size of the corresponding training data set for each task). That is, the system samples the batch based on the size of the corresponding training data set for the plurality of tasks. Thus, the system will train the neural network more frequently for tasks with larger amounts of training data.
This is referred to as "task-by-task" training 310 in fig. 3. In task-by-task training, the system randomly orders the tasks and then continues to train the neural network on the tasks according to the order. When training the neural network on a given task, the system trains the neural network on all batches of training data before proceeding to the next task (or terminating training for the last task in the sequence). Thus, in the example of fig. 3, the system would train the neural network on all 5 batches from task 2, then on all 3 batches from task 1, and then on all 7 batches from task 3. However, this training pattern can lead to catastrophic forgetfulness, where earlier tasks are "forgotten" to favor later tasks in sequence.
To help prevent such catastrophic forgetfulness, another example of which is referred to in fig. 3 as "weighted task sampling" training 340. In this training scheme, at each training step, for each task, the likelihood of the batch selected for that training step is based on the number of batches in the training data of the task relative to the total number of batches for all tasks. For example, it may be possible to equal the number of batches of tasks divided by the total number of training batches. The system can implement this scheme in a manner that ensures that the number of training steps for any task j is equal to U j/U, where U is the total number of lots for all tasks and U j is the total number of lots for task j. This can be done, for example, by randomly permuting an array of U elements, where the array includes U j elements per task j.
In some other implementations, during training, the system determines, for each of a plurality of tasks, a same number of updates to a plurality of parameters of the neural network. That is, the system selects batches such that the neural network's teachings for the task train on the same number of batches regardless of the size of the training data set used for the task. In other words, in the example of fig. 3, when training for 15 iterations, the system trains on 5 batches per task, even though task 3 has a larger training set than task 2 and task 1.
One example of this scheme is referred to as "alternating" training 320. In alternating training, the system alternates between tasks in a fixed repeating order, such that the same number of batches for each task are used for training.
Another example of this scheme is referred to in fig. 3 as "uniform task sampling" training 330. In this training scheme, at each training step, the likelihood that the batch selected for that training step corresponds to any given task is the same for all tasks, i.e., equal to 1 divided by the total number of tasks, T. The system is able to implement this scheme by randomly permuting the array with U elements, where the array includes U/T elements per task.
In some implementations, the training examples for each batch include a plurality of batches of network inputs corresponding to respective different ones of the plurality of modalities. This is referred to in fig. 3 as a "cumulative gradient" scheme 350. That is, as part of training, at each iteration of process 200, the system determines a single update to a plurality of parameters of the neural network using a plurality of batches of network inputs corresponding to respective different ones of the plurality of modalities. As shown in fig. 3, each larger batch includes a plurality of individual batches, one batch from each of the three tasks. Thus, at the end of training, the neural network has been trained on 5 batches from task 1, 5 batches from task 2, and 5 batches from task 3, because there are 5 training iterations performed.
In some implementations, the system uses one or more single-task neural networks to initialize values of parameters of the neural network prior to training the neural network using process 200. For example, the system can initialize a value of a parameter sharing a self-attention block equal to a training value of a corresponding block in a single task neural network that has been trained on one of the tasks.
As described above, after training, the neural network can be deployed in any of a variety of ways.
In some embodiments, after training, the neural network is used only to process inputs of a single particular modality of the plurality of modalities. Thus, in these embodiments, after training the neural network as described above, i.e., by training the neural network to process a respective input sequence corresponding to each of a plurality of modalities, the system is able to generate a new single-modality neural network by modifying the architecture. In particular, the system is able to remove modality-specific network blocks corresponding to respective modalities other than the specific modality. The system also removes task-specific network blocks corresponding to tasks performed on inputs of respective modalities other than the specific modality.
In some of these embodiments, after training, the neural network is configured to perform a plurality of machine learning tasks corresponding to the particular modality. Thus, as part of the new task-specific neural network, the system maintains all task-specific network blocks corresponding to tasks performed on inputs of a particular modality.
In other of these embodiments, after training, the neural network is configured to perform only a single machine learning task corresponding to a particular modality. In these implementations, the system removes all task specific network blocks except for one or more tasks that correspond to a single machine learning task.
In other embodiments, the neural network is still used to handle different inputs of different modalities after training, i.e., the system deploys the neural network as a multi-tasking, multi-modal neural network.
In any of the above embodiments, after the training described above and prior to deploying the neural network, the system is able to fine tune the neural network on 1) additional training data, 2) training data for tasks that use the neural network only after training, or 3) both.
FIG. 4 is a flow chart of an example process 400 for generating a newly input network output using a multi-tasking, multi-modal neural network after training. For convenience, process 400 will be described as being performed by a system of one or more computers located in one or more locations. For example, a suitably programmed neural network system (e.g., the neural network system 100 of fig. 1) can perform the process 400.
The system receives a respective network input corresponding to each of a plurality of modalities (step 402). In some implementations, for a given modality, the system performs all tasks of the modality in parallel for all received inputs. In some other implementations, the system also receives data specifying which task to perform on the network input for each network input.
The system processes each respective network input using the neural network to generate a respective network output for each of the network inputs.
Specifically, for each network input, the system determines a plurality of segments of the network input (step 404) and processes the segments using the corresponding modality of the word analyzer as described above to generate an input sequence (step 406). The system then processes the input sequence using the neural network to generate a respective predicted network output for at least one of the one or more machine learning tasks corresponding to the modality corresponding to the network input (step 408). That is, if the system receives data specifying which task to perform on the network input for each network input, the system only generates the output of the specified task(s). If not, for each network input, the system generates an output for each task corresponding to the modality of the network input.
An "embedding" as used in this specification is a vector of values having a predetermined dimension (e.g., having a predetermined number of values), such as floating point or other types of values.
As described above, a self-attention block is a neural network layer that includes an attention mechanism that operates on a self-attention block input (or an input derived from a layer input) to generate a self-attention block output. The self-attention mechanism may be causally masked such that any given position in the input sequence does not participate (e.g., uses data from) any position after the given position in the input sequence. There are many different possible attention mechanisms. Some examples of self-attention layers including attention mechanisms are described in the following: vaswani et al, "Attention is all you need (all you need is attention )",31st Conference on Neural Information Processing Systems(NIPS2017),Long Beach,CA,USA;Colin Raffel,Noam Shapeer,Adam Roberts,Katherine Lee,Sharan Narang,Michael Matena,Yanqi Zhou,Wei Li and Peter J Liu, exploring THE LIMITS of TRANSFER LEARNING WITH A unified text-to-text transducer (unified text-to-text transducer is used to explore transfer learning constraints ),arXiv preprint arXiv:1910.10683,2019;Daniel Adifingana,Minh-Thang Luong,David R.So,Jamie Hall,Noah Fieedel,Romal Thoppilan,Zi Yang,Apoorv Kulshrehtha,Gaurav Nemade、Yifeng Lu and Quoc v. Le, towards a human-like open-domain chatbot (open-domain-oriented chat robot), coRR, abs/2001.09977,2020; and Tom B Brown,Benjamin Mann,Nick Ryder,Melanie Subbiah,Jared Kaplan,Prafula Dhariwal,Arvind Neelkantan,Pranav Shyam,Grish Sastry,Amanda Askell et al, language models are few-shot learners (language model is small sample learner) arXivpreprint arXiv:2005.14165,2020.
Typically, the attention mechanism maps a set of query and key-value pairs to an output, where the query, key, and value are vectors. The output is calculated as a weighted sum of values, where the weight assigned to each value is calculated by a compatibility function (e.g., dot product or scaled dot product) of the query with the corresponding key.
In general, the self-attention mechanism is configured to associate different positions in the same sequence to determine as output a transformed version of the sequence. For example, the attention layer input may include a vector of each element of the input sequence. These vectors provide input to the self-attention mechanism and are used by the self-attention mechanism to determine a new representation of the same sequence of attention layer outputs, which similarly includes a vector for each element of the input sequence. The output of the self-attention mechanism may be used as an attention layer output or may be processed by one or more of a feed forward layer, a jump connection, or a normalization operation to provide an attention layer output.
In some implementations, the attention mechanism is configured to apply each of a query transformation, e.g., defined by matrix W Q, a key transformation, e.g., defined by matrix W K, and a value transformation, e.g., defined by matrix W V, to an attention layer input as input data X to the attention layer to derive a query matrix q=xw Q including a respective query for each vector in the input sequence, a key matrix k=xw K including a respective key for each vector in the input sequence, and a value matrix v=xw V including a respective value for each vector in the input sequence, which are used to determine an output attention sequence. For example, the attention mechanism may be a dot product attention mechanism: a dot product attention mechanism is applied by applying each query vector to each key vector to determine a respective weight for each value vector, and then combining the value vectors using the respective weights to determine the self-attention layer output of each element of the input sequence. The self-attention layer output may be scaled by a scaling factor (e.g., by the square root of the dimensions of the query and key) to achieve scaled dot product attention. Thus, for example, the output of the attention mechanism may be determined asV, where d is the dimension of the key (sum value) vector. In another embodiment, the attention mechanism includes an "additive attention" mechanism that uses a feed-forward network with hidden layers to calculate the compatibility function. The output of the attention mechanism may be further processed by one or more fully connected feedforward neural network layers.
Attention mechanisms may implement multi-headed attention, i.e. multiple different attention mechanisms may be applied in parallel. The outputs of these can then be combined (e.g., concatenated) with the learned linear transformation applied to reduce to the original dimension if necessary.
The term "configured" is used herein in relation to systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions, the system has installed thereon software, firmware, hardware, or a combination thereof, which in operation causes the system to perform the operations or actions. One or more computer programs for being configured to perform a particular operation or action means that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operation or action.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions can be encoded on a manually generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and includes all types of apparatus, devices, and machines for processing data, including for example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be or further comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus can optionally include code that creates an execution environment for the computer program, such as code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software application, app, module, software module, script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, such as files that store one or more modules, sub-programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured in any particular way, or structured at all, and it can be stored on a storage device in one or more locations. Thus, for example, an index database may include multiple data sets, each of which may be organized and accessed differently.
Similarly, in this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or in combination with, special purpose logic circuitry (e.g., an FPGA or ASIC) and one or more programmed computers.
A computer suitable for executing a computer program can be based on a general-purpose or special-purpose microprocessor or both, or any other kind of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such a device. In addition, the computer can be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices can also be used to provide interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer is able to interact with the user by sending and receiving documents to and from a device used by the user; for example, by sending a web page to a web browser on a user device in response to a request received from the web browser. Further, the computer can interact with the user by sending a text message or other form of message to a personal device (e.g., a smart phone that is running a messaging application) and returning a response message from the user.
The data processing apparatus for implementing the machine learning model can also include, for example, dedicated hardware accelerator units for handling common and computationally intensive parts (i.e., reasoning, workload) of machine learning training or production.
The machine learning model can be implemented and deployed using a machine learning framework (e.g., tensorFlow framework, microsoft Cognitive Toolkit framework, APACHE SINGA framework, or Apache MXNet framework).
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., an application through which a client computer, web browser, or user having a graphical user interface can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the internet.
The computing system can include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data, such as HTML pages, to the user device, for example, for the purpose of displaying data to and receiving user input from a user interacting with the device acting as a client. Data generated at the user device, such as the results of a user interaction, can be received at the server from the device.
While this specification contains many specifics, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are illustrated in the accompanying drawings and described in the claims in a particular order, this should not be construed as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying drawings do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (25)
1. A method of training a neural network to process a respective input sequence corresponding to each of a plurality of modalities and generate a respective network output of the input sequence,
Wherein the neural network is configured to perform, for each of the plurality of modalities, one or more machine learning tasks corresponding to the modality,
The training comprises:
acquiring a network input corresponding to a specific modality and comprising a plurality of elements;
Determining a plurality of tiles of the network input, wherein each tile includes a different subset of the elements of the network input;
Processing the plurality of tiles to generate an input sequence comprising a respective input element at each of a plurality of input locations, wherein some or all of the input elements correspond to respective different tiles;
Processing the input sequence using the neural network to generate a respective predicted network output for at least one of the one or more machine learning tasks corresponding to the particular modality,
Wherein the neural network comprises one or more self-attentive neural network layers, each self-attentive neural network layer being configured to apply a self-attentive mechanism to the input sequence or an intermediate representation of the input sequence, and
Wherein at least a subset of the self-attention neural network layer is shared across the plurality of modalities; and
And determining an update to a plurality of parameters of the neural network based on the errors in the corresponding predicted network outputs.
2. The method of claim 1, wherein the plurality of modalities includes one or more of an image modality, a video modality, or an audio modality.
3. The method of any of claims 1 or 2, wherein processing the input sequence using the neural network to generate a respective predicted network output for at least one of the one or more machine learning tasks corresponding to the particular modality comprises:
processing the input sequence using one or more modality-specific network blocks corresponding to the specific modality to generate a first intermediate representation of the input sequence;
Processing the first intermediate representation using one or more shared network blocks to generate a second intermediate representation of the input sequence; and
For each of the at least one machine learning task, processing the second intermediate representation using a respective task specific network block to generate the respective predicted network output for the machine learning task.
4. A method according to any of claims 1-3, wherein determining an update to a plurality of parameters of the neural network comprises:
identifying a training hyper-parameter set corresponding to the particular modality or the at least one machine learning task; and
The update is determined from the identified training hyper-parameter set.
5. The method of any of claims 1-4, wherein at least 50%, at least 60%, at least 70%, at least 80%, or at least 90% of the plurality of parameters of the neural network are shared across the plurality of modalities.
6. The method of any of claims 1-5, the training further comprising using each of a plurality of batches of network inputs to determine a respective update to the plurality of parameters of the neural network,
Wherein, for each batch of network inputs, each network input in the batch corresponds to a same modality of the plurality of modalities.
7. The method of claim 6, wherein, for each batch of network inputs, each network input in the batch corresponds to a same machine learning task of the one or more machine learning tasks corresponding to the modality of the batch.
8. The method of any of claims 6 or 7, wherein the training comprises determining, for each of the plurality of modalities, a same number of updates to the plurality of parameters of the neural network.
9. The method of any of claims 6-9, wherein the training comprises: for each of the plurality of modalities, for each machine learning task corresponding to the modality, a same number of updates to the plurality of parameters of the neural network are determined.
10. The method of any one of claims 6 or 7, wherein:
The training further includes generating, for each machine learning task corresponding to each of the plurality of modalities, one or more batches of network input from a training dataset corresponding to the machine learning task; and
During training, the network inputs of a batch are sampled according to the size of the corresponding training data set from which the batch was generated.
11. The method of any of claims 1-10, wherein the training further comprises: a single update to the plurality of parameters of the neural network is determined using a plurality of batches of network input corresponding to respective different ones of the plurality of modalities.
12. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the method of any one of claims 1-11.
13. One or more non-transitory computer storage media storing instructions which, when executed by one or more computers, cause the one or more computers to perform the method of any one of claims 1-11.
14. A system comprising a first neural network configured to process a respective input sequence corresponding to each of a plurality of modalities and to generate a respective network output of the input sequence, the first neural network having been trained using the method of any of claims 1-11.
15. A system comprising a first neural network configured to process an input sequence corresponding to a single particular modality of the plurality of modalities and to generate a respective output of the input sequence,
The first neural network has been trained by performing operations comprising:
training a second neural network to process a respective input sequence corresponding to each of the plurality of modalities using the method of any of claims 1-11; and
The architecture of the second neural network is modified to generate the first neural network.
16. The system of claim 15, wherein modifying the architecture of the second neural network to generate the first neural network comprises removing one or more modality-specific network blocks corresponding to respective modalities different from the specific modality.
17. The system of any of claims 15 or 16, wherein the first neural network is configured to perform a plurality of machine learning tasks corresponding to the particular modality.
18. The system of any of claims 15 or 16, wherein the first neural network is configured to perform a single machine learning task corresponding to the particular modality.
19. The system of any of claims 15-18, wherein modifying the architecture of the second neural network to generate the first neural network includes removing one or more task-specific network blocks corresponding to respective tasks for which the first neural network is not configured.
20. The system of any of claims 15-19, wherein modifying the architecture of the second neural network to generate the first neural network comprises:
modifying the architecture of the second neural network to generate an initial first neural network; and
A plurality of parameters of the initial first neural network are trimmed to generate the first neural network.
21. A method comprising the operation of the system of any one of claims 14-20.
22. One or more non-transitory computer storage media storing instructions which, when executed by one or more computers, cause the one or more computers to perform the operations of the system of any one of claims 14-20.
23. A method performed by one or more computers, the method comprising:
Receiving a respective network input corresponding to each of a plurality of modalities, wherein each network input includes a respective plurality of elements; and
Processing the respective network inputs using a neural network to generate a respective network output for each of the network inputs,
The processing includes, for each network input:
Determining a plurality of tiles of the network input, wherein each tile includes a different subset of the elements of the network input;
Processing the plurality of tiles to generate an input sequence comprising a respective input element at each of a plurality of input locations, wherein some or all of the input elements correspond to respective different tiles; and
Processing the input sequence using the neural network to generate a respective predicted network output for at least one of one or more machine learning tasks corresponding to a modality corresponding to the network input,
Wherein the neural network comprises one or more self-attention neural network layers, each self-attention neural network layer configured to apply a self-attention mechanism to the input sequence or an intermediate representation of the input sequence; and
Wherein at least a subset of the self-attention neural network layer is shared across the plurality of modalities.
24. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the method of claim 23.
25. One or more non-transitory computer storage media storing instructions which, when executed by one or more computers, cause the one or more computers to perform the method of claim 23.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US63/252,593 | 2021-10-05 |
Publications (1)
Publication Number | Publication Date |
---|---|
CN118043818A true CN118043818A (en) | 2024-05-14 |
Family
ID=
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110268422B (en) | Device layout optimization with reinforcement learning | |
US11568207B2 (en) | Learning observation representations by predicting the future in latent space | |
EP3602409B1 (en) | Selecting actions using multi-modal inputs | |
US11669744B2 (en) | Regularized neural network architecture search | |
CN111279362B (en) | Capsule neural network | |
EP3574454B1 (en) | Learning neural network structure | |
CN110799992B (en) | Use of simulation and domain adaptation for robot control | |
WO2019155052A1 (en) | Generative neural network systems for generating instruction sequences to control an agent performing a task | |
EP3593293B1 (en) | Imagination-based agent neural networks | |
CN110114784B (en) | Recursive environment predictor and method therefor | |
CN110476173B (en) | Hierarchical device placement with reinforcement learning | |
US11144782B2 (en) | Generating video frames using neural networks | |
WO2018211143A1 (en) | Neural network system | |
US11636347B2 (en) | Action selection using interaction history graphs | |
CN110663049A (en) | Neural network optimizer search | |
CN113348472A (en) | Convolutional neural network with soft kernel selection | |
EP3446258B1 (en) | Model-free control for reinforcement learning agents | |
CN117121016A (en) | Granular neural network architecture search on low-level primitives | |
US20230107409A1 (en) | Ensembling mixture-of-experts neural networks | |
CN118043818A (en) | Self-attention-based neural network for processing network metrics from multiple modalities | |
WO2023059737A1 (en) | Self-attention based neural networks for processing network inputs from multiple modalities | |
JP7483751B2 (en) | Training machine learning models using unsupervised data augmentation | |
EP3948682A1 (en) | Connection weight learning for guided architecture evolution |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication |