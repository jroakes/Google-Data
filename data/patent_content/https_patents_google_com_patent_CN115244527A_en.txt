CN115244527A - Cross example SOFTMAX and/or cross example negative mining - Google Patents
Cross example SOFTMAX and/or cross example negative mining Download PDFInfo
- Publication number
- CN115244527A CN115244527A CN202080098439.6A CN202080098439A CN115244527A CN 115244527 A CN115244527 A CN 115244527A CN 202080098439 A CN202080098439 A CN 202080098439A CN 115244527 A CN115244527 A CN 115244527A
- Authority
- CN
- China
- Prior art keywords
- query
- vector
- electronic resource
- generated
- queries
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/90335—Query processing
Abstract
Techniques are disclosed that enable learning an embedding space using a cross example, where a distance between a query and an electronic resource in the embedding space provides an indication of the relevance of the electronic resource to the query. Various implementations include learning the embedding space using a cross example Softmax technique. Various implementations include learning an embedding space using cross example negative mining. Techniques are disclosed that enable determination of additions or alternatives to electronic resources for a query based on comparing a query vector (e.g., an embedded spatial representation of the query) to a set of pre-stored candidate electronic resource vectors (e.g., an embedded spatial representation of the set of candidate electronic resources).
Description
Background
The information retrieval system can rely on neural network models to learn the embedding space, where the distance can encode the correlation between a given query and a candidate response to the given query. These embedding spaces can be trained using conventional methods (e.g., sampled Softmax, random negative mining, etc.) by given the correlation between the relative rankings of query optimization candidate responses.
Disclosure of Invention
Implementations described herein are directed to learning an embedding space using a crossover example such that distances in the learned embedding space are globally calibrated across queries. Distances generated using conventional techniques may not be comparable across queries, which may result in difficulty in determining, for example, how relevant a document is to a query based on the distance. For example, in a conventionally learned embedding space (e.g., an embedding space learned using a sampled Softmax method, a random negative mining method, etc.), a first distance between a first query and documents relevant to the first query may be greater than a distance between a second query and documents not relevant to the second query. In other words, using conventional techniques, the distance between a first query and a corresponding relevant document cannot be compared to the distance between a second query and a corresponding relevant document and/or irrelevant document(s). Further, using conventional techniques, it may be possible to ascertain that a given document is closest in distance to the query, but it may not be possible to ascertain that the given document is truly relevant to the query (e.g., is a true close match or only a surface match).
In contrast, implementations described herein are directed to learning the embedding space(s) using a crossover example such that distances in the learned embedding space are globally calibrated. For example, the distance between a first query and a first corresponding document should not be greater than the distance between a second query and document(s) unrelated to the second query. In other words, the distance between the first query and the one or more candidate documents can be compared to the distance between the second query and the one or more candidate documents. Furthermore, the distance between a given query and candidate documents is meaningful and reflects the true relevance of those candidate documents to the given query.
In some implementations, one or more neural network models can be trained using the intersection examples to learn the embedding space. For example, an input model can be trained to generate a query vector by processing a query, where the query vector is an embedded spatial representation of the query. Additionally or alternatively, the resource model can be trained to generate an electronic resource vector by processing an electronic resource (e.g., an image, a document, a web page, a bounding box, and/or additional resource (s)), where the electronic resource vector is an embedded spatial representation of the electronic resource.
In some implementations, the batch of training data can include a real value (ground route) query/electron resource pair, where each query in the batch of training data has a single corresponding electron resource, and where each electron resource has only a corresponding single query. In some implementations, each query can be processed using an input model to generate a corresponding query vector. Similarly, in some implementations, each electronic resource can be processed using a resource model to generate a corresponding electronic resource vector. For query/electronic resource real-valued pairs, a relevance score (e.g., a distance in embedding space) can be generated for each query/electronic resource real-valued pair based on the corresponding query vector and the corresponding electronic resource vector. For example, a relevance score can be generated by determining a dot product between a corresponding query vector and a corresponding electronic resource vector. Additionally or alternatively, a negative relevance score (e.g., distance in embedding space) can be determined for each given query and each electronic resource that is not paired with the true value of the given query based on the corresponding query vector and the corresponding electronic resource vector. For example, a negative relevance score can be generated by determining a dot-product between a corresponding query vector and a corresponding electronic resource vector that is not paired with the true value of a given query. In some implementations, the pairwise similarity matrix can be generated based on queries in the batch of training data and electronic resources in the batch of training data.
In some implementations, a query penalty can be determined for each query in a batch of training data. For example, each query loss can be based on one or more of the relevance score of the corresponding query and the negative relevance score of at least one additional query in the batch of training data. In other words, each query penalty can be based on the relevance score of the query and at least one negative relevance score of one or more cross-over examples (i.e., one or more additional queries). In some implementations, a training batch loss can be determined based on a query loss for each query in the batch. This training batch loss can then be used to update (e.g., back-propagate) one or more portions of the input model, one or more portions of the resource model, and/or one or more portions of the additional model(s).
In some implementations, each query penalty can be generated using a cross-example Softmax method, where each query penalty is based on each of the relevance scores corresponding to the queries and the generated negative query penalties. In other words, the query loss is based on each of the relevance score for the corresponding query, the negative relevance score generated for the corresponding query, and the negative relevance score generated for each additional query in the batch. In contrast, query loss can be determined using a conventional Softmax method (e.g., conventional sampling Softmax) based on the relevance scores generated for the corresponding queries and the negative relevance scores generated for the corresponding queries, without being based on the negative relevance scores generated for the additional queries in the batch of training data.
Additionally or alternatively, each query penalty can be generated using a cross-example negative mining method, where each query penalty is based on a subset of the correlation scores generated for the corresponding query and the negative correlation scores generated for the batch of training data (e.g., a subset of the negative correlation scores generated for the query and/or additional queries). In some implementations, the subset of negative relevance scores can be selected based on whether the negative relevance scores satisfy one or more conditions. For example, a subset of negative relevance scores can be selected to include negative scores having the highest values (e.g., the first k negative relevance scores having the k highest values). In contrast, query losses generated using a random negative mining method exclude negative relevance scores generated for additional queries based on the relevance scores generated for the corresponding queries and a subset of the negative relevance scores generated for the corresponding queries.
In some implementations, the trained input model can be used to determine the corresponding electronic resource. For example, a query vector can be generated by processing a query using a trained input model. In some implementations, the query vector can be compared to pre-stored candidate electronic resource vectors, where each candidate electronic resource vector was previously generated by processing a candidate electronic resource using a resource model. In some implementations, the input model and the resource model can be trained using the same batch loss(s). A candidate electron resource vector can be selected based on the comparison. For example, the candidate electronic resource vector can be selected based on a minimum distance between the candidate electronic resource vector and the query vector. Additionally or alternatively, an electronic resource corresponding to the query can be determined based on the selected candidate electronic resource vector. In some implementations, the computing system can perform the action(s) based on the determined electronic resource.
For example, a computing system can be used to determine an image (i.e., an electronic resource) in response to a natural language query (i.e., a query). The input model is capable of processing a natural language query to generate a query vector. A pre-stored candidate image vector can be generated by processing the candidate image using the resource model. In the illustrated example, the input model used to process the natural language query can be of a different model type and/or have a different model structure than the resource model used to process the candidate image. In some implementations, the input model and the resource model can be trained using the same generated training loss(s). In some of those implementations, the input model can be trained simultaneously with the resource model. The candidate image vector can be selected based on a distance between the selected candidate image vector and the query vector. In some implementations, an image corresponding to the natural language query can be determined based on the selected pre-stored candidate image vector. In some implementations, the computing system can perform the action(s) based on the determined image, such as displaying the image on a screen of the computing system.
As an additional example, a computing system can be used to determine a bounding box (i.e., an electronic resource) for an object captured in an image (i.e., a query). The image can be processed using the input model to generate an image vector. A pre-stored candidate bounding box vector can be generated by processing the candidate bounding boxes using a response model. The image vector can be compared to each of the pre-stored candidate bounding box vectors, and the candidate bounding box vectors can be selected based on the comparison. For example, a candidate bounding box vector having the shortest distance to the image vector can be selected. A bounding box can be determined based on the candidate bounding box vectors. In some implementations, the computing system can perform the action(s) based on the determined bounding box, such as displaying the bounding box around an object in the image, identifying the object captured in the bounding box, and so on.
Thus, various implementations set forth techniques for learning an embedding space using a cross example (e.g., using negative relevance score(s) generated for additional queries in a batch of training data). In contrast, conventional techniques can learn the embedding space based on the negative relevance score(s) generated for a given query, excluding additional negative relevance score(s) generated for additional queries. In some cases, the electronic resource that most closely corresponds to the query (e.g., the electronic resource that is closest to the query in the embedding space learned using conventional techniques) is not particularly relevant to the query. Computing resources (e.g., processor cycles, memory, battery power, etc.) can be conserved by providing users with electronic resources that only respond to queries based on the distance between the queries and the electronic resources in the embedding space learned using the cross-over example.
The above description is provided merely as a summary of some implementations disclosed herein. These and other implementations of the techniques are disclosed in additional detail below.
It should be understood that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
Drawings
FIG. 1 illustrates an example environment in which various implementations disclosed herein may be implemented.
Fig. 2A illustrates an example of generating a relevance score in accordance with various implementations disclosed herein.
Fig. 2B illustrates an example of generating a negative relevance score in accordance with various implementations disclosed herein.
Fig. 2C illustrates an example of generating a lot loss based on the correlation score(s) and the negative correlation score(s) in accordance with various implementations disclosed herein.
Fig. 3A and 3B illustrate example embedding spaces generated using conventional methods.
Fig. 3C and 3D illustrate example embedding spaces generated in accordance with implementations disclosed herein.
FIG. 4A illustrates an example pairwise similarity matrix generated based on a batch of training data.
Fig. 4B illustrates an example of true value pairs for a batch of training data.
Fig. 4C illustrates an example of generating a query penalty using a conventional Softmax method.
FIG. 4D illustrates an example of generating a query penalty using a conventional random negative mining method.
Fig. 4E illustrates an example of using a cross-example Softmax method to generate a query penalty in accordance with various implementations disclosed herein.
Fig. 4F illustrates an example of generating a query penalty using a cross-example negative mining method in accordance with various implementations disclosed herein.
FIG. 5 is a flow diagram illustrating an example process of training an input model and/or a response model in accordance with various implementations disclosed herein.
Fig. 6 is a flow diagram illustrating an example process for generating a query penalty using a cross example Softmax method in accordance with various implementations disclosed herein.
Fig. 7 is a flow diagram illustrating an example process for generating a query penalty using a cross-example negative mining method in accordance with various implementations disclosed herein.
Fig. 8 is a flow diagram illustrating an example process for determining electronic resources for a query in accordance with various implementations disclosed herein.
Fig. 9 illustrates an example architecture of a computing device.
Detailed Description
Modern image retrieval systems increasingly can rely on deep neural networks to learn the embedding space, where distance encodes the correlation between a given query and an image. These embedding spaces can conventionally be trained by optimizing the relative ranking of documents given a query. However, this can present challenges because the resulting absolute distance may not be comparable across queries, thereby making it difficult to determine whether a document is relevant to a query based on the distance of the document and the query alone. The technology disclosed herein addresses the cross example Softmax method for addressing the above challenges. In some implementations, the proposed crossover example Softmax penalty encourages all queries to be closer to their matching images, rather than all queries being closer to all unrelated images, at each iteration. This can result in globally more calibrated similarity measures and can make the distance more interpretable as a measure of correlation. Additional or alternative techniques are directed to a cross-example negative mining approach in which each query document pair can be compared to the "hardest" negative comparison across the entire batch. In some implementations, it can be shown that the proposed method can effectively improve global calibration and/or can improve retrieval performance.
The goal of large-scale information retrieval can be to efficiently find relevant documents for a given query among potentially billions of candidates. A typical example is image search: a text query is given to find relevant images. One conventional implementation can be to learn a real-valued scoring function to rank the set of candidate images that each query may involve. Since using large neural networks to compute the relevance of each query image pair can be prohibitively expensive, recent deep learning systems can solve this task by embedding the query and image together in a shared vector space. By encoding semantic correlations as distances in vector space, these systems are able to model complex semantic relationships while still allowing efficient retrieval using near-nearest neighbor searches through large databases of images.
Conventional depth metric learning methods can rely on pair-wise or triplet comparisons to place queries close to relevant images and far away from irrelevant images in vector space. However, motivated by the success of the large scale classification problem like ImageNet, many depth metric learning models can now use Softmax with cross entropy. Note that similar to information retrieval systems, models like ResNet or VGG can maximize the dot product between the image query representation and one relevant tag from a fixed set of 1,000 tags. The label can be encoded in the weight matrix of the last layer. In contrast to ImageNet training, it can be nearly infeasible for an information retrieval system to compute Softmax likelihoods for all documents, which can potentially be billions, and Softmax can instead be computed over a small random subset of documents. This is commonly referred to as regular sampling Softmax. In practice, this can be achieved by embedding random batches of pairs of queries/documents known to be related to each other into their vector representations. Other documents in the batch can be efficiently reused as negative examples for a given query. Since most of these random documents are unlikely to be information-rich for each query, random negative mining can be used, enabling the use of the most information-rich negative documents, e.g., those with the highest similarity scores.
Although methods based on sampling Softmax and triplets have been shown to be very effective in learning to capture representations of relative semantic similarity, key challenges still remain. In particular, these methods are invariant to absolute distance in vector space, since they only optimize the relative distance between a query and its matching documents, as compared to irrelevant documents. Thus, distances are not comparable across queries and cannot be interpreted as an absolute measure of relevance. As illustrated in FIGS. 3A and 3B, the related image 306 for query A302 is farther to its query than the unrelated images 316 and 318 to query B312. This lack of calibration can be a problem for retrieval systems that typically employ a global confidence threshold to determine which distance results are considered relevant.
In some implementations, the cross example Softmax approach can address this challenge by directly optimizing retrieval and similarity score calibration so that query/document similarity scores can be compared across multiple queries. In some implementations, the cross example Softmax method extends Softmax by introducing cross example comparisons. In some implementations, rather than maximizing the ratio between the distance of the query to its matching document compared to the distance of the query to all other documents, the cross example Softmax can maximize the ratio between the distance of the query to its matching document and the distances of all query/document pairs that are not related to each other. This can encourage any matching pair to be closer in vector space than any non-matching pair. Fig. 3C and 3D illustrate the effect of crossing example Softmax and how it results in a calibrated distance score.
In some implementations, the proposed method can further allow extending the concept of random negative mining to cross example negative mining. Rather than mining the most information-rich negative documents only for a given query, non-matching pairs can be selected with the highest similarity scores even though they are for different queries.
Metric learning using depth models has been applied to many applications, especially applications where the output space is very large. Early methods were based on Siamese networks with contrast loss on pairwise data or relative triplet similarity comparisons. Inspired by the success of the large-scale classification task on ImageNet, the latest model can be trained using a sample Softmax penalty with cross entropy. Recently, some work has proposed modifications to the sampling Softmax loss by normalizing, adding margins, and adjusting the scaled temperature. These methods focus on optimizing the relative ordering of tags for a given anchor query. In contrast, the cross example Softmax method disclosed herein is able to optimize the score of the correct tags for each input query, relative to the entire distribution of all possible negative query/tag pairs in the entire batch, even across queries.
In a large output space setting, most documents may not be relevant for any given query, and thus including them in the loss function may not be information rich for optimization. To address this challenge, several efforts have been proposed to mine the most difficult and most informative negative tags. However, as an approximation of Softmax per query, these methods can perform negative mining with respect to a single query at a time. The cross-example negative mining method disclosed herein can use negative example mining across examples, where the system can mine the globally most difficult negative queries/document comparisons in a batch.
Score calibration is always of interest to ensure that the scores are consistently normalized or interpretable. One common approach is to interpret the output of the Softmax function applied to the model logit as a probability. Although the output of Softmax is technically a probability distribution in the sense that it is normalized, calculating the probability for any tag also requires comparison with all other tags. In a large output space setting, this is usually not possible because the probability space is too large to compute. To address this challenge, the techniques disclosed herein address a new loss function that explicitly encourages calibration of the underlying logit. This can be done during training rather than as a post-recognition calibration step. This can allow tag scores to be compared across queries without having to compute scores for all other tags.
Consider a multiclass classification setting with a sample X ∈ X of an instance and its associated label Y ∈ Y, where | Y | = K. In some implementations, the goal can be to learn a scoring function that can rank tags for each instance according to their relevance
In the text-to-image retrieval application example, x i Is a text query and y i Is its corresponding relevant image. In some implementations, to score relevance between queries and images, a text encoder that projects text and images into a shared d-dimensional embedding space can be learned
In a standard multi-class classification setting, the model can be optimized using Softmax cross entropy loss over the entire label space, i.e., the scores of the correct label can be compared to the scores of all other labels. Ideally, the system is also able to compare the score of a matching document to all other documents in the database in a retrieval setting. However, since the number of documents can be billions, this can become prohibitive. To address this challenge, softmax cross-entropy losses are typically computed only on a random subset of tags, which is commonly referred to as sampled Softmax. Specifically, consider a batch B that includes N corresponding query/document pairs B uniformly sampled from the epochs of batch B t ＝{(x 1 ，y 1 )，...，(x N ，y N ) A small batch of. Given the vector representations of all text queries and images from the small batch, the system can compute all possible pairs
In cases where the number of overall documents is very large and the random subset of documents within each small batch is relatively small, it can generally be assumed that for a given query x i Within a batch, only its corresponding document y i Are relevant. In some implementations, it can be assumed that all other documents y sampled within the same batch j J ≠ i is independent of the query. The matching relationship between the query and the documents within the batch is illustrated in FIG. 4B, where a 1 indicates a matching relationship (e.g., a true value relationship) and a 0 indicates a mismatch. In the form of a sheet, the sheet is,
In some implementations, the sample Softmax cross entropy can be defined as the relevance score s of the document with which the query matches i，i And relevance scores of the query to all unmatched documents
fig. 4C illustrates per example loss for the second query. Matched pair s 2，2 Are highlighted in light grey and
In the sample Softmax method, the relevant documents are compared to only a small subset of the random documents. As a result, most of these documents will be query independent and therefore fail to provide information that guides optimization. As a means to overcome this, random negative mining selects the hardest negative documents only for each query within a randomly drawn subset of the documents of the batch. Formally, order
Thereby, the modified loss can be defined for each query only on the hardest documents as
Fig. 4D shows this scenario. The negative case set now includes only the hardest comparisons for a given query. In the figure, dark gray negative scores are
From equations (2) and (3) and the illustrations in fig. 4C and 4D, it becomes clear that sampling Softmax captures only the distance of the document with respect to a single given query only. Since the loss terms are invariant to absolute distance and distances are not compared across queries, distances in the learned vector space are not comparable across queries.
To encourage global calibration to enable the use of distance as an absolute measure of correlation, the techniques disclosed herein are directed to extend the cross example Softmax of Softmax by introducing a cross example negative example. The proposed loss can encourage all queries to be closer to their matching documents, rather than all queries to be closer to all unrelated documents.
In some implementations of the above-described embodiments,
in some implementations, using equation (4), a cross example Softmax cross entropy can be defined as
FIG. 4E illustrates that for the crossover example Softmax, the penalty for a single query can include the penalty from
In some implementations, random negative mining can be extended using a cross example negative example to mine the hardest negative comparisons across an entire batch. Similar to the above formula, let
this crossover example negative digging loss is illustrated in fig. 4F. In the example illustrated in FIG. 4F, the negative score for each query is mined from the entire batch. This means that the mined set of scores may contain all negative scores from some queries, such as line 1 in the graph, and no negative scores from other queries, such as line 3 in the graph.
Turning now to the figures, fig. 1 illustrates a block diagram of an example environment 100 in which implementations disclosed herein may be implemented. The example environment 100 includes a computing system 102, which computing system 102 can include a query engine 106, a resource engine 108, and/or additional engine(s) (not depicted). Additionally or alternatively, computing system 102 may be associated with one or more user interface input/output devices 104. Further, computing system 102 can be associated with input model 110, resource model 112, training engine 114, one or more batches of training data 116, resource vector 118, electronic resource 120, and/or one or more additional components (not depicted).
In some implementations, computing system 102 may include user interface input/output devices 104, which may include, for example, a physical keyboard, a touch screen (e.g., implementing a virtual keyboard or other text input mechanism), a microphone, a camera, a display screen, and/or a speaker. The user interface input/output devices may be integrated with one or more of the computing systems 102 of the user. For example, a user's mobile phone may include a user interface input output device; the standalone digital assistant hardware device may include user interface input/output devices; the first computing device may include user interface input device(s) and the separate computing device may include user interface output device(s); and the like. In some implementations, all or aspects of the computing system 102 may be implemented on a computing system that also includes user interface input/output devices. In some implementations, computing system 202 may include an automated assistant (not depicted), and all or aspects of the automated assistant may be implemented on computing device(s) that are separate and apart from the client device containing the user interface input/output devices (e.g., all or aspects may be implemented "in the cloud"). In some of those implementations, those aspects of the automated assistant can communicate with the computing device via one or more networks, such as a Local Area Network (LAN) and/or a Wide Area Network (WAN) (e.g., the internet).
Some non-limiting examples of computing system 102 include one or more of the following: a desktop computing device, a laptop computing device, a standalone hardware device dedicated at least in part to an automated assistant, a tablet computing device, a mobile phone computing device, a computing device of a vehicle (e.g., an in-vehicle communication system and an in-vehicle entertainment system, an in-vehicle navigation system), or a wearable apparatus comprising a user of a computing device (e.g., a watch of a user having a computing device, glasses of a user having a computing device, a virtual or augmented reality computing device). Additional and/or alternative computing systems may be provided. Computing system 102 may include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. The operations performed by computing system 102 may be distributed across multiple computing devices. For example, computing programs running on one or more computers in one or more locations can be coupled to each other through a network.
As illustrated in FIG. 1, a training engine 114 can be used to train the input model 110 and/or the resource model 112. In some implementations, the training engine 114 can process one or more batches 116 of training data to generate a batch penalty, where the batch penalty can be used to update one or more portions of the input model 110 and/or the resource model 112 (e.g., by back-propagation). For example, the batch 116 of training data can include a set of queries, a set of electronic resources, and true value pairs between the queries and the electronic resources. In some implementations, each of the pairs of real values is a pair of real values of a corresponding one of the electronic resources and a corresponding one of the queries. In some of those implementations, each electronic resource has only a corresponding single one of the pairs of real values.
In some implementations, the training engine 114 can generate a set of query vectors by processing each query in the set of queries of the batch 116 of training data using the input model 110. Each query vector can be an embedded spatial representation of the corresponding query. Additionally or alternatively, the training engine 114 can generate the set of electronic resource vectors by processing each electronic resource in the set of electronic resources of the batch 116 of training data using the resource model 112. Each electronic resource vector can be an embedded spatial representation of the corresponding electronic resource. In some implementations, the embedding space of the query vector is a shared embedding space with the electronic resource vector. Additionally or alternatively, the training engine 114 can determine a correlation between each query vector and each electronic resource vector corresponding to the batch 116 of training data. In some implementations, a relevance score can be determined for each truth-valued query/electronic resource pairing. For example, the training engine 114 can determine a relevance score (e.g., distance in embedding space) between a corresponding query vector and a corresponding electronic resource vector of a true value pair by determining a dot product between the corresponding query vector and the corresponding electronic resource vector. Additionally or alternatively, the training engine 114 can determine a negative relevance score for each query vector and for each electronic resource vector other than the corresponding electronic resource vector having a true value relationship to the query. In other words, in addition to true value pairs, a negative relevance score can be generated for each query and electronic resource pair. In some implementations, in addition to the true-value electronic resource pairing for the query, the training engine 114 can determine each negative relevance score (e.g., distance in embedding space) between the query vector and the corresponding electronic resource vector by determining a dot product between the corresponding query vector and the corresponding electronic resource vector.
The training engine 114 can determine a query loss corresponding to a query in the query set of the batch 116 of training data. In some implementations, the query loss can be based on a relevance score generated for the corresponding query and at least one negative relevance score generated for at least one additional query in the batch 116 of training data. In other words, the cross example can be used to generate a query loss (e.g., at least one negative relevance score generated for at least one additional query in the batch 116 of training data). In some implementations, the training engine 114 can use a cross-example Softmax method to generate query penalties, where query penalties are generated based on: each of (1) the relevance scores for the corresponding query, (2) the negative relevance scores generated for the corresponding query, and (3) the negative relevance scores generated for each additional query in the batch of training data.
Additionally or alternatively, the training engine 114 can use a cross-example negative mining approach to generate query loss. In some implementations, the training engine 114 can select a subset of negative relevance scores for use in generating query losses using a cross-example negative mining method, where the training engine 114 can select one or more negative relevance scores that satisfy one or more conditions. For example, the training engine 114 can select each negative relevance score corresponding to a batch 116 of training data that is above a threshold. Additionally or alternatively, the training engine 114 can select the first k negative correlation scores corresponding to the batch 116 of training data (e.g., the training engine 114 can select the first 10 negative correlation score values, the first 100 negative correlation score values, etc.). In some implementations, the training engine 114 can generate a negative relevance score for a query based on the relevance score of the query and the selected subset of negative relevance scores. In some of those implementations, the selected subset of negative relevance scores includes one or more negative relevance scores generated for the additional queries.
The training engine 114 can generate a batch loss for a batch 116 of training data. In some implementations, the batch loss is based on each query loss generated for the batch 116 of training data (e.g., the batch loss is based on the query loss for each of the queries in the set of queries for the batch of training data). The training engine can be used to update one or more portions of the input model 110 and/or the resource model 112 based on the generated batch losses.
The resource engine 108 can be used to generate an electronic resource vector 118 corresponding to a set of candidate electronic resources 120. In some implementations, the resource engine 108 can process each candidate electronic resource in the set of candidate electronic resources 120 to generate a corresponding candidate electronic resource vector 118. In some implementations, the candidate electronic resource vector 118 can be stored locally at the computing system 102. Additionally or alternatively, the candidate electronic resource vector 118 can be stored remotely from the computing system 102 and can be accessed by the computing system 102.
The query engine 106 can be used to determine one or more candidate electronic resources 120 corresponding to the received query. In some implementations, the query can be received via one or more of the user interface input devices 104. For example, a natural language text query can be received via a keyboard, an image query can be captured via one or more cameras, a spoken utterance query can be captured using one or more microphones, and/or additional or alternative queries can be provided by a user. The query engine 106 can generate corresponding query vectors by processing the query using the input model 110. Additionally or alternatively, the query engine 106 can determine electronic resources for the received query based on a distance between the query vector and the candidate electronic resource vector 118. For example, the electronic resource can be determined based on an electronic resource vector that is closest to the query vector in the embedding space.
2A-2C illustrate training an input model and a resource model, in accordance with some implementations. FIG. 2A illustrates an example of generating relevance scores 214A-N. In FIG. 2A, the batch 116 of training data includes real-valued query/electronic resource pairs 202A-202N. For example, the real-value pair 202A includes a query A204A and an electronic resource A206A. Similarly, the real-value pair 202N includes a query N204N and an electronic resource N206N. Each query 204A-N can be processed using the input model 110 to generate a corresponding query vector (i.e., an embedded spatial representation of the corresponding query). For example, query A204A can be processed using input model 110 to generate query vector A208A. Similarly, query N204N can be processed using input model 110 to generate query vector N208N. Additionally or alternatively, each electronic resource 206A-N can be processed using resource model 112 to generate a corresponding electronic resource vector (i.e., an embedded spatial representation of the corresponding electronic resource). For example, the electronic resource 206A can be processed using the resource model 112 to generate an electronic resource vector A210A. Similarly, the electronic resource 206N can be processed using the resource model 112 to generate an electronic resource vector N210N.
In the illustrated example, the relevance score engine 212 can be used to generate a relevance score for each query in the batch 116 of training data. For example, the relevance score engine 212 can process the query vector a 208A and the electronic resource vector a 210A (i.e., the vector corresponding to the true-value pair 202A) to generate the relevance score a 214A. Similarly, the relevance score engine 212 can process the query vector N208N and the electronic resource vector N210N (i.e., the vector corresponding to the true-value pair 202N) to generate a relevance score N214N. In some implementations, the relevance score engine 212 can generate the relevance score by determining a dot product between the query vector and the electronic resource vector. For example, a relevance score a 214A can be generated using the relevance score engine 212 by determining a dot product between the query vector a 208A and the electron resource vector a 210A. Similarly, a relevance score N214N can be generated using the relevance score engine 212 by determining a dot product between the query vector N208N and the electron resource vector N210N.
FIG. 2B illustrates an example of generating negative relevance scores 218A-M. In FIG. 2B, a negative relevance score engine 216 can be used to generate negative relevance scores 218A-K. In some implementations, a negative relevance score can be generated based on a given query and an electronic resource that is not a true value pair with the given query. In some implementations, a negative relevance score can be generated for each query electronic resource pair that is not a true value pair. For example, a negative relevance score a 218A can be generated by processing the query vector a 208A and the electronic resource vector N210N (i.e., the query vector and the electronic resource vector that do not correspond to true value pairs) using the negative relevance score engine 216. Similarly, a negative relevance score K218K can be generated by processing the query vector N208N and the electronic resource vector A210A (i.e., the query vector and the electronic resource vector that do not correspond to true value pairs) using the negative relevance score engine 216. In some implementations, the negative relevance score engine 216 can generate a corresponding negative relevance score by determining a dot-product between the query vector and the electronic resource vector. For example, the negative correlation score engine 216 can generate a negative correlation score a 218A by determining a dot product between the query vector a 208A and the electron resource vector N210N. Similarly, a negative relevance score, K218K, can be generated by the negative relevance score engine 216 by determining the dot-product between the query vector N208N and the electron resource vector A210A.
FIG. 2C illustrates generating a batch loss 226 and updating one or more portions of the input model 110 and/or the resource model 112 using the generated batch loss 226 (e.g., updating using back propagation). In some implementations, a query loss can be determined for each query in the batch of training data based on the relevance score 214 corresponding to the query and one or more negative relevance scores 218, wherein at least one of the negative relevance scores 218 is generated for additional queries. In some implementations, the query loss engine 220 can use each of the generated negative relevance scores when generating query losses (i.e., the cross-example Softmax method). For example, a query penalty A222A corresponding to query A204A can be generated by processing the relevance score A214A, each negative relevance score generated for query A, and each negative relevance score generated for each additional query in the batch 116 of training data. Similarly, a query loss N222N corresponding to query N204N can be generated by processing the relevance score N214N, each negative relevance score generated for query N, and each negative relevance score generated for each additional query in the batch 116 of training data.
Additionally or alternatively, in some implementations, the query loss engine 220 can use a subset of the generated negative relevance scores when generating query losses (i.e., a cross-example negative mining method). In some implementations, the query loss engine 220 can determine a subset of negative relevance scores that satisfy one or more conditions. For example, the query loss engine 220 can determine a subset of negative relevance scores, where each negative relevance score in the subset exceeds a threshold, is a positive value, and/or satisfies one or more additional conditions. Additionally or alternatively, the query loss engine 220 can determine the subset of negative relevance scores by selecting the k negative relevance scores with the highest values (e.g., selecting the top 10 negative relevance score values, the top 50 negative relevance score values, the top 100 negative relevance score values, and/or another number of the highest negative relevance score values). For example, the query loss engine 220 can process the subset of relevance scores a 214A and negative relevance scores 218 using the query loss engine 220 to generate query loss a 222A. Similarly, a subset of the relevance scores N214N and negative relevance scores 218 can be processed using a query loss engine 220 to generate a query loss N222N. In some implementations, the query loss engine 220 can determine the same subset of negative relevance score values. In some other implementations, the query loss engine 220 can determine different subsets of negative relevance score values for one or more queries in a batch of training data.
In some implementations, the query loss engine 220 can be used to generate a query loss for each query in the batch of training data. The batch loss can be generated based on one or more of the generated query losses. For example, the batch loss engine 224 can process the query loss A222A and the query loss N222N to generate the batch loss 226. One or more portions of the input model 110 and/or the resource model 112 can be updated using the batch loss 226 (e.g., using back propagation). 2A-2C are described with respect to a batch of training data that includes two queries, two electronic resources, and corresponding true value pairs. However, this is merely an example, and the batch of training data 116 can include additional queries, electronic resources, and/or true value pairs.
Fig. 3A and 3B illustrate an embedding space 304 learned using conventional Softmax techniques (e.g., sampled Softmax, random negative mining, etc.). Fig. 3A illustrates a query a 302 (e.g., a vector representation of query a in the embedding space 304) and candidate electronic resources 306, 308, and 310 (e.g., vector representations of electronic resources 306, 308, and 310 in the embedding space 304). In the illustrated example, the system can determine that the electronic resource 306 is responsive to query a 302. For example, the system can determine that the distance between the vector representation of query a 302 and the vector representation of electronic resource 306 is less than (1) the distance between the vector representation of query a 302 and the vector representation of electronic resource 308 and/or (2) the distance between the vector representation of query a 302 and the vector representation of electronic resource 310.
Fig. 3B illustrates query B312 (e.g., a vector representation of query B in the embedding space 304) and candidate electronic resources 314, 316, 318, and 320 (e.g., vector representations of electronic resources 314, 316, 318, and 320 in the embedding space 304). In the illustrated example, the system can determine that the electronic resource 314 is responsive to query B312. For example, the system can determine that the distance between the vector representation of query B312 and the vector representation of electronic resource 314 is less than (1) the distance between the vector representation of query B312 and the vector representation of electronic resource 316, (2) the distance between the vector representation of query B312 and the vector representation of electronic resource 318, and/or (3) the distance between the vector representation of query B312 and the vector representation of electronic resource 320.
However, the embedding space 304 has not been globally calibrated using the interleaved example. Although electronic resources can be determined for query a and query B, the distances between the queries and the corresponding electronic resources are not comparable. For example, the distance between query a 302 and its corresponding electronic resource 306 is greater than the distance between query B312 and the electronic resource 316 that is not responsive to query B.
In contrast, fig. 3C and 3D illustrate an embedding space 322 that uses cross example learning in accordance with some implementations described herein. Fig. 3C illustrates query a 302 (e.g., a vector representation of query a in the embedding space 322) and candidate electronic resources 306, 308, and 310 (e.g., vector representations of electronic resources 306, 308, and 310 in the embedding space 322). Fig. 3D illustrates query B312 (e.g., a vector representation of query B in the embedding space 322) and candidate electronic resources 314, 316, 318, and 320 (e.g., vector representations of electronic resources 314, 316, 318, and 320 in the embedding space 322).
Similar to fig. 3A and 3B, the system can determine that the electronic resource 306 is responsive to the query a 302 (e.g., based on a distance between the vector representation of the query a 302 and the vector representation of the electronic resource 306), and can determine that the electronic resource 314 is responsive to the query B312 (e.g., based on a distance between the vector representation of the query B312 and the vector representation of the electronic resource 314).
However, the embedding space 322 has been globally calibrated using the interleaved examples, such that the distance in the embedding space provides an indication of how relevant the electronic resource is to the query. For example, the distance in the embedding space 322 is globally calibrated such that the distance between query A and the corresponding responsive electronic resource 306 is comparable to the distance between query B and the corresponding responsive electronic resource 314. In the illustrated example, the electronic resource 314 is closer to query B312 than the electronic resource 306 is to query A302. In some implementations, this can indicate that the electronic resource 314 is more responsive to query B312, rather than that the electronic resource 306 is more responsive to query a 302.
FIG. 5 is a flow diagram illustrating a process 500 for training an input model and/or a response model using a crossover example in accordance with implementations disclosed herein. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include various components of various computer systems, such as one or more components of computing system 102 and/or computing system 910. Further, while the operations of process 500 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 502, the system receives a batch of training data that includes a set of queries, a set of electronic resources, and true value pairs. In some implementations, each query has a corresponding pairing of real values of electronic resources. Additionally or alternatively, each electronic resource has a single corresponding real value pair. For example, a batch of training data can include a set of natural language text queries, a set of images (i.e., electronic resources), and true value pairs of natural language text queries and images. Additional and/or alternative query set(s) and/or electronic resource set(s) can be utilized.
At block 504, the system generates a query vector for each query in the query set by processing the query using the input model. Additionally or alternatively, the system generates an electronic resource vector for each electronic resource in the set of electronic resources by processing the electronic resource using the resource model. For example, the system can generate a query vector by processing the corresponding query using the input model 110 of FIG. 1. Additionally or alternatively, the system can generate an electronic resource vector by processing the corresponding electronic resource using the resource model 112 of FIG. 1. In some implementations, each query vector is a shared embedded spatial representation of the corresponding query. Additionally or alternatively, each electronic resource is a shared embedded spatial representation of the corresponding electronic resource.
At block 506, the system generates a relevance score for each query in the query set based on (1) the query vector of the query and (2) the electronic resource vector having the pairing with the true value of the query. For example, the system can determine a relevance score for each of the real-value pairings by determining a dot product between a corresponding query vector (e.g., the query vector generated at block 504) and a corresponding electronic resource vector (e.g., the electronic resource vector generated at block 504).
At block 508, the system generates, for each query and for each electronic resource other than the true value pairs, a negative relevance score based on (1) the query vector for the corresponding query and (2) the electronic resource vector for the corresponding electronic resource. For example, for each query vector and each electronic resource vector that is not paired with the true value of the query vector, the system can generate a negative relevance score by determining a dot-product between the corresponding query vector (e.g., the query vector generated at block 504) and the corresponding electronic resource vector (e.g., the electronic resource vector generated at block 504).
At block 510, the system generates, for each query in the set of queries, a query penalty based on (1) the relevance score of the corresponding query and (2) one or more negative relevance scores generated for at least one additional query. In some implementations, the system can generate a query penalty using a cross example Softmax for each query. The process 600 of fig. 6 described herein is an example process for generating a query penalty using the cross example Softmax. Additionally or alternatively, the system can generate query losses using cross-example negative mining for each query. The process 700 of FIG. 7 described herein is an example process for generating a query penalty using cross example negative mining.
At block 512, the system generates a batch loss for the batch of training data based on the generated query loss. In some implementations, the system generates a batch penalty based on the query penalty generated for each query in the batch of training data.
At block 514, the system updates one or more portions of the input model and/or the resource model based on the batch loss (e.g., using back propagation). For example, the system can update one or more portions of the input model used at block 504 to generate a query vector using the batch loss. Additionally or alternatively, the system can update one or more portions of the resource model used at block 504 to generate the electronic resource vector using the batch loss.
FIG. 6 is a flow diagram illustrating a process 600 for generating a query penalty using a cross example Softmax method according to implementations disclosed herein. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include various components of various computer systems, such as one or more components of computing system 102 and/or computing system 910. Further, while the operations of process 600 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 602, the system selects a query from a set of queries. In some implementations, queries can be selected from a set of queries of a batch of training data. In some of those implementations, the batch of training data can include a set of queries, a set of electronic resources, and true value pairs of the queries and the electronic resources. For example, the system can select a query from the batch of training data received at block 502 of process 500 of FIG. 5.
At block 604, the system generates a query penalty using the cross example Softmax for the selected query based on: the relevance scores generated for the selected query are (1) a relevance score generated for the selected query, (2) each negative relevance score generated for the selected query, and (3) each negative relevance score generated for each additional query. In other words, the system can generate a query penalty for the selected query based on the relevance score of the selected query and each negative relevance score generated for the batch of training data. In some implementations, a relevance score can be generated at block 506 of process 500 of fig. 5. In some implementations, each of the negative relevance scores can be generated at block 508 of the process 500 of fig. 5.
At block 606, the system determines whether to process any additional queries. If so, the system returns to block 602, selects additional queries from the set of queries, and proceeds to block 604 to generate additional query penalties for the selected additional queries. If the system determines not to process any additional queries, the process ends. In some implementations, the system can determine not to process any additional queries when no unprocessed queries remain in the query set and/or based on whether the additional condition(s) are satisfied.
FIG. 7 is a flow diagram illustrating a process 700 for generating a query penalty using a cross-example negative mining method in accordance with implementations disclosed herein. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include various components of various computer systems, such as one or more components of computing system 102 and/or computing system 910. Further, while the operations of process 700 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 702, the system selects a subset of negative relevance scores from the set of negative relevance scores that satisfy one or more conditions. For example, a set of negative relevance scores can be generated at block 508 of the process 500 of fig. 5. For example, the system can select a subset of negative relevance scores based on whether the negative relevance scores exceed a threshold, the negative relevance scores do not exceed a threshold, and/or whether additional or alternative conditions are met. Additionally or alternatively, the system can select the top k negative relevance scores as the subset of negative relevance scores. For example, the system can select the first 20 negative correlation score values in the set of negative correlation scores. In some implementations, one or more negative relevance scores corresponding to the query are not selected in the subset of negative relevance scores. In some implementations, all negative relevance scores corresponding to the query are selected in the subset of negative relevance scores. In some implementations, negative relevance scores corresponding to the query are not selected in the subset of negative relevance scores.
At block 704, the system selects a query from the set of queries. In some implementations, queries can be selected from a set of queries of a batch of training data used to generate a set of relevance scores. In some of those implementations, the batch of training data can include a set of queries, a set of electronic resources, and true value pairs of the queries and the electronic resources. For example, the system can select a query from the batch of training data received at block 502 of process 500 of FIG. 5.
At block 706, the system generates a query penalty based on (1) the relevance scores generated for the selected query and (2) the selected subset of negative relevance scores using cross-example negative mining for the selected query.
At block 708, the system determines whether to process any additional queries. If so, the system returns to block 704, selects additional queries from the set of queries, and proceeds to block 706 to generate additional query penalties for the selected additional queries. If the system determines not to process any additional queries, the process ends. In some implementations, the system can determine not to process any additional queries when no unprocessed queries remain in the query set and/or based on whether the additional condition(s) are satisfied.
FIG. 8 is a flow diagram illustrating a process 800 for determining electronic resources for a query in accordance with implementations disclosed herein. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include various components of various computer systems, such as one or more components of computing system 102 and/or computing system 910. Further, while the operations of process 800 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 802, the system receives a query. For example, the system can receive a natural language text query from a user of the computing system.
At block 804, the system generates a query vector by processing the query using the input model.
At block 806, the system compares the query vector to a plurality of pre-stored candidate electronic resource vectors. In some implementations, each pre-stored candidate electronic resource vector is previously generated by processing the corresponding candidate electronic resource using a resource model. For example, the system can compare the query vector to a plurality of pre-stored candidate electronic resource vectors, wherein each candidate electronic resource vector corresponds to a candidate image of a candidate natural language text query. In some implementations, the system determines the distance between the query vector and each of the pre-stored candidate electronic resource vectors by determining a dot product between the query vector and each of the pre-stored candidate electronic resource vectors.
At block 808, the system selects a pre-stored candidate electronic resource vector based on the comparison. In some implementations, the system can select a pre-stored candidate electronic resource vector that is closest to the query vector (e.g., the pre-stored candidate electronic resource vector having the smallest distance determined at block 806).
At block 810, the system determines an electronic resource based on the selected pre-stored candidate electronic resource vector for the query.
At block 812, the system causes the computing system to perform one or more actions based on the determined electronic resource. For example, the system can display the determined electronic resource on a screen for a user of the computing system. In some implementations, the system can provide the distance between the corresponding query vector determined at block 806 and the corresponding pre-stored electronic resource vector to one or more components in the computing system downstream from the system. For example, the system can determine a bounding box (e.g., an electronic resource) for a provided image (e.g., a query). In some implementations, the system can provide the bounding box to an additional system that can be used to identify objects captured in the bounding box. Additionally or alternatively, the system can make a determination that the electronic resource is not presented to the user, but instead can request additional information from the user (e.g., can request information to clarify the query, can request a new query, etc.). For example, the system can receive an image as a query at block 802. The system can determine one or more bounding boxes of the image based on comparing the corresponding query vector to a plurality of pre-stored candidate electronic resource vectors corresponding to the candidate bounding boxes.
In some implementations, the system can receive a natural language query at block 802. The system can determine one or more images for the query based on comparing the corresponding query vector to a plurality of pre-stored candidate electronic resource vectors corresponding to the candidate images. In some implementations, the system can render one or more electronic resources based on a comparison of the query vector with pre-stored candidate electronic resource vectors (i.e., based on the relevance scores of the query vector and a plurality of pre-stored electronic resource vectors). In some implementations, the system can directly link to a single electronic resource based on a relevance score that satisfies one or more conditions (e.g., the relevance score exceeds a threshold indicating high relevance between the query and the electronic resource). In some implementations, the system can render electronic resources with correspondingly high relevance scores in a manner that, for example, emphasizes the electronic resources (e.g., renders the electronic resources more prominently, presents a large segment of the electronic resources, etc.), while additional electronic resources with lower corresponding relevance scores are rendered without emphasis. In some implementations, computing resources (e.g., bandwidth, processor cycles, memory, etc.) can be conserved by providing electronic resources to a user based on relevance scores corresponding to the electronic resources.
FIG. 9 is a block diagram of an example computing device 910 that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing device and/or other component(s) may include one or more components of the example computing device 910.
The computing device 910 typically includes at least one processor 914 that communicates with a number of peripheral devices via a bus subsystem 912. These peripheral devices may include a storage subsystem 924 (including, for example, a memory subsystem 925 and a file storage subsystem 926), a user interface output device 920, a user interface input device 922, and a network interface subsystem 916. The input and output devices allow a user to interact with the computing device 910. Network interface subsystem 916 provides an interface to external networks and couples to corresponding interface devices in other computing devices.
The user interface input devices 922 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touch screen incorporated into the display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 910 or onto a communication network.
User interface output devices 920 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a cathode ray tube ("CRT"), a flat panel device such as a liquid crystal display ("LCD"), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, such as via audio output devices. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 910 to a user or to another machine or computing device.
These software modules are typically executed by processor 914 alone or in combination with other processors. Memory 925 used in storage subsystem 924 can include a number of memories, including a main random access memory ("RAM") 930 for storing instructions and data during program execution and a read only memory ("ROM") 932 in which fixed instructions are stored. File storage subsystem 926 is capable of providing persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of certain implementations may be stored by file storage subsystem 926 in storage subsystem 924, or in other machines accessible by processor(s) 914.
The bus subsystem 912 provides a mechanism for the various components and subsystems of the computing device 910 to communicate with one another as intended. Although the bus subsystem 912 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses.
The systems described herein collect personal information about a user (or "participant" as often mentioned herein), or where personal information may be utilized, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current geographic location) or whether and/or how to receive content from a content server that may be more relevant to the user. In addition, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized where geographic location information is obtained (such as to a city, zip code, or state level) such that a particular geographic location of the user cannot be determined. Thus, the user may control how information is collected and/or used with respect to the user.
In some implementations, a method implemented by one or more processors is provided that includes identifying a batch of training data that includes a set of queries, a set of electronic resources, and a pair of real values, wherein each of the pair of real values is a pair of a corresponding one of the electronic resources and a real value of a corresponding one of the queries, and wherein each of the electronic resources has only a corresponding single pair of real values of the pair of real value. In some implementations, for each query in the set of queries, the method includes generating a corresponding query vector by processing the query using the input model. In some implementations, for each electronic resource in the set of electronic resources, the method includes generating a corresponding electronic resource vector by processing the electronic resource using the resource model. In some implementations, for each query in the set of queries, the method includes generating a relevance score based on (1) a corresponding query vector generated for the query and (2) a corresponding electronic resource vector generated for an electronic resource having a pairing with a true value of the query. In some implementations, the method includes, for each electronic resource other than the corresponding electronic resource having a pairing with the true value of the query, generating a corresponding negative relevance score based on (1) the corresponding query vector generated for the query and (2) the electronic resource vectors generated for the electronic resources other than the corresponding electronic resource having a relationship with the true value of the query. In some implementations, the method includes generating a query loss based on (1) a relevance score generated for the query and (2) at least one corresponding negative relevance score generated for at least one additional query of the set of queries. In some implementations, the method includes generating, for a batch of training data, a batch penalty based on the generated query penalty. In some implementations, the method includes updating one or more portions of the input model and/or the resource model based on the generated batch losses.
These and other implementations of the techniques disclosed herein can include one or more of the following features.
In some implementations, generating the query loss based on (1) the relevance score generated for the query and (2) the at least one corresponding negative relevance score generated for the at least one additional query of the set of queries includes generating the query loss based on: the set of queries includes (1) a relevance score generated for a query, (2) a corresponding negative relevance score generated for the query, and (3) all corresponding negative relevance scores generated for each of the additional queries in the set of queries.
In some implementations, generating the query loss based on (1) the relevance scores generated for the query and (2) at least one corresponding negative relevance score generated for at least one additional query of the set of queries includes selecting a subset of negative relevance scores, wherein the selected subset includes at least one negative relevance score generated for at least one additional query of the set of queries, and wherein selecting the subset satisfies one or more conditions based on the corresponding negative relevance scores of the subset. In some implementations, the method includes generating a query penalty based on (1) the relevance scores generated for the query and (2) a subset of the corresponding negative relevance scores.
In some implementations, each query in the set of queries is a natural language query, and wherein each electronic resource in the set of electronic resources is an image or a web page.
In some implementations, each query in the set of queries is an image of a captured object, and wherein each electronic resource in the set of electronic resources represents one or more corresponding bounding boxes.
In some implementations, subsequent to updating one or more portions of the input model and/or the resource model based on the generated batch loss, the method further includes deploying the trained input model on the computing system. In some implementations, the method includes receiving a user query via one or more user interface input devices of the computing system. In some implementations, the method includes determining a user query vector by processing the user query using the trained input model. In some implementations, the method includes determining the user electronic resource in response to the user query, wherein determining the user electronic resource in response to the user query includes comparing the user query vector to a plurality of pre-stored candidate electronic resource vectors, wherein each of the plurality of pre-stored candidate electronic resource vectors was previously generated by processing the corresponding electronic resource using the resource model. In some implementations, the method includes selecting a pre-stored candidate electronic resource vector based on the comparison. In some implementations, the method includes determining the user electronic resource based on the selected pre-stored candidate electronic resource vector. In some implementations, the method includes causing the computing system to perform one or more actions based on the determined user electronic resource.
In some implementations, for each query in the set of queries, generating a relevance score based on (1) a corresponding query vector generated for the query and (2) a corresponding electronic resource vector generated for an electronic resource having a pairing with a true value of the query includes determining a dot product between (1) the corresponding query vector generated for the query and (2) the corresponding electronic resource vector generated for the electronic resource having a pairing with a true value of the query. In some implementations, the method includes generating a relevance score based on the determined dot product.
In some implementations, for each query in the set of queries, generating, for each electronic resource other than the corresponding electronic resource having a pairing with the true value of the query, a corresponding negative relevance score based on (1) the corresponding query vector generated for the query and (2) the electronic resource vector generated for the electronic resource other than the corresponding electronic resource having a relationship with the true value of the query includes determining a dot product between (1) the corresponding query vector generated for the query and (2) the electronic resource vector generated for the electronic resource other than the corresponding electronic resource having a relationship with the true value of the query. In some implementations, the method includes generating a negative correlation score based on the determined dot product.
In some implementations, for each query in the set of queries, the generated query vector projects the query into the shared embedding space, and wherein, for each electronic resource in the set of electronic resources, the electronic resource vector projects the electronic resource into the shared embedding space.
In some implementations, a method implemented by one or more processors is provided that includes receiving an image of a capture object. In some implementations, the method includes generating an image vector by processing the image using the input model. In some implementations, a bounding box of an object captured in an image is determined, where determining the bounding box of the object includes comparing an image vector to a plurality of pre-stored candidate bounding box vectors, where each pre-stored candidate bounding box vector of the plurality of pre-stored candidate bounding box vectors was previously generated by processing the corresponding candidate bounding box using a resource model. In some implementations, the method includes selecting a pre-stored candidate bounding box vector based on the comparison. In some implementations, the method includes determining a bounding box of the object based on the selected pre-stored candidate bounding box vector. In some implementations, the method includes causing the computing device to perform one or more actions based on the determined bounding box of the object.
Additionally, some implementations include one or more processors (e.g., central processing unit(s) (CPU), graphics processing unit(s) (GPU), and/or tensor processing unit(s) (TPU)) of the one or more computing devices, wherein the one or more processors are operable to execute instructions stored in the associated memory, and wherein the instructions are configured to cause performance of any of the methods described herein. Some implementations also include one or more transitory or non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the methods described herein.
Claims (13)
1. A method implemented by one or more processors, the method comprising:
identifying a batch of training data comprising a set of queries, a set of electronic resources, and pairs of real values, wherein each of the pairs of real values is a pair of a corresponding one of the electronic resources and a real value of a corresponding one of the queries, and wherein each of the electronic resources has only a corresponding single pair of real values of the pair of real values;
for each query in the set of queries, generating a corresponding query vector by processing the query using an input model;
for each electronic resource in the set of electronic resources, generating a corresponding electronic resource vector by processing the electronic resource using a resource model;
for each query in the set of queries,
generating a relevance score based on (1) the corresponding query vector generated for the query and (2) the corresponding electronic resource vector generated for the electronic resource having a true value pair with the query; and is
For each electronic resource other than the corresponding electronic resource that has the pairing with the true value of the query, generating a corresponding negative relevance score based on (1) the corresponding query vector generated for the query and (2) the electronic resource vectors generated for the electronic resources other than the corresponding electronic resource that has a relationship with the true value of the query;
generating a query loss based on (1) the relevance score generated for the query and (2) at least one corresponding negative relevance score generated for at least one additional query of the set of queries;
generating, for a batch of the training data, a batch penalty based on the generated query penalty; and
updating one or more portions of the input model and/or the resource model based on the generated batch losses.
2. The method of claim 1, wherein generating the query penalty based on (1) the relevance score generated for the query and (2) the at least one corresponding negative relevance score generated for at least one additional query of the set of queries comprises:
generating the query penalty based on (1) the relevance scores generated for the queries, (2) the corresponding negative relevance scores generated for the queries, and (3) all of the corresponding negative relevance scores generated for each of the additional queries in the set of queries.
3. The method of claim 1, wherein generating the query loss based on (1) the relevance score generated for the query and (2) the at least one corresponding negative relevance score generated for at least one additional query of the set of queries comprises:
selecting a subset of the negative relevance scores, wherein the selected subset includes at least one negative relevance score generated for at least one additional query in the set of queries, and wherein selecting the subset satisfies one or more conditions based on the corresponding negative relevance scores of the subset; and
generating the query loss based on (1) the relevance scores generated for the query and (2) the subset of the corresponding negative relevance scores.
4. The method of any preceding claim, wherein each query of the set of queries is a natural language query, and wherein each electronic resource of the set of electronic resources is an image or a web page.
5. The method of any of claims 1-3, wherein each query in the set of queries is an image of a captured object, and wherein each electronic resource in the set of electronic resources represents one or more corresponding bounding boxes.
6. The method of any preceding claim, following updating of the one or more portions of the input model and/or the resource model based on the generated batch losses, and further comprising:
deploying the trained input model on the computing system;
receiving a user query via one or more user interface input devices of the computing system;
determining a user query vector by processing the user query using the trained input model;
determining a user electronic resource responsive to the user query, wherein determining the user electronic resource responsive to the user query comprises:
comparing the user query vector to a plurality of pre-stored candidate electronic resource vectors, wherein each of the plurality of pre-stored candidate electronic resource vectors was previously generated by processing a corresponding electronic resource using the resource model;
selecting a pre-stored candidate electronic resource vector based on the comparison; and is
Determining the user electronic resource based on the selected pre-stored candidate electronic resource vector; and
causing the computing system to perform one or more actions based on the determined user electronic resource.
7. The method of any preceding claim, wherein generating, for each query of the set of queries, the relevance score based on (1) the corresponding query vector generated for the query and (2) the corresponding electronic resource vector generated for the electronic resource having the real value pair with the query comprises:
determining a dot product between (1) the corresponding query vector generated for the query and (2) the corresponding electronic resource vector generated for the electronic resource having the pairing with the true value of the query; and
generating the relevance score based on the determined dot product.
8. The method of any preceding claim, wherein, for each query of the set of queries, for each electronic resource other than the corresponding electronic resource having the pairing with the true value of the query, generating the corresponding negative relevance score based on (1) the corresponding query vector generated for the query and (2) the electronic resource vector generated for the electronic resources other than the corresponding electronic resource having the relationship with the true value of the query comprises:
determining a dot product between (1) the corresponding query vector generated for the query and (2) the electronic resource vectors generated for the electronic resources other than the corresponding electronic resource having the true value relationship to the query; and
generating the negative relevance score based on the determined dot product.
9. The method of any preceding claim, wherein, for each query in the set of queries, the generated query vector projects the query into a shared embedding space, and wherein, for each electronic resource in the set of electronic resources, the electronic resource vector projects the electronic resource into the shared embedding space.
10. A method implemented by one or more processors, the method comprising:
receiving an image of a capture object;
generating an image vector by processing the image using an input model;
determining a bounding box of the object captured in the image, wherein determining the bounding box of the object comprises:
comparing the image vector to a plurality of pre-stored candidate bounded-frame vectors, wherein each of the plurality of pre-stored candidate bounded-frame vectors was previously generated by processing a corresponding candidate bounded frame using a resource model;
selecting a pre-stored candidate bounding box vector based on the comparison; and is provided with
Determining the bounding box of the object based on the selected pre-stored candidate bounding box vector; and
causing the computing device to perform one or more actions based on the determined bounding box of the object.
11. A computer program comprising instructions which, when executed by one or more processors of a computing system, cause the computing system to perform the method of any preceding claim.
12. A computing system configured to perform the method of any of claims 1-10.
13. A computer-readable storage medium storing instructions executable by one or more processors of a computing system to perform the method of any one of claims 1-10.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062988270P | 2020-03-11 | 2020-03-11 | |
US62/988,270 | 2020-03-11 | ||
PCT/US2020/023320 WO2021183151A1 (en) | 2020-03-11 | 2020-03-18 | Cross-example softmax and/or cross-example negative mining |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115244527A true CN115244527A (en) | 2022-10-25 |
Family
ID=70289463
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080098439.6A Pending CN115244527A (en) | 2020-03-11 | 2020-03-18 | Cross example SOFTMAX and/or cross example negative mining |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230111978A1 (en) |
EP (1) | EP4100847A1 (en) |
CN (1) | CN115244527A (en) |
WO (1) | WO2021183151A1 (en) |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11188824B2 (en) * | 2017-02-17 | 2021-11-30 | Google Llc | Cooperatively training and/or using separate input and subsequent content neural networks for information retrieval |
US20200090039A1 (en) * | 2017-07-17 | 2020-03-19 | Google Llc | Learning unified embedding |
-
2020
- 2020-03-18 US US17/910,756 patent/US20230111978A1/en active Pending
- 2020-03-18 CN CN202080098439.6A patent/CN115244527A/en active Pending
- 2020-03-18 WO PCT/US2020/023320 patent/WO2021183151A1/en unknown
- 2020-03-18 EP EP20719269.1A patent/EP4100847A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230111978A1 (en) | 2023-04-13 |
WO2021183151A1 (en) | 2021-09-16 |
EP4100847A1 (en) | 2022-12-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11816888B2 (en) | Accurate tag relevance prediction for image search | |
JP7073241B2 (en) | Improved font recognition by dynamically weighting multiple deep learning neural networks | |
US10235623B2 (en) | Accurate tag relevance prediction for image search | |
CN111444320B (en) | Text retrieval method and device, computer equipment and storage medium | |
EP3542319B1 (en) | Training neural networks using a clustering loss | |
CN105630856B (en) | Automatic aggregation of online user profiles | |
US9607014B2 (en) | Image tagging | |
US20210264106A1 (en) | Cross Data Set Knowledge Distillation for Training Machine Learning Models | |
US11500884B2 (en) | Search and ranking of records across different databases | |
CN112395487A (en) | Information recommendation method and device, computer-readable storage medium and electronic equipment | |
CN111274822A (en) | Semantic matching method, device, equipment and storage medium | |
CN106570196B (en) | Video program searching method and device | |
EP4060526A1 (en) | Text processing method and device | |
CN113780365A (en) | Sample generation method and device | |
JP2016110256A (en) | Information processing device and information processing program | |
CN117149967A (en) | Response generation method, device, server and computer readable storage medium | |
CN112883218A (en) | Image-text combined representation searching method, system, server and storage medium | |
CN115244527A (en) | Cross example SOFTMAX and/or cross example negative mining | |
CN115840817A (en) | Information clustering processing method and device based on contrast learning and computer equipment | |
CN117009534B (en) | Text classification method, apparatus, computer device and storage medium | |
US11887226B2 (en) | Using machine learning for iconography recommendations | |
US20230306087A1 (en) | Method and system of retrieving multimodal assets | |
US20240161367A1 (en) | Using machine learning for iconography recommendations | |
US20230394387A1 (en) | Content analysis and retrieval using machine learning | |
JP5202569B2 (en) | Machine learning method and machine learning system |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |