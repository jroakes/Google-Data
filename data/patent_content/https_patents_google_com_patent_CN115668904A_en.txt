CN115668904A - Automatic navigation of an Interactive Voice Response (IVR) tree on behalf of a human user - Google Patents
Automatic navigation of an Interactive Voice Response (IVR) tree on behalf of a human user Download PDFInfo
- Publication number
- CN115668904A CN115668904A CN202080100936.5A CN202080100936A CN115668904A CN 115668904 A CN115668904 A CN 115668904A CN 202080100936 A CN202080100936 A CN 202080100936A CN 115668904 A CN115668904 A CN 115668904A
- Authority
- CN
- China
- Prior art keywords
- client device
- user
- ivr
- given
- tree
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/487—Arrangements for providing information services, e.g. recorded voice services or time announcements
- H04M3/493—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals
- H04M3/4931—Directory assistance systems
- H04M3/4935—Connection initiated by DAS system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/901—Indexing; Data structures therefor; Storage structures
- G06F16/9027—Trees
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/42025—Calling or Called party identification service
- H04M3/42034—Calling party identification service
- H04M3/42059—Making use of the calling party identifier
- H04M3/42068—Making use of the calling party identifier where the identifier is used to access a profile
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/42136—Administration or customisation of services
- H04M3/4217—Managing service interactions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/42204—Arrangements at the exchange for service or number selection by voice
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/487—Arrangements for providing information services, e.g. recorded voice services or time announcements
- H04M3/493—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/487—Arrangements for providing information services, e.g. recorded voice services or time announcements
- H04M3/493—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals
- H04M3/4936—Speech interaction details
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/50—Centralised arrangements for answering calls; Centralised arrangements for recording messages for absent or busy subscribers ; Centralised arrangements for recording messages
- H04M3/51—Centralised call answering arrangements requiring operator intervention, e.g. call or contact centers for telemarketing
- H04M3/5166—Centralised call answering arrangements requiring operator intervention, e.g. call or contact centers for telemarketing in combination with interactive voice response systems or voice portals, e.g. as front-ends
Abstract
Embodiments are directed to utilizing an assistant to automatically navigate an Interactive Voice Response (IVR) tree to reach a target state during an auxiliary telephone call. The assistant can receive input for initiating an auxiliary telephone call, and based on the input, identify an entity to be engaged on behalf of the user during the auxiliary telephone call, and identify an IVR tree stored in association with the entity. In various implementations, navigation of the IVR tree can be modified based on interactions detected at the client device after initiating the secondary telephone call. In various implementations, a secondary phone call can be initiated from the search interface and a target state can be associated with a given search result. In various embodiments, the IVR tree can be dynamic in that only a subset of the candidate states of the IVR tree may be available as target states.
Description
Background
A user can interact with the automated assistant via a variety of computing devices, such as smartphones, tablet computers, wearable devices, automotive systems, standalone personal assistant devices, and so forth. The automated assistant receives input (e.g., verbal, touch, and/or key input) from the user and responds with a responsive output (e.g., visual and/or audible).
The user can interact with the automated assistant to cause the automated assistant to perform actions on behalf of the user. As one example, an automated assistant can navigate an Interactive Voice Response (IVR) system, can place telephone calls on behalf of users to perform given actions, and can engage in conversations with other users to perform the actions. For example, the user can provide user input requesting the automated assistant to make restaurant reservations over the phone and on behalf of the user. The automated assistant can initiate a telephone call with a particular restaurant, and can navigate an IVR system associated with the particular restaurant to make a reservation, and/or can provide reservation information to additional personnel associated with the particular restaurant to make the reservation. Then, when the person associated with the particular restaurant is an active participant in the telephone call, the automated assistant can notify the user, notify the user that a restaurant reservation was successfully made on behalf of the user during the telephone call, and/or cause the user to join the telephone call.
Disclosure of Invention
Embodiments disclosed herein relate to using an automated assistant to perform an assisted telephone call with an entity on behalf of a given user to navigate an Interactive Voice Response (IVR) tree of an IVR system associated with the entity. The IVR tree of the IVR system associated with an entity can be stored in a database accessible to the client device of a given user. Performing the secondary telephone call to navigate the IVR tree can include automatically navigating the IVR tree to a target state of the IVR tree. The target state of the IVR tree can be determined from among multiple based on user input detected at a client device of a given user. The automated assistant can automatically navigate the IVR tree to a target state of the IVR tree through instances of synthesized speech, which can be rendered as part of the secondary phone call, which can include injecting the synthesized speech or human speech into the secondary phone so that the synthesized speech or human speech is audibly perceivable to the IVR system (but not necessarily the given user), and/or instances of simulated button presses can be rendered as part of the secondary phone call, which can include injecting simulated button presses into the call so that the IVR system can perceive it. Button presses of the synthesized speech, human speech, and/or simulation can be determined based on corresponding values of parameters associated with navigating the IVR tree to the target state. The parameters can be stored in association with IVR trees associated with the entities, and corresponding values of the parameters can be retrieved from a database and/or requested from a given user of the client device. In some implementations, the automated assistant can cause the client device to render a notification requesting that a user of the client device join the secondary phone call. In some additional or alternative embodiments, the automated assistant can optionally perform tasks on behalf of the user after automatically navigating the IVR tree to the target state. For example, an automated assistant can engage in a conversation with a human representative associated with an entity to perform a task on behalf of a user.
Some embodiments relate to modifying navigation of an IVR tree based on user interactions detected at a client device after initiating a secondary telephone call. The automated assistant can generate and render prompts related to modifying navigation of the IVR tree, and can determine whether to modify navigation of the IVR tree based on user input received in response to the prompts. In some versions of those embodiments, modifying the navigation of the IVR tree may include modifying the target state to an alternate target state or adding additional target states to navigate to after navigating to the target state. For example, assume that the automated assistant is navigating to a target state associated with a finishing department of an IVR tree associated with a home decoration entity. In this example, if a given user interacts with the home and gardening portions of the web page associated with the home entity, the user may be prompted to additionally or alternatively navigate to a target state associated with the home and gardening departments of the IVR tree associated with the home entity. In some additional and/or alternative versions of those embodiments, modifying the navigation of the IVR tree may include modifying corresponding values of parameters associated with navigating the IVR tree to the target state. Continuing with the example above, if a given user interacts with a web page associated with a particular brand of paint, the user may be prompted to apply a corresponding value associated with the particular brand of paint to the paint brand parameters. In these and other ways, client device resources and/or network resources can be reserved by preventing a restart of another secondary phone call performed with respect to an alternate target state and/or alternate value for navigating the IVR tree. In addition, the secondary telephone call can end faster, thereby reducing the overall duration of computer and/or network resources used to perform the secondary telephone call. Furthermore, battery resources of the client device may also be reserved because the given user is not an active participant in the secondary phone call, and the client device may be placed in a locked or low-power state during the secondary phone call.
Some additional or alternative implementations involve providing IVR deep links to candidate states of the IVR tree and search results received in response to a search query determined based on user input. In some versions of those embodiments, a given IVR deep link can be provided based on recent user interaction at the client device prior to issuing a search query. For example, assume that a user of a client device is interacting with search results related to reporting missing baggage to an airline entity, and then issuing a search for the airline entity. In this example, the automated assistant can cause an IVR deep link associated with the target status of reporting missing baggage (and optionally more prominently than other IVR deep links that may also be provided) to be provided for selection by a given user of the client device based on recent user interactions. In some additional and/or alternative versions of those embodiments, a given IVR deep link can be provided based on the terms of the search query. For Example, assume that a given user of a client issues a search query to "report missing bags to Example Airlines". In this Example, the automated assistant can cause IVR deep links associated with the target status of reporting lost baggage to be provided for selection by a given user of the client device based on terms of the search query including "report lost baggage" and "Example Airlines". In these and other ways, the number of user inputs can be reduced, and thus client device resources can be conserved, as a given user need only select "IVR deep links". In addition, the secondary phone call can be ended more quickly, thereby reducing the overall duration of computer and/or network resources used to perform the secondary phone call. Furthermore, battery resources of the client device may also be reserved because the given user is not an active participant in the secondary phone call, and the client device may be placed in a locked or low-power state during the secondary phone call.
Some additional or alternative implementations relate to determining availability of candidate states based on contextual information associated with a client device of a given user and/or contextual information associated with a given user of a client device. The contextual information associated with the client device can include, for example, device state information (e.g., on state, off state, locked state, sleep state), date and/or time information associated with the current location of the client device, and/or other contextual information generated by the client device. For example, if a restaurant entity is open, the candidate statuses associated with the restaurant entity may include a takeaway candidate status and a delivery candidate status. However, if the restaurant entity is out of business, the candidate status associated with the restaurant entity may not include a takeaway candidate status or a delivery candidate status, or if the client device is located outside of the delivery radius, the candidate status associated with the restaurant entity may not include a delivery candidate status even if the restaurant is open. Further, the contextual information associated with the given user of the client device can include, for example, electronic communications created by or received by the user of the client device, user account information associated with various user accounts of the given user of the client device, monetary information of the given user of the client device, albums of the given user of the client device, social media profiles of the given user of the client device, user preferences of the user of the client device, personal information of the given user of the client device, and/or other information associated with the given user of the client device. For example, if a given user is gold or a member thereof at a hospitality entity, the candidate status associated with the hospitality entity may include a contact concierge status or a request for free upgrade status. However, if a given user is not gold or more members at a hospitality entity, the candidate states may omit these states. In these and other ways, client device resources and/or network resources can be conserved by preventing the restart of another secondary phone call performed in connection with a given user selecting a candidate state that may not be available. In addition, the secondary phone call can be ended more quickly, thereby reducing the overall duration of computer and/or network resources used to perform the secondary phone call. Furthermore, battery resources of the client device may also be reserved because the given user is not an active participant in the secondary phone call, and the client device may be placed in a locked or low-power state during the secondary phone call.
Some additional or alternative implementations involve rendering candidate states in various ways based on confidence levels of IVR trees associated with the entities. The confidence level of the IVR tree stored in association with the entity may be based on a number of secondary phone calls with the entity including navigating an IVR tree different from the IVR tree stored in association with the entity, and optionally based on a degree of difference between the navigated IVR tree and the stored IVR tree. For example, if the confidence level of the IVR tree satisfies a first threshold that indicates that the automated assistant is confident of an IVR tree height associated with the entity, the automated assistant can cause a sequence of button presses to be rendered at the client device to inform a given user of the client device how to quickly navigate the IVR tree to one or more candidate states before or when initiating the secondary telephone call (e.g., press "1", "3", "5" to report missing baggage), or the automated assistant can cause a selectable graphical element associated with the candidate states (which may not match the sequence of button presses) to be rendered on the client device, and, when selected, the selectable graphical element causes the automated assistant to automatically navigate to a target state associated with the selected selectable graphical element before or when initiating the secondary telephone call (e.g., press "5" to report missing baggage). As another example, if the confidence level of the IVR tree fails to satisfy the first threshold, but satisfies the second threshold, indicating that the automated assistant is lightly confident of the IVR tree associated with the entity, the automated assistant can cause the aforementioned selectable graphical elements (which may not match the sequence of button presses) associated with the candidate state to be rendered on the client device only when traversing a portion of the IVR tree that is lightly confident by the automated assistant. As yet another example, if the confidence level of the IVR tree fails to satisfy the first threshold and the second threshold, indicating that the automated assistant has no confidence in the IVR tree associated with the entity, the automated assistant can cause the aforementioned selectable graphical elements associated with the candidate state (which may not match the sequence of button presses) to be rendered at the client device after the automated assistant perceives the options of the IVR tree. In these and other ways, client device resources and/or network resources can be conserved by preventing the restart of another secondary phone call in the event that the automated assistant provides incorrect information to the user to navigate the IVR tree. Further, the secondary telephone call can be ended more quickly, thereby reducing the overall duration of computer and/or network resources for performing the secondary telephone call by providing a sequence of button presses or selectable elements associated with the candidate states. Furthermore, battery resources of the client device may also be reserved because the given user is not an active participant in the secondary phone call, and the client device may be placed in a locked or low-power state during the secondary phone call.
The above description is provided as an overview of only some embodiments disclosed herein. These and other embodiments are described herein in additional detail.
Drawings
FIG. 1 depicts a block diagram of an example environment in which various aspects of the present disclosure are presented and in which implementations disclosed herein can be implemented.
FIG. 2 depicts an example Interactive Voice Response (IVR) tree, according to various embodiments.
Fig. 3 depicts a flowchart illustrating an example method of modifying navigation of an Interactive Voice Response (IVR) tree based on user interaction at a client device associated with a given user, in accordance with various embodiments.
Fig. 4A, 4B, and 4C depict various non-limiting examples of user interfaces associated with modifying navigation of an Interactive Voice Response (IVR) tree based on user interaction at a client device associated with a given user, according to various embodiments.
Fig. 5 depicts a flowchart illustrating an example method of navigating an Interactive Voice Response (IVR) tree from a search interface at a client device associated with a given user, in accordance with various embodiments.
6A, 6B, 6C, 6D, and 6E depict various non-limiting examples of user interfaces associated with navigating an Interactive Voice Response (IVR) tree from a search interface at a client device associated with a given user, according to various embodiments.
FIG. 7 depicts a flowchart illustrating an example method of navigating a dynamic Interactive Voice Response (IVR) tree in accordance with various embodiments.
Fig. 8A, 8B, 8C, and 8D depict various non-limiting examples of user interfaces associated with navigating a dynamic Interactive Voice Response (IVR) tree, according to various embodiments.
FIG. 9 depicts an example architecture of a computing device according to various embodiments.
Detailed Description
Turning now to fig. 1, a block diagram of an example environment is depicted in which aspects of the present disclosure may be presented and in which embodiments disclosed herein may be implemented. A client device 110 is shown in fig. 1, and the client device 110 includes, in various embodiments, a user input engine 111, a device state engine 112, a rendering engine 113, a search engine 114, a speech recognition engine 120A1, a natural language understanding ("NLU") engine 130A1, and a speech synthesis engine 140A1.
The user input engine 111 is capable of detecting various types of user input at the client device 110. The user input detected at the client device 110 can include: verbal input of an additional user associated with the entity during the secondary phone call detected via a microphone of client device 110 and/or additional verbal input communicated from the additional client device to client device 110; touch input detected via a user interface input device (e.g., a touchscreen) of client device 110; and/or typing input detected through a user interface input device of client device 110 (e.g., via a virtual keyboard on a touch screen). The additional users associated with the entity can be, for example, additional human participants associated with additional client devices, additional automated assistants associated with additional client devices of the additional users, and/or other additional users.
The secondary telephone call described herein can be performed using various voice communication protocols, such as voice over internet protocol (VoIP), public Switched Telephone Network (PSTN), and/or other telephone communication protocols. As described herein, the synthesized speech can be rendered as part of the secondary phone call, which can include injecting the synthesized speech into the call such that it is perceptible to at least one participant of the secondary phone call. As also described herein, the simulated button press can be rendered as part of a secondary phone call, which can include injecting the simulated button press into the call so that it is perceivable by at least one participant of the secondary phone call. The synthesized speech can be generated and/or injected by the client device 110 as one of the endpoints of the call and/or can be generated and/or injected by a server in communication with the client device 110 and also connected to the secondary telephone call. As also described herein, the audible output can also be rendered outside of the secondary phone call, which does not include injecting the audible output into the call, but the audible output can be detected by a microphone of the client device 110 connected to the secondary phone call, and thus can be perceived at the time of the call. In some implementations, the secondary phone call can optionally be muted and/or filtering can be used to mitigate perception of audible output rendered outside of the secondary phone call in the secondary phone call.
In various implementations, the automated assistant 115 (generally indicated by the dashed line in fig. 1) can perform an auxiliary phone call at the client device 110 over the network 190 (e.g., wi-Fi, bluetooth, near field communication, local area network, wide area network, and/or other network) using the auxiliary phone call system 160. Secondary phone call system 160, in various embodiments, includes speech recognition engine 120A2, NLU engine 130A2, speech synthesis engine 140A2, and secondary phone call engine 150. In some implementations, the automated assistant 115 can navigate an Interactive Voice Response (IVR) tree of an IVR system associated with the entity on behalf of the user of the client device 110 using the secondary phone call system 160 during the secondary phone call with the additional user. In some additional or alternative implementations, automated assistant 115 can perform tasks on behalf of a user of client device 110 during a secondary phone call with an additional user using secondary phone call system 160. In some additional or alternative implementations, the automated assistant 115 can utilize the secondary phone call system 160 to navigate non-interactive systems (e.g., a waiting for answer system, a voicemail system) and/or identify whether the IVR system is malfunctioning or nothing is happening during the secondary phone call (and optionally terminate the secondary phone call and notify the user).
Further, in some implementations, after navigating the IVR tree and before performing any tasks on behalf of the user of the client device 110, the automated assistant 115 can obtain additional user consent to participate in a conversation with the automated assistant 115. For example, the automated assistant 115 can obtain consent to process audio data corresponding to the voice of the human participant when connecting with the human participant. If the automated assistant 115 obtains consent from additional users, the automated assistant 115 can verify that the target state of the IVR tree has been reached (e.g., as described below with respect to verification engine 155) and/or can perform tasks on behalf of the user (e.g., as described below with respect to task engine 157). However, if the automated assistant 115 does not obtain consent from the additional user, the automated assistant 115 can cause the client device 110 to render a notification to the user of the client device 110 (e.g., using the rendering engine 113) indicating that the user needs to take over the secondary telephone call and/or terminate the secondary telephone call and render the notification to the user of the client device 110 (e.g., using the rendering engine 113).
As described in detail below, the automated assistant 115 can perform an ancillary phone call using the ancillary phone call system 160 in response to detecting user input from a user of the client device 110 to initiate a phone call using the automated assistant 115. In some implementations, automated assistant 115 can determine, based on user input received at client device 110, a target state for navigation of an IVR tree associated with an entity to be engaged on behalf of a user of client device 110 during an assisted telephone call from among multiple candidate states. In some versions of those embodiments, the automated assistant 115 can receive a selection of the target state prior to or at the time of initiating the secondary phone call. In some additional or alternative versions of those embodiments, the automated assistant 115 can receive a selection sequence to get to the target state before or at the time of initiating the secondary phone call. The automated assistant 115 can navigate the IVR tree until the target state is reached and can cause (e.g., using the rendering engine 113) a notification to be rendered at the client device 110 that the automated assistant 115 has reached the target state. In various implementations, the automated assistant 115 can additionally perform tasks on behalf of the user when the goal state is reached, and can cause notifications to be rendered (e.g., using the rendering engine 113) at the client device 110 that include results of the performance of the tasks.
As shown in fig. 1, secondary telephone call system 160 can be implemented remotely (e.g., via a server and/or other remote client device) and communicate with client device 110. Although secondary telephone call system 160 is illustrated in fig. 1 as being implemented remotely over network 190 and in communication with client device 110, it should be understood that this is for purposes of example and is not meant to be limiting. For example, in various embodiments, secondary phone call system 160 can be implemented locally on client device 110. As another example, in various embodiments, secondary phone call system 160 can be implemented remotely at secondary phone call system 160 without any connection to client device 110 (e.g., a cloud-based secondary phone call), and client device 110 may optionally be in an off state or not connected to any network (e.g., determined using device state engine 112). Further, while the automated assistant 115 is shown in fig. 1 as being implemented locally on the client device 110 and remotely at the secondary telephone call system 160, it is to be understood that this is also for purposes of example and is not meant to be limiting. For example, in various embodiments, the automated assistant 115 can be implemented locally on the client device 110, while in other embodiments, the automated assistant 115 can be implemented locally on the client device 110 and interact with a separate cloud-based automated assistant (e.g., forming a logical instance of the automated assistant 115).
In implementations when user input engine 111 detects a spoken input by the user via a microphone of client device 110 (e.g., during an auxiliary telephone call) and/or receives audio data transmitted from an additional client device to client device 110 that captures additional spoken input from an additional user, speech recognition engine 120A1 of client device 110 can process the captured spoken input and/or capture the audio data of the additional spoken input using speech recognition model 120A. The speech recognition engine 120A1 can generate recognized text corresponding to the spoken input and/or additional spoken input based on the processing of the audio data. Further, NLU engine 130A1 of client device 110 can process the recognized text generated by speech recognition engine 120A1 using NLU model 130A to determine the intent included in the spoken input and/or the additional spoken input. For example, if the client device 110 detects a verbal input from the user that "calls Hypothertical Airline (Hypothetical Airline) to report missing baggage", the client device 110 can process audio data capturing the verbal input using the speech recognition model 120A to generate recognized text corresponding to the verbal input "calls Hypothertical Airline to report missing baggage", and can process the recognized text using the NLU model 130A to determine at least a first intent to initiate a call and a second intent to report missing baggage. As another example, if the client device 110 detects an additional spoken input that simply tells us why you are making a call, the client device 110 can process audio data that captures the additional spoken input using the speech recognition model 120A to generate recognized text corresponding to the additional spoken input that simply tells us why you are making a call, and can process the recognized text using the NLU model 130A to determine an intent to request natural language input that includes a request for information associated with why the secondary phone call was initiated (e.g., reporting lost luggage). In some versions of those embodiments, the client device 110 is capable of communicating the audio data, the identified text, and/or the intent to the secondary telephone call system 160.
In other implementations, when user input engine 111 detects spoken input by the user via a microphone of client device 110 and/or receives audio data transmitted from an additional client device to client device 110 that captures additional spoken input from an additional user (e.g., during an ancillary telephone call and/or during an ongoing call), automated assistant 115 can cause client device 110 to transmit the audio data that captures the spoken input and/or the additional spoken input to ancillary telephone call system 160. Speech recognition engine 120A2 and/or NLU engine 130A2 of secondary phone call system 160 can process audio data capturing spoken input and/or audio data capturing additional spoken utterances in a similar manner as described above with respect to speech recognition engine 120A1 and/or NLU engine 130A1 of client device 110. In some additional and/or alternative implementations, speech recognition engine 120A1 and/or NLU engine 130A1 of client device 110 can be used in a distributed manner in conjunction with speech recognition engine 120A2 and/or NLU engine 130A2 of secondary telephone call system 160. Further, speech recognition model 120A and/or NLU model 130A can be stored locally on client device 110 and/or remotely at a server in communication with client device 110 and/or secondary telephone call system 160 over network 190.
In various implementations, the speech recognition model 120A is an end-to-end speech recognition model, such that the speech recognition engines 120A1 and/or 120A2 can use the model to directly generate recognized text corresponding to the spoken input. For example, the speech recognition model 120A can be an end-to-end model for generating recognized text on a character-by-character basis (or on other token-by-token basis). One non-limiting example of such an end-to-end model for generating recognized text on a character-by-character basis is the recurrent neural network transducer (RNN-T) model. The RNN-T model is a form of sequence-to-sequence model that does not employ a mechanism of attention. In other implementations, the speech recognition model 120A is not an end-to-end speech recognition model, such that the speech recognition engines 120A1 and/or 120A2 can instead generate predicted phonemes (and/or other representations). For example, the predicted phonemes (and/or other representations) may then be used by the speech recognition engines 120A1 and/or 120A2 to determine recognized text that conforms to the predicted phonemes. In doing so, the speech recognition engines 120A1 and/or 120A2 can optionally employ decoding maps, dictionaries, and/or other resources.
In embodiments when the user input engine 111 detects touch and/or typing input via a user interface input device of the client device 110, the automated assistant 115 can cause an indication of the touch input and/or an indication of the typing input to be communicated from the client device 110 to the secondary telephone call system 160. In some versions of those embodiments, the indication of the touch input and/or the indication of the typed input can include underlying text or a representation of the touch input and/or text of the typed input, and the speech recognition model 120A is an end-to-end speech recognition model, the underlying text can be processed using the NLU model 130A to determine an intent of the underlying text and/or text.
As described herein, secondary phone call engine 150 of secondary phone call system 160 can further process recognized text generated by speech recognition engines 120A1 and/or 120A2, underlying text or representations of touch inputs detected at client device 110, underlying text of typing inputs detected at client device 110, and/or intentions determined by NLU engines 130A1 and/or 130 A2. In various embodiments, secondary telephone call engine 150 includes entity recognition engine 151, interactive Voice Response (IVR) engine 152, parameter engine 153, user interaction engine 154, verification engine 155, notification engine 156, and task execution engine 157.
In some implementations, entity identification engine 151 can identify an entity to engage on behalf of a user of client device 110 based on user interaction with client device 110 prior to initiating an ancillary telephone call using automated assistant 115. In some versions of those embodiments, the entity can be identified in response to receiving a user input for initiating the secondary telephone call. For example, if a user of client device 110 directs (e.g., verbally or by touch) input to a call interface element of a software application (e.g., associated with a contact in a contacts application, associated with search results in a browser application, and/or associated with other callable entities included in other software applications), entity identification engine 151 can identify an entity associated with the call interface element. For example, if the user input is directed to a call interface element in the browser application associated with "synthetic Airlines," the entity identification engine 151 can identify "synthetic Airlines" (or more generally, business entities or restaurant entities) as entities to be engaged on behalf of the user of the client device 110 during the secondary telephone call.
Further, the entity identification engine 151 can identify an entity from among a plurality of entities stored in the entity database 151A. In some implementations, multiple entities stored in the entity database 151A can be indexed by entity and/or a particular type of entity. For example, if the entity identification engine 151 identifies the entity "synthetic Airlines," synthetic Airlines "can be indexed as business entities in the entities database 151A, and can optionally be further indexed as airline entities. By storing and indexing the identified entities in the entity database 151A, the entity identification engine 151 is able to readily identify and retrieve entities, thereby reducing subsequent processing for identifying entities as they are encountered in future secondary telephone calls. Further, in various embodiments, each entity can be associated with a task in the entities database 151A.
In some implementations, and prior to receiving user input to initiate the secondary phone call, the automated assistant 115 (and/or additional instances of the automated assistant) can initiate automated phone calls with a plurality of entities stored in the entity database 151A. During these automated telephone calls, the automated assistant 115 (and/or additional instances of the automated assistant) can navigate the IVR systems associated with the entities to map out a corresponding IVR tree associated with each of the plurality of entities, and the mapping of the corresponding IVR trees can be stored in the IVR tree database 152A. In some versions of those embodiments, the automated assistant 115 (and/or additional instances of the automated assistant) may initiate multiple automated phone calls to a given entity of the multiple entities to map a variant of a given IVR tree associated with the given entity. For example, the automated assistant 115 may initiate a first automated telephone call with the restaurant entity when the restaurant entity is open and may initiate a second automated telephone call with the restaurant entity when the restaurant is out of business. In other words, the IVR engine 152 can cause the automated assistant 115 to map a given IVR tree associated with a restaurant entity in a different context (e.g., a context associated with a given client device or a context associated with a user) to determine whether the given IVR tree is dynamic. Thus, when IVR tree 152 identifies an IVR tree associated with an entity, an IVR tree can be identified that matches the current context of client device 110 and/or the user of client device 110 (e.g., as described with reference to fig. 7 and 8A-8D). In some additional or alternative embodiments, and prior to receiving user input to initiate the secondary telephone call, the entity can provide a representation of the IVR tree and the IVR tree can be stored in the IVR tree database 152A.
In various implementations, the automated assistant 115 can track changes over time, and optionally the degree of change, of the IVR tree based on secondary phone calls initiated in response to user input for initiating the secondary phone calls. These changes can be identified based on state differences between the stored IVR tree and the IVR tree currently being navigated by the automated assistant 115 and/or state differences between the stored audio data associated with the IVR tree and the audio data associated with the IVR tree currently being navigated by the automated assistant 115. For example, if the automated assistant 115 determines that the IVR tree associated with the given entity and traversed during the assisted telephone call is different from the IVR tree stored in the IVR tree database 152A (e.g., the IVR tree currently being navigated includes a new referral message, a different internal node or guide node, etc.), the automated assistant 115 can store a different state of the IVR tree currently being navigated along with the IVR tree already stored in the IVR tree database 152A in association with the given entity. Further, the inclusion of the new introduction message can serve as a signal that the IVR tree issued to the automated assistant 115 may include other downstream changes, and monitor those changes rather than immediately attempt to navigate the IVR tree (if possible). As a result, the automated assistant 115 may. As another example, if the automated assistant 115 determines that the audio data received during navigation of the IVR is different than the audio data stored in association with the IVR tree (e.g., based on comparing acoustic features of the received audio data and the stored audio data and/or comparing recognized text corresponding to the received audio data and the stored audio data), the automated assistant 115 can store the different audio data of the IVR tree that is to be currently navigated along with the IVR tree that has been stored in association with the given entity at the IVR tree database 152A. A different IVR tree may replace the IVR tree associated with a given entity if the difference between the stored IVR tree and the navigated IVR tree persists for a threshold number of secondary phone calls with the given entity. In some versions of those embodiments, if different IVR trees are encountered in the same context (e.g., determined based on context information associated with client device 110 and/or the user of client device 110), the IVR trees may be replaced by only considering the different IVR trees. In some versions of those embodiments, the confidence level associated with the IVR tree associated with a given entity may be based on the frequency with which the IVR tree traversed during the secondary telephone call matches the IVR tree stored in association with the IVR tree in the IVR tree database 152A. In some further versions of those embodiments, and as described with respect to fig. 6B-6E, the automated assistant 115 can render multiple candidate states in different manners based on the confidence level and whether the confidence level satisfies one or more thresholds.
In some implementations, the parameters engine 153 can identify parameters associated with navigating an IVR tree identified via the IVR engine 152 based on entities identified via the entity identification engine 151. The parameters of the IVR tree can be stored in association with the IVR tree identified via IVR engine 152 and can vary based on the navigation path used to navigate the IVR tree to the target state. The automated assistant 115 can use the corresponding values of the parameters to navigate the IVR tree. In some implementations, corresponding values of the parameters can be stored in the user profile database 153A, and the automated assistant 115 can retrieve corresponding values of the parameters from the user profile database 153A in response to identifying the parameters associated with navigating the IVR tree and without requesting any values from the user of the client device 110. In some additional or alternative versions of those embodiments, the automated assistant 115 can cause a corresponding value of the parameter to be requested from the user in response to identifying the parameter associated with navigating the IVR tree and in response to determining that the corresponding value cannot be retrieved from the user profile database 153A. For example, the automated assistant 115 can initiate a dialog with a user of the client device and cause one or more prompts to be rendered at the client device 110 via the rendering engine 113 to request corresponding values for any parameters that cannot be resolved using information from the user profile database 153A.
For example, referring briefly to FIG. 2, an example IVR tree 200 is depicted. The IVR tree 200 depicted in fig. 2 includes a plurality of nodes connected by a plurality of edges. The plurality of nodes can include, for example, at least one root node and at least one leaf node. For example, as shown in fig. 2, the IVR tree includes a root node 211, a plurality of internal nodes 221, 222, and 223, and a plurality of leaf nodes 231, 232, 233, 234, 235, and 236. The leaf nodes 231, 232, 233, 234, 235, and 236 can correspond to a plurality of candidate states, and a given one of the leaf nodes 231, 232, 233, 234, 235, and 236 can correspond to a target state. Notably, a plurality of nodes are connected by a plurality of corresponding edges. These nodes and/or edges can be stored as IVR trees (and optionally along with audio data associated with each node and/or edge) in the IVR tree database 152A. Each of the edges in the corresponding edges can be traversed based on input generated by the automated assistant 115 during the secondary phone call. For example, the corresponding edge can be traversed based on input corresponding to a numerical indicator that can be simulated as a button press by the automated assistant 115 (e.g., press "1" to traverse from the root node 211 to the interior node 1 221, press "2" to traverse from the root node 211 to the interior node 2 222, press "3" to traverse from the root node 211 to the leaf node 1 231, etc., as indicated by the edge in fig. 2), or based on input corresponding to free-form natural language input (e.g., say "book flight" to traverse from the root node 211 to the interior node 1 221, etc.).
In some implementations, the free-form natural language input can be synthesized speech audio data generated by the automated assistant 115 that includes synthesized speech to be rendered at the additional client device to traverse along the edges of the IVR tree. For example, the automated assistant can generate synthesized speech audio data that includes an indication of the above-described numerical indicator and/or a brief summary of why the secondary telephone call was placed. In some additional or alternative implementations, the free-form natural language input can be audio data that captures verbal input by a user of the client device 110 and is provided prior to initiating and/or during the secondary telephone call. For example, if the IVR tree requires the user to provide a spoken utterance to verify the identity of the user of the client device 110 (e.g., using voice recognition), the audio data can capture the user's spoken input.
In various implementations, the automated assistant 115 can track navigation of the IVR tree 200. By tracking the navigation of the IVR tree 200, the automated assistant 115 can determine when to provide input (e.g., synthesized speech, simulated button presses, and/or audio data) for navigating the IVR tree 200. For example, the automated assistant 115 can track the initiation of a secondary telephone call starting from the root node 211 and begin traversing along the edges of the IVR tree 200 to internal nodes 221, 222, and 223 and/or leaf nodes 231, 232, 233, 234, 235, and 236 to navigate the IVR tree 200 to a target state determined based on user input detected at the client device 110. As the automated assistant 115 traverses along the edges to different nodes to reach the target state, the automated assistant 115 is able to track the traversed edges and/or nodes. In this example, if the IVR tree 200 included an introductory message prior to presenting the main menu associated with the root node 211, the automated assistant 115 can determine that the introductory message is new and suppress any input for navigating the IVR tree until the main menu associated with the root node 211 is presented (or any other changes are presented). As another example, the automated assistant 115 can determine that any input to be provided to navigate the IVR tree to the target state may not be provided until the IVR system provides all options for navigating the IVR tree (e.g., the IVR tree does not support providing input at any time). In this example, the automated assistant 115 may withhold any input to navigate the IVR tree until all options for navigating the IVR tree are presented. In some implementations, the automated assistant 115 can return to the previous node traversed during navigation of the IVR tree 200.
As a non-limiting Example, assume that the IVR tree 200 corresponds to an IVR tree associated with a hypothetical entity, example Airlines. In this Example, the root node 211 may correspond to the main menu of the IVR system associated with Example Airlines, the internal nodes may correspond to the intermediate states 221, 222, and 223, and the leaf nodes 231, 232, 233, 234, 235, and 236 may correspond to the target states of the IVR tree. Further assume that leaf node 1 231 is associated with a target status of "with standing for talk", leaf node 2 232 is associated with a target status of "upcoming flight information", leaf node 3 233 is associated with a target status of "report missing baggage", leaf node 4 234 is associated with a target status of "reserve flight", leaf node 5 235 is associated with a target status of "change flight reservation", and leaf node 6 236 is associated with a target status of "cancel flight reservation". In some implementations, the target state to which the automated assistant 115 navigates the IVR tree during the secondary telephone call can be determined based on user input detected via the user input engine 111 of the client device 110 prior to initiating the secondary telephone call and/or during the secondary telephone call.
For example, assume that a user input is received at client device 110 indicating that the user wants to report missing baggage. In this example, the automated assistant 115 can determine to navigate to the leaf node 3 233 associated with the target state of "report missing baggage". In this example, the automated assistant 115 may know to traverse from the root node 211 to interior node 1 221, and from interior node 1 221 to leaf node 3 233 to report the missing baggage because the IVR tree 200 was previously mapped by the automated assistant 115 (and/or additional automated assistants), as described above with respect to the IVR engine 152. Upon navigating from the root node 211 to leaf node 3 233 and/or upon reaching leaf node 3 233, the automated assistant 115 can cause corresponding values of the parameters requested while navigating the IVR tree 200 to be rendered at additional client devices associated with the entity. In this example, the automated assistant 115 can render, at additional client devices associated with the entity, corresponding values of flight information (e.g., flight confirmation number, departure destination, arrival destination) associated with the most recent flight, personal information (e.g., name and phone number) of the user of the client device 110, and/or other corresponding values of the parameter in response to the corresponding values of the parameter being requested. Upon reaching leaf node 3 233, automated assistant 115 can optionally generate (e.g., via notification engine 156) a notification to be rendered (e.g., via rendering engine 113) at client device 110 requesting that the user join the secondary phone call and/or alerting the user of client device 110 to report the results of the missing baggage.
As another example, assume that a user input is received at the client device 110 indicating that the user wants to cancel the lost subscription. In this example, the automated assistant 115 can determine to navigate to the leaf node 6 236 associated with the target status of "report missing baggage". In this example, the automated assistant 115 may know to traverse from the root node 211 to the interior node 1 221, from the interior node 1 to the interior node 3 223, and from the interior node 3 223 to the leaf node 6 236 to cancel the flight reservation because the IVR tree 200 was previously mapped by the automated assistant 115 (and/or additional automated assistants), as described above with respect to the IVR engine 152. Upon navigating from the root node 211 to the leaf node 6 236 and/or upon reaching the leaf node 6 236, the automated assistant 115 can cause corresponding values of the parameters requested while navigating the IVR tree 200 to be rendered at additional client devices associated with the entity. In this Example, the automated assistant 115 can render, at additional client devices associated with the entity, corresponding values of flight information associated with the upcoming flight (e.g., flight confirmation number, departure destination, arrival destination), account information associated with the Example Airlines account (e.g., frequent flyer number), and/or other corresponding values of the parameter in response to the corresponding values of the parameter being requested. Upon reaching the leaf node 6 236, the automated assistant 115 can optionally generate (e.g., via the notification engine 156) a notification to be rendered at the client device 110 (e.g., via the rendering engine 113) requesting the user to join an auxiliary phone call and/or alerting the user of the client device 110 of the results of the cancellation of the flight.
Although the IVR tree 200 of fig. 2 is depicted as having particular nodes that are configured in a particular manner based on corresponding edges, it should be understood that this is for purposes of example and is not meant to be limiting, and it should be understood that the IVR tree can be configured in a nearly limitless variety of ways. Further, some nodes of IVR tree 200 may or may not be available to users of client devices 110 based on certain contexts (e.g., as described with respect to fig. 7 and 8A-8D). For example, assume that a given IVR tree is associated with a restaurant entity. In this example, the goal state associated with placing the take-away order may be available when the restaurant entity is open. In contrast, the goal state associated with placing a take order may not be available when the restaurant entity goes out of business.
Referring back to fig. 1, the user interaction engine 154 is capable of detecting user interactions at the client device 110. The user interactions can include, for example, search interactions with a web browser or web browser application, browsing interactions with a web browser or web browser application, navigation interactions with a web browser or navigation application, assistant interactions with the automated assistant 115, and/or other user interactions. In some implementations, the user interaction engine 154 can detect user interactions at the client device 110 by a user of the client device 110 or at an additional client device in communication with the client device 110 (e.g., over the network 190) after the automated assistant 115 initiates execution of the secondary telephone call. In some versions of those implementations, automated assistant 115 may determine whether the detected user interaction is associated with an entity engaged on behalf of the user of client device 110 during the secondary phone call. If the automated assistant 115 determines that the detected user interaction is associated with an entity, the automated assistant 115 can generate one or more prompts (e.g., via the notification engine 156) that ask the user of the client device 110 whether the navigation of the IVR tree should be modified. In some further versions of those embodiments, modifying the navigation of the IVR tree may include navigating to a different target state. For example, if the automated assistant 115 initiates an assisted telephone call to navigate an IVR tree associated with the home-decoration entity to navigate the IVR tree to a target state associated with a finishing department of the home-decoration entity, and the user of the client device 110 subsequently interacts with a web page associated with the home and gardening departments of the home-decoration entity, the automated assistant 115 may generate one or more prompts asking the user whether to additionally or alternatively navigate to the target state associated with the home and gardening departments. In other further versions of those embodiments, modifying the navigation of the IVR tree may include navigating to the same IVR tree with different parameters. For example, if the automated assistant 115 initiates an assisted telephone call to navigate an IVR tree associated with a home-decoration entity to navigate the IVR tree to a target state associated with a painting department of the home-decoration entity, and the user device 110 of the client subsequently interacts with a web page associated with a particular paint manufacturer, the automated assistant 115 may generate one or more prompts asking the user whether to utilize the particular paint manufacturer as a corresponding value for a parameter when navigating to the target state associated with the painting department. The navigation of the modified IVR tree is described in detail below (e.g., with reference to fig. 3 and 4A-4C).
In some additional or alternative implementations, the user interaction engine 154 can detect user interactions at the client device 110 by a user of the client device 110 or at additional client devices in communication with the client device 110 (e.g., over the network 190) before the automated assistant 115 initiates execution of the secondary telephone call. In some versions of those embodiments, the automated assistant 115 may identify an entity associated with the user interaction and may cause rendering of the candidate state of the IVR tree based on the user interaction if an auxiliary phone call is subsequently initiated with the identified entity. For Example, if a user is browsing a web page associated with a hypothetical airline entity — Example airfils, researching how to report missing baggage prior to initiating a secondary phone call with Example airfils, the automated assistant 115 may render (alone or more prominently than other selectable elements) a selectable graphical element associated with a target state associated with reporting missing baggage of an IVR tree associated with Example airfils. In other words, based on the user studying how to report missing baggage prior to initiating the secondary phone call, the automated assistant 115 can cause the rendering of the selectable graphical elements as "IVR deep links" to the target states of the IVR tree (e.g., as described with respect to fig. 5 and 6A).
In some implementations, the verification engine 155 can verify whether the state to which the automated assistant 115 navigated during the secondary phone call is actually the target state. In some versions of these implementations, when a human representative associated with the entity joins the secondary phone call, the automated assistant 115 can process the received audio data that captures verbal input of the human representative to verify that the state navigated to by the automated assistant 115 is in fact the target state. For example, assume that the automated assistant 115 is navigating an IVR tree associated with a home-adornment entity to a target state associated with a paint department of the home-adornment entity. Further assume that a human representative takes over an ancillary phone call from an IVR system associated with the home decoration entity and provides "thank you for the e-coating department, what do i can you do today? "oral input. In this example, the verification engine 155 can cause the automated assistant 115 to process audio data capturing the spoken input using the speech recognition engines 120A1 and/or 120A2, and can determine that the spoken input includes "paint department" based on processing the audio data capturing the spoken input. In some additional or alternative versions of these embodiments, verbal input may be received in response to synthesized speech audio data comprising synthesized speech to verify a target state of the navigated IVR tree. Continuing with the above example, assume that a human representative takes over an auxiliary phone call from an IVR system associated with the home-decoration entity and provides verbal input of "Hello". In this example, the verification engine 155 can cause the automated assistant 115 to render the synthesized speech "hello, which is the paint department? ". The verification engine 155 can cause the automated assistant 115 to process audio data that captures spoken input in response to the synthesized speech using the speech recognition engines 120A1 and/or 120A2 and can determine whether the spoken input verifies that the target state is reached (e.g., "yes," "this is a paint department," or some other confirmation).
In various implementations, if the state reached while navigating the IVR does not correspond to the target state, the verification engine 155 can cause the automated assistant 115 to render, at an additional client device associated with the human representative, synthesized speech requesting that the secondary telephone call be transferred to the target state. Continuing with the example above, if it is determined that the verbal input indicates that the automated assistant 115 has reached a status associated with the home and gardening department (rather than the paint department), the verification engine 155 can cause the automated assistant 115 to render a synthesized voice "can you transfer me to the paint department" on an additional client device associated with a human representative? ", and can again verify that the state after the transition corresponds to the desired target state of the user of client device 110.
The notification engine 156 can generate various notifications based on navigating the IVR tree associated with the identified entity and/or based on tasks performed by the automated assistant 115 on behalf of the user of the client device 110 during the secondary phone call without suspending the secondary phone call. For example, the automated assistant 115 can cause the notification engine 156 to generate a notification requesting the user of the client device 110 to join the secondary phone call when the target state is reached. In some implementations, the notification engine 156 can cause the automated assistant 115 to generate the notification requesting the user of the client device 110 to join the secondary phone call only after the verification engine 155 verifies that the automated assistant has actually reached the target state as described above with respect to the verification engine 155. As another example, if the automated assistant 115 is unable to resolve corresponding values of parameters during an assisted call, the automated assistant 115 can cause the notification engine 156 to generate one or more prompts to request continued navigation of corresponding values of any unresolved parameters needed for the IVR tree associated with the entity. As yet another example, if the automated assistant performs a task on behalf of the user of the client device 110 during the secondary phone call, the automated assistant 115 can cause the notification engine 156 to generate a notification that includes a result of the performance of the task.
In some implementations, the type of notification generated by notification engine 157 and rendered at client device 110 via rendering engine 113 and/or one or more attributes (e.g., volume, brightness, size) of the rendered notification can be based on the state of client device 110 (e.g., determined using device state engine 112) and/or the state of the ongoing secondary telephone call. The state of the ongoing secondary phone call can be based on, for example, a software application running in the foreground of the client device 110, a software application running in the background of the client device 110, whether the client device 110 is in a locked state, whether the client device 110 is in a dormant state, whether the client device 110 is in an off state, sensor data generated by sensors of the client device 110, and/or other data associated with the client device 110. For example, if the status of the client device 110 indicates that a software application (e.g., an automated assistant application, a telephone application, a secondary phone call application, and/or other software application) displaying a transcription of a secondary phone call is running in the foreground of the client device 110, the type of notification may be a banner notification, a pop-up notification, and/or other type of visual notification. As another example, if the state of the client device 110 indicates that the client device 110 is in a dormant or locked state, the type of notification may be an audible indication via a speaker and/or a vibration via a speaker or other hardware component of the client device 110 that mimics a phone call. As yet another example, more intrusive notifications (e.g., visual and audible at a first volume level) can be provided if sensor data from presence sensors, accelerometers, and/or other sensors of the client device indicate that the user is not currently nearby and/or currently holding the client device. On the other hand, if such sensor data indicates that the user is currently nearby and/or currently holding the client device, less intrusive notifications can be provided (e.g., visual only, or visual and audible at a second volume level that is less than the first volume level). As yet another example, more intrusive notifications can be provided when the state of the dialog indicates that the dialog is nearing completion, while less intrusive notifications can be provided when the state of the dialog indicates that the dialog is not nearing completion.
Further, in implementations in which the automated assistant 115 causes a notification to be rendered at the client device 110 requesting the user to join the secondary phone call, but the user does not join the secondary phone call within a threshold duration (e.g., 15 seconds, 30 seconds, 60 seconds, and/or other durations), the automated assistant can cause the secondary phone call to terminate. In some versions of these embodiments, the threshold duration can begin when a notification is rendered on the client device 110 requesting that the user join the secondary telephone call. In some additional or alternative versions of those embodiments, and prior to terminating the secondary telephone call, the state of the secondary telephone call can be stored in one or more databases of the client device 110 (e.g., random access memory of the client device). If the user of the client device 110 subsequently interacts with the notification to join the terminated secondary telephone call, the automated assistant 115 can reinitiate execution of the secondary telephone call by loading the stored state of the secondary telephone call such that the secondary telephone call is resumed in a given state that corresponds to the stored state of the secondary telephone call. In this manner, the automated assistant 115 does not need to navigate the portion of the IVR tree that has been navigated by the automated assistant 115.
In various implementations, and after the automated assistant 115 navigates to a target state of the IVR tree associated with the entity, the automated assistant is able to perform tasks on behalf of the user of the client device 110. In some versions of these embodiments, task engine 157 can determine the task to be performed on behalf of the client device 110 user based on user input detected at the client device 110 to initiate the secondary telephone call or based on other user input detected at the client device 110 prior to the automated assistant 115 initiating the secondary telephone call. For Example, if the user of the client device 110 provides verbal input "call Example Caf for a tonight booking", the task engine 157 can utilize the intent to initiate a call and make a restaurant booking (e.g., as determined using the NLU model 130A) to determine a task for making a restaurant booking based on the verbal input. The parameters described herein that are determined by the parameter engine 153 can also identify parameters of the task identified by the task engine 157 and determine corresponding values for the parameters of the task in a similar manner as described above with respect to the parameter engine 153.
The task engine 157 can cause the automated assistant 115 to engage in a conversation with a human representative associated with the identified entity during the secondary phone call to perform a task. For example, the task engine 157 can provide text and/or phonemes based on corresponding values of the parameters to the speech synthesis engines 140A1 and/or 140A2 to generate synthesized speech audio data. The synthesized speech audio data can be communicated to an additional client device of the human representative for audible rendering at the additional client device. For example, the speech synthesis engines 140A1 and/or 140A2 can determine a sequence of phonemes determined to correspond with values of parameters requested by the human representative and can process the sequence of phonemes using the speech synthesis model 140a to generate synthesized speech audio data. The synthesized speech audio data can be, for example, in the form of audio waveforms. In determining the phoneme sequence corresponding to the value of the parameter, the speech synthesis engines 140A1 and/or 140A2 can access a token-to-phoneme mapping stored locally at the client device 110 or stored at a server (e.g., over the network 190). The task engine 157 can cause the speech recognition engines 120A1 and/or 120A2 to process any audio data that captures spoken input provided by a human representative, and then the automated assistant 115 can generate further synthesized speech audio data using the speech synthesis engines 140A1 and/or 140 A2.
As described herein, the rendering engine 113 is capable of rendering various notifications or other outputs at the client device 110. The rendering engine 113 can audibly and/or visually render the various notifications described herein. Further, the rendering engine 113 can cause a transcription of the conversation to be rendered on a user interface of the client device 110. In some implementations, the transcription can correspond to a conversation between a user of the client device 110 and the automated assistant 115. In some additional or alternative implementations, the transcription can correspond to a conversation between the IVR system associated with the identified entity and the automated assistant 115. In other implementations, the transcription can correspond to a conversation between a human representative associated with the identified entity and the automated assistant 115.
In some implementations, the user input detected at the client device 110 via the user input engine 111 can be a search query received via a search interface of the client device 110. For example, the search interface can be implemented as part of a phone or contacts application, a browser application, an automated assistant application, a web-based browser, and/or any other interface that enables search functionality. Search engine 114 is capable of identifying and retrieving search results rendered at client device 110 via rendering engine 113. In some versions of these embodiments, the search results can include content responsive to the search query, and can optionally include one or more selectable elements. The one or more selectable elements can include, for example, deep links to one or more candidate states of the IVR tree associated with the entity, and which, when selected based on additional user input (e.g., touch, typing, and/or oral), cause the automated assistant 115 to navigate to a target state of the IVR tree corresponding to the selected candidate state of the IVR tree included in the search results. For example, if a user of client device 110 submits a search query for "home store" via a search interface of a telephony application, selectable elements associated with various departments corresponding to one or more candidate states of an IVR tree associated with the home store can be included in search results responsive to the search query. Further, if the user has recently interacted with other search results associated with "paint" (e.g., as described above with respect to the user interaction engine 154), a particular selectable element (e.g., "paint department") may be rendered more prominently than other selectable elements.
By using the techniques described herein, various technical advantages can be realized. As one non-limiting example, the automated assistant 115 can end the secondary phone call faster because the automated assistant does not need to hear the IVR system's options before navigating the IVR tree because it is stored in association with the entity. Network and computing resources can be saved because the duration of the secondary telephone call can be reduced by using the techniques disclosed herein. Furthermore, the number of user inputs can be reduced, and thus client device resources can be conserved, as a given user need only select an "IVR deep link" in various embodiments to initiate performance of a secondary telephone call. Furthermore, battery resources of client device 110 may also be reserved because the user is not an active participant in the secondary phone call, and client device 110 may be placed in a locked or low-power state during the secondary phone call.
Turning now to fig. 3, a flow diagram is depicted illustrating an example method 300 of modifying navigation of an IVR tree based on user interaction at a client device associated with a given user. For convenience, the operations of method 300 are described with reference to a system that performs the operations. The system of method 300 includes one or more processors and/or other components of a computing device (e.g., client device 110 of fig. 1, 4A-4D, 6A-6E, and/or 8A-8D, and/or computing device 910 of fig. 9, one or more servers, and/or other computing devices). Further, while the operations of method 300 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 352, the system receives user input from a given user for initiating a secondary telephone call via a client device associated with the given user. The user input for initiating the secondary telephone call can be one or more of a verbal input for initiating the secondary telephone call, a touch input for initiating the secondary telephone call, or a key input for initiating the secondary telephone call. Further, user input for initiating the secondary phone call can be received at various interfaces (e.g., a search interface, a voice interface, an automated assistant interface, and/or other interfaces from which the secondary phone call can be initiated).
At block 354, the system identifies entities to engage on behalf of a given user during an assisted telephone call based on user input. The system can identify an entity to engage during the secondary telephone call based on the user input received at block 352 and/or user interaction with the client device immediately prior to initiating the secondary telephone call session. For Example, the system can identify the entity of "Example Airlines" based on processing the verbal input received at the client device "report me missing luggage to Example Airlines". As another Example, the system can identify an entity of "Example Airlines" based on determining that a given user of the client device has selected a call interface element associated with Example Airlines or a particular graphical element associated with a candidate state of an IVR tree associated with Example Airlines.
At block 356, the system identifies an IVR tree associated with the identified entity, the IVR tree including a plurality of candidate states. The IVR tree associated with the identified entity can be stored in association with the entity identified at block 354 (e.g., in the entity database 151A of fig. 1 and/or in the IVR tree database 152A of fig. 1). The stored IVR trees associated with the entities may previously be stored in one or more databases based on the system (and/or other instances of the system) previously crawling the IVR systems associated with the identified entities, and/or the identified entities may be able to provide the IVR trees for the IVR systems associated with the identified entities. Further, the system can continuously update the IVR tree of the IVR system associated with the identified entity based on the plurality of secondary telephone calls navigating the IVR tree. The plurality of candidate states can correspond to particular nodes of the IVR tree to which the system can navigate during the secondary telephone call.
At block 358, the system determines a target state of the IVR tree from among a plurality of candidate states based on user input or additional user input. In some implementations, the target state can be determined based on the user input received at block 352. For Example, assume that the IVR tree is associated with an entity of Example Airlines, and that a given candidate state for IVR corresponds to a "report missing baggage" candidate state for the IVR tree associated with an entity of Example Airlines. Further assume that a verbal input "report me missing luggage to Example aircines" is received at the client device at block 352. In this example, a target state of "report lost baggage" can be identified based on the verbal input. In some additional or alternative implementations, the target state can be determined based on additional user input in addition to the user input received at block 352. For Example, assume that the IVR tree is associated with an entity of Example Airlines, and that a given candidate state of IVR corresponds to a "missing luggage" candidate state of the IVR tree associated with an entity of Example Airlines. Further assume that a verbal input of "Call Example Airlines" is received at the client device at block 352, or a selection of a selectable graphical element associated with the Call Example Airlines is received at the client device at block 352. In this example, one or more of the plurality of candidate states can be presented as output at the client device in response to receiving the user input, and the target state can be based on selecting a "report missing baggage" target state from among the one or more of the plurality of candidate states presented. The output may be presented prior to initiating the secondary telephone call or upon initiating the secondary telephone call.
At block 360, the system initiates execution of the secondary phone call to navigate the IVR tree to the target state. The system can initiate execution of the secondary phone by establishing a communication session between the client device associated with the given user and the additional client devices associated with the entities identified at block 354. The communication session can be established using various voice communication protocols including, for example, voIP, PSTN, and/or other telephony communication protocols. In some implementations, the system can navigate the IVR tree to a target state by simulating a button press to traverse a node of the IVR tree, by rendering synthesized speech audio data including synthesized speech to traverse a node of the IVR tree, and/or by rendering audio data including verbal input of a given user associated with the client device (e.g., as described with respect to fig. 2). In some versions of those embodiments, the system is also capable of utilizing corresponding values of parameters associated with navigating the IVR tree. The corresponding values of the parameters can be retrieved from one or more databases (e.g., user profile database 153A of fig. 1) and/or can be requested from a given user associated with the client device.
At block 362, the system determines whether a user interaction is detected at the client device. User interactions can include, for example, search interactions with a web browser or web browser application, browsing interactions with a web browser or web browser application, navigation interactions with a web browser or navigation application, assistant interactions with the automated assistant 115, and/or other user interactions with a client device. Notably, when the system performs a secondary telephone call, a given user associated with the client device may still interact with the client device such that the secondary telephone call is performed in the background of the client device. If, at an iteration of block 362, the system determines that no user interaction is detected at the client device, the system may proceed to block 370. Block 370 is described in detail below. If, at an iteration of block 362, the system determines that a user interaction is detected at the client device, the system may proceed to block 364.
At block 364, the system determines whether the user interaction detected at block 362 is associated with the entity identified at block 354. For example, the system can determine whether the user interaction is directed to a web page associated with the entity identified at block 354, to a direction to obtain an entity store associated with the entity identified at block 354, and/or to other user interactions of the entity. If, at an iteration of block 364, the system determines that the user interaction detected at block 362 is not associated with the entity identified at block 354, the system may proceed to block 370. Block 370 is described in detail below. If at the iteration of block 364 the system determines that the user interaction detected at block 362 is associated with the entity identified at block 354, the system may proceed to block 366.
At block 366, the system generates one or more prompts related to modifying navigation of the IVR tree based on the user interactions detected at block 362. In some implementations, modifying the navigation of the IVR tree can include modifying a target state for navigating the IVR tree to an alternate target state. For example, assume that an auxiliary phone call is initiated to navigate to a target state associated with a floor department of an IVR tree associated with a home entity, and further assume that a user interaction directed to a portion of a web page associated with a lighting department of the home entity is detected. In this example, the system can prompt a given user to change the goal state from being associated with the floor department to an alternate goal state associated with the lighting department.
In some additional or alternative embodiments, modifying the navigation of the IVR tree can include adding additional target states for navigating the IVR tree. Continuing with the example above, rather than prompting the given user to change the target status from being associated with the floor department to an alternate target status associated with the lighting department, the system can prompt the given user to add the lighting department as an additional target status in addition to the target status associated with the floor department. In this example, the user may be notified to join the secondary phone call upon reaching a target status associated with the floor department. After the given user interacts with the floor department, the given user may terminate the secondary telephone call. However, the system can keep the secondary phone call connected and can request that a human representative associated with the floor department transfer the secondary phone call to the lighting department, or the system can re-navigate the IVR tree to additional target states associated with the lighting department.
In some additional or alternative embodiments, modifying the navigation of the IVR tree can include modifying corresponding values of parameters used to navigate the IVR tree. For example, assume that an auxiliary phone call is initiated to navigate to a target state associated with a lighting department of a home appliance entity and an IVR tree associated with the home appliance entity, and further assume that a user interaction directed to a portion of a web page associated with a particular brand of smart light bulb sold at the home appliance entity is detected. In this example, the system can prompt the given user to include a corresponding value associated with a particular brand of smart light bulb for a parameter associated with navigating to a target state associated with the lighting department. Further, the system can cause one or more prompts to be visually and/or audibly rendered at a client device associated with a given user.
At block 368, the system determines whether to modify navigation of the IVR tree based on additional user input received in response to the one or more prompts. For example, if the user responds affirmatively to one or more prompts rendered at the client device, the navigation of the IVR tree can be modified based on the user interaction detected at block 362. However, if the user does not respond affirmatively (or responds negatively) to one or more prompts, the navigation of the IVR tree may not be modified. If at the iteration of block 368 the system determines not to modify the navigation of the IVR tree based on additional user input, the system may proceed to block 370.
At block 370, the system navigates the IVR tree to a target state. In embodiments where the system proceeds from blocks 362, 364, and/or 368 to block 370, the system can continue to navigate the IVR tree based on the target state determined at block 358. The system can navigate the IVR tree based on the simulated button press navigating the IVR tree such that synthesized speech audio data including the synthesized speech is rendered at the additional client device associated with the entity identified at block 354 and/or such that audio data including the spoken input is rendered at the additional client device associated with the entity identified at block 354. If at the iteration of block 368 the system determines to modify the navigation of the IVR tree based on additional user input, the system may proceed to block 372. At block 372, the system modifies the corresponding values of the parameters used to navigate the IVR tree or modifies the target state to additionally include additional target states or alternatively include alternative target states, as described above with respect to block 366. Navigating the IVR tree is described above with respect to fig. 2.
From either block 370 or block 372, the system may proceed to block 374. At block 374, the system can verify whether the secondary phone call has reached the target state. The system can process verbal input received at the client device from a human representative associated with the entity that has joined the call or an additional automated assistant of the IVR system associated with the entity to determine whether the system has reached the target state. For example, assume that the target state of the IVR tree is associated with a lighting department of a home decoration store entity. Further assume that a human representative or additional automated assistant provides "thank you for an electro-lighting department gate, i can help you do what? "oral input. In this example, the system can determine that the verbal input includes the term "lighting department" to verify that the system has actually reached the desired target state for a given user associated with the client device. In some implementations, the spoken input received at the client device (e.g., "hello, which is lighting department" or other synthesized speech requesting verification of the target state) may be provided in response to the system requesting the spoken input via synthesized speech rendered at an additional client device associated with the human representative or the additional automated assistant. If at an iteration of block 374 the system determines that the secondary phone call has reached another state than the target state, the system may proceed to block 376. At block 376, the system requests that the secondary phone call be transferred to a representative associated with the target state. The request to transfer the secondary phone call can be included in a synthesized voice rendered at an additional client device associated with the human representative or the additional automated assistant (e.g., "do you can transfer me to the lighting department" or other synthesized voice requesting that the secondary phone call be transferred). The system may then return to block 374 to verify whether the representation associated with the target state corresponds to the target state after the transition. If at an iteration of block 374 the system determines that the secondary phone call has reached the target state, the system may proceed to block 378.
At block 378, the system renders, via the client device, a notification indicating a result of the execution of the navigating IVR tree. For example, the notification can include a request to a given user associated with the client device to join the secondary phone call. In embodiments described herein in which the system also performs tasks on behalf of a given user associated with the client device and with respect to the entity, the notification can additionally or alternatively include results of the performance of the task.
Referring now to fig. 4A-4C, various non-limiting examples of user interfaces associated with modifying navigation of an IVR tree based on user interaction at a client device 110 associated with a given user are depicted. Fig. 4A-4C each depict a client device 110 having a graphical user interface 180, and may include one or more components of the client device of fig. 1. One or more aspects of an automated assistant associated with the client device 110 (e.g., automated assistant 115 of fig. 1) can be implemented locally on the client device 110 and/or in a distributed manner (e.g., via network 190 of fig. 1) on other client devices in network communication with the client device 110. For simplicity, the operations of fig. 4A-4C are described herein as being performed by an automated assistant. While the client device 110 of fig. 4A-4C is depicted as a mobile phone, it should be understood that this is not intended to be limiting. Client device 110 can be, for example, a standalone assistant device (e.g., with speakers and/or a display), a laptop computer, a desktop computer, an in-vehicle computing device, and/or any other client device capable of making telephone calls.
The graphical user interface 180 of fig. 4A-4C further includes: a text reply interface element 184 that the user may select to generate user input via a virtual keyboard or other touch and/or typing input; and a voice reply interface element 185 that the user may select to generate the user input via the microphone of the client device 110. In some implementations, the user can generate the user input via the microphone without selecting the voice reply interface element 185. For example, active monitoring of audible user input via a microphone may be performed to avoid the need for a user to select voice reply interface element 185. In some of these embodiments and/or others, the voice reply interface element 185 may be omitted. Further, in some implementations, the text reply interface element 184 may additionally and/or alternatively be omitted (e.g., the user may only provide audible user input). The graphical user interface 180 of fig. 4A-4C also includes system interface elements 181, 182, 183 that may be interacted with by a user to cause the computing device 110 to perform one or more actions.
In various implementations described herein, user input can be received to initiate a telephone call (e.g., an auxiliary telephone call) with an entity using an automated assistant. The user input can be verbal, touch, and/or key input including an indication to initiate the secondary telephone call. Further, the automated assistant can navigate an IVR tree associated with an entity to a target state from among a plurality of candidate states on behalf of a given user of the client device 110 and/or perform a task with respect to the entity. As shown in fig. 4A, the user interface 180 includes search results for a grocery store entity from a browser application accessible at the client device 110 (e.g., as indicated by the URL 411 of "www. In addition, the search results include a first search result 420 of "Hypothetical Grocenter" located at the first location, and a second search result 430 of "Hypothetical Grocenter" located at the second location.
In some implementations, the search results 420 and/or 430 can be associated with various selectable graphical elements that, when selected, cause the client device 110 to perform corresponding actions. For example, when the call graphical element 421 and/or 431 associated with a given one of the search results 420 and/or 430 is selected, the user input can indicate that a phone call action should be performed to the grocery store entity associated with the search results 430 and 420. As another example, when the directions graphical element 422 and/or 432 associated with a given one of the search results 420 and/or 430 is selected, the user input can indicate that a navigation action should be performed with respect to the grocery store entity associated with the search results 430 and 420. As another example, when a department graphical element 423 and/or 433 associated with a given one of search results 420 and/or 430 is selected, the user input can indicate that a browser-based operation should be performed to display the department of the grocery entity associated with search results 420 and/or 430. Although the secondary phone call is initiated from the browser application in fig. 4A, it should be understood that this is for purposes of example and is not intended to be limiting. For example, if the secondary phone call is initiated from a home screen of the client device 110, from a lock screen of the client device 110, and/or other state of the client device 110 using verbal input, the secondary phone call can be initiated from various software applications accessible at the client device 110 (e.g., an automated assistant application, a phone application, a contacts application, an email application, a text or SMS messaging application, and/or other software applications).
By way of example, assume that user input is detected at the client device 110 to initiate a telephone call using the first search result 420 of "contextual Grocer" at a first location. For example, the user input can be a verbal input of "call advanced character Grocer" (and selected based on the proximity of the first location to the client device 110) or a touch input directed to the call graphical element 421, as indicated by 490 A1. In some implementations, the call detail interface 470 can be rendered at the client device 110 prior to receiving user input to initiate a telephone call with "Hypothetical Grocer," or can be rendered at the client device 110 in response to receiving user input to initiate a telephone call with "Hypothetical Grocer" (as indicated by 490A 1). In some versions of those embodiments, the call details interface 470 can be rendered at the client device 110 as part of the user interface 180. In some other versions of those embodiments, the call details interface 470 can be an interface separate from the user interface 180 that overlays the user interface 180, and can include a call details interface element 186 that allows a user to expand the call details interface 470 to display additional call details (e.g., by sliding up on the call details interface element 186) and/or close the call details interface 470 (e.g., by sliding down on the call details interface element 186). Although the call details interface 470 is depicted as being located at the bottom of the user interface 180, it should be understood that this is for purposes of illustration and is not intended to be limiting. For example, the call details interface 470 can be rendered on the top of the user interface 180, on one side of the user interface 180, or an interface completely separate from the user interface 180.
In various embodiments, the call details interface 470 can include a plurality of graphical elements. In some versions of these embodiments, the graphical elements can be selectable such that when a given one of the graphical elements is selected, the client device 110 can perform a corresponding action. As shown in FIG. 4A, the Call detail interface 470 includes a First graphical element 471A for "Assisted Call, hypothetical grocery, first Location," a second graphical element 472A for "Regular Call," and a third graphical element 473A for "Save Contact 'Hypothetical grocery' (Save Contact 'Hypothetical grocery')". Notably, in some versions of those embodiments, the graphical element can include sub-elements to provide an indication of candidate states for navigating the IVR tree associated with the "contextual Grocer". For example, the first graphic element 471A can include: a first child element 471A1 of "product Department" associated with navigating the IVR tree associated with the hypokinetic Grocer to a target state of a portion corresponding to the agricultural Department; a second child element 471A2 of "Bakery Department" associated with navigating the IVR tree associated with the contextual Grocer to a target state corresponding to a portion of the baking Department; and a third sub-element 471A3 of "Deli Department" associated with navigating the IVR tree associated with the generic Grocer to a target state corresponding to a portion of the delicatessen Department.
For example, assume that user input is detected at the client device 110 to initiate execution of an auxiliary telephone call with the synthetic Groceter to navigate an IVR tree associated with the synthetic Groceter to a target state associated with the agricultural sector of the synthetic Groceter. The user input can be, for example, a verbal input such as "call agricultural product department of financial Groceer" or a touch input directed to the first sub-element 471A1, as indicated by 490A 2. In response to detecting the user input, the automated assistant can determine a target state for "product Department" for navigating the IVR tree associated with the Hypothetical Grocer. In some implementations, the automated assistant can identify corresponding values of parameters associated with navigating the IVR tree to a target state and/or corresponding values of tasks to be performed when the target state associated with the agricultural sector is reached, as described herein (e.g., with respect to parameter engine 153 of fig. 1 and with respect to fig. 2).
In some implementations, and as shown in fig. 4B, a given user of the client device 110 may still be able to interact with the client device 110 while the automated assistant is navigating the IVR tree to a target state during the secondary phone call. In some versions of those embodiments, user interactions detected at the client device 110 while the automated assistant is navigating the IVR tree may be used to modify the navigation of the IVR tree. For example, and as shown in fig. 4B, assume that a given user associated with client device 110 views a web page associated with the first search result 420 of "contextual Grocer" (e.g., as indicated by "www. In particular, the web page may include selectable graphical elements corresponding to information related to various departments of the contextual Grocerer. For example, a given user associated with the client device 110 may be able to select a first graphically selectable element 471B1 to view agricultural availability at the product department of the economic grow, a second graphically selectable element 471B2 to view cake availability at the bakery department of the economic grow, or a third graphically selectable element 471B3 to view meat availability at the deli department of the economic grow, or any other graphically selectable element that may be presented.
For purposes of example, when the automated assistant is navigating the IVR tree associated with the Hypothetical Groceger to a target state associated with the department of agricultural products, assume that the user selects the third graphical selectable element 471B3 to view meat availability at the delicatessen department of the Hypothetical Groceger, as indicated by 490B 1. In response to determining that a given user associated with the client device 110 is interacting with search results related to the delicatessen department of the Hypothecal Grocerer while the automated assistant is navigating the IVR tree associated with the Hypothecal Grocenter to a target state that is not associated with the meat department, the automated assistant can generate one or more prompts and render the one or more prompts as notifications at the user interface 180 of the client device 110. For example, the automated assistant can cause the notification 479 to be visually rendered in the call details interface 470. Notification 479 includes an indication of "wooden you ratio contact with the deli specific equipped telephone call" and also includes a first suggestion of "yes" 479B1 and a second suggestion of "no" 479B2. Assume further that, as shown in fig. 4B, and in response to rendering notification 479, additional user input is received at client device 110 indicating that a given user of client device 110 actually prefers to contact the delicatessen department rather than the agricultural department as originally intended (e.g., selecting first suggestion 479B1 of "yes," as indicated by 490B 2). In this example, the navigation of the IVR tree associated with the contextual Grocerer can be modified based on user interaction with the web page at the client device 110. For example, rather than navigating an IVR to a target state associated with a department of agricultural produce as originally intended by a given user associated with the client device 110, the automated assistant can instead navigate to an alternate target state associated with a department of delicatessen.
In various implementations, the automated assistant may only prompt a given user of the client device 110 to modify navigation of the IVR tree if the user interaction is associated with an entity engaged during the secondary telephone call. For example, a given user of the client device 110 may only be prompted to modify the navigation of the IVR tree based on user interactions associated with the hypological Grocer. In some additional or alternative implementations, the automated assistant may prompt a given user of client device 110 to modify navigation of the IVR tree even if the user interaction is not associated with an entity engaged during the secondary telephone call. For example, a given user of the client device 110 may be prompted to modify the navigation of the IVR tree based on user interactions associated with a web page of a particular brand of delicatessen that is different from the hypo-logical Grocer. In this example, the one or more reminders may also include a coupon or advertisement for the particular brand of deli.
In some implementations, modifying the navigation of the IVR system may be based on navigating the state of the IVR tree. For example, if the automated assistant has not reached the target state upon receiving additional user input in response to one or more prompts to modify navigation of the IVR tree, the automated assistant may modify corresponding values of parameters utilized in navigating the IVR tree to navigate to the target state, modify internal nodes (e.g., as described with respect to fig. 2) traversed while navigating the IVR tree to navigate to an alternate target state, and/or restart navigation of the IVR tree. As another example, if the automated assistant has reached the target state upon receiving additional user input in response to one or more prompts to modify navigation of the IVR tree, the automated assistant may request that the secondary phone call be transferred to an alternate target state determined based on the additional user input and/or restart navigation of the IVR tree.
For example, and with specific reference to FIG. 4C, assume that the automated assistant has navigated the IVR tree to a target state as indicated by the spoken utterance 452C1 from "Now connecting you with the product department" of the generic Grocerer IVR and the subsequent spoken utterance 452C2 from "Hello, product department" of the generic Grocenter agricultural product. Assume further that the automated assistant reaches the target state before receiving user input, indicated by 490B1 of fig. 4B, to modify the navigation of the IVR to an alternate target state associated with the delicatessen department. Generally, at this stage of navigating the IVR tree associated with the underlying computing group, the automated assistant may verify that it has reached the target state (e.g., based on the subsequent spoken utterance 452C 2), generate a notification requesting the user to join the secondary telephone call and/or may perform a task on behalf of the given user of the client device 110. However, since the given user provides the user input in FIG. 4B for modifying the navigation of the IVR tree, the automated assistant can invoke the synthesized speech 454C1 of "Hello, can transfer me to the deli department," and can receive the spoken utterance 452C3"one instance please wait" in response to the synthesized speech 452C3 while the Hypothecal Grocenter agricultural representative is holding the automated assistant on-line to transfer the call to the delicacy department, and the automated assistant can remain on-line until the spoken utterance 452C4 of "Hello, delicacy department" is received. Thus, at this stage of navigating the IVR tree associated with the underlying computing group, the automated assistant may verify that it has reached the target state (e.g., based on the spoken utterance 452C 4), and generate a notification (e.g., as indicated by 454C 2) requesting the user to join the secondary telephone call, and/or may perform a task on behalf of a given user of the client device 110.
Further, in various embodiments, the automated assistant can cause a transcription of various conversations to be visually rendered at the user interface 180 of the client device 110 (e.g., as shown in fig. 4C). For example, the transcript can be displayed at a home screen of the client device 110, at various software applications (e.g., an automated assistant application, a call application, and/or other applications). In some implementations, the transcription can include a conversation between an automated assistant, an IVR system, and/or a human representative associated with the entity. Further, in various embodiments, the automated assistant can also cause various graphical elements to be rendered in the call details interface 470. For example, a first selectable graphical element 442C associated with ending the secondary telephone call can be provided, a second selectable graphical element 443C associated with ending the given user joining the secondary telephone call can be provided, a third selectable graphical element 444C associated with the secondary telephone call audibly perceivable via a speaker of the client device 110 can be provided, and/or other selectable graphical elements associated with the secondary telephone call can be provided. In various implementations, these selectable graphical elements may be rendered only at the call details interface 470 in response to generating the notification as indicated by 452C 2.
While fig. 4A-4C are described herein with respect to modifying navigation of an IVR tree associated with a hyperkinetic Grocer from an original target state associated with a department of agricultural produce to an alternate target state associated with a department of delicateness, it should be understood that this is for purposes of example and is not meant to be limiting. For example, and as described above (e.g., with respect to the user interaction engine 154 of fig. 1, with respect to the IVR tree 200 of fig. 2, and with respect to the method 300 of fig. 3), modifying the navigation of the navigated IVR tree can include adding a delicatessen department as an additional target state that can be navigated to after navigating to an original target state associated with a department of agricultural produce. As another example, and also as described above (e.g., with respect to the user interaction engine 154 of fig. 1, with respect to the IVR tree 200 of fig. 2, and with respect to the method 300 of fig. 3), modifying the navigation of the IVR tree can include modifying corresponding values of parameters for navigating the IVR tree to an original target state associated with the agricultural sector. For example, if upon navigating to a target state associated with a department of agricultural produce, a given user indicates that the automated assistant should ask avocados, but begins viewing tomatoes on the agricultural portion of the web page associated with the Hypothetical Grocerer, the user may be prompted as to whether the automated assistant should additionally or alternatively ask tomatoes.
Furthermore, while FIG. 4C is depicted as including a transcription of an automated assistant navigating an IVR tree associated with a Hypothetical Grocer, it should be understood that this is for purposes of example and is not meant to be limiting. For example, it should be appreciated that the secondary phone call described above can be performed while the client device 110 is in a dormant, locked state when other software applications are running in the foreground and/or in other states. Further, in embodiments in which the automated assistant causes notifications to be rendered at the client device 110, the type of notification rendered at the client device is based on the state of the client device 110, as described herein. Furthermore, while fig. 4A-4C are described herein with respect to navigating an IVR tree associated with a hierarchical Grocer, it should be understood that this is also not intended to be limiting, and that the techniques described herein can be used to navigate any previously stored IVR tree and utilized with respect to a number of different entities.
Turning now to fig. 5, a flow diagram is depicted illustrating an example method 500 of navigating an IVR tree from a search interface at a client device associated with a given user. For convenience, the operations of method 500 are described with reference to a system that performs the operations. The system of method 500 includes one or more processors and/or other components of a computing device (e.g., client device 110 of fig. 1, 4A-4D, 6A-6E, and/or 8A-8D, and/or computing device 910 of fig. 9, one or more servers, and/or other computing devices). Further, while the operations of method 500 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 552, the system receives user input from a given user via a search interface of a client device associated with the given user. The user input for initiating the secondary telephone call can be one or more of a verbal input for initiating the secondary telephone call, a touch input for initiating the secondary telephone call, or a key input for initiating the secondary telephone call. Further, user input for initiating the secondary phone call can be received at various interfaces (e.g., a search interface, a voice interface, an automated assistant interface, and/or other interfaces from which the secondary phone call can be initiated). Further, for example, the search interface can be implemented as part of a phone or contacts application, a browser application, an automated assistant application, a web-based browser, and/or any other interface that enables search functionality.
At block 554, the system issues a search query based on the user input at a search interface of the client device. The search query can be published to one or more databases accessible to the client device (e.g., a restricted-access database and/or a public database), a search engine, and/or any other search-based system.
At block 556, the system identifies, based on the issued search query, a given search result associated with the entity to be engaged on behalf of the given user during the secondary phone call, the given search result associated with a target state for an IVR tree associated with the identified entity from among a plurality of candidate states. At block 558, the system causes at least rendering of the given search result at the client device. At block 560, the system receives a selection of a given search result from a given user via the search interface of the client device. The given search result can include content responsive to the search query issued at block 554. In some implementations, additional given search results corresponding to other candidate states can also be rendered along with the given search result. In some versions of those implementations, a given search result can be presented more prominently than additional given search results associated with other candidate states based on recent user interactions at the client device. For example, if a user is interacting with other search results related to various brands of paint or painting equipment, and the issued search query is for a home decoration entity, then a given search result can be associated with the home decoration entity, or more specifically, a painting department of the home decoration entity. Thus, upon selection of a given search result, the system can determine the scribble department as the target state for navigating the IVR tree associated with the home decoration entity. The selection of a given search result can be based on touch input, verbal input, and/or typed input.
At block 562, the system initiates execution of the secondary phone call to navigate the IVR tree to the target state in response to the selection. The system can initiate execution of the secondary phone by establishing a communication session between the client device associated with the given user and the additional client devices associated with the entities identified at block 556. The communication session can be established using various voice communication protocols including, for example, voIP, PSTN, and/or other telephony communication protocols. In some implementations, the system can navigate the IVR tree to a target state by simulating a button press to traverse a node of the IVR tree, by rendering synthesized speech audio data including synthesized speech to traverse a node of the IVR tree, and/or by rendering audio data including verbal input of a given user associated with the client device (e.g., as described with respect to fig. 2). In some versions of those embodiments, the system is also capable of utilizing corresponding values of parameters associated with navigating the IVR tree. The corresponding values of the parameters can be retrieved from one or more databases (e.g., user profile database 153A of fig. 1) and/or can be requested from a given user associated with the client device.
At block 564, the system renders, via the client device, a notification indicating a result of the execution of the navigating IVR tree. For example, the notification can include a request to a given user associated with the client device to join a secondary phone call. In embodiments described herein in which the system also performs tasks on behalf of a given user associated with the client device and with respect to the entity, the notification can additionally or alternatively include results of the performance of the task. In various embodiments, the navigation of the IVR tree described with respect to fig. 5 may be modified based on user interaction at the client device, as described above with respect to fig. 3 and 4A-4C.
Referring now to fig. 6A-6E, various non-limiting examples of user interfaces associated with navigating an IVR tree from a search interface at a client device 110 associated with a given user are depicted. Fig. 6A-6E each depict a client device 110 having a graphical user interface 180, and may include one or more components of the client device of fig. 1. The client device 110 may be substantially similar to the client device of fig. 4A-4C and include many of the same components described above with respect to fig. 4A-4C, including an example of an automated assistant (e.g., automated assistant 115 of fig. 1).
In various implementations described herein, user input can be received and a search query can be issued based on the user input. The user input can be spoken, touch, and/or typed input including a search query. For example, and as shown in fig. 6A, assume that a user provides a search query 684A of "Lost bag" at a browser application accessible at client device 110 (e.g., as indicated by URL 611 of "www. Exampleurl0.com/") (e.g., as included in text reply interface element 184). In addition, the search results include a first search result 620 that provides "Hypothetical Airline" that serves flights in North America. In some implementations, the search results 620 can be associated with various selectable graphical elements that, when selected, cause the client device 110 to perform corresponding actions. For example, when the call graphical element 621 associated with the search result 620 is selected, the user input can indicate that a phone call action to Hypothertical Airline should be performed. As another example, when the user account graphical element 622 associated with the search result 620 is selected, the user input can indicate that an account lookup action for a user account associated with a hypokinetic Airline should be performed. As yet another example, when the flights graphical element 623 associated with the search result 620 is selected, the user input can indicate that a flight lookup action for flights associated with the Hypothetical Airline should be performed.
In various implementations, the search results 620 can also include one or more IVR deep links to candidate states of an IVR tree associated with the entity. For example, and as shown in fig. 6A, a first IVR deep link 620A1 may be provided for "report last bag" corresponding to a candidate state for reporting lost baggage when navigating the IVR tree associated with the synthetic air interface, and a second IVR deep link 620A2 may be provided for "spread with a representation talk" corresponding to a candidate state for the synthetic air interface representative talk when navigating the IVR tree associated with the synthetic air interface. An additional graphical element 620A3 of "see more" may optionally be provided, and the additional graphical element 620A3, when selected, may cause candidate states associated with one or more additional IVR deep links to be rendered along with the first deep link 620A1 and the second deep link 620A2. In some versions of those embodiments, one or more of the rendered IVR deep links 620A1 and/or 620A2 can be based on the most recent user interaction at the client device 110. For example, a first IVR deep link 620A1 of "report lost bag" may be rendered at the client device based on a search query 684A that includes "lost bag" or based on a given user of the client device 110 previously interacting with other search results related to reporting lost bags. As another example, a second IVR deep link 620A2 of "speak with a delegate" may be rendered at the client device 110 based on it being the most popular target state from among the candidate states. In these examples, first IVR deep link 620A1 may be rendered more prominently than second IVR deep link 620A2 based on determining that it is more relevant to a given user of client device 110 (e.g., determined based on search query 684A and/or based on a given user of client device 110 previously interacting with other search results).
An auxiliary phone call to navigate the IVR tree associated with the hypological aircines to a target state associated with reporting missing baggage can be initiated based on a user input directed to the first IVR deep link 620A1, and execution of the auxiliary phone call to navigate the IVR tree to the target state can be initiated. In some implementations, the automated assistant can utilize resources of the client device 110 to initiate performance of the secondary phone call. In some additional or alternative implementations, the automated assistant can utilize resources of the remote system to initiate execution of the secondary phone call (e.g., a cloud-based secondary phone call). For example, assume that the user selected the first IVR deep link 620A1, but the client device 110 was unable to perform a telephone call using VoIP, PSTN, or other telephony communication protocols (e.g., via a laptop or desktop computer). In this example, the automated assistant 115 can cause a remote system (which may or may not communicate with the client device 110) to initiate and execute secondary phone calls on behalf of the user.
In various embodiments, the call detail interface 670 can be rendered at the client device 110 prior to receiving user input for initiating a telephone call with "synthetic air," or can be rendered at the client device 110 in response to receiving user input for initiating a telephone call with synthetic air. In some versions of those embodiments, the call details interface 670 can be rendered at the client device 110 as part of the user interface 180. In some other versions of those embodiments, the call details interface 670 can be an interface separate from the user interface 180 that overlays the user interface 180, and can include the call details interface element 186 that allows a user to expand the call details interface 670 to display additional call details (e.g., by sliding up on the call details interface element 186) and/or close the call details interface 670 (e.g., by sliding down on the call details interface element 186). Although the call details interface 670 is depicted as being located at the bottom of the user interface 180, it should be understood that this is for purposes of example and is not intended to be limiting. For example, the call details interface 670 can be rendered on the top of the user interface 180, on a side of the user interface 180, or an interface completely separate from the user interface 180.
In various embodiments, the call details interface 670 can include a plurality of graphical elements. In some versions of these embodiments, the graphical elements can be selectable such that when a given one of the graphical elements is selected, the client device 110 can perform a corresponding action. As shown in fig. 6A, the Call details interface 670 includes a first graphical element 671 of "Assisted Call," a second graphical element 672A of "Regular Call," and a third graphical element 673A of "Save Contact" advanced Airlines "(holding Contact's Hypothetical Airline')". An auxiliary phone call can additionally or alternatively be initiated from the call details interface 670 based on a user input directed to the first selectable graphical element 671, and the execution of the auxiliary phone call is based on the selection of the first selectable graphical element 671.
In various implementations, one or more IVR deep links associated with the candidate state may not be rendered. In some versions of those embodiments, the automated assistant may visually render various nodes (e.g., internal nodes and/or leaf nodes as described with respect to fig. 2) for navigating the IVR tree associated with the entity. The visual rendering of the various nodes may be based on confidence levels of the IVR tree stored in association with the entity. The confidence level of the IVR tree stored in association with the entity may be based on a number of secondary phone calls with the entity including navigating an IVR tree different from the IVR tree stored in association with the entity, and optionally based on a degree of difference between the navigated IVR tree and the stored IVR tree. In particular, fig. 6B-6E illustrate various non-limiting examples of how these various nodes may be presented to a user based on different confidence levels of the IVR tree associated with the generic aircraft.
For purposes of the example with respect to fig. 6B-6E, assume that the entity to be engaged during the secondary phone call is the contextual Airline described above with respect to fig. 6A, and that the user is viewing the first search result 610 associated with the contextual Airline, but has not received user input including the search query 684A. With specific reference to FIG. 6B, further assume that the automated assistant has confidence in the height of the IVR tree stored in association with the Hypothetical Airline. The automated assistant may be confident in the IVR tree height stored in association with the hypokinetic airling based on confidence levels determined by nodes based on the IVR tree remaining the same for most, if not all, of the secondary telephone calls that include the same IVR tree. In this example, rather than providing one or more IVR deep links, the automated assistant can cause information including instructions on how to navigate an IVR tree associated with the contextual airling before receiving user input to initiate execution of an auxiliary telephone call. For example, the call details interface 670 can include: first information of 671A of "Press '1', '3', '5' to report missing baggage", which indicates that a given user of the client device 110 can provide a button-down sequence to reach a target state of an IVR tree associated with reporting missing baggage; and a second information of 671B of "Press '5' to speak with a representative" indicating that a given user of client device 110 can provide a button Press sequence to reach a target state of the IVR tree associated with the representative conversation. A graphical element 671C of "see more" may optionally be provided, and when selected, graphical element 671C may cause additional information associated with other candidate states to be rendered along with first information 671A and second information 671B. Similar to fig. 6A, information associated with the candidate states rendered in the call details interface 670 may optionally be rendered based on recent user interactions with the client device 110 and/or the overall popularity of the candidate states.
With specific reference to fig. 6C and similar to fig. 6B, further assume that the automated assistant has confidence in the IVR tree height stored in association with the hypokinetic Airline. In this example, rather than providing one or more IVR deep links or information related to navigating directly to a desired target state through a button press sequence prior to initiating an auxiliary telephone call, the automated assistant can cause the candidate state to be rendered in a notification 679C in the call details interface 670 before receiving the spoken utterance 654C1 from a hypthetical airling IVR that includes the candidate state, as indicated by 652C 1. In particular, the candidate states can include those indicated by 679C1, 679C2, 679C3, 679C4, and 679C 5. Each of the candidate states included in notification 679C may be selectable such that a given user associated with client device 110 can simply select a given one of the candidate states as the target state and the automated assistant automatically navigates the secondary telephone call to the target state.
With specific reference to fig. 6D and in contrast to fig. 6C, further assume that the automated assistant has moderate confidence in the IVR tree stored in association with the logical aircines. Based on the confidence levels determined for the nodes of the IVR tree that include some differences for some ancillary telephone calls that contain the same IVR tree, the automated assistant may have moderate confidence in the IVR tree stored in association with the hypokinetic airling. In this example, rather than providing the notification 679D in the call details interface 670 before receiving the spoken utterance including the candidate state from the regenerative airling IVR, the automated assistant may wait until a first portion 652D1 of the spoken response including a subset of the candidate state is provided by the regenerative airling IVR to render the notification 679C including the candidate state. For example, the degree of difference between the stored IVR trees associated with the hypokinetic Airline may only be moderately confident in a subset of the candidate states included in the first portion 652D1 of the spoken utterance, but highly confident in the remaining candidate states. Thus, after the subset of candidate states is included in the first portion of the spoken utterance 652D1, the automated assistant can generate a notification 679D prior to receiving the second portion of the spoken utterance 652D2, as indicated by 654D 1. Similar to fig. 6C, each of the candidate states included in notification 679D may be selectable such that a given user associated with client device 110 can simply select a given one of the candidate states as the target state and the automated assistant automatically navigates the secondary telephone call to the target state.
With specific reference to fig. 6E and in contrast to fig. 6C and 6D, further assume that the automated assistant has no confidence in the IVR tree stored in association with the generic Airline. Based on the confidence levels determined based on the nodes of the IVR tree including the threshold number of differences in the secondary telephone calls that comprise different IVR trees, the automated assistant may not have confidence in the IVR tree stored in association with the contextual airling. In this example, rather than providing the notification 679E in the call details interface 670 prior to receiving the spoken utterance, the automated assistant may wait until a spoken response 652E1 including the candidate state is provided by the Hypothetical Airline IVR to render a notification 679D including the candidate state, as indicated by 654E 1. Similar to fig. 6C, each of the candidate states included in notification 679E may be selectable such that a given user associated with client device 110 can simply select a given one of the candidate states as the target state and the automated assistant automatically navigates the secondary telephone call to the target state.
Although fig. 6A-6E are described herein with respect to the entity hypological Airline, and certain graphical elements and information are depicted with the search results 620 and call detail interface, it should be understood that this is for purposes of example and is not intended to be limiting. It should be understood that the IVR tree associated with any entity employing the IVR system can be stored and subsequently used to perform the secondary telephone call. Further, it should be understood that the graphical elements and information provided by the automated assistant may be based on an IVR tree, and that the configuration of the IVR tree associated with an entity is virtually limitless.
Turning now to fig. 7, a flow diagram is depicted illustrating an example method 700 of navigating a dynamic Interactive Voice Response (IVR) tree. For convenience, the operations of method 700 are described with reference to a system performing the operations. The system of method 700 includes one or more processors and/or other components of a computing device (e.g., client device 110 of fig. 1, 4A-4D, 6A-6E, and/or 8A-8D, and/or computing device 910 of fig. 9, one or more servers, and/or other computing devices). Further, while the operations of method 700 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 752, the system receives user input from the given user for initiating a secondary phone call via a client device associated with the given user. The user input for initiating the secondary telephone call can be one or more of a verbal input for initiating the secondary telephone call, a touch input for initiating the secondary telephone call, or a key input for initiating the secondary telephone call. Further, user input for initiating the secondary phone call can be received at various interfaces (e.g., a search interface, a voice interface, an automated assistant interface, and/or other interfaces from which the secondary phone call can be initiated).
At block 754, the system identifies entities to engage on behalf of the given user during the secondary phone call based on the user input. The system can identify an entity to engage during the secondary telephone call based on the user input received at block 752 and/or user interaction with the client device immediately prior to initiating the secondary telephone call. For Example, the system can identify the entity of "Example Airlines" based on processing the verbal input received at the client device "report me missing luggage to Example Airlines". As another Example, the system can identify an entity of "Example Airlines" based on determining that a given user of the client device has selected a call interface element associated with the Example Airlines or a particular graphical element associated with a candidate state of the IVR tree associated with the Example Airlines.
At block 756, the system identifies an IVR tree associated with the entity identified at block 754, the IVR tree including a plurality of candidate states. The IVR tree associated with the identified entity can be stored in association with the entity identified at block 754 (e.g., in the entity database 151A of fig. 1 and/or the IVR tree database 152A of fig. 1). The stored IVR trees associated with the entities may previously be stored in one or more databases based on the system (and/or additional instances of the system) previously crawling the IVR systems associated with the identified entities, and/or the identified entities may be able to provide the IVR trees for the IVR systems associated with the identified entities. Further, the system can continuously update the IVR tree of the IVR system associated with the identified entity based on the plurality of secondary phone calls navigating the IVR tree. The plurality of candidate states can correspond to particular nodes of an IVR tree to which the system can navigate during the secondary telephone call.
At block 758, the system receives contextual information. In some embodiments, the frame 756 can include a selectable subframe 758A and/or a selectable subframe 758B. If so, at optional sub-box 758A, the system receives contextual information associated with the client device. The contextual information associated with the client device can include, for example, device state information (e.g., determined by the device state engine 112), date and/or time information associated with the current location of the client device, and/or other contextual information generated by the client device. If so, at the optional subframe 758B, the system receives contextual information associated with a given user of the client device. The contextual information associated with a given user of a client device can include, for example, electronic communications (e.g., emails, text messages, voice messages, etc.) created by or received by the user of the client device, user account information (e.g., account status information, accounts, account usernames, account passwords, etc.) associated with various user accounts of the given user of the client device, monetary information (e.g., bank accounts, credit card numbers, payment application information, etc.) of the given user of the client device, albums of the given user of the client device, social media profiles of the given user of the client device, user preferences of the user of the client device, personal information (e.g., names, phone numbers, physical addresses, email addresses, social security numbers, birth dates, etc.) of the given user of the client device, and/or other information associated with the given user of the client device.
At block 760, the system determines the availability of multiple candidate states as target states for the IVR tree based on contextual information associated with the client device and/or a given user of the client device. In various implementations, based on the context information, some candidate states of the IVR tree may or may not be available to a given user of the client device. For example, assume that the entity identified at block 754 is a restaurant entity. In this example, some candidate statuses may only be available during business hours of the restaurant (e.g., candidate statuses associated with placing take orders). As another example, assume that the entity identified at block 754 is an airline entity. In this example, some candidate states may only be available when the user account state is above a threshold membership state (e.g., only for members in the golden state and above). In other words, the IVR tree associated with the identified entity may be dynamic in that candidate states may or may not be available in certain contexts.
At block 762, the system initiates execution of the secondary phone call to navigate the IVR tree to the target state. The system can initiate performance of the secondary telephone call in response to additional user input selecting the target state. Further, the system can initiate performance of the secondary phone call by establishing a communication session between the client device associated with the given user and an additional client device associated with the entity identified at block 754. The communication session can be established using various voice communication protocols including, for example, voIP, PSTN, and/or other telephony communication protocols. In some implementations, the system can navigate the IVR tree to a target state by simulating a button press to traverse a node of the IVR tree, by rendering synthesized speech audio data including synthesized speech to traverse a node of the IVR tree, and/or by rendering audio data including verbal input of a given user associated with the client device (e.g., as described with respect to fig. 2). In some versions of those embodiments, the system is also capable of utilizing corresponding values of parameters associated with navigating the IVR tree. The corresponding values of the parameters can be retrieved from one or more databases (e.g., user profile database 153A of fig. 1) and/or can be requested from a given user associated with the client device.
At block 764, the system renders, via the client device, a notification indicating a result of the execution of the navigating IVR tree. For example, the notification can include a request for a given user associated with the client device to join the secondary phone call. In embodiments described herein where the system also performs tasks on behalf of a given user associated with the client device and in relation to the entity, the notification can additionally or alternatively include results of the performance of the task. In various embodiments, the navigation of the IVR tree described with respect to fig. 5 may be modified based on user interaction at the client device, as described above with reference to fig. 3 and 4A-4C.
Referring now to fig. 8A-8D, various non-limiting examples of user interfaces associated with navigating a dynamic IVR tree are depicted. Fig. 8A-8D each depict a client device 110 having a graphical user interface 180, and may include one or more components of the client device of fig. 1. The client device 110 may be substantially similar to the client device of fig. 4A-4C and include many of the same components described above with respect to fig. 4A-4C, including an instance of an automated assistant (e.g., automated assistant 115 of fig. 1).
In various implementations described herein, the IVR tree associated with an entity can be a dynamic IVR tree, as one or more candidate states associated with the IVR tree may or may not be available. The availability of one or more candidate states may be determined based on the context information. In some implementations, the contextual information can be contextual information associated with the client device 110, and can include, for example, device state information of the client device 110 (e.g., determined via the device state engine 112), a date and/or time associated with a current location of the client device 110, a current location of the client device, and/or other contextual information generated by the client device 110. In some additional or alternative implementations, the contextual information may be contextual information associated with the given user of the client device 110, and may include, for example, electronic communications (e.g., emails, text messages, voice messages, etc.) created by or received by the user of the client device, user account information (e.g., account status information, account numbers, account usernames, account passwords, etc.) associated with various user accounts of the given user of the client device 110, monetary information (e.g., bank account numbers, credit card numbers, payment application information, etc.) of the given user of the client device 110, albums of the given user of the client device 110, social media profiles of the given user of the client device 110, user preferences of the user of the client device 110, personal information (e.g., names, telephone numbers, physical addresses, email addresses, social security numbers, birth dates, etc.) of the given user of the client device, and/or other information associated with the given user of the client device.
With particular reference to fig. 8A and 8B, assume that a given user of client device 110 is viewing search results for restaurant entities at a browser application accessible at client device 110 (e.g., as indicated by URL 811A of "www. Further assume that the search results include at least search results 820A for "Hypothetical restaurants" that were open from 9 am to 11 pm. In some implementations, the search results 820A can be associated with various selectable graphical elements that, when selected, cause the client device 110 to perform corresponding actions. For example, when the call graphical element 821A associated with the search result 820A is selected, the user input can indicate that a phone call action to the Hypothetical Restaurant should be performed. As another example, when the directions graphical element 822A associated with the search result 820A is selected, the user input can indicate that a navigation action to the entity location of the contextual Restaurant should be performed. As yet another example, when menu graphical element 823A associated with search results 820A is selected, the user input can indicate that a menu lookup action for a menu associated with a generic Airline should be performed.
In various implementations, the call details interface 870 can be rendered at the client device 110 prior to receiving user input to initiate a telephone call with "Hypothertical restart," or can be rendered at the client device 110 in response to receiving user input to initiate a telephone call with Hypothertical restart. In some versions of those embodiments, the call details interface 870 can be rendered at the client device 110 as part of the user interface 180. In some other versions of those embodiments, the call details interface 870 can be a separate interface from the user interface 180 that overlays the user interface 180, and can include the call details interface element 186 that allows a user to expand the call details interface 870 to display additional call details (e.g., by sliding up on the call details interface element 186) and/or close the call details interface 870 (e.g., by sliding down on the call details interface element 186). While the call details interface 870 is depicted as being located at the bottom of the user interface 180, it is to be understood that this is for purposes of example and is not intended to be limiting. For example, the call details interface 870 can be rendered on the top of the user interface 180, on a side of the user interface 180, or an interface completely separate from the user interface 180.
In various embodiments, the call details interface 870 can include a plurality of graphical elements. In some versions of these embodiments, the graphical elements can be selectable such that when a given one of the graphical elements is selected, the client device 110 can perform a corresponding action. As shown in fig. 8A and 8B, the Call details interface 870 includes a first graphical element 871A of "Assisted Call," a second graphical element 872A of "Regular Call," and a third graphical element 873A of "Save Contact" advanced Call, "as shown in fig. 8A and 8B. The graphical element may also include sub-elements corresponding to candidate states associated with the IVR tree associated with the Hypothetical Restaurant. For example, and as shown in fig. 8A, a first graphical element 871A can include: a first sub-element 871A1 of "Make Reservations" associated with navigating the IVR tree associated with the Hypothetical reservation to a target state corresponding to a part where Restaurant Reservations are made; a second sub-element 871A2 of "Order cart out" associated with navigating the IVR tree associated with the generic Restaurant resource to a target state corresponding to the portion ordered out from the Restaurant; and a third child element 871A3 of "Order Delivery" associated with navigating the IVR tree associated with the Hypothertical Restaurant to the target state of the part corresponding to the ordered Delivery from the Hypothertical Restaurant. In contrast, and as shown in fig. 8B, the first graphical element 871A can include: a first sub-element 871B1 of "Make Reservations" associated with navigating the IVR tree associated with the Hypothetical reservation to a target state corresponding to a part where Restaurant Reservations are made; and a second sub-element 871B2 of "homes of Operation" associated with navigating the IVR tree associated with the Hypothetical Restauant to a target state corresponding to a portion of the business Hours listening to the Hypothetical Restauant.
Notably, the difference in candidate states included in fig. 8A and 8B may be based on contextual information associated with the client device 110 of a given user. Notably, and with reference to fig. 8A, the time and date information 812A at the current location of the client device 110 indicates a time of 9 am 30, and this corresponds to a hypokinetic respiratory being open (e.g., based on the open time being 9 am to 11 pm. Thus, the candidate state associated with the selectable sub-elements 871A2 and 871A3 may be available because the contextual Restaurant is open. In contrast, and referring to fig. 8B, the time and date information 812B at the current location of the client device 110 indicates a time of 8 am 30, and this corresponds to the hypokinetic Restaurant being out of business (e.g., based on business hours being 9 am 00 to 11 pm. Thus, the candidate state associated with selectable sub-elements 871A2 and 871A3 may not be available because the Hypothercal Restaurant is out of business, but the automated assistant may still be able to make Restaurant reservations and/or query business hours by navigating the IVR tree associated with the Hypothercal Restaurant, even if it is out of business. Thus, the IVR tree may be dynamic in that some candidate states may be available in a given context, while other candidate states may not be available in the given context. As another example, assume that in FIG. 8A the Hypothetical Restaurant is open, but the current location of the client device 110 is outside the delivery radius of the Hypothetical Restaurant. In this example, the candidate state associated with the third sub-element 871A3 of "Order Delivery" may be omitted. As yet another example, assume that in FIG. 8A, a Hypothertical Restaurant is open, but that no indoor and outdoor dining services are provided at the Hypothertical Restaurant. In this example, the candidate state associated with the first sub-element 871A1 of "Make Reservations" may be omitted.
With specific reference to fig. 8C and 8D, assume that a given user of client device 110 is viewing search results for a hotel entity on a browser application accessible at client device 110 (e.g., as indicated by URL 811C of "www. Assume further that the search results include at least a first search result 820C of "Hypothetical Hotel" and a second search result 830C of "Example Hotel". In some implementations, the search results 820C and/or 830C can be associated with various selectable graphical elements that, when selected, cause the client device 110 to perform corresponding actions. For example, when call graphical elements 821C and/or 831C associated with search results 820A and/or 830C, respectively, are selected, the user input can indicate that a telephone call action should be performed with the respective hotel entity. As another example, when directions graphical elements 822C and/or 832C associated with search results 820A and/or 830C, respectively, are selected, the user input can indicate that a navigation action to the entity location of the respective hotel entity should be performed. As yet another example, when account elements 823C and/or 833C associated with search results 820A and/or 830C, respectively, are selected, the user input can indicate that an account lookup action should be performed for a user account associated with the respective hotel entity.
In various implementations, the call details interface 870 can be rendered at the client device 110 prior to receiving the user input to initiate the telephone call with the respective one of the hotel entities, or can be rendered at the client device 110 in response to receiving the user input to initiate the telephone call with the respective one of the hotel entities. In some versions of those embodiments, the call details interface 870 can be rendered at the client device 110 as part of the user interface 180. In some other versions of those embodiments, the call details interface 870 can be an interface diagram that is separate from the user interface 180 that overlays the user interface 180 and can include the call details interface element 186 that allows a user to expand the call details interface 870 to display additional call details (e.g., by sliding up on the call details interface element 186) and/or close the call details interface 870 (e.g., by sliding down on the call details interface element 186). Although the call details interface 870 is depicted as being located at the bottom of the user interface 180, it should be understood that this is for purposes of example and is not intended to be limiting. For example, the call details interface 870 can be rendered on the top of the user interface 180, on a side of the user interface 180, or an interface completely separate from the user interface 180.
In various embodiments, the call details interface 870 can include a plurality of graphical elements. In some versions of these embodiments, the graphical elements can be selectable graphical elements such that when a given one of the graphical elements is selected, the client device 110 can perform a corresponding action. As shown in fig. 8C, the Call details interface 870 includes a first graphical element 871C of "Assisted Call," a second graphical element 872C of "Regular Call," and a third graphical element 873C of "Save Contact 'advanced Hotel' (Save Contact 'assumed Hotel')". The graphical element of FIG. 8C may also include sub-elements corresponding to candidate states associated with the IVR tree associated with the Hypothetical Hotel. For example, and as shown in fig. 8C, the first graphical element 871C can include: a first sub-element 871C1 of "Change Reservation" associated with navigating the IVR tree associated with the hyposynthetic Hotel to a target state corresponding to a part of changing the Hotel Reservation; a second sub-element 871C2 of "Request Free Upgrade" associated with navigating the IVR tree associated with the contextual Hotel to a target state corresponding to a portion of the target state requesting a Free Upgrade to an existing Hotel subscription; and a third sub-element 871C3 of "contact Direct Line" associated with navigating the IVR tree associated with the hypokinetic Hotel to the target state of the part corresponding to the Concierge part contacting the hypokinetic Hotel.
In contrast, and as shown in fig. 8D, the call details interface 870 includes: a first graphical element 871D of "Assisted Call, example Hotel" (as opposed to "advanced Hotel" in fig. 8C); a second graphic element 872D of "Regular Call"; and a third graphical element 873D of "Save Contact 'Example Hotel' (Save Contact 'Example Hotel')". The graphical element of FIG. 8D may also include sub-elements corresponding to candidate states associated with the IVR tree associated with the Example Hotel. For example, and as shown in fig. 8D, the first graphical element 871D can include: a first sub-element of "Change Reservation" 871D1 associated with navigating an IVR tree associated with an Example Hotel to a target state corresponding to a portion of the changed Hotel Reservation; a second sub-element 871D2 of "Cancel Reservation" associated with navigating the IVR tree associated with the Example Hotel to a target state corresponding to a part of canceling an existing Hotel Reservation; and a third child element 871D3 of "Front Desk" associated with navigating the IVR tree associated with the Hypothetical Hotel to the target state of the portion corresponding to the foreground tied at the Example Hotel.
Notably, the difference in candidate states included in fig. 8C and 8D may be based on context information associated with a given user of the client device 110 of the given user. Notably, and with reference to fig. 8C, the automated assistant is able to access a user account of a given user of the client device 110 associated with a hyphenative Hotel. For example, the automated assistant can access user account status (e.g., cupboard, bankboard, gold, etc.), user accounts, user account history including stops at various locations of the financial Hotel, etc. For purposes of example, assume that a given user of a client device 110 is a gold member of a Hypothetical Hotel. As a result, the candidate states associated with the selectable sub-elements 871C2 and 871C3 may be available based on the golden member state of a given user of the client device 110. In contrast, and referring to FIG. 8D, the automated assistant may not have any record of the user account associated with the Example Hotel. As a result, the candidate states associated with the selectable sub-elements 871C2 and 871C3 may not be available, but the automated assistant may still be able to navigate to the various candidate states depicted in fig. 8D. Thus, the IVR tree may be dynamic in that some candidate states may be available in a given context, while other candidate states may not be available in the given context.
Although fig. 8A and 8B are described herein with respect to availability of candidate states based on contextual information associated with a client device 110 of a given user, and fig. 8C and 8D are described herein with respect to availability of candidate states based on contextual information associated with a given user of a client device 110, it should be understood that this is for purposes of example and is not intended to be limiting. For example, the availability of the candidate state can be based on contextual information associated with the client device 110 of the given user and contextual information associated with the given user of the client device 110. For example, and referring to fig. 8C, if a given user of the client device 110 initiates a secondary phone call while the Concierge part of the hyphetic Hotel is not at work, the candidate state associated with the third sub-element 871C3 of "Concierge Direct Line" may be omitted even if the user account state of the user account associated with the hyphetic Hotel enables the given user of the client device 110 to directly contact the Concierge part of the hyphetic Hotel.
Further, although fig. 8A-8D are described herein with respect to certain entities, and certain graphical elements are depicted with the search results and call detail interfaces, it should be understood that this is for the sake of example and is not intended to be limiting. It should be understood that the IVR tree associated with any entity employing the IVR system can be stored and subsequently used to perform the secondary telephone call. Further, it should be understood that the graphical elements and information provided by the automated assistant may be based on an IVR tree, and that the configuration of the IVR tree associated with an entity is virtually limitless.
Fig. 9 is a block diagram of an example computing device 910 that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client device, cloud-based automated assistant component, and/or other components can include one or more components of the example computing device 910.
The computing device 910 typically includes at least one processor 914 that communicates with a number of peripheral devices via a bus subsystem 912. These peripheral devices may include: storage subsystem 924 (including, for example, memory subsystem 925 and file storage subsystem 926), user interface output devices 920, user interface input devices 922, and network interface subsystem 916. The input and output devices allow a user to interact with the computing device 910. Network interface subsystem 916 provides an interface to external networks and couples to corresponding interface devices in other computing devices.
The user interface input devices 922 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 910 or onto a communication network.
User interface output devices 920 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, for example, via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 910 to a user or to another machine or computing device.
These software modules are typically executed by processor 914 alone or in combination with other processors. Memory 925 used in storage subsystem 924 can include a plurality of memories including a main Random Access Memory (RAM) 930 for storing instructions and data during program execution and a Read Only Memory (ROM) 932 in which fixed instructions are stored. File storage subsystem 926 is capable of providing permanent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 926 in storage subsystem 924, or in other machines accessible to processor 914.
The bus subsystem 912 provides a mechanism for letting the various components and subsystems of the computing device 910 communicate with each other as intended. Although the bus subsystem 912 is shown schematically as a single bus, alternative embodiments of the bus subsystem may use multiple buses.
The computing device 910 can be of various types, including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 910 depicted in FIG. 9 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of the computing device 910 are possible with more or fewer components than the computing device depicted in fig. 9.
Where the systems described herein collect or otherwise monitor personal information about a user or personal and/or monitoring information may be utilized, the user may be provided with an opportunity to control whether programs or functions collect user information (e.g., information about the user's social network, social behaviors or activities, profession, user preferences, or the user's current geographic location), or whether and/or how to receive content from a content server that may be more relevant to the user. Further, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, the identity of the user may be processed such that no personally identifiable information of the user can be determined, or the geographic location of the user may be summarized where geographic location information is obtained (such as to a city, zip code, or state level) such that no particular geographic location of the user can be determined. Thus, the user may control how information about the user is collected and/or used.
In some implementations, a method implemented by one or more processors is provided and includes: receiving, from a given user via a client device associated with the given user, a user input for initiating a secondary telephone call; identifying, based on the user input, an entity to be engaged on behalf of the given user during the secondary telephone call; identifying an Interactive Voice Response (IVR) tree associated with the identified entity, the IVR tree including a plurality of candidate states; determining a target state of the IVR tree from among the plurality of candidate states based on the user input or additional user input; and initiate execution of the secondary phone call to navigate the IVR tree to the target state. The performing of the secondary phone call includes automatically navigating the secondary phone call to the target state of the IVR tree. The method further comprises: during execution of the secondary telephone call and while the IVR tree is navigated to the target state, detecting at least one user interaction directed to the client device and associated with the identified entity, and generating one or more prompts related to modifying navigation of the IVR tree based on the at least one user interaction directed to the client device. The method further comprises: in response to determining another additional user input received in response to rendering one or more of the additional prompts, modify navigation of the IVR tree, modify performance of the secondary telephone call based on the other additional user input.
These and other embodiments of the technology disclosed herein can optionally include one or more of the following features.
In some embodiments, the method may further comprise: after navigating the IVR tree to reach a particular state during the secondary telephone call, and based on the another additional user input, generating a notification requesting the given user associated with the client device to join the secondary telephone call, and causing the notification to be rendered at the client device associated with the given user. In some versions of those embodiments, causing the notification to be rendered at the client device associated with the given user may include one or more of: causing the client device to vibrate, causing the client device to audibly render the notification requesting the given user to join the secondary telephone call, or causing the client device to visually render the notification requesting the given user to join the secondary telephone call. In some additional or alternative versions of those embodiments, the method may further include, after navigating the IVR tree to the particular state, engaging in a conversation with a human representative associated with the identified entity to verify whether the particular state corresponds to the target state.
In some further versions of those embodiments, participating in the conversation with the human representative to verify whether the particular state corresponds to the target state may include: processing audio data that captures a spoken utterance of the human representation using a speech recognition model; and determine, based on processing the audio data, whether the human representative verifies that the particular state corresponds to the target state. Generating the notification requesting the given user associated with the client device to join the secondary telephone call may be in response to determining that the human representative verifies that the particular state corresponds to the target state. In other versions of those embodiments, rendering, at an additional client device associated with the human representative, synthesized speech audio data including synthesized speech for verifying whether the IVR tree was successfully navigated may further be performed. The audio data that captures the spoken utterance of the human representative may be received in response to rendering the synthesized speech audio data at the additional client device associated with the human representative.
In some additional or alternative versions of those embodiments, the method may further comprise: in response to determining that the human representative indicates that the particular state does not correspond to the target state, causing rendering of another synthesized speech audio data including another synthesized speech at an additional client device associated with the human representative to request that the secondary telephone call be transferred to another human representative associated with the identified entity.
In some embodiments, the method may further comprise: identifying a task to be performed on behalf of the given user during the secondary telephone call based on the user input or the additional user input; after navigating the IVR tree to the target state, and during the secondary telephone call, engage in a conversation with a human representative associated with the identified entity to complete the task on behalf of the given user. Participating in the conversation with the human representative may include: causing rendering of synthesized speech audio data including synthesized speech for completing the task at an additional client device associated with the human representative, processing audio data that captures a spoken utterance of the human representative using a speech recognition model, and the audio data being received in response to the rendering of the synthesized speech, and determining a result associated with performance of the task based on processing the audio data. In some versions of those embodiments the method may further include: generating a notification based on a result associated with execution of the task; and causing the notification to be rendered at the client device associated with the given user.
In some embodiments, the method may further comprise: in response to determining that the navigation of the IVR tree is not modified by the another additional user input received in response to rendering one or more of the additional prompts, continuing execution of the secondary phone call to navigate the IVR tree to the target state.
In some embodiments, the method may further comprise: in response to identifying the IVR tree associated with the identified entity, obtaining one or more prompts related to navigating the IVR tree associated with the identified entity, and causing one or more of the prompts to be rendered at the client device associated with the given user. The additional user input may be received in response to rendering one or more of the prompts.
In some implementations, the at least one user interaction at the client device associated with the given user may include one or more of: a search interaction at the client device associated with the identified entity, a browse interaction at the client device associated with the identified entity, or a navigation interaction at the client device associated with the identified entity.
In some implementations, automatically navigating the secondary phone call to the target state of the IVR tree can include one or more of: determining one or more values associated with corresponding parameters for navigating the IVR tree based on the user input or the additional user input. In some versions of those embodiments, modifying the navigation of the IVR may include determining an alternate target state of the IVR tree based on the user interaction or the another additional user input. Performing the modification of the secondary phone call may include automatically navigating the secondary phone call to the alternate target state of the IVR tree based on one or more of the values associated with the corresponding parameter. In some further versions of those embodiments, modifying the navigation of the IVR may include determining one or more alternative target states of the IVR tree based on the user interaction or the another additional user input. Performing the modification of the secondary phone call may include automatically navigating the secondary phone call to the target state of the IVR tree based on one or more alternative values associated with the corresponding parameter.
In some implementations, automatically navigating the secondary phone call to the target state of the IVR tree can include one or more of: simulating rendering one or more button presses at additional client devices associated with the identified entity to automatically navigate the IVR tree until the target state is reached; causing rendering of synthesized speech audio data comprising synthesized speech at additional client devices associated with the identified entity to automatically navigate the IVR tree until the target state is reached; or cause rendering of audio data that captures a spoken utterance of the given user associated with the client device at an additional client device associated with the identified entity.
In some implementations, a method implemented by one or more processors is provided and includes: receiving user input from a given user via a search interface of a client device associated with the given user; issuing, at the search interface of the client device, a search query based on the user input; identifying, based on the issued search query, a given search result associated with an entity to be engaged on behalf of the given user during an assisted telephone call, the given search result associated with a target state from among a plurality of candidate states for an Interactive Voice Response (IVR) tree associated with the identified entity; causing rendering of at least the given search result at the search interface of the client device; receiving, via the search interface of the client device, a selection of the given search result from the given user in response to rendering the given search result at the search interface of the client device; and initiating execution of the secondary telephone call in response to receiving the selection of the given search result. The performing of the secondary phone call includes automatically navigating the secondary phone call to the target state of the IVR tree.
These and other embodiments of the technology disclosed herein can optionally include one or more of the following features.
In some embodiments, the method may further comprise: identifying additional given search results associated with the entity based on the issued search query, the additional given search results being associated with additional target states for the IVR tree among the plurality of candidate states. In some versions of those embodiments the method may further include causing the additional given search result to be rendered with the given search result. In some further versions of those embodiments, causing the additional given search result to be rendered with the given search result may include rendering the given search result more prominently than the additional given search result.
In still other versions of those embodiments, rendering the given search result more prominently than the additional given search result may include rendering the given search result more prominently than the additional given search result based on the given user's recent user interaction with the client device. In still further versions of those embodiments, the recent user interaction of the given user with the client device may include the given user accessing content associated with the given search result. In yet another additional or alternative version of those embodiments, rendering the given search result more prominently than the additional given search result may include rendering the given search result more prominently than the additional given search result based on a popularity of the given search result. In still further versions of those embodiments, the popularity of the given search result may be based on click-through rates of a plurality of users other than the given user associated with the given search result.
In some implementations, a method implemented by one or more processors is provided and includes: receiving, from a given user via a client device associated with the given user, a user input for initiating a secondary telephone call; identifying, based on the user input, an entity to be engaged on behalf of the given user during the secondary telephone call; identifying an Interactive Voice Response (IVR) tree associated with the identified entity, the IVR tree including a plurality of candidate states; determining availability of the plurality of candidate states as target states of the IVR tree based on context information associated with the client device or the given user associated with the client device; and determining the target state of the IVR tree from among the plurality of candidate states available based on the user input or additional user input; and initiating execution of the secondary phone call to navigate the IVR tree to the target state. The performing of the secondary phone call includes automatically navigating the secondary phone call to the target state of the IVR tree.
These and other embodiments of the technology disclosed herein can optionally include one or more of the following features.
In some implementations, the contextual information can be associated with the client device, and the contextual information can include one or more of: a current location of the client device, a current time at the current location of the client device, or a current date at the current location of the client device. In some versions of those embodiments, the given IVR tree selected from among the plurality of different IVR trees may be based on one or more of: the current location of the client device, or the current time at the current location of the client device.
In some implementations, the contextual information may be associated with the given user associated with the client device, and wherein the contextual information may include one or more of: an electronic communication created at or received at the client device, user account information associated with a user account of the given user, or monetary information associated with the given user. In some versions of those embodiments, the given IVR tree selected from among the plurality of different IVR trees may be based on one or more of: an electronic communication created at or received at the client device, user account information associated with the user account of the given user, or monetary information associated with the given user.
Further, some embodiments include one or more processors (e.g., central Processing Units (CPUs), graphics Processing Units (GPUs), and/or Tensor Processing Units (TPUs)) of the one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to cause performance of any of the above-described methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the above-described methods. Some embodiments also include a computer program product comprising instructions executable by one or more processors to perform any of the above-described methods.
It should be understood that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the beginning of this disclosure are contemplated as being part of the subject matter disclosed herein.
Claims (31)
1. A method implemented by one or more processors, the method comprising:
receiving, from a given user via a client device associated with the given user, a user input to initiate a secondary telephone call;
identifying, based on the user input, an entity to be engaged on behalf of the given user during the secondary telephone call;
identifying an interactive voice response, IVR, tree associated with the identified entity, the IVR tree including a plurality of candidate states;
determining a target state of the IVR tree from among the plurality of candidate states based on the user input or additional user input; and
initiating execution of the secondary phone call to navigate the IVR tree to the target state, wherein execution of the secondary phone call comprises automatically navigating the secondary phone call to the target state of the IVR tree; and
during execution of the secondary telephone call, and while the IVR tree is navigated to the target state:
detecting at least one user interaction directed to the client device and associated with the identified entity, an
Generating one or more prompts related to modifying navigation of the IVR tree based on the at least one user interaction directed to the client device, an
In response to determining another additional user input received in response to rendering one or more of the additional prompts, modifying navigation of the IVR tree:
modifying performance of the secondary telephone call based on the other additional user input.
2. The method of claim 1, further comprising:
after navigating the IVR tree to reach a particular state during the secondary telephone call, and based on the other additional user input:
generating a notification requesting the given user associated with the client device to join the secondary telephone call, an
Causing the notification to be rendered at the client device associated with the given user.
3. The method of claim 2, wherein causing the notification to be rendered at the client device associated with the given user comprises one or more of: causing the client device to vibrate such that the client device audibly renders the notification requesting the given user to join the secondary phone call or such that the client device visually renders the notification requesting the given user to join the secondary phone call.
4. The method of claim 2 or claim 3, further comprising:
after navigating the IVR tree to the particular state:
engage in a conversation with a human representative associated with the identified entity to verify whether the particular state corresponds to the target state.
5. The method of claim 4, wherein participating in the conversation with the human representative to verify whether the particular state corresponds to the target state comprises:
processing audio data that captures a spoken utterance of the human representation using a speech recognition model; and
determining whether the human representative verifies that the particular state corresponds to the target state based on processing the audio data,
wherein generating the notification requesting the given user associated with the client device to join the secondary telephone call is in response to determining that the human representative verifies that the particular state corresponds to the target state.
6. The method of any of the preceding claims, further comprising:
causing rendering of synthesized speech audio data comprising synthesized speech for verifying whether the IVR tree was successfully navigated at an additional client device associated with the human representative,
wherein the audio data that captures the spoken utterance of the human representative is received in response to rendering the synthesized speech audio data at the additional client device associated with the human representative.
7. The method of claim 4, further comprising:
in response to determining that the human representative indicates that the particular state does not correspond to the target state:
causing rendering of another synthesized speech audio data including another synthesized speech at the additional client device associated with the human representative to request that the secondary telephone call be transferred to another human representative associated with the identified entity.
8. The method of any of the preceding claims, further comprising:
identifying a task to be performed on behalf of the given user during the secondary telephone call based on the user input or the additional user input;
after navigating the IVR tree to the target state, and during the secondary telephone call:
engaging in a conversation with a human representative associated with the identified entity to complete the task on behalf of the given user, wherein engaging in the conversation with the human representative comprises:
causing rendering of synthesized speech audio data including synthesized speech for completing the task at an additional client device associated with the human representative,
processing audio data capturing a spoken utterance of the human representation using a speech recognition model, and the audio data being received in response to rendering of the synthesized speech, an
Determining a result associated with performance of the task based on processing the audio data.
9. The method of claim 8, further comprising:
generating a notification based on a result associated with execution of the task; and
causing the notification to be rendered at the client device associated with the given user.
10. The method of any one of the preceding claims, further comprising:
in response to determining that the other additional user input received in response to rendering one or more of the additional prompts does not modify the navigation of the IVR tree:
continuing to execute the secondary telephone call to navigate the IVR tree to the target state.
11. The method of any of the preceding claims, further comprising:
in response to identifying the IVR tree associated with the identified entity:
obtaining one or more prompts related to navigating the IVR tree associated with the identified entity, an
Cause one or more of the cues to be rendered at the client device associated with the given user,
wherein the additional user input is received in response to rendering one or more of the prompts.
12. The method of any of the preceding claims, wherein the at least one user interaction at the client device associated with the given user comprises one or more of: a search interaction at the client device associated with the identified entity, a browsing interaction at the client device associated with the identified entity, or a navigation interaction at the client device associated with the identified entity.
13. The method of any preceding claim, wherein automatically navigating the secondary telephone call to the target state of the IVR tree comprises one or more of:
determining one or more values associated with corresponding parameters for navigating the IVR tree based on the user input or the additional user input.
14. The method of claim 13, wherein modifying the navigation of the IVR comprises:
determining an alternate target state for the IVR tree based on the user interaction or the another additional user input,
wherein performance of the modification of the secondary telephone call comprises automatically navigating the secondary telephone call to the alternate target state of the IVR tree based on one or more of the values associated with the corresponding parameter.
15. The method of claim 13, wherein modifying the navigation of the IVR comprises:
determining one or more alternative target states for the IVR tree based on the user interaction or the another additional user input,
wherein performance of the modification of the secondary phone call comprises automatically navigating the secondary phone call to the target state of the IVR tree based on one or more alternative values associated with the corresponding parameter.
16. The method of any preceding claim, wherein automatically navigating the secondary telephone call to the target state of the IVR tree comprises one or more of:
simulating rendering one or more button presses at additional client devices associated with the identified entity to automatically navigate the IVR tree until the target state is reached,
causing rendering of synthesized speech audio data including synthesized speech at additional client devices associated with the identified entity to automatically navigate the IVR tree until the target state is reached, or
Causing rendering, at an additional client device associated with the identified entity, audio data that captures a spoken utterance of the given user associated with the client device.
17. A method implemented by one or more processors, the method comprising:
receiving user input from a given user via a search interface of a client device associated with the given user;
issuing, at the search interface of the client device, a search query based on the user input;
identifying, based on the issued search query, a given search result associated with an entity to be engaged on behalf of the given user during the secondary telephone call, the given search result associated with a target state from among a plurality of candidate states for an interactive voice response, IVR, tree associated with the identified entity;
causing rendering of at least the given search result at the search interface of the client device;
receiving, via the search interface of the client device, a selection of the given search result from the given user in response to rendering the given search result at the search interface of the client device; and
in response to receiving the selection of the given search result, initiating execution of the secondary phone call, wherein execution of the secondary phone call comprises automatically navigating the secondary phone call to the target state of the IVR tree.
18. The method of claim 17, further comprising:
identifying additional given search results associated with the entity based on the issued search query, the additional given search results being associated with additional target states for the IVR tree among the plurality of candidate states.
19. The method of claim 18, further comprising:
causing the additional given search result to be rendered with the given search result.
20. The method of claim 19, wherein causing the additional given search result to be rendered with the given search result comprises rendering the given search result more prominently than the additional given search result.
21. The method of claim 20, wherein rendering the given search result more prominently than the additional given search result comprises rendering the given search result more prominently than the additional given search result based on a recent user interaction of the given user with the client device.
22. The method of claim 21, wherein the recent user interaction of the given user with the client device includes the given user accessing content associated with the given search result.
23. The method of any of claims 20-22, wherein rendering the given search result more prominently than the additional given search result includes rendering the given search result more prominently than the additional given search result based on a popularity of the given search result.
24. The method of any of claims 20-23, wherein the popularity of the given search result is based on click-through rates of a plurality of users other than the given user associated with the given search result.
25. A method implemented by one or more processors, the method comprising:
receiving, from a given user via a client device associated with the given user, a user input to initiate a secondary telephone call;
identifying, based on the user input, an entity to be engaged on behalf of the given user during the secondary telephone call;
identifying an interactive voice response, IVR, tree associated with the identified entity, the IVR tree including a plurality of candidate states;
determining availability of the plurality of candidate states as target states of the IVR tree based on context information associated with the client device or the given user associated with the client device; and
determining the target state of the IVR tree from among the plurality of candidate states available based on the user input or additional user input; and
initiating execution of the secondary phone call to navigate the IVR tree to the target state, wherein execution of the secondary phone call comprises automatically navigating the secondary phone call to the target state of the IVR tree.
26. The method of claim 25, wherein the contextual information is associated with the client device, and wherein the contextual information comprises one or more of: a current location of the client device, a current time at a current location of the client device, or a current date at the current location of the client device.
27. The method as claimed in claim 26, wherein a given IVR tree selected from among a plurality of different IVR trees is based on one or more of: a current location of the client device, or a current time at the current location of the client device.
28. The method of any of claims 25 to 27, wherein the contextual information is associated with the given user associated with the client device, and wherein the contextual information comprises one or more of: an electronic communication created at or received at the client device, user account information associated with a user account of the given user, or monetary information associated with the given user.
29. The method of any of claims 25 to 28, wherein a given IVR tree selected from among a plurality of different IVR trees is based on one or more of: an electronic communication created at or received at the client device, user account information associated with a user account of the given user, or monetary information associated with the given user.
30. At least one computing device comprising:
at least one processor; and
a memory storing instructions that, when executed, cause the at least one processor to perform the method of any of claims 1 to 29.
31. A non-transitory computer-readable storage medium storing instructions that, when executed, cause the at least one processor to perform the method of any one of claims 1-29.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063088178P | 2020-10-06 | 2020-10-06 | |
US63/088,178 | 2020-10-06 | ||
US17/068,511 | 2020-10-12 | ||
US17/068,511 US11303749B1 (en) | 2020-10-06 | 2020-10-12 | Automatic navigation of an interactive voice response (IVR) tree on behalf of human user(s) |
PCT/US2020/064918 WO2022076009A1 (en) | 2020-10-06 | 2020-12-14 | Automatic navigation of an interactive voice response (ivr) tree on behalf of human user(s) |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115668904A true CN115668904A (en) | 2023-01-31 |
Family
ID=80932507
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080100936.5A Pending CN115668904A (en) | 2020-10-06 | 2020-12-14 | Automatic navigation of an Interactive Voice Response (IVR) tree on behalf of a human user |
Country Status (6)
Country | Link |
---|---|
US (3) | US11303749B1 (en) |
EP (1) | EP4128730A1 (en) |
JP (1) | JP2023535120A (en) |
KR (2) | KR20240006719A (en) |
CN (1) | CN115668904A (en) |
WO (1) | WO2022076009A1 (en) |
Families Citing this family (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11303749B1 (en) * | 2020-10-06 | 2022-04-12 | Google Llc | Automatic navigation of an interactive voice response (IVR) tree on behalf of human user(s) |
US20220294903A1 (en) * | 2021-03-12 | 2022-09-15 | Avaya Management L.P. | Virtual private agent for machine-based interactions with a contact center |
AU2021448947A1 (en) * | 2021-06-01 | 2023-12-21 | Paymentus Corporation | Methods, apparatuses, and systems for dynamically navigating interactive communication systems |
US11637927B2 (en) * | 2021-07-15 | 2023-04-25 | International Business Machines Corporation | Automated chatbot generation from an interactive voice response tree |
US20230058051A1 (en) * | 2021-08-17 | 2023-02-23 | Toshiba Tec Kabushiki Kaisha | System and method for voice activated file transfer |
US11895270B2 (en) * | 2022-02-17 | 2024-02-06 | Lenovo (Singapore) Pte. Ltd | Phone tree traversal system and method |
JP7438479B1 (en) | 2023-10-24 | 2024-02-27 | 株式会社IVRy | Automatic voice response device, automatic voice response method, automatic voice response program, and automatic voice response system |
Family Cites Families (124)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5815566A (en) | 1991-10-10 | 1998-09-29 | Executone Information Systems, Inc. | Apparatus and method for dynamic inbound/outbound call management and for scheduling appointments |
US6061433A (en) * | 1995-10-19 | 2000-05-09 | Intervoice Limited Partnership | Dynamically changeable menus based on externally available data |
US6377567B1 (en) | 1997-07-16 | 2002-04-23 | Mci Communications Corporation | System and method for distributing data collected from call center services |
US6157705A (en) * | 1997-12-05 | 2000-12-05 | E*Trade Group, Inc. | Voice control of a server |
US6304653B1 (en) | 1998-12-04 | 2001-10-16 | At&T Corp. | Method and apparatus for intelligent data network call setup |
US7792773B2 (en) | 2002-10-23 | 2010-09-07 | Genesys Telecommunications Laboratories, Inc. | Method and system for enabling automated and real-time discovery of skills available to agents and systems in a multimedia communications network |
US6731725B1 (en) | 1999-06-28 | 2004-05-04 | Keith A. Merwin | Computerized system for the receipt, recordation, scheduling and redelivery of telephone messages |
US6922465B1 (en) | 1999-10-14 | 2005-07-26 | Gte Mobilnet Incorporated | Method and system for reporting events in telecommunication networks |
US6757362B1 (en) | 2000-03-06 | 2004-06-29 | Avaya Technology Corp. | Personal virtual assistant |
WO2001067225A2 (en) | 2000-03-06 | 2001-09-13 | Kanisa Inc. | A system and method for providing an intelligent multi-step dialog with a user |
JP4167057B2 (en) | 2000-09-01 | 2008-10-15 | エリザ コーポレーション | Speech recognition method and system for determining the status of outgoing telephone calls |
US20030009530A1 (en) | 2000-11-08 | 2003-01-09 | Laurent Philonenko | Instant message presence protocol for facilitating communication center activity |
US7299259B2 (en) | 2000-11-08 | 2007-11-20 | Genesys Telecommunications Laboratories, Inc. | Method and apparatus for intelligent routing of instant messaging presence protocol (IMPP) events among a group of customer service representatives |
US20040240642A1 (en) | 2001-06-18 | 2004-12-02 | Crandell Jeffrey L. | Apparatus, systems and methods for managing incoming and outgoing communication |
US7609829B2 (en) | 2001-07-03 | 2009-10-27 | Apptera, Inc. | Multi-platform capable inference engine and universal grammar language adapter for intelligent voice application execution |
US7362854B2 (en) | 2001-09-28 | 2008-04-22 | Gateway Inc. | Portable electronic device having integrated telephony and calendar functions |
CA2429171C (en) | 2002-06-27 | 2016-05-17 | Yi Tang | Voice controlled business scheduling system and method |
US20040213384A1 (en) | 2003-04-23 | 2004-10-28 | Alles Harold Gene | Remote access, control, and support of home automation system |
US20050147227A1 (en) | 2003-12-31 | 2005-07-07 | France Telecom, S.A. | Method and system for alerting call participant of a change in a call hold status |
WO2005067595A2 (en) | 2004-01-05 | 2005-07-28 | Tools For Health, Inc. | System for remote control of an automated call system |
US7697026B2 (en) | 2004-03-16 | 2010-04-13 | 3Vr Security, Inc. | Pipeline architecture for analyzing multiple video streams |
US7084758B1 (en) | 2004-03-19 | 2006-08-01 | Advanced Micro Devices, Inc. | Location-based reminders |
US7933260B2 (en) | 2004-06-29 | 2011-04-26 | Damaka, Inc. | System and method for routing and communicating in a heterogeneous network environment |
US7646858B2 (en) | 2004-08-06 | 2010-01-12 | Powerphone, Inc. | Protocol builder for a call handling system |
US8185399B2 (en) | 2005-01-05 | 2012-05-22 | At&T Intellectual Property Ii, L.P. | System and method of providing an automated data-collection in spoken dialog systems |
US20060215824A1 (en) | 2005-03-28 | 2006-09-28 | David Mitby | System and method for handling a voice prompted conversation |
US7869583B2 (en) | 2005-08-05 | 2011-01-11 | International Business Machines Corporation | Real time meeting setup service |
US7856257B2 (en) | 2005-08-22 | 2010-12-21 | Panasonic Corporation | Portable communications terminal |
AU2005338854A1 (en) | 2005-12-06 | 2007-06-14 | Daniel John Simpson | Interactive natural language calling system |
US7773731B2 (en) | 2005-12-14 | 2010-08-10 | At&T Intellectual Property I, L. P. | Methods, systems, and products for dynamically-changing IVR architectures |
JP4197344B2 (en) | 2006-02-20 | 2008-12-17 | インターナショナル・ビジネス・マシーンズ・コーポレーション | Spoken dialogue system |
JP2008015439A (en) | 2006-07-07 | 2008-01-24 | Takashi I | Voice recognition system |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
WO2008095002A1 (en) | 2007-01-30 | 2008-08-07 | Eliza Corporation | Systems and methods for producing build calls |
JP4311458B2 (en) | 2007-02-27 | 2009-08-12 | ブラザー工業株式会社 | Control arrangement information setting device and computer program |
US8022807B2 (en) | 2007-06-15 | 2011-09-20 | Alarm.Com Incorporated | Alarm system with two-way voice |
US20090022293A1 (en) | 2007-07-18 | 2009-01-22 | Routt Karen E | Telecommunications System for Monitoring and for Enabling a Communication Chain between Care Givers and Benefactors and for Providing Alert Notification to Designated Recipients |
US10069924B2 (en) | 2007-07-25 | 2018-09-04 | Oath Inc. | Application programming interfaces for communication systems |
US8676273B1 (en) | 2007-08-24 | 2014-03-18 | Iwao Fujisaki | Communication device |
US20090089100A1 (en) | 2007-10-01 | 2009-04-02 | Valeriy Nenov | Clinical information system |
US7945456B2 (en) | 2007-10-01 | 2011-05-17 | American Well Corporation | Documenting remote engagements |
US8175651B2 (en) | 2007-12-17 | 2012-05-08 | Motorola Mobility, Inc. | Devices and methods for automating interactive voice response system interaction |
JP5189858B2 (en) | 2008-03-03 | 2013-04-24 | アルパイン株式会社 | Voice recognition device |
US7734029B2 (en) | 2008-03-17 | 2010-06-08 | Transcend Products, Llc | Apparatus, system, and method for automated call initiation |
US9008628B2 (en) | 2008-05-19 | 2015-04-14 | Tbm, Llc | Interactive voice access and notification system |
US20140279050A1 (en) | 2008-05-21 | 2014-09-18 | The Delfin Project, Inc. | Dynamic chatbot |
US9003300B2 (en) | 2008-10-03 | 2015-04-07 | International Business Machines Corporation | Voice response unit proxy utilizing dynamic web interaction |
US8644488B2 (en) | 2008-10-27 | 2014-02-04 | Nuance Communications, Inc. | System and method for automatically generating adaptive interaction logs from customer interaction text |
US8943394B2 (en) | 2008-11-19 | 2015-01-27 | Robert Bosch Gmbh | System and method for interacting with live agents in an automated call center |
US20100228590A1 (en) | 2009-03-03 | 2010-09-09 | International Business Machines Corporation | Context-aware electronic social networking |
JP5277067B2 (en) * | 2009-04-27 | 2013-08-28 | 株式会社日立システムズ | IVR system |
US8611876B2 (en) | 2009-10-15 | 2013-12-17 | Larry Miller | Configurable phone with interactive voice response engine |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US8903073B2 (en) | 2011-07-20 | 2014-12-02 | Zvi Or-Bach | Systems and methods for visual presentation and selection of IVR menu |
US9001819B1 (en) * | 2010-02-18 | 2015-04-07 | Zvi Or-Bach | Systems and methods for visual presentation and selection of IVR menu |
US20110270687A1 (en) | 2010-04-30 | 2011-11-03 | Bazaz Gaurav | Apparatus and method for controlled delivery of direct marketing messages |
AU2011316871A1 (en) | 2010-10-21 | 2013-05-23 | Micro Macro Assets Llc | System and method for maximizing efficiency of call transfer speed |
US8938058B2 (en) | 2010-10-21 | 2015-01-20 | Micro Macro Assets Llc | System and method for providing sales and marketing acceleration and effectiveness |
US20120109759A1 (en) | 2010-10-27 | 2012-05-03 | Yaron Oren | Speech recognition system platform |
US8520504B2 (en) | 2010-12-10 | 2013-08-27 | Cable Television Laboratories, Inc. | Method and system of controlling state devices operable to support calls between endpoints |
US8660543B2 (en) | 2010-12-21 | 2014-02-25 | Verizon Patent And Licensing Inc. | Call management system |
US8612233B2 (en) | 2011-01-05 | 2013-12-17 | International Business Machines Corporation | Expert conversation builder |
KR20140039194A (en) | 2011-04-25 | 2014-04-01 | 비비오, 인크. | System and method for an intelligent personal timeline assistant |
JP2014522499A (en) | 2011-06-16 | 2014-09-04 | コーニンクレッカ フィリップス エヌ ヴェ | Improved spatial sampling for list-mode PET acquisition using planned table / gantry movement |
US8572264B2 (en) | 2011-07-15 | 2013-10-29 | American Express Travel Related Services Company, Inc. | Systems and methods for state awareness across communication channels and statefully transitioning between communication channels |
US20130060587A1 (en) | 2011-09-02 | 2013-03-07 | International Business Machines Corporation | Determining best time to reach customers in a multi-channel world ensuring right party contact and increasing interaction likelihood |
US20130090098A1 (en) | 2011-10-10 | 2013-04-11 | Arvind Gidwani | Systems and methods for dialing into interactive voice systems with minimal user interaction |
US9098551B1 (en) * | 2011-10-28 | 2015-08-04 | Google Inc. | Method and system for ranking content by click count and other web popularity signals |
US9088650B2 (en) | 2011-11-29 | 2015-07-21 | Impact Dialing, Inc. | Predictive dialing based on simulation |
US20140200928A1 (en) | 2011-12-15 | 2014-07-17 | Advance Response, LLC. | Methods and apparatus for automated web portal and voice system data aggregation |
US8594297B2 (en) | 2011-12-27 | 2013-11-26 | Avaya Inc. | System and method for enhanced call routing |
WO2013102083A1 (en) | 2011-12-30 | 2013-07-04 | Mindforce Consulting, Llc | Designing a real sports companion match-play crowdsourcing electronic game |
US20140310365A1 (en) | 2012-01-31 | 2014-10-16 | Global Relay Communications Inc. | System and Method for Tracking Messages in a Messaging Service |
KR20130099423A (en) | 2012-02-29 | 2013-09-06 | 주식회사 예스피치 | System and method for connecting professional counselor |
KR101959296B1 (en) | 2012-07-23 | 2019-03-18 | 삼성전자 주식회사 | Method and apparatus for calling of a portable terminal |
KR101909141B1 (en) | 2012-07-27 | 2018-10-17 | 엘지전자 주식회사 | Electronic device and method for controlling electronic device |
JP6025037B2 (en) | 2012-10-25 | 2016-11-16 | パナソニックＩｐマネジメント株式会社 | Voice agent device and control method thereof |
US20140122618A1 (en) | 2012-10-26 | 2014-05-01 | Xiaojiang Duan | User-aided learning chatbot system and method |
CN103795877A (en) | 2012-10-29 | 2014-05-14 | 殷程 | Intelligent voice |
US20150358790A1 (en) | 2012-11-12 | 2015-12-10 | ENORCOM Corporation | Automated mobile system |
KR101821358B1 (en) | 2013-01-22 | 2018-01-25 | 네이버 주식회사 | Method and system for providing multi-user messenger service |
US9282157B2 (en) | 2013-03-12 | 2016-03-08 | Microsoft Technology Licensing, Llc | Intermediary API for providing presence data to requesting clients |
KR101543366B1 (en) | 2013-04-05 | 2015-08-11 | 포인트아이 주식회사 | Method And Apparatus for Evaluating Call Or Agent by Using Customer Emotion Index |
WO2014171935A1 (en) | 2013-04-17 | 2014-10-23 | Empire Technology Development Llc | Scheduling computing tasks for multi-processor systems |
US9049274B2 (en) | 2013-05-03 | 2015-06-02 | James Siminoff | System and method for automatic call scheduling |
JP6180022B2 (en) | 2013-09-27 | 2017-08-16 | 株式会社日本総合研究所 | Call center response control system and response control method thereof |
US10075592B2 (en) | 2013-11-19 | 2018-09-11 | Gen3Ventures, LLC | Intelligent call lead generation |
US9189742B2 (en) | 2013-11-20 | 2015-11-17 | Justin London | Adaptive virtual intelligent agent |
US9721570B1 (en) * | 2013-12-17 | 2017-08-01 | Amazon Technologies, Inc. | Outcome-oriented dialogs on a speech recognition platform |
US10469663B2 (en) | 2014-03-25 | 2019-11-05 | Intellisist, Inc. | Computer-implemented system and method for protecting sensitive information within a call center in real time |
US10104225B2 (en) | 2014-04-02 | 2018-10-16 | Softbank Corp. | Communication system and communication method |
CN106604928A (en) | 2014-04-02 | 2017-04-26 | 罗契斯特大学 | Macrocyclic peptidomimetics for alpha-helix mimicry |
US9754284B2 (en) | 2014-05-22 | 2017-09-05 | Excalibur Ip, Llc | System and method for event triggered search results |
US10104178B2 (en) | 2014-05-27 | 2018-10-16 | Genesys Telecommunications Laboratories, Inc. | System for managing communications activated by a trigger event |
US20150347399A1 (en) | 2014-05-27 | 2015-12-03 | Microsoft Technology Licensing, Llc | In-Call Translation |
EP3158691A4 (en) | 2014-06-06 | 2018-03-28 | Obschestvo S Ogranichennoy Otvetstvennostiyu "Speactoit" | Proactive environment-based chat information system |
CN105450876A (en) * | 2014-06-11 | 2016-03-30 | 阿里巴巴集团控股有限公司 | Voice broadcast method and related system |
US9516167B2 (en) | 2014-07-24 | 2016-12-06 | Genesys Telecommunications Laboratories, Inc. | Media channel management apparatus for network communications sessions |
US9407762B2 (en) | 2014-10-10 | 2016-08-02 | Bank Of America Corporation | Providing enhanced user authentication functionalities |
CN105592237B (en) | 2014-10-24 | 2019-02-05 | 中国移动通信集团公司 | A kind of method, apparatus and intelligent customer service robot of session switching |
US9921922B2 (en) | 2014-11-18 | 2018-03-20 | Globally Tele-Connected, Llc | System, method and computer program product for contact information backup and recovery |
US10091356B2 (en) | 2015-01-06 | 2018-10-02 | Cyara Solutions Pty Ltd | Interactive voice response system crawler |
AU2016204717A1 (en) * | 2015-01-06 | 2016-09-29 | Cyara Solutions Pty Ltd | Interactive voice response system crawler |
US10291776B2 (en) | 2015-01-06 | 2019-05-14 | Cyara Solutions Pty Ltd | Interactive voice response system crawler |
CN107113346A (en) | 2015-01-16 | 2017-08-29 | 株式会社Ntt都科摩 | Communication terminal, outflow calling-control method and program |
KR102217220B1 (en) | 2015-02-02 | 2021-02-18 | 삼성전자주식회사 | Device for Performing Call Reservation and Method Thereof |
US20160277569A1 (en) | 2015-03-16 | 2016-09-22 | Tsemed Pitronot Ltd | System and method for coordinating calls between two or more communication devices |
CN106303102A (en) | 2015-06-25 | 2017-01-04 | 阿里巴巴集团控股有限公司 | Automatization's calling-out method, Apparatus and system |
US9473637B1 (en) | 2015-07-28 | 2016-10-18 | Xerox Corporation | Learning generation templates from dialog transcripts |
US20170039194A1 (en) | 2015-08-03 | 2017-02-09 | EDCO Health Information Soultions, Inc. | System and method for bundling digitized electronic records |
US20170061091A1 (en) | 2015-08-26 | 2017-03-02 | Uptake Technologies, Inc. | Indication of Outreach Options for Healthcare Facility to Facilitate Patient Actions |
US20170177298A1 (en) | 2015-12-22 | 2017-06-22 | International Business Machines Corporation | Interacting with a processing stsyem using interactive menu and non-verbal sound inputs |
US20170289332A1 (en) | 2016-03-30 | 2017-10-05 | Tal Lavian | Systems and Methods for Visual Presentation and Selection of IVR Menu |
JP6736691B2 (en) | 2016-06-13 | 2020-08-05 | グーグル エルエルシー | Escalation to a human operator |
US10827064B2 (en) | 2016-06-13 | 2020-11-03 | Google Llc | Automated call requests with status updates |
US10593349B2 (en) | 2016-06-16 | 2020-03-17 | The George Washington University | Emotional interaction apparatus |
US9992335B2 (en) * | 2016-10-28 | 2018-06-05 | Microsoft Technology Licensing, Llc | Caller assistance system |
US20180133900A1 (en) | 2016-11-15 | 2018-05-17 | JIBO, Inc. | Embodied dialog and embodied speech authoring tools for use with an expressive social robot |
US10257352B2 (en) | 2017-03-01 | 2019-04-09 | International Business Machines Corporation | End user experience via distributed option tree generation |
US10666581B2 (en) | 2017-04-26 | 2020-05-26 | Google Llc | Instantiation of dialog process at a particular child node state |
WO2018195875A1 (en) | 2017-04-27 | 2018-11-01 | Microsoft Technology Licensing, Llc | Generating question-answer pairs for automated chatting |
US10447860B1 (en) * | 2017-05-17 | 2019-10-15 | Amazon Technologies, Inc. | Interactive voice response using a cloud-based service |
US10944864B2 (en) | 2019-03-26 | 2021-03-09 | Ribbon Communications Operating Company, Inc. | Methods and apparatus for identification and optimization of artificial intelligence calls |
US11303749B1 (en) * | 2020-10-06 | 2022-04-12 | Google Llc | Automatic navigation of an interactive voice response (IVR) tree on behalf of human user(s) |
-
2020
- 2020-10-12 US US17/068,511 patent/US11303749B1/en active Active
- 2020-12-14 CN CN202080100936.5A patent/CN115668904A/en active Pending
- 2020-12-14 EP EP20839194.6A patent/EP4128730A1/en active Pending
- 2020-12-14 KR KR1020247000568A patent/KR20240006719A/en active Application Filing
- 2020-12-14 WO PCT/US2020/064918 patent/WO2022076009A1/en unknown
- 2020-12-14 KR KR1020227042494A patent/KR102624148B1/en active IP Right Grant
- 2020-12-14 JP JP2022570378A patent/JP2023535120A/en active Pending
-
2022
- 2022-03-09 US US17/690,201 patent/US11843718B2/en active Active
-
2023
- 2023-10-02 US US18/375,857 patent/US20240031483A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022076009A1 (en) | 2022-04-14 |
US20220201119A1 (en) | 2022-06-23 |
EP4128730A1 (en) | 2023-02-08 |
KR102624148B1 (en) | 2024-01-11 |
US11843718B2 (en) | 2023-12-12 |
US11303749B1 (en) | 2022-04-12 |
JP2023535120A (en) | 2023-08-16 |
US20220109753A1 (en) | 2022-04-07 |
KR20230003253A (en) | 2023-01-05 |
KR20240006719A (en) | 2024-01-15 |
US20240031483A1 (en) | 2024-01-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN115668904A (en) | Automatic navigation of an Interactive Voice Response (IVR) tree on behalf of a human user | |
US20240144924A1 (en) | Providing suggested voice-based action queries | |
CN109983430B (en) | Determining graphical elements included in an electronic communication | |
US11727220B2 (en) | Transitioning between prior dialog contexts with automated assistants | |
US20170277993A1 (en) | Virtual assistant escalation | |
US9148394B2 (en) | Systems and methods for user interface presentation of virtual agent | |
US9276802B2 (en) | Systems and methods for sharing information between virtual agents | |
US9679300B2 (en) | Systems and methods for virtual agent recommendation for multiple persons | |
US9262175B2 (en) | Systems and methods for storing record of virtual agent interaction | |
US9560089B2 (en) | Systems and methods for providing input to virtual agent | |
GB2555922A (en) | Selecting chatbot output based on user state | |
US20140164532A1 (en) | Systems and methods for virtual agent participation in multiparty conversation | |
US20140164312A1 (en) | Systems and methods for informing virtual agent recommendation | |
US20140164953A1 (en) | Systems and methods for invoking virtual agent | |
EP2912567A1 (en) | System and methods for virtual agent recommendation for multiple persons | |
JP2024020472A (en) | Semi-delegated calls with automated assistants on behalf of human participants | |
US11790005B2 (en) | Methods and systems for presenting privacy friendly query activity based on environmental signal(s) |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |