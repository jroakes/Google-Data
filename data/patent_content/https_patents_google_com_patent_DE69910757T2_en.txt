DE69910757T2 - WAVELET-BASED FACIAL MOTION DETECTION FOR AVATAR ANIMATION - Google Patents
WAVELET-BASED FACIAL MOTION DETECTION FOR AVATAR ANIMATION Download PDFInfo
- Publication number
- DE69910757T2 DE69910757T2 DE69910757T DE69910757T DE69910757T2 DE 69910757 T2 DE69910757 T2 DE 69910757T2 DE 69910757 T DE69910757 T DE 69910757T DE 69910757 T DE69910757 T DE 69910757T DE 69910757 T2 DE69910757 T2 DE 69910757T2
- Authority
- DE
- Germany
- Prior art keywords
- image
- node
- facial
- feature
- positions
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Lifetime
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/246—Analysis of motion using feature-based methods, e.g. the tracking of corners or segments
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/262—Analysis of motion using transform domain methods, e.g. Fourier domain methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/42—Global feature extraction by analysis of the whole pattern, e.g. using frequency domain transformations or autocorrelation
- G06V10/422—Global feature extraction by analysis of the whole pattern, e.g. using frequency domain transformations or autocorrelation for representing the structure of the pattern or shape of an object therefor
- G06V10/426—Graphical representations
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/44—Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components
- G06V10/443—Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components by matching or filtering
- G06V10/449—Biologically inspired filters, e.g. difference of Gaussians [DoG] or Gabor filters
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
- G06V40/165—Detection; Localisation; Normalisation using facial parts and geometric relationships
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20048—Transform domain processing
- G06T2207/20064—Wavelet transform [DWT]
Abstract
Description
Gebiet der ErfindungField of the Invention
Die vorliegende Erfindung betrifft die dynamische Erkennung von Gesichts-Merkmalen und insbesondere ein auf visueller Basis arbeitendes Bewegungserfassungssystem, das ein in Echtzeit durchgeführtes Auffinden, Verfolgen und Klassifizieren von Gesichts-Merkmalen zur Eingabe in eine Graphik-Maschine ermöglicht, die einen Avatar animiert.The present invention relates to the dynamic recognition of facial features and especially one on visual Basis working motion detection system, which is a real time by run Finding, tracking and classifying facial features Input into a graphics machine that animates an avatar.
Hintergrund der Erfindungbackground the invention
Virtuelle Räume, die mit Avataren gefüllt sind, bieten eine attraktive Möglichkeit zur Ausübung einer gemeinsamen Benutzerumgebung. Existierende gemeinsame Benutzerumgebungen sind jedoch generell ungeeignet zur Erkennung von Gesichts-Merkmalen mit hinreichender Qualität dahingehend, dass ein Benutzer körperlich repräsentiert werden kann, d. h. dass ein Avatar mit dem Erscheinungsbild, dem Gesichtausdruck oder der Gestik des Benutzers erstellt werden kann. Eine qualitativ gute Erkennung von Gesichts-Merkmalen bietet beträchtliche Vorteile, da es sich bei der Mimik um eine bereits seit der Vorzeit bestehende Art der Kommunikation handelt. Somit wird durch eine körperliche Repräsentierung eines Benutzers die Attraktivität virtueller Räume verbessert.Virtual spaces filled with avatars offer an attractive option to exercise a common user environment. Existing shared user environments However, they are generally unsuitable for recognizing facial features with sufficient quality in that a user is physically represents can be d. H. that an avatar with the appearance that Facial expression or the gesture of the user can be created. Good quality detection of facial features offers considerable Advantages, since the facial expressions have been around since time immemorial existing type of communication. Thus, by a physical representation a user's attractiveness virtual spaces improved.
Bei existierenden Verfahren zur Erkennung von Gesichts-Merkmalen werden typischerweise Markierungen verwendet, die am Gesicht einer Person angeheftet werden. Die Verwendung von Markierungenn zur Erfassung von Gesichtsbewegung ist umständlich und hat dazu geführt, dass die Verwendung einer Gesichtsbewegungs-Erfassung auf kostenintensive Anwendungsfälle wie z. B. bei Filmproduktionen beschränkt blieb.With existing methods of detection facial features typically use markers that are pinned to a person’s face. The use of Markers for detecting facial movement is cumbersome and it lead to, that the use of facial motion detection is costly use cases such as B. remained limited in film productions.
Somit besteht ein beträchtlicher Bedarf an auf visueller Basis arbeitenden Bewegungserfassungssystemen, bei denen eine praktische und effiziente Erkennung von Gesichts-Merkmalen implementiert ist. Die vorliegende Erfindung erfüllt diesen Bedarf.So there is a considerable one Need for motion detection systems working on a visual basis, where practical and efficient recognition of facial features is implemented. The present invention fulfills this need.
L. Wiskott et al. beschreiben in "Face Recognition by elastic bunch graph matching", IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, Vol. 19, No. 7, Juli 1997, pp. 775–779, IEEE Comput. Soc. Press, USA ein System, mit dem menschliche Gesichter anhand einzelner Bilder aus einer großen Datei erkannt werden können, die ein Bild pro Person enthält, wobei die Gesichter durch markierte graphische Darstellungen auf der Basis einer Gabor-Elementarwellentransformation repräsentiert sind. Bilddiagramme neuer Gesichter werden durch elastisches Bunch Graph Matching extrahiert und durch eine einfache Ähnlichkeitsfunktion verglichen.L. Wiskott et al. describe in "Face Recognition by elastic bunch graph matching ", IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, Vol. 19, No. July 7, 1997, pp. 775-779, IEEE Comput. Soc. Press, USA a system with which human faces based on individual images from a big one File can be recognized which contains one picture per person the faces by marked graphical representations represents the basis of a Gabor elementary wave transformation are. Image diagrams of new faces are made by elastic bunch Graph matching extracted and by a simple similarity function compared.
T. Maurer et al. erläutern in „Tracking and learning graphs and pose on image sequences of faces", PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, KILLINGTON, VT, USA, 14.–16. Okt. 1996, pp. 176–181, IEEE Comput. Soc. Press, USA ein System, das in der Lage ist, Erkennungsmerkmale wie z. B. die Augen, den Mund oder das Kinn eines Gesichts in Echtzeit-Bildsequenzen zu verfolgen. Das System verfolgt das Gesicht ohne vorherige Kenntnis von Gesichtern, und die Ergebnisse dieses Verfolgungsvorgangs werden verwendet, um den Gesichtsausdruck einzuschätzen. Für die visuellen Merkmale werden Gabor-Filter verwendet.T. Maurer et al. explain in “Tracking and learning graphs and pose on image sequences of faces ", PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, KILLINGTON, VT, USA, 14-16. Oct. 1996, pp. 176-181, IEEE Comput. Soc. Press, USA a system that is capable of identifying features such as B. the eyes, mouth or chin of a face in real-time image sequences to pursue. The system tracks the face without prior knowledge of faces, and the results of this tracking process will be used to assess facial expression. For the visual characteristics Gabor filter used.
Die Europäische Patentanmeldung EP-A-0 807 902 (Cyberclass Limited, 19. November 1997) beschreibt ein Verfahren und eine Vorrichtung zum Erzeugen beweglicher Charakteristika, wobei eine virtuelle Figur erzeugt wird, indem eine sich in Echtzeit verändernde 3D-Wiedergabe der Struktur des Charakteristikums mit einer auf die Struktur-Wiedergabe abgebildeten 3D-Oberflächenwiedergabe des Charakteristikums und mit einer 2D-Wiedergabe häufig veränderter Bereiche der Fläche kombiniert wird.The European patent application EP-A-0 807 902 (Cyberclass Limited, November 19, 1997) describes a method and a device for generating movable characteristics, wherein a virtual figure is created by changing in real time 3D rendering of the structure of the characteristic with one on the 3D surface rendering of the characteristic and structure shown with 2D rendering often modified Areas of the area is combined.
Ein rekursives Verfolgen von Bildpunkten mittels Anpassung markierter Diagramme ist beschrieben von Chandrashekhar et al. in "Recursive Tracking of Image Points using Labelled Graph Matching", PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, 1991.A recursive tracking of pixels diagrams marked by means of adjustment is described by Chandrashekhar et al. in "Recursive Tracking of Image Points using Labeled Graph Matching ", PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, 1991.
Überblick über die ErfindungOverview of the invention
Die Erfindung, wie sie in den Ansprüchen 1 und 21 aufgeführt ist, besteht in einer Vorrichtung und in einem entsprechenden Verfahren zum Erkennen der Gesichtsbewegungen, der Gesichtszüge oder der Gesichtscharakteristik einer Person. Die Ergebnisse der Gesichtserkennung können zum Animieren eines Avatar-Bildes verwendet werden. Die Avatar-Vorrichtung verwendet eine Bildverarbeitungstechnik, die auf Modell-Diagrammen und Gruppen-Diagrammen basiert, welche Bild-Merkmale effizient in Form von Strahlen repräsentieren, die aus Elementarwellen-Transformationen an markanten Punkten eines Gesichts-Bilds, welche leicht identifizierenden Merkmalen entsprechen, zusammengesetzt sind. Das Erkennungssystem ermöglicht das Verfolgen der natürlichen Charakteristika einer Person, ohne dass irgendwelche unnatürlichen Elemente mit den natürlichen Charakteristika der Person interferieren.The invention as set out in claims 1 and 21 listed is an apparatus and a corresponding method to recognize facial movements, facial features or the facial characteristics of a person. The results of facial recognition can can be used to animate an avatar image. The avatar device uses an image processing technique based on model diagrams and group charts based on what image features are efficient in Represent the shape of rays, that from elementary wave transformations at prominent points one Facial image, which correspond to easily identifying features, are composed. The detection system enables the tracking of natural ones Characteristics of a person without any unnatural Elements with the natural Interfering characteristics of the person.
Der Merkmalserkennungsvorgang arbeitet mit einer Sequenz von Einzelbildern, wobei jedes Einzelbild durch eine Elementarwellen-Transformation transformiert wird, um ein transformiertes Einzelbild zu erzeugen. Die Schaltungspunkt-Stellen, die den Wellen-Strahlen eines Modell-Diagramms zu dem transformierten Bild hin zugeordnet sind, werden initialisiert, indem das Modell-Diagramm über das transformierte Einzelbild bewegt wird und das Modell-Diagramm an einer Stelle in dem transformierten Einzelbild platziert wird, an der eine maximale Strahl-Ähnlichkeit zwischen den Elementarwellen-Strahlen an den Schaltungspunkt-Stellen und dem transformierten Einzelbild herrscht. Die Position einer oder mehrerer Schaltungspunkt-Stellen des Modell-Diagramms wird zwischen den Einzelbildern verfolgt. Ein verfolgter Schaltungs punkt wird reinitialisiert, falls die Position des Schaltungspunkts über eine vorbestimmte Positionsbeschränkung zwischen Einzelbildern hinaus abweicht.The feature recognition process works with a sequence of frames, each frame by an elementary wave transformation is transformed to a transformed one Generate single image. The switching point places the wave rays a model diagram associated with the transformed image are initialized by moving the model diagram over the transformed frame is moved and the model diagram is placed at a point in the transformed frame, at which a maximum beam similarity between the elementary wave rays at the node locations and the transformed single image prevails. The position of a or more nodes of the model diagram tracked between frames. A tracked circuit point is reinitialized if the position of the node is above a predetermined position restriction differs between frames.
Gemäß einer Ausführungsform der Erfindung kann das Auffinden der Gesichts-Merkmale auf einem elastischen Bunch Graph Matching zum individuellen Erstellen eines Kopf-Modells basieren. Ferner kann das Gruppen-Diagramm zur Gesichtsbild-Analyse mehrere (z. B. 18) Positions-Schaltungspunkte enthalten, denen Unterscheidungsmerkmale eines menschlichen Gesichts zugeordnet sind.According to one embodiment the invention can find the facial features on an elastic bunch Graph matching based on the individual creation of a head model. Furthermore, the group diagram for face image analysis can have several (e.g. 18) contain position nodes that differentiate them of a human face.
Weitere Merkmale und Vorteile der vorliegenden Erfindung werden ersichtlich aus der folgenden detaillierten Beschreibung der bevorzugten Ausführungsformen im Zusammenhang mit den beigefügten Zeichnungen, in denen die Prinzipien der Erfindung anhand von Beispielen veranschaulicht sind.Other features and advantages of present invention will become apparent from the following detailed Description of the preferred embodiments in context with the attached Drawings illustrating the principles of the invention by way of example are illustrated.
Kurzbeschreibung der ZeichnungenSummary of the drawings
Detaillierte Beschreibung der bevorzugten AusführungsformenDetailed description of the preferred embodiments
Die vorliegende Erfindung besteht in einer Vorrichtung und einem entsprechenden Verfahren zum Erkennen der Gesichtsbewegungen, Gesichtszüge und Gesichtseigenschaften und dgl., um basierend auf der Gesichtserkennung ein Avatar-Bild zu erzeugen und zu animieren. In der Avatar-Vorrichtung wird eine Bildverarbeitungstechnik verwendet, die auf Modell-Diagrammen und Gruppen-Diagrammen basiert, welche Bild-Merkmale effektiv als Strahlen repräsentieren. Die Strahlen werden durch Wellen-Transformationen gebildet, die an Schaltungspunktstellen oder Stellen markanter Merkmale auf einem Bild verarbeitet werden, welche leicht identifizierbaren Merkmalen entsprechen. Die Schaltungspunkte werden erfasst und verfolgt, um ein Avatar-Bild entsprechend den Gesichtsbewegungen der Person zu animieren. Ferner kann bei der Gesichtserkennung eine Strahlen-Ähnlichkeit verwendet werden, um die Gesichtszüge und -eigenschaften der Person zu bestimmen und somit zu ermöglichen, die natürlichen Eigenschaften einer Person zu verfolgen, ohne dass unnatürliche Elemente mit den natürlichen Eigenschaften interferieren können.The present invention exists in a device and a corresponding method for recognition of facial movements, facial features and facial features and the like to make an avatar image based on face recognition to generate and animate. In the avatar device, one Image processing technology used on model diagrams and Group diagrams are based on what image features are effective as rays represent. The rays are formed by wave transformations that at connection point locations or locations of distinctive features on one Image are processed, which are easily identifiable features correspond. The circuit points are captured and tracked to an avatar image corresponding to the person's facial movements animate. Furthermore, a radiation similarity can be used in face recognition used to improve the facial features and characteristics of the person determine and thus enable the natural Tracing a person's properties without adding unnatural elements with the natural Properties can interfere.
Gemäß
Das Bilderzeugungssytem
In dem Gesichtsmerkmals-Vorgang werden
die digitalisierten Bilder Operationen unterzogen, um die Gesichts-Merkmale
der Person aufzufinden (Block
Das Gesichts-Merkmal kann durch elastisches
Graph Matching gemäß
Die Elementarwellen-Transformation
wird nun anhand von
Die Elementarwelle ist eine ebene Welle mit einem durch ein Gaus'sches Fenster beschränkten Wellenvektor k , dessen Größe relativ zur Wellenlänge durch σ parametrisiert ist. Durch den Ausdruck in der Klammer wird die Gleichstrom-Komponente entfernt. Die Amplitude des Wellenvektors k kann wie folgt gewählt werden, wobei ν sich auf die beschriebenen räumlichen Auflösungen bezieht.The elementary wave is a flat one Wave with one through a Gaussian Limited windows Wave vector k, its size relative to the wavelength parameterized by σ is. The expression in the parenthesis removes the DC component. The amplitude of the wave vector k can be chosen as follows, where ν is to the spatial described resolutions refers.
Eine Elementarwelle, die an der Bildposition x zentriert
ist, wird verwendet, um die Elementarwellen-Komponente Jk aus dem Bild mit Grauwellenverteilung I(x)
zu extrahieren.
Der Raum der Wellenvektoren k wird
typischerweise in einer diskreten Hierarchie von 5 Auflösungsniveaus
(die sich durch Halboktaven unterscheiden) und 8 Ausrichtungen auf
jedem Auflösungsniveau
abgetastet (siehe z. B.
Ein markiertes Bild-Diagramm
- 00
- Pupille des rechten Augespupil of the right eye
- 11
- Pupille des linken Augespupil of the left eye
- 22
- oberer Bereich der Naseupper Area of the nose
- 33
- rechte Ecke der rechten Augenbraueright Corner of the right eyebrow
- 44
- linke Ecke der rechten Augenbraueleft Corner of the right eyebrow
- 55
- rechte Ecke der linken Augenbraueright Corner of the left eyebrow
- 66
- linke Ecke der linken Augenbraueleft Corner of the left eyebrow
- 77
- rechtes Nasenlochright Nose hole
- 88th
- Nasenspitzenose
- 99
- linkes Nasenlochleft Nose hole
- 1010
- rechter Mundwinkelright mouth
- 1111
- Mitte der Oberlippecenter the upper lip
- 1212
- linker Mundwinkelleft mouth
- 1313
- Mitte der Unterlippecenter the lower lip
- 1414
- Unterbereich des rechten Ohrssubfield of the right ear
- 1515
- Oberbereich des rechten Ohrsupper area of the right ear
- 1616
- Oberbereich des linken Ohrsupper area of the left ear
- 1717
- Unterbereich des linken Ohrssubfield of the left ear
Zum Wiedergeben eines Gesichts wird
eine Datenstruktur verwendet, die als Gruppen-Diagramm
Wiederum zwecks Auffindens eines Gesichts in einem Einzelbild wird das Diagramm bewegt und skaliert und verzerrt, bis eine Stelle lokalisiert worden ist, an der das Diagramm am besten passt. (Die am besten passenden Strahlen in den Gruppen-Strahlen haben die größte Ähnlichkeit mit den Strahlen, die an den tatsächlichen Positionen der Knotenpunkten aus dem Bild extrahiert werden.) Da sich Gesichts-Merkmale von Gesicht zu Gesicht unterscheiden, ist das Diagramm für die Aufgabe in einer umfassenderen Weise ausgebildet; z. B. werden jedem Knotenpunkt Strahlen des entsprechenden Landmarks zugewiesen, die von 10 bis 100 individuellen Gesichtern abgenommen worden sind.Again, to find one Facing in a single image, the diagram is moved and scaled and distorted until a location where the Diagram fits best. (The best fitting rays in the Group rays have the greatest similarity with the rays that are at the actual positions of the nodes extracted from the image.) Because facial features differ from face to distinguish face, the chart for the task is in a more comprehensive Trained way; z. B. each node will be rays of the corresponding Landmarks assigned from 10 to 100 individual faces have been removed.
Es werden zwei verschiedenen Strahlen-Ähnlichkeitsfunktionen für zwei verschiedene oder sogar komplementäre Aufgaben verwendet. Falls die Komponenten eines Strahls J in der Form mit der Amplitude und der Phase φj geschrieben werden, besteht eine Form der Ähnlichkeit der beiden Strahlen J und J ' in dem normalisierten Skalar-Produkt des Amplituden-Vektors Two different ray similarity functions are used for two different or even complementary tasks. If the components of a beam J are written in the form with the amplitude and the phase φ j , one form of similarity between the two beams J and J 'is the normalized scalar product of the amplitude vector
Die andere Ähnlichkeits-Funktion hat die Form The other similarity function has the form
Diese Funktion enthält einen Relativverschiebungsvektor zur Angabe der Verschiebung zwischen den Bildpunkten, auf die sich die beiden Strahlen beziehen. Wenn während des Diagramm-Anpassens zwei Strahlen verglichen werden, wird die Ähnlichkeit zwischen ihnen in Bezug auf d maximiert, was zu einer präzisen Bestimmung der Strahl-Position führt. Es werden beide Ähnlichkeits-Funktionen verwendet, wobei oft der phasen-unempfindlichen Version (die mit der Relativposition sanft variiert) der Vorzug gegeben wird, wenn ein Diagramm- zuerst angepasst wird, und der phasen-empfindlichen Version, wenn der Strahl präzise positioniert wird.This function contains a relative displacement vector to indicate the displacement between the image points to which the two rays refer. If two beams are compared during the chart fitting, the similarity between them with respect to d is maximized, resulting in a precise determination of the beam position. Both similarity functions are used, with the phase-insensitive version (which varies gently with the relative position) often being given preference when a Tue agramm- is adjusted first, and the phase sensitive version when the beam is precisely positioned.
Nachdem die Gesichts-Merkmale lokalisiert
worden sind, können
die Gesichts-Merkmale über aufeinanderfolgende
Einzelbilder hinweg verfolgt werden, wie
Die Position X_n eines einzelnen
Knotenpunkts in einem Bild I_n der Bildsequenz ist entweder durch Landmark-Suchen
in dem Bild I_nb mittels des oben beschriebenen Lankmark-Suchverfahrens
(Block
Bei einer ersten Verfolgungstechnik wird eine Linearbewegungsvorhersage verwendet. Die Suche nach der entsprechenden Knotenpunkt-Position X_(n + 1) in dem neuen Bild I_(n + 1) wird an einer Position gestartet, die von einer Bewegungsschätzvorrichtung generiert wird. Es wird ein Disparitätsvektor (X_n – X_(n – 1)) berechnet, der unter Annahme einer konstanten Geschwindigkeit die Verschiebung des Knotenpunkts zwischen den vorhergehenden beiden Einzelbildern repräsentiert. Der Disparitäts- oder Verschiebungsvektor D_n kann der Position X_n hinzuaddiert werden, um die Knotenpunkt-Position X_(n + 1) vorherzusagen. Dieses Linearbewegungs-Modell ist besonders vorteilhaft zur Aufnahme einer Bewegung mit konstanter Geschwindigkeit. Das Linearbewegungs-Modell leistet auch dann eine gute Verfolgung, wenn die Einzelbild-Rate im Vergleich mit der Beschleunigung der verfolgten Objekte hoch ist. Das Linearbewegungs-Modell arbeitet jedoch nur unzureichend, falls die Einzelbild-Rate im Vergleich mit der Beschleunigung der Objekte in der Einzelbild-Sequenz zu niedrig ist. Da es für jedes Bewegungsmodell schwierig ist, Objekte unter derartigen Bedingungen nachzuführen, wird die Verwendung einer Kamera mit höherer Einzelbild-Rate empfohlen.With a first tracking technique a linear motion prediction is used. The search for the corresponding node position X_ (n + 1) in the new image I_ (n + 1) is started at a position by a motion estimator is generated. A disparity vector (X_n - X_ (n - 1)) is calculated, the shift assuming a constant speed the node between the previous two frames represents. The disparity or displacement vector D_n can be added to position X_n to the node position Predict X_ (n + 1). This linear motion model is special advantageous for recording a movement at a constant speed. The linear motion model also does a good tracking, when the frame rate compared to the acceleration of the tracked objects is high. However, the linear motion model works insufficient if the frame rate compared to the Acceleration of the objects in the single image sequence too low is. Since it is for every movement model is difficult to objects under such conditions to track, we recommend using a camera with a higher frame rate.
Das Linearbewegungs-Modell kann möglicherweise einen zu großen geschätzten Bewegungsvektor D_n erzeugen, der zu einer Akkumulierung des Fehlers in der Bewegungsschätzung führen könnte. Deshalb kann die Linearvorhersage mit einem Dämpfungsfaktor f D gedämpft werden. Der resultierende geschätzte Bewegungsvektor lautet D_n = f_D*(X_n – X_(n – 1)). Ein geeigneter Dämpfungsfaktor beträgt 0,9. Falls kein vorheriges Einzelbild I_(n – 1) existiert, z. B. für ein Einzelbild unmittelbar nach dem Auffinden eines Landmark, wird der geschätzte Bewegungsvektor auf Null gesetzt (D_n = 0).The linear motion model can possibly one too big estimated Generate motion vector D_n, which leads to an accumulation of the error in motion estimation to lead could. The linear prediction can therefore be damped with a damping factor f D. The resulting estimated Motion vector is D_n = f_D * (X_n - X_ (n - 1)). A suitable damping factor is 0.9. If no previous frame I_ (n - 1) exists, e.g. B. for a single image immediately after finding a landmark, the estimated motion vector set to zero (D_n = 0).
In
Die Verfolgung eines Knotenpunkts an der Gaus'schen Bildpyramide wird generell zuerst auf der gröbsten Ebene durchgeführt, und anschließend geht bewegt sich die Verfolgung zu den feinsten Ebenen hin vor. Auf der gröbsten Gaus'schen Ebene des tatsächlichen Einzelbilds I_(n + 1) wird an der Position X_(n + 1) mittels der gedämpften Linearbewegungs-Schätzung X_(n + 1) = (X_n + D_n) wie oben beschrieben ein Strahl extrahiert und mit dem entsprechenden Strahl verglichen, der auf der gröbsten Gaus'schen Ebene des vorherigen Einzelbilds berechnet worden ist. Aus diesen beiden Strahlen wird die Disparität bestimmt, d. h. der 2D-Vektor R, der aus X_(n + 1) auf die Position weist, die dem Strahl aus dem vorherigen Einzelbild am besten entspricht. Diese neue Position wird X_(n + 1) zugewiesen. Die Disparitätsberechnung wird noch detaillierter beschrieben. Die Position auf der nächstfeineren Gaus'schen Ebene des tatsächlichen Bilds (das 2*X_(n + 1) ist), die der Position X_(n + 1) auf der gröbsten Gaus'schen Ebene entspricht, ist der Startpunkt für die Disparitäts-Berechnung auf dieser nächstfeineren Ebene. Der an diesem Punkt extrahierte Strahl wird mit dem entsprechenden Strahl verglichen, der auf der gleichen Gaus'schen Ebene des vorherigen Einzelbilds berechnet wurde. Dieser Vorgang wird für sämtliche Gaus'schen Ebenen wiederholt, bis die Ebene der feinsten Auflösung erreicht worden ist, oder bis die Gaus'sche Ebene erreicht worden ist, die zum Bestimmen der Position des Knotenpunkts spezifiziert ist, welcher der Position des vorherigen Rahmens entspricht.The pursuit of a node on the Gaus'schen Image pyramid is generally carried out first at the roughest level, and subsequently the pursuit moves to the finest levels. On the coarsest Gaussian level of the actual Frame I_ (n + 1) is at position X_ (n + 1) using the steamed Linear motion estimation X_ (n + 1) = (X_n + D_n) extracts a beam as described above and compared with the corresponding ray, that on the coarsest Gausian plane of the previous one Frame has been calculated. These two rays become the disparity determined, d. H. the 2D vector R, which from X_ (n + 1) to the position points that best matches the beam from the previous frame. This new position is assigned to X_ (n + 1). The disparity calculation will be described in more detail. The position on the next finer Gaussian level of the actual Image (which is 2 * X_ (n + 1)) that corresponds to the position X_ (n + 1) on the coarsest Gaussian level is the starting point for the disparity calculation on this next finer Level. The beam extracted at this point is matched with the corresponding one Beam compared, on the same Gaus level of the previous frame was calculated. This process is repeated for all Gaussian levels, until the level of the finest resolution is reached or until Gaussian Level has been reached to determine the position of the node is specified which corresponds to the position of the previous frame.
In
Nachdem die neue Position des verfolgten Knotenpunkts in dem tatsächlichen Einzelbild bestimmt worden ist, werden an dieser Position die Strahlen auf sämtlichen Gaus'schen Ebenen berechnet. Ein für das vorherige Einzelbild berechnetes gespeichertes Array von Strahlen, das den verfolgten Knotenpunkt repräsentiert, wird dann durch ein für das tatsächliche Einzelbild berechnetes neues Array von Strahlen ersetzt.After the new position of the pursued Node in the actual Frame has been determined, the rays are at this position on all Gaussian levels calculated. One for stored array of rays calculated from the previous frame, that represents the tracked node is then replaced by a for the actual Single frame calculated new array of rays replaced.
Die Verwendung der Gaus'schen Bildpyramide bietet zwei Hauptvorteile: Erstens sind die Bewegungen der Knotenpunkte in Hinblick auf die Pixel auf einer gröberen Ebene viel kleiner als in dem Originalbild, so dass eine Verfolgung einfach dadurch ermöglicht wird, dass nur eine örtliche Bewegung anstelle einer erschöpfenden Suche in einem großen Bildbereich durchgeführt wird. Zweitens erfolgt die Berechnung der Strahlen-Komponenten bei niedrigen Frequenzen sehr viel schneller, da die Berechnung mit einem kleinen Kern-Fenster an einem heruntergetasteten Bild vorgenommen wird, statt mit einem großen Kern-Fenster an dem Bild mit der Originalauflösung.The use of the Gaus pyramid offers two main advantages: first are the movements of the nodes much smaller than in terms of pixels on a coarser level in the original image so tracking is made easy by that just a local Movement instead of exhaustive Search in a big one Image area performed becomes. Secondly, the radiation components are calculated at low frequencies much faster since the calculation with a small core window is done on a downsampled image instead of a huge Core window on the image with the original resolution.
Zu beachten ist, dass die entsprechende Ebene dynamisch gewählt werden kann; beispielsweise kann im Fall des Verfolgens von Gesichts-Merkmalen die entsprechende Ebene in Abhängigkeit von der tatsächlichen Größe des Gesichts gewählt werden. Auch die Größe der Gaus'schen Bildpyramide kann im Verlauf des Verfolgungsvorgangs geändert werden, d. h. die Größe kann vergrößert werden, wenn die Bewegung schneller wird, und verkleinert werden, wenn die Bewegung sich verlangsamt. Typischerweise ist die maximale Knotenpunkt-Bewegung auf der gröbsten Gaus'schen Ebene auf 4 Pixel beschränkt. Ferner ist anzumerken, dass die Bewegungsschätzung oft nur auf der gröbsten Ebene vorgenommen wird.It should be noted that the corresponding Level chosen dynamically can be; for example, in the case of tracking facial features the corresponding level depending from the actual Size of the face chosen become. Also the size of the Gaus pyramid can be changed during the tracking process, i.e. H. the size can be enlarged, when the movement gets faster, and shrink when the Movement slows down. Typically, the maximum node movement is on the coarsest Gaussian level limited to 4 pixels. It should also be noted that the motion estimation is often only at the roughest level is made.
Im folgenden wird die Berechnung des Vektors der Verschiebung zwischen zwei gegebenen Strahlen auf der gleichen Gaus'schen Ebene (d. h, des Disparitätsvektors) beschrieben. Zum Berechnen des Verschiebungsvektors zwischen zwei aufeinanderfolgenden Einzelbildern wird ein Verfahren verwendet, das ursprünglich für die Disparitätsschätzung bei Stereobildern entwickelt wurde, basierend auf D. J. Fleet und A. D. Jepson, "Computation of component image velocity from local phase information", in: International Journal of Computer Vision, Vol. 5, Ausgabe 1, pp. 77–104, 1990, und W. M. Theimer und H. A. Mallot, "Phase-based binocular vergence control and depth reconstruction using active vision", in: CVGIP: Image Understanding, Vol. 60, Ausgabe 3, pp. 343– 358, November 1994.The following is the calculation of the vector of the displacement between two given rays the same Gaussian Level (i.e., the disparity vector) described. To calculate the displacement vector between two successive frames, a method is used that originally for the Disparity estimate at Stereo images was developed based on D. J. Fleet and A. D. Jepson, "Computation of component image velocity from local phase information ", in: International Journal of Computer Vision, Vol. 5, Issue 1, pp. 77-104, 1990, and W. M. Theimer and H. A. Mallot, "Phase-based binocular vergence control and depth reconstruction using active vision ", in: CVGIP: Image Understanding, Vol. 60, edition 3, pp. 343-358, November 1994.
Die starke Variation der Phasen der komplexen Filter-Reaktionen wird explizit verwendet, um die Pixel-Verschiebung mit Subpixel-Genauigkeit zu berechnen (Wiskott, L., "Labeled Graphs and Dynamic Link Matching for Face Recognition and Scene Analysis", Verlag Harri Deutsch, Thun – Frankfurt am Main, Reihe Physik 53 (Dissertation 1995). Durch Schreiben der Reaktion J auf den j-ten Gabor-Filter in Hinblick auf die Amplitude αj und die Phase j kann eine Ähnlichkeitsfunktion definiert werden als The strong variation of the phases of the complex filter reactions is used explicitly to calculate the pixel shift with sub-pixel accuracy (Wiskott, L., "Labeled Graphs and Dynamic Link Matching for Face Recognition and Scene Analysis", publisher Harri Deutsch, Thun - Frankfurt am Main, series Physik 53 (dissertation 1995) By writing the reaction J to the j-th Gabor filter with regard to the amplitude α j and the phase j, a similarity function can be defined as
Unter der Annahme, dass J und J' zwei Strahlen an den Positionen X und X' = X + d sind, kann die Verschiebung d festgestellt werden, indem die Ähnlichkeit S in Bezug auf d maximiert wird, wobei kj die Wellenvektoren repräsentiert, die dem Jj erzeugenden Filter zugeordnet sind. Da die Schätzung von d nur für kleine Verschiebungen, d. h. eine große Überlappung der Gabor-Strahlen präzise ist, werden große Verschiebungsvektoren nur als eine erste Schätzung behandelt, und der Vorgang wird in der folgenden Weise wiederholt. Zuerst werden nur die Filter-Reaktionen der niedrigsten Frequenzebene verwendet, was in einem ersten Schätzwert d_1 resultiert. Als nächstes wird diese Schätzung ausgeführt, und der Strahl J wird an der Position X_1 = x + d_1 neuberechnet, die näher an der Position X' des Strahls J' liegt. Dann werden die beiden niedrigsten Frequenzebenen für die Schätzung der Verschiebung d_2 verwendet, und der Strahl J wird an der Position X_2 = X_1 + d_2 neuberechnet. Dies wird iterierend durchgeführt, bis die höchste verwendete Frequenzebene erreicht ist, und die endgültige Disparität d zwischen den beiden Start-Strahlen J und J' ist gegeben als die Summe d = d_1 + d_2 + .... Somit können Verschiebungen bis zur Hälfte der Wellenlänge des Kerns mit der niedrigsten Frequenz berechnet werden (siehe Wiskott 1995 oben).Assuming that J and J 'are two beams at positions X and X' = X + d, the displacement d can be determined by maximizing the similarity S with respect to d, where k j represents the wave vectors that are assigned to the J j generating filter. Since the estimate of d is precise only for small shifts, ie a large overlap of the Gabor beams, large shift vectors are only treated as a first estimate and the process is repeated in the following manner. First, only the filter reactions of the lowest frequency level are used, which results in a first estimate d_1. Next, this estimation is carried out and the beam J is recalculated at the position X_1 = x + d_1 which is closer to the position X 'of the beam J'. Then the two lowest frequency planes are used to estimate the displacement d_2 and the beam J is recalculated at the position X_2 = X_1 + d_2. This is carried out iteratively until the highest frequency level used is reached, and the final disparity d between the two start beams J and J 'is given as the sum d = d_1 + d_2 + .... Thus, shifts up to half the Wavelength of the core with the lowest frequency can be calculated (see Wiskott 1995 above).
Obwohl die Verschiebungen mittels
floatender Punkt-Nummern bestimmt werden, können Strahlen nur an (ganzzahligen)
Pixel-Positionen extrahiert (d. h. durch Konvolution berechnet)
werden, was in einem systematischen Rundungsfehler führt. Zum
Kompensieren dieses Subpixel-Fehlers Δd müssen die Phasen der komplexen
Gabor-Filter-Reaktionen entsprechend
Ein Verfolgungsfehler kann detektiert
werden, indem geprüft
wird, ob ein Vertrauens- oder Ähnlichkeitswert
kleiner ist als ein vorbestimmter Schwellenwert (Block
Ein Verfolgungsfehler kann auch detektiert
werden, wenn bestimmte geometrische Beschränkungen verletzt werden (Block
Falls der verfolgte Knotenpunkt aufgrund
eines Verfolgungsfehlers ausgeschaltet wird, kann der Knotenpunkt
an der korrekten Position reaktiviert werden (Block
Bei der Verfolgungskorrektur werden
für zahlreiche
verschiedene Haltungen und Skalierungen keine Gruppen-Diagramme
benötigt,
da die Drehung in der Bildebene sowie die Skalierung berücksichtigt
werden kann, indem entweder der örtliche
Bildbereich oder die Strahlen des Gruppen-Diagramms entsprechend
transformiert werden, wie
Die Geschwindigkeit des Reinitialisierungsvorgangs kann erhöht werden, indem die Tatsache genutzt wird, dass die Identität der verfolgten Person während einer Bildsequenz gleichbleibt. Somit kann in einem anfänglichen Lernsitzung eine erste Sequenz der Person aufgenommen werden, wobei die Person ein volles Repertoire frontaler Gesichtausdrücke zeigt. Diese erste Sequenz kann mit hoher Präzision verfolgt werden, indem das oben beschriebene Verfolgungs- und Korrekturschema basierend auf einem großen generalisierten Gruppen-Diagramm verwendet wird, das Information über zahlreiche verschiedene Positionen enthält. Dieser Vorgang kann offline durchgeführt werden und erzeugt ein neues personalisiertes Gruppen-Diagramm. Das personalisierte Gruppen-Diagramm kann dann verwendet werden, um diese Person mit einer schnellen Rate in Echtzeit zu verfolgen, da das personalisierte Gruppen-Diagramm weitaus kleiner ist als das größere, generalisierte Gruppen-Diagramm.The speed of the reinitialization process can be increased by taking advantage of the fact that the identity of the persecuted person remains the same during an image sequence. Thus, a first sequence of the person can be recorded in an initial learning session, the person showing a full repertoire of frontal facial expressions. This first sequence can be tracked with high precision using the tracking and correction scheme described above based on a large generalized group diagram that contains information about numerous different positions. This process can be done offline and creates a new personalized group diagram. The personalized group diagram can then be used to track that person at a fast rate in real time because the personalized group diagram is much smaller than the larger, gene Realized group diagram.
Die Geschwindigkeit des Reinitialisierungsvorgangs kann auch erhöht werden, indem eine Teil-Gruppendiagramm-Reinitialisierung vorgenommen wird. Ein Teilgruppen-Diagramm enthält nur ein Subset der Knotenpunkte eines vollen Gruppen-Diagramms. Das Subset kann so klein sein, dass es nur einen einzigen Knotenpunkt umfasst.The speed of the reinitialization process can also be increased are made by sub-group diagram reinitialization becomes. A subgroup diagram contains only a subset of the nodes a full group diagram. The subset can be so small that it only covers a single node.
Eine Haltungs-Schätzungs-Gruppendiagramm verwendet
eine Familie zweidimensionaler Gruppen-Diagramme, die in der Bildebene
definiert sind. Die unterschiedlichen Diagramme innerhalb einer
Familie berücksichtigen
die unterschiedlichen Haltungen und/oder Skalierungen des Kopfs.
In dem Landmark-Suchvorgang
wird der Versuch unternommen, jedes Gruppen-Diagramm aus der Familie
an das eingegebene Bild anzupassen, um die Haltung oder die Größe des Kopfs
in dem Bild zu bestimmen. Ein Beispiel eines derartigen Haltungseinschätzungs-Vorgangs
ist in
Nach der Anfangs-Anpassung für jedes
Diagramm werden die Ähnlichkeiten
der End-Positionen verglichen (Block
Um ein Gesicht bei beliebigem Abstand von der Kamera robust auffinden zu können, kann ein ähnlicher Ansatz verwendet werden, bei dem zwei oder drei Gruppen-Diagramme mit unterschiedlichen Skalierungen benutzt werden. Dabei wird angenommen, dass das Gesicht in dem Bild die gleiche Skalierung hat wie das Gruppen-Diagramm, das die stärkste Reaktion auf das Gesichts-Bild zeigt.Around a face at any distance Being able to find them robustly from the camera can be a similar one Approach used where two or three group diagrams with different scales can be used. It is assumed that the face in the picture has the same scaling as that Group diagram, which is the strongest Reaction to the face image shows.
Bei einer dreidimensionalen (3D-) Landmark-Suchtechnik, die der oben beschriebenen ähnlich ist, können auch mehrere Gruppen-Diagramme verwendet werden, die an unterschiedliche Haltungen angepasst sind. Bei dem 3D-Ansatz wird jedoch nur eines einziges Gruppen-Diagramm verwendet, das in einem 3D-Raum definiert ist. Die Geometrie des 3D-Raums reflektiert eine durchschnittliche Gesichts- oder Kopf-Geometrie. Durch Extrahieren von Strahlen aus den Bildern der Gesichter mehrerer Personen bei unterschiedlichen Dreh-Graden wird ein 3D-Gruppen-Diagramm erzeugt, das dem 2D-Ansatz analog ist. Jeder Strahl wird nun mit den drei Dreh-Winkeln parametrisiert. Wie bei dem 2D-Ansatz werden die Knotenpunkte an den zweckmäßigen Punkten der Kopf-Oberfläche angeordnet. Dann werden in dem Anpassungs-Vorgang Projektionen des 3D-Diagramms verwendet. Eine wichtige Generalisierung des 3D-Ansatzes besteht darin, dass für jeden Knotenpunkt die zugeordnete parametrisierte Familie von Gruppen-Strahlen an verschiedene Haltungen angepasst ist. Die zweite Generalisierung besteht darin, dass das Diagramm Euklidische Transformationen im 3D-Raum und nicht nur Transformationen in der Bildebene erfahren kann.With a three-dimensional (3D) Landmark search technique similar to that described above can also be used Multiple group diagrams are used that are related to different Attitudes are adjusted. With the 3D approach, however, only one becomes uses only group diagram that defines in a 3D space is. The geometry of the 3D space reflects an average Face or head geometry. By extracting rays from it the images of the faces of several people at different Turning degrees will creates a 3D group diagram that is analogous to the 2D approach. Everyone Beam is now parameterized with the three rotation angles. As in In the 2D approach, the nodes become the appropriate points the head surface arranged. Then projections of the 3D chart used. An important generalization of the 3D approach is that for everyone Node the assigned parameterized family of group rays is adapted to different attitudes. The second generalization is that the diagram Euclidean transformations in 3D space and can not only experience transformations in the image plane.
Der Diagramm-Anpassungsvorgang kann als ein Grob-zu-fein-Ansatz formuliert werden, bei dem zunächst Diagramme mit weniger Knotenpunkten verwendet werden und dann in nachfolgenden Schritten dichtere Diagramme verwendet werden. Der Grob-zu-fein-Ansatz ist besonders zweckmäßig, falls eine Hochpräzisions-Lokalisierung der Merkmals-Punkte in bestimmten Bereichen des Gesichts erwünscht ist. Somit wird Rechenaufwand eingespart, indem ein hierarchischer Ansatz verwendet wird, bei dem die Landmark-Suche zuerst mit gröberer Auflösung durchgeführt wird und anschließend die angepassten Diagramme mit höherer Auflösung geprüft werden, um bestimmte Bereiche detaillierter zu analysieren.The chart adjustment process can can be formulated as a rough-to-fine approach, starting with diagrams be used with fewer nodes and then in subsequent ones Steps denser diagrams can be used. The rough-to-fine approach is particularly useful if high-precision localization the feature points are desired in certain areas of the face. This saves computing time by using a hierarchical approach is used, in which the landmark search is carried out first with a coarser resolution and subsequently the adapted diagrams are checked with higher resolution, to analyze certain areas in more detail.
Ferner kann der Rechenaufwand bei einer Mehrfachprozessormaschine leicht dahingehend aufgeteilt werden, dass, nachdem die groben Bereiche gefunden worden sind, einige wenige Child-Prozesse parallel ihren eigenen Teil des Gesamtbilds zu bearbeiten beginnen. Am Ende der Child-Vorgänge teilen die Vorgänge die Merkmals-Koordinaten, die sie lokalisiert haben, dem Master-Vorgang mit, in dem diese in geeigneter Weise skaliert und kombiniert werden, um sie zurück in das Originalbild einzupassen, so dass die Gesamt-Berechungszeit beträchtlich reduziert wird.Furthermore, the computing effort can a multiprocessor machine can be easily divided that after the rough areas have been found, a few Child processes work in parallel on their own part of the overall picture kick off. At the end of the child operations share the operations Feature coordinates that you have located, the master process with by scaling and combining them in a suitable way, to get them back fit into the original image so that the total calculation time is considerable is reduced.
Gemäß
- – Auge offen/geschlossen
- – Mund offen/geschlossen
- – Zunge sichtbar oder nicht
- – Haartyp-Klassifizierung
- – Gesichtsfalten-Detektion (z. B. an der Stirn)
- - Eye open / closed
- - Mouth open / closed
- - Tongue visible or not
- - Hair type classification
- - facial wrinkle detection (e.g. on the forehead)
Somit können bei der Verfolgung zwei Informationsquellen verwendet werden. Eine Informationsquelle basiert auf den Merkmals-Positionen, d. h. den Knotenpunkt-Positionen, und die andere Informationsquelle basiert auf den Merkmals-Klassen. Die Merkmalsklassen-Information basiert stärker auf der Texdtur und kann, indem der örtliche Bildbereich mit einem Satz gespeicherter Beispiele verglichen wird, mit niedrigerer Auflösung und Verfolgungs-Genauigkeit funktionieren als die Merkmalsklassen-Information, die ausschließlich auf den Knotenpunkt-Positionen basiert.Thus, in pursuit, two Information sources are used. An information source is based on the feature positions, d. H. the node positions, and the other source of information is based on the feature classes. The feature class information is based more on the texture and can by the local Image area is compared to a set of saved examples, with lower resolution and tracking accuracy function as the feature class information, the only based on the node positions.
Die Gesichts-Erkennung gemäß der Erfindung
kann gemäß
Das generische Gesichts-Modell kann an eine repräsentative Anzahl von Individuen angepasst werden und kann so ausgelegt werden, dass eine realistische Animation und die Wiedergabe eines weiten Bereichs von Gesichtszügen und/oder -ausdrücken durchgeführt werden kann. Das generische Modell kann durch die folgenden Techniken erstellt werden.The generic face model can to a representative Number of individuals can be adjusted and can be designed that realistic animation and rendering a wide one Range of facial features and / or expressions carried out can be. The generic model can be created by the following techniques to be created.
- 1. Mono-Kamera-Systeme können verwendet werden, um einen realistischen Avatar zur Verwendung in Low-End – Tele-Immersions-Systemen zu erzeugen (T. Akimoto et al., 1993). Gesichtsprofil-Informationen von Individuen, wie sie aus den sagitalen und koronalen Ebenen wahrgenommen werden, können kombiniert werden, um den Avatar zu erhalten.1. Mono camera systems can be used to create a realistic avatar for use in low-end tele-immersion systems to generate (T. Akimoto et al., 1993). Facial profile information from Individuals as perceived from the sagital and coronal levels can be can be combined to get the avatar.
- 2. Stereo-Kamera-Systeme sind in der Lage, präzise 3D-Messungen durchzuführen, wenn die Kameras voll kalibriert sind. (Kamera-Parameter werden durch einen Kalibrierungsvorgang berechnet.) Dann kann ein individuelles Gesichts-Modell erstellt werden, indem ein generisches Gesichts-Modell an die berechneten 3D-Daten angepasst wird. Da Stereo-Algorithmen keine präzise Information zu nicht texturierten Bereichen liefern, kann eine Projektion aktiv texturierten Lichts verwendet werden.2. Stereo camera systems are capable of precise 3D measurements perform, when the cameras are fully calibrated. (Camera parameters will be calculated by a calibration process.) Then an individual Face model can be created by using a generic face model is adapted to the calculated 3D data. Because stereo algorithms not precise A projection can provide information on non-textured areas actively textured light can be used.
- 3. Gesichts-basierende Stereo-Techniken, bei den die Markierungen an dem individuellen Gesicht verwendet werden, um präzise 3D-Positionen der Markierungen zu berechnen. Die 3D-Information wird dann verwendet, um ein generisches Modell anzupassen.3. Face-based stereo techniques using the markers can be used on the individual face to create precise 3D positions to calculate the markings. The 3D information is then used to customize a generic model.
- 4. Dreidimensionale Digitalisieren, bei denen ein Sensor oder eine Lokalisierungsvorrichtung über jeden zu messenden Oberflächenpunkt bewegt wird.4. Three-dimensional digitizing, where a sensor or a locating device over every surface point to be measured is moved.
- 5. Aktives strukturiertes Licht, bei dem Muster projiziert werden und der resultierende Video-Strom zum Extrahieren von 3D-Messwerten verarbeitet wird.5. Active structured light where patterns are projected and the resulting video stream for extracting 3D measurements is processed.
- 6. Laser-basierte Oberflächenabtastvorrichtungen (wie z. B. die von Cyberware Inc. entwickelten), die präzise Gesichts-Messwerte liefern.6. Laser-based surface scanners (such as those developed by Cyberware Inc.), the precise facial metrics deliver.
- 7. Eine Kombination der vorherigen Techniken.7. A combination of the previous techniques.
Diese verschiedenen Techniken sind weisen für den Benutzer nicht den gleichen Grad an Praktikabilität auf. Einige sind in der Lage, Meswerte über das Individuum auf Einzelzeitpunkt-Basis zu bilden (wobei sich das Gesicht für die Dauer des Messvorgangs in einer gewünschten Position befindet), während andere eine Sammlung von Tastwerten benötigen und in der Verwendung umständlicher sind.These are different techniques ways for the user does not have the same level of practicability. Some are able to take measurements over to form the individual on an individual time basis (whereby the Face for the duration of the measuring process is in a desired position), while others need a collection of tactile values and in use cumbersome are.
Ein generisches dreidimensionales Kopf-Modell für eine bestimmte Person kann unter Verwendung zweier Gesichts-Bilder erzeugt werden, die eine Frontal- und eine Profilansicht zeigen. Die Gesichts-Erkennung ermöglicht eine effiziente und robuste Erzeugung des 3D-Kopf-Modells.A generic three-dimensional Head model for a particular person can use two facial images generated that show a frontal and a profile view. Facial recognition enables efficient and robust generation of the 3D head model.
Die Gesichtskontur-Extraktion wird
zusammen mit der Lokalisierung der Augen, der Nase, des Mundes und
des Kinns der Person durchgeführt.
Diese Merkmals-Lokalisierung kann erhalten werden, indem die elastische
Bunch-Graph-Technik
in Kombination mit hierarchischer Anpassung verwendet wird, um Gesichts-Merkmale
gemäß
Ein Avatar-Bild kann durch die folgenden allgemeinen Techniken animiert werden (siehe F. I. Parke und K. Waters, Computer Facial Animation, A. K. Peters Ltd., Wellesley, Massachusetts 1996).An avatar picture can be created by the following general techniques (see F.I. Parke and K. Waters, Computer Facial Animation, A.K. Peters Ltd., Wellesley, Massachusetts 1996).
- 1. Haupt-Einzelbilderfassung und geometrische Interpolation, wobei eine Anzahl von Haupt-Haltungen und -Ausdrücken definiert werden. Dann wird eine geometrische Interpolation zwischen den Haupt-Einzelbildern vorgenommen, um die Animation zu erzeugen. Ein derartiges System wird häufig als ein arbeitsbasiertes (oder arbeitsgesteuertes) Modell bezeichnet.1. Main single image capture and geometric Interpolation, which defines a number of main attitudes and expressions become. Then there is a geometric interpolation between the main frames made to create the animation. Such a system becomes common referred to as a work-based (or work-driven) model.
- 2. Direkt-Parametrisierung, bei der die Gesichtsausdrücke und die Haltung direkt auf ein Set von Parametern abgebildet werden, die dann zum Steuern des Modells verwendet werden.2. Direct parameterization in which the facial expressions and the posture is mapped directly to a set of parameters, which are then used to control the model.
- 3. Pseudo-Muskel-Modelle, die mittels geometrischer Verformungen Muskelbetätigungen simulieren.3. Pseudo-muscle models using geometric deformations muscle activations simulate.
- 4. Modelle auf Muskel-Basis, bei denen die Muskeln und die Haut mittels physischer Modelle modelliert werden.4. Muscle-based models in which the muscles and skin be modeled using physical models.
- 5. 2D- und 3D-Morphing, wobei ein 2D-Morphing zwischen Bildern in einem Video-Strom verwendet wird, um eine 2D-Animation zu erzeugen. Ein Set von Landmarks wird identifiziert und verwendet, um zwischen zwei Bildern einer Sequenz eine Kette (warp) zu bilden. Eine derartige Technik kann auf 3D erweitert werden (siehe F. F. Pighin, J. Hecker, D. Lischinski, R. Szeliski und D.H. Salesin, Synthesizing Realistic Facial Expressions from Photographs, in: SIGGRAPH 98 Conference Proceedings, pp. 75–84, Juli 1998).5. 2D and 3D morphing, with 2D morphing between images is used in a video stream to generate a 2D animation. A set of landmarks is identified and used to distinguish between two Form a chain (warp) of images of a sequence. Such one Technology can be expanded to 3D (see F. F. Pighin, J. Hecker, D. Lischinski, R. Szeliski and D.H. Salesin, Synthesizing Realistic Facial Expressions from Photographs, in: SIGGRAPH 98 Conference Proceedings, pp. 75-84, July 1998).
- 6. Weitere Ansätze wie z. B. Steuerpunkte und Modelle mit finiten Elementen.6. Other approaches such as B. Control points and models with finite elements.
Bei diesen Techniken wird durch die Gesichts-Erkennung der Animationsvorgang verbessert, indem eine automatische Extraktion und Charakterisierung von Gesichts-Merkmalen durchgeführt wird. Extrahierte Merkmale können verwendet werden, um im Fall von Haupt-Einzelbilderzeugungs- und Interpolations-Modellen Ausdrücke zu interpolieren, oder um bei Direkt-Parametrisierungs-Modellen oder Pseudo-Muskel- oder Muskel-Modellen Parameter zu wählen. Im Falle des 2D- und 3D-Morphing kann die Gesichts-Erkennung verwendet werden, um automatisch Merkmale auf einem Gesicht zu wählen, welche die passende Information zum Durchführen der geometrischen Transformation bilden.With these techniques the Face detection improves the animation process by using a automatic extraction and characterization of facial features carried out becomes. Extracted characteristics can used to be in the case of main single image generation and Interpolation models expressions to interpolate, or for direct parameterization models or to choose pseudo-muscle or muscle models parameters. in the In the case of 2D and 3D morphing, facial recognition can be used to automatically choose which features on a face the appropriate information for performing the geometric transformation form.
Ein Beispiel einer Avatar-Animation,
bei der Gesichtsmerkmals-Verfolgung und- Klassifikation verwendet
werden, lässt
sich anhand von
Gemäß
Um die Bildbereiche übergangslos
in das Einzelbild einzupassen, kann eine Gaus'sche Unschärfeerzeugung verwendet werden.
Für eine
realistische Wiedergabe kann ein örtliches Bild-Morphing benötigt werden,
da die Animation möglicherweise
nicht zusammenhängend
in dem Sinne ist, dass eine Abfolge von Bildern wie durch die Erkennung
vorgegeben präsentiert
werden kann. Das Morphing kann realisiert werden durch lineare Interpolation
entsprechender Punkte in dem Bildraum. Zum Erzeugen von Zwischenbildern
wird eine lineare Interpolation unter Verwendung folgender Gleichungen
angewandt:
Somit kann das rekonstruierte Gesicht
in der entfernten Anzeigevorrichtung zusammengesetzt werden, indem
Teile von Bildern, die den im Lern-Schritt detektierten Gesichtsausdrücken entsprechen,
zusammengefügt
werden. Somit zeigt der Avatar Merkmale, die der die Animation befehlenden
Person entsprechen. Folglich wird bei der Initialisierung ein Set
abgenommener Bilder, die jedem verfolgten Gesichts-Merkmal entsprechen,
und ein "Gesichts-Behälter" als resultierendes
Bild des Gesichts nach dem Entfernen jedes Merkmals verwendet. Die
Animation wird gestartet, und der Gesichtserkennungs-Vorgang wird verwendet,
um spezifische Markierungen zu generieren, die in der bereits beschriebenen
Weise übertragen
werden. Das Dekodieren erfolgt durch Wählen von Bild-Stücken, die
der übertragenen
Kennzeichnung zugeordnet sind, z. B. des Bildes des Munds, das mit
der Kennzeichnung "lächelnder
Mund"
Ein fortgeschritteneres Niveau der
Avatar-Animation kann erreicht werden, wenn die bereits erwähnte dynamische
Textur-Erzeugung mit eher herkömmlichen
Techniken des Volumen-Morphings gemäß
Ein zweckmäßige Erweiterung der auf Sicht
basierenden Avatar-Animation besteht darin, die Gesichts-Erkennung
mit Sprach-Erkennung in integrieren, um die korrekte Lippenbewegung
gemäß
Obwohl vorstehend die bevorzugten Ausführungsformen der Erfindung offenbart sind, wird darauf hingewiesen, dass Fachleute verschiedene Änderungen an den bevorzugten Ausführungsformen vornehmen können, ohne den Bereich der Endung zu verlassen. Die Erfindung ist nur durch die folgenden Ansprüche definiert.Although the preferred ones above embodiments of the invention, it is noted that those skilled in the art various changes on the preferred embodiments can make without leaving the area of the extension. The invention is only by the following claims Are defined.
Claims (22)
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US8161598P | 1998-04-13 | 1998-04-13 | |
US81615P | 1998-04-13 | ||
US188079 | 1998-11-06 | ||
US09/188,079 US6272231B1 (en) | 1998-11-06 | 1998-11-06 | Wavelet-based facial motion capture for avatar animation |
PCT/US1999/007933 WO1999053443A1 (en) | 1998-04-13 | 1999-04-12 | Wavelet-based facial motion capture for avatar animation |
Publications (2)
Publication Number | Publication Date |
---|---|
DE69910757D1 DE69910757D1 (en) | 2003-10-02 |
DE69910757T2 true DE69910757T2 (en) | 2004-06-17 |
Family
ID=26765754
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE69910757T Expired - Lifetime DE69910757T2 (en) | 1998-04-13 | 1999-04-12 | WAVELET-BASED FACIAL MOTION DETECTION FOR AVATAR ANIMATION |
Country Status (10)
Country | Link |
---|---|
US (1) | US6580811B2 (en) |
EP (1) | EP1072018B1 (en) |
JP (2) | JP3970520B2 (en) |
KR (1) | KR100530812B1 (en) |
AT (1) | ATE248409T1 (en) |
AU (1) | AU3639699A (en) |
BR (1) | BR9909611B1 (en) |
CA (1) | CA2327304A1 (en) |
DE (1) | DE69910757T2 (en) |
WO (1) | WO1999053443A1 (en) |
Families Citing this family (212)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
AU3639699A (en) | 1998-04-13 | 1999-11-01 | Eyematic Interfaces, Inc. | Wavelet-based facial motion capture for avatar animation |
DE19837004C1 (en) * | 1998-08-14 | 2000-03-09 | Christian Eckes | Process for recognizing objects in digitized images |
US6714661B2 (en) | 1998-11-06 | 2004-03-30 | Nevengineering, Inc. | Method and system for customizing facial feature tracking using precise landmark finding on a neutral face image |
US7050655B2 (en) * | 1998-11-06 | 2006-05-23 | Nevengineering, Inc. | Method for generating an animated three-dimensional video head |
US7050624B2 (en) | 1998-12-04 | 2006-05-23 | Nevengineering, Inc. | System and method for feature location and tracking in multiple dimensions including depth |
SG91841A1 (en) * | 1999-11-03 | 2002-10-15 | Kent Ridge Digital Labs | Face direction estimation using a single gray-level image |
US6792144B1 (en) | 2000-03-03 | 2004-09-14 | Koninklijke Philips Electronics N.V. | System and method for locating an object in an image using models |
US6948131B1 (en) * | 2000-03-08 | 2005-09-20 | Vidiator Enterprises Inc. | Communication system and method including rich media tools |
US6992665B2 (en) * | 2000-03-29 | 2006-01-31 | Minolta Co., Ltd. | Method and device for fitting surface to point group, modeling device, and computer program |
US7227526B2 (en) | 2000-07-24 | 2007-06-05 | Gesturetek, Inc. | Video-based image control system |
AU2001277148B2 (en) * | 2000-07-24 | 2007-09-20 | Google Llc | Method and system for customizing facial feature tracking using precise landmark finding on a neutral face image |
JP2004509391A (en) * | 2000-07-24 | 2004-03-25 | アイマティック・インターフェイシズ・インコーポレイテッド | Avatar video conversion method and device using expressionless facial image |
DE10063402A1 (en) * | 2000-12-19 | 2002-06-20 | Dietrich Karl Werner | Electronic reproduction of a human individual and storage in a database by capturing the identity optically, acoustically, mentally and neuronally |
US7116330B2 (en) * | 2001-02-28 | 2006-10-03 | Intel Corporation | Approximating motion using a three-dimensional model |
US6917703B1 (en) | 2001-02-28 | 2005-07-12 | Nevengineering, Inc. | Method and apparatus for image analysis of a gabor-wavelet transformed image using a neural network |
US6697072B2 (en) * | 2001-03-26 | 2004-02-24 | Intel Corporation | Method and system for controlling an avatar using computer vision |
US7392287B2 (en) | 2001-03-27 | 2008-06-24 | Hemisphere Ii Investment Lp | Method and apparatus for sharing information using a handheld device |
US8035612B2 (en) | 2002-05-28 | 2011-10-11 | Intellectual Ventures Holding 67 Llc | Self-contained interactive video display system |
US8300042B2 (en) | 2001-06-05 | 2012-10-30 | Microsoft Corporation | Interactive video display system using strobed light |
US7259747B2 (en) * | 2001-06-05 | 2007-08-21 | Reactrix Systems, Inc. | Interactive video display system |
SE0102360D0 (en) * | 2001-07-02 | 2001-07-02 | Smart Eye Ab | Method for image analysis |
EP1282070B1 (en) * | 2001-08-01 | 2004-04-21 | ZN Vision Technologies AG | Hierarchical adaptation of an image model |
US6834115B2 (en) * | 2001-08-13 | 2004-12-21 | Nevengineering, Inc. | Method for optimizing off-line facial feature tracking |
US6853379B2 (en) | 2001-08-13 | 2005-02-08 | Vidiator Enterprises Inc. | Method for mapping facial animation values to head mesh positions |
US6876364B2 (en) | 2001-08-13 | 2005-04-05 | Vidiator Enterprises Inc. | Method for mapping facial animation values to head mesh positions |
CN1273912C (en) | 2001-08-23 | 2006-09-06 | 索尼公司 | Robot apparatus, face recognition method and face recognition apparatus |
US7130446B2 (en) * | 2001-12-03 | 2006-10-31 | Microsoft Corporation | Automatic detection and tracking of multiple individuals using multiple cues |
JP2005514683A (en) * | 2001-12-06 | 2005-05-19 | ニューヨーク・ユニバーシティ | Logic device, data structure, system and method for multi-linear representation of multimodal data populations for synthesis, recognition and compression |
US6831603B2 (en) | 2002-03-12 | 2004-12-14 | Menache, Llc | Motion tracking system and method |
US7710391B2 (en) | 2002-05-28 | 2010-05-04 | Matthew Bell | Processing an image utilizing a spatially varying pattern |
DE10226257A1 (en) | 2002-06-13 | 2003-12-24 | Bosch Gmbh Robert | Method for detecting a person in a room |
AU2003301043A1 (en) | 2002-12-13 | 2004-07-09 | Reactrix Systems | Interactive directed light/sound system |
US20040152512A1 (en) * | 2003-02-05 | 2004-08-05 | Collodi David J. | Video game with customizable character appearance |
US7009561B2 (en) | 2003-03-11 | 2006-03-07 | Menache, Llp | Radio frequency motion tracking system and method |
JP2004289254A (en) | 2003-03-19 | 2004-10-14 | Matsushita Electric Ind Co Ltd | Videophone terminal |
US7711155B1 (en) * | 2003-04-14 | 2010-05-04 | Videomining Corporation | Method and system for enhancing three dimensional face modeling using demographic classification |
US7415148B2 (en) * | 2003-08-04 | 2008-08-19 | Raytheon Company | System and method for detecting anomalous targets including cancerous cells |
US7512255B2 (en) * | 2003-08-22 | 2009-03-31 | Board Of Regents, University Of Houston | Multi-modal face recognition |
JP2005100367A (en) * | 2003-09-02 | 2005-04-14 | Fuji Photo Film Co Ltd | Image generating apparatus, image generating method and image generating program |
CA2440015A1 (en) * | 2003-09-03 | 2005-03-03 | National Research Council Of Canada | Second order change detection in video |
CN1902930B (en) | 2003-10-24 | 2010-12-15 | 瑞克楚斯系统公司 | Method and system for managing an interactive video display system |
US7536032B2 (en) | 2003-10-24 | 2009-05-19 | Reactrix Systems, Inc. | Method and system for processing captured image information in an interactive video display system |
US20050163379A1 (en) * | 2004-01-28 | 2005-07-28 | Logitech Europe S.A. | Use of multimedia data for emoticons in instant messaging |
US20050168485A1 (en) * | 2004-01-29 | 2005-08-04 | Nattress Thomas G. | System for combining a sequence of images with computer-generated 3D graphics |
US7626569B2 (en) * | 2004-10-25 | 2009-12-01 | Graphics Properties Holdings, Inc. | Movable audio/video communication interface system |
TW200620049A (en) * | 2004-12-14 | 2006-06-16 | Bextech Inc | System and method for processing stream video process |
US20060184066A1 (en) * | 2005-02-15 | 2006-08-17 | Baylor College Of Medicine | Method for aiding stent-assisted coiling of intracranial aneurysms by virtual parent artery reconstruction |
JP4645223B2 (en) * | 2005-02-18 | 2011-03-09 | 富士通株式会社 | Face tracking program and face tracking method |
US8235725B1 (en) | 2005-02-20 | 2012-08-07 | Sensory Logic, Inc. | Computerized method of assessing consumer reaction to a business stimulus employing facial coding |
DE602005012673D1 (en) * | 2005-02-21 | 2009-03-26 | Mitsubishi Electric Corp | Fast method for detecting objects through statistical pattern matching |
EP1693782B1 (en) * | 2005-02-21 | 2009-02-11 | Mitsubishi Electric Information Technology Centre Europe B.V. | Method for facial features detection |
GB2438783B8 (en) | 2005-03-16 | 2011-12-28 | Lucasfilm Entertainment Co Ltd | Three-dimensional motion capture |
US7353034B2 (en) | 2005-04-04 | 2008-04-01 | X One, Inc. | Location sharing and tracking using mobile phones or other wireless devices |
US9128519B1 (en) | 2005-04-15 | 2015-09-08 | Intellectual Ventures Holding 67 Llc | Method and system for state-based control of objects |
US8081822B1 (en) | 2005-05-31 | 2011-12-20 | Intellectual Ventures Holding 67 Llc | System and method for sensing a feature of an object in an interactive video display |
US8606950B2 (en) * | 2005-06-08 | 2013-12-10 | Logitech Europe S.A. | System and method for transparently processing multimedia data |
US7209577B2 (en) * | 2005-07-14 | 2007-04-24 | Logitech Europe S.A. | Facial feature-localized and global real-time video morphing |
KR101103115B1 (en) | 2005-08-12 | 2012-01-04 | 소니 컴퓨터 엔터테인먼트 인코포레이티드 | Face image display, face image display method, and face image display program |
US7672504B2 (en) * | 2005-09-01 | 2010-03-02 | Childers Edwin M C | Method and system for obtaining high resolution 3-D images of moving objects by use of sensor fusion |
US8094928B2 (en) | 2005-11-14 | 2012-01-10 | Microsoft Corporation | Stereo video for gaming |
US8098277B1 (en) | 2005-12-02 | 2012-01-17 | Intellectual Ventures Holding 67 Llc | Systems and methods for communication between a reactive video system and a mobile communication device |
JP4760349B2 (en) * | 2005-12-07 | 2011-08-31 | ソニー株式会社 | Image processing apparatus, image processing method, and program |
US20070230794A1 (en) * | 2006-04-04 | 2007-10-04 | Logitech Europe S.A. | Real-time automatic facial feature replacement |
ATE477548T1 (en) | 2006-04-26 | 2010-08-15 | Aware Inc | QUALITY AND SEGMENTATION OF A FINGERPRINT PREVIEW |
US7961947B2 (en) * | 2006-07-28 | 2011-06-14 | Sony Corporation | FACS cleaning in motion capture |
US7782866B1 (en) | 2006-09-29 | 2010-08-24 | Qurio Holdings, Inc. | Virtual peer in a peer-to-peer network |
JP2008171108A (en) * | 2007-01-10 | 2008-07-24 | Matsushita Electric Ind Co Ltd | Face condition determining device and imaging device |
US20080166052A1 (en) * | 2007-01-10 | 2008-07-10 | Toshinobu Hatano | Face condition determining device and imaging device |
US8130225B2 (en) | 2007-01-16 | 2012-03-06 | Lucasfilm Entertainment Company Ltd. | Using animation libraries for object identification |
US8199152B2 (en) | 2007-01-16 | 2012-06-12 | Lucasfilm Entertainment Company Ltd. | Combining multiple session content for animation libraries |
US8542236B2 (en) * | 2007-01-16 | 2013-09-24 | Lucasfilm Entertainment Company Ltd. | Generating animation libraries |
US7849420B1 (en) | 2007-02-26 | 2010-12-07 | Qurio Holdings, Inc. | Interactive content representations enabling content sharing |
US7840903B1 (en) | 2007-02-26 | 2010-11-23 | Qurio Holdings, Inc. | Group content representations |
US9098167B1 (en) | 2007-02-26 | 2015-08-04 | Qurio Holdings, Inc. | Layered visualization of content representations |
DE102007010662A1 (en) * | 2007-03-02 | 2008-09-04 | Deutsche Telekom Ag | Method for gesture-based real time control of virtual body model in video communication environment, involves recording video sequence of person in end device |
DE102007010664A1 (en) * | 2007-03-02 | 2008-09-04 | Deutsche Telekom Ag | Method for transferring avatar-based information in video data stream in real time between two terminal equipments, which are arranged in avatar-based video communication environment, involves recording video sequence of person |
WO2008123967A1 (en) * | 2007-04-02 | 2008-10-16 | Roamware, Inc. | System and method for making a face call |
US8260266B1 (en) | 2007-06-26 | 2012-09-04 | Qurio Holdings, Inc. | Method and system for third-party discovery of proximity-based services |
US20090037822A1 (en) * | 2007-07-31 | 2009-02-05 | Qurio Holdings, Inc. | Context-aware shared content representations |
US9111285B2 (en) * | 2007-08-27 | 2015-08-18 | Qurio Holdings, Inc. | System and method for representing content, user presence and interaction within virtual world advertising environments |
US20090067706A1 (en) * | 2007-09-12 | 2009-03-12 | Artec Ventures | System and Method for Multiframe Surface Measurement of the Shape of Objects |
AU2008299883B2 (en) | 2007-09-14 | 2012-03-15 | Facebook, Inc. | Processing of gesture-based user interactions |
KR102341800B1 (en) | 2007-09-26 | 2021-12-21 | 에이큐 미디어 인크 | Audio-visual navigation and communication |
US8261307B1 (en) | 2007-10-25 | 2012-09-04 | Qurio Holdings, Inc. | Wireless multimedia content brokerage service for real time selective content provisioning |
US8159682B2 (en) | 2007-11-12 | 2012-04-17 | Intellectual Ventures Holding 67 Llc | Lens system |
US8144153B1 (en) | 2007-11-20 | 2012-03-27 | Lucasfilm Entertainment Company Ltd. | Model production for animation libraries |
KR100896065B1 (en) * | 2007-12-17 | 2009-05-07 | 한국전자통신연구원 | Method for producing 3d facial animation |
JP5004181B2 (en) * | 2008-01-11 | 2012-08-22 | Ｋｄｄｉ株式会社 | Region identification device and content identification device |
US8126858B1 (en) | 2008-01-23 | 2012-02-28 | A9.Com, Inc. | System and method for delivering content to a communication device in a content delivery system |
EP2263190A2 (en) * | 2008-02-13 | 2010-12-22 | Ubisoft Entertainment S.A. | Live-action image capture |
US8259163B2 (en) | 2008-03-07 | 2012-09-04 | Intellectual Ventures Holding 67 Llc | Display with built in 3D sensing |
US8098938B1 (en) | 2008-03-17 | 2012-01-17 | Google Inc. | Systems and methods for descriptor vector computation |
US8466931B2 (en) * | 2008-04-24 | 2013-06-18 | International Business Machines Corporation | Color modification of objects in a virtual universe |
US8212809B2 (en) * | 2008-04-24 | 2012-07-03 | International Business Machines Corporation | Floating transitions |
US8233005B2 (en) * | 2008-04-24 | 2012-07-31 | International Business Machines Corporation | Object size modifications based on avatar distance |
US8259100B2 (en) * | 2008-04-24 | 2012-09-04 | International Business Machines Corporation | Fixed path transitions |
US7953255B2 (en) | 2008-05-01 | 2011-05-31 | At&T Intellectual Property I, L.P. | Avatars in social interactive television |
US8676001B2 (en) | 2008-05-12 | 2014-03-18 | Google Inc. | Automatic discovery of popular landmarks |
US8595218B2 (en) | 2008-06-12 | 2013-11-26 | Intellectual Ventures Holding 67 Llc | Interactive display management systems and methods |
US9015778B2 (en) | 2008-06-25 | 2015-04-21 | AT&T Intellectual Property I. LP | Apparatus and method for media on demand commentaries |
US8839327B2 (en) | 2008-06-25 | 2014-09-16 | At&T Intellectual Property Ii, Lp | Method and apparatus for presenting media programs |
US8990705B2 (en) * | 2008-07-01 | 2015-03-24 | International Business Machines Corporation | Color modifications of objects in a virtual universe based on user display settings |
US8471843B2 (en) | 2008-07-07 | 2013-06-25 | International Business Machines Corporation | Geometric and texture modifications of objects in a virtual universe based on real world user characteristics |
US8243988B1 (en) | 2008-07-31 | 2012-08-14 | Google Inc. | Clustering images using an image region graph |
US20100070987A1 (en) * | 2008-09-12 | 2010-03-18 | At&T Intellectual Property I, L.P. | Mining viewer responses to multimedia content |
US20100070858A1 (en) * | 2008-09-12 | 2010-03-18 | At&T Intellectual Property I, L.P. | Interactive Media System and Method Using Context-Based Avatar Configuration |
US8225348B2 (en) | 2008-09-12 | 2012-07-17 | At&T Intellectual Property I, L.P. | Moderated interactive media sessions |
US20100094936A1 (en) * | 2008-10-15 | 2010-04-15 | Nokia Corporation | Dynamic Layering of an Object |
US8872832B2 (en) * | 2008-12-18 | 2014-10-28 | Digital Domain 3.0, Inc. | System and method for mesh stabilization of facial motion capture data |
US9142024B2 (en) * | 2008-12-31 | 2015-09-22 | Lucasfilm Entertainment Company Ltd. | Visual and physical motion sensing for three-dimensional motion capture |
US20100177117A1 (en) | 2009-01-14 | 2010-07-15 | International Business Machines Corporation | Contextual templates for modifying objects in a virtual universe |
US8406507B2 (en) | 2009-01-14 | 2013-03-26 | A9.Com, Inc. | Method and system for representing image patches |
US8649555B1 (en) * | 2009-02-18 | 2014-02-11 | Lucasfilm Entertainment Company Ltd. | Visual tracking framework |
US8738647B2 (en) * | 2009-02-18 | 2014-05-27 | A9.Com, Inc. | Method and system for image matching |
US9276761B2 (en) * | 2009-03-04 | 2016-03-01 | At&T Intellectual Property I, L.P. | Method and apparatus for group media consumption |
US8275623B2 (en) | 2009-03-06 | 2012-09-25 | At&T Intellectual Property I, L.P. | Method and apparatus for analyzing discussion regarding media programs |
US8224042B2 (en) * | 2009-03-12 | 2012-07-17 | Seiko Epson Corporation | Automatic face recognition |
US8442330B2 (en) * | 2009-03-31 | 2013-05-14 | Nbcuniversal Media, Llc | System and method for automatic landmark labeling with minimal supervision |
KR101555347B1 (en) | 2009-04-09 | 2015-09-24 | 삼성전자 주식회사 | Apparatus and method for generating video-guided facial animation |
US8396287B2 (en) | 2009-05-15 | 2013-03-12 | Google Inc. | Landmarks from digital photo collections |
US8321787B2 (en) | 2009-06-30 | 2012-11-27 | International Business Machines Corporation | Managing multiple virtual world accounts from a single virtual lobby interface |
US8413218B1 (en) * | 2009-07-02 | 2013-04-02 | United Services Automobile Association (Usaa) | Systems and methods for videophone identity cloaking |
US20110025689A1 (en) * | 2009-07-29 | 2011-02-03 | Microsoft Corporation | Auto-Generating A Visual Representation |
US8307308B2 (en) | 2009-08-27 | 2012-11-06 | International Business Machines Corporation | Updating assets rendered in a virtual world environment based on detected user interactions in another world |
KR101615719B1 (en) * | 2009-09-18 | 2016-04-27 | 삼성전자주식회사 | Apparatus and method for extracting user's third dimension facial expression |
US7961910B2 (en) | 2009-10-07 | 2011-06-14 | Microsoft Corporation | Systems and methods for tracking a model |
US8867820B2 (en) | 2009-10-07 | 2014-10-21 | Microsoft Corporation | Systems and methods for removing a background of an image |
US8963829B2 (en) | 2009-10-07 | 2015-02-24 | Microsoft Corporation | Methods and systems for determining and tracking extremities of a target |
US8564534B2 (en) | 2009-10-07 | 2013-10-22 | Microsoft Corporation | Human tracking system |
US8266652B2 (en) | 2009-10-15 | 2012-09-11 | At&T Intellectual Property I, L.P. | Apparatus and method for transmitting media content |
US9830605B2 (en) * | 2009-10-30 | 2017-11-28 | At&T Intellectual Property I, L.P. | Apparatus and method for product marketing |
US8224756B2 (en) | 2009-11-05 | 2012-07-17 | At&T Intellectual Property I, L.P. | Apparatus and method for managing a social network |
US8760469B2 (en) * | 2009-11-06 | 2014-06-24 | At&T Intellectual Property I, L.P. | Apparatus and method for managing marketing |
US9031379B2 (en) * | 2009-11-10 | 2015-05-12 | At&T Intellectual Property I, L.P. | Apparatus and method for transmitting media content |
US8316303B2 (en) | 2009-11-10 | 2012-11-20 | At&T Intellectual Property I, L.P. | Method and apparatus for presenting media programs |
US10708663B2 (en) | 2009-11-13 | 2020-07-07 | At&T Intellectual Property I, L.P. | Apparatus and method for media on demand commentaries |
US8387088B2 (en) * | 2009-11-13 | 2013-02-26 | At&T Intellectual Property I, Lp | Method and apparatus for presenting media programs |
US8839306B2 (en) | 2009-11-20 | 2014-09-16 | At&T Intellectual Property I, Lp | Method and apparatus for presenting media programs |
US9100550B2 (en) * | 2009-11-20 | 2015-08-04 | At&T Intellectual Property I, L.P. | Apparatus and method for managing a social network |
US8373741B2 (en) | 2009-11-20 | 2013-02-12 | At&T Intellectual Property I, Lp | Apparatus and method for collaborative network in an enterprise setting |
US9094726B2 (en) * | 2009-12-04 | 2015-07-28 | At&T Intellectual Property I, Lp | Apparatus and method for tagging media content and managing marketing |
US8238671B1 (en) | 2009-12-07 | 2012-08-07 | Google Inc. | Scene classification for place recognition |
US8774527B1 (en) | 2009-12-07 | 2014-07-08 | Google Inc. | Matching an approximately located query image against a reference image set using cellular base station and wireless access point information |
US8189964B2 (en) | 2009-12-07 | 2012-05-29 | Google Inc. | Matching an approximately located query image against a reference image set |
CN101976330B (en) * | 2010-09-26 | 2013-08-07 | 中国科学院深圳先进技术研究院 | Gesture recognition method and system |
US8422782B1 (en) | 2010-09-30 | 2013-04-16 | A9.Com, Inc. | Contour detection and image classification |
US8990199B1 (en) | 2010-09-30 | 2015-03-24 | Amazon Technologies, Inc. | Content search with category-aware visual similarity |
US8463036B1 (en) | 2010-09-30 | 2013-06-11 | A9.Com, Inc. | Shape-based search of a collection of content |
US8682041B2 (en) * | 2011-01-28 | 2014-03-25 | Honeywell International Inc. | Rendering-based landmark localization from 3D range images |
US8509525B1 (en) | 2011-04-06 | 2013-08-13 | Google Inc. | Clustering of forms from large-scale scanned-document collection |
US8937620B1 (en) | 2011-04-07 | 2015-01-20 | Google Inc. | System and methods for generation and control of story animation |
US8913055B2 (en) * | 2011-05-31 | 2014-12-16 | Honda Motor Co., Ltd. | Online environment mapping |
US9013489B2 (en) * | 2011-06-06 | 2015-04-21 | Microsoft Technology Licensing, Llc | Generation of avatar reflecting player appearance |
US8948447B2 (en) | 2011-07-12 | 2015-02-03 | Lucasfilm Entertainment Companyy, Ltd. | Scale independent tracking pattern |
US8564684B2 (en) * | 2011-08-17 | 2013-10-22 | Digimarc Corporation | Emotional illumination, and related arrangements |
KR101381439B1 (en) * | 2011-09-15 | 2014-04-04 | 가부시끼가이샤 도시바 | Face recognition apparatus, and face recognition method |
US9508176B2 (en) | 2011-11-18 | 2016-11-29 | Lucasfilm Entertainment Company Ltd. | Path and speed based character control |
CN106961621A (en) * | 2011-12-29 | 2017-07-18 | 英特尔公司 | Use the communication of incarnation |
DE112013001461B4 (en) * | 2012-03-14 | 2023-03-23 | Google LLC (n.d.Ges.d. Staates Delaware) | Modify a participant's appearance during a video conference |
CN107257403A (en) | 2012-04-09 | 2017-10-17 | 英特尔公司 | Use the communication of interaction incarnation |
CN102880866B (en) * | 2012-09-29 | 2014-12-17 | 宁波大学 | Method for extracting face features |
US9147275B1 (en) | 2012-11-19 | 2015-09-29 | A9.Com, Inc. | Approaches to text editing |
US9330301B1 (en) | 2012-11-21 | 2016-05-03 | Ozog Media, LLC | System, method, and computer program product for performing processing based on object recognition |
US9336435B1 (en) | 2012-11-21 | 2016-05-10 | Ozog Media, LLC | System, method, and computer program product for performing processing based on object recognition |
US9043349B1 (en) | 2012-11-29 | 2015-05-26 | A9.Com, Inc. | Image-based character recognition |
US9342930B1 (en) | 2013-01-25 | 2016-05-17 | A9.Com, Inc. | Information aggregation for recognized locations |
US10708545B2 (en) * | 2018-01-17 | 2020-07-07 | Duelight Llc | System, method, and computer program for transmitting face models based on face data points |
WO2014153689A1 (en) | 2013-03-29 | 2014-10-02 | Intel Corporation | Avatar animation, social networking and touch screen applications |
US9508197B2 (en) | 2013-11-01 | 2016-11-29 | Microsoft Technology Licensing, Llc | Generating an avatar from real time image data |
US9424598B1 (en) | 2013-12-02 | 2016-08-23 | A9.Com, Inc. | Visual search in a controlled shopping environment |
CN106462724B (en) * | 2014-04-11 | 2019-08-02 | 北京市商汤科技开发有限公司 | Method and system based on normalized images verification face-image |
US9792716B2 (en) * | 2014-06-13 | 2017-10-17 | Arcsoft Inc. | Enhancing video chatting |
US9536161B1 (en) | 2014-06-17 | 2017-01-03 | Amazon Technologies, Inc. | Visual and audio recognition for scene change events |
US9665804B2 (en) * | 2014-11-12 | 2017-05-30 | Qualcomm Incorporated | Systems and methods for tracking an object |
US9830728B2 (en) | 2014-12-23 | 2017-11-28 | Intel Corporation | Augmented facial animation |
RU2596062C1 (en) | 2015-03-20 | 2016-08-27 | Автономная Некоммерческая Образовательная Организация Высшего Профессионального Образования "Сколковский Институт Науки И Технологий" | Method for correction of eye image using machine learning and method of machine learning |
US10489957B2 (en) * | 2015-11-06 | 2019-11-26 | Mursion, Inc. | Control system for virtual characters |
WO2017101094A1 (en) | 2015-12-18 | 2017-06-22 | Intel Corporation | Avatar animation system |
CN108463787B (en) | 2016-01-05 | 2021-11-30 | 瑞尔D斯帕克有限责任公司 | Gaze correction of multi-perspective images |
US10534955B2 (en) * | 2016-01-22 | 2020-01-14 | Dreamworks Animation L.L.C. | Facial capture analysis and training system |
US9892326B2 (en) * | 2016-03-31 | 2018-02-13 | International Business Machines Corporation | Object detection in crowded scenes using context-driven label propagation |
WO2017200950A1 (en) | 2016-05-19 | 2017-11-23 | Reald Spark, Llc | Wide angle imaging directional backlights |
CN109496258A (en) | 2016-05-23 | 2019-03-19 | 瑞尔D斯帕克有限责任公司 | Wide-angle image directional backlight |
US10282530B2 (en) * | 2016-10-03 | 2019-05-07 | Microsoft Technology Licensing, Llc | Verifying identity based on facial dynamics |
US10401638B2 (en) | 2017-01-04 | 2019-09-03 | Reald Spark, Llc | Optical stack for imaging directional backlights |
US10606814B2 (en) | 2017-01-18 | 2020-03-31 | Microsoft Technology Licensing, Llc | Computer-aided tracking of physical entities |
US10679669B2 (en) | 2017-01-18 | 2020-06-09 | Microsoft Technology Licensing, Llc | Automatic narration of signal segment |
US10637814B2 (en) | 2017-01-18 | 2020-04-28 | Microsoft Technology Licensing, Llc | Communication routing based on physical status |
US11094212B2 (en) | 2017-01-18 | 2021-08-17 | Microsoft Technology Licensing, Llc | Sharing signal segments of physical graph |
US10635981B2 (en) | 2017-01-18 | 2020-04-28 | Microsoft Technology Licensing, Llc | Automated movement orchestration |
US10482900B2 (en) | 2017-01-18 | 2019-11-19 | Microsoft Technology Licensing, Llc | Organization of signal segments supporting sensed features |
US10437884B2 (en) | 2017-01-18 | 2019-10-08 | Microsoft Technology Licensing, Llc | Navigation of computer-navigable physical feature graph |
CN106770332B (en) * | 2017-02-14 | 2019-04-16 | 杭州字节信息技术有限公司 | A kind of electronic die blank defects detection implementation method based on machine vision |
US10408992B2 (en) | 2017-04-03 | 2019-09-10 | Reald Spark, Llc | Segmented imaging directional backlights |
KR102439054B1 (en) | 2017-05-16 | 2022-09-02 | 애플 인크. | Emoji recording and sending |
DK179948B1 (en) | 2017-05-16 | 2019-10-22 | Apple Inc. | Recording and sending Emoji |
WO2019032604A1 (en) | 2017-08-08 | 2019-02-14 | Reald Spark, Llc | Adjusting a digital representation of a head region |
KR102079378B1 (en) * | 2017-09-26 | 2020-02-19 | 고려대학교 산학협력단 | Method and apparatus for reconstructing of video |
US11109014B2 (en) | 2017-11-06 | 2021-08-31 | Reald Spark, Llc | Privacy display apparatus |
WO2019147771A1 (en) | 2018-01-25 | 2019-08-01 | Reald Spark, Llc | Touch screen for privacy display |
US11017575B2 (en) | 2018-02-26 | 2021-05-25 | Reald Spark, Llc | Method and system for generating data to provide an animated visual representation |
US11573679B2 (en) * | 2018-04-30 | 2023-02-07 | The Trustees of the California State University | Integration of user emotions for a smartphone or other communication device environment |
DK180212B1 (en) | 2018-05-07 | 2020-08-19 | Apple Inc | USER INTERFACE FOR CREATING AVATAR |
DK201870378A1 (en) | 2018-05-07 | 2020-01-13 | Apple Inc. | Displaying user interfaces associated with physical activities |
CN110634174B (en) * | 2018-06-05 | 2023-10-10 | 深圳市优必选科技有限公司 | Expression animation transition method and system and intelligent terminal |
CN109377563A (en) * | 2018-11-29 | 2019-02-22 | 广州市百果园信息技术有限公司 | A kind of method for reconstructing of face wire frame model, device, equipment and storage medium |
US11107261B2 (en) | 2019-01-18 | 2021-08-31 | Apple Inc. | Virtual avatar animation based on facial feature movement |
DK201970531A1 (en) | 2019-05-06 | 2021-07-09 | Apple Inc | Avatar integration with multiple applications |
CN111582203A (en) * | 2020-05-13 | 2020-08-25 | 广州云从鼎望科技有限公司 | Image recognition processing method, system, device and medium |
US11544931B2 (en) * | 2020-05-26 | 2023-01-03 | Otis Elevator Company | Machine learning based human activity detection and classification in first and third person videos |
KR20230003154A (en) | 2020-06-08 | 2023-01-05 | 애플 인크. | Presentation of avatars in three-dimensional environments |
US20230143019A1 (en) * | 2021-11-10 | 2023-05-11 | Evr Studio Co., Ltd. | Method of generating facial expression and three-dimensional (3d) graphic interface device using the same |
Family Cites Families (40)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPS59226981A (en) | 1983-06-08 | 1984-12-20 | Fujitsu Ltd | Method and device for pattern matching |
JPS60592A (en) | 1983-06-15 | 1985-01-05 | 三菱電機株式会社 | Nap preventor |
US4827413A (en) | 1987-06-16 | 1989-05-02 | Kabushiki Kaisha Toshiba | Modified back-to-front three dimensional reconstruction algorithm |
US4907169A (en) * | 1987-09-30 | 1990-03-06 | International Technical Associates | Adaptive tracking vision and guidance system |
US5168529A (en) | 1988-08-29 | 1992-12-01 | Rayethon Company | Confirmed boundary pattern matching |
AU7989991A (en) | 1990-05-29 | 1991-12-31 | Axiom Innovation Limited | Machine vision stereo matching |
JPH0771288B2 (en) | 1990-08-24 | 1995-07-31 | 神田通信工業株式会社 | Automatic view adjustment method and device |
GB9019538D0 (en) * | 1990-09-07 | 1990-10-24 | Philips Electronic Associated | Tracking a moving object |
US5220441A (en) | 1990-09-28 | 1993-06-15 | Eastman Kodak Company | Mechanism for determining parallax between digital images |
WO1992008204A2 (en) | 1990-10-24 | 1992-05-14 | Siemens Aktiengesellschaft | Method of detecting and estimating the position in space of objects from a two-dimensional image |
US5159647A (en) * | 1991-03-04 | 1992-10-27 | David Sarnoff Research Center, Inc. | Fast and efficient search method for graphical data |
US5680487A (en) | 1991-12-23 | 1997-10-21 | Texas Instruments Incorporated | System and method for determining optical flow |
US5333165A (en) | 1992-02-27 | 1994-07-26 | John K. Grady | Method and apparatus for three-dimensional video signals |
JP3298072B2 (en) | 1992-07-10 | 2002-07-02 | ソニー株式会社 | Video camera system |
US5383013A (en) | 1992-09-18 | 1995-01-17 | Nec Research Institute, Inc. | Stereoscopic computer vision system |
US5550928A (en) | 1992-12-15 | 1996-08-27 | A.C. Nielsen Company | Audience measurement system and method |
JPH06301393A (en) * | 1993-04-13 | 1994-10-28 | Matsushita Electric Ind Co Ltd | Voice block detector and voice recognition system |
US5511153A (en) | 1994-01-18 | 1996-04-23 | Massachusetts Institute Of Technology | Method and apparatus for three-dimensional, textured models from plural video images |
US5581625A (en) | 1994-01-31 | 1996-12-03 | International Business Machines Corporation | Stereo vision system for counting items in a queue |
JP3242529B2 (en) | 1994-06-07 | 2001-12-25 | 松下通信工業株式会社 | Stereo image matching method and stereo image parallax measurement method |
US5736982A (en) | 1994-08-03 | 1998-04-07 | Nippon Telegraph And Telephone Corporation | Virtual space apparatus with avatars and speech |
US5699449A (en) | 1994-11-14 | 1997-12-16 | The University Of Connecticut | Method and apparatus for implementation of neural networks for face recognition |
US5714997A (en) | 1995-01-06 | 1998-02-03 | Anderson; David P. | Virtual reality television system |
US5982853A (en) * | 1995-03-01 | 1999-11-09 | Liebermann; Raanan | Telephone for the deaf and method of using same |
JP2840816B2 (en) * | 1995-03-13 | 1998-12-24 | 株式会社エイ・ティ・アール通信システム研究所 | Facial expression detection device |
US5588033A (en) | 1995-06-06 | 1996-12-24 | St. Jude Children's Research Hospital | Method and apparatus for three dimensional image reconstruction from multiple stereotactic or isocentric backprojections |
US5715325A (en) | 1995-08-30 | 1998-02-03 | Siemens Corporate Research, Inc. | Apparatus and method for detecting a face in a video image |
US5774591A (en) | 1995-12-15 | 1998-06-30 | Xerox Corporation | Apparatus and method for recognizing facial expressions and facial gestures in a sequence of images |
US5802220A (en) * | 1995-12-15 | 1998-09-01 | Xerox Corporation | Apparatus and method for tracking facial motion through a sequence of images |
US5809171A (en) | 1996-01-05 | 1998-09-15 | Mcdonnell Douglas Corporation | Image processing method and apparatus for correlating a test image with a template |
US5764803A (en) | 1996-04-03 | 1998-06-09 | Lucent Technologies Inc. | Motion-adaptive modelling of scene content for very low bit rate model-assisted coding of video sequences |
GB9610212D0 (en) * | 1996-05-16 | 1996-07-24 | Cyberglass Limited | Method and apparatus for generating moving characters |
US5828769A (en) | 1996-10-23 | 1998-10-27 | Autodesk, Inc. | Method and apparatus for recognition of objects via position and orientation consensus of local image encoding |
US6044168A (en) * | 1996-11-25 | 2000-03-28 | Texas Instruments Incorporated | Model based faced coding and decoding using feature detection and eigenface coding |
US5917937A (en) | 1997-04-15 | 1999-06-29 | Microsoft Corporation | Method for performing stereo matching to recover depths, colors and opacities of surface elements |
US6052123A (en) * | 1997-05-14 | 2000-04-18 | International Business Machines Corporation | Animation reuse in three dimensional virtual reality |
US5995119A (en) * | 1997-06-06 | 1999-11-30 | At&T Corp. | Method for generating photo-realistic animated characters |
US6011562A (en) | 1997-08-01 | 2000-01-04 | Avid Technology Inc. | Method and system employing an NLE to create and modify 3D animations by mixing and compositing animation data |
US6301370B1 (en) * | 1998-04-13 | 2001-10-09 | Eyematic Interfaces, Inc. | Face recognition from video images |
AU3639699A (en) | 1998-04-13 | 1999-11-01 | Eyematic Interfaces, Inc. | Wavelet-based facial motion capture for avatar animation |
-
1999
- 1999-04-12 AU AU36396/99A patent/AU3639699A/en not_active Abandoned
- 1999-04-12 BR BRPI9909611-0A patent/BR9909611B1/en not_active IP Right Cessation
- 1999-04-12 WO PCT/US1999/007933 patent/WO1999053443A1/en active IP Right Grant
- 1999-04-12 JP JP2000543930A patent/JP3970520B2/en not_active Expired - Lifetime
- 1999-04-12 CA CA002327304A patent/CA2327304A1/en not_active Abandoned
- 1999-04-12 EP EP99918494A patent/EP1072018B1/en not_active Expired - Lifetime
- 1999-04-12 AT AT99918494T patent/ATE248409T1/en not_active IP Right Cessation
- 1999-04-12 DE DE69910757T patent/DE69910757T2/en not_active Expired - Lifetime
- 1999-04-12 KR KR10-2000-7011375A patent/KR100530812B1/en not_active IP Right Cessation
-
2001
- 2001-05-31 US US09/871,370 patent/US6580811B2/en not_active Expired - Lifetime
-
2006
- 2006-12-06 JP JP2006329956A patent/JP4177402B2/en not_active Expired - Fee Related
Also Published As
Publication number | Publication date |
---|---|
WO1999053443A1 (en) | 1999-10-21 |
BR9909611A (en) | 2000-12-19 |
EP1072018B1 (en) | 2003-08-27 |
EP1072018A1 (en) | 2001-01-31 |
ATE248409T1 (en) | 2003-09-15 |
JP2007109255A (en) | 2007-04-26 |
US6580811B2 (en) | 2003-06-17 |
CA2327304A1 (en) | 1999-10-21 |
JP4177402B2 (en) | 2008-11-05 |
BR9909611B1 (en) | 2012-08-07 |
KR20010042673A (en) | 2001-05-25 |
KR100530812B1 (en) | 2005-11-28 |
JP3970520B2 (en) | 2007-09-05 |
DE69910757D1 (en) | 2003-10-02 |
AU3639699A (en) | 1999-11-01 |
JP2002511620A (en) | 2002-04-16 |
US20010033675A1 (en) | 2001-10-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
DE69910757T2 (en) | WAVELET-BASED FACIAL MOTION DETECTION FOR AVATAR ANIMATION | |
US6272231B1 (en) | Wavelet-based facial motion capture for avatar animation | |
DE69922183T2 (en) | FACE RECOGNITION FROM VIDEO IMAGES | |
Chen et al. | Animatable neural radiance fields from monocular rgb videos | |
DE69934478T2 (en) | Method and apparatus for image processing based on metamorphosis models | |
Park et al. | Articulated pose estimation with tiny synthetic videos | |
Vetter | Synthesis of novel views from a single face image | |
DE602004009960T2 (en) | SYSTEM AND METHOD FOR DETECTING AND COMPARING ANATOMIC STRUCTURES USING THE APPEARANCE AND FORM | |
Gavrila et al. | Tracking of humans in action: A 3-D model-based approach | |
DE112004000393B4 (en) | System and method for tracking a global shape of a moving object | |
JPH10228544A (en) | Encoding and decoding of face based on model used characteristic detection and encoding of inherent face | |
DE10304360A1 (en) | Non-rigid image capture for medical and stereo imaging by definition of a higher dimensioned information space for shape capture that results in improved shape recognition | |
Cheng et al. | Capture and representation of human walking in live video sequences | |
Huang et al. | Few-shot human motion transfer by personalized geometry and texture modeling | |
Achenbach et al. | Accurate Face Reconstruction through Anisotropic Fitting and Eye Correction. | |
Ong et al. | Tracking hybrid 2D-3D human models from multiple views | |
Li et al. | Three-dimensional motion estimation via matrix completion | |
Yin et al. | 3D face recognition based on high-resolution 3D face modeling from frontal and profile views | |
CN113593001A (en) | Target object three-dimensional reconstruction method and device, computer equipment and storage medium | |
DE102004026782A1 (en) | Method and apparatus for computer-aided motion estimation in at least two temporally successive digital images, computer-readable storage medium and computer program element | |
Dai | Modeling and simulation of athlete’s error motion recognition based on computer vision | |
Pan et al. | Modeling for deformable body and motion analysis: A review | |
Jian et al. | Realistic face animation generation from videos | |
Yu et al. | Facial video coding/decoding at ultra-low bit-rate: a 2D/3D model-based approach | |
Jiang | Application of Rotationally Symmetrical Triangulation Stereo Vision Sensor in National Dance Movement Detection and Recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
8327 | Change in the person/name/address of the patent owner |
Owner name: NEVENGINEERING,INC., SANTA MONICA, CALIF., US |
|
8364 | No opposition during term of opposition | ||
8327 | Change in the person/name/address of the patent owner |
Owner name: GOOGLE, INC., MOUNTAIN VIEW, CALIF., US |