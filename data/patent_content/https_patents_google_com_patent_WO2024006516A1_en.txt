WO2024006516A1 - Sound separation based on distance estimation using machine learning models - Google Patents
Sound separation based on distance estimation using machine learning models Download PDFInfo
- Publication number
- WO2024006516A1 WO2024006516A1 PCT/US2023/026708 US2023026708W WO2024006516A1 WO 2024006516 A1 WO2024006516 A1 WO 2024006516A1 US 2023026708 W US2023026708 W US 2023026708W WO 2024006516 A1 WO2024006516 A1 WO 2024006516A1
- Authority
- WO
- WIPO (PCT)
- Prior art keywords
- sounds
- computing device
- computer
- audio
- far
- Prior art date
Links
- 238000000926 separation method Methods 0.000 title claims abstract description 77
- 238000010801 machine learning Methods 0.000 title description 65
- 238000000034 method Methods 0.000 claims abstract description 105
- 238000013528 artificial neural network Methods 0.000 claims abstract description 84
- 239000000203 mixture Substances 0.000 claims abstract description 79
- 238000012549 training Methods 0.000 claims description 87
- 230000006870 function Effects 0.000 claims description 37
- 238000013500 data storage Methods 0.000 claims description 15
- 230000004044 response Effects 0.000 claims description 14
- 230000001419 dependent effect Effects 0.000 claims description 8
- 230000002452 interceptive effect Effects 0.000 claims description 6
- 238000004590 computer program Methods 0.000 claims description 5
- 238000004519 manufacturing process Methods 0.000 claims description 5
- 230000000306 recurrent effect Effects 0.000 claims description 5
- 230000002708 enhancing effect Effects 0.000 claims description 4
- 230000006403 short-term memory Effects 0.000 claims description 4
- 238000013527 convolutional neural network Methods 0.000 claims description 3
- 238000003860 storage Methods 0.000 description 35
- 238000004422 calculation algorithm Methods 0.000 description 32
- 238000003491 array Methods 0.000 description 20
- 238000004891 communication Methods 0.000 description 20
- 238000009826 distribution Methods 0.000 description 20
- 238000010586 diagram Methods 0.000 description 16
- 230000015556 catabolic process Effects 0.000 description 15
- 238000006731 degradation reaction Methods 0.000 description 15
- 230000000694 effects Effects 0.000 description 15
- 238000012545 processing Methods 0.000 description 12
- 238000011156 evaluation Methods 0.000 description 10
- 230000005540 biological transmission Effects 0.000 description 8
- 230000015654 memory Effects 0.000 description 7
- 230000009467 reduction Effects 0.000 description 7
- 230000005236 sound signal Effects 0.000 description 7
- 238000012360 testing method Methods 0.000 description 7
- 230000001965 increasing effect Effects 0.000 description 5
- 241000282412 Homo Species 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 230000002787 reinforcement Effects 0.000 description 4
- 238000010200 validation analysis Methods 0.000 description 4
- 238000004364 calculation method Methods 0.000 description 3
- 230000008859 change Effects 0.000 description 3
- 230000006872 improvement Effects 0.000 description 3
- 230000000670 limiting effect Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 230000008447 perception Effects 0.000 description 3
- 230000001755 vocal effect Effects 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 238000013478 data encryption standard Methods 0.000 description 2
- 230000007812 deficiency Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 239000011521 glass Substances 0.000 description 2
- 238000012417 linear regression Methods 0.000 description 2
- 230000000873 masking effect Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 238000003062 neural network model Methods 0.000 description 2
- 238000005192 partition Methods 0.000 description 2
- 238000002360 preparation method Methods 0.000 description 2
- 238000013526 transfer learning Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 1
- 206010011878 Deafness Diseases 0.000 description 1
- 101000987144 Homo sapiens Molybdenum cofactor sulfurase Proteins 0.000 description 1
- 238000002679 ablation Methods 0.000 description 1
- 238000010521 absorption reaction Methods 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 238000013329 compounding Methods 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 125000004122 cyclic group Chemical group 0.000 description 1
- 230000007423 decrease Effects 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 238000006073 displacement reaction Methods 0.000 description 1
- 230000005670 electromagnetic radiation Effects 0.000 description 1
- 238000005538 encapsulation Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 238000010348 incorporation Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007477 logistic regression Methods 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 230000001151 other effect Effects 0.000 description 1
- 238000003909 pattern recognition Methods 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000000704 physical effect Effects 0.000 description 1
- 238000011176 pooling Methods 0.000 description 1
- 230000008569 process Effects 0.000 description 1
- 230000005855 radiation Effects 0.000 description 1
- 230000002829 reductive effect Effects 0.000 description 1
- 238000002310 reflectometry Methods 0.000 description 1
- 238000011160 research Methods 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 238000013515 script Methods 0.000 description 1
- 238000012163 sequencing technique Methods 0.000 description 1
- 238000004088 simulation Methods 0.000 description 1
- 239000000779 smoke Substances 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 238000010408 sweeping Methods 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000002604 ultrasonography Methods 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0272—Voice signal separating
- G10L21/0308—Voice signal separating characterised by the type of parameter measurement, e.g. correlation techniques, zero crossing techniques or predictive techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
- G06N3/0442—Recurrent networks, e.g. Hopfield networks characterised by memory or gating, e.g. long short-term memory [LSTM] or gated recurrent units [GRU]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
- G10L25/30—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique using neural networks
Definitions
- Sound separation refers to the extraction of a subset of sounds from a mixture of sounds. Extracting estimates of clean speech in the presence of interference is a long-standing research problem in signal processing. This task is referred to as speech enhancement when the interference is non-speech, and speech separation when the interference is speech.
- Assisted listening devices such as hearing aids, are an important application for these methods, with a typical task being to aid in conversations held in a crowded or noisy space.
- Some techniques such as beamforming use multiple microphones and generally use the direction of arrival (DOA) of sources as a cue to separate them.
- DOA direction of arrival
- Near-field beamforming methods can additionally estimate source distance using the spherical nature of acoustic wave fronts.
- such techniques may only distinguish distances of nearby sounds within a range limited by the size of the microphone array, for example, for a line array with halfwavelength, X/2, spacing and total length /., the maximum range is L 2 /2A.
- DOA and distance estimation may be considered jointly for sound events with two microphones.
- reverberation parameters such as T60 and direct-to-reverberation ratio (DRR).
- DRR direct-to-reverberation ratio
- Neural networks can also be trained to estimate reverberation parameters from a single microphone, as well as room volume. The success of these methods suggests that neural networks are able to perceive cues from raw audio to accurately predict properties of acoustic transfer functions, and that neural networks may be trained to estimate the distance of a source.
- the network may be trained to learn the DRR and estimate distances based on this. Accordingly, the network may not have to estimate room characteristics, and estimate the distance directly instead.
- the quality of a voice signal may be evaluated.
- a system can be configured to evaluate voice quality without access to the original clean audio.
- it may be challenging to access a close-talking microphone signal, or the far end of the communications system.
- Modern deep neural networks (DNNs) can make such a system viable as humans can judge the quality of a vocal signal, even without knowing the language. Also, for example, such measurement can be performed by predicting physical and/or explainable properties.
- the quality of the degradation detectors may be analyzed, and these can be configured to estimate mean opinion scores (MOS), and explain how different degradations affect user’s perceptions, as indicated by a one-dimensional MOS measure.
- MOS mean opinion scores
- a neural network may be trained to separate audio into near and far sounds, and output the predicted near and/or far sounds.
- a distance estimation network e.g., based on DRR
- a sound separation network may be trained to separate the audio into the component audio tracks, and the distance estimation model may be utilized to estimate the respective distances of the separated component audio tracks. Subsequently, the predicted component audio tracks may be identified as near and far sounds based on a distance threshold.
- a computer-implemented method of applying a trained neural network for sound separation based on distance estimation includes receiving, by an audio input component of a computing device, an audio mixture from one or more sources.
- the method also includes predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component.
- the method additionally includes determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component.
- the method further includes providing, by the computing device, the predicted one or more near sounds.
- a computing device in a second aspect, includes one or more processors and data storage.
- the data storage has stored thereon computer-executable instructions that, when executed by one or more processors, cause the computing device to carry out functions of sound separation based on distance estimation.
- the functions include: receiving, by an audio input component of a computing device, an audio mixture from one or more sources; predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component; determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the predicted one or more near sounds.
- a computer program includes instructions that, when executed by a computer, cause the computer to carry out functions of sound separation based on distance estimation.
- the functions include: receiving, by an audio input component of a computing device, an audio mixture from one or more sources; predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component; determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the predicted one or more near sounds.
- an article of manufacture includes one or more computer readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to carry out functions of sound separation based on distance estimation.
- the functions include: receiving, by an audio input component of a computing device, an audio mixture from one or more sources; predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component; determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the predicted one or more near sounds.
- a system to carry out functions of sound separation based on distance estimation includes: means for receiving, by an audio input component of a computing device, an audio mixture from one or more sources; means for predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component; means for determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and means for providing, by the computing device, the predicted one or more near sounds.
- a computer-implemented method of for sound separation based on distance estimation includes receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator.
- the method also includes training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component.
- the method additionally includes providing, by the computing device, the trained neural network.
- a computing device for sound separation based on distance estimation includes one or more processors and data storage.
- the data storage has stored thereon computer-executable instructions that, when executed by one or more processors, cause the computing device to carry out functions.
- the functions include: receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator; training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the trained neural network.
- a computer program for sound separation based on distance estimation includes instructions that, when executed by a computer, cause the computer to carry out functions.
- the functions include: receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator; training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the trained neural network.
- an article of manufacture for sound separation based on distance estimation includes one or more computer readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to carry out functions.
- the functions include: receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator; training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the trained neural network.
- a system for sound separation based on distance estimation includes means for receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator; means for training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and means for providing, by the computing device, the trained neural network.
- FIG. 1 A is a diagram illustrating an example distance based sound separation model, in accordance with example embodiments.
- FIG. IB is a diagram illustrating another example distance based sound separation model, in accordance with example embodiments.
- FIG. 1C is a diagram illustrating another example distance based sound separation model, in accordance with example embodiments.
- FIG. 2A illustrates an example source distribution by distance from a microphone, in accordance with example embodiments.
- FIG. 2B illustrates an example two dimensional (2D) spatial distribution, in accordance with example embodiments.
- FIG. 3 is a table illustrating mean SI-SDRi for distance threshold, in accordance with example embodiments.
- FIG. 4 is a table illustrating performance as a function of distance threshold, in accordance with example embodiments.
- FIG. 5 is an image illustrating a near target with one near source and a far target with one near source, in accordance with example embodiments.
- FIG. 6 is an image illustrating a near target with two near sources and a far target with two near sources, in accordance with example embodiments.
- FIG. 7 is an image illustrating a near target with three near sources and a far target with three near sources, in accordance with example embodiments.
- FIG. 8 is an image illustrating a near target with four near sources and a far target with four near sources, in accordance with example embodiments.
- FIG. 9A is a table illustrating performance as a function of SPP, in accordance with example embodiments.
- FIG. 9B is a table illustrating performance as a function of percentage silent training targets, in accordance with example embodiments.
- FIG. 10 is a diagram illustrating an example explainable voice-quality detector model, in accordance with example embodiments.
- FIG. 11 A is an example scatter plot illustrating a prediction performance for SNR, in accordance with example embodiments.
- FIG. 11B is an example scatter plot illustrating a prediction performance for T60, in accordance with example embodiments.
- FIG. 11C is an example scatter plot illustrating a prediction performance for direct-to- reverberation (DRR), in accordance with example embodiments.
- FIG. 1 ID is an example scatter plot illustrating a prediction performance based on distance using RoomSim data, in accordance with example embodiments.
- FIG. 12 is a diagram illustrating training and inference phases of a machine learning model, in accordance with example embodiments
- FIG. 13 depicts a distributed computing architecture, in accordance with example embodiments.
- FIG. 14 is a block diagram of a computing device, in accordance with example embodiments.
- FIG. 15 depicts a network of computing clusters arranged as a cloud-based server system, in accordance with example embodiments.
- FIG. 16 is a flowchart of a method, in accordance with example embodiments.
- FIG. 17 is a flowchart of another method, in accordance with example embodiments.
- This application relates, in one aspect, to distance-based sound separation.
- a user may desire to hear any sounds that occur within a local region around them, and block sounds coming from farther away.
- a system that accomplishes this would allow the user to engage in normal conversations without the interference of a crowded environment, and without becoming deaf to ordinary non-speech events in their immediate area.
- there may be cues for distance that are related to superficial characteristics of the sound, rather than to a fine spectro-temporal structure of the sound.
- a system that relies on such cues may have a computational advantage, and an advantage in terms of generalization, compared to a system that has to perform deep pattern recognition in order to separate sounds.
- Humans have a perceptual ability to estimate the relative distances of various sources of sounds. Accordingly, in applications such as assisted listening, there is a need for a machine learning model to perform such distance-based separation akin to human hearing abilities.
- Extracting estimates of clean speech in the presence of interference is a challenging problem in signal processing. This task is generally referred to as speech enhancement when the interference is non-speech, and speech separation when the interference is speech. More generally, sound separation refers to the extraction of a subset of sounds from a mixture of sounds.
- Hearing aids and other assisted listening devices are useful applications for these methods, with a typical task being to aid in conversations held in a noisy space. But implementing sound separation for assisted listening is challenging, requiring low algorithmic latency and limited computation and memory. Further, it may be challenging to distill sounds that a user may wish to hear.
- a neural network may be trained to separate signals into two component signals, such as a first set of signals corresponding to nearby sources, and a second set of signals corresponding to sources that are further away, wherein a distance of the sources is measured relative to a distance threshold.
- the techniques described herein eliminate deficiencies in class-based methods that attempt to include non-speech sounds, as such methods may fail when there are interfering sounds of the same classes as the desired sounds.
- the techniques described herein eliminate deficiencies in methods that are based on beamforming, that have physical limitations associated with a maximum distance associated with near-field beamforming.
- acoustic cues can be implicitly leveraged to identify one or more near sources and one or more far sources in order to separate them. This is in contrast to relying on a prediction of reverb parameters.
- the techniques described herein group sources together based only on whether they are near or far based on a distance threshold, without necessarily altering a reverberance of the sources. Multiple audio sources can be analyzed.
- FIG. 1A is a diagram illustrating an example distance based sound separation model 100, in accordance with example embodiments.
- microphone 105 may receive audio from various sources. Although microphone 105 is illustrated as a standalone device, it can be integrated within a computing device, a speaker, a headset, a mobile computing device, a camera, etc.
- An example threshold distance 110 is indicated as a distance measured from microphone 105.
- the audio sources may include one or more users, such as first user 115, second user 120, and third user 125. As illustrated, first user 115 may be within threshold distance 110, whereas second user 120 and third user 125 may be outside threshold distance 110.
- Microphone 105 may receive audio signals from human and non-human sources. For example, microphone 105 may receive first audio signal 115Afrom first user 115, second audio signal 120 A from second user 120, and third audio signal 125 A from third user 125. These audio signals may be mixed by an audio mixer 105A to generate audio mixture 130. Subsequently, audio mixture 130 may be input into distance based sound separation model 135, which may separate audio mixture 130 into near estimate 140 (e.g., to include audio from first user 115), and far estimate 145 (e.g., to include audio from second user 120 and third user 125). Although far estimate 145 is illustrated as a mixture of second audio signal 120A and third audio signal 125A, in some embodiments, these signals may be separated as well.
- FIG. IB is a diagram illustrating another example distance based sound separation model, in accordance with example embodiments.
- FIG. IB shares one or more components with FIG. 1A.
- the distance based sound separation model 135 of FIG. 1A e.g., trained to separate near and far sounds
- distance estimation model 150 e.g., based on DRR
- FIG. 1C is a diagram illustrating another example distance based sound separation model, in accordance with example embodiments.
- FIG. 1C shares one or more components with FIG. 1A and FIG. IB.
- the distance based sound separation model 135 of FIG. 1A e.g., trained to separate near and far sounds
- the distance based sound separation model 135 of FIG. 1A can be a combination of a sound separation model 155 and a distance estimation model 150.
- sound separation model 155 may be trained to separate audio mixture 130 into component audio tracks, and the distance estimation model 150 may be utilized to estimate the respective distances of the separated component audio tracks.
- the predicted component audio tracks may be identified as near estimate 140 and far estimate 145 based on distance threshold 110. Details of sound separation model 155 are not provided herein, as known sound separation networks may be utilized.
- Absorption by the air is generally a frequency-dependent effect of distance, and may play a role over distances beyond a certain distance threshold, for example, beyond tens of meters.
- Other effects of distance on sound can originate from spatial effects that could be detected using an array of microphones.
- a neural network for distance-based sound separation may be trained using mixtures of near and far sounds, the acoustic properties of the sounds having been emulated using an acoustic room simulator.
- ground-truth targets for the near and far signals relative to a distance threshold may be used.
- the sounds, both near and far are speech.
- the neural network can be trained to perform sound separation based on distance. In particular, in some examples, such as with a distance threshold of 1.5 meters (m), a single nearby speaker, and as many as four distant speakers, distance-based sound separation can achieve improvements of up to 4.4 dB in scale-invariant signal to noise ratio (SI-SNR).
- SI-SNR scale-invariant signal to noise ratio
- an example architecture for a neural network may be based on a short-time Fourier transform (STFT) masking.
- STFT short-time Fourier transform
- ms 32 millisecond (ms) square-root Hann window with a 16 ms hop
- the 0.3 -power-compressed magnitude of the STFT, Y, of the time-domain mixture, y may be fed as input to L layers of a unidirectional long short-term memory model (LSTM) with N units each.
- LSTM unidirectional long short-term memory model
- a fully-connected layer with a sigmoid activation may be applied to create two masks for the input STFT Y, where the two masks may be denoted as M near and Mf ar .
- each of the masked STFTs for near and far may be passed through inverse and forward STFT operations: O K) ⁇ .
- a training loss may be based on a mean-squared error between 0.3 -power-compressed magnitude of target X and estimate X, based on the relationship below:
- a first weight (e.g., 0.8) on the near loss, and a second weight (e.g., 0.2) on the far loss may be used.
- the weighted composition may be denoted .
- a weighted loss may enable the model to focus on a performance of near targets, which is more likely to be desired as an output in a practical application.
- an Adam optimizer with a learning rate of 3 x 10 -5 , a batch size of 128, may be used, and training can be performed, for example, on one million steps on 16 Google Cloud TPU v3 cores.
- an image-based acoustic room simulator with frequency-dependent wall filters may be used to generate reverb impulse responses (RIRs) for rooms with varied acoustic properties, and with randomized microphone and source locations.
- RIRs reverb impulse responses
- Basic distance-related phenomena such as an amplitude and DRR effects can be well simulated.
- clean speech recordings may be randomly assigned to the source locations within each room and convolved with the corresponding RIRs.
- Training examples for each room may be generated by combining sources within a threshold distance into a near target x near , and sources beyond a threshold distance into a far target x ⁇ ar .
- the near and far targets may be added to create mixture, x near + x ⁇ ar . This mixture may be input to train the model for sound separation.
- each of the randomly generated rooms 5 speaker locations and 1 microphone location may be randomly generated, such that the distance between the microphone and each source is maintained to be uniformly distributed, subject to a rejection of samples falling outside each room.
- the resulting distribution of distances from the microphones is illustrated in Figure 2.
- FIG. 2A illustrates an example source distribution 200A by distance from a microphone, in accordance with example embodiments.
- Source distribution 200A is a histogram that indicates a distance from a microphone in meters along the horizontal axis, and a number of sources along the vertical axis. For example, within each generated room, 5 speaker locations and 1 microphone location may be randomly generated, such that the distance between the microphone and each source is uniformly distributed subject to staying inside the walls of each randomly generated room. This distribution of distances from the microphones is illustrated in FIG. 2 A.
- FIG. 2B illustrates an example two dimensional (2D) spatial distribution 200B, in accordance with example embodiments. As illustrated, a distance from a microphone is indicated in meters along the horizontal axis. Spatial distribution 200B differs from what might be expected in, for example, a restaurant setting, where an average number of sources at a given distance radius is likely to increase as the distance increases.
- speech recordings from the Libri-Light dataset may be used for training, with validation, and test partitions from the Libri-Speech dataset may be used.
- the speaker IDs in the Libri-Speech dataset are unique for each partition.
- rooms may be generated with dimensions varying from 3.0 X 4.0 X 2.13 meters to 7.0 x 8.0 x 3.05 meters.
- Libri-Light clips may be reverberated and mixed according to a randomly chosen RIR to create clips that are 10 seconds in duration.
- Source utterances shorter than 10 seconds are offset at random intervals within the 10 second clip, and source utterances longer than 10 seconds may be clipped to a random 10 second interior segment.
- source and microphone locations may be used to determine which sources are near or far in relation to the microphone in the given room, and for a given threshold distance.
- a source presence probability SPP may be applied to each source, so that the total number of sources in a room can vary from 0 to 5, with a distribution dependent on the chosen SPP.
- the choice of distance threshold may affect the number of sources considered near versus far for a given room.
- the choice of distance threshold may affect the fraction of examples where the sources are considered near or where the sources are considered far, resulting in silent far targets or silent near targets.
- Distances thresholds of 0.8/1.5/3.0 meters can result in training data with silent far target fractions of 0.0/0.001/0.04, respectively. It is notable that some thresholds result in a majority of training examples containing a silent target, even for an SPP of 1.0.
- SI_SDRjj Scalar invariant SDR improvement
- SIJDRfx, ?) Sl_SDR(x, x) - SI_SDR(x; y)
- SI SDR measures signal fidelity with respect to a reference signal while allowing for a gain mismatch, as provided below:
- SI_SDRi diverges when one of the a targets is silent, because the input mixture will comprise the non-silent target and achieves oo dB SI SDR, and the silent target achieves — oo dB SI SDR.
- a noise reduction metric may be used for a silent near target to measure how much of the far sound leaks into the silent near output. The following expression may be used:
- NoiseReduction (,y, x near ) io iogio(
- the distance-based separation network described herein may be compared to a conventional speech separation baseline where the same network architecture is trained (but with more masking outputs and a permutation-invariant loss) to output 5 separate source estimates, one for each possible speaker.
- a conventional speech separation baseline where the same network architecture is trained (but with more masking outputs and a permutation-invariant loss) to output 5 separate source estimates, one for each possible speaker.
- To upper-bound the performance of this baseline oracle knowledge of the true near and far targets may be used to find an optimal grouping of the 5 separate source estimates into near and far estimates. In all but one condition, direct distance-based separation outperforms this upper bound on the “separate-all” baseline.
- FIG. 3 is a table 300 illustrating mean scale-invariant signal to distortion ratio improvement (SI-SDRi) for distance threshold, in accordance with example embodiments.
- LSTM Long Short Term Memory
- SPP training source presence probability
- ’ Distance-based” refers to the method of directly training a model to separate near sounds from far sounds as described herein.
- “Separate-all” is a baseline in which the same network architecture is trained to output 5 individual sources, which may then be grouped into one or more near estimates and one or more far estimates. They may be bucketed according to a number of near speakers (1 near speaker implies 4 far speakers, etc.).
- FIG. 4 is a table 400 illustrating performance as a function of distance threshold, in accordance with example embodiments.
- performance may be evaluated for a model (e.g., the model evaluated in table 300 of FIG. 3), trained with varied distance thresholds.
- SI_SDRi is given for all examples with 1 non-silent near source.
- Noise reduction may be determined for the 271 examples with silent near targets.
- performance may generally degrade with increased distance threshold values, despite the fact that higher thresholds can result in more non-silent training targets. This is illustrated in Figures 2A-B, and described under “Data Preparation,” where a threshold at 3 meters is closer to the center of the distribution than a threshold of 0.8 or 1.0, and thus may change the training examples to have fewer silent targets.
- FIGS. 5-8 illustrate example plots for SI_SDRi in decibels (dB) as a function of input SI-SDR for one or more near targets and one or more far targets, in accordance with example embodiments.
- SI_SDRi may be determined for a model (e.g., the model evaluated in table 300 of FIG. 3) on test examples with a distance threshold of 1.5 m. The results may be divided by the number of sources in the near target.
- FIG. 5 comprises a first distribution 500A illustrating a near target with one near source and a second distribution 500B illustrating a far target with one near source, in accordance with example embodiments.
- FIG. 6 comprises a first distribution 600A illustrating a near target with two near sources and a second distribution 600B illustrating a far target with two near sources, in accordance with example embodiments.
- FIG. 7 comprises a first distribution 700A illustrating a near target with three near sources and a second distribution 700B illustrating a far target with three near sources, in accordance with example embodiments.
- FIG. 8 comprises a first distribution 800A illustrating a near target with four near sources and a second distribution 800B illustrating a far target with four near sources, in accordance with example embodiments.
- the SPP may be varied.
- an SPP of 1.0 may indicate that the 5 sources are present in the mixture, and in general, a SPP of p induces a binomial distribution 2(5, p) over a number of sources. This generally results in a modification of the distribution of near and far speaker counts.
- the baseline model described with reference to FIG. 3 was trained with a SPP of 0.5; however, the evaluation set used in other sections may use a SPP of 1.0 for ease of slicing data by the number of near speakers. This indicates that training data and the evaluation data differ in terms of near and far speaker counts.
- a version of the evaluation set may be generated with a SPP of 0.5.
- This can provide a set with 527 silent near target examples, 411 examples with sound in near and far (i.e. 1 to 4 speakers in each target, sum ⁇ 5), and 62 examples with silent far targets.
- the baseline model may be trained with a SPP of 1.0 instead of 0.5. Results for the baseline model and evaluation data at SPPs of 0.5 and 1.0 are illustrated in FIG. 9A.
- FIG. 9A is a table 900A illustrating performance as a function of SPP, in accordance with example embodiments.
- performance may be evaluated for a model (e.g., the model evaluated in table 300 of FIG. 3, except for SPP).
- SI_SDRi may be provided for 727 examples with one or more non-silent near targets and one or more far targets. Noise reduction may be calculated for the 271 examples with silent near targets.
- SI_SDRi may be for 394 non-silent near and far target examples, noise reduction for 538 silent near targets examples.
- the RIRs may be pre-filtered to achieve certain percentages of silent targets (e.g., prior to applying SPP).
- the results are illustrated in table 900B in FIG. 9B for SPPs of 1.0 and 0.5.
- FIG. 9B is a table 900B illustrating performance as a function of percentage silent training targets, in accordance with example embodiments.
- “% rooms silent target” may refer to a number of rooms in the training set with silent near or far targets, before the SPP was applied.
- the noise reduction may be calculated only for the 271 evaluation examples with silent near targets.
- the correlation between increased performance and decreased silent targets seen in table 900B suggests that the degradation illustrated in FIG. 4 with increased distance threshold is a true artifact of the distance, as opposed to an effect of the change in training data between models.
- SPP and percentage of rooms with silent targets are compounding effects. As expected, providing the model with more examples with non-silent targets increases SI_SDRi on non-silent examples, but can degrade noise reduction on silent near examples.
- a system for estimating a distance of a sound source based on DRR is described.
- the quality of a voice signal after it is recorded and/or transmitted may be measured. This may be performed without the use of the original or reference signal, as human listeners are able to judge the quality of a vocal signal, even without knowing the spoken language.
- the vocal quality may be described by characterizing several degradations of the signal that can be interpreted and acted upon by the system engineers and/or the user. As described herein, the amount of non-speech noise, degradations due to reverberation and the distance to the speaker, may be estimated.
- Dereverberation is a technique to remove effects of reverberation from sound, after reverberant sound has been captured by microphones.
- Reverberation is generally caused by sound reflections in an enclosed space (e.g., a room) and can be quantified by a room reverberation time and a direct-to-reverberant ratio (DRR).
- DRR direct-to-reverberant ratio
- Dereverberation increases the direct-to-reverberant ratio resulting in sound that is perceived as closer and clearer.
- the DRR at a given location may be determined as a ratio of the sound pressure level of a direct sound from a directional source to the reverberant sound pressure level incident at the given location.
- the relationship is provide as follows:
- n d is a sample index of a peak of the direct sound arrival and n 0 is a number of samples corresponding to a selected temporal window (e.g., ranging from 1.0 to 2.5 ms).
- DRR can be computed from Eqn. 5. However, in an absence of RIR, the DRR may be computed independently, using an estimation of the sound sources.
- a mean opinion score is one way to measure voice quality.
- listeners may be asked to rate the quality of a voice signal on a five point scale. Listeners may generally judge which signal is cleaner when the degradation varies along any one axis (e.g., noise, reverb, compression artifacts). However, listeners may find it more challenging to put them on a metric scale. Also, for example, given a MOS rating, it can be challenging to infer what aspect of the recording or communication setup may need to be adjusted.
- Modem DNN speech-enhancement systems may be trained to learn which portions of a speech spectrogram are part of the voice and which are noise. Such models may be used to predict various degradations of the signal. For illustrative purposes, the various degradation estimates may be combined to determine a MOS, and a contribution of each degradation to the overall MOS may be determined.
- the models may be trained with two types of data: RoomSim and MOS.
- the individual feature models may be trained with real speech and noise in a simulated room environment.
- the MOS features may be trained with datasets from the Conferencing Speech 2022 challenge with human judgements.
- FIG. 10 is a diagram illustrating an example explainable voice-quality detector model, in accordance with example embodiments.
- Input audio 1005 may be received by block 1010 for training with RoomSim data.
- Block 1010 may include one or more TasNet blocks 1020 and one or more Regress blocks 1025.
- the two outputs of each TasNet block 1020 are the putative speech and noise signals, and these are fed into the one or more Regress blocks 1025.
- the one or more Regress blocks 1025 can independently provide outputs 1030 to block 1015 for training with the Microsoft/TU Berlin Public Switched Telephone Network (PSTN) data.
- PSTN Microsoft/TU Berlin Public Switched Telephone Network
- the outputs 1030 may include DRR (used to estimate the distance of a sound source), signal-to- noise ratio (SNR), T60 (the time it takes for the RIR to decay by 60 dB from the level of the direct path), and distance based sound separation.
- DRR used to estimate the distance of a sound source
- SNR signal-to- noise ratio
- T60 the time it takes for the RIR to decay by 60 dB from the level of the direct path
- the regressor 1035 can drive these signals to change their representation to generate the MOS 1040.
- feature detectors may be trained with public audio processed by a room simulator.
- the clean speech may be from the Libri-Light dataset for training and from Libri-TTS for evaluation.
- Non-speech sounds may be used from a variety of publicly available resources, including freesound.org.
- Freesound is a collaborative database of sounds licensed by the Creative Commons (CC).
- audio from the Free Universal Sound Separation (FUSS) dataset may be used.
- FUSS Free Universal Sound Separation
- artificial sounds such as synthesizer noises
- a sound classification network may be trained on AudioSet to avoid clips likely containing speech.
- sources may be reverberated and mixed on the fly, and the validation and test sets may each be based on about 9 hours of speech data.
- Room impulse responses may be generated with an image-method room simulator with frequency-dependent wall filters.
- the RIR can be generated by sampling random locations for a microphone and sound sources within a room defined using a random size, such as, with a width from 3 to 7 m, length from 4 to 8 m, and height between 2.13 and 3.05 m.
- source “image” locations may be randomly perturbed (e.g., by up to 8 cm in each direction) to avoid a “sweeping echo” effect.
- 140,000 training, 20,000 validation datasets may be generated, along with 20,000 test rooms that are used to generate train, validate and test data.
- the speech and noise can be “recorded” in rooms with simulated impulse response between the location of a source and a microphone.
- a random speech signal may be selected, three random noises may be selected, and a random room size may be selected, and the sound sources and microphone can be placed at random in the room.
- Random gains can be applied to the sources that result in speech with an average level (e.g., of XX dB) relative to full scale with standard deviation (e.g., of sXX dB).
- the non-speech noise has an average level of YY dB with standard deviation sYY dB.
- the ground truth may be determined for the features as the room’s environment is mixed and simulated.
- the SNR is the power of the chosen speech signal divided by the power of all the noise signals, in dB.
- the RIR defines both a direct path, the height of the first peak in the RIR, and the reverberant signals, and later peaks.
- the DRR is then the power in the first RIR peak, divided by the sum of all the other peaks, measured in dB.
- Determination of the T60 time the time it takes for the RIR to decay by 60 dB from the level of the direct path, can also be done from the RIR.
- linear regression may be applied in the log domain on the RIR. This regression can be carried out over the “valid” portions of the RIR, for example, by removing everything to just after the first (direct) response, and deleting the points after the noise floor.
- a line may be fit to the remaining points in the log domain to estimate the slope of the reverberation decay, and the T60 time may be estimated from this.
- the distance from the speech source to the microphone is a known quantity since the RoomSim calculation picks these two positions at random, and the true distance can be determined based on this.
- models to estimate MOS 1040 may be determined using three datasets from the Conferencing Speech 2022 challenge.
- Example datasets are:
- PSTN A random subset of the PSTN data.
- the PSTN data provides MOS data for 58709 10 s samples of speech with added noise that is also corrupted by various telephone network connections. This dataset has an MOS estimate based on an average of 4.6 human judgements.
- NISQA The NISQA corpus file data, also from TU Berlin.
- An average MOS estimate, the 95% confidence interval, and the number of judgements for each sample may be provided. From this information the inter-subject agreement can be determined and the standard deviation of human estimates of the MOS may be estimated as 0.86.
- a convolutional TasNet architecture 1020 can be used.
- TasNet 1020 can be configured to learn what speech looks like, and generate a mask that separates the speech from the noise.
- TasNet 1020 may be configured to predict a time-frequency mask, which is applied to the original signal to predict a clean signal, and from this a mean-squared loss may be computed.
- the “speech” and the “noise” outputs of the TasNet network may be combined with the following operations: ReLU that involves dividing “speech” energy by “noise” energy and then taking the logarithm to form a feature estimate.
- the “speech” and “noise” signals are in quotes because that is their role in the normal speech-enhancement network.
- the voice-quality models described herein as each physical feature model is trained end-to-end, the network is free to use these signals in any form as long as the final feature output is moving towards the desired value.
- the same architecture can be used to estimate SNR (+/- 40dB), DRR (+/- 40dB), T60 (0-ls) or distance (0-20m).
- the input to the physical feature networks can be audio that have been segmented into 8s frames.
- the RoomSim calculation 1010 can be part of the training pipeline so it can be efficiently executed on a TPU.
- the chosen (random) speech and noise signals may be convolved with the RIR for the chosen source and microphone positions to form the received microphone signals.
- a small bias may be added to the logs and divisions so that the results are finite.
- the network may be trained to a configuration close to a final performance in a short time (e.g., less than a day using 4 x 2 arrays of TPUs).
- Regressor 1035 can be linear or DNN and may be configured to predict a MOS 1040 from the physical estimates.
- a linear regression can provide a more intuitive model since the decision factors are linear weights, but a linear regressor may not perform as well as a non-linear regressor.
- a 3 layer DNN may be used, with 10 hidden units, batch normalization, and skip connections.
- a model may be trained for each dataset using 80% of the physical data inferred from that dataset, and then test the model on the remaining 20%.
- FIGS. 11A-D illustrate scatter plots for each of these feature predictors.
- FIG. 11A is an example scatter plot 1300A illustrating a prediction performance for SNR, in accordance with example embodiments.
- the horizontal axis represents the true SNR in dB and the vertical axis represents the predicted SNR in dB.
- the SNR model achieves a correlation of 96% and an error with a standard deviation of +/-15dB.
- FIG. 11B is an example scatter plot 1300B illustrating a prediction performance for T60, in accordance with example embodiments.
- the horizontal axis represents the true T60 in seconds and the vertical axis represents the predicted T60 in seconds.
- the T60 model achieves a correlation of 99% and an error with a standard deviation of 130ms.
- the same architecture that was used to predict power above is predicting a time.
- the T60 time is a function of the room geometry and wall reflectivity, and is generally not under the user’s control.
- the log of T60 may be predicted as a relevant measure.
- FIG. 11C is an example scatter plot 1300C illustrating a prediction performance for DRR, in accordance with example embodiments.
- the horizontal axis represents the true DRR in dB and the vertical axis represents the predicted DRR in dB.
- the DRR describes how much of the received signal is due to direct path and a high DRR suggests that the speaker is close to the microphone, and the direct path dominates over reverberations.
- the model provides a correlation of 97%, and the error has a standard deviation of 1 ,5dB.
- the DRR may be used to estimate a distance of a source, and an audio mix may be separated into near and far sounds based on the estimated distance.
- FIG. 1 ID is an example scatter plot 1300D illustrating a prediction performance based on distance using RoomSim data, in accordance with example embodiments.
- the horizontal axis represents the true distance in meters and the vertical axis represents the predicted distance in meters.
- the distance predictions appear to have an 89% correlation with the true results and are lower than preceding measures.
- the standard deviation of the errors is about Im, and the log of distance may be determined as an estimate.
- the estimates of four physical features may be combined into a model for MOS using linear and DNN regressors.
- separate linear and DNN models may be generated for each dataset.
- a root-mean-squared error (RMSE) of 0.67 may be achieved. This compares favorably with the inter-subject standard deviation of the human data (0.86).
- a DNN model may be trained and tested using just the NISQA VAL SIM data from NISQA. Using the 10,000 utterances supplied, a Pearson correlation between the predicted and expected MOS of 0.63 and RMSE of 0.85 may be achieved.
- a relative significance of features may be determined by building and testing the MOS models with a subset of the physical features. The correlation between the true and the predicted MOS is then a measure of the importance of that feature set to the true MOS.
- SNR appears to be a significant feature, followed by DRR, and distance (which is related to DRR), and T60 appears to be much less important, perhaps because humans may not perceive reverberation due to the precedence effect, at least within the range of T60 values in the training and evaluation sets.
- a correlation of 67% which may be achieved using just the SNR estimate, indicates an r2 of 43%, suggesting that 43% of the variance in the MOS data may be explained by the SNR estimates.
- the DRR estimates explain a couple more percentages. In effect such an ablation study demonstrates, on average, how different physical features factor into the average human MOS judgements.
- the model described herein may be analyzed by artificially generating a range of feature values and measuring the MOS predictions.
- the MOS may be higher for high SNR and DRR.
- An explainable model for voice quality can have various applications. For example, a user who is recording a conversation may be provided with actionable feedback that allows them to improve their audio or video recordings. Also, for example, system engineers may want to know when the noise or distortion is large, which might indicate that there are transmission problems.
- the quality of the MOS estimator may be adjusted by adding additional physically quantifiable features, such as degradations from the codec, transmission errors, speech intelligibility, and so on. Also, for example, more sophisticated network components, such as attention and non-linear pooling may be added.
- explainability may be a more desirable quantity to measure than absolute accuracy, especially as humans are limited to a five-point scale and the ambiguity that comes with differentiating between noise and reverberation is more important to quality.
- a separate full TasNet model may be used for each predictor.
- the number of superblocks may be reduced from 4 to 1 and the number of blocks within a superblock.
- the model described herein can perform distance based sound separation.
- the model can determine a distance threshold to identify one or more near sounds and one or more far sounds.
- the distance threshold can be a learnable hyperparameter of the neural network.
- the distance threshold may be input by a user via an interactive graphical user interface.
- the model may recommend one or more suggested distance thresholds to the user, and the user may select one of the suggested distance thresholds.
- the user may input the distance threshold (e.g., using a slidable bar).
- a virtual model of the user location in an environment may be provided, and the user may use a graphic object (e.g., an adjustable circle, ellipse, a free form boundary diagram, and so forth) to input the distance threshold.
- the user may be provided with a graphic illustration of a first table at a restaurant where the user is seated with two other individuals, and one or more additional tables. The user may select the table, the two other individuals, and/or draw a boundary around the table and the two other individuals to indicate a distance threshold.
- the neural network can perform one or more of suppressing nearby sounds, suppressing far sounds, enhancing near sounds, or enhancing far sounds.
- the neural network can be combined with an audio separation network that separates one or more sources of sound from audio mixture, and the neural network can then identify the one or more near sounds and the one or more far sounds based on the separated one or more sources of sound (e.g., using distance estimation). Also, for example, subsequent to the near and far sounds being identified, an audio separation network can separate the near and far sounds into respective component tracks.
- an audio separation network can separate one or more sources of sound, and the distances of the one or more sources of sound may be estimated (e.g., by a distance estimation neural network), and the neural network can then identify the one or more near sounds and the one or more far sounds based on the estimated distances.
- the model may reside on a mobile device and perform distance based sound separation.
- the model may reside at a remote server to perform distance based sound separation for audio received at a computing device.
- the techniques may be applied to assisted listening devices, intelligent voice assistants, intelligent speaker calibration, robotic control circuitry (e.g., identifying one or more near objects and/or one or more far objects based on sound separation), autonomous and semi-autonomous driving (e.g., identifying one or more near traffic sounds and one or more far traffic sounds), intelligent cameras, security and/or surveillance cameras, and so forth.
- robotic control circuitry e.g., identifying one or more near objects and/or one or more far objects based on sound separation
- autonomous and semi-autonomous driving e.g., identifying one or more near traffic sounds and one or more far traffic sounds
- intelligent cameras security and/or surveillance cameras, and so forth.
- FIG. 12 shows diagram 1200 illustrating a training phase 1202 and an inference phase 1204 of trained machine learning model(s) 1232, in accordance with example embodiments.
- Some machine learning techniques involve training one or more machine learning algorithms on an input set of training data to recognize patterns in the training data and provide output inferences and/or predictions about (patterns in the) training data.
- the resulting trained machine learning algorithm can be termed as a trained machine learning model.
- FIG. 12 shows training phase 1202 where one or more machine learning algorithms 1220 are being trained on training data 1210 to become trained machine learning model(s) 1232.
- trained machine learning model(s) 1232 can receive input data 1230 and one or more inference/prediction requests 1240 (perhaps as part of input data 1230) and responsively provide as an output one or more inferences and/or prediction(s) 1250.
- trained machine learning model(s) 1232 can include one or more models of one or more machine learning algorithms 1220.
- Machine learning algorithm(s) 1220 may include, but are not limited to: an artificial neural network (e.g., a herein-described convolutional neural networks, a recurrent neural network, a Bayesian network, a hidden Markov model, a Markov decision process, a logistic regression function, a support vector machine, a suitable statistical machine learning algorithm, and/or a heuristic machine learning system).
- Machine learning algorithm(s) 1220 may be supervised or unsupervised, and may implement any suitable combination of online and offline learning.
- machine learning algorithm(s) 1220 and/or trained machine learning model(s) 1232 can be accelerated using on-device coprocessors, such as graphic processing units (GPUs), tensor processing units (TPUs), digital signal processors (DSPs), and/or application specific integrated circuits (ASICs).
- on-device coprocessors can be used to speed up machine learning algorithm(s) 1220 and/or trained machine learning model(s) 1232.
- trained machine learning model(s) 1232 can be trained, reside on, and be executed, to provide inferences on a particular computing device, and/or otherwise can make inferences for the particular computing device.
- machine learning algorithm(s) 1220 can be trained by providing at least training data 1210 as training input using unsupervised, supervised, semisupervised, and/or reinforcement learning techniques.
- Unsupervised learning involves providing a portion (or all) of training data 1210 to machine learning algorithm(s) 1220 and machine learning algorithm(s) 1220 determining one or more output inferences based on the provided portion (or all) of training data 1210.
- Supervised learning involves providing a portion of training data 1210 to machine learning algorithm(s) 1220, with machine learning algorithm(s) 1220 determining one or more output inferences based on the provided portion of training data 1210, and the output inference(s) are either accepted or corrected based on correct results associated with training data 1210.
- supervised learning of machine learning algorithm(s) 1220 can be governed by a set of rules and/or a set of labels for the training input, and the set of rules and/or set of labels may be used to correct inferences of machine learning algorithm(s) 1220.
- Semi-supervised learning involves having correct results for part, but not all, of training data 1210. During semi-supervised learning, supervised learning is used for a portion of training data 1210 having correct results, and unsupervised learning is used for a portion of training data 1210 not having correct results.
- Reinforcement learning involves machine learning algorithm(s) 1220 receiving a reward signal regarding a prior inference, where the reward signal can be a numerical value.
- machine learning algorithm(s) 1220 can output an inference and receive a reward signal in response, where machine learning algorithm(s) 1220 are configured to try to maximize the numerical value of the reward signal.
- reinforcement learning also utilizes a value function that provides a numerical value representing an expected total of the numerical values provided by the reward signal over time.
- machine learning algorithm(s) 1220 and/or trained machine learning model(s) 1232 can be trained using other machine learning techniques, including but not limited to, incremental learning and curriculum learning.
- machine learning algorithm(s) 1220 and/or trained machine learning model(s) 1232 can use transfer learning techniques.
- transfer learning techniques can involve trained machine learning model(s) 1232 being pre-trained on one set of data and additionally trained using training data 1210.
- machine learning algorithm(s) 1220 can be pre-trained on data from one or more computing devices and a resulting trained machine learning model provided to computing device CD1, where CD1 is intended to execute the trained machine learning model during inference phase 1204. Then, during training phase 1202, the pre-trained machine learning model can be additionally trained using training data 1210, where training data 1210 can be derived from kernel and non-kernel data of computing device CD1.
- This further training of the machine learning algorithm(s) 1220 and/or the pretrained machine learning model using training data 1210 of CDl’s data can be performed using either supervised or unsupervised learning.
- training phase 1202 can be completed.
- the trained resulting machine learning model can be utilized as at least one of trained machine learning model(s) 1232.
- Training data 1210 can be a plurality of audio mixtures, wherein each audio mixture comprises one or more first sounds that are proximate to the audio input component, and one or more second sounds that are not proximate to the audio input component.
- trained machine learning model(s) 1232 can be provided to a computing device, if not already on the computing device.
- Inference phase 1204 can begin after trained machine learning model(s) 1232 are provided to computing device CD1.
- trained machine learning model(s) 1232 can receive input data 1230 and generate and output one or more corresponding inferences and/or prediction(s) 1250 about input data 1230.
- input data 1230 can be used as an input to trained machine learning model(s) 1232 for providing corresponding inference(s) and/or prediction(s) 1250 to kernel components and non-kernel components.
- trained machine learning model(s) 1232 can generate inference(s) and/or prediction(s) 1250 in response to one or more inference/prediction requests 1240.
- trained machine learning model(s) 1232 can be executed by a portion of other software.
- trained machine learning model(s) 1232 can be executed by an inference or prediction daemon to be readily available to provide inferences and/or predictions upon request.
- Input data 1230 can include data from computing device CD1 executing trained machine learning model(s) 1232 and/or input data from one or more computing devices other than CD1.
- input data 1230 can include an audio mixture from one or more sources received by an audio input component of a computing device. Other types of input data are possible as well.
- Inference(s) and/or prediction(s) 1250 can include output audio, output intermediate audio, numerical values, and/or other output data produced by trained machine learning model(s) 1232 operating on input data 1230 (and training data 1210).
- trained machine learning model(s) 1232 can use output inference(s) and/or prediction(s) 1250 as input feedback 1160.
- Trained machine learning model(s) 1232 can also rely on past inferences as inputs for generating new inferences.
- a neural network for distance based sound separation can be an example of machine learning algorithm(s) 1220.
- a neural network for distance estimation of sound sources in an audio mixture can be an example of machine learning algorithm(s) 1220.
- a neural network for separation of sound sources in the audio mixture can be an example of machine learning algorithm(s) 1220.
- the trained version of the neural network can be an example of trained machine learning model(s) 1232.
- an example of the one or more inference / prediction request(s) 1240 can be a request to predict the one or more near sounds and the one or more far sounds and a corresponding example of inferences and/or prediction(s) 1250 can be the predicted one or more near sounds and the one or more far sounds.
- one computing device CD SOLO can include one or more of the trained versions of the distance based sound separation network, a distance estimation network, or an audio separation network, perhaps after training. Then, computing device CD SOLO can receive a request to predict the one or more near sounds and the one or more far sounds, and use the trained version of the neural network to predict the one or more near sounds and the one or more far sounds.
- two or more computing devices CD CLI and CD SRV can be used to provide output audio; e.g., a first computing device CD CLI can generate and send requests to predict the one or more near sounds and the one or more far sounds to a second computing device CD SRV. Then, CD SRV can use the trained version of the neural network, to predict the one or more near sounds and the one or more far sounds, and respond to the requests from CD CLI for the predicted one or more near sounds and the one or more far sounds. Then, upon reception of responses to the requests, CD CLI can provide the requested predicted one or more near sounds and the one or more far sounds (e.g., using a user interface and/or a display, a printed copy, an electronic communication, etc.).
- FIG. 13 depicts a distributed computing architecture 1300, in accordance with example embodiments.
- Distributed computing architecture 1300 includes server devices 1308, 1310 that are configured to communicate, via network 1306, with programmable devices 1304a, 1304b, 1304c, 1304d, 1304e.
- Network 1306 may correspond to a local area network (LAN), a wide area network (WAN), a WLAN, a WWAN, a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices.
- Network 1306 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet.
- FIG. 13 only shows five programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices.
- programmable devices 1304a, 1304b, 1304c, 1304d, 1304e may be any sort of computing device, such as a mobile computing device, desktop computer, wearable computing device, head-mountable device (HMD), network terminal, a mobile computing device, and so on.
- HMD head-mountable device
- programmable devices can be directly connected to network 1306.
- programmable devices can be indirectly connected to network 1306 via an associated computing device, such as programmable device 1304c.
- programmable device 1304c can act as an associated computing device to pass electronic communications between programmable device 1304d and network 1306.
- a computing device can be part of and/or inside a vehicle, such as a car, a truck, a bus, a boat or ship, an airplane, etc.
- a programmable device can be both directly and indirectly connected to network 1306.
- Server devices 1308, 1310 can be configured to perform one or more services, as requested by programmable devices 1304a-1304e.
- server device 1308 and/or 1310 can provide content to programmable devices 1304a-1304e.
- the content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video.
- the content can include compressed and/or uncompressed content.
- the content can be encrypted and/or unencrypted. Other types of content are possible as well.
- server device 1308 and/or 1310 can provide programmable devices 1304a-1304e with access to software for database, search, computation, graphical, audio, video, World Wide Web/Internet utilization, and/or other functions. Many other examples of server devices are possible as well.
- FIG. 14 is a block diagram of an example computing device 1400, in accordance with example embodiments.
- computing device 1400 shown in FIG. 14 can be configured to perform at least one function for distance based sound separation, and/or method 1600, and/or 1700.
- Computing device 1400 may include a user interface module 1401, a network communications module 1402, one or more processors 1403, data storage 1404, one or more camera(s) 1412, one or more sensors 1414, and power system 1416, all of which may be linked together via a system bus, network, or other connection mechanism 1405.
- User interface module 1401 can be operable to send data to and/or receive data from external user input/output devices.
- user interface module 1401 can be configured to send and/or receive data to and/or from user input devices such as a touch screen, a computer mouse, a keyboard, a keypad, a touch pad, a trackball, a joystick, a voice recognition module, and/or other similar devices.
- user input devices such as a touch screen, a computer mouse, a keyboard, a keypad, a touch pad, a trackball, a joystick, a voice recognition module, and/or other similar devices.
- User interface module 1401 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays, light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later developed.
- CTR cathode ray tubes
- LEDs light emitting di
- User interface module 1401 can also be configured to generate audible outputs, with devices such as a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices. User interface module 1401 can further be configured with one or more haptic devices that can generate haptic outputs, such as vibrations and/or other outputs detectable by touch and/or physical contact with computing device 1400. In some examples, user interface module 1401 can be used to provide a graphical user interface (GUI) for utilizing computing device 1400, such as, for example, a graphical user interface of a mobile phone device. In some examples, user interface module 1401 can be used to provide a user-adjustable control to receive a threshold distance.
- GUI graphical user interface
- Network communications module 1402 can include one or more devices that provide one or more wireless interface(s) 1407 and/or one or more wireline interface(s) 1408 that are configurable to communicate via a network.
- Wireless interface(s) 1407 can include one or more wireless transmitters, receivers, and/or transceivers, such as a BluetoothTM transceiver, a Zigbee® transceiver, a Wi-FiTM transceiver, a WiMAXTM transceiver, an LTETM transceiver, and/or other type of wireless transceiver configurable to communicate via a wireless network.
- Wireline interface(s) 1408 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiberoptic link, or a similar physical connection to a wireline network.
- wireline transmitters such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiberoptic link, or a similar physical connection to a wireline network.
- USB Universal Serial Bus
- network communications module 1402 can be configured to provide reliable, secured, and/or authenticated communications.
- information for facilitating reliable communications e.g., guaranteed message delivery
- a message header and/or footer e.g., packet/message sequencing information, encapsulation headers and/or footers, size/time information, and transmission verification information such as cyclic redundancy check (CRC) and/or parity check values.
- CRC cyclic redundancy check
- Communications can be made secure (e.g., be encoded or encrypted) and/or decry pted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, Data Encryption Standard (DES), Advanced Encryption Standard (AES), a Rivest- Shamir- Adelman (RSA) algorithm, a Diffie-Hellman algorithm, a secure sockets protocol such as Secure Sockets Layer (SSL) or Transport Layer Security (TLS), and/or Digital Signature Algorithm (DSA).
- DES Data Encryption Standard
- AES Advanced Encryption Standard
- RSA Rivest- Shamir- Adelman
- SSL Secure Sockets Layer
- TLS Transport Layer Security
- DSA Digital Signature Algorithm
- Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decry pt/decode) communications.
- One or more processors 1403 can include one or more general purpose processors, and/or one or more special purpose processors (e.g., digital signal processors, tensor processing units (TPUs), graphics processing units (GPUs), application specific integrated circuits, etc.).
- One or more processors 1403 can be configured to execute computer-readable instructions 1306 that are contained in data storage 1404 and/or other instructions as described herein.
- Data storage 1404 can include one or more non-transitory computer-readable storage media that can be read and/or accessed by at least one of one or more processors 1403.
- the one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one of one or more processors 1403.
- data storage 1404 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other examples, data storage 1404 can be implemented using two or more physical devices.
- Data storage 1404 can include computer-readable instructions 1406 and perhaps additional data.
- data storage 1404 can include storage required to perform at least part of the herein-described methods, scenarios, and techniques and/or at least part of the functionality of the herein-described devices and networks.
- data storage 1404 can include storage for a trained neural network model 1410 (e.g., a model of trained neural networks such as a vision and language model).
- computer-readable instructions 1406 can include instructions that, when executed by one or more processors 1403, enable computing device 1400 to provide for some or all of the functionality of trained neural network model 1410.
- computing device 1400 can include one or more camera(s) 1412.
- Camera(s) 1412 can include one or more image capture devices, such as still and/or video cameras, equipped to capture light and record the captured light in one or more images; that is, camera(s) 1412 can generate image(s) of captured light.
- the one or more images can be one or more still images and/or one or more images utilized in video imagery.
- Camera(s) 1412 can capture light and/or electromagnetic radiation emitted as visible light, infrared radiation, ultraviolet light, and/or as one or more other frequencies of light.
- computing device 1400 can include one or more sensors 1414. Sensors 1414 can be configured to measure conditions within computing device 1400 and/or conditions in an environment of computing device 1400 and provide data about these conditions.
- sensors 1414 can include one or more of (i) sensors for obtaining data about computing device 1400, such as, but not limited to, a thermometer for measuring a temperature of computing device 1400, a battery sensor for measuring power of one or more batteries of power system 1416, and/or other sensors measuring conditions of computing device 1400; (ii) an identification sensor to identify other objects and/or devices, such as, but not limited to, a Radio Frequency Identification (RFID) reader, proximity sensor, one-dimensional barcode reader, two-dimensional barcode (e.g., Quick Response (QR) code) reader, and a laser tracker, where the identification sensors can be configured to read identifiers, such as RFID tags, barcodes, QR codes, and/or other devices and/or object configured to be read and provide at least identifying information
- RFID Radio Frequency
- Power system 1416 can include one or more batteries 1418 and/or one or more external power interfaces 1420 for providing electrical power to computing device 1400. Each battery of the one or more batteries 1418 can, when electrically coupled to the computing device 1400, act as a source of stored electrical power for computing device 1400.
- One or more batteries 1418 of power system 1416 can be configured to be portable. Some or all of one or more batteries 1418 can be readily removable from computing device 1400. In other examples, some or all of one or more batteries 1418 can be internal to computing device 1400, and so may not be readily removable from computing device 1400. Some or all of one or more batteries 1418 can be rechargeable.
- a rechargeable battery can be recharged via a wired connection between the battery and another power supply, such as by one or more power supplies that are external to computing device 1400 and connected to computing device 1400 via the one or more external power interfaces.
- one or more batteries 1418 can be non-rechargeable batteries.
- One or more external power interfaces 1420 of power system 1416 can include one or more wired-power interfaces, such as a USB cable and/or a power cord, that enable wired electrical power connections to one or more power supplies that are external to computing device 1400.
- One or more external power interfaces 1420 can include one or more wireless power interfaces, such as a Qi wireless charger, that enable wireless electrical power connections, such as via a Qi wireless charger, to one or more external power supplies.
- computing device 1400 can draw electrical power from the external power source the established electrical power connection.
- power system 1416 can include related sensors, such as battery sensors associated with the one or more batteries or other types of electrical power sensors.
- FIG. 15 depicts a cloud-based server system in accordance with an example embodiment.
- functionality of distance based sound separation, and/or a computing device can be distributed among computing clusters 1509a, 1509b, 1509c.
- Computing cluster 1509a can include one or more computing devices 1500a, cluster storage arrays 1510a, and cluster routers 1511 connected by a local cluster network 1512a.
- computing cluster 1509b can include one or more computing devices 1500b, cluster storage arrays 1510b, and cluster routers 1511b connected by a local cluster network 1512b.
- computing cluster 1509c can include one or more computing devices 1500c, cluster storage arrays 1510c, and cluster routers 1511c connected by a local cluster network 1512c.
- computing clusters 1509a, 1509b, 1509c can be a single computing device residing in a single computing center.
- computing clusters 1509a, 1509b, 1509c can include multiple computing devices in a single computing center, or even multiple computing devices located in multiple computing centers located in diverse geographic locations.
- FIG. 15 depicts each of computing clusters 1509a, 1509b, 1509c residing in different physical locations.
- data and services at computing clusters 1509a, 1509b, 1509c can be encoded as computer readable information stored in non-transitory, tangible computer readable media (or computer readable storage media) and accessible by other computing devices.
- computing clusters 1509a, 1509b, 1509c can be stored on a single disk drive or other tangible storage media, or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations.
- each of computing clusters 1509a, 1509b, and 1509c can have an equal number of computing devices, an equal number of cluster storage arrays, and an equal number of cluster routers. In other embodiments, however, each computing cluster can have different numbers of computing devices, different numbers of cluster storage arrays, and different numbers of cluster routers. The number of computing devices, cluster storage arrays, and cluster routers in each computing cluster can depend on the computing task or tasks assigned to each computing cluster.
- computing devices 1500a can be configured to perform various computing tasks of a distance based sound separation neural network, an audio separation network, a distance estimation network, and/or a computing device.
- the various functionalities of a neural network, and/or a computing device can be distributed among one or more of computing devices 1500a, 1500b, 1500c.
- Computing devices 1500b and 1500c in respective computing clusters 1509b and 1509c can be configured similarly to computing devices 1500a in computing cluster 1509a.
- computing devices 1500a, 1500b, and 1500c can be configured to perform different functions.
- computing tasks and stored data associated with a distance based sound separation neural network, an audio separation network, a distance estimation network, and/or a computing device can be distributed across computing devices 1500a, 1500b, and 1500c based at least in part on the processing requirements of a neural network, and/or a computing device, the processing capabilities of computing devices 1500a, 1500b, 1500c, the latency of the network links between the computing devices in each computing cluster and between the computing clusters themselves, and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency, and/or other design goals of the overall system architecture.
- Cluster storage arrays 1510a, 1510b, 1510c of computing clusters 1509a, 1509b, 1509c can be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives.
- the disk array controllers alone or in conjunction with their respective computing devices, can also be configured to manage backup or redundant copies of the data stored in the cluster storage arrays to protect against disk drive or other cluster storage array failures and/or network failures that prevent one or more computing devices from accessing one or more cluster storage arrays.
- cluster storage arrays 1510a, 1510b, 1510c Similar to the manner in which the functions of a distance based sound separation neural network, an audio separation network, a distance estimation network, and/or a computing device can be distributed across computing devices 1500a, 1500b, 1500c of computing clusters 1509a, 1509b, 1509c, various active portions and/or backup portions of these components can be distributed across cluster storage arrays 1510a, 1510b, 1510c.
- some cluster storage arrays can be configured to store one portion of the data of a first layer of a neural network, and/or a computing device, while other cluster storage arrays can store other portion(s) of data of second layer of a neural network, and/or a computing device.
- some cluster storage arrays can be configured to store the data of an encoder of a neural network, while other cluster storage arrays can store the data of a decoder of a neural network. Additionally, some cluster storage arrays can be configured to store backup versions of data stored in other cluster storage arrays.
- Cluster routers 1511a, 1511b, 1511c in computing clusters 1509a, 1509b, 1509c can include networking equipment configured to provide internal and external communications for the computing clusters.
- cluster routers 1511a in computing cluster 1509a can include one or more internet switching and routing devices configured to provide (i) local area network communications between computing devices 1500a and cluster storage arrays 1510a via local cluster network 1512a, and (ii) wide area network communications between computing cluster 1509a and computing clusters 1509b and 1509c via wide area network link 1513a to network 1306.
- Cluster routers 1511b and 1511c can include network equipment similar to cluster routers 1511a, and cluster routers 1511b and 1511c can perform similar networking functions for computing clusters 1509b and 1509b that cluster routers 1511a perform for computing cluster 1509a.
- the configuration of cluster routers 1511a, 1511b, 1511c can be based at least in part on the data communication requirements of the computing devices and cluster storage arrays, the data communications capabilities of the network equipment in cluster routers 1511a, 1511b, 1511c, the latency and throughput of local cluster networks 1512a, 112b, 1512c, the latency, throughput, and cost of wide area network links 1513a, 1513b, 1513c, and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency and/or other design criteria of the moderation system architecture.
- FIG. 16 is a flowchart of a method 1600, in accordance with example embodiments.
- Method 1600 can be executed by a computing device, such as computing device 1400.
- Method 1600 can begin at block 1610, where the method involves receiving, by an audio input component of a computing device, an audio mixture from one or more sources.
- the method involves predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component. [170] At block 1630, the method involves determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component.
- the method involves providing, by the computing device, the predicted one or more near sounds.
- the predicting of the respective distances involves predicting, by a trained audio separation neural network, one or more sounds corresponding to the one or more sources. Such embodiments also involve predicting the respective distances for the predicted one or more sounds.
- the distance estimation neural network may be a convolutional neural network.
- the distance estimation neural network may be a recurrent neural network.
- the recurrent neural network may be a Long Short-Term Memory (LSTM) network.
- LSTM Long Short-Term Memory
- Some embodiments involve the trained distance estimation neural network having been trained to predict the respective distances by determining a plurality of room impulse responses (RIRs) with an image method room simulator. Such embodiments also involve determining, based on the plurality of RIRs, the direct-to-reverb eration ratio (DRR). In some embodiments, the determining of the plurality of RIRs may be performed with frequency-dependent wall filters.
- RIRs room impulse responses
- DRR direct-to-reverb eration ratio
- the predicting of the respective distances involves separating human speech from ambient noise.
- the separating of the human speech from the ambient noise may be performed by a TasNet model.
- the separating of the human speech from the ambient noise involves determining a time-frequency mask. Such embodiments also involve applying the time-frequency mask to the audio mixture to predict a signal comprising the human speech.
- Some embodiments involve providing, by an interactive graphical user interface of the computing device, a user-adjustable control to receive the threshold distance.
- Some embodiments involve receiving, by the user-adjustable control, a particular threshold distance input by a user of the computing device, and wherein the providing of the predicted one or more near sounds and the one or more far sounds is based on the particular threshold distance. [180] In some embodiments, the providing of the predicted one or more near sounds involves suppressing, by the computing device, at least one of the one or more far sounds.
- the providing of the predicted one or more near sounds involves enhancing, by the computing device, at least one of the one or more near sounds.
- Some embodiments involve providing, by the computing device, the predicted one or more far sounds.
- Some embodiments involve determining, by the computing device, a request to predict the one or more near sounds and the one or more far sounds. Such embodiments involve sending the request to predict the one or more near sounds and the one or more far sounds from the computing device to a second computing device, the second computing device comprising a trained version of the neural network. Such embodiments also involve, after sending the request, the computing device receiving, from the second computing device, the predicted one or more near sounds and the one or more far sounds, and the providing of the predicted one or more near sounds and the one or more far sounds involves providing the predicted one or more near sounds and the one or more far sounds as received from the second computing device.
- Some embodiments involve obtaining a trained neural network at the computing device, and the predicting of the one or more near sounds and the one or more far sounds involves predicting by the computing device using the trained neural network.
- FIG. 17 is a flowchart of a method 1700, in accordance with example embodiments.
- Method 1700 can be executed by a computing device, such as computing device 1400.
- Method 1700 can begin at block 1710, where the method involves receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator.
- the method involves training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component.
- the method involves providing, by the computing device, the trained neural network.
- the acoustic simulator may be an image-method room simulator with frequency dependent wall filters.
- the generating of the audio involves reproducing, by the acoustic simulator, one or more acoustic properties of component sounds in a given audio mixture of the plurality of audio mixtures.
- Some embodiments involve generating, by the acoustic simulator, one or more reverb impulse responses (RIRs) for a room, wherein the room is associated with a plurality of acoustic properties.
- RIRs reverb impulse responses
- a location of the audio input component in the room may be randomized.
- a location of at least one sound source in the room may be randomized.
- Some embodiments involve training an audio separation neural network to separate a given audio mixture of the plurality of audio mixtures into one or more sounds corresponding to one or more sources in the given audio mixture. Such embodiments involve applying the distance estimation neural network to estimate a distance of the one or more sources from the audio input component. The determining of the one or more near sounds and the one or more far sounds may be based on the estimated distance.
- the training of the neural network may be performed at the computing device.
- each block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments.
- Alternative embodiments are included within the scope of these example embodiments.
- functions described as blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved.
- more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
- a block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique.
- a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data).
- the program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique.
- the program code and/or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage medium.
- the computer readable medium may also include non-transitory computer readable media such as non-transitory computer-readable media that stores data for short periods of time like register memory, processor cache, and random access memory (RAM).
- the computer readable media may also include non-transitory computer readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or nonvolatile storage systems.
- a computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- a block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
Abstract
A computer-implemented method of applying a trained neural network for sound separation based on distance estimation is provided. The method includes receiving, by an audio input component of a computing device, an audio mixture from one or more sources. The method includes predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component. The method includes determining one or more near sounds and one or more far sounds based on the respective distances. The near sounds correspond to sources that are located within a threshold distance of the audio input component, and the far sounds correspond to sources that are not located within the threshold distance of the audio input component. The method includes providing the predicted one or more near sounds.
Description
Sound Separation based on Distance Estimation using Machine Learning Models
CROSS-REFERENCE TO RELATED APPLICATIONS/ INCORPORATION BY REFERENCE
[1] This application claims priority to U.S. Provisional Patent Application No. 63/367,350, filed on June 30, 2022, which is hereby incorporated by reference in its entirety.
BACKGROUND
[2] Sound separation refers to the extraction of a subset of sounds from a mixture of sounds. Extracting estimates of clean speech in the presence of interference is a long-standing research problem in signal processing. This task is referred to as speech enhancement when the interference is non-speech, and speech separation when the interference is speech. Assisted listening devices, such as hearing aids, are an important application for these methods, with a typical task being to aid in conversations held in a crowded or noisy space.
SUMMARY
[3] Implementing sound separation for assisted listening is fraught with challenges. The requirement of low algorithmic latency increases the difficulty of the separation problem. Limitations in computational power and memory further strain the ability of source separation neural networks, for which the quality of separation is typically an increasing function of model size. Another challenge in assisted listening is that different users may have different preferences for sounds that are to be enhanced, and/or sounds that are to be suppressed.
[4] The problem of selecting sounds that are to be enhanced has been approached by focusing on speech or otherwise classifying the sounds, as well as using visual input to select the sounds of interest. However, selective listening methods may be cumbersome and may require user effort to control which sounds are enhanced. Such methods may also typically exclude sounds other than speech that the user may want to hear, such as nearby music, the clinking of wine glasses at a dinner table, or the sound of a dropped set of keys. Class-based methods that attempt to include non-speech sounds can fail when there are interfering sounds of the same classes as the desired sounds.
[5] Some techniques such as beamforming use multiple microphones and generally use the direction of arrival (DOA) of sources as a cue to separate them. Near-field beamforming methods can additionally estimate source distance using the spherical nature of acoustic wave
fronts. However, such techniques may only distinguish distances of nearby sounds within a range limited by the size of the microphone array, for example, for a line array with halfwavelength, X/2, spacing and total length /., the maximum range is L2 /2A. In some embodiments, DOA and distance estimation may be considered jointly for sound events with two microphones.
[6] Some techniques have demonstrated the feasibility of estimating reverberation parameters such as T60 and direct-to-reverberation ratio (DRR). Neural networks can also be trained to estimate reverberation parameters from a single microphone, as well as room volume. The success of these methods suggests that neural networks are able to perceive cues from raw audio to accurately predict properties of acoustic transfer functions, and that neural networks may be trained to estimate the distance of a source.
[7] Accordingly, there is a need for a sound separation model that can operate on one or more microphones, and that is not subject to physical limitations on a maximum distance that limits near-field beamforming techniques. Also, for example, rather than predict reverb parameters, the model proposed herein can implicitly leverage acoustic cues to identify one or more near sources and one or more far sources in order to separate them.
[8] Although single-channel dereverberation methods attempt to separate a direct path of a reverberant signal (i.e., the shortest path of sound propagation) from the reverberant components (i.e. the longest paths of sound propagation), the goal of distance-based sound separation as described herein, is to group sources together based only on whether they are near or far based on a distance threshold, without a need to alter a reverberance associated with the sources. Also, dereverberation methods are generally based on an assumption of a single reverberant source, while the distance-based sound separation as described herein can handle multiple sources.
[9] Also described herein is a system to estimate distances of audio sources. For example, the network may be trained to learn the DRR and estimate distances based on this. Accordingly, the network may not have to estimate room characteristics, and estimate the distance directly instead. In some embodiments, the quality of a voice signal may be evaluated. For example, a system can be configured to evaluate voice quality without access to the original clean audio. In many applications, it may be challenging to access a close-talking microphone signal, or the far end of the communications system. Modern deep neural networks (DNNs) can make such a system viable as humans can judge the quality of a vocal signal, even without knowing the language. Also, for example, such measurement can be performed by predicting physical and/or explainable properties. These features may characterize the noise, distortion, and/or
reverberation, and allow users and system engineers to take action on perceived degradations. The quality of the degradation detectors may be analyzed, and these can be configured to estimate mean opinion scores (MOS), and explain how different degradations affect user’s perceptions, as indicated by a one-dimensional MOS measure.
[10] Accordingly, distance based sound separation may be performed in several ways. In a first example, a neural network may be trained to separate audio into near and far sounds, and output the predicted near and/or far sounds. In another example, a distance estimation network (e.g., based on DRR), may be trained to estimate respective distances of audio sources, and the sources may be identified as near and far sounds based on a distance threshold. In another example, a sound separation network may be trained to separate the audio into the component audio tracks, and the distance estimation model may be utilized to estimate the respective distances of the separated component audio tracks. Subsequently, the predicted component audio tracks may be identified as near and far sounds based on a distance threshold.
[11] In one aspect, a computer-implemented method of applying a trained neural network for sound separation based on distance estimation is provided. The method includes receiving, by an audio input component of a computing device, an audio mixture from one or more sources. The method also includes predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component. The method additionally includes determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component. The method further includes providing, by the computing device, the predicted one or more near sounds.
[12] In a second aspect, a computing device is provided. The computing device includes one or more processors and data storage. The data storage has stored thereon computer-executable instructions that, when executed by one or more processors, cause the computing device to carry out functions of sound separation based on distance estimation. The functions include: receiving, by an audio input component of a computing device, an audio mixture from one or more sources; predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component; determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to
sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the predicted one or more near sounds.
[13] In a third aspect, a computer program is provided. The computer program includes instructions that, when executed by a computer, cause the computer to carry out functions of sound separation based on distance estimation. The functions include: receiving, by an audio input component of a computing device, an audio mixture from one or more sources; predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component; determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the predicted one or more near sounds.
[14] In a fourth aspect, an article of manufacture is provided. The article of manufacture includes one or more computer readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to carry out functions of sound separation based on distance estimation. The functions include: receiving, by an audio input component of a computing device, an audio mixture from one or more sources; predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component; determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the predicted one or more near sounds.
[15] In a fifth aspect, a system to carry out functions of sound separation based on distance estimation is provided. The system includes: means for receiving, by an audio input component of a computing device, an audio mixture from one or more sources; means for predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component; means for determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not
located within the threshold distance of the audio input component; and means for providing, by the computing device, the predicted one or more near sounds.
[16] In a sixth aspect, a computer-implemented method of for sound separation based on distance estimation is provided. The method includes receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator. The method also includes training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component. The method additionally includes providing, by the computing device, the trained neural network.
[17] In a seventh aspect, a computing device for sound separation based on distance estimation is provided. The computing device includes one or more processors and data storage. The data storage has stored thereon computer-executable instructions that, when executed by one or more processors, cause the computing device to carry out functions. The functions include: receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator; training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the trained neural network.
[18] In an eighth aspect, a computer program for sound separation based on distance estimation is provided. The computer program includes instructions that, when executed by a computer, cause the computer to carry out functions. The functions include: receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator; training, based on the training data and for an input audio mixture, a distance estimation neural
network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the trained neural network.
[19] In a ninth aspect, an article of manufacture for sound separation based on distance estimation is provided. The article of manufacture includes one or more computer readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to carry out functions. The functions include: receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator; training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the trained neural network.
[20] In a tenth aspect, a system for sound separation based on distance estimation is provided. The system includes means for receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator; means for training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and means for providing, by the computing device, the trained neural network.
[21] The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects, embodiments, and features described above, further
aspects, embodiments, and features will become apparent by reference to the figures and the following detailed description and the accompanying drawings.
BRIEF DESCRIPTION OF THE FIGURES
[22] FIG. 1 A is a diagram illustrating an example distance based sound separation model, in accordance with example embodiments.
[23] FIG. IB is a diagram illustrating another example distance based sound separation model, in accordance with example embodiments.
[24] FIG. 1C is a diagram illustrating another example distance based sound separation model, in accordance with example embodiments.
[25] FIG. 2A illustrates an example source distribution by distance from a microphone, in accordance with example embodiments.
[26] FIG. 2B illustrates an example two dimensional (2D) spatial distribution, in accordance with example embodiments.
[27] FIG. 3 is a table illustrating mean SI-SDRi for distance threshold, in accordance with example embodiments.
[28] FIG. 4 is a table illustrating performance as a function of distance threshold, in accordance with example embodiments.
[29] FIG. 5 is an image illustrating a near target with one near source and a far target with one near source, in accordance with example embodiments.
[30] FIG. 6 is an image illustrating a near target with two near sources and a far target with two near sources, in accordance with example embodiments.
[31] FIG. 7 is an image illustrating a near target with three near sources and a far target with three near sources, in accordance with example embodiments.
[32] FIG. 8 is an image illustrating a near target with four near sources and a far target with four near sources, in accordance with example embodiments.
[33] FIG. 9A is a table illustrating performance as a function of SPP, in accordance with example embodiments.
[34] FIG. 9B is a table illustrating performance as a function of percentage silent training targets, in accordance with example embodiments.
[35] FIG. 10 is a diagram illustrating an example explainable voice-quality detector model, in accordance with example embodiments.
[36] FIG. 11 A is an example scatter plot illustrating a prediction performance for SNR, in accordance with example embodiments.
[37] FIG. 11B is an example scatter plot illustrating a prediction performance for T60, in accordance with example embodiments.
[38] FIG. 11C is an example scatter plot illustrating a prediction performance for direct-to- reverberation (DRR), in accordance with example embodiments.
[39] FIG. 1 ID is an example scatter plot illustrating a prediction performance based on distance using RoomSim data, in accordance with example embodiments.
[40] FIG. 12 is a diagram illustrating training and inference phases of a machine learning model, in accordance with example embodiments
[41] FIG. 13 depicts a distributed computing architecture, in accordance with example embodiments.
[42] FIG. 14 is a block diagram of a computing device, in accordance with example embodiments.
[43] FIG. 15 depicts a network of computing clusters arranged as a cloud-based server system, in accordance with example embodiments.
[44] FIG. 16 is a flowchart of a method, in accordance with example embodiments.
[45] FIG. 17 is a flowchart of another method, in accordance with example embodiments.
DETAILED DESCRIPTION
[46] This application relates, in one aspect, to distance-based sound separation. For example, a user may desire to hear any sounds that occur within a local region around them, and block sounds coming from farther away. A system that accomplishes this would allow the user to engage in normal conversations without the interference of a crowded environment, and without becoming deaf to ordinary non-speech events in their immediate area. Generally, there may be cues for distance that are related to superficial characteristics of the sound, rather than to a fine spectro-temporal structure of the sound. Accordingly, a system that relies on such cues may have a computational advantage, and an advantage in terms of generalization, compared to a system that has to perform deep pattern recognition in order to separate sounds. Humans have a perceptual ability to estimate the relative distances of various sources of sounds. Accordingly, in applications such as assisted listening, there is a need for a machine learning model to perform such distance-based separation akin to human hearing abilities.
[47] Extracting estimates of clean speech in the presence of interference is a challenging problem in signal processing. This task is generally referred to as speech enhancement when the interference is non-speech, and speech separation when the interference is speech. More
generally, sound separation refers to the extraction of a subset of sounds from a mixture of sounds.
[48] Hearing aids and other assisted listening devices are useful applications for these methods, with a typical task being to aid in conversations held in a noisy space. But implementing sound separation for assisted listening is challenging, requiring low algorithmic latency and limited computation and memory. Further, it may be challenging to distill sounds that a user may wish to hear.
[49] Generally speaking, the problem of selecting sounds that are to be enhanced and sounds that are to be suppressed is generally approached by focusing only on speech, or otherwise classifying the sounds, as well as by using visual inputs to select the sounds of interest. However, these methods may not be appropriate in many situations. For example, selective listening methods may be cumbersome and may require user effort to control which sounds are enhanced. Also, for example, such selective listening methods typically exclude sounds other than speech that the user may want to hear, such as nearby music, the clinking of wine glasses at dinner, and/or the tell-tale sound of a dropped set of keys. Class-based methods that attempt to include non-speech sounds can fail when there are interfering sounds of the same classes as the desired sounds.
[50] As described herein, a neural network may be trained to separate signals into two component signals, such as a first set of signals corresponding to nearby sources, and a second set of signals corresponding to sources that are further away, wherein a distance of the sources is measured relative to a distance threshold.
Overview of distance based sound separation of near and far sources
[51] The techniques described herein eliminate deficiencies in class-based methods that attempt to include non-speech sounds, as such methods may fail when there are interfering sounds of the same classes as the desired sounds. The techniques described herein eliminate deficiencies in methods that are based on beamforming, that have physical limitations associated with a maximum distance associated with near-field beamforming. As described herein, acoustic cues can be implicitly leveraged to identify one or more near sources and one or more far sources in order to separate them. This is in contrast to relying on a prediction of reverb parameters. The techniques described herein group sources together based only on whether they are near or far based on a distance threshold, without necessarily altering a reverberance of the sources. Multiple audio sources can be analyzed. The proposed neural networks are computationally efficient, and use fewer resources. Such networks may also be configured to reside on a mobile computing device.
[52] FIG. 1A is a diagram illustrating an example distance based sound separation model 100, in accordance with example embodiments. As illustrated, microphone 105 may receive audio from various sources. Although microphone 105 is illustrated as a standalone device, it can be integrated within a computing device, a speaker, a headset, a mobile computing device, a camera, etc. An example threshold distance 110 is indicated as a distance measured from microphone 105. The audio sources may include one or more users, such as first user 115, second user 120, and third user 125. As illustrated, first user 115 may be within threshold distance 110, whereas second user 120 and third user 125 may be outside threshold distance 110. Microphone 105 may receive audio signals from human and non-human sources. For example, microphone 105 may receive first audio signal 115Afrom first user 115, second audio signal 120 A from second user 120, and third audio signal 125 A from third user 125. These audio signals may be mixed by an audio mixer 105A to generate audio mixture 130. Subsequently, audio mixture 130 may be input into distance based sound separation model 135, which may separate audio mixture 130 into near estimate 140 (e.g., to include audio from first user 115), and far estimate 145 (e.g., to include audio from second user 120 and third user 125). Although far estimate 145 is illustrated as a mixture of second audio signal 120A and third audio signal 125A, in some embodiments, these signals may be separated as well.
[53] FIG. IB is a diagram illustrating another example distance based sound separation model, in accordance with example embodiments. FIG. IB shares one or more components with FIG. 1A. As illustrated, the distance based sound separation model 135 of FIG. 1A (e.g., trained to separate near and far sounds) can be a distance estimation model 150. For example, distance estimation model 150 (e.g., based on DRR), may be trained to estimate respective distances of audio sources from audio mixture 130, and the sources may be identified as near estimate 140 and far estimate 145 based on distance threshold 110.
[54] FIG. 1C is a diagram illustrating another example distance based sound separation model, in accordance with example embodiments. FIG. 1C shares one or more components with FIG. 1A and FIG. IB. As illustrated, the distance based sound separation model 135 of FIG. 1A (e.g., trained to separate near and far sounds) can be a combination of a sound separation model 155 and a distance estimation model 150. For example, sound separation model 155 may be trained to separate audio mixture 130 into component audio tracks, and the distance estimation model 150 may be utilized to estimate the respective distances of the separated component audio tracks. Subsequently, the predicted component audio tracks may be identified as near estimate 140 and far estimate 145 based on distance threshold 110. Details
of sound separation model 155 are not provided herein, as known sound separation networks may be utilized.
[55] Possible cues for distance perception may be traced to physical effects. The intensity I of a direct path component of sound varies with distance d according to inverse-square law I oc 1/d2. However, in an enclosed area, the intensity of a reverberation for the sound, is generally independent of its distance to the source of the sound, such as a microphone. Therefore, the DRR ratio decreases with distance, and may be leveraged as a cue for human distance perception. In some examples, there may be a proximity effect with directional microphones in which low-frequencies are more accentuated for closer sources, and this applies well for sources less than a certain distance threshold, for example, less than one meter away. Absorption by the air is generally a frequency-dependent effect of distance, and may play a role over distances beyond a certain distance threshold, for example, beyond tens of meters. Other effects of distance on sound can originate from spatial effects that could be detected using an array of microphones.
[56] In some embodiments, a neural network for distance-based sound separation may be trained using mixtures of near and far sounds, the acoustic properties of the sounds having been emulated using an acoustic room simulator. For example, ground-truth targets for the near and far signals relative to a distance threshold may be used. For illustrative purposes, in the examples herein, the sounds, both near and far, are speech. As described herein, the neural network can be trained to perform sound separation based on distance. In particular, in some examples, such as with a distance threshold of 1.5 meters (m), a single nearby speaker, and as many as four distant speakers, distance-based sound separation can achieve improvements of up to 4.4 dB in scale-invariant signal to noise ratio (SI-SNR).
Example Models to identify near and far sounds
[57] To separate sound based on distance, an example architecture for a neural network may be based on a short-time Fourier transform (STFT) masking. In some embodiments, for the STFT, a 32 millisecond (ms) square-root Hann window with a 16 ms hop may be used. The 0.3 -power-compressed magnitude of the STFT, Y, of the time-domain mixture, y, may be fed as input to L layers of a unidirectional long short-term memory model (LSTM) with N units each. Then, a fully-connected layer with a sigmoid activation may be applied to create two masks for the input STFT Y, where the two masks may be denoted as Mnear and Mfar. To
ensure consistency of the STFT, each of the masked STFTs for near and far may be passed through inverse and forward STFT operations:
[58] In some embodiments, a training loss may be based on a mean-squared error between 0.3 -power-compressed magnitude of target X and estimate X, based on the relationship below:
(Eqn. 1)
[59] In some embodiments, a first weight (e.g., 0.8) on the near loss, and a second weight (e.g., 0.2) on the far loss may be used. For example, the weighted composition may be denoted
Acoustic Simulation
[60] To generate a large amount of training data, an image-based acoustic room simulator with frequency-dependent wall filters may be used to generate reverb impulse responses (RIRs) for rooms with varied acoustic properties, and with randomized microphone and source locations. Basic distance-related phenomena such as an amplitude and DRR effects can be well simulated. Also, for example, clean speech recordings may be randomly assigned to the source locations within each room and convolved with the corresponding RIRs. Training examples for each room may be generated by combining sources within a threshold distance into a near target xnear, and sources beyond a threshold distance into a far target x^ar. Generally, the near and far targets may be added to create mixture, xnear + x^ar. This mixture may be input to train the model for sound separation.
[61] In some embodiments, within each of the randomly generated rooms, 5 speaker locations and 1 microphone location may be randomly generated, such that the distance between the microphone and each source is maintained to be uniformly distributed, subject to a rejection of samples falling outside each room. The resulting distribution of distances from the microphones is illustrated in Figure 2.
[62] FIG. 2A illustrates an example source distribution 200A by distance from a microphone, in accordance with example embodiments. Source distribution 200A is a histogram that indicates a distance from a microphone in meters along the horizontal axis, and
a number of sources along the vertical axis. For example, within each generated room, 5 speaker locations and 1 microphone location may be randomly generated, such that the distance between the microphone and each source is uniformly distributed subject to staying inside the walls of each randomly generated room. This distribution of distances from the microphones is illustrated in FIG. 2 A.
[63] FIG. 2B illustrates an example two dimensional (2D) spatial distribution 200B, in accordance with example embodiments. As illustrated, a distance from a microphone is indicated in meters along the horizontal axis. Spatial distribution 200B differs from what might be expected in, for example, a restaurant setting, where an average number of sources at a given distance radius is likely to increase as the distance increases.
Data Preparation
[64] In some embodiments, speech recordings from the Libri-Light dataset may be used for training, with validation, and test partitions from the Libri-Speech dataset may be used. The speaker IDs in the Libri-Speech dataset are unique for each partition. For the synthetic room data, rooms may be generated with dimensions varying from 3.0 X 4.0 X 2.13 meters to 7.0 x 8.0 x 3.05 meters.
[65] For example, during training, randomly chosen Libri-Light clips may be reverberated and mixed according to a randomly chosen RIR to create clips that are 10 seconds in duration. Source utterances shorter than 10 seconds are offset at random intervals within the 10 second clip, and source utterances longer than 10 seconds may be clipped to a random 10 second interior segment.
[66] Also, for example, source and microphone locations may be used to determine which sources are near or far in relation to the microphone in the given room, and for a given threshold distance. To vary the number of sources, a source presence probability (SPP) may be applied to each source, so that the total number of sources in a room can vary from 0 to 5, with a distribution dependent on the chosen SPP.
[67] Because the rooms are generated independently of a specific distance threshold, the choice of distance threshold may affect the number of sources considered near versus far for a given room. In particular, the choice of distance threshold may affect the fraction of examples where the sources are considered near or where the sources are considered far, resulting in silent far targets or silent near targets. Distances thresholds of 0.8/1.5/3.0 meters can result in training data with silent far target fractions of 0.0/0.001/0.04, respectively. It is notable that
some thresholds result in a majority of training examples containing a silent target, even for an SPP of 1.0.
Metrics
[68] For training examples where both near and far targets have sources present (i.e. neither target is silent), a scale invariant SDR improvement (SI_SDRjj can be used, as provided below:
SIJDRfx, ?) = Sl_SDR(x, x) - SI_SDR(x; y)
(Eqn. 2)
[69] SI SDR measures signal fidelity with respect to a reference signal while allowing for a gain mismatch, as provided below:
(Eqn. 3)
[70] where a = argmin |2. SI_SDRi diverges when one of the a
NoiseReduction (,y, xnear) = io iogio(| | | |2/| \xnear\ I2)
(Eqn. 4)
[71] which measures a power reduction of the separated output xnear relative to the input audio mixture power y.
Results: Effect of distance threshold
[72] The distance-based separation network described herein may be compared to a conventional speech separation baseline where the same network architecture is trained (but with more masking outputs and a permutation-invariant loss) to output 5 separate source estimates, one for each possible speaker. To upper-bound the performance of this baseline, oracle knowledge of the true near and far targets may be used to find an optimal grouping of the 5 separate source estimates into near and far estimates. In all but one condition, direct distance-based separation outperforms this upper bound on the “separate-all” baseline.
[73] FIG. 3 is a table 300 illustrating mean scale-invariant signal to distortion ratio improvement (SI-SDRi) for distance threshold, in accordance with example embodiments. The values illustrated are based on a threshold of 1.5 m, Long Short Term Memory (LSTM) depth L = 4, width N = 400, and training source presence probability (SPP) of 0.5, on an evaluation set of 1200 5-speaker examples with SPP of 1.0. ’’Distance-based” refers to the method of directly training a model to separate near sounds from far sounds as described herein.
“Separate-all” is a baseline in which the same network architecture is trained to output 5 individual sources, which may then be grouped into one or more near estimates and one or more far estimates. They may be bucketed according to a number of near speakers (1 near speaker implies 4 far speakers, etc.).
[74] FIG. 4 is a table 400 illustrating performance as a function of distance threshold, in accordance with example embodiments. For example, performance may be evaluated for a model (e.g., the model evaluated in table 300 of FIG. 3), trained with varied distance thresholds. SI_SDRi is given for all examples with 1 non-silent near source. Noise reduction may be determined for the 271 examples with silent near targets. As indicated, performance may generally degrade with increased distance threshold values, despite the fact that higher thresholds can result in more non-silent training targets. This is illustrated in Figures 2A-B, and described under “Data Preparation,” where a threshold at 3 meters is closer to the center of the distribution than a threshold of 0.8 or 1.0, and thus may change the training examples to have fewer silent targets.
[75] FIGS. 5-8 illustrate example plots for SI_SDRi in decibels (dB) as a function of input SI-SDR for one or more near targets and one or more far targets, in accordance with example embodiments. For example, SI_SDRi may be determined for a model (e.g., the model evaluated in table 300 of FIG. 3) on test examples with a distance threshold of 1.5 m. The results may be divided by the number of sources in the near target.
[76] FIG. 5 comprises a first distribution 500A illustrating a near target with one near source and a second distribution 500B illustrating a far target with one near source, in accordance with example embodiments.
[77] FIG. 6 comprises a first distribution 600A illustrating a near target with two near sources and a second distribution 600B illustrating a far target with two near sources, in accordance with example embodiments.
[78] FIG. 7 comprises a first distribution 700A illustrating a near target with three near sources and a second distribution 700B illustrating a far target with three near sources, in accordance with example embodiments.
[79] FIG. 8 comprises a first distribution 800A illustrating a near target with four near sources and a second distribution 800B illustrating a far target with four near sources, in accordance with example embodiments.
Results: Effect of distance threshold
[80] Combinations of LSTM with depth L E {2, 4, 6} and width N E {200, 400, 600} are evaluated, except L = 6 and N = 600, which can exceed resource constraints. Overall, larger models appear to perform better, with LSTM width N contributing more to performance increases. The SLSDR difference between the smallest model and the largest model for distance thresholds of 0.8 m, 1.5 m, and 3.0 m appears to be 0.9 dB near / 1.2 dB far, 1.5 dB near / 1.6 dB far, and 1.8 dB near / 1.4 dB far, respectively. As illustrated, with an increasing model size, SI_SDRi appears to improve more for higher distance thresholds.
Results: Effect of varied speaker count in evaluation set
[81] To explore whether the model uses the relationship between the number of near sources and the number of far sources, the SPP may be varied. For example, an SPP of 1.0 may indicate that the 5 sources are present in the mixture, and in general, a SPP of p induces a binomial distribution 2(5, p) over a number of sources. This generally results in a modification of the distribution of near and far speaker counts. For example, the baseline model described with reference to FIG. 3 was trained with a SPP of 0.5; however, the evaluation set used in other sections may use a SPP of 1.0 for ease of slicing data by the number of near speakers. This indicates that training data and the evaluation data differ in terms of near and far speaker counts.
[82] In order to study this mismatch, a version of the evaluation set may be generated with a SPP of 0.5. This can provide a set with 527 silent near target examples, 411 examples with sound in near and far (i.e. 1 to 4 speakers in each target, sum < 5), and 62 examples with silent far targets. The baseline model may be trained with a SPP of 1.0 instead of 0.5. Results for the baseline model and evaluation data at SPPs of 0.5 and 1.0 are illustrated in FIG. 9A.
[83] FIG. 9A is a table 900A illustrating performance as a function of SPP, in accordance with example embodiments. For example, performance may be evaluated for a model (e.g., the model evaluated in table 300 of FIG. 3, except for SPP). For Eval SPP = 1, SI_SDRi may be provided for 727 examples with one or more non-silent near targets and one or more far targets. Noise reduction may be calculated for the 271 examples with silent near targets. For Eval SPP =0 .5, SI_SDRi may be for 394 non-silent near and far target examples, noise reduction for 538 silent near targets examples.
[84] Generally, the performance of the models appears to be optimal when the training SPP matches the SPP associated with the evaluation data, as may be expected. However, the effect of mismatch for near SI_SDRi appears to be relatively minor. For far SI_SDRi, more degradation with mismatched SPPs may be observed, indicating that source count may be a significant cue for far source estimation.
Results: Effect of silent targets
[85] In some embodiments, to study the effect of silent targets, the RIRs may be pre-filtered to achieve certain percentages of silent targets (e.g., prior to applying SPP). The results are illustrated in table 900B in FIG. 9B for SPPs of 1.0 and 0.5.
[86] FIG. 9B is a table 900B illustrating performance as a function of percentage silent training targets, in accordance with example embodiments. “% rooms silent target” may refer to a number of rooms in the training set with silent near or far targets, before the SPP was applied. The noise reduction may be calculated only for the 271 evaluation examples with silent near targets. The correlation between increased performance and decreased silent targets seen in table 900B suggests that the degradation illustrated in FIG. 4 with increased distance threshold is a true artifact of the distance, as opposed to an effect of the change in training data between models. Note that SPP and percentage of rooms with silent targets are compounding effects. As expected, providing the model with more examples with non-silent targets increases SI_SDRi on non-silent examples, but can degrade noise reduction on silent near examples.
Overview of Distance Estimation Models
[87] A system for estimating a distance of a sound source based on DRR is described. For example, the quality of a voice signal after it is recorded and/or transmitted may be measured. This may be performed without the use of the original or reference signal, as human listeners are able to judge the quality of a vocal signal, even without knowing the spoken language. The vocal quality may be described by characterizing several degradations of the signal that can be interpreted and acted upon by the system engineers and/or the user. As described herein, the amount of non-speech noise, degradations due to reverberation and the distance to the speaker, may be estimated.
[88] Dereverberation is a technique to remove effects of reverberation from sound, after reverberant sound has been captured by microphones. Reverberation is generally caused by sound reflections in an enclosed space (e.g., a room) and can be quantified by a room reverberation time and a direct-to-reverberant ratio (DRR). Dereverberation increases the direct-to-reverberant ratio resulting in sound that is perceived as closer and clearer.
[89] When a discrete time room impulse response (RIR), h(n), is known, the DRR at a given location may be determined as a ratio of the sound pressure level of a direct sound from a directional source to the reverberant sound pressure level incident at the given location. The relationship is provide as follows:
(Eqn. 5)
[90] Where nd is a sample index of a peak of the direct sound arrival and n0 is a number of samples corresponding to a selected temporal window (e.g., ranging from 1.0 to 2.5 ms). When the RIR, h(ri), is known, DRR can be computed from Eqn. 5. However, in an absence of RIR, the DRR may be computed independently, using an estimation of the sound sources.
[91] A mean opinion score (MOS) is one way to measure voice quality. To determine the MOS, listeners may be asked to rate the quality of a voice signal on a five point scale. Listeners may generally judge which signal is cleaner when the degradation varies along any one axis (e.g., noise, reverb, compression artifacts). However, listeners may find it more challenging to put them on a metric scale. Also, for example, given a MOS rating, it can be challenging to infer what aspect of the recording or communication setup may need to be adjusted.
[92] Modem DNN speech-enhancement systems may be trained to learn which portions of a speech spectrogram are part of the voice and which are noise. Such models may be used to predict various degradations of the signal. For illustrative purposes, the various degradation estimates may be combined to determine a MOS, and a contribution of each degradation to the overall MOS may be determined.
Example Training Datasets
[93] In some embodiments, the models may be trained with two types of data: RoomSim and MOS. The individual feature models may be trained with real speech and noise in a simulated room environment. The MOS features may be trained with datasets from the Conferencing Speech 2022 challenge with human judgements.
[94] FIG. 10 is a diagram illustrating an example explainable voice-quality detector model, in accordance with example embodiments. Input audio 1005 may be received by block 1010 for training with RoomSim data. Block 1010 may include one or more TasNet blocks 1020 and one or more Regress blocks 1025. The two outputs of each TasNet block 1020 are the putative speech and noise signals, and these are fed into the one or more Regress blocks 1025. The one or more Regress blocks 1025 can independently provide outputs 1030 to block 1015 for training with the Microsoft/TU Berlin Public Switched Telephone Network (PSTN) data. The outputs 1030 may include DRR (used to estimate the distance of a sound source), signal-to- noise ratio (SNR), T60 (the time it takes for the RIR to decay by 60 dB from the level of the
direct path), and distance based sound separation. However, the regressor 1035 can drive these signals to change their representation to generate the MOS 1040.
Training with RoomSim
[95] In some embodiments, feature detectors may be trained with public audio processed by a room simulator. The clean speech may be from the Libri-Light dataset for training and from Libri-TTS for evaluation. Non-speech sounds may be used from a variety of publicly available resources, including freesound.org. Freesound is a collaborative database of sounds licensed by the Creative Commons (CC). As another example, audio from the Free Universal Sound Separation (FUSS) dataset may be used. For audio from Freesound, artificial sounds (such as synthesizer noises) may be filtered out based on user annotated tags. Also, for example, a sound classification network may be trained on AudioSet to avoid clips likely containing speech. During training, sources may be reverberated and mixed on the fly, and the validation and test sets may each be based on about 9 hours of speech data.
[96] Room impulse responses (RIRs) may be generated with an image-method room simulator with frequency-dependent wall filters. For each example, the RIR can be generated by sampling random locations for a microphone and sound sources within a room defined using a random size, such as, with a width from 3 to 7 m, length from 4 to 8 m, and height between 2.13 and 3.05 m. During RIR generation, source “image” locations may be randomly perturbed (e.g., by up to 8 cm in each direction) to avoid a “sweeping echo” effect.
[97] In some embodiments, 140,000 training, 20,000 validation datasets may be generated, along with 20,000 test rooms that are used to generate train, validate and test data. The speech and noise can be “recorded” in rooms with simulated impulse response between the location of a source and a microphone. In some embodiments, a random speech signal may be selected, three random noises may be selected, and a random room size may be selected, and the sound sources and microphone can be placed at random in the room. Random gains can be applied to the sources that result in speech with an average level (e.g., of XX dB) relative to full scale with standard deviation (e.g., of sXX dB). The non-speech noise has an average level of YY dB with standard deviation sYY dB.
Physical Degradations
[98] In some embodiments, the ground truth may be determined for the features as the room’s environment is mixed and simulated. The SNR is the power of the chosen speech signal divided by the power of all the noise signals, in dB. The RIR defines both a direct path, the
height of the first peak in the RIR, and the reverberant signals, and later peaks. The DRR is then the power in the first RIR peak, divided by the sum of all the other peaks, measured in dB.
[99] Determination of the T60 time, the time it takes for the RIR to decay by 60 dB from the level of the direct path, can also be done from the RIR. In order to perform this calculation robustly, in the face of a noise floor, linear regression may be applied in the log domain on the RIR. This regression can be carried out over the “valid” portions of the RIR, for example, by removing everything to just after the first (direct) response, and deleting the points after the noise floor. A line may be fit to the remaining points in the log domain to estimate the slope of the reverberation decay, and the T60 time may be estimated from this.
[100] Also, for example, the distance from the speech source to the microphone is a known quantity since the RoomSim calculation picks these two positions at random, and the true distance can be determined based on this.
MOS Data
[101] In some embodiments, models to estimate MOS 1040 may be determined using three datasets from the Conferencing Speech 2022 challenge. Example datasets are:
[102] PSTN: A random subset of the PSTN data. For example, the PSTN data provides MOS data for 58709 10 s samples of speech with added noise that is also corrupted by various telephone network connections. This dataset has an MOS estimate based on an average of 4.6 human judgements.
[103] NISQA: The NISQA corpus file data, also from TU Berlin.
[104] Tencent: The without Reverberation Train Dev MOS portion of the Tencent data.
[105] An average MOS estimate, the 95% confidence interval, and the number of judgements for each sample may be provided. From this information the inter-subject agreement can be determined and the standard deviation of human estimates of the MOS may be estimated as 0.86.
Models: Feature Models
[106] As illustrated in FIG. 10, a convolutional TasNet architecture 1020 can be used. TasNet 1020 can be configured to learn what speech looks like, and generate a mask that separates the speech from the noise. In speech enhancement tasks, TasNet 1020 may be configured to predict a time-frequency mask, which is applied to the original signal to predict a clean signal, and from this a mean-squared loss may be computed.
[107] In the feature detectors, the “speech” and the “noise” outputs of the TasNet network may be combined with the following operations: ReLU that involves dividing “speech” energy
by “noise” energy and then taking the logarithm to form a feature estimate. The “speech” and “noise” signals are in quotes because that is their role in the normal speech-enhancement network. However, in the voice-quality models described herein, as each physical feature model is trained end-to-end, the network is free to use these signals in any form as long as the final feature output is moving towards the desired value. The same architecture can be used to estimate SNR (+/- 40dB), DRR (+/- 40dB), T60 (0-ls) or distance (0-20m).
[108] The input to the physical feature networks can be audio that have been segmented into 8s frames. The RoomSim calculation 1010 can be part of the training pipeline so it can be efficiently executed on a TPU. In some embodiments, for each trial, the chosen (random) speech and noise signals may be convolved with the RIR for the chosen source and microphone positions to form the received microphone signals. In some embodiments, a small bias may be added to the logs and divisions so that the results are finite. In some embodiments, the network may be trained to a configuration close to a final performance in a short time (e.g., less than a day using 4 x 2 arrays of TPUs).
Models: MOS Models
[109] Regressor 1035 can be linear or DNN and may be configured to predict a MOS 1040 from the physical estimates. In some embodiments, a linear regression can provide a more intuitive model since the decision factors are linear weights, but a linear regressor may not perform as well as a non-linear regressor. Thus, for better results a 3 layer DNN may be used, with 10 hidden units, batch normalization, and skip connections. A model may be trained for each dataset using 80% of the physical data inferred from that dataset, and then test the model on the remaining 20%.
[HO] The results may be described in three ways: feature prediction accuracy, MOS predictor accuracy, and explaining MOS via known degradations.
Results: Feature predictors
[Hl] In some embodiments, four different feature predictor networks may be trained, one for each of the current features: SNR, DRR, T60 and distance. Performance of these networks may be evaluated on a separate validation set (and/or on simulated data). FIGS. 11A-D illustrate scatter plots for each of these feature predictors.
[112] FIG. 11A is an example scatter plot 1300A illustrating a prediction performance for SNR, in accordance with example embodiments. The horizontal axis represents the true SNR in dB and the vertical axis represents the predicted SNR in dB. The SNR model achieves a correlation of 96% and an error with a standard deviation of +/-15dB.
[113] FIG. 11B is an example scatter plot 1300B illustrating a prediction performance for T60, in accordance with example embodiments. The horizontal axis represents the true T60 in seconds and the vertical axis represents the predicted T60 in seconds. The T60 model achieves a correlation of 99% and an error with a standard deviation of 130ms. The same architecture that was used to predict power above is predicting a time. The T60 time is a function of the room geometry and wall reflectivity, and is generally not under the user’s control. In some embodiments, the log of T60 may be predicted as a relevant measure.
[114] FIG. 11C is an example scatter plot 1300C illustrating a prediction performance for DRR, in accordance with example embodiments. The horizontal axis represents the true DRR in dB and the vertical axis represents the predicted DRR in dB. The DRR describes how much of the received signal is due to direct path and a high DRR suggests that the speaker is close to the microphone, and the direct path dominates over reverberations. The model provides a correlation of 97%, and the error has a standard deviation of 1 ,5dB. As described with reference to FIG. IB and 1C, the DRR may be used to estimate a distance of a source, and an audio mix may be separated into near and far sounds based on the estimated distance.
[115] FIG. 1 ID is an example scatter plot 1300D illustrating a prediction performance based on distance using RoomSim data, in accordance with example embodiments. The horizontal axis represents the true distance in meters and the vertical axis represents the predicted distance in meters. The distance predictions appear to have an 89% correlation with the true results and are lower than preceding measures. The standard deviation of the errors is about Im, and the log of distance may be determined as an estimate.
Results: MOS predictors
[116] In some embodiments, the estimates of four physical features may be combined into a model for MOS using linear and DNN regressors. In some embodiments, separate linear and DNN models may be generated for each dataset. With a full DNN model using the PSTN data, a root-mean-squared error (RMSE) of 0.67 may be achieved. This compares favorably with the inter-subject standard deviation of the human data (0.86). For comparison purposes, a DNN model may be trained and tested using just the NISQA VAL SIM data from NISQA. Using the 10,000 utterances supplied, a Pearson correlation between the predicted and expected MOS of 0.63 and RMSE of 0.85 may be achieved.
[117] Also, for example, a relative significance of features may be determined by building and testing the MOS models with a subset of the physical features. The correlation between the true and the predicted MOS is then a measure of the importance of that feature set to the true
MOS. With reference to FIGS. 11A-D, SNR appears to be a significant feature, followed by DRR, and distance (which is related to DRR), and T60 appears to be much less important, perhaps because humans may not perceive reverberation due to the precedence effect, at least within the range of T60 values in the training and evaluation sets. A correlation of 67%, which may be achieved using just the SNR estimate, indicates an r2 of 43%, suggesting that 43% of the variance in the MOS data may be explained by the SNR estimates. The DRR estimates explain a couple more percentages. In effect such an ablation study demonstrates, on average, how different physical features factor into the average human MOS judgements.
Explaining MOS via degradations
[118] The model described herein may be analyzed by artificially generating a range of feature values and measuring the MOS predictions. Generally, the MOS may be higher for high SNR and DRR. The default values used, when not on the grid, can be DRR = OdB, SNR = lOdB, T60 = 400ms, distance = 2m.).
[119] An explainable model for voice quality can have various applications. For example, a user who is recording a conversation may be provided with actionable feedback that allows them to improve their audio or video recordings. Also, for example, system engineers may want to know when the noise or distortion is large, which might indicate that there are transmission problems. The quality of the MOS estimator may be adjusted by adding additional physically quantifiable features, such as degradations from the codec, transmission errors, speech intelligibility, and so on. Also, for example, more sophisticated network components, such as attention and non-linear pooling may be added.
[120] Generally speaking, explainability may be a more desirable quantity to measure than absolute accuracy, especially as humans are limited to a five-point scale and the ambiguity that comes with differentiating between noise and reverberation is more important to quality. In some embodiments, a separate full TasNet model may be used for each predictor. The number of superblocks may be reduced from 4 to 1 and the number of blocks within a superblock.
Example Applications
[121] The model described herein can perform distance based sound separation. The model can determine a distance threshold to identify one or more near sounds and one or more far sounds. In some embodiments, the distance threshold can be a learnable hyperparameter of the neural network.
[122] In some embodiments, the distance threshold may be input by a user via an interactive graphical user interface. For example, the model may recommend one or more suggested
distance thresholds to the user, and the user may select one of the suggested distance thresholds. As another example, the user may input the distance threshold (e.g., using a slidable bar). In some embodiments, a virtual model of the user location in an environment may be provided, and the user may use a graphic object (e.g., an adjustable circle, ellipse, a free form boundary diagram, and so forth) to input the distance threshold. For example, the user may be provided with a graphic illustration of a first table at a restaurant where the user is seated with two other individuals, and one or more additional tables. The user may select the table, the two other individuals, and/or draw a boundary around the table and the two other individuals to indicate a distance threshold.
[123] In some embodiments, the neural network can perform one or more of suppressing nearby sounds, suppressing far sounds, enhancing near sounds, or enhancing far sounds.
[124] In some embodiments, the neural network can be combined with an audio separation network that separates one or more sources of sound from audio mixture, and the neural network can then identify the one or more near sounds and the one or more far sounds based on the separated one or more sources of sound (e.g., using distance estimation). Also, for example, subsequent to the near and far sounds being identified, an audio separation network can separate the near and far sounds into respective component tracks.
[125] In some embodiments, an audio separation network can separate one or more sources of sound, and the distances of the one or more sources of sound may be estimated (e.g., by a distance estimation neural network), and the neural network can then identify the one or more near sounds and the one or more far sounds based on the estimated distances.
[126] Also, for example, the model may reside on a mobile device and perform distance based sound separation.
[127] Also, for example, the model may reside at a remote server to perform distance based sound separation for audio received at a computing device.
[128] Generally speaking, the techniques may be applied to assisted listening devices, intelligent voice assistants, intelligent speaker calibration, robotic control circuitry (e.g., identifying one or more near objects and/or one or more far objects based on sound separation), autonomous and semi-autonomous driving (e.g., identifying one or more near traffic sounds and one or more far traffic sounds), intelligent cameras, security and/or surveillance cameras, and so forth.
[129] These and other example applications are contemplated within a scope of this disclosure.
Training Machine Learning Models for Generating Inferences/Predictions
[130] FIG. 12 shows diagram 1200 illustrating a training phase 1202 and an inference phase 1204 of trained machine learning model(s) 1232, in accordance with example embodiments. Some machine learning techniques involve training one or more machine learning algorithms on an input set of training data to recognize patterns in the training data and provide output inferences and/or predictions about (patterns in the) training data. The resulting trained machine learning algorithm can be termed as a trained machine learning model. For example, FIG. 12 shows training phase 1202 where one or more machine learning algorithms 1220 are being trained on training data 1210 to become trained machine learning model(s) 1232. Then, during inference phase 1204, trained machine learning model(s) 1232 can receive input data 1230 and one or more inference/prediction requests 1240 (perhaps as part of input data 1230) and responsively provide as an output one or more inferences and/or prediction(s) 1250.
[131] As such, trained machine learning model(s) 1232 can include one or more models of one or more machine learning algorithms 1220. Machine learning algorithm(s) 1220 may include, but are not limited to: an artificial neural network (e.g., a herein-described convolutional neural networks, a recurrent neural network, a Bayesian network, a hidden Markov model, a Markov decision process, a logistic regression function, a support vector machine, a suitable statistical machine learning algorithm, and/or a heuristic machine learning system). Machine learning algorithm(s) 1220 may be supervised or unsupervised, and may implement any suitable combination of online and offline learning.
[132] In some examples, machine learning algorithm(s) 1220 and/or trained machine learning model(s) 1232 can be accelerated using on-device coprocessors, such as graphic processing units (GPUs), tensor processing units (TPUs), digital signal processors (DSPs), and/or application specific integrated circuits (ASICs). Such on-device coprocessors can be used to speed up machine learning algorithm(s) 1220 and/or trained machine learning model(s) 1232. In some examples, trained machine learning model(s) 1232 can be trained, reside on, and be executed, to provide inferences on a particular computing device, and/or otherwise can make inferences for the particular computing device.
[133] During training phase 1202, machine learning algorithm(s) 1220 can be trained by providing at least training data 1210 as training input using unsupervised, supervised, semisupervised, and/or reinforcement learning techniques. Unsupervised learning involves providing a portion (or all) of training data 1210 to machine learning algorithm(s) 1220 and machine learning algorithm(s) 1220 determining one or more output inferences based on the provided portion (or all) of training data 1210. Supervised learning involves providing a portion of training data 1210 to machine learning algorithm(s) 1220, with machine
learning algorithm(s) 1220 determining one or more output inferences based on the provided portion of training data 1210, and the output inference(s) are either accepted or corrected based on correct results associated with training data 1210. In some examples, supervised learning of machine learning algorithm(s) 1220 can be governed by a set of rules and/or a set of labels for the training input, and the set of rules and/or set of labels may be used to correct inferences of machine learning algorithm(s) 1220.
[134] Semi-supervised learning involves having correct results for part, but not all, of training data 1210. During semi-supervised learning, supervised learning is used for a portion of training data 1210 having correct results, and unsupervised learning is used for a portion of training data 1210 not having correct results. Reinforcement learning involves machine learning algorithm(s) 1220 receiving a reward signal regarding a prior inference, where the reward signal can be a numerical value. During reinforcement learning, machine learning algorithm(s) 1220 can output an inference and receive a reward signal in response, where machine learning algorithm(s) 1220 are configured to try to maximize the numerical value of the reward signal. In some examples, reinforcement learning also utilizes a value function that provides a numerical value representing an expected total of the numerical values provided by the reward signal over time. In some examples, machine learning algorithm(s) 1220 and/or trained machine learning model(s) 1232 can be trained using other machine learning techniques, including but not limited to, incremental learning and curriculum learning.
[135] In some examples, machine learning algorithm(s) 1220 and/or trained machine learning model(s) 1232 can use transfer learning techniques. For example, transfer learning techniques can involve trained machine learning model(s) 1232 being pre-trained on one set of data and additionally trained using training data 1210. More particularly, machine learning algorithm(s) 1220 can be pre-trained on data from one or more computing devices and a resulting trained machine learning model provided to computing device CD1, where CD1 is intended to execute the trained machine learning model during inference phase 1204. Then, during training phase 1202, the pre-trained machine learning model can be additionally trained using training data 1210, where training data 1210 can be derived from kernel and non-kernel data of computing device CD1. This further training of the machine learning algorithm(s) 1220 and/or the pretrained machine learning model using training data 1210 of CDl’s data can be performed using either supervised or unsupervised learning. Once machine learning algorithm(s) 1220 and/or the pre-trained machine learning model has been trained on at least training data 1210, training phase 1202 can be completed. The trained resulting machine learning model can be utilized as at least one of trained machine learning model(s) 1232. Training data 1210 can be a plurality
of audio mixtures, wherein each audio mixture comprises one or more first sounds that are proximate to the audio input component, and one or more second sounds that are not proximate to the audio input component.
[136] In particular, once training phase 1202 has been completed, trained machine learning model(s) 1232 can be provided to a computing device, if not already on the computing device. Inference phase 1204 can begin after trained machine learning model(s) 1232 are provided to computing device CD1.
[137] During inference phase 1204, trained machine learning model(s) 1232 can receive input data 1230 and generate and output one or more corresponding inferences and/or prediction(s) 1250 about input data 1230. As such, input data 1230 can be used as an input to trained machine learning model(s) 1232 for providing corresponding inference(s) and/or prediction(s) 1250 to kernel components and non-kernel components. For example, trained machine learning model(s) 1232 can generate inference(s) and/or prediction(s) 1250 in response to one or more inference/prediction requests 1240. In some examples, trained machine learning model(s) 1232 can be executed by a portion of other software. For example, trained machine learning model(s) 1232 can be executed by an inference or prediction daemon to be readily available to provide inferences and/or predictions upon request. Input data 1230 can include data from computing device CD1 executing trained machine learning model(s) 1232 and/or input data from one or more computing devices other than CD1. For example, input data 1230 can include an audio mixture from one or more sources received by an audio input component of a computing device. Other types of input data are possible as well.
[138] Inference(s) and/or prediction(s) 1250 can include output audio, output intermediate audio, numerical values, and/or other output data produced by trained machine learning model(s) 1232 operating on input data 1230 (and training data 1210). In some examples, trained machine learning model(s) 1232 can use output inference(s) and/or prediction(s) 1250 as input feedback 1160. Trained machine learning model(s) 1232 can also rely on past inferences as inputs for generating new inferences.
[139] A neural network for distance based sound separation can be an example of machine learning algorithm(s) 1220. A neural network for distance estimation of sound sources in an audio mixture can be an example of machine learning algorithm(s) 1220. A neural network for separation of sound sources in the audio mixture can be an example of machine learning algorithm(s) 1220. After training, the trained version of the neural network can be an example of trained machine learning model(s) 1232. In this approach, an example of the one or more inference / prediction request(s) 1240 can be a request to predict the one or more near
sounds and the one or more far sounds and a corresponding example of inferences and/or prediction(s) 1250 can be the predicted one or more near sounds and the one or more far sounds.
[140] In some examples, one computing device CD SOLO can include one or more of the trained versions of the distance based sound separation network, a distance estimation network, or an audio separation network, perhaps after training. Then, computing device CD SOLO can receive a request to predict the one or more near sounds and the one or more far sounds, and use the trained version of the neural network to predict the one or more near sounds and the one or more far sounds.
[141] In some examples, two or more computing devices CD CLI and CD SRV can be used to provide output audio; e.g., a first computing device CD CLI can generate and send requests to predict the one or more near sounds and the one or more far sounds to a second computing device CD SRV. Then, CD SRV can use the trained version of the neural network, to predict the one or more near sounds and the one or more far sounds, and respond to the requests from CD CLI for the predicted one or more near sounds and the one or more far sounds. Then, upon reception of responses to the requests, CD CLI can provide the requested predicted one or more near sounds and the one or more far sounds (e.g., using a user interface and/or a display, a printed copy, an electronic communication, etc.).
Example Data Network
[142] FIG. 13 depicts a distributed computing architecture 1300, in accordance with example embodiments. Distributed computing architecture 1300 includes server devices 1308, 1310 that are configured to communicate, via network 1306, with programmable devices 1304a, 1304b, 1304c, 1304d, 1304e. Network 1306 may correspond to a local area network (LAN), a wide area network (WAN), a WLAN, a WWAN, a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices. Network 1306 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet.
[143] Although FIG. 13 only shows five programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices. Moreover, programmable devices 1304a, 1304b, 1304c, 1304d, 1304e (or any additional programmable devices) may be any sort of computing device, such as a mobile computing device, desktop computer, wearable computing device, head-mountable device (HMD), network terminal, a mobile computing device, and so on. In some examples, such as illustrated by programmable devices 1304a, 1304b, 1304c, 1304e, programmable devices can be directly connected to network 1306. In other examples, such as illustrated by programmable device 1304d,
programmable devices can be indirectly connected to network 1306 via an associated computing device, such as programmable device 1304c. In this example, programmable device 1304c can act as an associated computing device to pass electronic communications between programmable device 1304d and network 1306. In other examples, such as illustrated by programmable device 1304e, a computing device can be part of and/or inside a vehicle, such as a car, a truck, a bus, a boat or ship, an airplane, etc. In other examples not shown in FIG. 13, a programmable device can be both directly and indirectly connected to network 1306.
[144] Server devices 1308, 1310 can be configured to perform one or more services, as requested by programmable devices 1304a-1304e. For example, server device 1308 and/or 1310 can provide content to programmable devices 1304a-1304e. The content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video. The content can include compressed and/or uncompressed content. The content can be encrypted and/or unencrypted. Other types of content are possible as well.
[145] As another example, server device 1308 and/or 1310 can provide programmable devices 1304a-1304e with access to software for database, search, computation, graphical, audio, video, World Wide Web/Internet utilization, and/or other functions. Many other examples of server devices are possible as well.
Computing Device Architecture
[146] FIG. 14 is a block diagram of an example computing device 1400, in accordance with example embodiments. In particular, computing device 1400 shown in FIG. 14 can be configured to perform at least one function for distance based sound separation, and/or method 1600, and/or 1700.
[147] Computing device 1400 may include a user interface module 1401, a network communications module 1402, one or more processors 1403, data storage 1404, one or more camera(s) 1412, one or more sensors 1414, and power system 1416, all of which may be linked together via a system bus, network, or other connection mechanism 1405.
[148] User interface module 1401 can be operable to send data to and/or receive data from external user input/output devices. For example, user interface module 1401 can be configured to send and/or receive data to and/or from user input devices such as a touch screen, a computer mouse, a keyboard, a keypad, a touch pad, a trackball, a joystick, a voice recognition module, and/or other similar devices. User interface module 1401 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays, light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later
developed. User interface module 1401 can also be configured to generate audible outputs, with devices such as a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices. User interface module 1401 can further be configured with one or more haptic devices that can generate haptic outputs, such as vibrations and/or other outputs detectable by touch and/or physical contact with computing device 1400. In some examples, user interface module 1401 can be used to provide a graphical user interface (GUI) for utilizing computing device 1400, such as, for example, a graphical user interface of a mobile phone device. In some examples, user interface module 1401 can be used to provide a user-adjustable control to receive a threshold distance.
[149] Network communications module 1402 can include one or more devices that provide one or more wireless interface(s) 1407 and/or one or more wireline interface(s) 1408 that are configurable to communicate via a network. Wireless interface(s) 1407 can include one or more wireless transmitters, receivers, and/or transceivers, such as a Bluetooth™ transceiver, a Zigbee® transceiver, a Wi-Fi™ transceiver, a WiMAX™ transceiver, an LTE™ transceiver, and/or other type of wireless transceiver configurable to communicate via a wireless network. Wireline interface(s) 1408 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiberoptic link, or a similar physical connection to a wireline network.
[150] In some examples, network communications module 1402 can be configured to provide reliable, secured, and/or authenticated communications. For each communication described herein, information for facilitating reliable communications (e.g., guaranteed message delivery) can be provided, perhaps as part of a message header and/or footer (e.g., packet/message sequencing information, encapsulation headers and/or footers, size/time information, and transmission verification information such as cyclic redundancy check (CRC) and/or parity check values). Communications can be made secure (e.g., be encoded or encrypted) and/or decry pted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, Data Encryption Standard (DES), Advanced Encryption Standard (AES), a Rivest- Shamir- Adelman (RSA) algorithm, a Diffie-Hellman algorithm, a secure sockets protocol such as Secure Sockets Layer (SSL) or Transport Layer Security (TLS), and/or Digital Signature Algorithm (DSA). Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decry pt/decode) communications.
[151] One or more processors 1403 can include one or more general purpose processors, and/or one or more special purpose processors (e.g., digital signal processors, tensor processing units (TPUs), graphics processing units (GPUs), application specific integrated circuits, etc.). One or more processors 1403 can be configured to execute computer-readable instructions 1306 that are contained in data storage 1404 and/or other instructions as described herein.
[152] Data storage 1404 can include one or more non-transitory computer-readable storage media that can be read and/or accessed by at least one of one or more processors 1403. The one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one of one or more processors 1403. In some examples, data storage 1404 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other examples, data storage 1404 can be implemented using two or more physical devices.
[153] Data storage 1404 can include computer-readable instructions 1406 and perhaps additional data. In some examples, data storage 1404 can include storage required to perform at least part of the herein-described methods, scenarios, and techniques and/or at least part of the functionality of the herein-described devices and networks. In some examples, data storage 1404 can include storage for a trained neural network model 1410 (e.g., a model of trained neural networks such as a vision and language model). In particular of these examples, computer-readable instructions 1406 can include instructions that, when executed by one or more processors 1403, enable computing device 1400 to provide for some or all of the functionality of trained neural network model 1410.
[154] In some examples, computing device 1400 can include one or more camera(s) 1412. Camera(s) 1412 can include one or more image capture devices, such as still and/or video cameras, equipped to capture light and record the captured light in one or more images; that is, camera(s) 1412 can generate image(s) of captured light. The one or more images can be one or more still images and/or one or more images utilized in video imagery. Camera(s) 1412 can capture light and/or electromagnetic radiation emitted as visible light, infrared radiation, ultraviolet light, and/or as one or more other frequencies of light.
[155] In some examples, computing device 1400 can include one or more sensors 1414. Sensors 1414 can be configured to measure conditions within computing device 1400 and/or conditions in an environment of computing device 1400 and provide data about these conditions. For example, sensors 1414 can include one or more of (i) sensors for obtaining data about computing device 1400, such as, but not limited to, a thermometer for measuring a
temperature of computing device 1400, a battery sensor for measuring power of one or more batteries of power system 1416, and/or other sensors measuring conditions of computing device 1400; (ii) an identification sensor to identify other objects and/or devices, such as, but not limited to, a Radio Frequency Identification (RFID) reader, proximity sensor, one-dimensional barcode reader, two-dimensional barcode (e.g., Quick Response (QR) code) reader, and a laser tracker, where the identification sensors can be configured to read identifiers, such as RFID tags, barcodes, QR codes, and/or other devices and/or object configured to be read and provide at least identifying information; (iii) sensors to measure locations and/or movements of computing device 1400, such as, but not limited to, a tilt sensor, a gyroscope, an accelerometer, a Doppler sensor, a GPS device, a sonar sensor, a radar device, a laser-displacement sensor, and a compass; (iv) an environmental sensor to obtain data indicative of an environment of computing device 1400, such as, but not limited to, an infrared sensor, an optical sensor, a light sensor, a biosensor, a capacitive sensor, a touch sensor, a temperature sensor, a wireless sensor, a radio sensor, a movement sensor, a microphone, a sound sensor, an ultrasound sensor and/or a smoke sensor; and/or (v) a force sensor to measure one or more forces (e.g., inertial forces and/or G-forces) acting about computing device 1400, such as, but not limited to one or more sensors that measure: forces in one or more dimensions, torque, ground force, friction, and/or a zero moment point (ZMP) sensor that identifies ZMPs and/or locations of the ZMPs. Many other examples of sensors 1414 are possible as well.
[156] Power system 1416 can include one or more batteries 1418 and/or one or more external power interfaces 1420 for providing electrical power to computing device 1400. Each battery of the one or more batteries 1418 can, when electrically coupled to the computing device 1400, act as a source of stored electrical power for computing device 1400. One or more batteries 1418 of power system 1416 can be configured to be portable. Some or all of one or more batteries 1418 can be readily removable from computing device 1400. In other examples, some or all of one or more batteries 1418 can be internal to computing device 1400, and so may not be readily removable from computing device 1400. Some or all of one or more batteries 1418 can be rechargeable. For example, a rechargeable battery can be recharged via a wired connection between the battery and another power supply, such as by one or more power supplies that are external to computing device 1400 and connected to computing device 1400 via the one or more external power interfaces. In other examples, some or all of one or more batteries 1418 can be non-rechargeable batteries.
[157] One or more external power interfaces 1420 of power system 1416 can include one or more wired-power interfaces, such as a USB cable and/or a power cord, that enable wired
electrical power connections to one or more power supplies that are external to computing device 1400. One or more external power interfaces 1420 can include one or more wireless power interfaces, such as a Qi wireless charger, that enable wireless electrical power connections, such as via a Qi wireless charger, to one or more external power supplies. Once an electrical power connection is established to an external power source using one or more external power interfaces 1420, computing device 1400 can draw electrical power from the external power source the established electrical power connection. In some examples, power system 1416 can include related sensors, such as battery sensors associated with the one or more batteries or other types of electrical power sensors.
Cloud-Based Servers
[158] FIG. 15 depicts a cloud-based server system in accordance with an example embodiment. In FIG. 15, functionality of distance based sound separation, and/or a computing device can be distributed among computing clusters 1509a, 1509b, 1509c. Computing cluster 1509a can include one or more computing devices 1500a, cluster storage arrays 1510a, and cluster routers 1511 connected by a local cluster network 1512a. Similarly, computing cluster 1509b can include one or more computing devices 1500b, cluster storage arrays 1510b, and cluster routers 1511b connected by a local cluster network 1512b. Likewise, computing cluster 1509c can include one or more computing devices 1500c, cluster storage arrays 1510c, and cluster routers 1511c connected by a local cluster network 1512c.
[159] In some embodiments, computing clusters 1509a, 1509b, 1509c can be a single computing device residing in a single computing center. In other embodiments, computing clusters 1509a, 1509b, 1509c can include multiple computing devices in a single computing center, or even multiple computing devices located in multiple computing centers located in diverse geographic locations. For example, FIG. 15 depicts each of computing clusters 1509a, 1509b, 1509c residing in different physical locations.
[160] In some embodiments, data and services at computing clusters 1509a, 1509b, 1509c can be encoded as computer readable information stored in non-transitory, tangible computer readable media (or computer readable storage media) and accessible by other computing devices. In some embodiments, computing clusters 1509a, 1509b, 1509c can be stored on a single disk drive or other tangible storage media, or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations.
[161] In some embodiments, each of computing clusters 1509a, 1509b, and 1509c can have an equal number of computing devices, an equal number of cluster storage arrays, and an equal number of cluster routers. In other embodiments, however, each computing cluster can have
different numbers of computing devices, different numbers of cluster storage arrays, and different numbers of cluster routers. The number of computing devices, cluster storage arrays, and cluster routers in each computing cluster can depend on the computing task or tasks assigned to each computing cluster.
[162] In computing cluster 1509a, for example, computing devices 1500a can be configured to perform various computing tasks of a distance based sound separation neural network, an audio separation network, a distance estimation network, and/or a computing device. In one embodiment, the various functionalities of a neural network, and/or a computing device can be distributed among one or more of computing devices 1500a, 1500b, 1500c. Computing devices 1500b and 1500c in respective computing clusters 1509b and 1509c can be configured similarly to computing devices 1500a in computing cluster 1509a. On the other hand, in some embodiments, computing devices 1500a, 1500b, and 1500c can be configured to perform different functions.
[163] In some embodiments, computing tasks and stored data associated with a distance based sound separation neural network, an audio separation network, a distance estimation network, and/or a computing device can be distributed across computing devices 1500a, 1500b, and 1500c based at least in part on the processing requirements of a neural network, and/or a computing device, the processing capabilities of computing devices 1500a, 1500b, 1500c, the latency of the network links between the computing devices in each computing cluster and between the computing clusters themselves, and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency, and/or other design goals of the overall system architecture.
[164] Cluster storage arrays 1510a, 1510b, 1510c of computing clusters 1509a, 1509b, 1509c can be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives. The disk array controllers, alone or in conjunction with their respective computing devices, can also be configured to manage backup or redundant copies of the data stored in the cluster storage arrays to protect against disk drive or other cluster storage array failures and/or network failures that prevent one or more computing devices from accessing one or more cluster storage arrays.
[165] Similar to the manner in which the functions of a distance based sound separation neural network, an audio separation network, a distance estimation network, and/or a computing device can be distributed across computing devices 1500a, 1500b, 1500c of computing clusters 1509a, 1509b, 1509c, various active portions and/or backup portions of these components can be distributed across cluster storage arrays 1510a, 1510b, 1510c. For example, some cluster
storage arrays can be configured to store one portion of the data of a first layer of a neural network, and/or a computing device, while other cluster storage arrays can store other portion(s) of data of second layer of a neural network, and/or a computing device. Also, for example, some cluster storage arrays can be configured to store the data of an encoder of a neural network, while other cluster storage arrays can store the data of a decoder of a neural network. Additionally, some cluster storage arrays can be configured to store backup versions of data stored in other cluster storage arrays.
[166] Cluster routers 1511a, 1511b, 1511c in computing clusters 1509a, 1509b, 1509c can include networking equipment configured to provide internal and external communications for the computing clusters. For example, cluster routers 1511a in computing cluster 1509a can include one or more internet switching and routing devices configured to provide (i) local area network communications between computing devices 1500a and cluster storage arrays 1510a via local cluster network 1512a, and (ii) wide area network communications between computing cluster 1509a and computing clusters 1509b and 1509c via wide area network link 1513a to network 1306. Cluster routers 1511b and 1511c can include network equipment similar to cluster routers 1511a, and cluster routers 1511b and 1511c can perform similar networking functions for computing clusters 1509b and 1509b that cluster routers 1511a perform for computing cluster 1509a.
[167] In some embodiments, the configuration of cluster routers 1511a, 1511b, 1511c can be based at least in part on the data communication requirements of the computing devices and cluster storage arrays, the data communications capabilities of the network equipment in cluster routers 1511a, 1511b, 1511c, the latency and throughput of local cluster networks 1512a, 112b, 1512c, the latency, throughput, and cost of wide area network links 1513a, 1513b, 1513c, and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency and/or other design criteria of the moderation system architecture.
Example Methods of Operation
[168] FIG. 16 is a flowchart of a method 1600, in accordance with example embodiments. Method 1600 can be executed by a computing device, such as computing device 1400. Method 1600 can begin at block 1610, where the method involves receiving, by an audio input component of a computing device, an audio mixture from one or more sources.
[169] At block 1620, the method involves predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component.
[170] At block 1630, the method involves determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component.
[171] At block 1640, the method involves providing, by the computing device, the predicted one or more near sounds.
[172] In some embodiments, the predicting of the respective distances involves predicting, by a trained audio separation neural network, one or more sounds corresponding to the one or more sources. Such embodiments also involve predicting the respective distances for the predicted one or more sounds.
[173] In some embodiments, the distance estimation neural network may be a convolutional neural network.
[174] In some embodiments, the distance estimation neural network may be a recurrent neural network. In some embodiments, the recurrent neural network may be a Long Short-Term Memory (LSTM) network.
[175] Some embodiments involve the trained distance estimation neural network having been trained to predict the respective distances by determining a plurality of room impulse responses (RIRs) with an image method room simulator. Such embodiments also involve determining, based on the plurality of RIRs, the direct-to-reverb eration ratio (DRR). In some embodiments, the determining of the plurality of RIRs may be performed with frequency-dependent wall filters.
[176] In some embodiments, the predicting of the respective distances involves separating human speech from ambient noise. In some embodiments, the separating of the human speech from the ambient noise may be performed by a TasNet model.
[177] In some embodiments, the separating of the human speech from the ambient noise involves determining a time-frequency mask. Such embodiments also involve applying the time-frequency mask to the audio mixture to predict a signal comprising the human speech.
[178] Some embodiments involve providing, by an interactive graphical user interface of the computing device, a user-adjustable control to receive the threshold distance.
[179] Some embodiments involve receiving, by the user-adjustable control, a particular threshold distance input by a user of the computing device, and wherein the providing of the predicted one or more near sounds and the one or more far sounds is based on the particular threshold distance.
[180] In some embodiments, the providing of the predicted one or more near sounds involves suppressing, by the computing device, at least one of the one or more far sounds.
[181] In some embodiments, the providing of the predicted one or more near sounds involves enhancing, by the computing device, at least one of the one or more near sounds.
[182] Some embodiments involve providing, by the computing device, the predicted one or more far sounds.
[183] Some embodiments involve determining, by the computing device, a request to predict the one or more near sounds and the one or more far sounds. Such embodiments involve sending the request to predict the one or more near sounds and the one or more far sounds from the computing device to a second computing device, the second computing device comprising a trained version of the neural network. Such embodiments also involve, after sending the request, the computing device receiving, from the second computing device, the predicted one or more near sounds and the one or more far sounds, and the providing of the predicted one or more near sounds and the one or more far sounds involves providing the predicted one or more near sounds and the one or more far sounds as received from the second computing device.
[184] Some embodiments involve obtaining a trained neural network at the computing device, and the predicting of the one or more near sounds and the one or more far sounds involves predicting by the computing device using the trained neural network.
[185] FIG. 17 is a flowchart of a method 1700, in accordance with example embodiments. Method 1700 can be executed by a computing device, such as computing device 1400. Method 1700 can begin at block 1710, where the method involves receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator.
[186] At block 1720, the method involves training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component.
[187] At block 1730, the method involves providing, by the computing device, the trained neural network.
[188] In some embodiments, the acoustic simulator may be an image-method room simulator with frequency dependent wall filters. In some embodiments, the generating of the audio
involves reproducing, by the acoustic simulator, one or more acoustic properties of component sounds in a given audio mixture of the plurality of audio mixtures.
[189] Some embodiments involve generating, by the acoustic simulator, one or more reverb impulse responses (RIRs) for a room, wherein the room is associated with a plurality of acoustic properties. In some embodiments, a location of the audio input component in the room may be randomized. In some embodiments, a location of at least one sound source in the room may be randomized.
[190] Some embodiments involve training an audio separation neural network to separate a given audio mixture of the plurality of audio mixtures into one or more sounds corresponding to one or more sources in the given audio mixture. Such embodiments involve applying the distance estimation neural network to estimate a distance of the one or more sources from the audio input component. The determining of the one or more near sounds and the one or more far sounds may be based on the estimated distance.
[191] In some embodiments, the training of the neural network may be performed at the computing device.
[192] The present disclosure is not to be limited in terms of the particular embodiments described in this application, which are intended as illustrations of various aspects. Many modifications and variations can be made without departing from its spirit and scope, as will be apparent to those skilled in the art. Functionally equivalent methods and apparatuses within the scope of the disclosure, in addition to those enumerated herein, will be apparent to those skilled in the art from the foregoing descriptions. Such modifications and variations are intended to fall within the scope of the appended claims.
[193] The above detailed description describes various features and functions of the disclosed systems, devices, and methods with reference to the accompanying figures. In the figures, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, figures, and claims are not meant to be limiting. Other embodiments can be utilized, and other changes can be made, without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.
[194] With respect to any or all of the ladder diagrams, scenarios, and flow charts in the figures and as discussed herein, each block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments.
Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments, for example, functions described as blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved. Further, more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
[195] A block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique. Alternatively or additionally, a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data). The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and/or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage medium.
[196] The computer readable medium may also include non-transitory computer readable media such as non-transitory computer-readable media that stores data for short periods of time like register memory, processor cache, and random access memory (RAM). The computer readable media may also include non-transitory computer readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example. The computer readable media may also be any other volatile or nonvolatile storage systems. A computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
[197] Moreover, a block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
[198] While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are provided for explanatory purposes and are not intended to be limiting, with the true scope being indicated by the following claims.
Claims
1. A computer-implemented method of applying a trained neural network for sound separation based on distance estimation, comprising: receiving, by an audio input component of a computing device, an audio mixture from one or more sources; predicting, by a trained distance estimation neural network and based on the audio mixture, respective distances of the one or more sources from the audio input component; determining one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the predicted one or more near sounds.
2. The computer-implemented method of claim 1, wherein the predicting of the respective distances further comprises: predicting, by a trained audio separation neural network, one or more sounds corresponding to the one or more sources; and predicting the respective distances for the predicted one or more sounds.
3. The computer-implemented method of claim 1, wherein the distance estimation neural network is a convolutional neural network.
4. The computer-implemented method of claim 1, wherein the distance estimation neural network is a recurrent neural network.
5. The computer-implemented method of claim 4, wherein the recurrent neural network is a Long Short-Term Memory (LSTM) network.
6. The computer-implemented method of claim 1, the trained distance estimation neural network having been trained to predict the respective distances by:
determining a plurality of room impulse responses (RIRs) with an image method room simulator; and determining, based on the plurality of RIRs, the direct-to-reverberation ratio (DRR).
7. The computer-implemented method of claim 6, wherein the determining of the plurality of RIRs is performed with frequency-dependent wall filters.
8. The computer-implemented method of claim 1, wherein the predicting of the respective distances further comprises: separating human speech from ambient noise.
9. The computer-implemented method of claim 8, wherein the separating of the human speech from the ambient noise is performed by a TasNet model.
10. The computer-implemented method of claim 8, wherein the separating of the human speech from the ambient noise comprises: determining a time-frequency mask; and applying the time-frequency mask to the audio mixture to predict a signal comprising the human speech.
11. The computer-implemented method of claim 1, further comprising: providing, by an interactive graphical user interface of the computing device, a user- adjustable control to receive the threshold distance.
12. The computer-implemented method of claim 11, further comprising: receiving, by the user-adjustable control, a particular threshold distance input by a user of the computing device, and wherein the providing of the predicted one or more near sounds and the one or more far sounds is based on the particular threshold distance.
13. The computer-implemented method of claim 1, wherein the providing of the predicted one or more near sounds further comprises: suppressing, by the computing device, at least one of the one or more far sounds.
14. The computer-implemented method of claim 1, wherein the providing of the predicted one or more near sounds further comprises: enhancing, by the computing device, at least one of the one or more near sounds.
15. The computer-implemented method of claim 1, further comprising: providing, by the computing device, the predicted one or more far sounds.
16. The computer-implemented method of claim 1, further comprising: determining, by the computing device, a request to predict the one or more near sounds and the one or more far sounds; sending the request to predict the one or more near sounds and the one or more far sounds from the computing device to a second computing device, the second computing device comprising a trained version of the neural network; after sending the request, the computing device receiving, from the second computing device, the predicted one or more near sounds and the one or more far sounds, and wherein the providing of the predicted one or more near sounds and the one or more far sounds comprises providing the predicted one or more near sounds and the one or more far sounds as received from the second computing device.
17. The computer-implemented method of claim 1, further comprising: obtaining a trained neural network at the computing device, and wherein the predicting of the one or more near sounds and the one or more far sounds comprises predicting by the computing device using the trained neural network.
18. A computer-implemented method for sound separation based on distance estimation, comprising: receiving, by an audio input component of a computing device, training data comprising a plurality of audio mixtures, wherein each audio mixture comprises audio generated by an acoustic simulator; training, based on the training data and for an input audio mixture, a distance estimation neural network to: predict respective distances of one or more sources in the input audio mixture from the audio input component, and
determine one or more near sounds and one or more far sounds based on the respective distances, wherein the one or more near sounds correspond to sources that are located within a threshold distance of the audio input component, and the one or more far sounds correspond to sources that are not located within the threshold distance of the audio input component; and providing, by the computing device, the trained neural network.
19. The computer-implemented method of claim 18, wherein the acoustic simulator is an image-method room simulator with frequency dependent wall filters.
20. The computer-implemented method of claim 18, wherein the generating of the audio comprises: reproducing, by the acoustic simulator, one or more acoustic properties of component sounds in a given audio mixture of the plurality of audio mixtures.
21. The computer-implemented method of claim 18, further comprising: generating, by the acoustic simulator, one or more reverb impulse responses (RIRs) for a room, wherein the room is associated with a plurality of acoustic properties.
22. The computer-implemented method of claim 21 , wherein a location of the audio input component in the room is randomized.
23. The computer-implemented method of claim 21, wherein a location of at least one sound source in the room is randomized.
24. The computer-implemented method of claim 18, further comprising: training an audio separation neural network to separate a given audio mixture of the plurality of audio mixtures into one or more sounds corresponding to one or more sources in the given audio mixture; applying the distance estimation neural network to estimate a distance of the one or more sources from the audio input component, and wherein the determining of the one or more near sounds and the one or more far sounds is based on the estimated distance.
25. The computer-implemented method of claim 18, wherein the training of the neural network is performed at the computing device.
26. A computing device, comprising: one or more processors; and data storage, wherein the data storage has stored thereon computer-executable instructions that, when executed by the one or more processors, cause the computing device to carry out functions comprising the computer-implemented method of any one of claims 1-24.
27. The computing device of claim 26, wherein the computing device is a mobile phone.
28. A computer program comprising instructions that, when executed by a computer, cause the computer to perform steps in accordance with the method of any one of claims 1-24.
29. An article of manufacture comprising one or more non-transitory computer readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to carry out functions that comprise the computer-implemented method of any one of claims 1-24.
30. A system, comprising: means for carrying out the computer-implemented method of any one of claims 1-24.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202263367350P | 2022-06-30 | 2022-06-30 | |
US63/367,350 | 2022-06-30 |
Publications (1)
Publication Number | Publication Date |
---|---|
WO2024006516A1 true WO2024006516A1 (en) | 2024-01-04 |
Family
ID=87429193
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/US2023/026708 WO2024006516A1 (en) | 2022-06-30 | 2023-06-30 | Sound separation based on distance estimation using machine learning models |
PCT/US2023/026706 WO2024006514A1 (en) | 2022-06-30 | 2023-06-30 | Distance based sound separation using machine learning models |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/US2023/026706 WO2024006514A1 (en) | 2022-06-30 | 2023-06-30 | Distance based sound separation using machine learning models |
Country Status (1)
Country | Link |
---|---|
WO (2) | WO2024006516A1 (en) |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210219048A1 (en) * | 2018-06-07 | 2021-07-15 | Nippon Telegraph And Telephone Corporation | Acoustic signal separation apparatus, learning apparatus, method, and program thereof |
-
2023
- 2023-06-30 WO PCT/US2023/026708 patent/WO2024006516A1/en unknown
- 2023-06-30 WO PCT/US2023/026706 patent/WO2024006514A1/en unknown
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210219048A1 (en) * | 2018-06-07 | 2021-07-15 | Nippon Telegraph And Telephone Corporation | Acoustic signal separation apparatus, learning apparatus, method, and program thereof |
Non-Patent Citations (2)
Title |
---|
KATHARINE PATTERSON ET AL: "Distance-Based Sound Separation", ARXIV.ORG, CORNELL UNIVERSITY LIBRARY, 201 OLIN LIBRARY CORNELL UNIVERSITY ITHACA, NY 14853, 1 July 2022 (2022-07-01), XP091261795 * |
TOBIAS GBURREK ET AL: "Deep Neural Network based Distance Estimation for Geometry Calibration in Acoustic Sensor Networks", ARXIV.ORG, CORNELL UNIVERSITY LIBRARY, 201 OLIN LIBRARY CORNELL UNIVERSITY ITHACA, NY 14853, 24 June 2020 (2020-06-24), XP081709319 * |
Also Published As
Publication number | Publication date |
---|---|
WO2024006514A1 (en) | 2024-01-04 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110178178B (en) | Microphone selection and multiple speaker segmentation with ambient Automatic Speech Recognition (ASR) | |
Avila et al. | Feature pooling of modulation spectrum features for improved speech emotion recognition in the wild | |
US11812254B2 (en) | Generating scene-aware audio using a neural network-based acoustic analysis | |
Leng et al. | Binauralgrad: A two-stage conditional diffusion probabilistic model for binaural audio synthesis | |
Majumder et al. | Few-shot audio-visual learning of environment acoustics | |
WO2022218134A1 (en) | Multi-channel speech detection system and method | |
US11790930B2 (en) | Method and system for dereverberation of speech signals | |
Ick et al. | Blind acoustic room parameter estimation using phase features | |
Ding et al. | Joint estimation of binaural distance and azimuth by exploiting deep neural networks | |
WO2024006516A1 (en) | Sound separation based on distance estimation using machine learning models | |
Nagaraja et al. | VoIPLoc: passive VoIP call provenance via acoustic side-channels | |
Barber et al. | End-to-end Alexa device arbitration | |
Nidadavolu et al. | Single channel far field feature enhancement for speaker verification in the wild | |
Gburrek et al. | On source-microphone distance estimation using convolutional recurrent neural networks | |
Qi et al. | Sparsity-Constrained Weight Mapping for Head-Related Transfer Functions Individualization from Anthropometric Features. | |
US11756570B2 (en) | Audio-visual separation of on-screen sounds based on machine learning models | |
Gomez et al. | Speaker Diarization and Identification From Single Channel Classroom Audio Recordings Using Virtual Microphones | |
Park et al. | Multistream diarization fusion using the minimum variance Bayesian information criterion | |
Manocha et al. | Nord: Non-matching reference based relative depth estimation from binaural speech | |
US20230296767A1 (en) | Acoustic-environment mismatch and proximity detection with a novel set of acoustic relative features and adaptive filtering | |
US20240087593A1 (en) | System and Method for Acoustic Channel Identification-based Data Verification | |
US20230419989A1 (en) | Conditioned Separation of Arbitrary Sounds based on Machine Learning Models | |
US20220343917A1 (en) | Scene-aware far-field automatic speech recognition | |
Al-Karawi et al. | The effects of distance and reverberation time on speaker recognition performance | |
Nespoli et al. | Relative Acoustic Features for Distance Estimation in Smart-Homes |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
121 | Ep: the epo has been informed by wipo that ep was designated in this application |
Ref document number: 23744974Country of ref document: EPKind code of ref document: A1 |