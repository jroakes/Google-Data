CN114270434A - Two-pass end-to-end speech recognition - Google Patents
Two-pass end-to-end speech recognition Download PDFInfo
- Publication number
- CN114270434A CN114270434A CN202080040756.2A CN202080040756A CN114270434A CN 114270434 A CN114270434 A CN 114270434A CN 202080040756 A CN202080040756 A CN 202080040756A CN 114270434 A CN114270434 A CN 114270434A
- Authority
- CN
- China
- Prior art keywords
- training
- output
- rnn
- encoder
- las
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/04—Segmentation; Word boundary detection
- G10L15/05—Word boundary detection
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
- G10L2015/0635—Training updating or merging of old and new templates; Mean values; Weighting
Abstract
A two-pass Automatic Speech Recognition (ASR) model can be used to perform ASR on a streaming device to generate a textual representation of an utterance captured in audio data. Various embodiments include a first pass portion of an ASR model for generating streaming candidate recognition(s) of utterances captured in audio data. For example, the first pass portion can include a recurrent neural network transformer (RNN-T) decoder. Various embodiments include a second pass portion of the ASR model for revising the streaming candidate recognition(s) of the utterance and generating a text representation of the utterance. For example, the second pass portion can include a listen-to-spell (LAS) decoder. Various embodiments include a shared encoder shared between an RNN-T decoder and an LAS decoder.
Description
Background
Automated assistants (also referred to as "personal assistants," "mobile assistants," etc.) may be interacted with by users via various client devices, such as smartphones, tablet computers, wearable devices, automotive systems, standalone personal assistant devices, etc. The automated assistant receives input from the user that includes spoken natural language input (i.e., an utterance) and may respond by performing an action, by controlling another device, and/or providing responsive content (e.g., visual and/or audible natural language output). Automated assistants that interact with them via the client device may be implemented via the client device itself and/or via one or more remote computing devices (e.g., computing devices in the cloud) in network communication with the client device.
The automated assistant can convert audio data corresponding to a spoken utterance of the user into corresponding text (or other semantic representation). For example, audio data can be generated based on detection of a user's spoken language via one or more microphones of a client device that includes an automated assistant. The automatic assistant can include a speech recognition engine that attempts to recognize various characteristics of the spoken utterance captured in the audio data, such as the sounds (e.g., phonemes) produced by the spoken utterance, the order in which the sounds were uttered, the prosody, intonation, and so forth of the speech. In addition, the speech recognition engine can identify text words or phrases represented by these characteristics. The text can then be further processed by the automated assistant (e.g., using a natural language understanding engine and/or a dialog state engine) to determine the response content of the spoken utterance. The speech recognition engine can be implemented by the client device and/or by one or more automated assistant components that are remote from the client device but in network communication with the client device.
Disclosure of Invention
The techniques described herein are directed to generating a textual representation of a spoken utterance using an end-to-end (E2E) two-pass Automatic Speech Recognition (ASR) model. The two-pass ASR model includes a streaming first-pass portion and a non-streaming second-pass portion. In many embodiments, the first pass portion includes a recurrent neural network transformer (RNN-T) decoder and is capable of generating candidate text representation(s) of the spoken utterance in a streaming manner. In various implementations, a non-streaming second pass portion (e.g., a portion including a listen-to-spell (LAS) decoder) can be used to improve the initial candidate text representation(s) generated in the first pass portion. For example, the LAS decoder can be used to re-rank candidate textual representations generated using the RNN-T decoder. In many implementations, the two-pass ASR model can include a shared encoder, where the RNN-T decoder and the LAS decoder share the shared encoder. The use of a shared encoder can reduce the model size of the two-pass ASR model and/or can provide improved computational efficiency compared to, for example, using a dedicated encoder for the RNN-T decoder and a dedicated encoder for the LAS decoder. In other words, when utilizing a two-pass ASR model in generating a textual representation of a spoken utterance, the use of a shared encoder enables efficient utilization of memory and/or computing resources. This protection of memory and/or computing resources may be particularly influential when the two-pass ASR model is stored and utilized by client devices that typically have limited memory and/or computing resources. For example, the use of a shared encoder can enable on-device ASR to be performed on client device(s) whose limited resources can prevent, at least in some situations such as low-battery situations, ASR on devices that use other model(s).
As an example, the client device can use one or more microphones of the client device to capture "turn on the living room lights" spoken utterances. Spoken utterances of "turn on the lighting room lights" can be processed in a streaming manner using a shared encoder to generate shared encoder output, and the shared encoder output can be processed using an RNN-T decoder to generate a first pass candidate text representation of the streaming(s) of "turn on the lighting room lights". In response to determining that the user has finished speaking, the first pass candidate text representation(s) and the shared encoder output can be processed using an LAS decoder to generate a text representation of "turn on the moving room lights".
In some embodiments, a two-pass model can be trained using a combined loss function that includes both RNN-T loss and LAS loss. In some of those embodiments, the training process can include: (1) training an encoder and an RNN-T decoder; (2) freezing the encoder trained in (1) and using the frozen encoder in training the LAS decoder; and (3) training the shared encoder, the RNN-T decoder, and the LAS decoder simultaneously using the combined loss function. In some versions of those embodiments, the LAS decoder can additionally be trained using a Minimum Word Error Rate (MWER) training process.
In some implementations, the two-pass ASR model can also include additional encoders. For example, the shared encoder output generated using the shared encoder can be processed using an additional encoder to generate an additional encoder output, wherein the additional encoder adapts the shared encoder output to be more suitable for the LAS. The additional encoder output can be processed in a second pass using the LAS decoder instead of the shared encoder output. Latency (e.g., delay between user speaking and generation of the result) can be reduced when the LAS decoder processes additional encoder outputs as compared to when the LAS decoder processes shared encoder outputs.
Some embodiments disclosed herein address reducing endpoint latency. Endpointing generally refers to a process of determining when a spoken utterance is complete. The endpointer latency is the amount of time between when the spoken utterance is actually completed and when the endpointing process determines that the spoken utterance is actually completed. High-end-point latency can create delays in generating responses to spoken utterances. Further, with the two-pass ASR model described herein, the non-streaming second-pass portion may, because it is non-streaming, process the corresponding data until the end-point indication indicates that the spoken utterance is complete. Thus, mitigating the terminator latency enables the second pass portion to be utilized more quickly, thereby enabling the final text representation to be determined with reduced latency. In various implementations, the endpointer latency can be reduced by training a two-pass ASR model (e.g., the RNN-T decoder portion of the ASR model) to predict a query-ending symbol, such as </s >, and determining that a spoken utterance is complete when the query-ending symbol is predicted. In some embodiments, a training penalty can be introduced for issuing query-ending symbols too early or too late in training RNN-T. Accordingly, embodiments disclosed herein are capable of training the RNN-T to predict a query end symbol with reduced latency and use the query end symbol as an indication of an endpoint, thereby mitigating the endpointer latency. Furthermore, training the RNN-T predicted query end symbols can alleviate the need to use a separate end-pointing model. Using a separate endpoint indication model may require the use of additional computing resources (e.g., memory resources and/or processor resources).
Accordingly, various embodiments provide techniques for using a two-pass ASR model in streaming ASR on a device. For example, conventional ASR systems can require capturing audio data at a client device, transmitting the audio data and/or a representation of the audio data to a remote server, processing the audio data and/or the representation of the audio data at the remote server to generate a textual representation of an utterance captured in the audio data, and transmitting the textual representation of the utterance back to the client device. In contrast, on-device ASR systems generate a textual representation of an utterance locally at a client device without transmitting data to or from a remote server. In many embodiments, the use of on-device ASR including a two-pass ASR model can conserve computing resources (e.g., battery power, processor cycles, memory, etc.) by removing the need to transmit data to and receive data from a remote server when compared to conventional ASR. Additionally, on-device ASR can provide user privacy advantages by processing audio data to locally generate text representations without transmitting the audio data to a remote server. Furthermore, the on-device ASR system provides improved reliability over conventional ASR models. For example, the on-device ASR system can generate a textual representation of the audio data when a network connection for transmitting the audio data to a remote server is unavailable, such as when a wireless network fails. In contrast, conventional ASR systems are unable to generate a textual representation of audio data when a network connection for transmitting data to and from a remote server is unavailable.
It should be understood that all combinations of the foregoing concepts and further concepts described in greater detail herein are considered a part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
Fig. 1 illustrates an example of generating a textual representation of an utterance, in accordance with various embodiments disclosed herein.
Fig. 2 illustrates an example of generating a textual representation of an utterance using a two-pass ASR model in accordance with various embodiments disclosed herein.
FIG. 3 illustrates a block diagram of an example environment in which embodiments disclosed herein may be implemented.
FIG. 4 is a flow diagram illustrating an example process of generating a training instance for training a two-pass ASR model in accordance with various embodiments disclosed herein.
FIG. 5 is a flow diagram illustrating an example process of training the first pass portion of a two-pass ASR model in accordance with embodiments disclosed herein.
FIG. 6 is a flow diagram illustrating an example process of training a second pass portion of a two-pass ASR model in accordance with embodiments disclosed herein.
FIG. 7 is a flow diagram illustrating an example process of training a two-pass ASR model in accordance with embodiments disclosed herein.
FIG. 8 is a flow diagram illustrating an example process for generating a textual representation of an utterance using a two-pass ASR model in accordance with an embodiment disclosed herein.
FIG. 9 illustrates a block diagram of another example environment in which embodiments disclosed herein may be implemented.
Fig. 10 illustrates an example architecture of a computing device.
Detailed Description
The requirements for many applications of state-of-the-art speech recognition systems can include not only low Word Error Rate (WER) but also low latency. In particular, for many use cases, the system must be able to decode the dialogs in a streaming manner and faster than real-time. Recently, the streaming recurrent neural network transducer (RNN-T) end-to-end (E2E) model has been shown to be a good candidate for on-device speech recognition, with improved WER and latency metrics compared to conventional on-device models. However, this model can still lag behind the large state-of-the-art conventional model in quality. On the other hand, the non-streaming E2E listen, join, and spelling (LAS) model has shown comparable quality to the large conventional model. The techniques described herein bring the quality of the E2E streaming model closer to that of conventional systems by incorporating an LAS network as a second pass component while still complying with latency constraints.
The E2E model for speech recognition combines Acoustic Models (AM), Pronunciation Models (PM), and Language Models (LM) into a single network, and has shown competitive results compared to conventional ASR systems with separate AM, PM, and LM. The E2E models are particularly attractive for on-device ASR because they can outperform conventional models on comparable size devices.
Running ASR on a device with direct user interaction presents many challenges in many implementations. First, the recognition result must be streamed. I.e. the word should appear on the screen as soon as it is spoken. Second, the model must have a small time delay (i.e., the delay between the user speaking and the text appearing) to run on the mobile device in real time or faster than real time. Third, the model must be able to leverage user context (e.g., a list of contacts, song titles, etc.) to improve recognition accuracy. The RNN-T E2E model is able to satisfy these constraints.
Non-streaming E2E models, such as listen, join, and spell (LAS), have shown competitive performance with large conventional models. However, LAS models are not streaming because they must participate in the entire audio segment, making their use in interactive applications challenging.
In two-pass decoding, the second-pass model is typically used to improve the initial output from the first-pass model by using lattice rescoring or n-best re-rankings. Keeping the user perceived delay low while obtaining quality gain is a major challenge in case of applying the second pass model. Language model rescoring is typically used for multi-pass decoding, but has recently been used with LAS models to rescore hypotheses from the first pass conventional model. An LAS decoder that takes acoustic information from the encoder and language model information from previous predictions can be considered absolutely more powerful than the second pass language model. Thus, the techniques described herein explore the use of the LAS model for the second pass processing.
In particular, a two-pass architecture is explored in which the RNN-T decoder and the LAS decoder share the encoder network. The shared encoder allows for reduced model size and computational cost compared to having a dedicated encoder for the RNN-T decoder and having a dedicated encoder for the LAS decoder that is fundamentally different. During the inference, the RNN-T decoder generates a streaming prediction, while the LAS decoder finally determines the prediction. Various embodiments explore the trade-off by running the LAS decoder as a beam search as opposed to re-scoring the hypotheses generated by the RNN-T decoder. In some embodiments, computational cost can be reduced by running the first pass RNN-T model on adaptive beams and pruning the first pass before rescoring.
Many embodiments can be trained with accented speech to make the model more robust to different pronunciations. In addition, given the increased amount of training data, varying learned rate schedules are explored. In terms of time delay, the use of the RNN-T model issued full stop decisions to turn off the microphone was explored, and various optimizations were also introduced to improve the speed of LAS re-scoring.
In many embodiments, on-device ASR means that instead of streaming audio from the device to the server, the text is recognized on the server and then the results are streamed back to the device, the recognition being performed entirely on the device. This has important implications for reliability, privacy and latency.
Running the ASR model on the device presents many additional user interaction constraints. First, the recognition result should be streamed; the recognized words should appear on the screen as they are spoken. Second, the latency (i.e., the delay between when the user stops speaking and when the hypothesis is finally determined) must be low. RNN-T models that meet these on-device constraints have been shown in recent studies to be competitive in quality. However, under low latency constraints, they can lag conventional server side-stream ASR systems. On the other hand, non-streaming models such as LAS have been shown to outperform conventional ASR systems. However, LAS models are not streamed as they must participate in the entire audio segment.
In various embodiments, the RNN-T + LAS ASR model can be extended to develop an on-device E2E model that outperforms conventional models in both WER and latency. First, in terms of quality, the model can be trained on multi-domain audio-text utterance pairs using sources from different domains (e.g., including search traffic, telephony data, video data, and/or additional domain data). This not only increases the acoustic diversity, but also increases the vocabulary seen by the E2E model, since the E2E model is trained only on audio-text pairs that are a small fraction compared to the plain text LM data used by conventional models. Because transcription and audio characteristics vary between domains, the domain ID in many implementations can be added as input to the model. Training with multi-domain data and feeding into the domain IDs can improve on models trained only on speech search data. Second, also in terms of quality, the model can be trained to improve robustness to different pronunciations. Conventional models handle this by using a dictionary that may have multiple pronunciations for a word. Since the E2E model can directly predict terms, different pronunciations can be addressed by including accent english data from different regions. Third, given the increasing audio-text pairs used in training, it is explored to use a constant learning rate rather than gradually attenuating the learning rate, giving training samples uniform weight as training progresses.
Embodiments disclosed herein can improve latency of the E2E ASR model. An Endpoint (EP) latency can be generally defined as the amount of time it takes for a microphone to effectively "turn off" after a user stops speaking (e.g., the microphone may not actually turn off but instead may consider the current utterance being processed to be complete). Typically, an external Voice Activity Detector (VAD) is used to make the microphone off decision. For conventional ASR systems, a query termination (EOQ) endpointer is typically used to obtain improved EP latency. Integrating EOQ end-point into the E2E model by predicting query end symbols (e.g., </s >) to help turn off the microphone can improve latency. In many embodiments, a penalty in RNN-T training can be introduced for issuing query end symbols (e.g., </s >) too early or too late. Second, the computational latency of the second pass re-scoring model can be improved. In particular, the second pass run time of the LAS can be reduced by batch reasoning over multiple arcs of re-scored bins and offloading portions of the computation to the first pass as well. LAS re-scoring also enables a better tradeoff between WER and EP latency due to improved recognition quality.
A two-pass E2E architecture in accordance with many embodiments is illustrated in fig. 2.An input acoustic frame can be represented as x ═ x1...xTWherein
In several implementations, the E2E model can be trained using only audio-text pairs, which are a small portion of the data compared to the trillions of plain text data used to train conventional LMs. Previous work only used search utterances. To increase the vocabulary and diversity of the training data, the use of more training data can be explored by incorporating multi-domain utterances. These multi-domain utterances span the search, far-field, telephone, video domain, and/or additional speech domains. In many embodiments, the data set is anonymized and transcribed by hand. Additionally or alternatively, the transcribed utterance can be completed in a semi-supervised manner.
One of the problems with using multi-domain data is that each domain has a different transcription convention. For example, search data has a number in the written domain (e.g., $100), while other queries are often in the spoken domain (e.g., one hundred dollars). Another problem is with multiple speakers. The search query contains only one speaker per utterance, whereas other queries contain multiple speakers. Since the goal of some embodiments is to improve the quality of search queries, it was explored to feed the domain id, which is one of the domains, as a one-hot vector to the E2E model. In many embodiments, it is sufficient to feed only the domain-id to the shared encoder.
Conventional ASR systems are capable of operating on a phonetic representation of a word. In particular, the lexicon maps each word in the vocabulary to several pronunciations represented as a sequence of phonemes, and this mapping is fixed prior to training. This presents a challenge when accents are present. For example, due to voice variations, it is challenging to construct an accurate english recognizer for the united states, australia, uk, canada, india and irish english variants.
Attempts to solve these problems by merging phone sets are difficult. Using a dictionary with an on-device E2E system significantly increases memory footprint because the size of the dictionary can be very large (e.g., over 0.5 GB). In addition, the increased number of phonemes causes confusion and creates a data sparseness problem. Finally, the decision regarding the phone set and pronunciation of a word is not made directly from the data.
Rather, embodiments disclosed herein directly predict terms. The model itself decides how to handle pronunciation and speech changes based on the data. The size of the variants is fixed regardless of the number of variants. As a simple strategy for improving robustness to different accents, the same data as described can be used to explore including additional training data from different english accent regions. For example, data from australia, new zealand, uk, irish, india, kenya, nigeria and south africa can be utilized. The data from these regions can be scaled down by a factor of, for example, 0.125 during training. This number is empirically chosen as the maximum value that does not degrade performance on the U.S. english collection.
Spelling conventions vary from one variation of english to another. Since the training data is transcribed using the spelling convention of the region, using the original transcription can potentially cause unnecessary confusion during training. For example, the E2E model may attempt to learn to detect accents in order to decide which spelling convention to use, thereby degrading robustness. Instead, in some embodiments, VarCon can be used to convert the transcription into an american spelling convention. For each word in the target, a many-to-one mapping of VarCon can be used for the conversion, and then the converted sentence is targeted. In addition, all reference transcriptions can also be converted to American spelling during reasoning when evaluating the accent test set.
In many embodiments, an exponentially decaying learning rate can be used in training both RNN-T and LAS. Given the increased amount of multi-domain training data compared to searching only data, various embodiments use a constant learning rate. To help model convergence, an Exponential Moving Average (EMA) of weights can be maintained during training and the EMA weights can be used for evaluation.
External Voice Activity Detector (VAD) based endpointing devices are typically used to detect speech and filter out non-speech. Whenever the VAD observes speech, then follows a fixed interval of silence, it declares the query to be over (EOQ). An EOQ-based endpoint that directly predicts query end symbols (e.g., </s >) can improve latency. In many embodiments, EOQ detectors can also be folded into the E2E system for joint end-pointing and recognition by introducing </s > tags into the training target vocabulary of the RNN-T model. During beam search decoding, </s > are special symbols that signal that the microphone should be turned off. Premature prediction of </s > causes deletion errors, whereas late prediction increases latency.
In several embodiments, the method can be expandedDevelop a federated RNN-T end-point (EP) model and enable discovery of a peer-to-peer network</s>The tag imposes additional early and late penalties to solve the above problem. Specifically, for x ═ x1，...，xTEvery input frame and every label y ═ y in }1，...，yTDuring training, RNN-T calculates the UXT matrix P used in the training loss calculationRNN-T(y | x). Here the label yUIs that</s>The last tag t in the sequence can be assigned</s>Expressed as the frame index after the last non-silent phoneme resulting from the forced alignment of the audio with the conventional model. RNN-T logarithmic probability RRNN-T(yU| x) is modified to include a prediction of early or late at each time step t</s>Penalty of (2). t is tbufferGiven at reference t</s>A grace period thereafter, before applying the late penalty, however alphaearlyAnd alphalateScales for early and late penalties, respectively. All hyper-parameters can optionally be tuned experimentally.
log PRNN-T(yu|xt)+max(0，αearly*(t</s>-t))+max(0，αlate*(t-t</s>-tbuffer)) (1)
In various embodiments, the RNN-T model is trained on a mixture of data from different domains. This presents a challenge for the endpointer model, as different applications may require different endpointing behavior. It is advisable to aggressively indicate endpoints for queries like short searches, but can cause deletions for long transcription tasks like video. Since some embodiments aim to improve the latency of a search query, it is possible to use the fed domain-id to only add </s > tags to the search query, which addresses latency in the search query while not affecting other domains.
In several embodiments, instead of re-scoring the n-best list, LAS re-scoring can be applied to tree-based lattices for efficiency, as it avoids duplicate computation of common prefixes between candidate sequences. When each trellis branch is extended for re-scoring, batch reasoning with arcs can be used to further reduce LAS latency, as it more efficiently utilizes matrix-matrix multiplication. Furthermore, the second pass latency can be reduced by streaming off the computations of the additional encoder and attention source keys and values to the first pass, the output of which is buffered to be used in the second pass.
As one particular example of model parameters and/or training parameters, in some implementations, the RNN-T + LAS model can be trained using a 128-dimensional logarithmic Mel feature front end. The features are computed using a 32 millisecond window with 10 millisecond hops. Features from 4 consecutive frames are stacked to form a 512-dimensional input representation, which is further sub-sampled by a factor of 3 and passed to the model. The LSTM layer in the model is unidirectional, with 2,048 cells and a projection layer with 640 cells. The shared encoder can include 8 LSTM layers with a temporal reduction layer after the second layer. The RNN-T decoder comprises a prediction network with 2 LSTM layers and a joint network with a single feed forward layer with 640 units. The additional LAS-specific encoder includes 2 LSTM layers. The LAS decoder includes a multi-headed attention with 4 heads of attention, which is fed to 2 LSTM layers. Both decoders can be trained to predict 4,096 entries.
Continuing with this particular example, the RNN-T model can have a large number of parameters, such as 120M parameters. The additional encoder and LAS decoder can also have a large number of parameters, but optionally have fewer parameters than the RNN-T model (e.g., the additional encoder and LAS decoder can have 57M parameters). All parameters can be quantized to 8-bit fixed points. The total model size in memory/disk can be, for example, 177 MB.
In addition to different training sets, multi-conditional training (MTR) and random data downsampling to 8kHz can also be used to further increase data diversity. The noise data can be generated with a signal-to-noise ratio (SNR) from 0 to 30dB, where the average SNR is 12dB, and where T60 time ranges from 0 milliseconds to 900 milliseconds, averaging 500 milliseconds. Noise segments can be sampled from videos and/or noisy ambient recordings of everyday life. Both an 8kHz version and a 16kHz version of the data can be generated, each with equal probability, so that the model becomes robust to different sampling rates.
Turning now to the figures, FIG. 1 illustrates an example of a two-pass ASR process in accordance with many embodiments. Fig. 1 includes a horizontal axis 100 representing time and includes time points 102, 104, 106, and 108. A first point in time 102 indicates where the user started speaking and is followed by a second point in time 104 indicating that the user has finished speaking. Time point 106 provides an indication when the two-pass ASR system determines that the user has finished speaking the utterance, and this follows time 104. In the illustrated example, the duration between time 104 and time 106 (i.e., the time it takes for the system to determine that the user has finished speaking) is much shorter than the duration between times 102 and 104 (i.e., the amount of time the user speaks an utterance). Additionally or alternatively, time point 108 indicates when the system generated a textual representation of the utterance, where time 108 follows time 106.
In some implementations, the first pass streaming portion of the ASR model 110 is the duration between time 102 and time 106 (i.e., the time from when the user started speaking and the system determined that the user has finished speaking). In the illustrated example, the first pass streaming portion of the ASR model 110 begins when the user begins speaking. However, this is merely illustrative and first pass portion 110 can begin slightly after the user begins speaking (e.g., first pass portion 110 can begin 0.01 seconds, 0.05 seconds, 0.1 seconds, 0.5 seconds, 1 second, and/or another duration after the user begins speaking). Similarly, in the illustrated example, first pass streaming portion 110 ends when the system determines that the user has finished speaking utterance 106. However, this is merely illustrative and first pass portion 110 can end slightly after the system determines that the user has finished speaking utterance 106 (e.g., first pass portion 110 can end 0.01 seconds, 0.05 seconds, 0.1 seconds, 0.5 seconds, 1 second, and/or another duration after the system determines that the user has finished speaking). In many implementations, the ASR system streams one or more candidate streaming text representations of an utterance while the user is speaking the utterance. In some embodiments, the first-pass streaming portion of the ASR model 110 includes a shared encoder and an RNN-T decoder. Additionally or alternatively, the first pass streaming portion of the ASR model 110 can include additional encoders in addition to the shared encoder and the RNN-T decoder.
In some implementations, the second pass portion of the ASR model 114 is the duration between time 106 and time 108 (i.e., the time from when the system has determined that the user has finished speaking the utterance and when the system generates the final text representation of the utterance). In the illustrated example, the second pass portion 114 begins when the system determines that the user has finished speaking the utterance 106. However, this is merely illustrative and second pass streaming portion 114 can begin slightly after the system determines that the user has finished speaking (e.g., second pass portion 114 can begin 0.01 seconds, 0.05 seconds, 0.1 seconds, 0.5 seconds, 1 second, and/or an additional duration after the system determines that the user has finished speaking). In some implementations, the second pass portion 114 includes a LAS decoder, where the LAS decoder is able to refine the candidate text representations generated in a streaming manner during the first pass by processing the output generated in the first pass portion, such as the output generated using a shared encoder and/or the output generated using an additional encoder.
FIG. 2 is a block diagram illustrating another example process 250 for generating a textual representation of audio data using a two-pass ASR model. The first pass portion of the ASR model can include processing the audio data 202, capturing utterances spoken by humans, generating a shared encoder output 206 using the shared encoder 204, and processing the shared encoder output 206 using an RNN-T decoder 208 to generate an RNN-T output 210. Additionally or alternatively, the first pass portion can include processing the shared encoder output 206 using the additional encoder 252 to generate an additional encoder output 254. In many embodiments, the additional encoder output 254 is capable of encoding the shared encoder output 206 such that the additional encoder output 254 is more suitable for processing using the LAS decoder 212.
In many embodiments, the second pass portion is non-streaming and can include processing the additional encoder output 254 and the RNN-T output 210 using the LAS decoder 212 to generate the LAS output 214. A final text representation 216 of the utterance captured in the audio data can be generated based on the LAS output 214.
FIG. 3 illustrates an example environment 300 in which embodiments disclosed herein can be implemented. Fig. 3 includes a client device 302. In many implementations, the client device 302 is capable of executing an instance of an automated assistant (not depicted). The client computing device 302 may be, for example: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system and an in-vehicle entertainment system, an in-vehicle navigation system), a stand-alone interactive speaker, a smart appliance such as a smart television, and/or a wearable apparatus of a user that includes a computing device (e.g., a watch of a user with a computing device, glasses of a user with a computing device, a virtual or augmented reality computing device). Additional and/or alternative client computing devices may be provided.
The example environment 300 includes a two-pass ASR engine 304, a shared encoder module 306, an additional encoder module 308, an RNN-T decoder module 310, an LAS decoder module 312, a training engine 316, a training instance engine 318, a two-pass ASR model 314, a training instance 320, and/or additional or alternative engines and/or models (not depicted). The two-pass ASR engine 304, the shared encoder module 306, the additional encoder module 308, the RNN-T decoder module 310, and the LAS decoder module 312 are example components in which the techniques described herein may interface. In some implementations, one or more aspects of one or more of the engines and/or modules 304, 306, 308, 310, 312, and 316 of fig. 3 may be combined. For example, aspects of the shared encoder module 306 may be combined with aspects of the additional encoder module 308.
The training instance engine 320 is capable of generating training instances 318. For example, the training instance engine 320 can generate one or more training instances, where each training instance includes audio data that captures an utterance and a ground truth text representation of the utterance. In some implementations, the training instance 318 can be generated by the training instance 320 in accordance with the process 400 of fig. 4 described herein.
In some implementations, the two-pass ASR model 314 can include a shared encoder portion, an additional encoder portion, an RNN-T decoder portion, and an LAS decoder portion (e.g., the two-pass ASR model can be in accordance with the model illustrated in fig. 2 as described herein). For example, the training engine 316 can train the two-pass ASR model 314 by: (1) training the shared encoder and RNN-T decoder portions in accordance with process 500 of fig. 5, (2) training the additional encoder and LAS decoder using the shared encoder trained in step (1), wherein the shared encoder is frozen during training of the additional encoder and LAS decoder in accordance with process 650 of fig. 6, and (3) refining the trained shared encoder, additional encoder, RNN-T decoder, and LAS decoder using common losses in accordance with process 750 of fig. 7. In some implementations, the training engine 316 can use average word error rate (MWER) training to further refine the LAS decoder portion.
Generating a textual representation of the utterance is described herein with respect to process 850 of fig. 8. The two-pass ASR engine 304 can generate a textual representation of the captured audio data using the two-pass ASR model 314. In some implementations, the shared encoder module 306 of the two-pass ASR engine 304 can process the captured audio data using the shared encoder portion of the two-pass ASR model 314 to stream shared encoder output. Additionally or alternatively, the shared encoder module 306 can store the streaming encoder output in a buffer for additional use by the two-pass ASR engine 304, such as for use by additional encoder modules 308, LAS decoder module 312, and/or additional portion(s) of the two-pass ASR engine 304. The additional encoder module 308 can process the shared encoder output using the additional encoder portion of the two-pass ASR model 314 to generate additional encoder output. In some implementations, the additional encoder module 308 can process the shared encoder output in a streaming manner because the shared encoder output is generated using the shared encoder module 306. In some other implementations, the additional encoder module 308 can process the shared encoder output stored in the buffer by the shared encoder module 306.
The RNN-T decoder module 310 can process the shared encoder output using the RNN-T decoder portion of the two-pass ASR model 314 to generate one or more candidate text representations of the utterance. In some implementations, the RNN-T decoder module 310 can process shared encoder output that is streamed by the shared encoder module 306. The LAS decoder module 312 can generate a text representation of the utterance by processing candidate text representations of the utterance generated using the RNN-T decoder module and either the shared encoder output stored in a buffer using the shared encoder module 306 or the additional encoder output generated using the additional encoder module 308.
FIG. 4 is a flow diagram illustrating a process 400 of generating one or more training instances that can be used to train a two-pass ASR model, in accordance with various embodiments. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include one or more components, such as one or more processors (e.g., CPU(s), GPU(s), and/or TPU (s)). While the operations of process 400 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 402, the system selects an instance of audio data that captures the utterance. For example, an example of audio data can capture the human spoken utterance "Turn the thermal up the three degrees" of thermostat.
At block 404, the system determines a textual representation of the utterance. In many implementations, the textual representation of the utterance can be determined by a human reviewer of the selected instance of the audio data. Additionally or alternatively, additional ASR systems can be used to determine a textual representation of the utterance. In many implementations, the textual representation of the audio data can be determined using an additional ASR system in which the generated textual representation is reviewed by a human reviewer for accuracy.
At block 406, the system stores a training instance that includes (1) an instance of audio data that captured the utterance and (2) a textual representation of the utterance.
At block 408, the system determines whether any additional training instances are generated. If so, the system proceeds back to block 402, selecting to capture additional instances of audio data for the utterance before proceeding to blocks 404 and 406 to use the additional instances of audio data. In many implementations, the system can determine whether to generate additional training instances based on whether one or more conditions are satisfied, such as whether a threshold number of training instances have been generated, whether there are any remaining unprocessed instances of audio data, and/or whether additional condition(s) are satisfied. If, at block 408, the system determines that no additional training instances have been generated, the process ends.
FIG. 5 is a flow diagram illustrating a process 500 of training a shared encoder and RNN-T decoder of a two-pass ASR model in accordance with various embodiments. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include one or more components, such as one or more processors (e.g., CPU(s), GPU(s), and/or TPU (s)). While the operations of process 500 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 502, the system selects a training instance that includes (1) an instance of audio data that captured the utterance and (2) a ground truth representation of the utterance. In some implementations, the training examples can be generated in accordance with the process 400 of fig. 4.
At block 504, the system processes the audio data portion of the training instance using a shared encoder to generate a shared encoder output and can process the shared encoder output using an RNN-T decoder to generate a predicted RNN-T output.
At block 506, the system generates an RNN-T loss based on the predicted RNN-T output and the ground truth text representation portion of the training instance.
At block 508, the system updates one or more portions of the shared encoder and/or one or more portions of the RNN-T decoder based on the RNN-T loss.
At block 510, the system determines whether to perform additional training. If so, the system proceeds back to block 502, selecting additional training instances before proceeding to blocks 504, 506, and 508 using the additional training instances before performing additional iterations of block 510. In some implementations, the system can determine to perform more training if there are one or more additional unprocessed training instances and/or if other criteria/criteria are still not met. Other criteria/criteria can include, for example, whether a threshold number of time periods have occurred and/or whether training of a threshold duration has occurred. Although the process 500 is described with respect to non-batch learning techniques, batch learning may additionally and/or alternatively be utilized. If, at block 510, the system determines that no additional training is to be performed, the process ends.
In some embodiments, the RNN-T decoder can be trained to generate query end markers, such as. In some of those embodiments, the RNN-T decoder can be trained with a training penalty for early or late prediction of the end-of-query marker. For example, a training example can include audio data that captures an utterance of "what is the weather tomorrow". Training penalties can be used when the predicted RNN-T output predicts early query completion markers, such as "what is the weather" ("Weathering") ". Similarly, a training penalty can be used when the predicted RNN-T output predicts too late a query completion marker, such as "what is the heat tomorrow [ pause ] [ pause ] </s > (how the tomorrow [ pause ] [ pause ] </s >)".
FIG. 6 is a flow diagram illustrating a process 650 of training an additional encoder and LAS decoder of a two-pass ASR model in accordance with various embodiments. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include one or more components, such as one or more processors (e.g., CPU(s), GPU(s), and/or TPU (s)). While the operations of procedure 650 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 652, the system selects a training instance that includes (1) an instance of audio data that captured the utterance and (2) a ground truth representation of the utterance. In some embodiments, the selected training instance is different at all from the training instances used to train the shared encoder and/or RNN-T decoder portions of the two-pass ASR model. In some other implementations, the selected training instances are additionally utilized to train the shared encoder and/or RNN-T decoder portion of the two-pass ASR model. In some implementations, the training examples can be generated in accordance with the process 400 of fig. 4.
At block 654, the system processes the audio data portion of the training instance using (1) the shared encoder, (2) the additional encoder, and (3) the LAS decoder to generate a predicted LAS output. For example, the shared encoder can process audio data to generate a shared encoder output, the additional encoder can process the shared encoder output to generate an additional encoder output, and the LAS decoder can process the additional encoder output to generate a predicted LAS output. In many embodiments, the shared encoder can be trained previously while the RNN-T decoder is trained in accordance with the process 500 of FIG. 5.
At block 656, the system generates a loss based on the ground truth representation portion of the predicted LAS output training instance.
At block 658, the system updates one or more portions of the additional encoder based on the LAS loss and/or updates one or more portions of the LAS decoder based on the LAS loss, but does not update the shared encoder. In other words, a previously trained shared encoder can be frozen while training additional encoders and/or LAS decoders.
At block 660, the system determines whether any additional training is performed. If so, the system proceeds back to block 652, selects additional training instances, then performs iterations of blocks 654, 656, and 658 based on the additional training instances, and then performs additional iterations of block 660. In some implementations, the system can determine to perform more if there are one or more additional unprocessed training instances and/or if other criteria/criteria are still not met. Other criteria/criteria can include, for example, whether a threshold number of time periods have occurred and/or whether training of a threshold duration has occurred. Although the process 650 is described with respect to non-batch learning techniques, batch learning may additionally and/or alternatively be utilized. If, at block 660, the system determines that no additional training is to be performed, the process ends.
FIG. 7 is a flow diagram illustrating an example process 750 of jointly training a shared encoder, an additional encoder, an RNN-T decoder, and/or a LAS decoder to refine a trained two-pass ASR model, in accordance with various embodiments. For example, the shared encoder and/or RNN-T decoder can be trained in accordance with the process 500 of fig. 5 and/or the additional encoder and/or LAS decoder can be trained in accordance with the process 650 of fig. 6. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include one or more components, such as one or more processors (e.g., CPU(s), GPU(s), and/or TPU (s)). While the operations of process 750 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 752, the system selects a training instance that includes (1) audio data that captured the utterance and (2) a ground truth text representation of the utterance. In some implementations, the selected training instance is radically different from the training instances of the shared encoder, the additional encoder, the RNN-T decoder, and/or the LAS decoder used to initially train the two-pass ASR model. In some other implementations, the selected training instance is additionally utilized to initially train the shared encoder, the additional encoder, the RNN-T decoder, and/or the LAS decoder of the two-pass ASR model. In some implementations, the training examples can be generated in accordance with the process 400 of fig. 4. In some implementations, the LAS decoder can be trained additionally using MWER training (not depicted) after the LAS decoder is trained using combining losses.
At block 754, the system processes the audio data using the initially trained shared encoder to generate a shared encoder output. For example, the shared encoder can be initially trained in accordance with the process 500 of fig. 5.
At block 756, the system processes the shared encoder output using the initially trained RNN-T decoder to generate a predicted RNN-T output. For example, the RNN-T decoder can be initially trained in accordance with the process 500 of FIG. 5.
At block 758, the system processes the shared encoder output using the initially trained additional encoder to generate an additional encoder output. For example, the additional encoders can be initially trained in accordance with the process 650 of fig. 6.
At block 760, the system processes the additional encoder output using the initially trained LAS decoder to generate a predicted LAS output. For example, the LAS decoder can be initially trained in accordance with the process 650 of fig. 6.
At block 762, the system generates a combined penalty based on: (1) a predicted RNN-T output, (2) a predicted LAS output, and (3) a ground truth text representation portion of the training instance. For example, the system can generate Lcombined(x，y*)＝λLRNNT(x，y*)+(1-λ)LLAS(x，y*) Where x is an instance of audio data, y*Is a ground truth text representation part of the training example, LRNNTIs the RNN-T loss of the ground truth text representation part based on the predicted RNN-T output and training examples, and LLASIs based on predicted LAS output and loss of LAS of the ground truth text representation portion of the training instance. Additional and alternative combined losses can be utilized in accordance with many embodiments.
At block 764, the system updates (1) one or more portions of the shared encoder based on the combining loss; (2) one or more portions of an additional encoder; (3) one or more portions of an RNN-T decoder; and/or (4) one or more portions of an LAS decoder.
At block 766, the system determines whether to perform any additional training. If so, the system proceeds back to block 752, selects an additional training instance, then performs iterations of blocks 754, 756, 758, 760, 762, and 764 based on the additional training instance, and then performs additional iterations of block 766. In some implementations, the system can determine that more training is to be performed if there are one or more additional unprocessed training instances and/or if other criteria/criteria are still not satisfied. Other criteria/criteria can include, for example, whether a threshold number of time periods have occurred and/or whether training of a threshold duration has occurred. Although the process 750 is described with respect to non-batch learning techniques, batch learning can additionally and/or alternatively be utilized. If, at block 766, the system determines that no additional training is to be performed, the process ends. In some implementations, the LAS decoder can be trained additionally using MWER training (not depicted) after the LAS decoder is trained using combining losses.
FIG. 8 is a flow diagram illustrating a process 850 of generating a text representation of an utterance captured in audio data using a two-pass ASR model, wherein the two-pass ASR model includes a shared encoder, an additional encoder, an RNN-T decoder, and an LAS decoder, in accordance with various embodiments. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include one or more components, such as one or more processors (e.g., CPU(s), GPU(s), and/or TPU (s)). While the operations of process 800 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 852, the system receives streaming audio data, where the audio data captures an utterance spoken by a human. In many implementations, audio data is captured using one or more microphones of the client device. For example, the audio data can be captured using a microphone of a stand-alone interactive speaker.
At block 854, before the human has finished speaking and in a streaming manner, the system generates one or more candidate text representations of the utterance by processing the audio data using the first-pass portion of the two-pass ASR model. In many embodiments, the system processes audio data using a first-pass portion of a two-pass ASR model by: (1) processing the audio data using a shared encoder to generate a shared encoder output; (2) processing the shared encoder output using an RNN-T decoder to generate one or more candidate textual representations of the utterance; and (3) processing the shared encoder output using the additional encoder to generate an additional encoder output.
At block 856, the system determines that the human has finished speaking. In some implementations, the system can determine that a human has finished speaking based on a query end marker generated using the RNN-T decoder. In some implementations, the system can determine that a human has finished speaking based on a query end marker generated by processing audio data using an endpoint model (not depicted).
In block 858, in response to determining that the human has finished speaking, the system generates a text representation of the utterance using the second pass portion of the two-pass ASR model by using the LAS decoder to process (1) the additional encoder output and (2) one or more candidate text representations of the utterance to generate the text representation of the utterance. In some implementations, the LAS decoder can be used to re-score the first K hypotheses generated using the RNN-T decoder (i.e., the first K candidate text representations generated using the RNN-T decoder). For example, the LAS decoder can be used to re-score the first three hypotheses, the first five hypotheses, the first twenty hypotheses, and/or another number of top hypotheses generated using the RNN-T decoder. For example, for each of the top hypotheses, the LAS decoder can be run in a teacher-forced mode with attention to the additional encoder output to generate an LAS score that combines the probabilities of the candidate hypotheses and the attention parameter(s). The system can select the hypothesis with the highest LAS score as the textual representation of the utterance. Additionally or alternatively, the LAS decoder can be used to re-score the tree-based lattices in which the top candidate text representation of the utterance is represented as a lattice. For example, the LAS decoder can be used to process each lattice arc in a teacher-forced mode with attention to additional encoder outputs to update the probabilities in the arcs. After updating the probabilities using the LAS decoder, the system can identify the text representation of the utterance as the candidate text representation having the highest probability.
At block 860, the system performs one or more actions based on the textual representation of the utterance. For example, the system can render output based on the text representation on a screen of the client device for the speaker. In some implementations, the system can render the output based on one or more of the candidate text representations while the human is speaking the utterance, such as rendering the output based on the candidate text representation having the highest probability. In some such implementations, the system can render the output based on the text representation by revising the output rendered while the human is speaking the utterance. Additionally or alternatively, the system can render one or more responses to the textual representation of the utterance. For example, the system can render a response of "it is going to rain" in response to a human speaking "what is the weather today" to the human being. Additionally or alternatively, the system can control one or more client devices based on the textual representation of the utterance. For example, the system can turn on four lights in the living room in response to the utterance of "turn on the moving room lights". In various implementations, the system can perform additional and/or alternative actions based on the final textual representation of the utterance.
Turning now to FIG. 9, an example environment is illustrated in which various embodiments can be implemented. Fig. 9 is initially depicted and includes a client computing device 902 that executes an instance of an automated assistant client 904. The one or more cloud-based automated assistant components 910 can be implemented on one or more computing systems (collectively, "cloud" computing systems) communicatively coupled to the client device 902 via one or more local and/or wide area networks (e.g., the internet), indicated generally at 908.
An instance of the automated assistant client 904, through its interaction with the one or more cloud-based automated assistant components 910, can form what, from the perspective of the user, is a logical instance of the automated assistant 900 with which the user can participate in a human-machine conversation. An example of such an automated assistant 900 is depicted in fig. 9. It should therefore be appreciated that in some implementations, a user engaged with an automated assistant client 904 executing on a client device 902 may actually engage with his or her own logical instance of automated assistant 900. For the sake of brevity and simplicity, the term "automated assistant" as used herein as "serving" a particular user will generally refer to a combination of an automated assistant client 904 executing on a client device 902 operated by the user and one or more cloud-based automated assistant components 910 (which may be shared among multiple automated assistant clients of multiple client computing devices). It should also be understood that in some implementations, the automatic assistant 900 may respond to requests from any user, regardless of whether the user is actually "served" by that particular instance of the automatic assistant 900.
The client computing device 902 may be, for example: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a stand-alone interactive speaker, a smart appliance such as a smart television, and/or a wearable apparatus of a user that includes a computing device (e.g., a watch of a user with a computing device, glasses of a user with a computing device, a virtual or augmented reality computing device). Additional and/or alternative client computing devices may be provided. In various implementations, the client computing device 902 may optionally operate one or more other applications in addition to the automated assistant client 904, such as a messaging client (e.g., SMS, MMS, online chat), browser, and so forth. In some of those various implementations, one or more of the other applications can optionally interface with the automatic assistant 900 (e.g., via an application programming interface), or include instances of their own automatic assistant application (which may also interface with the cloud-based automatic assistant component 910).
The automated assistant 900 participates in a human-machine conversation session with a user via user interface input and output devices of the client device 902. To protect user privacy and/or conserve resources, in many cases a user must often explicitly invoke the automatic assistant 900 before the automatic assistant will fully process the spoken utterance. Explicit invocation of the automated assistant 900 can occur in response to certain user interface inputs received at the client device 902. For example, user interface inputs that can invoke the automated assistant 900 via the client device 902 can optionally include actuations of hardware and/or virtual buttons of the client device 902. Further, the automated assistant client can include one or more local engines 906, such as an invocation engine operable to detect the presence of one or more spoken invocation phrases. The invocation engine can invoke the automated assistant 900 in response to detecting one of the spoken invocation phrases. For example, the invocation engine can invoke automatic Assistant 900 in response to detecting spoken invocation phrases such as "Hey Assistant," OK Assistant, "and/or" Assistant. The invocation engine can continuously process a stream of audio data frames based on output from one or more microphones of the client device 602 (e.g., without being in an "inactive" mode) to monitor for the occurrence of spoken invocation phrases. Upon detecting the occurrence of a spoken call phrase, the call engine (e.g., after temporary storage in a buffer) discards any audio data frames that do not include the spoken call phrase. However, when the invocation engine detects the occurrence of a spoken invocation phrase in the processed frame of audio data, the invocation engine is able to invoke the automated assistant 900. As used herein, "invoking" the automatic assistant 900 can include causing one or more previously inactive functions of the automatic assistant 900 to be activated. For example, invoking the automatic assistant 900 can include causing one or more local engines 906 and/or cloud-based automatic assistant components 910 to further process the audio data frame on which the detected invocation phrase is based, and/or one or more following audio data frames (although no further processing of the audio data frames occurs prior to invocation). For example, the local and/or cloud-based components can process the captured audio data using a two-pass ASR model in response to a call to the automated assistant 900.
The one or more local engines 906 of the automated assistant 900 are optional and can include, for example, the above-described invocation engine, a local speech-to-text ("STT") engine (which converts captured audio to text), a local text-to-speech ("TTS") engine (which converts text to speech), a local natural language processor (which determines the meaning of the audio and/or text converted from audio), and/or other local components. Because the client device 902 is relatively constrained in terms of computing resources (e.g., processor cycles, memory, battery, etc.), the local engine 906 may have limited functionality with respect to any counterpart included in the cloud-based automated assistant component 910.
Relative to any counterpart of the local engine(s) 906, the cloud-based automated assistant component 910 utilizes the nearly unlimited resources of the cloud to perform more robust and/or accurate processing of audio data and/or other user interface input. Again, in various embodiments, the client device 902 can provide audio data and/or other data to the cloud-based automatic assistant component 910 in response to the invocation engine detecting a spoken invocation phrase or detecting some other explicit invocation of the automatic assistant 900.
The illustrated cloud-based automatic assistant component 910 includes a cloud-based TTS module 912, a cloud-based STT module 914, a natural language processor 916, a conversation state tracker 918, and a conversation manager 920. In some implementations, one or more of the engines and/or modules of the automated assistant 900 may be omitted, combined, and/or implemented in a component separate from the automated assistant 900. Further, the automated assistant 900 can include additional and/or alternative engines and/or modules in some implementations. The cloud-based STT module 914 can convert the audio data into text, which can then be provided to the natural language processor 916.
The cloud-based TTS module 912 can convert text data (e.g., natural language responses formulated by the automated assistant 900) into computer-generated speech output. In some implementations, the TTS module 912 can provide the computer-generated speech output to the client device 902 for direct output, e.g., using one or more speakers. In other implementations, the textual data (e.g., natural language responses) generated by the automated assistant 900 may be provided to one of the local engine(s) 906, which may then convert the textual data into computer-generated speech that is output locally.
The natural language processor 916 of the automatic assistant 900 processes the free-form natural language input and generates annotated output based on the natural language input for use by one or more other components of the automatic assistant 900. For example, the natural language processor 916 is capable of processing natural language free form input, i.e., text input as a conversion by the STT module 914 to audio data provided by a user via the client device 902. The generated annotated output may include one or more annotations of the natural language input and optionally one or more (e.g., all) of the words of the natural language input.
In some implementations, the natural language processor 916 is configured to identify and annotate various types of grammar information in the natural language input. In some implementations, the natural language processor 916 can additionally and/or alternatively include an entity tagger (not depicted) configured to annotate entity references in one or more segments, such as references to persons (including, for example, literary characters, celebrities, public personalities, etc.), organizations, places (real and fictional), and so forth. In some implementations, the natural language processor 916 may additionally and/or alternatively include a coreference parser (not depicted) configured to group or "cluster" references to the same entity based on one or more contextual cues. For example, a coreference parser may be utilized to parse the word "there" in the natural language input "I liked the Hypothetical cafe where I liked we eaten the last time" into "the Hypothetical cafe". In some implementations, one or more components of the natural language processor 916 may rely on annotations from one or more other components of the natural language processor 916. In some implementations, in processing a particular natural language input, one or more components of the natural language processor 916 may determine one or more annotations using related prior inputs and/or other related data other than the particular natural language input.
In some implementations, the dialog state tracker 918 can be configured to track a "dialog state" that includes, for example, a belief state of one or more users' goals (or "intentions") during a human-machine dialog session and/or across multiple dialog sessions. In determining dialog state, some dialog state trackers may seek to determine the most likely value(s) for the slot(s) instantiated in a dialog based on user and system utterances in the dialog session. Some techniques utilize a fixed ontology that defines a set of slots and a set of values associated with those slots. Some techniques may additionally or alternatively be customized to individual slots and/or domains. For example, some techniques may require training a model for each slot type in each domain.
The dialog manager 920 may be configured to map the current dialog state, e.g., as provided by the dialog state tracker 918, to one or more "response actions" of a plurality of candidate response actions that are then performed by the automated assistant 900. The responsive action may occur in various forms depending on the current dialog state. For example, initial and midstream dialog states corresponding to turns of the dialog session that occurred before the last turn (e.g., when performing a task desired by the end user) may be mapped to various response actions, including the automated assistant 900 outputting additional natural language dialogs. The response dialog may include, for example, requesting the user to provide parameters for some action that the dialog state tracker 918 believes the user intends to perform (i.e., filling the slot). In some implementations, the response action may include actions such as "request" (e.g., finding a parameter for slot filling), "propose" (e.g., suggest an action or course of action for the user), "select", "notify" (e.g., provide the user with the requested information), "not match" (e.g., notify the user that the user's last input was not understood), command to the peripheral (e.g., to turn off the light bulb), and so on.
Fig. 10 is a block diagram of an example computing device 1010 that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing devices and/or other components may include one or more components of the example computing device 1010.
The computing device 1010 typically includes at least one processor 1014 that communicates with a number of peripheral devices via a bus subsystem 1012. These peripheral devices may include storage subsystems 1024, including, for example, memory subsystem 1025 and file storage subsystem 1026, user interface output devices 1020, user interface input devices 1022, and network interface subsystem 1016. The input and output devices allow a user to interact with the computing device 1010. Network interface subsystem 1016 provides an interface to external networks and is coupled to corresponding interface devices in other computing devices.
The user interface input devices 1022 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 1010 or onto a communication network.
User interface output devices 1020 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a cathode ray tube ("CRT"), a flat panel device such as a liquid crystal display ("LCD"), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 1010 to a user or to another machine or computing device.
These software modules are typically executed by the processor 1014, either alone or in combination with other processors. Memory 1025 used in storage subsystem 1024 can include a number of memories including a main random access memory ("RAM") 1030 for storing instructions and data during program execution and a read only memory ("ROM") 1032 in which fixed instructions are stored. File storage subsystem 1026 is capable of providing persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 1026 in storage subsystem 1024 or in other machines accessible by processor(s) 1014.
Where the systems described herein collect personal information about a user (or often referred to herein as a "participant") or may make use of the personal information, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current geographic location) or whether and/or how to receive content from a content server that may be more relevant to the user. In addition, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized where geographic location information is obtained (such as to a city, zip code, or state level) such that a particular geographic location of the user cannot be determined. Thus, the user may control how information is collected and/or used with respect to the user.
In some implementations, a method implemented by a processor(s) is provided and includes receiving audio data that includes a sequence of segments and capturing utterances spoken by a human speaker. The method further comprises, for each of the segments, and in order: the segments are processed using a first pass portion of an automatic speech recognition ("ASR") model to generate a recurrent neural network transducer ("RNN-T") output. Processing each of the segments using the first pass portion of the ASR model includes: processing the segments using a shared encoder portion to generate a shared encoder output; adding the shared encoder output as a next item in the shared encoder buffer; processing the shared encoder output using the RNN-T decoder portion to generate a corresponding portion of the RNN-T output; and processing the shared encoder output using the additional encoder to generate an additional encoder output. The method further includes determining one or more first-pass candidate text representations of the utterance based on the RNN-T output. The method further includes determining that the human speaker has finished speaking the utterance. The method further includes, in response to determining that the human speaker has finished speaking the utterance: processing the shared encoder output from the shared encoder buffer using the additional encoder to generate an additional encoder output; generating a listen-in spelling ("LAS") output based on processing the additional encoder output and (a) the RNN-T output and/or (b) one or more first-pass candidate text representations of the utterance using a second-pass LAS decoder portion of the ASR model; and generating a final text representation of the utterance based on the LAS output.
These and other implementations of the technology can include one or more of the following features.
In some implementations, receiving the audio data includes capturing the audio data using one or more microphones of the client device.
In some implementations, the one or more first-pass candidate text representations of the utterance are first-pass lattice representations. In some of those implementations, generating the LAS output based on processing the additional encoder output and the one or more first pass candidate text representations of the utterance using a second pass LAS decoder portion of the ASR model includes: for each lattice arc in the first pass lattice representation, processing the lattice arc using an LAS decoder in a teacher-forced mode with attention to the additional encoder output to update the probability of the first pass candidate text representation corresponding to the arc; and generating an LAS output by selecting the candidate first-pass text representation having the highest updated probability.
In some embodiments, the method further comprises generating a plurality of training instances, wherein generating the plurality of training instances comprises: selecting an instance of training audio data that captures a training utterance spoken by a training human speaker; determining a ground truth representation of the training utterance; and storing a training instance comprising training audio data and a ground truth text representation of the training utterance. In some of those embodiments, the method further comprises training the ASR model. Training the AS model can include, for each of a plurality of training instances and until one or more conditions are satisfied: processing instances of training audio data using a shared encoder to generate a shared encoder training output; processing the shared encoder training output using an RNN-T decoder to generate a predicted RNN-T training output; determining a loss based on the predicted RNN-T training output and a ground truth representation of the training utterance; and updating one or more portions of the shared encoder portion based on the determined loss and/or updating one or more portions of the RNN-T decoder portion based on the determined loss. In some versions of those embodiments, training the ASR model further comprises, for each of the plurality of training instances and until one or more second conditions are satisfied: processing an instance of training audio data using a shared encoder to generate a second shared encoder training output; processing the second shared encoder training output using the additional encoder to generate an additional encoder training output; processing the additional encoder training output using the LAS decoder to generate an LAS training output; determining a second loss based on the LAS training output and a live ground representation of the training utterance; and updating one or more portions of the additional encoder based on the determined impairments and/or updating one or more portions of the LAS decoder based on the determined impairments. In some of those versions, training the ASR model further comprises, for each of a plurality of training instances and until one or more third conditions are satisfied: processing an instance of training audio data using a shared encoder to generate a third shared encoder training output; processing the third shared encoder training output using an RNN-T decoder to generate a second RNN-T training output; determining an RNN-T loss based on the second RNN-T training output and a ground truth representation of the training utterance; processing the third shared encoder training output using the additional encoder to generate a second additional encoder training output; processing the second additional encoder training output using the LAS decoder to generate a second LAS training output; determining a loss of LAS based on the second LAS training output and a ground truth representation of the training utterance; determining a common loss based on the RNN-T loss and the LAS loss; one or more portions of the shared encoder are updated based on the common loss and/or one or more portions of the additional encoder are updated based on the common loss and/or one or more portions of the RNN-T decoder are updated based on the common loss and/or one or more portions of the LAS decoder are updated based on the common loss. Optionally, training the ASR model further comprises training the LAS decoder using average word error rate training.
In some implementations, the RNN-T output includes a query end marker generated using the first pass portion of the ASR model that indicates that the human speaker has finished speaking. In some versions of those embodiments, determining that the human speaker has finished speaking the utterance includes determining that the human speaker has finished speaking the utterance in response to identifying a query end marker in the RNN-T output. In some of those versions, the method further includes training the ASR model, and training the ASR model includes penalizing the RNN-T decoder portion for generating the query end marker too early or too late.
Additionally, some embodiments include one or more processors (e.g., central processing unit(s) (CPU), graphics processing unit(s) (GPU), and/or tensor processing unit(s) (TPU) of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to cause performance of any of the methods described herein.
While several embodiments have been described and illustrated herein, various other means and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be utilized and each of such variations and/or modifications is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are intended to be exemplary and the actual parameters, dimensions, materials, and/or configurations will depend upon the particular application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than one routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, where such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (15)
1. A method implemented by one or more processors, the method comprising:
receiving audio data comprising a sequence of segments and capturing utterances spoken by a human speaker;
for each of the segments, and in order:
processing the segment using a first pass portion of an automatic speech recognition ("ASR") model to generate a recurrent neural network transformer ("RNN-T") output, wherein processing the segment using the first pass portion of the ASR model comprises:
processing the segments using a shared encoder portion to generate a shared encoder output,
adding the shared encoder output as a next item in a shared encoder buffer,
processing the shared encoder output using an RNN-T decoder portion to generate a corresponding portion of an RNN-T output, an
Processing the shared encoder output using an additional encoder to generate an additional encoder output;
determining one or more first-pass candidate text representations for the utterance based on the RNN-T output;
determining that the human speaker has finished speaking the utterance;
in response to determining that the human speaker has finished speaking the utterance:
processing the shared encoder output from the shared encoder buffer using an additional encoder to generate an additional encoder output;
generating a listen-in spelling ("LAS") output based on processing the additional encoder output and at least one of (a) the RNN-T output or (b) one or more first-pass candidate text representations of the utterance using a second-pass LAS decoder portion of the ASR model; and
generating a final text representation of the utterance based on the LAS output.
2. The method of claim 1, wherein receiving the audio data that includes the segmented sequence and captures the utterance spoken by the human speaker comprises capturing the audio data using one or more microphones of a client device.
3. The method of any preceding claim, wherein the one or more first-pass candidate text representations of the utterance are first-pass lattice representations.
4. The method of claim 3, wherein generating LAS output based on processing the additional encoder output and the one or more first pass candidate text representations of the utterance using a second pass LAS decoder portion of the ASR model comprises:
for each lattice arc in the first pass lattice representation, processing the lattice arc using the LAS decoder in a teacher-forced mode with attention to the additional encoder output to update a probability of the first pass candidate text representation corresponding to the arc; and
generating the LAS output by selecting the first pass candidate text representation having the highest updated probability.
5. The method of any preceding claim, further comprising:
generating a plurality of training instances, wherein generating each training instance comprises:
selecting an instance of training audio data that captures a training utterance spoken by a training human speaker;
determining a ground truth representation for the training utterance; and
storing the training instance comprising the training audio data and a ground truth text representation of the training utterance.
6. The method of claim 5, further comprising training the ASR model, wherein training the ASR model comprises:
for each of the plurality of training instances and until one or more conditions are met:
processing instances of training audio data using the shared encoder to generate a shared encoder training output;
processing the shared encoder training output using the RNN-T decoder to generate a predicted RNN-T training output;
determining a loss based on the predicted RNN-T training output and a ground truth representation of the training utterance;
updating one or more portions of the shared encoder portion based on the determined loss and/or updating one or more portions of the RNN-T decoder portion based on the determined loss.
7. The method of claim 6, wherein training the ASR model further comprises:
for each of the plurality of training instances and until one or more second conditions are met:
processing, using the shared encoder, an instance of training audio data to generate a second shared encoder training output;
processing the second shared encoder training output using the additional encoder to generate an additional encoder training output;
processing the additional encoder training output using the LAS decoder to generate a LAS training output;
determining a second loss based on the LAS training output and a ground truth representation of the training utterance; and
updating one or more portions of the additional encoder based on the determined loss and/or updating one or more portions of the LAS decoder based on the determined loss.
8. The method of claim 7, wherein training the ASR model further comprises:
for each of the plurality of training instances and until one or more third conditions are met:
processing, using the shared encoder, instances of training audio data to generate a third shared encoder training output;
processing the third shared encoder training output using the RNN-T decoder to generate a second RNN-T training output;
determining an RNN-T loss based on the second RNN-T training output and a ground truth representation of the training utterance;
processing the third shared encoder training output using the additional encoder to generate a second additional encoder training output;
processing the second additional encoder training output using the LAS decoder to generate a second LAS training output;
determining a LAS loss based on the second LAS training output and a ground truth representation of the training utterance;
determining a common loss based on the RNN-T loss and the LAS loss; and
updating one or more portions of the shared encoder based on the common loss and/or updating one or more portions of the additional encoder based on the common loss and/or updating one or more portions of the RNN-T decoder based on the common loss and/or updating one or more portions of the LAS decoder based on the common loss.
9. The method of claim 8, wherein training the ASR model further comprises training the LAS decoder using average word error rate training.
10. The method of any preceding claim, wherein the RNN-T output comprises a query end marker generated using a first pass portion of the ASR model indicating that the human speaker has finished speaking.
11. The method of claim 10, wherein determining that the human speaker has finished speaking the utterance comprises determining that the human speaker has finished speaking the utterance in response to identifying the query end marker in the RNN-T output.
12. The method of claim 11, wherein training the ASR model comprises penalizing the RNN-T decoder portion for generating the query end marker too early or too late.
13. A computer program comprising instructions which, when executed by one or more processors of a computing system, cause the computing system to perform the method of any preceding claim.
14. A client device comprising one or more processors executing instructions stored in a memory of the client device to perform the method of any of claims 1-13.
15. A computer-readable storage medium storing instructions executable by one or more processors of a computing system to perform the method of any one of claims 1-13.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962943703P | 2019-12-04 | 2019-12-04 | |
US62/943,703 | 2019-12-04 | ||
PCT/US2020/063012 WO2021113443A1 (en) | 2019-12-04 | 2020-12-03 | Two-pass end to end speech recognition |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114270434A true CN114270434A (en) | 2022-04-01 |
Family
ID=74003923
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080040756.2A Pending CN114270434A (en) | 2019-12-04 | 2020-12-03 | Two-pass end-to-end speech recognition |
Country Status (4)
Country | Link |
---|---|
US (1) | US20220238101A1 (en) |
EP (1) | EP3966811A1 (en) |
CN (1) | CN114270434A (en) |
WO (1) | WO2021113443A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117351331A (en) * | 2023-10-24 | 2024-01-05 | 北京云上曲率科技有限公司 | Method and device for adding adapter for large visual model |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220310061A1 (en) * | 2021-03-26 | 2022-09-29 | Google Llc | Regularizing Word Segmentation |
CN113539273B (en) * | 2021-09-16 | 2021-12-10 | 腾讯科技（深圳）有限公司 | Voice recognition method and device, computer equipment and storage medium |
WO2023205367A1 (en) * | 2022-04-21 | 2023-10-26 | Google Llc | Joint segmenting and automatic speech recognition |
US11880645B2 (en) | 2022-06-15 | 2024-01-23 | T-Mobile Usa, Inc. | Generating encoded text based on spoken utterances using machine learning systems and methods |
CN116665656B (en) * | 2023-07-24 | 2023-10-10 | 美智纵横科技有限责任公司 | Speech recognition model generation method, speech recognition method, device and chip |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7184957B2 (en) * | 2002-09-25 | 2007-02-27 | Toyota Infotechnology Center Co., Ltd. | Multiple pass speech recognition method and system |
US9799327B1 (en) * | 2016-02-26 | 2017-10-24 | Google Inc. | Speech recognition with attention-based recurrent neural networks |
US10176802B1 (en) * | 2016-03-21 | 2019-01-08 | Amazon Technologies, Inc. | Lattice encoding using recurrent neural networks |
-
2020
- 2020-12-03 CN CN202080040756.2A patent/CN114270434A/en active Pending
- 2020-12-03 EP EP20829114.6A patent/EP3966811A1/en active Pending
- 2020-12-03 WO PCT/US2020/063012 patent/WO2021113443A1/en unknown
- 2020-12-03 US US17/616,135 patent/US20220238101A1/en active Pending
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117351331A (en) * | 2023-10-24 | 2024-01-05 | 北京云上曲率科技有限公司 | Method and device for adding adapter for large visual model |
Also Published As
Publication number | Publication date |
---|---|
US20220238101A1 (en) | 2022-07-28 |
WO2021113443A1 (en) | 2021-06-10 |
EP3966811A1 (en) | 2022-03-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7417634B2 (en) | Using context information in end-to-end models for speech recognition | |
JP7234415B2 (en) | Context Bias for Speech Recognition | |
US20220238101A1 (en) | Two-pass end to end speech recognition | |
US10923111B1 (en) | Speech detection and speech recognition | |
US20210312914A1 (en) | Speech recognition using dialog history | |
JP7336537B2 (en) | Combined Endpoint Determination and Automatic Speech Recognition | |
EP3776536B1 (en) | Two-pass end to end speech recognition | |
US11158307B1 (en) | Alternate utterance generation | |
KR20230073297A (en) | Transformer-transducer: one model that integrates streaming and non-streaming speech recognition | |
US11605387B1 (en) | Assistant determination in a skill | |
US11532301B1 (en) | Natural language processing | |
JP2023165012A (en) | Proper noun recognition in end-to-end speech recognition | |
JP2023175029A (en) | Attention-based joint acoustic and text on-device end-to-end model | |
US11715458B2 (en) | Efficient streaming non-recurrent on-device end-to-end model | |
US11544504B1 (en) | Dialog management system | |
US11437026B1 (en) | Personalized alternate utterance generation | |
US11646035B1 (en) | Dialog management system | |
US11756533B2 (en) | Hot-word free pre-emption of automated assistant response presentation | |
US11978438B1 (en) | Machine learning model updating | |
US20240096316A1 (en) | Multi-assistant device control | |
KR20230156795A (en) | Word segmentation regularization |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |