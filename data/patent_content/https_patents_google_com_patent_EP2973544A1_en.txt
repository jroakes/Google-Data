EP2973544A1 - Language modeling of complete language sequences - Google Patents
Language modeling of complete language sequencesInfo
- Publication number
- EP2973544A1 EP2973544A1 EP13798880.4A EP13798880A EP2973544A1 EP 2973544 A1 EP2973544 A1 EP 2973544A1 EP 13798880 A EP13798880 A EP 13798880A EP 2973544 A1 EP2973544 A1 EP 2973544A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- queries
- component
- determining
- language
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000012549 training Methods 0.000 claims abstract description 261
- 238000000034 method Methods 0.000 claims abstract description 57
- 238000004590 computer program Methods 0.000 claims abstract description 15
- 238000013518 transcription Methods 0.000 claims description 136
- 230000035897 transcription Effects 0.000 claims description 136
- 230000004044 response Effects 0.000 claims description 28
- 238000012545 processing Methods 0.000 claims description 10
- 230000008569 process Effects 0.000 description 19
- 238000004891 communication Methods 0.000 description 5
- 238000010586 diagram Methods 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 4
- 230000003993 interaction Effects 0.000 description 2
- 239000003550 marker Substances 0.000 description 2
- 239000000203 mixture Substances 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 244000141353 Prunus domestica Species 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 238000009472 formulation Methods 0.000 description 1
- 239000012634 fragment Substances 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000013138 pruning Methods 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
Definitions
- language models are often trained using a set of training data that include examples of language usage.
- language models include statistical data about the words or other language units that make up utterances in the training data.
- a language model can store probability data for assigning scores to a defined set of language sequences.
- Each of the language sequences can be a complete user input sequence previously provided by one or more users, such as a complete phrase, a complete sentence, a complete query, or a complete text message.
- the language model can store a probability based on the frequency at which the language sequence occurs in its entirety in a set of training data. For example, the language model may store probabilities for particular complete language sequences that occur with a high frequency in the training data.
- a computing system can use the language model in speech recognition, to assign a probability to a candidate transcription for an utterance.
- the candidate transcription is one of the predetermined set of language sequences
- the computing system can use the language model to assign a probability to the candidate transcription as a whole (e.g., using a stored probability corresponding to the candidate transcription), rather than calculating the probability based on, for example, multiple conditional probabilities for components (e.g., n-grams) of the candidate transcription.
- a first candidate transcription for one or more utterances receives a first candidate transcription for one or more utterances; determining that the first candidate transcription is one of a predetermined set of language sequences associated with a first component of a language model; in response to determining that the candidate transcription is one of the predetermined set of language sequences associated with a first component of the language model, determining a first score for the first candidate transcription using the first component of the language model; evaluating the first candidate transcription based on the first score; receiving a second candidate transcription for the one or more utterances;
- Training the second component of the language model includes generating data indicating a second probability distribution for which a second sum of probabilities of occurrence of the selected queries is a second value.
- Determining the adjustment data includes determining a weighting value based on the first value and the second value.
- Determining the adjustment data includes determining a weighting value to apply to output of the second component, the weighting value equalizing a portion of a probability distribution of the second component with a corresponding portion of a probability distribution of the first component.
- Training the second component of the language model based on the training data includes training an n-gram model.
- accessing training data indicating queries submitted by one or more users includes (i) accessing first training data indicating first queries associated with a first geographical area and (ii) accessing second training data indicating second queries associated with a second geographical area that is larger than the first geographical area.
- Determining, for each of the queries, a count of the number of times the training data indicates the query was submitted includes determining, for each of the first queries, a count of a number of times the first training data indicates the query was submitted. Selecting the proper subset of the queries based on the counts includes selecting queries from among the first queries associated with the first geographical area.
- Training the first component of the language model based on the counts includes training the first component based on the counts indicating the number of times the first training data indicates that the selected queries were submitted.
- Training the second component of the language model based on the training data includes training the second component of the language model based on the second training data indicating queries associated with the second geographical area.
- These and other embodiments may each optionally include one or more of the following features. For instance, receiving a query, determining that the received query is associated with the first geographical area, and in response to determining that the received query is associated with the first geographical area, selecting the first component from among a plurality of language models corresponding to different geographical areas. Using the first component to evaluate one or more candidate transcriptions that are included in the selected queries, and using the second component to evaluate one or more candidate transcriptions that are not included in the selected queries.
- Receiving a candidate transcription for one or more utterances determining that the candidate transcription is one of the selected queries; in response to determining that the candidate transcription is one of the selected queries, determining a score for the candidate transcription using the first component of the language model; and evaluating the candidate transcription based on the score.
- determining that the candidate transcription is not one of the selected queries in response to determining that the candidate transcription is not one of the selected queries, determining a score for the candidate transcription using the second component of the language model; and evaluating the candidate transcription based on the score.
- Advantageous implementations can include one or more of the following features.
- the speed and accuracy of speech recognition may be improved.
- the perplexity of a language model can be reduced.
- the probabilities indicated by a language model can reflect the frequency of occurrence of the language sequences in training data better than an n-gram model.
- FIG. 2 is a flow diagram that illustrates an example of a process for training a language model.
- FIGS. 3-5 are flow diagrams that illustrate examples of processes for using a language model in speech recognition.
- FIG. 1 is a diagram that illustrates an example of a system 100 for training a language model 120.
- the system 100 includes a computing system 1 10 that communicates with one or more data storage devices 1 12.
- FIG. 1 also illustrates a flow of data, shown as stages (A) to (F), which represent a flow of data. Stages (A) to (F) may occur in the illustrated sequence, or in a sequence that is different from the illustrated sequence.
- the computing system 1 10 can generate a language model 120 that includes two components, a first component that can assign scores to a defined set of language sequences, and a second component that can assign scores to any language sequence.
- the defined set of language sequences can be complete segments of language, such as full sentences or complete queries.
- the set can be a set of high-frequency queries that occur in training data.
- the probability score that the first component assigns to a query can be based on the relative frequency that the query occurs in its entirety in the training data (e.g., as a user's entire query string, not as a portion or subset of a query). Because the first component assigns probabilities based on the actual frequency at which entire queries occur in the training data, the first component may predict the likelihood of occurrence of the high-frequency queries better than other types of language models, such as n-gram models.
- the second component can be a language model that can assign probabilities to the high-frequency queries as well as other queries.
- the second component may be capable of assigning a probability (e.g., a non-zero probability) to any language sequence, for example, using the chain rule to combine multiple conditional probabilities.
- the second component may be, but is not limited to, an n-gram language model that generates a probability score for a language sequence based on conditional probabilities of n-grams in the language sequence.
- the first component When a language sequence is included in the defined set of high-frequency queries, the first component is used to output a probability score for the language sequence.
- the second component When a language sequence is not in the defined set of high-frequency queries, the second component is used to output a probability score. Backing off from the first component to the second component can ensure that the language model can assign a probability score to any language sequence, beyond just the sequences that occur in the defined set for the first component or even in the training data.
- the computing system 1 10 accesses training data 1 14, which will be used to train, e.g., generate or update, the language model 120.
- the training data 1 14 includes a variety of different language sequences.
- the training data 1 14 is query data that indicates queries 1 16 submitted by users.
- the training data 1 14 may include one or more query logs that indicate queries submitted by any of multiple users over a particular time period, such as one week, one month, one year, etc.
- the training data 1 14 may indicate queries submitted by users in a particular geographical area, or queries in a particular language.
- the query data indicates queries of a particular type or queries from a particular source, for example, voice queries (e.g., queries input to a device as utterances of users).
- voice queries e.g., queries input to a device as utterances of users.
- the voice queries are queries detected by devices in a particular class or category of device, such as mobile devices (e.g., phones, smartphones, or other hand held devices).
- the voice queries may be queries input at devices produced by one or more particular manufacturers, or queries input at devices of one or more particular models.
- the training data 1 14 is standardized.
- the text of the queries can be adjusted to a common format, misspellings may be corrected, plurals can be changed to singular and vice versa, etc.
- the computing system 1 10 determines the frequency that the various language sequences, e.g., the queries 1 16, occur in the training data 1 14. In the illustrated example, the computing system 1 10 determines counts 1 18 representing the number of times that the respective queries 1 16 were submitted. For example, the computing system 1 10 identifies the different queries 1 16 indicated by the training data 1 14, and counts the number of instances of each query 1 16 in the training data 1 14.
- the queries 1 16 are illustrated as including a sequence beginning marker, " ⁇ S>,” and a sequence ending marker, " ⁇ /S>.” These indicators show that the queries 1 16 are entire queries, each representing a user's entire query string, and do not represent substring portions of queries.
- the computing system 1 10 can parse the training data 1 10 to determine appropriate positions of sequence beginnings and endings. For example, the computing system 1 10 may designate beginning and ending markers so that language sequences each encompass the entire text of an individual log entry, record, or message. In addition, or as an alternative, the computing system 1 10 may designate beginning and ending markers corresponding to the positions of predetermined punctuation symbols (e.g., periods, commas, or other delimiters), metadata, formatting characteristics, and so on. Other techniques for parsing the training data 1 14 may also be used.
- predetermined punctuation symbols e.g., periods, commas, or other delimiters
- Each count 1 18 represents the number of times that users have submitted the corresponding query in its entirety. For example, each count 1 18 indicates a number of times that users submitted a language sequence that includes all of the terms of the corresponding query 1 16, in the order the terms appear in the corresponding query 1 16, and without inclusion of any terms that are not included in the corresponding query 1 16 (e.g., without any additional terms before, after, or between the terms of the corresponding query 1 16).
- the computing system 1 10 determines a count of 758 for the query " ⁇ S> weather today ⁇ /S>," showing that the training data 1 14 indicates that this query was submitted 758 times. Similarly, the computing system 1 10 determines that the query " ⁇ S> great vacation spots ⁇ /S>" was submitted 676 times, and so on.
- the computing system 1 10 also determines a total count 1 19 of the number of query submissions (e.g., combined number of instances of all queries 1 16) indicated by the training data 1 14.
- the total count 1 19 is equal to the sum of all of the counts 1 18.
- the computing system 1 10 may count only non-overlapping text segments in the training data 1 14.
- Different queries 1 16 may include some of the same terms, but each particular text segment in the training data 1 14 only contributes to the count 1 18 for a single query 1 16. That is, when a segment of text in the training data 1 14 is counted as an instance of a query 1 16, no portion, e.g., substring, of that particular segment of text is used to increment the count 1 18 for any other query 1 16.
- a particular segment of the training data 1 14 may include a query string including four terms, wi w 2 w 3 w 4 .
- the computing system 1 10 when evaluating this portion of the training data 1 14, increments the count 1 18 for the corresponding query 1 16 that includes the four terms.
- the computing system 1 10 then moves on and evaluates the next text segment that occurs in the training data 1 14, without incrementing counts for other queries, such as wi w 2 W3 or w 2 W3 w 4, based on the particular text segment.
- This is different from the manner in which n-gram counts are typically determined for n-gram models.
- a sliding window is passed over text segments and n-grams are considered to occur at multiple overlapping positions of the sliding window.
- a single instance of a term in the training data may contribute to counts for multiple different n-grams.
- the computing system 1 10 selects a set of language sequences from among the language sequences indicated by the training data 1 14.
- the computing system 1 10 can select language sequences based on one or more criteria.
- the computing system 1 10 selects language sequences based on the frequency of occurrence of the language sequences. For example, the computing system 1 10 can select the most frequently occurring language sequences (e.g., by selecting queries 1 16 with the highest counts 1 18).
- language sequences can be selected based on length (e.g., number of terms or words in a language sequence), a geographical location associated with the language sequence, or other factors.
- the computing system 1 10 selects a set 122 of the queries 1 16 based on the counts 1 18, where the set 122 is a proper subset of the queries 1 16.
- the selected set 122 includes the queries 1 16 that occur with the highest frequency in the training data 1 14.
- the computing system 1 10 selects a
- the computing system 1 10 can select N queries 1 16 having the highest counts 1 18, where N is a predetermined integer.
- the computing system 1 10 selects queries 1 16 that have a count 1 18 that satisfies a count threshold 126.
- the computing system 1 10 can select queries 1 16 that have a count 1 18 that meets or exceeds a minimum threshold value.
- the minimum threshold value can be, for example, two, five, ten, or another value.
- the queries 1 16 that do not satisfy count threshold 126, such as queries 1 16 shown in a set 124, are excluded from the selected set 122.
- queries 1 16 of any length may be selected if the corresponding count satisfies the count threshold.
- the queries 1 16 in the set 122 have different lengths, and some of the queries 1 16 in the set 122 may have a length of five, six, seven, or more terms.
- the selected set 122 is selected based on the length of the queries 1 16 (e.g., a number of terms in each query) and/or other criteria.
- the computing system 1 10 may select queries that include no more than a maximum number of terms (e.g., 20 terms, 15 terms, 10 terms, etc.).
- the computing system 1 10 may select queries 1 16 that occur in the training data 1 14 at least a minimum number of times and each include no more than a maximum number of terms.
- the computing system 1 10 uses different count thresholds for queries 1 16 that include different numbers of terms. For example, the computing system 1 10 can select queries that include two terms and have counts of at least fifty, the computing system 1 10 can select queries that include three terms and have counts 1 18 of at least forty, and the computing system 1 10 can select queries that include four terms and have counts 1 18 of at least thirty. For queries 1 16 that include five or more terms, the computing system 1 10 can select queries that have counts 1 18 of at least fifteen.
- the count thresholds can be set so that queries with a higher number of terms have a lower count threshold, increasing the likelihood that longer queries are selected.
- the computing system 1 10 During stage (D), the computing system 1 10 generates a first component 150 of the language model 120. To generate the first component 150, the computing system 1 10 determines a score 130 for each of various language sequences.
- the score 130 can be a relative frequency of occurrence of a language sequence in the training data.
- the scores 130 indicate probabilities of occurrence of the various language sequences.
- the score 130 for a language sequence can be based on the frequency that the language sequence occurs in its entirety in the training data 1 14 as a discrete, complete user input sequence, and not based on conditional probabilities of occurrence of various subsets, e.g., n-grams, of the language sequence.
- the computing system 1 10 generates the scores 130 based on the relative frequencies that the language sequences occur in the training data as complete queries.
- the scores 130 are generated based on the counts 1 18, which indicate the number of times the respective queries 1 16 were submitted.
- the relative frequency of occurrence for a query 1 16 can be determined as the count 1 18 for the query 1 16 divided by the total count 1 19.
- the relative frequency can be determined as the corresponding count 1 18 (e.g., 758) divided by the total count 1 19 for all queries (e.g., 126,333).
- the relative frequencies for the queries 1 16 form a probability distribution where the range of possible outcomes is limited to selection of one of the queries 1 16 occurring in the training data 1 14.
- Each relative frequency is the probability of occurrence of the corresponding query, out of the set of all the queries 1 16.
- the sum of the relative frequencies or probabilities for all queries 1 16 is one, indicating that the scores 130 indicate a proper distribution over the queries 1 16.
- the scores 130 are the relative frequencies or probabilities for the queries 1 16, however other techniques can be used to generate scores that indicate the probability distribution.
- the computing system 1 10 generates the first component 150 to include scores 130 for only a proper subset of the queries 1 16. For many sets of training data, including data for every language sequence would result in an impractically large language model. To efficiently store data for queries that are likely to occur again, the computing system 1 10 stores scores 130 for only the selected queries 1 16 in the set 122. As discussed above, the queries 1 16 in the set 122 can be the most frequently occurring queries 1 16, or can be selected according to other criteria. The first component 150 does not include scores for queries 1 16 outside the set 122.
- scores 130 are illustrated for queries 1 16 in the set 124, indicating that these queries 1 16 are allocated a non-zero probability of occurrence in the probability distribution for the first component 150. Nevertheless, the computing system 1 10 does not need to calculate or store scores for the queries 1 16 in the set 124. To conserve storage space and reduce computational costs, the first component 150 may omit data identifying the queries 1 16 in the set 124 and include scores 130 for the queries in the set 124.
- the first component 150 includes sequence data 152 that indicates the queries 1 16 in the set 122, e.g., a list of the highest-frequency queries 1 16 in the training data 1 14.
- the first component 150 also includes probability data 154 that indicates a portion of the probability distribution for the queries 1 16 occurring in the training data.
- the probability data 154 includes scores 130 for the queries 1 16 in the set 122, and not for queries outside the set 122. As a result, the first component 150 is only used to queries 1 16 in the set 122.
- the first component 150 can provide a score that indicates a probability of occurrence of the language sequence as a complete user input sequence or submission, e.g., not as a portion or proper subset of a larger language sequence.
- a score indicates a probability of occurrence of the language sequence as a complete user input sequence or submission, e.g., not as a portion or proper subset of a larger language sequence.
- an entire query string of a query submitted by a user can be a complete user submission.
- Another example of a complete user submission is the entirety of a message, such as a short message service (SMS) text message, an e-mail message, or a social networking post or status update.
- SMS short message service
- the language sequences in the predetermined set can include different numbers of terms.
- each language sequence can be a complete sentence, or other defined segment of natural language.
- the language sequences can be, for example, sentence fragments, complete phrases, complete sentences, or complete paragraphs within training data.
- the boundaries of a complete language sequence can be indicated by, for example, punctuation, grammar, formatting, metadata, and/or other factors.
- a period, question mark, exclamation mark, or other punctuation may indicate the end of a language sequence and the end of a language sequence and/or beginning of a new language sequence.
- the beginning and end of a document, or beginning and end of a section within a document can indicate the beginning and end of a language sequence.
- complete language sequences may be delimited by a period of silence or noise (e.g., a pause in dictation) of at least a minimum duration. Accordingly, in a dictation setting, a transcription for utterances occurring between silences of a predetermined duration can be considered a complete language sequence.
- a period of silence or noise e.g., a pause in dictation
- stage (E) the computing system 1 10 generates a second
- the range of possible outcomes, e.g., different language sequences with non-zero probability of occurrence, for the second component 160 is not limited to the queries 1 16 in the set 122, or in some
- the second component 160 may be used to assign scores to a larger set of language sequences than the first component 150.
- the second component 160 can be configured to assign a score to any language sequence, including the queries 1 16 in the set 122, queries 1 16 in the set 124, and queries that do not occur in the training data 1 14.
- the second component 160 is needed so that the language model 120 can assign a score to other language sequences.
- the language model 120 may need to be able to assign scores to any of the queries 1 16, to proper subsets of the terms in a query 1 16, and to language sequences that did not occur in the training data.
- the second component 160 provides the flexibility to allow the language model 120 to assign a score to a larger set of language sequences than those stored in the first component 150. As discussed below, the second component 160 can assign scores to all of the queries 1 16 in the set 122, as well as others. This ability can be used to calibrate the language model 120 as discussed further below.
- the second component 160 can be an n-gram model, e.g., unigram model, bigram model, trigram model, etc.
- the second component 160 can include n-gram data 162 that identifies a set of n-grams 132 occurring within the training data 1 14.
- the second component 160 also includes probability data 164 that can be used to assign scores to language sequences composed of one or more of the n-grams.
- the probability data 164 can indicate conditional probabilities that indicate the probability of occurrence of a particular term given the previous occurrence of one or more other terms in a sequence.
- the computing system 1 10 generates a bigram model based on the queries 1 16.
- Each of the n-grams 132 is a bigram (e.g., an n- gram where n is two).
- the second component 160 can include data about n-grams for multiple values of n, for example, including data about probabilities for unigrams, bigrams, and trigrams.
- the computing system 1 10 identifies various bigrams within the queries 1 16. For example, the query "new movie trailers” includes the bigrams "new movie” and "movie trailers.” The computing system 1 10 then determines n-gram counts 166 of the number of times that the respective bigrams occur in the training data 1 14.
- the n-gram counts 166 do not necessarily indicate only the number of times the corresponding n-gram 132 occurs as a complete query 1 16. Rather, each count 166 indicates the number of times the corresponding n- gram 132 appears at any position within a query 1 16, even if the n-gram 132 is only a portion of a larger query. For example, the query " ⁇ S> new movie trailers ⁇ /S>" occurs 536 times in the training data 1 14 as a complete query, but the bigram "new movie" occurs a greater number of times.
- the n-gram count 166 for the bigram "new movie” also reflects instances of the bigram in all queries 1 16, including queries such as “new movie showtimes,” “new movie reviews,” “tickets new movie,” and so on.
- the exact query “ ⁇ S> weather today ⁇ /S>” occurs 758 times in the training data 1 14
- the n-gram count 166 for the bigram "weather today” is 1958.
- the n-gram count 166 includes instances of the bigram within queries such as “ ⁇ S> weather today new york ⁇ /S>” and “ ⁇ S> weather today boston ⁇ /S>,” while those instances are excluded from the count 1 18 because they are not instances of " ⁇ S> weather today ⁇ /S>” as a complete query.
- the language sequence boundaries, e.g., indicated by ⁇ S> and ⁇ /S>, for queries that include the bigram do not necessarily align with the language sequence boundaries for the full query including the same terms as the bigram.
- the computing system 1 10 determines conditional probabilities for the n-grams 132. For example, the computing system 1 10 determines scores 168 that indicate a probability that the last term of an n-gram 132 will occur, given the occurrence of the preceding term(s) in the n-gram 132. For example, for the bigram "new movie,” the corresponding score 168 can indicate a probability that, given the occurrence of the term "new,” the next term in a language sequence will be "movie.” [0058] When the second component 160 is used to evaluate a language sequence, the computing system 1 10 or other system can use the scores 168 to determine a probability for a language sequence composed of various n-grams 132. For example, due to the chain rule, the product of multiple conditional probabilities for the various n-grams 132 that occur in a language sequence can represent the probability of occurrence of the language sequence.
- the computing system 1 10 can use any of various smoothing techniques when generating the second component 160. For example, the counts 166, the scores 168, or other parameters can determined to permit the n-gram component 160 to assign a probability to any language sequence, even if the language sequence does not appear in the training data 1 14.
- the computing system 1 10 generates the second component 160 using all of the training data 1 14. In some implementations, the computing system 1 10 generates the second component 160 using only a portion of the training data 1 14, for example, the training data 1 14 excluding instances of the queries 1 16 selected for the set 122.
- the computing system 1 10 generates adjustment data 170 that calibrates the language model 120.
- the adjustment data 170 can adjust the output of the second component 160 to be more consistent with the output of the first component 150.
- the adjustment data 170 can cause the total probability that the second component 160 assigns to the queries in the set 122 to be equal to the total probability that the first component 150 assigns to the same queries. In this manner, the adjustment data 170 can ensure that the total probability for all the queries 1 16 in the training data 1 14 equal one.
- the scores 130 indicate that the probabilities or relative frequencies of all queries 1 16 occurring in the training data sum to 1 .
- the scores 168 also indicate that probabilities for all queries 1 16 occurring in the training data also sum to 1 .
- the two components 150, 160 may assign different fractions to the queries 1 16 in the set 122.
- the probability data 164 may indicate that the sum of probabilities for the queries 1 16 in the set 122 is 0.75, or, that there is a 75% chance that a random query is one of the queries in the set 122.
- the remaining share of the probability distribution (e.g., 0.25) is the sum of probabilities for the queries 1 16 outside the set 122.
- the sum of probabilities that the first component 150 provides for the queries 1 16 in the set 122 may be different.
- the relative frequency of queries 1 16 in the training data 1 14 e.g., the counts 1 18
- the second component 160 overestimates the probabilities of the queries 1 16 outside the set 122, by allocating 25% of the probability distribution rather than the 20% that corresponds to their actual frequency of occurrence in the training data 1 14.
- the computing system 1 10 may determine one or more weighting values to set the combined probabilities for the queries 1 16 outside the set 122 to 0.2, rather than 0.25.
- a weighting value can be determined that scales the probability scores produced by the second component 160.
- the computing system 1 10 can determine the total probability, P1 , that the first component 150 assigns to the queries 1 16 in the set 122.
- the total can be the sum of the scores 130.
- the total probability can be the total number of instances of queries 1 16 in the set 122 (e.g., a sum of the counts 1 18 for the queries 1 16 in the set 122) divided by the total number of query instances in the training data 1 14 (e.g., the total count 1 19).
- the computing system 1 10 can also determine a total probability, P2, that the second component 160 assigns to the queries 1 16 in the set 122, for example, as a sum of the probability scores produced by the n-gram model for the queries 1 16 in the set 122.
- the weighting value can then be multiplied with each probability score produced by the second component 160 to bring the probabilities in line with those of the first component 150.
- the language model 120 With output of the second component 160 adjusted by the adjustment data 170, the language model 120 as a whole represents a proper probability distribution modeling the characteristics of the training data. For example, a sum of probabilities provided by the second component 160 for queries 1 16 outside the set 122, each scaled by the weighting value, plus a sum of probabilities provided by the first component 150 for queries 1 16 in the set 122 can equal one.
- a query q is defined as ⁇ S>w-
- the query q has an integer number of terms, x.
- the probability of occurrence of the query q is expressed as P(q), and ma be calculated as shown below, in Equation 1 :
- set S is a defined set of queries or other language
- Equation 1 the term f(q) is the relative frequency of occurrence of the query q among the queries in the training data.
- the term P n -gram(q) is the probability of occurrence of the query q indicated by an n-gram model or other model, e.g., the second component 160.
- the term a is a weighting value that normalizes the model, e.g., adjustment data 170.
- the set S is defined as the set of queries for which C(q) > k x and x ⁇ M, where: C(q) is an absolute count of occurrences of the query q in the training data 1 14; k x is a count threshold value for queries having a number of terms equal to x; and M is an integer representing a maximum number of terms.
- the set S is defined based on other criteria, for example, based on a device associated with a language sequence or a geographical location associated with a language sequence.
- the weighting value a is determined as shown below, in Equation 2:
- the computing system 1 10 prunes the first component 150 using the second component 160. After the second component 160 is generated, one or more of the queries 1 16 in the set 122 may be removed, reducing the size of the first component 150. For each query in the set 122, the computing system 1 10 can evaluate a probability score assigned by the first component 150 with respect to a probability score for the same query assigned by the second component 160. When the different components 150, 160 assign probabilities to a query, and the probabilities do not satisfy a desired relationship the query can be removed from the set 122. In some implementations, queries are
- ⁇ is a scaling factor.
- ⁇ may be equal to 1 , indicating that queries 1 16 for which the second component 160 assigns a higher probability than the first component 150 are removed from the set 122.
- the computing system 1 10 may re-calculate various parameters of the language model 120. For example, when the set 122 changes, the computing system can re-calculate the adjustment data 170 to calibrate the second component 160 to the changed first component 150.
- the computing system 1 10 stores the first component 150, the second component 160, and the adjustment data 170.
- the computing system 1 10 or another system can use the language model 120 to evaluate language sequences, such as candidate transcriptions for voice queries, as discussed further below.
- using the combination of the first component 150 and the second component 160 reduces the perplexity of the language model 120, compared to an n-gram model alone.
- the combined or hybrid language model 120 may additionally increase accuracy in some implementations.
- location-specific voice queries may be common in one city, for example, due to references to particular landmarks, attractions, or customs of the city, but may not be common elsewhere.
- the location- specific queries may have a high-frequency with respect to the total queries from the city, but may have a very low frequency with respect to queries received from the county, state, or country.
- the first component 150 can be location-specific.
- the queries 1 16 in the set 122 can be selected based on the location from which the query was submitted.
- the computing system 1 10 can select queries 1 16 that were submitted using a device located in a particular geographical area.
- the first component 150 can store the location-specific queries and corresponding probability scores.
- the probability scores for the location-specific queries can be based on a particular subset of training data having queries from the geographical area, in order to reflect the relative frequency among for that geographical area.
- the language-specific first component 150 can be used to assign probability scores.
- the computing system 1 10 can use a general model, such as an n-gram model trained using state-wide or nation-wide training data.
- the computing system 1 10 uses a different language model as the second component 160, for example, an existing language model, a language model trained by another system, or a language model trained using different training data 1 14.
- a different language model that can assign probability scores to the queries 1 16 in the set 122, as well as other queries, may be used as the second component 160.
- adjustment data 170 that normalizes the output of the second component 160 relative to the first component 150 can be determined as discussed above, regardless of how the second component is generated or what training data 1 14 is used. As a result, different sets of training data can be used to train the different components 150, 160.
- the training data 1 14 can indicate text messages, and the counts 1 18 can be counts of the number of times the respective text messages occur in their entirety.
- the sequence data 152 of the first component 150 can indicate a list of the most common text messages in the training data 1 14.
- the probability data 154 can indicate the probability of occurrence of the most common text messages, according to their respective frequency of occurrence in the training data.
- the sequence data 152 of the first component 150 can indicate the most common full sentences that occur in a set of training data, and the probability data 154 can indicate the probability of occurrence of the respective most common full sentences.
- FIG. 2 is a flow diagram that illustrates an example of a process 200 for training a language model.
- the process 200 is described as being performed by the computing system 1 10, but can be performed by one or more other computing devices, including combinations of multiple computing devices.
- the computing system 1 10 accesses training data (202).
- the training data indicates multiple complete language sequences, for example, full sentences or complete submissions by users.
- the training data indicates queries submitted by one or more users.
- the training data can include query logs indicating voice queries spoken by different users.
- the computing system 1 10 determines a count for each of the language sequences (204). For example, the computing system 1 10 determines, for each of a set of queries in the training data, a count of the number of times the training data indicates the query was submitted.
- the computing system 1 10 selects a proper subset of the complete language sequences (206). For example, a proper subset of the language sequences, e.g., queries, in the training data are selected based on the counts. Language sequences that have a count that satisfies a count threshold can be selected. Language sequences having a count that equals or exceeds a minimum threshold value can be selected, and the minimum threshold value can be greater than one. As a result, the proper subset can include the language sequences, such as queries, that occur with highest frequency in the training data, e.g., have the highest counts.
- queries having a first number of terms are selected based on a first threshold
- queries having a second number of terms are selected based on a second threshold.
- the second number of terms can be different from the first number of terms
- the second threshold can be different from the first threshold
- the computing system 1 10 trains a first component of a language model (208).
- the first model indicates probabilities based on relative frequencies of occurrence of the complete language sequences.
- the first component can be trained based on the counts, and can include first probability data indicating relative frequencies that the selected language sequences, e.g. selected queries, occur in the training data.
- the first component can include data that identifies the selected language sequences, e.g., a list of the language sequences in the proper subset.
- the first probability data can be a set of scores for the selected language sequences, where each score indicates a relative frequency of the corresponding query.
- training the first component includes determining, for each of a set of selected queries, a score indicating a relative frequency of occurrence of the selected query, as a complete query, in the training data.
- the computing system 1 10 trains a second component of a language model (210).
- the second component can include second probability data for assigning scores to language sequences that are not included in the selected language sequences. For example, when the language sequences are queries, the second component can be capable of assigning scores to at least some queries in addition to selected queries for which the first component includes scores. In some instances, the second component may be configured to assign a score to any sequence of terms.
- the second component can also assign scores to each of the queries that the first component assigns scores, which facilitates calibration of the first component and the second component.
- the second component can be an n-gram model. To train the n-gram model, the computing system 1 10 can determine conditional probabilities that each indicate a probability of occurrence of a term given occurrence of one or more other terms. The second component can be trained based on the training data as a whole, or based on a proper subset of the training data. For example, the proper subset of training data can exclude instances of the selected language sequences. [0088] In some implementations, the second component is a standard back-off language model. The second component can assign a probability to any language sequence, including the selected language sequences. The second component can assign a probability to each of the selected language sequences, which permits the second component to be calibrated relative to the first component.
- the second component can assign probability incrementally, using the chain rule, e.g., that the probability of a sequence equals the product of conditional probabilities of subcombinations of terms in the sequence, sometimes expressed as P(w-
- ...w n )
- ...w n )
- the computing system 1 10 determines adjustment data that normalizes the second probability data with respect to the first probability data (212).
- the adjustment data can be a weighting value to apply to output of the second component.
- the weighting value can equalize a portion of a probability distribution of the second component with a corresponding portion of a probability distribution of the first component.
- the adjustment data can equalize the share of probability that the second component assigns to language sequences other than the selected language sequences with the share of probability that the first component assigns to those sequences.
- the adjustment data can be generated by determining an aggregate share of probability that each of the first component and the second component allocate to the selected language sequences, and determining a weighting value based on the two different aggregate shares of probability.
- the computing system 1 10 stores the first component, the second component, and the adjustment data (214).
- training the first component includes generating a first probability distribution over a set of possible outcomes limited to the language sequences (e.g., queries) occurring in the training data
- training the second component includes generating a second probability distribution for which the range of possible outcomes is not limited to a defined set of language sequences (e.g., queries).
- the computing system 1 10 can determine one or more weighting values for weighting the second probability distribution relative to the first probability distribution to form combined probability distribution.
- the sum of probabilities for the language sequences (e.g., queries) sums to 1 .
- training the first component of the language model includes generating data indicating a first probability distribution for which a first sum of probabilities of occurrence of the selected language sequences is a first value.
- Training the second component of the language model includes generating data indicating a second probability distribution for which a second sum of probabilities of occurrence of the selected language sequences is a second value.
- Determining the adjustment data includes determining a weighting value based on the first value and the second value.
- the process 200 includes determining a first score for a particular query using the first component of the language model, determining a second score for the particular query using the second component of the language model, and determining that the first score and the second score do not satisfy a predetermined relationship.
- the particular query is removed from the selected queries to generate an altered set of selected queries.
- second adjustment data can be determined based on the altered set of selected queries.
- accessing training data includes (i) accessing first training data indicating first language sequences associated with a first geographical area, e.g., a city, county, or state, and (ii) accessing second training data indicating second queries associated with a second geographical area that is larger than the first geographical area.
- first geographical area e.g., a city, county, or state
- second training data indicating second queries associated with a second geographical area that is larger than the first geographical area.
- the second geographical area can include the first geographical area.
- the counts can be determined based on the first training data, and the selected language sequences can be selected from among the first queries associated with the first geographical area.
- the first language model can be trained based on the counts indicating the number of times the first training data indicates that the selected queries were submitted, and the second component of the language model can be trained based on the second training data indicating queries associated with the second geographical area, or based on the first training data and the second training data.
- the computing system receives a query and determines that the received query is associated with the first geographical area. In response to determining that the received query is associated with the first geographical area, the computing system 1 10 selects the first component from among a plurality of language models or language model components corresponding to different geographical areas. The computing system 1 10 uses the first component to evaluate one or more candidate transcriptions that are included in the selected queries indicated by the first component, and uses the second component to evaluate one or more candidate transcriptions that are not included in the selected queries.
- FIG. 3 illustrates a process 300 for evaluating a candidate transcription.
- the process 300 can be performed by one or more processing devices, for example, a server, a collection of multiple servers, a client device, a collection of processors within a client device, or a combination or sub-combination thereof.
- the process 300 is described as being performed by the computing system 1 10, but can be performed by other devices.
- the computing system 1 10 receives a candidate transcription for one or more utterances (302).
- the computing system 1 10 looks up the candidate transcription in the first component of a language model (304). For example, the computing system 1 10 determines whether a list of language sequences included in the first component includes the candidate transcription.
- the computing system 1 10 determines whether the candidate transcription was found in the set of language sequences stored by the first component (306).
- the computing system 1 10 determines a probability score for the candidate transcription using the first component (308). If the candidate transcription was not found, the computing system determines a probability score for the candidate transcription using a second component of the language model (310). For example, the computing system 1 10 can "back off' to a generalized n-gram model. The computing system 1 10 normalizes the probability score from the second component (312), for example, by multiplying the probability score by a weighting value that calibrates the second component relative to the first component. The computing system 1 10 then evaluates the candidate transcription using the probability score (314), which is either a score from the first component or a normalized score from the second component. [0098] FIG.
- the process 400 can be performed by one or more processing devices, for example, a server, a collection of multiple servers, a client device, a collection of processors within a client device, or a combination or sub-combination thereof.
- the process 400 is described below as being performed by the computing system 1 10, but the process 400 may be performed by other devices.
- the computing system receives a candidate transcription for audio data containing one or more utterances (402).
- the one or more utterances may include a voice query spoken by a user and detected by a microphone of a mobile device, such as a phone.
- the computing system determines that the candidate transcription is one of a predetermined set of language sequences (404).
- the predetermined set of language sequences can be a set of language sequences stored in or otherwise associated with a first component of a language model that includes multiple components.
- the computing system can determine that the first component of the language model stores a probability corresponding to the candidate transcription, indicating that the candidate transcription is one of the predetermined set of language sequences.
- the first component of the language model can be trained to determine probabilities for each of the language sequences in the predetermined set.
- the first component is trained to determine probabilities only for the language sequences in the predetermined set.
- the first component may include data indicating respective probabilities of occurrence of the language sequences, determined based on respective counts of a number of times each language sequence occurs in training data as a complete language sequence (e.g., as a discrete submission by a user, such as a query or text message, rather than occurring as only a portion of a larger phrase).
- the first component can assign the probability for the candidate transcription as a whole, without building the probability using conditional probabilities of subsets (e.g., n-grams) of the candidate
- the predetermined set language sequences can be complete language sequences that satisfy one or more thresholds for length and/or frequency of occurrence.
- the predetermined set of language sequences can be a set of voice queries submitted by one or more users, where each of the voice queries occurs at least a minimum number of times in training data used to train the first component of the language model.
- the predetermined set language sequences can be complete language sequences submitted by one or more users, for example, complete text messages, complete e-mail messages, complete phrases, complete sentences, etc.
- the predetermined set of language sequences includes language sequences of a variety of lengths.
- the predetermined set of language sequences includes language sequences of a variety of lengths.
- predetermined set of language sequences can include sequences that are respectively formed of one word, two words, three words, four words, and five words.
- one or more of the language sequences can include six or more words.
- the computing system determines a score for the first candidate transcription using the first component of the language model (406). For example, the computing system 1 10 can access a stored probability based on the frequency of occurrence of the candidate transcription as a complete user input sequence in a set of training data.
- the computing system 1 10 uses the score to evaluate the candidate transcription (408). For example, the computing system 1 10 can rank the first candidate transcription among a set of multiple candidate transcriptions based on the assigned score.
- the multiple candidate transcriptions can be, for example, a set of N candidate transcriptions having the highest likelihood of occurrence according to a first language model, where N is an integer, such as five, ten, twenty, fifty, etc.
- the computing system 1 10 may then re-score the multiple candidate transcriptions using scores determined using the hybrid model including the first component and second component described above.
- the computing system 1 10 can combine scores of multiple language models and/or acoustic models, and may rank the candidate transcriptions based on the combined scores.
- FIG. 5 illustrates a process 500 for evaluating a candidate transcription.
- the process 500 can be performed by one or more processing devices, for example, a server, a collection of multiple servers, a client device, a collection of processors within a client device, or a combination or sub-combination thereof.
- the process 500 is described below as being performed by the computing system 1 10, but the process 500 may be performed by other devices.
- the computing system 1 10 receives a candidate transcription for audio data containing one or more utterances (502).
- the computing system 1 10 determines that the candidate transcription is not one of a predetermined set of language sequences (504).
- the predetermined set of language sequences can be a set for which a first language model component includes corresponding probability scores.
- the computing system 1 10 determines a score for the candidate transcription using a second language model component (506). The computing system 1 10 then evaluates the candidate transcription base on the score (508).
- Embodiments of the invention and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification
- Embodiments of the invention may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer-readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable medium may be a non-transitory computer readable storage medium, a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- a computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks;
- magneto optical disks and CD ROM and DVD-ROM disks.
- the processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments of the invention may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the invention may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the invention, or any combination of one or more such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- HTML file may be replaced by an XML, JSON, plain text, or other types of files.
- XML XML
- JSON XML
- plain text XML
- table or hash table other data structures (such as spreadsheets, relational databases, or structured files) may be used.
Abstract
Description
Claims
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361781140P | 2013-03-14 | 2013-03-14 | |
US13/875,406 US9786269B2 (en) | 2013-03-14 | 2013-05-02 | Language modeling of complete language sequences |
PCT/US2013/070732 WO2014158239A1 (en) | 2013-03-14 | 2013-11-19 | Language modeling of complete language sequences |
Publications (2)
Publication Number | Publication Date |
---|---|
EP2973544A1 true EP2973544A1 (en) | 2016-01-20 |
EP2973544B1 EP2973544B1 (en) | 2019-04-17 |
Family
ID=51531824
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP13798880.4A Active EP2973544B1 (en) | 2013-03-14 | 2013-11-19 | Language modeling of complete language sequences |
Country Status (4)
Country | Link |
---|---|
US (1) | US9786269B2 (en) |
EP (1) | EP2973544B1 (en) |
CN (1) | CN105229723B (en) |
WO (1) | WO2014158239A1 (en) |
Families Citing this family (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3005347A1 (en) * | 2013-05-31 | 2016-04-13 | Longsand Limited | Processing of audio data |
KR20150144447A (en) * | 2014-06-16 | 2015-12-28 | 한국전자통신연구원 | Method and Apparatus for modifying a message |
US9871755B2 (en) * | 2014-10-23 | 2018-01-16 | Facebook, Inc. | Encoding portions of a message |
CN106971728A (en) * | 2016-01-14 | 2017-07-21 | 芋头科技（杭州）有限公司 | A kind of quick identification vocal print method and system |
US9959864B1 (en) * | 2016-10-27 | 2018-05-01 | Google Llc | Location-based voice query recognition |
CN108573697B (en) * | 2017-03-10 | 2021-06-01 | 北京搜狗科技发展有限公司 | Language model updating method, device and equipment |
WO2019217013A1 (en) * | 2018-05-08 | 2019-11-14 | Google Llc | Contrastive sequence-to-sequence data selector |
US10867609B2 (en) * | 2018-05-18 | 2020-12-15 | Sorenson Ip Holdings, Llc | Transcription generation technique selection |
EP3707703A1 (en) * | 2018-11-28 | 2020-09-16 | Google LLC. | Training and/or using a language selection model for automatically determining language for speech recognition of spoken utterance |
KR20190113693A (en) * | 2019-09-18 | 2019-10-08 | 엘지전자 주식회사 | Artificial intelligence apparatus and method for recognizing speech of user in consideration of word usage frequency |
CN112417093B (en) * | 2020-11-11 | 2024-03-08 | 北京三快在线科技有限公司 | Model training method and device |
CN115938351B (en) * | 2021-09-13 | 2023-08-15 | 北京数美时代科技有限公司 | ASR language model construction method, system, storage medium and electronic equipment |
CN117577119B (en) * | 2024-01-17 | 2024-04-05 | 清华大学 | Fake voice detection method, system, equipment and medium integrating large language model |
Family Cites Families (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH05197389A (en) | 1991-08-13 | 1993-08-06 | Toshiba Corp | Voice recognition device |
US5293584A (en) | 1992-05-21 | 1994-03-08 | International Business Machines Corporation | Speech recognition system for natural language translation |
US5613036A (en) | 1992-12-31 | 1997-03-18 | Apple Computer, Inc. | Dynamic categories for a speech recognition system |
US5651096A (en) | 1995-03-14 | 1997-07-22 | Apple Computer, Inc. | Merging of language models from two or more application programs for a speech recognition system |
US6167377A (en) | 1997-03-28 | 2000-12-26 | Dragon Systems, Inc. | Speech recognition language models |
WO2000023983A1 (en) | 1998-10-21 | 2000-04-27 | Koninklijke Philips Electronics N.V. | Method of determining parameters of a statistical language model |
US6477488B1 (en) | 2000-03-10 | 2002-11-05 | Apple Computer, Inc. | Method for dynamic context scope selection in hybrid n-gram+LSA language modeling |
US20020123894A1 (en) * | 2001-03-01 | 2002-09-05 | International Business Machines Corporation | Processing speech recognition errors in an embedded speech recognition system |
US7222074B2 (en) * | 2001-06-20 | 2007-05-22 | Guojun Zhou | Psycho-physical state sensitive voice dialogue system |
AU2003220606A1 (en) | 2002-03-27 | 2003-10-13 | Universiity Of Southern California | Phrase- based joint probability model for statistical machine translation |
JP2004334193A (en) | 2003-05-01 | 2004-11-25 | Microsoft Corp | System with composite statistical and rule-based grammar model for speech recognition and natural language understanding |
US7356526B2 (en) * | 2003-09-30 | 2008-04-08 | International Business Machines Corporation | Estimating the compilation time of a query optimizer |
US7593843B2 (en) * | 2004-03-30 | 2009-09-22 | Microsoft Corporation | Statistical language model for logical form using transfer mappings |
US8666725B2 (en) * | 2004-04-16 | 2014-03-04 | University Of Southern California | Selection and use of nonstatistical translation components in a statistical machine translation framework |
US7447636B1 (en) | 2005-05-12 | 2008-11-04 | Verizon Corporate Services Group Inc. | System and methods for using transcripts to train an automated directory assistance service |
US7599840B2 (en) * | 2005-07-15 | 2009-10-06 | Microsoft Corporation | Selectively using multiple entropy models in adaptive coding and decoding |
US7756708B2 (en) | 2006-04-03 | 2010-07-13 | Google Inc. | Automatic language model update |
US9318108B2 (en) * | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8041568B2 (en) | 2006-10-13 | 2011-10-18 | Google Inc. | Business listing search |
US8135590B2 (en) * | 2007-01-11 | 2012-03-13 | Microsoft Corporation | Position-dependent phonetic models for reliable pronunciation identification |
US20110060587A1 (en) * | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US8615389B1 (en) * | 2007-03-16 | 2013-12-24 | Language Weaver, Inc. | Generation and exploitation of an approximate language model |
US8825466B1 (en) * | 2007-06-08 | 2014-09-02 | Language Weaver, Inc. | Modification of annotated bilingual segment pairs in syntax-based machine translation |
US10496753B2 (en) * | 2010-01-18 | 2019-12-03 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
CN101593518A (en) * | 2008-05-28 | 2009-12-02 | 中国科学院自动化研究所 | The balance method of actual scene language material and finite state network language material |
CN101382937B (en) * | 2008-07-01 | 2011-03-30 | 深圳先进技术研究院 | Multimedia resource processing method based on speech recognition and on-line teaching system thereof |
US8712776B2 (en) * | 2008-09-29 | 2014-04-29 | Apple Inc. | Systems and methods for selective text to speech synthesis |
US20120158705A1 (en) * | 2010-12-16 | 2012-06-21 | Microsoft Corporation | Local search using feature backoff |
US8630860B1 (en) * | 2011-03-03 | 2014-01-14 | Nuance Communications, Inc. | Speaker and call characteristic sensitive open voice search |
US8972260B2 (en) | 2011-04-20 | 2015-03-03 | Robert Bosch Gmbh | Speech recognition using multiple language models |
CN102810311B (en) * | 2011-06-01 | 2014-12-03 | 株式会社理光 | Speaker estimation method and speaker estimation equipment |
US8812474B2 (en) * | 2011-07-14 | 2014-08-19 | Nuance Communications, Inc. | Methods and apparatus for identifying and providing information sought by a user |
US20130086027A1 (en) * | 2011-09-30 | 2013-04-04 | Nuance Communications, Inc. | Techniques for the receipt and processing of user-specified queries |
US9043205B2 (en) * | 2012-06-21 | 2015-05-26 | Google Inc. | Dynamic language model |
US8856007B1 (en) * | 2012-10-09 | 2014-10-07 | Google Inc. | Use text to speech techniques to improve understanding when announcing search results |
-
2013
- 2013-05-02 US US13/875,406 patent/US9786269B2/en active Active
- 2013-11-19 WO PCT/US2013/070732 patent/WO2014158239A1/en active Application Filing
- 2013-11-19 CN CN201380076599.0A patent/CN105229723B/en active Active
- 2013-11-19 EP EP13798880.4A patent/EP2973544B1/en active Active
Also Published As
Publication number | Publication date |
---|---|
CN105229723A (en) | 2016-01-06 |
US9786269B2 (en) | 2017-10-10 |
US20140278407A1 (en) | 2014-09-18 |
CN105229723B (en) | 2019-10-22 |
EP2973544B1 (en) | 2019-04-17 |
WO2014158239A1 (en) | 2014-10-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP2973544B1 (en) | Language modeling of complete language sequences | |
US11875789B2 (en) | Language models using domain-specific model components | |
EP2702586B1 (en) | Cross-lingual initialization of language models | |
CN107430859B (en) | Mapping input to form fields | |
US9437189B2 (en) | Generating language models | |
US8700396B1 (en) | Generating speech data collection prompts | |
US7966171B2 (en) | System and method for increasing accuracy of searches based on communities of interest | |
US8682661B1 (en) | Robust speech recognition | |
US9123333B2 (en) | Minimum bayesian risk methods for automatic speech recognition | |
US20150371633A1 (en) | Speech recognition using non-parametric models | |
US20150269934A1 (en) | Enhanced maximum entropy models | |
CN110291582B (en) | Language model biasing system | |
Aleksic et al. | Improved recognition of contact names in voice commands | |
US9594744B2 (en) | Speech transcription including written text | |
US20200143799A1 (en) | Methods and apparatus for speech recognition using a garbage model | |
US9110880B1 (en) | Acoustically informed pruning for language modeling | |
US8959020B1 (en) | Discovery of problematic pronunciations for automatic speech recognition systems | |
US11232786B2 (en) | System and method to improve performance of a speech recognition system by measuring amount of confusion between words | |
Ni et al. | Investigation of using different Chinese word segmentation standards and algorithms for automatic speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20150828 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAX | Request for extension of the european patent (deleted) | ||
17Q | First examination report despatched |
Effective date: 20161005 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTG | Intention to grant announced |
Effective date: 20180702 |
|
GRAJ | Information related to disapproval of communication of intention to grant by the applicant or resumption of examination proceedings by the epo deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTC | Intention to grant announced (deleted) | ||
INTG | Intention to grant announced |
Effective date: 20181206 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602013054072Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1122425Country of ref document: ATKind code of ref document: TEffective date: 20190515Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R081Ref document number: 602013054072Country of ref document: DEOwner name: GOOGLE LLC, MOUNTAIN VIEW, USFree format text: FORMER OWNER: GOOGLE TECHNOLOGY HOLDINGS LLC, MOUNTAIN VIEW, CALIF., US |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20190417 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190817Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190717Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190717Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190718Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1122425Country of ref document: ATKind code of ref document: TEffective date: 20190417 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190817 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602013054072Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
26N | No opposition filed |
Effective date: 20200120 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: TRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20191119Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20191130Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20191130 |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20191130 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20191119 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20191130 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20131119Ref country code: MTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190417 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230505 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20231127Year of fee payment: 11 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20231127Year of fee payment: 11Ref country code: DEPayment date: 20231129Year of fee payment: 11 |