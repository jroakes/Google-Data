DE202017104895U1 - Hotword detection on multiple devices - Google Patents
Hotword detection on multiple devices Download PDFInfo
- Publication number
- DE202017104895U1 DE202017104895U1 DE202017104895.0U DE202017104895U DE202017104895U1 DE 202017104895 U1 DE202017104895 U1 DE 202017104895U1 DE 202017104895 U DE202017104895 U DE 202017104895U DE 202017104895 U1 DE202017104895 U1 DE 202017104895U1
- Authority
- DE
- Germany
- Prior art keywords
- computing device
- data
- hotword
- audio data
- server
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000001514 detection method Methods 0.000 title description 29
- 238000000034 method Methods 0.000 claims abstract description 65
- 238000012545 processing Methods 0.000 claims abstract description 49
- 238000004590 computer program Methods 0.000 claims abstract description 34
- 230000004044 response Effects 0.000 claims abstract description 31
- 230000009471 action Effects 0.000 claims description 51
- 238000013528 artificial neural network Methods 0.000 claims description 6
- 230000015654 memory Effects 0.000 description 35
- 230000008569 process Effects 0.000 description 32
- 238000013518 transcription Methods 0.000 description 21
- 230000035897 transcription Effects 0.000 description 21
- 238000004891 communication Methods 0.000 description 17
- 230000007958 sleep Effects 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 3
- 230000001413 cellular effect Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 238000012706 support-vector machine Methods 0.000 description 2
- 241001446467 Mama Species 0.000 description 1
- 238000010411 cooking Methods 0.000 description 1
- 238000010586 diagram Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000011435 rock Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000001502 supplementing effect Effects 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Abstract
Computerprogrammprodukt, wobei das Computerprogrammprodukt Code umfasst, der bei Ausführung durch eine oder mehrere Computervorrichtungen, die eine oder die mehreren Computervorrichtungen veranlasst, ein Verfahren auszuführen, wobei das Verfahren umfasst: Empfangen, durch eine Computervorrichtung, von Audiodaten, die einer Äußerung entsprechen; Bestimmen, dass die Äußerung wahrscheinlich ein bestimmtes vordefiniertes Hotword umfasst; als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst, Senden, an einen Server, (i) Daten, die anzeigen, dass die Computervorrichtung wahrscheinlich das bestimmte vordefinierte Hotword empfangen hat, (ii) Daten, welche die Computervorrichtung identifizieren, und (iii) Daten, die eine Gruppe von in der Nähe befindlichen Computervorrichtungen identifizieren, welche die Computervorrichtung umfasst; Empfangen, vom Server, eines Befehls, Spracherkennungsverarbeitung an den Audiodaten zu beginnen; und als Reaktion auf das Empfangen des Befehls, Spracherkennungsverarbeitung an den Audiodaten zu beginnen, Verarbeiten von mindestens einem Abschnitt der Audiodaten unter Verwendung eines automatisierten Spracherkenners auf der Computervorrichtung.A computer program product, the computer program product comprising code that, when executed by one or more computing devices that causes one or more computing devices to perform a method, the method comprising: receiving, by a computing device, audio data that corresponds to an utterance; Determining that the utterance is likely to include a particular predefined hotword; in response to determining that the utterance is likely to include the particular predefined hotword, send, to a server, (i) data indicating that the computing device has likely received the particular predefined hotword, (ii) data identifying the computing device and (iii) data identifying a group of nearby computing devices comprising the computing device; Receiving, from the server, a command to begin speech recognition processing on the audio data; and in response to receiving the command to begin speech recognition processing on the audio data, processing at least a portion of the audio data using an automated speech recognizer on the computing device.
Description
TECHNISCHES GEBIET TECHNICAL AREA
Diese Beschreibung betrifft generell automatisierte Spracherkennung. This description generally relates to automated speech recognition.
ALLGEMEINER STAND DER TECHNIK GENERAL PRIOR ART
Die Realität eines sprachgesteuerten Haushaltes oder einer anderen Umgebung – d. h. eine, in der ein Benutzer nur eine Abfrage oder einen Befehl laut aussprechen muss und ein computerbasiertes System die Abfrage beantwortet und/oder verursacht, dass der Befehl ausgeführt wird – ist da. Eine sprachgesteuerte Umgebung (z. B. Haushalt, Arbeitsplatz, Schule usw.) kann unter Verwendung eines Netzwerks von verbundenen Mikrofonvorrichtungen, die in den verschiedenen Räumen oder Bereichen der Umgebung verteilt sind, implementiert werden. Durch solch ein Netzwerk von Mikrofonen hat ein Benutzer die Möglichkeit, das System von im Wesentlichen irgendwo in der Umgebung mündlich abzufragen, ohne die Notwendigkeit, einen Computer oder eine andere Vorrichtung vor sich oder gar in der Nähe zu haben. Während er in der Küche kocht, kann ein Benutzer beispielsweise das System fragen „Wie viele Milliliter in drei Tassen?“ und als Reaktion eine Antwort vom System z. B. in der Form einer künstlichen Sprachausgabe erhalten. Alternativ könnte ein Benutzer dem System Fragen stellen wie „Wann schließt meine nahest gelegene Tankstelle“, oder beim Vorbereiten, aus dem Haus zu gehen „Sollte ich eine Jacke anziehen?“. The reality of a voice-controlled household or other environment - d. H. one in which a user needs to pronounce only one query or command aloud and a computer-based system responds to the query and / or causes the command to be executed - is there. A voice-controlled environment (e.g., home, work, school, etc.) may be implemented using a network of connected microphone devices distributed in the various rooms or areas of the environment. Through such a network of microphones, a user has the ability to verbally query the system from essentially anywhere in the environment without the need to have a computer or other device in front of or even nearby. For example, while cooking in the kitchen, a user may ask the system "How many milliliters in three cups?" And in response to a response from the system, for example. In the form of an artificial speech output. Alternatively, a user could ask the system questions such as "When does my nearest gas station close" or when preparing to leave the house "Should I wear a jacket?".
Weiter kann ein Benutzer eine Abfrage des Systems anfordern und/oder einen Befehl ausgeben, der die persönlichen Informationen des Benutzers betrifft. Beispielsweise könnte ein Benutzer das System fragen „Wann ist mein Treffen mit John?“ oder das System anweisen „Erinnere mich daran, John anzurufen, wenn ich nach Hause komme“. Further, a user may request a query of the system and / or issue a command concerning the personal information of the user. For example, a user might ask the system "When is my meeting with John?" Or instruct the system "Remind me to call John when I get home."
KURZDARSTELLUNG SUMMARY
Bei einem sprachgesteuerten System ist die Art und Weise, in der der Benutzer mit dem System interagiert, hauptsächlich, wenn nicht ausschließlich, mittels Spracheingabe konzipiert. Daher kann das System, das potenziell alle Äußerungen aufnimmt, die in der Umgebung auftreten, einschließlich denjenigen, die nicht an das System gerichtet sind, eine Art von Erkennung aufweisen, wann irgendeine gegebene Äußerung an das System gerichtet ist, anstatt im Gegensatz dazu z. B. an eine in der Umgebung anwesende Person gerichtet zu sein. Eine Art, das zu erreichen, ist, ein Hotword zu verwenden, das anhand der Übereinstimmung unter den Benutzern in der Umgebung als ein vorbestimmtes Wort reserviert ist, das ausgesprochen wird, um die Aufmerksamkeit des Systems aufzurufen. Bei einer beispielhaften Umgebung ist das Hotword, das verwendet wird, um die Aufmerksamkeit des Systems aufzurufen, die Worte „OK Computer“. Daher wird jedes Mal, wenn die Worte „OK Computer“ gesprochen werden, dies durch ein Mikrofon aufgenommen, zum System transportiert, das Spracherkennungstechniken ausführen oder Audiomerkmale und neuronale Netze verwenden kann, um zu bestimmen, ob das Hotword gesprochen wurde, und, wenn dies so ist, auf einen folgenden Befehl oder eine Abfrage wartet. Dementsprechend weisen an das System gerichtete Äußerungen die allgemeine Form [HOTWORD] [ABFRAGE] auf, wobei „HOTWORD“ in diesem Beispiel „OK Computer“ und „ABFRAGE“ jede Frage, Befehl, Aussage oder andere Anforderung sein kann, die Sprache sein kann, die vom System erkannt, geparst und behandelt wurde, entweder allein oder in Verbindung mit dem Server über das Netzwerk. In a voice-driven system, the way in which the user interacts with the system is mainly, if not exclusively, designed using voice input. Therefore, the system potentially accommodating all utterances that occur in the environment, including those that are not addressed to the system, may have some kind of recognition of when any given utterance is directed to the system, rather than, in contrast, e.g. B. to be addressed to a person present in the environment. One way to accomplish this is to use a hotword that is reserved by the compliance among the users in the environment as a predetermined word that is pronounced to call the attention of the system. In an exemplary environment, the hotword used to call the system's attention is the words "OK Computer". Therefore, each time the words "OK Computer" are spoken, it is picked up by a microphone, transported to the system, which can perform speech recognition techniques or use audio features and neural networks to determine if the hotword has been spoken, and if so so is waiting for a following command or query. Accordingly, utterances directed to the system have the general form [HOTWORD], where "HOTWORD" in this example, "OK computer" and "QUERY" may be any question, command, statement or other requirement that may be language, detected, parsed and handled by the system, either alone or in conjunction with the server over the network.
Innovative Aspekte des hierin beschriebenen Gegenstandes betreffen Computerprogrammprodukte, Systeme und Ausrüstung einschließlich Computerprogrammen, die auf einem Computerspeichermedium codiert sind, zur Hotword-Detektion auf mehreren Vorrichtungen. Verschiedene Implementierungen sind derart konfiguriert, dass die Verwendung von Computerressourcen reduziert ist, wenn sich mehr als eine Vorrichtung nahe einem Benutzer befindet, wenn der Benutzer ein Hotword spricht. Innovative aspects of the subject matter described herein relate to computer program products, systems, and equipment, including computer programs encoded on a computer storage medium, for hotword detection on multiple devices. Various implementations are configured such that the use of computer resources is reduced when there is more than one device near a user when the user is speaking a hotword.
Ein innovativer Aspekt des in dieser Beschreibung beschriebenen Gegenstandes kann in Verfahren verkörpert sein, welche die Aktionen umfassen, Empfangen durch eine Computervorrichtung, von Audiodaten, die einer Äußerung entsprechen; Bestimmen, dass die Äußerung wahrscheinlich ein bestimmtes vordefiniertes Hotword umfasst; als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst, Senden, an einen Server, (i) von Daten, die anzeigen, dass die Computervorrichtung wahrscheinlich das bestimmte vordefinierte Hotword empfangen hat, (ii) von Daten, welche die Computervorrichtung identifizieren, und (iii) von Daten, die eine Gruppe von in der Nähe befindlichen Computervorrichtungen identifizieren, welche die Computervorrichtung umfasst; Empfangen eines Befehls vom Server, Spracherkennungsverarbeitung an den Audiodaten zu beginnen; und als Reaktion auf das Empfangen des Befehls, Spracherkennungsverarbeitung an den Audiodaten zu beginnen, Verarbeiten von mindestens einem Abschnitt der Audiodaten unter Verwendung eines automatisierten Spracherkenners auf der Computervorrichtung. An innovative aspect of the subject matter described in this specification may be embodied in methods including the actions of receiving, by a computing device, audio data corresponding to an utterance; Determining that the utterance is likely to include a particular predefined hotword; in response to determining that the utterance is likely to include the particular predefined hotword, send, to a server, (i) data indicating that the computing device has likely received the particular predefined hotword, (ii) data representing the Identify a computing device, and (iii) data identifying a group of nearby computing devices comprising the computing device; Receiving a command from the server to begin speech recognition processing on the audio data; and in response to receiving the command to begin speech recognition processing on the audio data, processing at least a portion of the audio data using an automated speech recognizer on the computing device.
Diese und andere Ausführungsformen können jeweils optional ein oder mehrere von den folgenden Merkmalen umfassen. Die Aktionen umfassen weiter das Empfangen, von einer zusätzlichen Computervorrichtung, von Daten, die anzeigen, dass die zusätzliche Computervorrichtung konfiguriert ist, auf ein bestimmtes vordefiniertes Hotword anzusprechen; Senden, an die zusätzliche Computervorrichtung, der Daten, die anzeigen, dass die Computervorrichtung konfiguriert ist, auf ein bestimmtes vordefiniertes Hotword anzusprechen; Bestimmen eines Gruppenbezeichners für eine Gruppe von Computervorrichtungen, welche die Computervorrichtung und die zusätzliche Computervorrichtung umfasst; und Senden, an die zusätzliche Computervorrichtung, des Gruppenbezeichners. Die zusätzliche Computervorrichtung ist eine Computervorrichtung, die sich in der Nähe der Computervorrichtung befindet. Die Daten, welche die Gruppe von in der Nähe befindlichen Computervorrichtungen identifizieren, welche die Computervorrichtung umfasst, sind den Gruppenbezeichner. Die Aktionen umfassen weiter das Empfangen, durch die Computervorrichtung, zusätzlicher Audiodaten, die einer zusätzlichen Äußerung entsprechen; das Bestimmen, dass die zusätzliche Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst; als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst, Senden, an den Server, (i) der Daten, die anzeigen, dass die Computervorrichtung wahrscheinlich das bestimmte vordefinierte Hotword empfangen hat, (ii) der Daten, welche die Computervorrichtung identifizieren, und (iii) der Daten, welche die Gruppe von in der Nähe befindlichen Computervorrichtungen identifiziert, welche die Computervorrichtung umfasst; das Empfangen, von dem Server, eines Befehls, Spracherkennungsverarbeitung an den Audiodaten nicht zu beginnen; und als Reaktion auf das Empfangen des Befehls, Spracherkennungsverarbeitung an den Audiodaten nicht zu beginnen, das Verarbeiten der Audiodaten unter Verwendung des automatisierten Spracherkenners auf der Computervorrichtung einzustellen. These and other embodiments may each optionally include one or more of the following features. The actions further include receiving, from an additional one Computing device, data indicating that the additional computing device is configured to respond to a particular predefined hotword; Sending, to the additional computing device, the data indicating that the computing device is configured to respond to a particular predefined hotword; Determining a group identifier for a group of computing devices comprising the computing device and the additional computing device; and sending, to the additional computing device, the group identifier. The additional computing device is a computing device located near the computing device. The data identifying the group of nearby computing devices comprising the computing device is the group identifier. The actions further include receiving, by the computing device, additional audio data corresponding to an additional utterance; determining that the additional utterance is likely to include the particular predefined hotword; in response to determining that the utterance is likely to include the particular predefined hotword, send, to the server, (i) the data indicating that the computing device has likely received the particular predefined hotword, (ii) the data containing the Identify a computing device, and (iii) the data identifying the group of nearby computing devices comprising the computing device; not to start receiving from the server, a command, voice recognition processing on the audio data; and in response to receiving the command not to begin speech recognition processing on the audio data, to discontinue processing the audio data using the automated speech recognizer on the computing device.
Die Aktionen umfassen weiter das Bestimmen einer Lautstärke der mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten; und als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst, Senden, an den Server, der Lautstärke der mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten. Die Aktion des Bestimmens einer Lautstärke der mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten umfasst das Bestimmen einer Leistung der Audiodaten, die mit dem bestimmten vordefinierten Hotword verknüpft sind, und das Bestimmen einer Leistung von Audiodaten, die mit dem bestimmten vordefinierten Hotword nicht verknüpft sind, und welche die Computervorrichtung vor den Audiodaten empfangen hat, die mit dem bestimmten vordefinierten Hotword verknüpft sind. Die Lautstärke der mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten basiert auf der Leistung der Audiodaten, die mit dem bestimmten vordefinierten Hotword verknüpft sind, und der Leistung der Audiodaten, die mit dem bestimmten vordefinierten Hotword nicht verknüpft sind, und welche die Computervorrichtung vor den Audiodaten empfangen hat, die mit dem bestimmten vordefinierten Hotword verknüpft sind. Die Aktionen umfassen weiter das Bestimmen einer Konfidenzpunktzahl, die eine Wahrscheinlichkeit reflektiert, dass die mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten dem bestimmten vordefinierten Hotword entsprechen; und als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst, das Senden, an den Server, der Konfidenzpunktzahl. The actions further include determining a volume of the audio data associated with the particular predefined hotword; and in response to determining that the utterance is likely to include the particular predefined hotword, sending, to the server, the volume of the audio data associated with the particular predefined hotword. The action of determining a volume of the audio data associated with the particular predefined hotword includes determining a performance of the audio data associated with the particular predefined hotword and determining a performance of audio data not associated with the particular predefined hotword, and which has received the computing device prior to the audio data associated with the particular predefined hotword. The volume of the audio data associated with the particular predefined hotword is based on the performance of the audio data associated with the particular predefined hotword and the performance of the audio data not associated with the particular predefined hotword and which the computing device receives before the audio data has associated with the particular predefined hotword. The actions further include determining a confidence score that reflects a probability that the audio associated with the particular predefined hotword matches the particular predefined hotword; and in response to determining that the utterance is likely to include the particular predefined hotword, sending to the server the confidence score.
Die Aktion des Bestimmens einer Konfidenzpunktzahl, die eine Wahrscheinlichkeit reflektiert, dass die mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten dem bestimmten vordefinierten Hotword entsprechen, umfasst das Bestimmen von Audiomerkmalen von den mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten; und basierend auf den Audiomerkmalen, das Bestimmen der Konfidenzpunktzahl unter Verwendung eines neuronalen Netzwerks. Die Aktionen umfassen weiter, als Reaktion auf das Bestimmen, das die Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst, das Senden, an den Server, von Daten, die einen Ort der Computervorrichtung anzeigen. Die Aktionen umfassen weiter, als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst, das Senden, an den Server, von Daten, die eine abgelaufene Zeit seit einer vorhergehenden Verwendung der Computervorrichtung anzeigen. Die Aktionen umfassen weiter, als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst, das Senden, an den Server, von Daten, die eine durch die Computervorrichtung zuvor ausgeführte Aktion anzeigen. The action of determining a confidence score that reflects a likelihood that the audio associated with the particular predefined hotword matches the particular predefined hotword includes determining audio features from the audio associated with the particular predefined hotword; and based on the audio features, determining the confidence score using a neural network. The actions further include, in response to determining that the utterance is likely to include the particular predefined hotword, sending to the server data indicating a location of the computing device. The actions further include, in response to determining that the utterance includes the particular predefined hotword, sending, to the server, data indicating an elapsed time since previous use of the computing device. The actions further include, in response to determining that the utterance is likely to include the particular predefined hotword, sending, to the server, data indicative of an action previously performed by the computing device.
Andere Ausführungsformen dieses Aspekts umfassen entsprechende Systeme, Vorrichtungen und Computerprogramme, die auf Computerspeichervorrichtungen aufgezeichnet sind, von denen jedes konfiguriert ist, die Operationen der Verfahren auszuführen. Other embodiments of this aspect include corresponding systems, devices, and computer programs recorded on computer storage devices, each of which is configured to perform the operations of the methods.
Ein weiterer innovativer Aspekt des in dieser Beschreibung beschriebenen Gegenstandes kann in Verfahren verkörpert werden, welche die Aktionen des Empfangens, durch einen Server und von einer Computervorrichtung, (i) von Daten, die anzeigen, dass die Computervorrichtung wahrscheinlich ein bestimmtes vordefiniertes Hotword empfangen hat, (ii) von Daten, welche die Computervorrichtung identifizieren, und (iii) von Daten, die eine Gruppe von Computervorrichtungen identifiziert, die der Computervorrichtung nahe sind, und welche die Computervorrichtung umfasst; das Zugreifen auf Kontextdaten, die einen Kontext der Computervorrichtung anzeigen; basierend auf den Kontextdaten der Computervorrichtung, das Bestimmen, dass die Computervorrichtung Spracherkennungsverarbeitung an mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten beginnt; und das Senden, zur Computervorrichtung, eines Befehls, Spracherkennungsverarbeitung an den mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten zu beginnen. Another innovative aspect of the subject matter described in this specification may be embodied in methods that include the actions of receiving, by a server, and by a computing device, (i) data indicating that the computing device has likely received a particular predefined hotword, (ii) data identifying the computing device; and (iii) data identifying a group of computing devices proximate to the computing device and comprising the computing device; accessing Context data indicating a context of the computing device; based on the context data of the computing device, determining that the computing device begins speech recognition processing on audio data associated with the determined predefined hotword; and sending, to the computing device, an instruction to begin speech recognition processing on the audio data associated with the determined predefined hotword.
Diese und andere Ausführungsformen können jeweils optional ein oder mehrere von den folgenden Merkmalen umfassen. Die Aktionen umfassen weiter das Empfangen, durch einen Server und von einer zusätzlichen Computervorrichtung, (i) von Daten, die anzeigen, dass die zusätzliche Computervorrichtung wahrscheinlich das bestimmte vordefinierte Hotword empfangen hat, (ii) von Daten, welche die zusätzliche Computervorrichtung identifizieren, und (iii) von Daten, welche die Gruppe von Computervorrichtungen identifizieren, die der zusätzlichen Computervorrichtung nahe sind, und welche die Computervorrichtung und die zusätzliche Computervorrichtung umfasst; das Zugreifen auf Kontextdaten, die einen Kontext der zusätzlichen Computervorrichtung anzeigen; basierend auf den Kontextdaten der Computervorrichtung und den Kontextdaten der zusätzlichen Computervorrichtung, das Bestimmen, dass die zusätzliche Computervorrichtung Spracherkennungsverarbeitung an den mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten nicht beginnt; und das Senden, an die zusätzliche Computervorrichtung, eines Befehls, Spracherkennungsverarbeitung an den mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten nicht zu beginnen. These and other embodiments may each optionally include one or more of the following features. The actions further include receiving, by a server and from an additional computing device, (i) data indicating that the additional computing device has likely received the particular predefined hotword, (ii) data identifying the additional computing device, and (iii) data identifying the group of computing devices proximate to the additional computing device and comprising the computing device and the additional computing device; accessing context data indicating a context of the additional computing device; based on the context data of the computing device and the context data of the additional computing device, determining that the additional computing device does not begin speech recognition processing on the audio data associated with the determined predefined hotword; and not sending to the additional computing device a command to start voice recognition processing on the audio data associated with the determined predefined hotword.
Die Aktionen umfassen weiter das Empfangen, durch einen Server und von einer ersten Computervorrichtung, von Daten, die anzeigen, dass die erste Computervorrichtung konfiguriert ist, auf ein bestimmtes vordefiniertes Hotword anzusprechen; das Empfangen, durch einen Server und von einer zweiten Computervorrichtung, von Daten, die anzeigen, dass die zweite Computervorrichtung konfiguriert ist, auf das bestimmte vordefinierten Hotword anzusprechen; das Bestimmen, durch den Server, eines Gruppenbezeichners für eine Gruppe von Computervorrichtungen, welche die erste Computervorrichtung und die zweite Computervorrichtung umfasst; und das Senden, an die erste Computervorrichtung und die zweite Computervorrichtung, des Gruppenbezeichners. Die Daten, welche die Gruppe von Computervorrichtungen identifizieren, die sich nahe der Computervorrichtung befinden, und welche die Computervorrichtung umfasst, sind den Gruppenbezeichner. Die mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten sind Audiodaten, die einer Äußerung entsprechen. Die Kontextdaten, die einen Kontext der Computervorrichtung anzeigen, umfassen Daten, die eine von mehreren Fähigkeiten der Computervorrichtung anzeigen. The actions further include receiving, by a server and from a first computing device, data indicating that the first computing device is configured to respond to a particular predefined hotword; receiving, by a server and from a second computing device, data indicating that the second computing device is configured to respond to the particular predefined hotword; determining, by the server, a group identifier for a group of computing devices comprising the first computing device and the second computing device; and sending, to the first computing device and the second computing device, the group identifier. The data identifying the group of computing devices located near the computing device and comprising the computing device is the group identifier. The audio data associated with the particular predefined hotword is audio data that corresponds to an utterance. The context data indicating a context of the computing device includes data indicative of one of several capabilities of the computing device.
Die Aktion des Bestimmens, dass die Computervorrichtung Spracherkennungsverarbeitung an mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten beginnt, basiert auf der einen von mehreren Fähigkeiten der Computervorrichtung. Die Kontextdaten, die einen Kontext der Computervorrichtung anzeigen, umfassen Daten, die eine abgelaufene Zeit seit einer vorhergehenden Verwendung der Computervorrichtung anzeigen. Die Aktion des Bestimmens, dass die Computervorrichtung Spracherkennungsverarbeitung an mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten beginnt, basiert auf der abgelaufenen Zeit seit der vorhergehenden Verwendung der Computervorrichtung. Die Kontextdaten, die einen Kontext der Computervorrichtung anzeigen, umfassen Daten, die eine durch die Computervorrichtung zuvor ausgeführte Aktion anzeigen. Die Aktion des Bestimmens, dass die Computervorrichtung Spracherkennungsverarbeitung an mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten beginnt, basiert auf der durch die Computervorrichtung zuvor ausgeführten Aktion. Die Aktionen umfassen weiter das Empfangen, von der Computervorrichtung, von Daten die eine Lautstärke der mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten anzeigen. Die Aktion des Bestimmens, dass die Computervorrichtung Spracherkennungsverarbeitung an den mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten beginnt, basiert weiter auf der Lautstärke der mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten. The action of determining that the computer device has speech recognition processing on The audio data associated with the particular predefined hotword starts based on the one of several capabilities of the computing device. The context data indicating a context of the computing device includes data indicating an elapsed time since a previous use of the computing device. The action of determining that the computer device begins speech recognition processing on audio data associated with the particular predefined hotword is based on the elapsed time since the previous use of the computing device. The context data indicating a context of the computing device includes data indicative of an action previously performed by the computing device. The action of determining that the computer device voice recognition processing begins on audio data associated with the particular predefined hotword is based on the action previously performed by the computing device. The actions further include receiving, from the computing device, data indicating a volume of the audio data associated with the particular predefined hotword. The action of determining that the voice recognition processing computer device begins on the audio data associated with the particular predefined hotword is further based on the volume of the audio data associated with the particular predefined hotword.
Die Aktionen umfassen weiter das Empfangen, durch den Server und von der Computervorrichtung, von Daten, die einen Ort der Computervorrichtung anzeigen. Die Aktion des Bestimmens, dass die Computervorrichtung Spracherkennungsverarbeitung an mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten beginnt, basiert weiter auf dem Ort der Computervorrichtung. Die Aktionen umfassen weiter das Empfangen, von der Computervorrichtung, von Daten, die eine Aktion anzeigen, die mit den Audiodaten verknüpft sind, und von Daten, die Aktionen anzeigen, die durch die Computervorrichtung innerhalb eines bestimmten Zeitraums nach dem Empfangen des Befehls ausgeführt werden, Spracherkennungsverarbeitung an den mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten zu beginnen; und das Aktualisieren der Kontextdaten der Computervorrichtung basierend auf den Daten, welche die Aktion anzeigen, die mit den Audiodaten verknüpft ist, und den Daten, welche die Aktionen anzeigen, die durch die Computervorrichtung innerhalb des bestimmten Zeitraums nach dem Empfangen des Befehls ausgeführt werden, Spracherkennungsverarbeitung an den mit dem bestimmten vordefinierten Hotword verknüpften Audiodaten zu beginnen. The actions further include receiving, by the server and from the computing device, data indicating a location of the computing device. The action of determining that the computer device begins speech recognition processing on audio data associated with the particular predefined hotword is further based on the location of the computing device. The actions further include receiving, from the computing device, data indicative of an action associated with the audio data, and data indicating actions performed by the computing device within a particular time period after receiving the command, Start speech recognition processing on the audio data associated with the particular predefined hotword; and updating the context data of the computing device based on the data indicating the action associated with the audio data and the data indicative of the actions performed by the computing device within the designated time period after receiving the command, speech recognition processing to begin the audio associated with the particular predefined hotword.
Andere Ausführungsformen dieses Aspekts umfassen entsprechende Systeme, Vorrichtungen und Computerprogramme, die auf Computerspeichervorrichtungen aufgezeichnet sind, von denen jede konfiguriert ist, die Operationen der Verfahren auszuführen. Other embodiments of this aspect include corresponding systems, devices, and computer programs recorded on computer storage devices, each of which is configured to perform the operations of the methods.
Bestimmte Ausführungsformen des in dieser Spezifikation beschriebenen Gegenstands können so implementiert werden, dass sie einen oder mehrere der folgenden Vorteile verwirklichen. Ein Benutzer kann ein Hotword in einem Raum mit mehreren Vorrichtungen sprechen, die fähig sind, auf Hotwords anzusprechen, und nur eine Vorrichtung wird ansprechen. Der Benutzer kann ein Hotword sprechen und nur die naheste Vorrichtung wird auf das Hotword ansprechen, selbst wenn andere Vorrichtungen das Hotword hören. Das System, das die Vorrichtung auswählt, das auf das Hotword ansprechen soll, kann eine geeignete Vorrichtung basierend auf dem Kontext der Vorrichtung und anderen in der Nähe befindlich Vorrichtungen auswählen. Certain embodiments of the subject matter described in this specification may be implemented to achieve one or more of the following advantages. A user can speak a hotword in a room with multiple devices capable of responding to hotwords, and only one device will respond. The user can speak a hotword and only the closest device will respond to the hotword even if other devices hear the hotword. The system that selects the device to respond to the hotword may select an appropriate device based on the context of the device and other nearby devices.
Die Details von einer oder mehreren Ausführungsformen des in dieser Patentschrift beschriebenen Gegenstandes sind in den begleitenden Zeichnungen und der nachfolgenden Beschreibung dargelegt. Andere Merkmale, Aspekte und Vorteile des Gegenstands werden aus der Beschreibung, den Zeichnungen und den Ansprüchen offensichtlich. The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects and advantages of the subject matter will become apparent from the description, drawings and claims.
KURZE BESCHREIBUNG DER ZEICHNUNGEN BRIEF DESCRIPTION OF THE DRAWINGS
Gleiche Bezugsnummern und Bezeichnungen in den verschiedenen Zeichnungen verweisen auf ähnliche Elemente. Like reference numerals and designations in the various drawings refer to similar elements.
AUSFÜHRLICHE BESCHREIBUNG DETAILED DESCRIPTION
Da Systeme, die konfiguriert sind, auf Hotwords anzusprechen, häufiger anzutreffen sind, steigt die Wahrscheinlichkeit, dass sich mehr als eines nahe einem Benutzer befindet, wenn der Benutzer das Hotword spricht. Beispielsweise kann ein Benutzer im Wohnzimmer sein und fernsehen. Ein Tablet kann auf dem Tisch liegen und ein Telefon kann sich auf der Couch neben dem Benutzer befinden. Gemäß verschiedener Ausführungsformen, wenn das Telefon, das Tablet und der Fernseher ein Hotword detektiert, kommunizieren diese jeweils mit einem Server und benachrichtigen den Server, dass die Vorrichtung ein Hotword detektiert hat. Mit jeder an den Server gesendeten Benachrichtigung sendet jede Vorrichtung einen Gruppenbezeichner und die Vorrichtungsart. Der Gruppenbezeichner ist Daten, die eine Vorrichtungsgruppe identifizieren, von der nur eine Vorrichtung auf ein Hotword ansprechen sollte. Beispielsweise können Vorrichtungen im gleichen Raum einen Gruppenbezeichner teilen. Because systems that are configured to respond to hotwords are more common, the likelihood that there is more than one close to a user when the user speaks the hotword increases. For example, a user can be in the living room and watch TV. A tablet can be on the table and a telephone can be on the couch next to the user. According to various embodiments, when the phone, tablet, and TV detect a hotword, they each communicate with a server and notify the server that the device has detected a hotword. With each notification sent to the server, each device sends a group identifier and the device type. The group identifier is data that identifies a device group of which only one device should respond to a hotword. For example, devices in the same room can share a group identifier.
Wenn der Server Daten von jedem von dem Telefon, Tablet und Fernseher empfängt, vergleicht der Server die Kontextdaten jeder Vorrichtung, um zu bestimmen, welche auf das Hotword ansprechen sollte. Die Kontextdaten können die Fähigkeiten der Vorrichtungen, die Zeit, seit jede Vorrichtung zuvor verwendet wurde, die vorhergehend ausgeführte Aktion, der Ort der Vorrichtung und die Lautstärke des detektierten Hotwords umfassen. Basierend auf diesen Kontextdaten wählt der Server eine der Vorrichtungen zum Ansprechen auf das Hotword aus. Beispielsweise kann der Server das Tablet zum Ansprechen auf das Hotword basierend auf den Vorrichtungen auswählen, die sich im Zuhause des Benutzers befinden, und auf der Tatsache, dass der Benutzer meistens das Tablet verwendet, während er zu Zuhause ist. Der Server stellt dann einen Befehl an das Tablet bereit, um die Sprache des Benutzers zu verarbeiten. Das Tablet verarbeitet den Befehl „Musik abspielen“ und beginnt Musik abzuspielen. Der Server stellt auch Befehle an den Fernseher und das Telefon bereit, die Sprache des Benutzers nicht zu verarbeiten. Das Telefon kehrt zu seinem vorhergehenden Zustand zurück und der Fernseher fährt fort, die Fernsehshow ohne Unterbrechung abzuspielen. When the server receives data from each of the phone, tablet, and TV, the server compares the context data of each device to determine which one should respond to the hotword. The context data may include the capabilities of the devices, the time since each device was previously used, the previously performed action, the location of the device, and the volume of the detected hotword. Based on this contextual data, the server selects one of the hotter response devices. For example, the server may select the tablet to respond to the hotword based on the devices that are in the user's home and the fact that the user most often uses the tablet while at home. The server then provides a command to the tablet to process the user's language. The tablet processes the "play music" command and begins to play music. The server also provides commands to the TV and the phone not to process the user's speech. The phone returns to its previous state and the TV continues to play the TV show without interruption.
Auf diese Weise wird die Verwendung von Computerressourcen reduziert, wenn ein Benutzer einen Sprachbefehl an das System richtet. Anstatt, dass jede Vorrichtung unabhängig voneinander auf den Sprachbefehl anspricht, wird das System in der hierin näher beschriebenen Art und Weise konfiguriert, sodass nur eine der Vorrichtungen die angeforderte Aktion als Reaktion auf den Sprachbefehl annimmt. This reduces the use of computer resources when a user makes a voice command to the system. Instead of each device being independently responsive to the voice command, the system is configured in the manner described in more detail herein, such that only one of the devices accepts the requested action in response to the voice command.
Im gezeigten Beispiel in
Bei einigen Implementierungen befinden sich das Telefon
Bei einigen Implementierungen befinden sich das Telefon
Bei Stufe B spricht der Benutzer
Bei Stufe B1 empfängt das Telefon
Zu annähernd der gleichen Zeit, zu der das Telefon
Während das Telefon
In Stufe C1 erzeugt das Telefon
Während ungefähr der gleichen Zeit, zu der das Telefon
In Stufe C2 und ähnlich zu Stufe C1 erzeugt der Fernseher
Bei einigen Implementierungen und wie veranschaulicht in Stufe C1' sendet das Telefon
Bei einigen Implementierungen und in Stufe C1' sendet das Telefon
Bei einigen Implementierungen und wie veranschaulicht in Stufe C1' sendet das Telefon
Bei einigen Implementierungen und wie veranschaulicht in Stufe C1' sendet das Telefon
Bei einigen Implementierungen und wie veranschaulicht in Stufe C2' sendet der Fernseher
Der Fernseher
Bei einigen Implementierungen sendet der Fernseher
Der Server
Bei diesem Beispiel vergleicht der Server
Innerhalb des Schwellenzeitraums empfängt der Server
In Stufe D greift der Server
Ähnlich kann der Server
Bei einigen Implementierungen können die Kontextdaten
Um eine Vorrichtung auszuwählen und Spracherkennung an der Äußerung
Bei einigen Implementierungen kann der Server
Bei einigen Implementierungen kann der Server
Bei einigen Implementierungen kann der Vorrichtungsselektor
Bei einigen Implementierungen kann der Server
Bei einigen Implementierungen kann der Server
In Stufe E wählt der Server
In Stufe F1 und F2 sendet der Server
In den Stufen G1 und G2 führen das Telefon
Bei einigen Implementierungen kann das Telefon
Bei einigen Implementierungen können zwei Vorrichtungen, die möglicherweise von der gleichen Art sind, im gleichen Raum enden und beide sind mit einem unterschiedlichen Gruppenbezeichner verknüpft. Wenn in diesem Fall jede Vorrichtung ein Hotword-Detektionsdatenpaket an den Server
Bei einigen Implementierungen kann der Sprachbefehl
Bei einigen Implementierungen kann das Telefon
Bei einigen Implementierungen kann das Telefon
Das System empfängt Audiodaten, die einer Äußerung (
Das System bestimmt, dass die Äußerung wahrscheinlich ein bestimmtes vordefiniertes Hotword (
Das System sendet als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das bestimmte vordefinierte Hotword umfasst, an einen Server (i) Daten, die anzeigen, dass die Computervorrichtung wahrscheinlich das bestimmte vordefinierte Hotword empfangen hat, (ii) Daten, welche die Computervorrichtung identifizieren, und (iii) Daten, die eine Gruppe von in der Nähe befindlichen Computervorrichtungen identifizieren, welche die Computervorrichtung (
Bei einigen Implementierungen bestimmt das System eine Hotword-Konfidenzpunktzahl, welche die Wahrscheinlichkeit reflektiert, dass das System ein Hotword detektiert hat. Das System sendet dann die Hotword-Konfidenzpunktzahl an den Server als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das Hotword umfasst. Das System kann ein neuronales Netzwerk verwenden, um die Hotword-Konfidenzpunktzahl zu bestimmen. Bei einigen Implementierungen kann das System seinen geografische Ort als Reaktion auf das Bestimmen, dass die Äußerung wahrscheinlich das Hotword umfasst, an den Server senden. Bei einigen Implementierungen kann das System Daten senden, die eine abgelaufene Zeit seit einer vorhergehenden Verwendung der Computervorrichtung anzeigen und möglicherweise die vorhergehende Aktion einschließen. Der Benutzer kann beispielsweise das System vor zwei Minuten verwendet haben, um eine Textnachricht zu senden. Das System kann diese Informationen an den Server senden. In some implementations, the system determines a hotword confidence score that reflects the likelihood that the system has detected a hotword. The system then sends the hotword confidence score to the server in response to determining that the utterance is likely to comprise the hotword. The system can use a neural network to determine the hotword confidence score. In some implementations, the system may send its geographic location to the server in response to determining that the utterance is likely to comprise the hotword. In some implementations, the system may send data indicating an elapsed time since previous use of the computing device and possibly including the previous action. For example, the user may have used the system two minutes ago to send a text message. The system can send this information to the server.
Das System empfängt vom Server, ein Befehl, Spracherkennungsverarbeitung an den Audiodaten (
Das System empfängt, durch einen Server und von einer Computervorrichtung, (i) Daten, die anzeigen, dass die Computervorrichtung wahrscheinlich ein bestimmtes vordefiniertes Hotword empfangen hat, (ii) Daten, welche die Computervorrichtung identifizieren, und (iii) Daten, die eine Gruppe von Computervorrichtungen identifizieren, die sich nahe der Computervorrichtung befinden, und welche die Computervorrichtung (
Das System greift auf Kontextdaten zu, die einen Kontext der Computervorrichtung (
Das System bestimmt basierend auf den Kontextdaten der Computervorrichtung, dass die Computervorrichtung Spracherkennungsverarbeitung an Audiodaten beginnt, die mit dem bestimmten vordefinierten Hotword (
Das System sendet an die Computervorrichtung einen Befehl, Spracherkennungsverarbeitung an den Audiodaten zu beginnen, die mit dem bestimmten vordefinierten Hotword (
Bei einigen Implementierungen kann das System Daten von der Computervorrichtung empfangen, die vom Benutzer vorgenommene Aktionen identifizieren, nachdem die Computervorrichtung die Audiodaten verarbeitet hat. Die Computervorrichtung kann den Sprachbefehl der Audiodaten ausführen und einen Benutzer die Ausführung des Sprachbefehls stoppen lassen. Der Benutzer kann dann die Aktion des Sprachbefehls auf der durch das System nicht ausgewählten Computervorrichtung manuell ausführen. In diesem Fall wird das System diese Daten empfangen und bestimmen, dass das System die falsche Vorrichtung zum Ansprechen auf das Hotword ausgewählt hat. Das System kann dann Maschinenlernen verwenden, um den Selektionsalgorithmus anzupassen. Bei einigen Implementierungen kann der Selektionsalgorithmus benutzerspezifisch sein. In some implementations, the system may receive data from the computing device identifying actions taken by the user after the computing device has processed the audio data. The computing device may execute the voice command of the audio data and let a user stop the execution of the voice command. The user can then manually execute the action of the voice command on the computer device not selected by the system. In this case, the system will receive this data and determine that the system has selected the wrong device to address the hotword. The system can then use machine learning to customize the selection algorithm. In some implementations, the selection algorithm may be user-specific.
Die Computervorrichtung
Der Speicher
Die Speichervorrichtung
Die Hochgeschwindigkeitsschnittstelle
Die Computervorrichtung
Die mobile Computervorrichtung
Der Prozessor
Der Prozessor
Der Speicher
Der Speicher kann beispielsweise Flash-Speicher und/oder NVRAM-Speicher (nicht flüchtiger Random Access Memory) wie nachfolgend beschrieben umfassen. Bei einigen Implementierungen werden Befehle in einem Informationsträger gespeichert, sodass die Befehle bei Ausführung durch eine oder mehrere Verarbeitungsvorrichtungen (beispielsweise Prozessor
Die mobile Computervorrichtung
Die mobile Computervorrichtung
Die mobile Computervorrichtung
Verschiedene Implementierungen der hier beschriebenen Systeme und Techniken können in digitalen elektronischen Schaltungen, integrierten Schaltungen, speziell konzipierten ASICs (anwendungsorientierten integrierten Schaltungen), Computerhardware, Firmware, Software und/oder Kombinationen davon realisiert sein. Diese verschiedenen Implementierungen können eine Implementierung in einem oder mehreren Computerprogrammen umfassen, die auf einem programmierbaren System ausführbar und/oder interpretierbar sind, das mindestens einen programmierbaren Prozessor umfasst, der ein spezieller oder für allgemeine Zwecke sein kann und der zum Empfangen von Daten und Anweisungen von und zum Übertragen von Daten und Befehlen an ein Speichersystem, mindestens eine Eingabevorrichtung und mindestens eine Ausgabevorrichtung gekoppelt ist. Various implementations of the systems and techniques described herein may be implemented in digital electronic circuits, integrated circuits, specially designed ASICs (application-oriented integrated circuits), computer hardware, firmware, software, and / or combinations thereof. These various implementations may include implementation in one or more computer programs executable and / or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, and for receiving data and instructions from and coupled to a storage system, at least one input device and at least one output device for transmitting data and commands.
Diese Computerprogramme (auch bekannt als Programme, Software, Softwareanwendungen oder Code) umfassen Maschinenbefehle für einen programmierbaren Prozessor und können in einer höheren prozeduralen und/oder objektorientierten Programmiersprache und/oder in Assembler-/Maschinensprache implementiert sein. Wie hierin verwendet, bezeichnen die Begriffe „maschinenlesbares Medium“ und „computerlesbares Medium“ ein beliebiges Computerprogrammprodukt, eine beliebige Vorrichtung und/oder ein beliebiges Gerät (z. B. Magnetplatten, optische Platten, Speicher, programmierbare Logikbausteine (PLDs)), die verwendet werden, um einem programmierbaren Prozessor Maschinenbefehle und/oder Daten bereitzustellen, einschließlich eines maschinenlesbaren Mediums, das Maschinenbefehle als ein maschinenlesbares Signal empfängt. Der Begriff „maschinenlesbares Signal“ bezeichnet ein beliebiges Signal, das verwendet wird, um einem programmierbaren Prozessor Maschinenanweisungen und/oder Daten bereitzustellen. These computer programs (also known as programs, software, software applications, or code) include machine instructions for a programmable processor and may be implemented in a higher level of procedural and / or object-oriented programming language and / or assembler / machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, device, and / or device (eg, magnetic disks, optical disks, memory, programmable logic devices (PLDs)) that uses to provide machine instructions and / or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and / or data to a programmable processor.
Um eine Interaktion mit einem Benutzer bereitzustellen, können die hier beschriebenen Systeme und Techniken auf einem Computer implementiert werden, der eine Anzeigevorrichtung (wie z. B. einen CRT-(Kathodenstrahlröhre) oder LCD-(Flüssigkristallanzeige)-Monitor) aufweist, um dem Benutzer Informationen anzuzeigen, und eine Tastatur und eine Zeigeeinrichtung (z. B. eine Maus oder ein Trackball) mittels denen der Benutzer eine Eingabe an den Computer bereitstellen kann. Es können auch andere Arten von Vorrichtungen verwendet werden, um für eine Interaktion mit einem Benutzer zu sorgen; beispielsweise kann eine an den Benutzer bereitgestellte Rückkopplung eine beliebige Form von sensorischer Rückkopplung sein, wie z. B. eine visuelle Rückkopplung, auditorische Rückkopplung oder taktile Rückkopplung; und die Eingabe vom Benutzer kann in beliebiger Form empfangen werden, einschließlich akustischer, Sprach- oder taktiler Eingabe. To provide interaction with a user, the systems and techniques described herein may be implemented on a computer having a display device (such as a CRT (Cathode Ray Tube) or LCD (Liquid Crystal Display) monitor) to the user Display information and a keyboard and pointing device (eg, a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with a user; For example, feedback provided to the user may be any form of sensory feedback, such as: A visual feedback, auditory feedback or tactile feedback; and the input from the user may be received in any form, including acoustic, voice or tactile input.
Die hier beschriebenen Systeme und Techniken können in einem Computersystem implementiert werden, das eine Backendkomponente (z. B. als einen Datenserver) umfasst, oder das eine Middlewarekomponente (z. B. einen Anwendungsserver) umfasst, oder das eine Frontendkomponente (z. B. einen Clientcomputer mit einer grafischen Benutzerschnittstelle oder einem Webbrowser umfasst, durch die bzw. den ein Benutzer mit hier beschriebenen Systemimplementationen und Techniken interagieren kann) oder jeder Kombination aus solchen Backend-, Middleware- oder Frontendkomponenten. Die Komponenten des Systems können durch eine beliebige Form oder ein beliebiges Medium digitaler Datenkommunikation, wie z. B. ein Kommunikationsnetzwerk, miteinander verbunden sein). Beispiele von Kommunikationsnetzwerken umfassen ein lokales Netzwerk („LAN“), ein Fernnetzwerk („WAN“) und das Internet. The systems and techniques described herein may be implemented in a computer system that includes a back end component (eg, as a data server), or that includes a middleware component (eg, an application server), or that has a front end component (e.g. a client computer with a graphical user interface or web browser through which a user can interact with system implementations and techniques described herein) or any combination of such backend, middleware or frontend components. The components of the system may be replaced by any form or medium of digital data communication, such as digital data communication. A communication network). Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the Internet.
Das Computersystem kann Clients und Server umfassen. Ein Client und ein Server befinden sich im Allgemeinen entfernt voneinander und interagieren typischerweise über ein Kommunikationsnetzwerk. Die Beziehung zwischen Client und Server entsteht aufgrund von Computerprogrammen, die auf den jeweiligen Computern laufen und die eine Client-Server-Beziehung zueinander aufweisen. The computer system may include clients and servers. A client and a server are generally remote and typically interact over a communications network. The relationship between client and server is due to computer programs that run on the respective computers and that have a client-server relationship with each other.
Obwohl einige Implementierungen vorstehend im Detail beschrieben wurden, sind andere Modifikationen möglich. Während eine Kundenanwendung als Zugreifen auf den bzw. die Delegierten beschrieben ist, können bei anderen Implementierungen der bzw. die Delegierten durch andere Anwendungen verwendet werden, die durch einen oder mehrere Prozessoren implementiert sind, wie beispielsweise eine Anwendung, die auf einem oder mehreren Servern ausführt. Außerdem erfordern die in den Figuren dargestellten logischen Abläufe nicht die bestimmte dargestellte Reihenfolge oder sequenzielle Reihenfolge, um wünschenswerte Ergebnisse zu erzielen. Darüber hinaus können andere Aktionen vorgesehen oder Aktionen aus den beschriebenen Abläufen eliminiert werden und andere Komponenten können zu den beschriebenen Systemen hinzugefügt oder davon entfernt werden. Dementsprechend liegen andere Implementierungen im Geltungsbereich der folgenden Ansprüche. Although some implementations have been described in detail above, other modifications are possible. While a customer application is described as accessing the delegate (s), in other implementations the delegate (s) may be used by other applications implemented by one or more processors, such as an application running on one or more servers , In addition, the logical flows shown in the figures do not require the particular order or sequential order shown to achieve desirable results. In addition, other actions may be provided or actions may be eliminated from the described procedures, and other components may be added to or removed from the described systems. Accordingly, other implementations are within the scope of the following claims.
Claims (22)
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662378869P | 2016-08-24 | 2016-08-24 | |
US62/378,869 | 2016-08-24 | ||
US15/278,269 | 2016-09-28 | ||
US15/278,269 US9972320B2 (en) | 2016-08-24 | 2016-09-28 | Hotword detection on multiple devices |
Publications (1)
Publication Number | Publication Date |
---|---|
DE202017104895U1 true DE202017104895U1 (en) | 2017-11-27 |
Family
ID=59579964
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE202017104895.0U Active DE202017104895U1 (en) | 2016-08-24 | 2017-08-15 | Hotword detection on multiple devices |
Country Status (5)
Country | Link |
---|---|
US (6) | US9972320B2 (en) |
EP (1) | EP3501022A1 (en) |
CN (1) | CN109791763B (en) |
DE (1) | DE202017104895U1 (en) |
WO (1) | WO2018038888A1 (en) |
Families Citing this family (102)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9812128B2 (en) * | 2014-10-09 | 2017-11-07 | Google Inc. | Device leadership negotiation among voice interface devices |
US10509626B2 (en) | 2016-02-22 | 2019-12-17 | Sonos, Inc | Handling of loss of pairing between networked devices |
US10095470B2 (en) | 2016-02-22 | 2018-10-09 | Sonos, Inc. | Audio response playback |
US9965247B2 (en) | 2016-02-22 | 2018-05-08 | Sonos, Inc. | Voice controlled media playback system based on user profile |
US10743101B2 (en) | 2016-02-22 | 2020-08-11 | Sonos, Inc. | Content mixing |
US9947316B2 (en) | 2016-02-22 | 2018-04-17 | Sonos, Inc. | Voice control of a media playback system |
US10264030B2 (en) | 2016-02-22 | 2019-04-16 | Sonos, Inc. | Networked microphone device control |
US9779735B2 (en) * | 2016-02-24 | 2017-10-03 | Google Inc. | Methods and systems for detecting and processing speech signals |
WO2017167406A1 (en) * | 2016-04-01 | 2017-10-05 | Intel Corporation | Device identification through dialog |
US9978390B2 (en) | 2016-06-09 | 2018-05-22 | Sonos, Inc. | Dynamic player selection for audio signal processing |
US10152969B2 (en) | 2016-07-15 | 2018-12-11 | Sonos, Inc. | Voice detection by multiple devices |
US10134399B2 (en) | 2016-07-15 | 2018-11-20 | Sonos, Inc. | Contextualization of voice inputs |
US10115400B2 (en) | 2016-08-05 | 2018-10-30 | Sonos, Inc. | Multiple voice services |
US9972320B2 (en) | 2016-08-24 | 2018-05-15 | Google Llc | Hotword detection on multiple devices |
US9942678B1 (en) | 2016-09-27 | 2018-04-10 | Sonos, Inc. | Audio playback settings for voice interaction |
US9743204B1 (en) | 2016-09-30 | 2017-08-22 | Sonos, Inc. | Multi-orientation playback device microphones |
US10181323B2 (en) | 2016-10-19 | 2019-01-15 | Sonos, Inc. | Arbitration-based voice recognition |
US10127908B1 (en) * | 2016-11-11 | 2018-11-13 | Amazon Technologies, Inc. | Connected accessory for a voice-controlled device |
US10332523B2 (en) | 2016-11-18 | 2019-06-25 | Google Llc | Virtual assistant identification of nearby computing devices |
US10559309B2 (en) * | 2016-12-22 | 2020-02-11 | Google Llc | Collaborative voice controlled devices |
CN117577099A (en) | 2017-04-20 | 2024-02-20 | 谷歌有限责任公司 | Method, system and medium for multi-user authentication on a device |
US10848591B2 (en) * | 2017-04-25 | 2020-11-24 | Amazon Technologies, Inc. | Sender and recipient disambiguation |
WO2019005227A1 (en) | 2017-06-30 | 2019-01-03 | Google Llc | Methods, systems, and media for voice-based call operations |
EP3646567B1 (en) * | 2017-06-30 | 2022-05-18 | Google LLC | Methods, systems, and media for connecting an iot device to a call |
US10475449B2 (en) | 2017-08-07 | 2019-11-12 | Sonos, Inc. | Wake-word detection suppression |
JP6513749B2 (en) * | 2017-08-09 | 2019-05-15 | レノボ・シンガポール・プライベート・リミテッド | Voice assist system, server device, voice assist method thereof, and program for execution by computer |
US10048930B1 (en) | 2017-09-08 | 2018-08-14 | Sonos, Inc. | Dynamic computation of system response volume |
US10446165B2 (en) | 2017-09-27 | 2019-10-15 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US10621981B2 (en) | 2017-09-28 | 2020-04-14 | Sonos, Inc. | Tone interference cancellation |
US10482868B2 (en) | 2017-09-28 | 2019-11-19 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US10466962B2 (en) | 2017-09-29 | 2019-11-05 | Sonos, Inc. | Media playback system with voice assistance |
KR102455199B1 (en) * | 2017-10-27 | 2022-10-18 | 엘지전자 주식회사 | Artificial intelligence device |
CN107919119A (en) * | 2017-11-16 | 2018-04-17 | 百度在线网络技术（北京）有限公司 | Method, apparatus, equipment and the computer-readable medium of more equipment interaction collaborations |
US10102858B1 (en) * | 2017-11-29 | 2018-10-16 | International Business Machines Corporation | Dynamically changing audio keywords |
US10880650B2 (en) | 2017-12-10 | 2020-12-29 | Sonos, Inc. | Network microphone devices with automatic do not disturb actuation capabilities |
US10818290B2 (en) | 2017-12-11 | 2020-10-27 | Sonos, Inc. | Home graph |
WO2019152722A1 (en) | 2018-01-31 | 2019-08-08 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US10425781B1 (en) * | 2018-02-22 | 2019-09-24 | Amazon Technologies, Inc. | Outputting notifications using device groups |
US10616726B1 (en) | 2018-02-22 | 2020-04-07 | Amazon Technologies, Inc. | Outputing notifications using device groups |
US10425780B1 (en) * | 2018-02-22 | 2019-09-24 | Amazon Technologies, Inc. | Outputting notifications using device groups |
US11183182B2 (en) * | 2018-03-07 | 2021-11-23 | Google Llc | Systems and methods for voice-based initiation of custom device actions |
KR102520068B1 (en) | 2018-03-07 | 2023-04-10 | 구글 엘엘씨 | Systems and methods for voice-based initiation of custom device actions |
US11307880B2 (en) | 2018-04-20 | 2022-04-19 | Meta Platforms, Inc. | Assisting users with personalized and contextual communication content |
US10782986B2 (en) | 2018-04-20 | 2020-09-22 | Facebook, Inc. | Assisting users with personalized and contextual communication content |
US11886473B2 (en) | 2018-04-20 | 2024-01-30 | Meta Platforms, Inc. | Intent identification for agent matching by assistant systems |
US11676220B2 (en) | 2018-04-20 | 2023-06-13 | Meta Platforms, Inc. | Processing multimodal user input for assistant systems |
US11715042B1 (en) | 2018-04-20 | 2023-08-01 | Meta Platforms Technologies, Llc | Interpretability of deep reinforcement learning models in assistant systems |
EP3564949A1 (en) * | 2018-04-23 | 2019-11-06 | Spotify AB | Activation trigger processing |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
KR20190130376A (en) * | 2018-05-14 | 2019-11-22 | 삼성전자주식회사 | System for processing user utterance and controlling method thereof |
US10649727B1 (en) * | 2018-05-14 | 2020-05-12 | Amazon Technologies, Inc. | Wake word detection configuration |
US10867609B2 (en) * | 2018-05-18 | 2020-12-15 | Sorenson Ip Holdings, Llc | Transcription generation technique selection |
US10847178B2 (en) | 2018-05-18 | 2020-11-24 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection |
US10959029B2 (en) | 2018-05-25 | 2021-03-23 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
US10681460B2 (en) | 2018-06-28 | 2020-06-09 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
KR102025566B1 (en) * | 2018-07-27 | 2019-09-26 | 엘지전자 주식회사 | Home appliance and voice recognition server system using artificial intelligence and method for controlling thereof |
US11076035B2 (en) | 2018-08-28 | 2021-07-27 | Sonos, Inc. | Do not disturb feature for audio notifications |
US10461710B1 (en) | 2018-08-28 | 2019-10-29 | Sonos, Inc. | Media playback system with maximum volume setting |
CN109377987B (en) | 2018-08-31 | 2020-07-28 | 百度在线网络技术（北京）有限公司 | Interaction method, device, equipment and storage medium between intelligent voice equipment |
JP7035924B2 (en) * | 2018-09-11 | 2022-03-15 | 日本電信電話株式会社 | Channel selection device, channel selection method, and program |
US10587430B1 (en) | 2018-09-14 | 2020-03-10 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
US10878811B2 (en) | 2018-09-14 | 2020-12-29 | Sonos, Inc. | Networked devices, systems, and methods for intelligently deactivating wake-word engines |
US11024331B2 (en) | 2018-09-21 | 2021-06-01 | Sonos, Inc. | Voice detection optimization using sound metadata |
US10811015B2 (en) | 2018-09-25 | 2020-10-20 | Sonos, Inc. | Voice detection optimization based on selected voice assistant service |
US11152003B2 (en) * | 2018-09-27 | 2021-10-19 | International Business Machines Corporation | Routing voice commands to virtual assistants |
US11100923B2 (en) | 2018-09-28 | 2021-08-24 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US10692518B2 (en) | 2018-09-29 | 2020-06-23 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection via multiple network microphone devices |
US11899519B2 (en) | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
US11145306B1 (en) * | 2018-10-31 | 2021-10-12 | Ossum Technology Inc. | Interactive media system using audio inputs |
EP3654249A1 (en) | 2018-11-15 | 2020-05-20 | Snips | Dilated convolutions and gating for efficient keyword spotting |
US11183183B2 (en) * | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11132989B2 (en) * | 2018-12-13 | 2021-09-28 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US10602268B1 (en) | 2018-12-20 | 2020-03-24 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
US10867604B2 (en) | 2019-02-08 | 2020-12-15 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
US11580969B2 (en) * | 2019-03-27 | 2023-02-14 | Lg Electronics Inc. | Artificial intelligence device and method of operating artificial intelligence device |
US11120794B2 (en) | 2019-05-03 | 2021-09-14 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
US10586540B1 (en) | 2019-06-12 | 2020-03-10 | Sonos, Inc. | Network microphone device with command keyword conditioning |
WO2021015308A1 (en) * | 2019-07-19 | 2021-01-28 | 엘지전자 주식회사 | Robot and trigger word recognition method therefor |
US11138969B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US10871943B1 (en) | 2019-07-31 | 2020-12-22 | Sonos, Inc. | Noise classification for event detection |
US11138975B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
CN110457078B (en) * | 2019-08-09 | 2020-11-24 | 百度在线网络技术（北京）有限公司 | Intelligent service method, device and equipment |
CN110517692A (en) * | 2019-08-30 | 2019-11-29 | 苏州思必驰信息科技有限公司 | Hot word audio recognition method and device |
CN110660390B (en) * | 2019-09-17 | 2022-05-03 | 百度在线网络技术（北京）有限公司 | Intelligent device wake-up method, intelligent device and computer readable storage medium |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
CN110890092B (en) * | 2019-11-07 | 2022-08-05 | 北京小米移动软件有限公司 | Wake-up control method and device and computer storage medium |
US11665013B1 (en) * | 2019-12-13 | 2023-05-30 | Amazon Technologies, Inc. | Output device selection |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
CN111276139B (en) * | 2020-01-07 | 2023-09-19 | 百度在线网络技术（北京）有限公司 | Voice wake-up method and device |
CN111312239B (en) * | 2020-01-20 | 2023-09-26 | 北京小米松果电子有限公司 | Response method, response device, electronic equipment and storage medium |
US11556307B2 (en) | 2020-01-31 | 2023-01-17 | Sonos, Inc. | Local voice data processing |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11308962B2 (en) | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
US11551700B2 (en) | 2021-01-25 | 2023-01-10 | Sonos, Inc. | Systems and methods for power-efficient keyword detection |
US20220284883A1 (en) * | 2021-03-05 | 2022-09-08 | Comcast Cable Communications, Llc | Keyword Detection |
Family Cites Families (130)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4363102A (en) | 1981-03-27 | 1982-12-07 | Bell Telephone Laboratories, Incorporated | Speaker identification system using word recognition templates |
JPS59180599A (en) | 1983-03-31 | 1984-10-13 | 日本電気ホームエレクトロニクス株式会社 | Voice recognition controller to be carried on vehicle |
US5659665A (en) | 1994-12-08 | 1997-08-19 | Lucent Technologies Inc. | Method and apparatus for including speech recognition capabilities in a computer system |
JP3674990B2 (en) | 1995-08-21 | 2005-07-27 | セイコーエプソン株式会社 | Speech recognition dialogue apparatus and speech recognition dialogue processing method |
US5895448A (en) | 1996-02-29 | 1999-04-20 | Nynex Science And Technology, Inc. | Methods and apparatus for generating and using speaker independent garbage models for speaker dependent speech recognition purpose |
US6023676A (en) | 1996-12-12 | 2000-02-08 | Dspc Israel, Ltd. | Keyword recognition system and method |
SE511418C2 (en) | 1997-03-13 | 1999-09-27 | Telia Ab | Method of speech verification / identification via modeling of typical non-typical characteristics. |
US8209184B1 (en) | 1997-04-14 | 2012-06-26 | At&T Intellectual Property Ii, L.P. | System and method of providing generated speech via a network |
US6076055A (en) | 1997-05-27 | 2000-06-13 | Ameritech | Speaker verification method |
US5897616A (en) | 1997-06-11 | 1999-04-27 | International Business Machines Corporation | Apparatus and methods for speaker verification/identification/classification employing non-acoustic and/or acoustic models and databases |
JPH1152976A (en) | 1997-07-29 | 1999-02-26 | Nec Home Electron Ltd | Voice recognition device |
JP3524370B2 (en) | 1998-02-19 | 2004-05-10 | 富士通テン株式会社 | Voice activation system |
US6141644A (en) | 1998-09-04 | 2000-10-31 | Matsushita Electric Industrial Co., Ltd. | Speaker verification and speaker identification based on eigenvoices |
US6744860B1 (en) | 1998-12-31 | 2004-06-01 | Bell Atlantic Network Services | Methods and apparatus for initiating a voice-dialing operation |
US6671672B1 (en) | 1999-03-30 | 2003-12-30 | Nuance Communications | Voice authentication system having cognitive recall mechanism for password verification |
US6408272B1 (en) | 1999-04-12 | 2002-06-18 | General Magic, Inc. | Distributed voice user interface |
JP3357629B2 (en) | 1999-04-26 | 2002-12-16 | 旭化成株式会社 | Equipment control system |
US8645137B2 (en) | 2000-03-16 | 2014-02-04 | Apple Inc. | Fast, language-independent method for user authentication by voice |
DE10015960C2 (en) | 2000-03-30 | 2003-01-16 | Micronas Munich Gmbh | Speech recognition method and speech recognition device |
US6567775B1 (en) | 2000-04-26 | 2003-05-20 | International Business Machines Corporation | Fusion of audio and video based speaker identification for multimedia information access |
US6826159B1 (en) | 2000-05-24 | 2004-11-30 | Cisco Technology, Inc. | System and method for providing speaker identification in a conference call |
EP1168736A1 (en) | 2000-06-30 | 2002-01-02 | Alcatel | Telecommunication system and method with a speech recognizer |
US7016833B2 (en) | 2000-11-21 | 2006-03-21 | The Regents Of The University Of California | Speaker verification system using acoustic data and non-acoustic data |
US6973426B1 (en) | 2000-12-29 | 2005-12-06 | Cisco Technology, Inc. | Method and apparatus for performing speaker verification based on speaker independent recognition of commands |
US20020194003A1 (en) | 2001-06-05 | 2002-12-19 | Mozer Todd F. | Client-server security system and method |
US6701293B2 (en) | 2001-06-13 | 2004-03-02 | Intel Corporation | Combining N-best lists from multiple speech recognizers |
JP4224250B2 (en) | 2002-04-17 | 2009-02-12 | パイオニア株式会社 | Speech recognition apparatus, speech recognition method, and speech recognition program |
JP2003345391A (en) * | 2002-05-23 | 2003-12-03 | Denso Corp | Terminal, voice recognition server, voice recognition system and computer program |
US20030231746A1 (en) | 2002-06-14 | 2003-12-18 | Hunter Karla Rae | Teleconference speaker identification |
US7224981B2 (en) | 2002-06-20 | 2007-05-29 | Intel Corporation | Speech recognition of mobile devices |
TW200409525A (en) | 2002-11-26 | 2004-06-01 | Lite On Technology Corp | Voice identification method for cellular phone and cellular phone with voiceprint password |
US7457745B2 (en) | 2002-12-03 | 2008-11-25 | Hrl Laboratories, Llc | Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments |
EP1429314A1 (en) | 2002-12-13 | 2004-06-16 | Sony International (Europe) GmbH | Correction of energy as input feature for speech processing |
US7533023B2 (en) | 2003-02-12 | 2009-05-12 | Panasonic Corporation | Intermediary speech processor in network environments transforming customized speech parameters |
US7222072B2 (en) | 2003-02-13 | 2007-05-22 | Sbc Properties, L.P. | Bio-phonetic multi-phrase speaker identity verification |
US7571014B1 (en) | 2004-04-01 | 2009-08-04 | Sonos, Inc. | Method and apparatus for controlling multimedia players in a multi-zone system |
US8290603B1 (en) | 2004-06-05 | 2012-10-16 | Sonos, Inc. | User interfaces for controlling and manipulating groupings in a multi-zone media system |
US20070198262A1 (en) | 2003-08-20 | 2007-08-23 | Mindlin Bernardo G | Topological voiceprints for speaker identification |
EP1511277A1 (en) | 2003-08-29 | 2005-03-02 | Swisscom AG | Method for answering an incoming event with a phone device, and adapted phone device |
US7305078B2 (en) | 2003-12-18 | 2007-12-04 | Electronic Data Systems Corporation | Speaker identification during telephone conferencing |
US20050165607A1 (en) | 2004-01-22 | 2005-07-28 | At&T Corp. | System and method to disambiguate and clarify user intention in a spoken dialog system |
US8214447B2 (en) | 2004-06-08 | 2012-07-03 | Bose Corporation | Managing an audio network |
US7720012B1 (en) | 2004-07-09 | 2010-05-18 | Arrowhead Center, Inc. | Speaker identification in the presence of packet losses |
US8412521B2 (en) | 2004-08-20 | 2013-04-02 | Multimodal Technologies, Llc | Discriminative training of document transcription system |
US8521529B2 (en) | 2004-10-18 | 2013-08-27 | Creative Technology Ltd | Method for segmenting audio signals |
KR100679043B1 (en) | 2005-02-15 | 2007-02-05 | 삼성전자주식회사 | Apparatus and method for spoken dialogue interface with task-structured frames |
US8041570B2 (en) | 2005-05-31 | 2011-10-18 | Robert Bosch Corporation | Dialogue management using scripts |
US8709018B2 (en) | 2005-09-16 | 2014-04-29 | Applied Medical Technology, Inc. | Non-balloon low profile feed device with insertion/removal tool |
US7603275B2 (en) | 2005-10-31 | 2009-10-13 | Hitachi, Ltd. | System, method and computer program product for verifying an identity using voiced to unvoiced classifiers |
JP2006227634A (en) | 2006-03-29 | 2006-08-31 | Seiko Epson Corp | Equipment control method using voice recognition, equipment control system using voice recognition and recording medium which records equipment control program using voice recognition |
US8595007B2 (en) | 2006-06-15 | 2013-11-26 | NITV Federal Services, LLC | Voice print recognition software system for voice identification and matching |
CN1996847B (en) | 2006-12-27 | 2010-05-19 | 中国科学院上海技术物理研究所 | Cooperative network based image and multi-media data communication and storage system |
US8099288B2 (en) | 2007-02-12 | 2012-01-17 | Microsoft Corp. | Text-dependent speaker verification |
US8838457B2 (en) | 2007-03-07 | 2014-09-16 | Vlingo Corporation | Using results of unstructured language model based speech recognition to control a system-level function of a mobile communications facility |
US20110060587A1 (en) | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US8352264B2 (en) * | 2008-03-19 | 2013-01-08 | Canyon IP Holdings, LLC | Corrective feedback loop for automated speech recognition |
US20080252595A1 (en) | 2007-04-11 | 2008-10-16 | Marc Boillot | Method and Device for Virtual Navigation and Voice Processing |
US8503686B2 (en) | 2007-05-25 | 2013-08-06 | Aliphcom | Vibration sensor and acoustic voice activity detection system (VADS) for use with electronic systems |
US8385233B2 (en) | 2007-06-12 | 2013-02-26 | Microsoft Corporation | Active speaker identification |
GB2450886B (en) | 2007-07-10 | 2009-12-16 | Motorola Inc | Voice activity detector and a method of operation |
GB2458461A (en) | 2008-03-17 | 2009-09-23 | Kai Yu | Spoken language learning system |
US8060358B2 (en) | 2008-03-24 | 2011-11-15 | Microsoft Corporation | HMM alignment for combining translation systems |
US8504365B2 (en) | 2008-04-11 | 2013-08-06 | At&T Intellectual Property I, L.P. | System and method for detecting synthetic speaker verification |
US8145482B2 (en) | 2008-05-25 | 2012-03-27 | Ezra Daya | Enhancing analysis of test key phrases from acoustic sources with key phrase training models |
KR101056511B1 (en) | 2008-05-28 | 2011-08-11 | (주)파워보이스 | Speech Segment Detection and Continuous Speech Recognition System in Noisy Environment Using Real-Time Call Command Recognition |
US8676586B2 (en) | 2008-09-16 | 2014-03-18 | Nice Systems Ltd | Method and apparatus for interaction or discourse analytics |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US9922640B2 (en) | 2008-10-17 | 2018-03-20 | Ashwin P Rao | System and method for multimodal utterance detection |
KR101519104B1 (en) | 2008-10-30 | 2015-05-11 | 삼성전자 주식회사 | Apparatus and method for detecting target sound |
US8326637B2 (en) | 2009-02-20 | 2012-12-04 | Voicebox Technologies, Inc. | System and method for processing multi-modal device interactions in a natural language voice services environment |
US8209174B2 (en) | 2009-04-17 | 2012-06-26 | Saudi Arabian Oil Company | Speaker verification system |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
CN101923853B (en) | 2009-06-12 | 2013-01-23 | 华为技术有限公司 | Speaker recognition method, equipment and system |
US8311838B2 (en) | 2010-01-13 | 2012-11-13 | Apple Inc. | Devices and methods for identifying a prompt corresponding to a voice input in a sequence of prompts |
US8626511B2 (en) | 2010-01-22 | 2014-01-07 | Google Inc. | Multi-dimensional disambiguation of voice commands |
US8543402B1 (en) | 2010-04-30 | 2013-09-24 | The Intellisis Corporation | Speaker segmentation in noisy conversational speech |
US8306814B2 (en) | 2010-05-11 | 2012-11-06 | Nice-Systems Ltd. | Method for speaker source classification |
KR101672212B1 (en) | 2010-06-15 | 2016-11-04 | 엘지전자 주식회사 | Mobile terminal and operation method thereof |
US8719018B2 (en) | 2010-10-25 | 2014-05-06 | Lockheed Martin Corporation | Biometric speaker identification |
US8874773B2 (en) * | 2010-11-30 | 2014-10-28 | Gary W. Grube | Obtaining group and individual emergency preparedness communication information |
CN102741918B (en) | 2010-12-24 | 2014-11-19 | 华为技术有限公司 | Method and apparatus for voice activity detection |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
US9159324B2 (en) | 2011-07-01 | 2015-10-13 | Qualcomm Incorporated | Identifying people that are proximate to a mobile device user via social graphs, speech models, and user context |
US20130024196A1 (en) | 2011-07-21 | 2013-01-24 | Nuance Communications, Inc. | Systems and methods for using a mobile device to deliver speech with speaker identification |
US8660847B2 (en) | 2011-09-02 | 2014-02-25 | Microsoft Corporation | Integrated local and cloud based speech recognition |
US8340975B1 (en) | 2011-10-04 | 2012-12-25 | Theodore Alfred Rosenberger | Interactive speech recognition device and system for hands-free building control |
US8886169B2 (en) * | 2011-10-25 | 2014-11-11 | At&T Intellectual Property I, Lp | Apparatus and method for providing enhanced telephonic communications |
US9031847B2 (en) | 2011-11-15 | 2015-05-12 | Microsoft Technology Licensing, Llc | Voice-controlled camera operations |
WO2013078388A1 (en) | 2011-11-21 | 2013-05-30 | Robert Bosch Gmbh | Methods and systems for adapting grammars in hybrid speech recognition engines for enhancing local sr performance |
US8825020B2 (en) | 2012-01-12 | 2014-09-02 | Sensory, Incorporated | Information access and device control using mobile phones and audio in the home environment |
US20130262873A1 (en) | 2012-03-30 | 2013-10-03 | Cgi Federal Inc. | Method and system for authenticating remote users |
US9117449B2 (en) | 2012-04-26 | 2015-08-25 | Nuance Communications, Inc. | Embedded system for construction of small footprint speech recognition with user-definable constraints |
US9093076B2 (en) | 2012-04-30 | 2015-07-28 | 2236008 Ontario Inc. | Multipass ASR controlling multiple applications |
KR20130133629A (en) | 2012-05-29 | 2013-12-09 | 삼성전자주식회사 | Method and apparatus for executing voice command in electronic device |
US10354650B2 (en) * | 2012-06-26 | 2019-07-16 | Google Llc | Recognizing speech with mixed speech recognition models to generate transcriptions |
US20140006825A1 (en) * | 2012-06-30 | 2014-01-02 | David Shenhav | Systems and methods to wake up a device from a power conservation state |
US9536528B2 (en) | 2012-07-03 | 2017-01-03 | Google Inc. | Determining hotword suitability |
JP6131537B2 (en) | 2012-07-04 | 2017-05-24 | セイコーエプソン株式会社 | Speech recognition system, speech recognition program, recording medium, and speech recognition method |
TWI474317B (en) | 2012-07-06 | 2015-02-21 | Realtek Semiconductor Corp | Signal processing apparatus and signal processing method |
US9058806B2 (en) | 2012-09-10 | 2015-06-16 | Cisco Technology, Inc. | Speaker segmentation and recognition based on list of speakers |
US8983836B2 (en) | 2012-09-26 | 2015-03-17 | International Business Machines Corporation | Captioning using socially derived acoustic profiles |
WO2014064324A1 (en) * | 2012-10-26 | 2014-05-01 | Nokia Corporation | Multi-device speech recognition |
US8996372B1 (en) | 2012-10-30 | 2015-03-31 | Amazon Technologies, Inc. | Using adaptation data with cloud-based speech recognition |
US9704486B2 (en) * | 2012-12-11 | 2017-07-11 | Amazon Technologies, Inc. | Speech recognition power management |
US9256269B2 (en) | 2013-02-20 | 2016-02-09 | Sony Computer Entertainment Inc. | Speech recognition system for performing analysis to a non-tactile inputs and generating confidence scores and based on the confidence scores transitioning the system from a first power state to a second power state |
US9349386B2 (en) | 2013-03-07 | 2016-05-24 | Analog Device Global | System and method for processor wake-up based on sensor data |
US9361885B2 (en) | 2013-03-12 | 2016-06-07 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US9312826B2 (en) | 2013-03-13 | 2016-04-12 | Kopin Corporation | Apparatuses and methods for acoustic channel auto-balancing during multi-channel signal extraction |
US8768687B1 (en) | 2013-04-29 | 2014-07-01 | Google Inc. | Machine translation of indirect speech |
US9058805B2 (en) | 2013-05-13 | 2015-06-16 | Google Inc. | Multiple recognizer speech recognition |
WO2015025330A1 (en) | 2013-08-21 | 2015-02-26 | Kale Aaditya Kishore | A system to enable user to interact with an electronic processing device using voice of the user |
US9865255B2 (en) | 2013-08-29 | 2018-01-09 | Panasonic Intellectual Property Corporation Of America | Speech recognition method and speech recognition apparatus |
US9343068B2 (en) * | 2013-09-16 | 2016-05-17 | Qualcomm Incorporated | Method and apparatus for controlling access to applications having different security levels |
US8775191B1 (en) | 2013-11-13 | 2014-07-08 | Google Inc. | Efficient utterance-specific endpointer triggering for always-on hotwording |
US9373321B2 (en) | 2013-12-02 | 2016-06-21 | Cypress Semiconductor Corporation | Generation of wake-up words |
US8938394B1 (en) | 2014-01-09 | 2015-01-20 | Google Inc. | Audio triggers based on context |
US9639854B2 (en) * | 2014-06-26 | 2017-05-02 | Nuance Communications, Inc. | Voice-controlled information exchange platform, such as for providing information to supplement advertising |
US9257120B1 (en) | 2014-07-18 | 2016-02-09 | Google Inc. | Speaker verification using co-location information |
US10275138B2 (en) * | 2014-09-02 | 2019-04-30 | Sonos, Inc. | Zone recognition |
US9424841B2 (en) | 2014-10-09 | 2016-08-23 | Google Inc. | Hotword detection on multiple devices |
US9318107B1 (en) | 2014-10-09 | 2016-04-19 | Google Inc. | Hotword detection on multiple devices |
US9812126B2 (en) * | 2014-11-28 | 2017-11-07 | Microsoft Technology Licensing, Llc | Device arbitration for listening devices |
JP6754184B2 (en) * | 2014-12-26 | 2020-09-09 | パナソニック インテレクチュアル プロパティ コーポレーション オブ アメリカＰａｎａｓｏｎｉｃ Ｉｎｔｅｌｌｅｃｔｕａｌ Ｐｒｏｐｅｒｔｙ Ｃｏｒｐｏｒａｔｉｏｎ ｏｆ Ａｍｅｒｉｃａ | Voice recognition device and voice recognition method |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US10506068B2 (en) * | 2015-04-06 | 2019-12-10 | Microsoft Technology Licensing, Llc | Cloud-based cross-device digital pen pairing |
US10026399B2 (en) * | 2015-09-11 | 2018-07-17 | Amazon Technologies, Inc. | Arbitration between voice-enabled devices |
US9875081B2 (en) * | 2015-09-21 | 2018-01-23 | Amazon Technologies, Inc. | Device selection for providing a response |
US9967330B2 (en) * | 2015-12-01 | 2018-05-08 | Dell Products L.P. | Virtual resource bank for localized and self determined allocation of resources |
US10678828B2 (en) * | 2016-01-03 | 2020-06-09 | Gracenote, Inc. | Model-based media classification service using sensed media noise characteristics |
US9972320B2 (en) | 2016-08-24 | 2018-05-15 | Google Llc | Hotword detection on multiple devices |
-
2016
- 2016-09-28 US US15/278,269 patent/US9972320B2/en active Active
-
2017
- 2017-08-02 CN CN201780052132.0A patent/CN109791763B/en active Active
- 2017-08-02 WO PCT/US2017/045123 patent/WO2018038888A1/en unknown
- 2017-08-02 EP EP17751221.7A patent/EP3501022A1/en active Pending
- 2017-08-15 DE DE202017104895.0U patent/DE202017104895U1/en active Active
-
2018
- 2018-04-13 US US15/952,434 patent/US10242676B2/en active Active
-
2019
- 2019-03-22 US US16/362,284 patent/US10714093B2/en active Active
-
2020
- 2020-05-28 US US16/885,790 patent/US11276406B2/en active Active
-
2022
- 2022-03-10 US US17/691,698 patent/US11887603B2/en active Active
-
2024
- 2024-01-18 US US18/416,680 patent/US20240153507A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US9972320B2 (en) | 2018-05-15 |
US20240153507A1 (en) | 2024-05-09 |
US11276406B2 (en) | 2022-03-15 |
US10714093B2 (en) | 2020-07-14 |
WO2018038888A1 (en) | 2018-03-01 |
US10242676B2 (en) | 2019-03-26 |
US20200365159A1 (en) | 2020-11-19 |
US20190287534A1 (en) | 2019-09-19 |
CN109791763A (en) | 2019-05-21 |
US20180286406A1 (en) | 2018-10-04 |
CN109791763B (en) | 2024-03-01 |
US20220199090A1 (en) | 2022-06-23 |
US20180061419A1 (en) | 2018-03-01 |
EP3501022A1 (en) | 2019-06-26 |
US11887603B2 (en) | 2024-01-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
DE202017104895U1 (en) | Hotword detection on multiple devices | |
DE102017121086B4 (en) | INTERACTIVE VOICE ACTIVATED DEVICES | |
DE102017012415B4 (en) | Identification of a virtual assistant from nearby computing devices | |
JP6750125B2 (en) | Hot word trigger suppression for recording media | |
DE202015010012U1 (en) | Wake word detection on multiple devices | |
DE102015110621B4 (en) | Smart subtitles | |
DE102019112380A1 (en) | Method and system for robust speaker recognition activation | |
DE112011103893B4 (en) | Improve the scalability of a multipoint conference for co-located subscribers | |
DE202017105819U1 (en) | Multi-user personalization on a voice interface device | |
DE202017106466U1 (en) | Real-time streaming dialog management | |
DE212015000185U1 (en) | Social memories | |
DE112017000142T5 (en) | MULTIMODAL TRANSMISSION OF PACKAGED DATA | |
DE102011107992A1 (en) | System and method for logging to events based on keywords | |
DE102017115383A1 (en) | AUDIO SLICER | |
DE202017105741U1 (en) | Automated speech pronunciation allocation | |
DE112017000177T5 (en) | Authentication of packetized audio signals | |
DE112016002055T5 (en) | PRE-DECRODUCTION FOR APPLICATION STREAMING | |
DE102016125141A1 (en) | Search result with previous retrieval of language requests | |
DE102021122502B4 (en) | Execution of voice commands | |
CA2927331C (en) | Systems and methods for transcript processing | |
DE102017122216A1 (en) | Multi-user personalization on a voice interface device | |
DE202017104587U1 (en) | Gesture-activated remote control | |
DE102021207673A1 (en) | INTERRUPT FOR NOISE CANCELLATION AUDIO DEVICES | |
DE102021206372A1 (en) | INTELLIGENT ROUTING FOR AUDIO OUTPUT DEVICES | |
US20160062610A1 (en) | Information Display System That Displays Appropriate Information Corresponding to Contents of Ongoing Conference or Presentation on Terminal and Recording Medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
R207 | Utility model specification | ||
R081 | Change of applicant/patentee |
Owner name: GOOGLE LLC (N.D.GES.D. STAATES DELAWARE), MOUN, USFree format text: FORMER OWNER: GOOGLE INC., MOUNTAIN VIEW, CALIF., US |
|
R082 | Change of representative |
Representative=s name: MAIKOWSKI & NINNEMANN PATENTANWAELTE PARTNERSC, DE |
|
R150 | Utility model maintained after payment of first maintenance fee after three years | ||
R151 | Utility model maintained after payment of second maintenance fee after six years |