CN111602136A - Method for creating histopathology ground truth masks using slide re-staining - Google Patents
Method for creating histopathology ground truth masks using slide re-staining Download PDFInfo
- Publication number
- CN111602136A CN111602136A CN201880086389.2A CN201880086389A CN111602136A CN 111602136 A CN111602136 A CN 111602136A CN 201880086389 A CN201880086389 A CN 201880086389A CN 111602136 A CN111602136 A CN 111602136A
- Authority
- CN
- China
- Prior art keywords
- image
- stain
- tissue sample
- image data
- ground truth
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 title claims abstract description 70
- 238000010186 staining Methods 0.000 title claims abstract description 19
- WZUVPPKBWHMQCE-UHFFFAOYSA-N Haematoxylin Chemical compound C12=CC(O)=C(O)C=C2CC2(O)C1C1=CC=C(O)C(O)=C1OC2 WZUVPPKBWHMQCE-UHFFFAOYSA-N 0.000 claims abstract description 24
- YQGOJNYOYNNSMM-UHFFFAOYSA-N eosin Chemical compound [Na+].OC(=O)C1=CC=CC=C1C1=C2C=C(Br)C(=O)C(Br)=C2OC2=C(Br)C(O)=C(Br)C=C21 YQGOJNYOYNNSMM-UHFFFAOYSA-N 0.000 claims abstract description 13
- 230000002055 immunohistochemical effect Effects 0.000 claims description 39
- 238000012549 training Methods 0.000 claims description 19
- 238000012545 processing Methods 0.000 claims description 15
- 238000010801 machine learning Methods 0.000 claims description 14
- 238000013528 artificial neural network Methods 0.000 claims description 13
- 230000007170 pathology Effects 0.000 claims description 13
- 238000005406 washing Methods 0.000 claims description 7
- 239000000126 substance Substances 0.000 claims description 5
- 239000003086 colorant Substances 0.000 claims description 3
- 238000002372 labelling Methods 0.000 claims description 3
- 238000012546 transfer Methods 0.000 claims description 2
- 210000004881 tumor cell Anatomy 0.000 abstract description 5
- 238000009877 rendering Methods 0.000 abstract description 2
- 210000001519 tissue Anatomy 0.000 description 79
- 210000004027 cell Anatomy 0.000 description 24
- 206010028980 Neoplasm Diseases 0.000 description 15
- 201000011510 cancer Diseases 0.000 description 12
- 230000008569 process Effects 0.000 description 7
- 241001510071 Pyrrhocoridae Species 0.000 description 6
- 238000013527 convolutional neural network Methods 0.000 description 6
- WSFSSNUMVMOOMR-UHFFFAOYSA-N Formaldehyde Chemical compound O=C WSFSSNUMVMOOMR-UHFFFAOYSA-N 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- LFQSCWFLJHTTHZ-UHFFFAOYSA-N Ethanol Chemical compound CCO LFQSCWFLJHTTHZ-UHFFFAOYSA-N 0.000 description 3
- 238000003745 diagnosis Methods 0.000 description 3
- 238000003384 imaging method Methods 0.000 description 3
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 3
- CTQNGGLPUBDAKN-UHFFFAOYSA-N O-Xylene Chemical compound CC1=CC=CC=C1C CTQNGGLPUBDAKN-UHFFFAOYSA-N 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 239000011521 glass Substances 0.000 description 2
- 239000012188 paraffin wax Substances 0.000 description 2
- 238000007447 staining method Methods 0.000 description 2
- 239000008096 xylene Substances 0.000 description 2
- 241000527994 Cyclotella gamma Species 0.000 description 1
- 102000011782 Keratins Human genes 0.000 description 1
- 108010076876 Keratins Proteins 0.000 description 1
- 206010027476 Metastases Diseases 0.000 description 1
- 108010037490 Peptidyl-Prolyl Cis-Trans Isomerase NIMA-Interacting 4 Proteins 0.000 description 1
- 102100031653 Peptidyl-prolyl cis-trans isomerase NIMA-interacting 4 Human genes 0.000 description 1
- 206010060862 Prostate cancer Diseases 0.000 description 1
- 208000000236 Prostatic Neoplasms Diseases 0.000 description 1
- 230000002378 acidificating effect Effects 0.000 description 1
- 239000000427 antigen Substances 0.000 description 1
- 102000036639 antigens Human genes 0.000 description 1
- 108091007433 antigens Proteins 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 210000004556 brain Anatomy 0.000 description 1
- 210000000481 breast Anatomy 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 230000002596 correlated effect Effects 0.000 description 1
- 230000000875 corresponding effect Effects 0.000 description 1
- 230000002380 cytological effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 201000010099 disease Diseases 0.000 description 1
- 208000037265 diseases, disorders, signs and symptoms Diseases 0.000 description 1
- 210000002919 epithelial cell Anatomy 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 210000005260 human cell Anatomy 0.000 description 1
- 238000010191 image analysis Methods 0.000 description 1
- 230000001788 irregular Effects 0.000 description 1
- 210000002751 lymph Anatomy 0.000 description 1
- 210000001165 lymph node Anatomy 0.000 description 1
- 230000009401 metastasis Effects 0.000 description 1
- 238000000386 microscopy Methods 0.000 description 1
- 230000000877 morphologic effect Effects 0.000 description 1
- DHRLEVQXOMLTIM-UHFFFAOYSA-N phosphoric acid;trioxomolybdenum Chemical compound O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.O=[Mo](=O)=O.OP(O)(O)=O DHRLEVQXOMLTIM-UHFFFAOYSA-N 0.000 description 1
- 210000002307 prostate Anatomy 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 230000004083 survival effect Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 238000002560 therapeutic procedure Methods 0.000 description 1
- 239000013598 vector Substances 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/60—Type of objects
- G06V20/69—Microscopic objects, e.g. biological cells or cellular parts
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/60—Type of objects
- G06V20/69—Microscopic objects, e.g. biological cells or cellular parts
- G06V20/695—Preprocessing, e.g. image segmentation
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01N—INVESTIGATING OR ANALYSING MATERIALS BY DETERMINING THEIR CHEMICAL OR PHYSICAL PROPERTIES
- G01N1/00—Sampling; Preparing specimens for investigation
- G01N1/28—Preparing specimens for investigation including physical details of (bio-)chemical methods covered elsewhere, e.g. G01N33/50, C12Q
- G01N1/30—Staining; Impregnating ; Fixation; Dehydration; Multistep processes for preparing samples of tissue, cell or nucleic acid material and the like for analysis
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01N—INVESTIGATING OR ANALYSING MATERIALS BY DETERMINING THEIR CHEMICAL OR PHYSICAL PROPERTIES
- G01N1/00—Sampling; Preparing specimens for investigation
- G01N1/28—Preparing specimens for investigation including physical details of (bio-)chemical methods covered elsewhere, e.g. G01N33/50, C12Q
- G01N1/34—Purifying; Cleaning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/0002—Inspection of images, e.g. flaw detection
- G06T7/0012—Biomedical image inspection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30004—Biomedical image processing
- G06T2207/30024—Cell structures in vitro; Tissue sections in vitro
Abstract
A method for generating a ground truth mask for a microscope slide on which a tissue sample is placed includes the step of staining the tissue sample with a hematoxylin and eosin (H & E) stain. A first magnified image of the H & E stained tissue specimen is obtained, for example, with a whole slide scanner. The H & E stain is then washed from the tissue sample. A second, different stain is applied to the tissue sample, e.g., a particular stain, e.g., an IHC stain. A second magnified image of the tissue sample stained with a second different stain is obtained. The first and second magnified images are then registered with each other. Annotation (e.g., a rendering operation) is then performed on the first or second magnified images to form a ground truth mask in the form of a closed polygonal region that encompasses tumor cells present in the first or second magnified images.
Description
Background
The present disclosure relates to the field of digital pathology, and more particularly, to a method for generating a ground truth mask in a digital image of a tissue sample. In this document, the term "mask" refers to a closed polygonal region in the tissue sample image that surrounds a region of interest, such as a tumor cell (e.g., cancer). In this document, the term "ground truth" refers to the fact that a mask is provided by direct observation of the sample (i.e., empirical evidence), as opposed to information provided by inference, and a label (e.g., "cancerous") that can be assigned to a tissue sample.
Digital images of tissue samples having a ground truth mask and associated labels (e.g., "cancerous") for the sample are used in a variety of scenarios, including as training examples for constructing machine learning models. Such machine learning models may be developed for a variety of purposes, including aiding diagnosis, clinical decision support, and making predictions about patients providing tissue samples, such as predicting survival or response to therapy.
Machine learning models for making predictions from images of slides containing tissue samples require accurate ground truth masks and label assignments for the samples. As mentioned above, the mask takes the form of a polygon that outlines a region of interest, such as a tumor cell. One example of a method for creating a mask is described in pending U.S. patent application serial No. 15/621837, filed 2017, 6, 13, of gamma, the contents of which are incorporated herein by reference. Creating accurate ground truth masks and assigning labels is labor intensive and fatiguing, and in addition, pathologists can make rating errors (gradingerror). The present disclosure describes methods for more accurate and faster creation of ground truth masks.
PCT application serial No. PCT/US2017/019051 filed on 23/2/2017 and the scientific and technical literature cited therein, the contents of which are incorporated herein by reference, discloses a neural network pattern recognizer for recognizing cancer cells in digitally magnified images of tissue samples. The inclusion-v 3 deep convolutional neural network architecture on which current pattern recognizers are based is described in the scientific literature. See the following references, the contents of which are incorporated herein by reference: c.szegydy et al,Going Deeper with Convolutions,arXiv:1409.4842[cs.CV](September 2014)；C.Szegedy et al.,Rethinking the Inception Architecture for Computer Vision,arXiv:1512.00567[cs.CV](December 2015); see also U.S. patent application serial No. 14/839,452 filed on 28.8.2015 of c.szegyd et al "Processing Images Using Deep Neural Networks". The fourth generation (referred to as inclusion-v 4) is considered an alternative architecture for the pattern recognizer. See C.Szegedy et al, inclusion-v 4, inclusion-ResNet and the Impact of residual connections on Learning, arXiv:1602.0761[ cs](February 2016). See also U.S. patent application serial No. 15/395,530, "ImageClassification Neural Networks," filed on 30.12.2016, c.vanhoucke. The description of convolutional neural networks in these papers and patent applications is incorporated herein by reference.
In today's pathology, staging and diagnosis of cancer from tissue samples is typically performed on H & E (hematoxylin and eosin) stained tissue samples. Furthermore, machine learning models are typically trained from images of H & E stained tissue samples. H & E stains are non-specific stains that highlight the overall morphology of the tissue. In contrast, there are specific stains (including immunohistochemical stains, IHC, Verhoeffs stain, Masson's trichrome stain, etc.) that highlight specific antigens, such as tumor markers. A pathologist can usually provide (render) a diagnosis and delineate the tumor on the H & E image, but sometimes for difficult cases, images with a specific stain, such as HC stain, are required. It is also believed that the use of IHC stained (or other specific staining) slides may speed up the human examination and labeling process. However, in general, tissue slides may have either an H & E stain or an IHC stain, but generally cannot have both. Thus, a common solution is to cut a continuous slice of tissue and subject both to staining, microscopy and digital image capture, where slice N is stained with H & E and slice N +1 is stained with an IHC stain so that the IHC stained tissue is sufficiently similar in morphology (about 5 microns away, about 5% of human cell diameter) to be used with the H & E stained tissue image. However, morphological differences in successive slices are still significant and can lead to inaccuracies in the ground truth labels and masks generated in this manner. Another factor that further increases the difference in morphology of successive slices is that the two slices are processed separately during processing, and that the stretching of the two tissues may differ slightly during processing (e.g., placing the freshly cut glass slices on a glass carrier).
Disclosure of Invention
In one aspect, the present disclosure utilizes a technique referred to as "re-staining" individual tissue samples on slides to create accurate ground truth masks and associated labels for the tissue samples. Although in the prior art methods described above, different serial sections of tissue samples were obtained and two different tissue samples were subjected to H & E staining/imaging and IHC (or other specific) staining/imaging, the methods of the present disclosure provide for creating a ground truth mask using H & E and specific staining from a single tissue sample. In this way, the tissue for both images will be the same, and therefore, it is possible and easily done to switch the ground truth mask from both the specifically stained image and the H & E stained image.
In one aspect, a method for generating a mask for a digital image of a tissue sample is provided. The method comprises the following steps: receiving first image data representing a tissue sample stained with a first stain (e.g., H & E); receiving second image data representing the tissue sample after washing the tissue sample to remove the first stain from the tissue sample and staining the tissue sample with a second stain (e.g., a particular stain); registering (register) the first image data and the second image data to generate registered data; receiving data indicative of a region of interest represented in the second image data; and determining a mask in the first image data based on the received data indicative of the region of interest represented in the second image data and the registration data. The identification of data indicative of a region of interest in the second image data may be performed by a user (herein labeled) or may be performed by an algorithm. Once the region of interest is identified and the mask is created in the second image data, it may be transferred to the first image data due to the registration of the first and second images.
The method can of course be performed on a large number of slides containing tissue samples, creating a training set of H & E slides with a ground truth mask around cancer or other cells. Such a set of digital images may be provided to a neural network pattern recognizer as a training set for training the pattern recognizer. The neural network pattern recognizer may be, for example, a design cited in the previously cited scientific and patent literature.
In some aspects, the method may further include one or more of the following features. The mask and the second image data may be provided to a neural network pattern recognizer as a training example for automatically recognizing the region of interest. The data indicative of the region of interest represented in the first image data may be a region of interest comprising cells determined to be cancerous cells. Receiving data indicative of a region of interest represented in the first image data may include processing the first image data, for example, with an algorithm or automated process in software. For example, processing the first image data may include determining pixels of the first image data having a predetermined value (e.g., above a stain color threshold). Processing the first image data may include processing the first image data using a trained classifier or a pattern recognizer. The data indicative of the region of interest represented by the first image data may be annotation data input by a user. Receiving data indicative of a region of interest represented in the first image data may include displaying the first image data to a user. Receiving data indicative of a region of interest represented in the first image data may include displaying the first image data and the second image data side-by-side.
In one aspect, a method for generating a ground truth mask and label associated with a microscope slide containing a tissue sample is provided. The method assumes, as input, that a tissue sample (e.g., formalin fixed and paraffin embedded) is prepared and applied to a microscope slide. The method comprises the following steps:
1. the slides are stained with a first stain (e.g., hematoxylin and eosin (H & E), a widely known cytological staining procedure).
2. The stained slide is scanned and imaged by an entire slide scanner with a high resolution digital camera, typically at a magnification M such as 10X, 20X, or 40X. This first digital image of the slide stained with the first stain (e.g., H & E) is stored in memory, for example, in memory of the entire slide scanner or memory of an associated pathology workstation.
3. The slide is then subjected to a washing step to wash the first stain from the tissue on the slide.
4. A second, different (i.e., specific) stain is applied to the slide. This second or specific stain may be used in one of a variety of immunohistochemical chemical (IHC) stains, such as PIN4 for prostate cancer, or cytokeratin AE1/AE3 for lymph node epithelial cell metastasis, Verhoeff stain, Masson trichrome stain, and the like. In the present disclosure, the re-staining, denoted by the term "second stain", is intended to encompass a second stain consisting of a single specific stain and also of a plurality of specific stains, sometimes referred to in the art as a "multiple stain".
5. The slides stained with the second stain are scanned and imaged at the same magnification level M by the entire slide scanner with electronic camera. This second digital image of the slide is again stored in memory.
6. The first digital image and the second digital image of the slide are aligned and registered with respect to each other. Assuming that the tissues are physically identical, the residual alignment error should be close to zero.
7. The first digital image or the second digital image is annotated so as to form a ground truth mask in the form of a closed polygonal region surrounding the tumor cells present in the first image or the second image. For example, the registered first and second digital images are displayed together (superimposed or displayed side-by-side as an image pair) using a tool on a workstation and used by a human annotator to generate (i.e., draw) a ground truth mask on the slide image and assign a label to the slide. For example, a human annotator can draw a polygonal outline (i.e., a mask) of a cancer cell on the second image or the first image. One example of the figure may be in accordance with the teachings of pending U.S. patent application serial No. 15/621837 filed on 2017, 6 month 13 of c.gamma. If the annotator draws a mask on the second image, the mask is transferred to the first (H & E) image, which is possible because the two images are aligned or registered with each other.
In one variation of the above method, steps 1-6 are performed, but the ground truth mask is algorithmically (i.e., automatically using software instructions) created from the second (specific stain) image and transferred to the first (H & E) image. The algorithm creating the ground truth mask in the second image may be performed using one of several possible techniques:
a) thresholding the second image on a particular stain color and drawing a polygon around a region having pixels with stain colors above a threshold; or
b) A neural network or other classifier trained to discriminate cancerous cells in the stained tissue sample image is used, and the classifier is applied to the second image.
In one possible embodiment, after creating the ground truth mask and label, in any of the variations described above, the first digital image (H & E) with the mask and label is then provided as a training instance to a machine learning system trained to discriminate cancer cells in stained tissue images.
In another aspect, a method for generating a ground truth mask for a digital image of a tissue sample is provided. The tissue sample is placed on a microscope slide. The method comprises the following steps:
a) obtaining a first magnified image of the tissue sample stained with a first stain;
b) obtaining a second magnified image of the tissue sample stained with a second different stain;
c) registering the first and second magnified images with each other; and
d) forming a ground truth mask in the second magnified image, the ground truth mask being in the form of a closed polygonal region surrounding a region of the cell of interest present in the second magnified image; and
e) the ground truth mask is transferred from the second magnified image to the first magnified image.
In yet another aspect, a workstation is provided that includes a processing unit and a display. The display is configured to display a registered digitally magnified image of a single slide containing a tissue sample sequentially stained with (1) hematoxylin and eosin and (2) a second different stain, respectively. The workstation is configured with (a) a user interface tool through which an operator examining the registered digitally magnified images on the display can annotate the digitally magnified images of the tissue sample stained with the second stain to form closed polygons around a region of the images containing a region of cells of interest to create a ground truth mask; or (b) code that algorithmically creates a ground truth mask in the image. The workstation is further configured to transfer the ground truth mask to a digitally magnified image of the tissue sample stained with hematoxylin and eosin.
In another aspect, a system for creating a ground truth mask in a digitally amplified image of a tissue sample is disclosed. The system includes, in combination, an entire slide scanner for scanning a stained slide containing a tissue sample, a wash station configured with a means for washing H & E stains from the tissue sample and chemicals; and a pathology workstation as described in the preceding paragraph.
The pathology workstation or system above may further comprise a machine learning system for building a machine learning model from training examples from digitally enlarged images of tissue samples stained with hematoxylin and eosin. The training examples take the form of digitally magnified pathology images annotated by the pathology workstation and method of the present disclosure.
The methods described herein may be used to obtain images of tissue samples stained with common stains (e.g., hematoxylin and eosin), where regions of interest are identified that are typically difficult to identify in images stained with common stains. This may be achieved by identifying regions of interest in images of the same tissue sample stained with different stains (e.g., Immunohistochemical (IHC) stains), allowing the regions of interest to be more easily identified by manual input using the images by a trained operator or by using image processing techniques. The images with identified regions of interest stained with common stains may be used to train a neural network to identify corresponding regions of interest in images stained with the same stain. Such a trained neural network may be able to identify regions of interest with certain properties, for example including cancerous cells, using images that human operators typically find challenging to identify. In this way, images obtained using common stains such as H & E may be used to identify regions of interest that are not typically recognizable by a human operator.
It will be appreciated that aspects may be implemented in any convenient form. For example, aspects may be implemented by a suitable computer program, which may be carried on a suitable carrier medium, which may be a tangible carrier medium (e.g., a disk) or an intangible carrier medium (e.g., a communications signal). Aspects may also be implemented using suitable apparatus, which may take the form of a programmable computer running a computer program arranged to implement the invention. Aspects may be combined such that features described in the context of one aspect may be implemented in another aspect.
Drawings
FIG. 1 is a diagrammatic view of a laboratory environment including a pathology workstation in which the method is practiced.
FIG. 2 is a flow diagram illustrating a method for generating a ground truth mask in accordance with one embodiment.
Fig. 3 is an illustration of registered H & E and IHC magnified images of the same tissue sample shown side-by-side, wherein the IHC image has areas of darker contrast indicative of cancer cells in the tissue sample.
FIG. 4 is a flow diagram of a second embodiment of a method in which a ground truth mask is algorithmically created in a second magnified image (e.g., an IHC image).
Fig. 5 is a diagram of a portion of a tissue sample image in which a mask is algorithmically drawn or created over a region of interest, such as a tumor cell. It will be appreciated that for some tissue samples, there may be more than one mask created for a given slide image.
Fig. 6 is a more detailed illustration of the machine learning system of fig. 1.
Detailed Description
Turning attention now to fig. 1, fig. 1 is an illustration of a laboratory 100 environment in which the method is practiced. A tissue sample, typically formalin fixed and typically paraffin embedded, is placed on a microscope slide 102 and the tissue sample is placed into a stainer 104 that applies a stain to the tissue sample. Commercial stainers for applying H & E, IHC and other specific stains to tissue samples are known and available from many suppliers. Initially, the stainer 104 stains the slide with H & E stain and places a cover slip (cover slip) on the tissue sample. The slides are then provided to the entire slide scanner 106 as shown at 105. Such scanners are also widely known and available from many suppliers. The entire slide scanner 106 scans the slide at a user-specified magnification, such as 10X, 20X, or 40X. The entire slide scanner includes a digital camera for capturing an enlarged color digital image of the specimen. The digitally magnified images of the H & E stained slides are then stored, either locally throughout the slide scanner 106, in a cloud network, or on the local hard disk 114 of the pathology workstation 110.
After being scanned by the entire slide scanner 106, the slide is then sent to a wash station 108 containing a wash device and associated chemicals and trained personnel to wash out (i.e., remove) the H & E stain so that the slide can be re-stained with a second, different stain, such as a specific stain, e.g., an IHC stain or multiple stains. For example. The wash station 108 includes equipment and chemicals so that the user can perform any well-known procedure for washing H & E stains from tissue samples. In one example, the following procedure is employed:
1. the slides were immersed in xylene to remove the coverslip.
2. Once the coverslip was removed, the slide was rinsed several times with xylene. The slides were then rinsed 2 to 3 times with EtOH and then several times with water. Most eosin is usually washed off in water. The slide is now placed in acidic alcohol for one or two minutes. The slide was again rinsed with water.
After performing the procedure for washing the H & E stain from the tissue sample, the slide is then returned to the stainer 104 as shown at 109 and stained with a second different or specific stain (e.g., IHC stain) and then sent to the entire slide scanner 106 as shown at 105. The slide is scanned and imaged in the scanner 106 at the same magnification as the first (H & E) image, and a second digitally magnified image of the slide stained with a second, different stain is stored, for example, in memory in the scanner, in the cloud, or on the hard disk 114 of the workstation 110.
The two digitally magnified images of the tissue sample are then registered with each other. Registration may be performed in any convenient manner, for example, using well-known image processing techniques to determine the X/Y coordinates of various key features (such as high contrast regions, corners, boundaries, etc.) in the images, such that the X/Y pixel location of each key feature of one of the images may be correlated or matched with the X/Y pixel location of the same key feature in the other image. In one embodiment, SIFT (feature extraction method) is used to determine regions with sharpness or color distribution gradients to identify key features or locations (e.g., 10 key features) in an image and determine matching feature vectors. The registration step is performed for two reasons: 1) such that the first image 118 and the second image 120, respectively (fig. 1) can be displayed side-by-side or overlaid on each other on the display 112 of the workstation in a coordinated manner, and 2) the X/Y position of each vertex in the mask (polygon) can be transferred into the first (H & E) image when the user annotates one image, typically the second (IHC) image 120, to draw the mask. The workstation includes user interface tools including a keyboard 116, mouse, and display 112 to facilitate annotation of the image to draw a mask for the specimen and assign a label, e.g., "cancerous".
After performing the labeling process that generates masks for the samples and assigns labels, in one embodiment, the labeled H & E images of the tissue samples with the masks are provided to the machine learning system 130 as training examples of the machine learning model in the training system 130. This aspect will be described in more detail in a later part of this document in connection with fig. 6.
FIG. 2 is a flow diagram illustrating a method for generating a ground truth mask in a stained tissue sample according to one embodiment using the system of FIG. 1 described above. In step 202, the sample is stained with an H & E stain. At step 204, a slide containing a stained tissue sample is scanned at a magnification M (e.g., 10X, 20, and/or 40X) throughout the slide scanner 106. The scanner generates a first digital (H & E) image 118. At step 206, the H & E stain is washed from the sample, for example, using the wash station 108 of fig. 1. At step 208, a second, different stain is applied to the sample, e.g., via the stainer 104 of fig. 1. The second stain may be, for example, an IHC stain, a nuclear stain, or some other specific stain related to the tissue type or possible disease state in question. At step 210, a slide carrying a specimen stained with a second, different stain is scanned with the entire slide scanner. The slide is typically scanned at the same magnification M as it is scanned when H & E staining is employed, however it is also possible to scan at one magnification and then down-sample or up-sample the image data to obtain digital image data at other magnifications that may be used. In fig. 1 and 2, the second digital image is indicated at 120.
As described above, at step 212, the two digital images are registered with each other.
At step 214, creation of a ground truth mask is performed, for example, using a user interface tool and a side-by-side display of the registered images. The user annotates the first digital image or the second digital image on the workstation 112 as shown in FIG. 1. The user annotation is used to generate a ground truth mask in the digital image. If a mask is created in the second image, it is transferred to the first (H & E) image.
Fig. 3 is an illustration of registered H & E118 and IHC 120 magnified images of the same tissue sample shown side-by-side, wherein the IHC image has areas of darker contrast indicative of cancer cells in the tissue sample. As an example of the annotation process, referring now to fig. 3, the user views a side-by-side display of two digital images and can see a high contrast dark brown region 300 in the IHC image by examining the IHC image 120 and then draw a polygon around the dark region within the high contrast region 300 using a simple drawing tool (using a keyboard or more typically a mouse (or touch sensitive display 112)). The rendering may be accomplished using the techniques of the previously cited U.S. patent application to Gamma. The user may wish to zoom in to view the area 300 in more detail. When a zoom control is used on the display, both images 118 and 120 are zoomed in together and maintain a registered or aligned state so that the user can see the same details in both images. When a polygonal area is drawn in one of the images, a mask is automatically created on the same pixel X/Y address in the other of the images because the two images are registered. It is also noted that in this method, the two images 118 and 120 are the same exact tissue, since only one tissue sample is used, and is stained and imaged with H & E and IHC stains in sequence. Fig. 5 is an illustrative example of a mask 504 (line) created in an image of a tissue sample 500 having various tissue features 502. Mask 504 is a closed polygon area with any arbitrary boundary composed of vertices and connecting lines. Typically, the mask is highly irregular in shape and is manually constructed according to the process of FIG. 2 by a user (typically a trained pathologist) operating the workstation and using simple controls on the workstation's user interface to draw the perimeter of the mask. The X/Y locations of the vertices of the mask boundary in the IHC image are stored in memory so that the mask can be transferred to the first (H & E) image.
A second embodiment of this method of automatically or algorithmically generating a ground truth mask is described in connection with fig. 4. Until the image registration step, the method is substantially the same as the process of fig. 6. Specifically, at step 402, the tissue sample is stained with H & E, at step 404, the sample is scanned at magnification M throughout the slide scanner, and a first digital H & E image is generated at 118. At step 406, the H & E stain is washed from the sample. At step 408, the slide is returned to the stainer of FIG. 1 and a second, different stain, e.g., an IHC stain, is applied to the sample. At step 410, the slide with the second stain is scanned at magnification M throughout the slide scanner, producing a second digital image 120. At step 412, the first digital image and the second digital image are registered with each other, as described above.
At step 414, a ground truth mask is algorithmically generated in the second digital image 120. Image analysis or classification methods are performed on the second (IHC) image in software to identify cancer cells in the image, and then construct closed polygons around these cells. The construction of closed polygons may be limited to detecting cancerous cells in regions with minimal size, thereby creating more useful masks and avoiding the creation of masks of isolated cells or small cell populations. The algorithmic creation of the ground truth mask in the second image may be performed using one of several possible techniques:
a) thresholding the second image on a particular stain color and drawing a polygon around a region having pixels with stain colors above a threshold; or
b) A neural network or other classifier trained to discriminate cancer cells in the stained tissue sample image is used to find cancer cells in the second image. For example, neural networks and other classification methods for automatically detecting regions of interest and cancerous cells in digital images based on staining cell morphology and pixel intensity variations in the digital images are known in the art, and therefore, a detailed description is omitted herein for brevity. See, e.g., PCT/US2017/019051 filed on 2017, 2/23 and discussion of previously cited scientific and patent literature.
Once any of the above methods are performed, the X/Y coordinates of the vertices of the polygons forming the mask are stored. The mask may then be rendered on the workstation display by showing the mask in the IHC image 120.
In addition, the mask created from the IHC image algorithm is then transferred to the H & E image 120. This may be due to the registration of the two images and because the known X/Y locations of the vertices in the IHC image 120 are directly translated to the H & E image. This step is useful for generating training examples for the machine learning system 130 of fig. 1. In particular, the machine learning system 130, shown in more detail in fig. 6, includes a number of training examples 600 in the form of enlarged H & E digital images of tissue samples, each sample having a mask or boundary (shown as a small rectangle in fig. 6) that depicts the presence of ground truth for cancer cells in the region. The training example 600 is used to train a classification engine, such as a Convolutional Neural Network (CNN) pattern recognizer 602, to discern the presence of cancerous cells in a tissue sample of the type of the training example 600 (e.g., prostate, breast, lymph, brain, or other tissue type). As shown at 604, the H & E image 118 with the mask 505 (created according to the process of fig. 2 or fig. 4) is added to the training instance set 600. Additionally, once the appropriate number of training instances 600 are obtained and the performance of the CNN pattern recognizer 602 reaches a point of high precision (and verified by an appropriate test set of individual images), step 414 of fig. 4 may be performed and a mask 505 generated using the CNN pattern recognizer 602. The training set 600 may be obtained from a library of tissue slices that may be subjected to the staining, re-staining, and imaging steps of the present disclosure.
A particular stain (e.g., IHC) image provides more specific insight into the morphology of cells and nuclei than creating a ground truth mask using only H & E images, and is easier to view due to the higher contrast of the region of interest caused by the stain. Compared to serial tissue sections (typically 5 microns apart) stained using H & E and IHC, the re-staining method of the present disclosure provides the following advantages: (1) the two images can be perfectly aligned with the smallest possible residual alignment error, and (2) because the exact same tissue is used, every feature present in one image is also present in the other image. For example, if there is a small tumor area in one slide, it may not be visible in an adjacent serial section slide. However, this is not the case with re-staining. Furthermore, the re-staining method allows for almost accurate registration of the H & E and IHC images, so the ground truth mask can be easily converted from the IHC image to the H & E image.
While in most cases it is expected that the tissue sample will be stained first with H & E and then with a second specific stain, this is not absolutely necessary, and it is possible to initially stain with a specific stain (e.g., IHC stain), collect a digital image at magnification M, wash away the specific stain, then re-stain with H & E, and then capture the H & E image at magnification M. This "reverse" approach may result in a reduction in the image quality of the H & E image because more re-staining marks are created when attempting to wash away a particular stain.
Claims (24)
1. A method for generating a mask for a digital image of a tissue sample, comprising:
receiving first image data representing a tissue sample stained with a first stain;
receiving second image data representing the tissue sample after washing the tissue sample to remove the first stain from the tissue sample and staining the tissue sample with a second stain;
registering the first image data and the second image data to generate registration data;
receiving data indicative of a region of interest represented in the second image data; and
a mask in the first image data is determined based on the received data indicative of the region of interest represented in the second image data and the registration data.
2. The method of claim 1, further comprising:
the neural network pattern recognizer is trained using the first image data and the determined mask.
3. The method of claim 1 or 2, wherein the first stain is hematoxylin and eosin.
4. The method of any preceding claim, wherein the data indicative of a region of interest represented in the second image data is a region of interest comprising cells determined to be cancerous cells.
5. The method of any preceding claim, wherein receiving data indicative of a region of interest represented in the second image data comprises processing the second image data.
6. The method of claim 5, wherein processing the second image data comprises determining pixels of the second image data having predetermined values.
7. The method of claim 6, wherein processing the second image data comprises processing the second image data using a trained classifier.
8. The method of any preceding claim, wherein the second stain comprises an Immunohistochemical (IHC) stain.
9. A method according to any preceding claim, wherein the data indicative of the region of interest represented in the second image data is annotation data input by a user.
10. The method of claim 9, wherein receiving data indicative of a region of interest represented in the second image data comprises displaying the second image data to a user.
11. The method of claim 9 or 10, wherein receiving data indicative of a region of interest represented in the second image data comprises displaying the first image data and the second image data side-by-side.
12. A system, comprising:
a memory storing processor readable instructions; and
one or more processors arranged to read and execute instructions stored in the memory;
wherein the processor readable instructions comprise instructions arranged to control a computer to perform the method of any preceding claim.
13. A method for generating a ground truth mask for a digital image of a tissue sample placed on a microscope slide, the method comprising the steps of:
a) obtaining a first magnified image of the tissue sample stained with a first stain;
b) obtaining a second magnified image of the tissue sample stained with a second different stain;
c) registering the first and second magnified images with each other;
d) forming a ground truth mask in the second magnified image in the form of a closed polygonal region surrounding a region of the cell of interest present in the second magnified image; and
e) the ground truth mask is transferred from the second magnified image to the first magnified image.
14. The method of claim 13, wherein the first stain comprises hematoxylin and eosin, and wherein the second stain comprises a specific stain.
15. The method of claim 13, wherein the second stain comprises an Immunohistochemical (IHC) stain.
16. The method of any of claims 13 to 15, wherein the labeling step further comprises the steps of: the first magnified image and the second magnified image are displayed side by side.
17. The method of any of claims 13 to 15, wherein the registering step further comprises the steps of: the superimposed first and second magnified images are displayed.
18. The method of any of claims 13 to 15, wherein the ground truth mask is formed by performing an annotation step on the second magnified image.
19. The method of any of claims 13 to 15, wherein the ground truth mask is formed by algorithmically creating a ground truth mask in the second magnified image.
20. The method of claim 13, wherein algorithmically creating a ground truth mask comprises the steps of: the second magnified image is thresholded over a particular stain color and a polygon is drawn around the region having pixels with stain colors above the threshold.
21. The method of claim 13, wherein algorithmically creating a ground truth mask comprises the steps of: a trained neural network pattern recognizer trained to discriminate cancerous cells in the stained tissue sample image is used to operate on the second image and to construct polygons around regions in the second image that the pattern recognizer deems to contain cancerous cells.
22. A pathology workstation configured to assist a user in generating ground truth masks for a stained slide containing a tissue sample, wherein the workstation comprises a processing unit and a display, wherein the display is configured to display a registered digitally magnified image of a single slide containing a tissue sample sequentially stained with (1) hematoxylin and eosin and (2) a second different stain, respectively,
wherein the workstation is configured with (a) user interface tools through which an operator reviewing the registered digitally magnified image on the display annotates the digitally magnified image of the tissue sample stained with the second stain to form closed polygons around a region of the image containing the cell region of interest to create a ground truth mask, or (b) code that algorithmically creates a ground truth mask in the image, and
wherein the workstation is further configured to transfer the ground truth mask to a digitally magnified image of the tissue sample stained with hematoxylin and eosin.
23. A system for creating a ground truth mask in a digital image of a tissue sample, comprising, in combination:
a whole slide scanner for scanning a stained slide containing a tissue sample;
a wash station configured with a means for washing a stain from a tissue sample and a chemical; and
the pathology workstation of claim 22.
24. The pathology workstation of claim 22, further comprising a machine learning system for building a machine learning model from training examples in the form of digital images of tissue samples stained with hematoxylin and eosin with ground truth masks around a region of interest in the samples, wherein the training examples comprise digital images labeled by the pathology workstation of claim 22.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2018/013353 WO2019139591A1 (en) | 2018-01-11 | 2018-01-11 | Method for creating histopathological ground truth masks using slide restaining |
Publications (1)
Publication Number | Publication Date |
---|---|
CN111602136A true CN111602136A (en) | 2020-08-28 |
Family
ID=61054597
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880086389.2A Pending CN111602136A (en) | 2018-01-11 | 2018-01-11 | Method for creating histopathology ground truth masks using slide re-staining |
Country Status (4)
Country | Link |
---|---|
US (1) | US11783604B2 (en) |
EP (1) | EP3732611A1 (en) |
CN (1) | CN111602136A (en) |
WO (1) | WO2019139591A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113256618A (en) * | 2021-06-23 | 2021-08-13 | 重庆点检生物科技有限公司 | Tumor identification system and method based on IHC staining |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6499643B2 (en) | 2013-10-01 | 2019-04-10 | ベンタナ メディカル システムズ， インコーポレイテッド | Line-based image registration and cross-image annotation device, system, and method |
CN111602136A (en) * | 2018-01-11 | 2020-08-28 | 谷歌有限责任公司 | Method for creating histopathology ground truth masks using slide re-staining |
EP3762934A4 (en) * | 2018-03-07 | 2022-04-06 | Verdict Holdings Pty Ltd | Methods for identifying biological material by microscopy |
JP2019195304A (en) | 2018-05-10 | 2019-11-14 | 学校法人順天堂 | Image analysis method, device, computer program, and generation method of deep learning algorithm |
US11633146B2 (en) | 2019-01-04 | 2023-04-25 | Regents Of The University Of Minnesota | Automated co-registration of prostate MRI data |
US11631171B2 (en) * | 2019-01-10 | 2023-04-18 | Regents Of The University Of Minnesota | Automated detection and annotation of prostate cancer on histopathology slides |
US11972621B2 (en) * | 2019-04-24 | 2024-04-30 | Icahn School Of Medicine At Mount Sinai | Systems and methods to label structures of interest in tissue slide images |
JP7381003B2 (en) * | 2019-04-26 | 2023-11-15 | 学校法人順天堂 | METHODS, APPARATUS AND COMPUTER PROGRAMS TO ASSIST DISEASE ANALYSIS AND METHODS, APPARATUS AND PROGRAMS FOR TRAINING COMPUTER ALGORITHM |
WO2022126010A1 (en) * | 2020-12-11 | 2022-06-16 | Tempus Labs, Inc. | Systems and methods for generating histology image training datasets for machine learning models |
US20220375604A1 (en) * | 2021-04-18 | 2022-11-24 | Mary Hitchcock Memorial Hospital, For Itself And On Behalf Of Dartmouth-Hitchcock Clinic | System and method for automation of surgical pathology processes using artificial intelligence |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1836253A (en) * | 2003-08-14 | 2006-09-20 | 美国西门子医疗解决公司 | System and method for locating compact objects in images |
US20110286654A1 (en) * | 2010-05-21 | 2011-11-24 | Siemens Medical Solutions Usa, Inc. | Segmentation of Biological Image Data |
US20170160171A1 (en) * | 2015-11-20 | 2017-06-08 | Oregon Health And Science University | Multiplex immunohistochemistry image cytometry |
US20170270420A1 (en) * | 2016-03-20 | 2017-09-21 | Definiens Ag | System for Predicting the Recurrence of Cancer in a Cancer Patient |
US20170372471A1 (en) * | 2016-06-28 | 2017-12-28 | Contextvision Ab | Method and system for detecting pathological anomalies in a digital pathology image and method for annotating a tissue slide |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7629125B2 (en) | 2006-11-16 | 2009-12-08 | General Electric Company | Sequential analysis of biological samples |
US8351676B2 (en) | 2010-10-12 | 2013-01-08 | Sony Corporation | Digital image analysis using multi-step analysis |
AU2015212984A1 (en) | 2014-01-28 | 2016-06-23 | Ventana Medical Systems, Inc. | Adaptive classification for whole slide tissue segmentation |
EP3250901A1 (en) * | 2015-01-31 | 2017-12-06 | Roche Diagnostics GmbH | Systems and methods for meso-dissection |
CN106885593A (en) | 2017-02-24 | 2017-06-23 | 美的集团股份有限公司 | A kind of household electrical appliance detection method and system |
US10013781B1 (en) | 2017-06-13 | 2018-07-03 | Google Llc | Sewing machine-style polygon drawing method |
CN111433817A (en) * | 2017-12-15 | 2020-07-17 | 威里利生命科学有限责任公司 | Generating a virtual stain image of an unstained sample |
CN111602136A (en) * | 2018-01-11 | 2020-08-28 | 谷歌有限责任公司 | Method for creating histopathology ground truth masks using slide re-staining |
-
2018
- 2018-01-11 CN CN201880086389.2A patent/CN111602136A/en active Pending
- 2018-01-11 US US16/959,725 patent/US11783604B2/en active Active
- 2018-01-11 WO PCT/US2018/013353 patent/WO2019139591A1/en unknown
- 2018-01-11 EP EP18701914.6A patent/EP3732611A1/en active Pending
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1836253A (en) * | 2003-08-14 | 2006-09-20 | 美国西门子医疗解决公司 | System and method for locating compact objects in images |
US20110286654A1 (en) * | 2010-05-21 | 2011-11-24 | Siemens Medical Solutions Usa, Inc. | Segmentation of Biological Image Data |
US20170160171A1 (en) * | 2015-11-20 | 2017-06-08 | Oregon Health And Science University | Multiplex immunohistochemistry image cytometry |
US20170270420A1 (en) * | 2016-03-20 | 2017-09-21 | Definiens Ag | System for Predicting the Recurrence of Cancer in a Cancer Patient |
US20170372471A1 (en) * | 2016-06-28 | 2017-12-28 | Contextvision Ab | Method and system for detecting pathological anomalies in a digital pathology image and method for annotating a tissue slide |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113256618A (en) * | 2021-06-23 | 2021-08-13 | 重庆点检生物科技有限公司 | Tumor identification system and method based on IHC staining |
Also Published As
Publication number | Publication date |
---|---|
WO2019139591A1 (en) | 2019-07-18 |
EP3732611A1 (en) | 2020-11-04 |
US20200372235A1 (en) | 2020-11-26 |
US11783604B2 (en) | 2023-10-10 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11783604B2 (en) | Method for creating histopathological ground truth masks using slide restaining | |
US20230419694A1 (en) | Virtual staining for tissue slide images | |
CN111727436B (en) | Pathological prediction of unstained tissue | |
US20200388033A1 (en) | System and method for automatic labeling of pathology images | |
CA2866118C (en) | Automatic image alignment | |
Paramanandam et al. | Automated segmentation of nuclei in breast cancer histopathology images | |
CN112435243A (en) | Automatic analysis system and method for full-slice digital pathological image | |
US20050123181A1 (en) | Automated microscope slide tissue sample mapping and image acquisition | |
US20200175325A1 (en) | Systems and methods for encoding image features of high-resolution digital images of biological specimens | |
EP3640837A1 (en) | System for co-registration of medical images using a classifier | |
US20090304244A1 (en) | Method and a system for presenting sections of a histological specimen | |
EP3440629B1 (en) | Spatial index creation for ihc image analysis | |
Jen et al. | In silico multi-compartment detection based on multiplex immunohistochemical staining in renal pathology | |
WO2016076104A1 (en) | Image processing method, image processing device, and program | |
US11983912B2 (en) | Pathology predictions on unstained tissue | |
Taheri et al. | Robust nuclei segmentation in cyto-histopathological images using statistical level set approach with topology preserving constraint | |
US20230395238A1 (en) | System and method for virtual and chemical staining of tissue samples | |
WO2024025969A1 (en) | Architecture-aware image tiling for processing pathology slides | |
CN115917594A (en) | Entire slide annotation transfer using geometric features | |
JP2021525890A (en) | Computer implementation process for images of biological samples | |
Kladnik | Test PDF submission |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |