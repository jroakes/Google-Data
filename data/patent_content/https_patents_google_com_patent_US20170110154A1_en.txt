US20170110154A1 - Generating videos of media items associated with a user - Google Patents
Generating videos of media items associated with a user Download PDFInfo
- Publication number
- US20170110154A1 US20170110154A1 US14/885,285 US201514885285A US2017110154A1 US 20170110154 A1 US20170110154 A1 US 20170110154A1 US 201514885285 A US201514885285 A US 201514885285A US 2017110154 A1 US2017110154 A1 US 2017110154A1
- Authority
- US
- United States
- Prior art keywords
- media
- video
- item
- rule
- items
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 claims abstract description 59
- 230000007704 transition Effects 0.000 claims abstract description 52
- 230000015654 memory Effects 0.000 claims description 41
- 230000001360 synchronised effect Effects 0.000 claims description 14
- 230000000694 effects Effects 0.000 claims description 9
- 238000004590 computer program Methods 0.000 claims description 5
- 238000012986 modification Methods 0.000 claims description 4
- 230000004048 modification Effects 0.000 claims description 4
- 230000003068 static effect Effects 0.000 claims description 4
- 230000006870 function Effects 0.000 claims description 3
- 238000004891 communication Methods 0.000 description 25
- 230000011218 segmentation Effects 0.000 description 21
- 230000008569 process Effects 0.000 description 9
- 238000010586 diagram Methods 0.000 description 5
- 238000012545 processing Methods 0.000 description 4
- 230000010267 cellular communication Effects 0.000 description 3
- 230000001815 facial effect Effects 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- RZVAJINKPMORJF-UHFFFAOYSA-N Acetaminophen Chemical compound CC(=O)NC1=CC=C(O)C=C1 RZVAJINKPMORJF-UHFFFAOYSA-N 0.000 description 1
- 206010011469 Crying Diseases 0.000 description 1
- 241000287181 Sturnus vulgaris Species 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 230000008451 emotion Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 239000000126 substance Substances 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/02—Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers
- G11B27/031—Electronic editing of digitised analogue information signals, e.g. audio or video signals
- G11B27/036—Insert-editing
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/02—Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers
- G11B27/031—Electronic editing of digitised analogue information signals, e.g. audio or video signals
- G11B27/034—Electronic editing of digitised analogue information signals, e.g. audio or video signals on discs
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/102—Programmed access in sequence to addressed parts of tracks of operating record carriers
- G11B27/105—Programmed access in sequence to addressed parts of tracks of operating record carriers of operating discs
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
- G11B27/28—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/34—Indicating arrangements
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/222—Studio circuitry; Studio devices; Studio equipment
- H04N5/262—Studio circuits, e.g. for mixing, switching-over, change of character of image, other special effects ; Cameras specially adapted for the electronic generation of special effects
- H04N5/2628—Alteration of picture size, shape, position or orientation, e.g. zooming, rotation, rolling, perspective, translation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/7605—Television signal recording on discs or drums
Definitions
- Implementations generally relate to media items and videos.
- a method to generate a video includes grouping media items associated with a user into segments based on a timestamp associated with each media item and a total number of media items. The method may further include selecting target media from the media items for each of the segments based on media attributes associated with the media item, wherein the target media includes two or more media items.
- the method may further include generating the video that includes the target media for each of the segments by: generating a first animation that illustrates a first transition from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, wherein the first item is illustrated as being attached to the second item in the first animation, determining whether the target media includes one or more additional items, and responsive to determining that the target media includes one or more additional items, generating a second animation that illustrates a second transition from the second item to a first additional item and that further illustrates additional transitions between a remainder of the one or more additional items.
- the method may further include adding a song to the video, wherein one or more features of the song are synchronized with the first transition and the second transition.
- the method may further include the timestamp being associated with each media item represents at least one of an upload time associated with uploading each media item, a capture time associated with capturing each media item, a creation time associated with creating each media item, and a last modification made to each media item.
- the method may further include selecting the media items from media associated with the user based on the timestamp being indicative of a particular time period.
- Generating the first animation may further include illustrating a zoom-out effect on the first item, wherein the zoom-out effect is synchronized to the one or more features of the song.
- the method may further include grouping the media item associated with the user into the plurality of segments by grouping media items associated with an event into two or more segments based on an event number of media items associated with the event as compared to the total number of media items.
- the method may further include selecting the target media from the media item for each of the plurality of segments based on the media attributes associated with the media item includes selecting based on a share tendency.
- the method may further include a number of target media being a function of the total number of media items as determined prior to the grouping of the media items.
- a system to generate a video includes one or more processors coupled to a memory, a media selector module, and a video generation module.
- the media selector module may be stored in the memory and executable by the one or more processors to select target media from media items by, for each of the media items: randomly selecting a first rule from a plurality of rules, determining whether the media item satisfies the first rule, responsive to a determination that the media item does not satisfy the first rule, continuing to select a subsequent rule from the plurality of rules until the subsequent rule is satisfied or the plurality of rules are exhausted, and determining that the media item is a target media item if the media item satisfied the first rule or the subsequent rule.
- the video generation module may be stored in the memory, coupled to the media selector module, and executable by the one or more processors to generate the video that includes the target media.
- the video generation module is configured to generate the video by: generating two or more animations, wherein each animation illustrates a transition between two media items of the target media items and arranging the two or more animations into a sequence.
- the system may further include a music module stored on the memory and executable by the one or more processors, the music module configured to add a song to the video and synchronize one or more features of the song with the transitions in the two or more animations.
- the media selector module may be configured to select the first rule from the plurality of rules that include two or more of an important entity rule, a landmark rule, a face frequency rule, a face affinity rule, and a highlights rule.
- the video generation module may be configured to generate a cover image that is target media that appears first in the video and wherein the video generation module identifies the cover image by determining whether the cover image satisfies at least the first rule and the subsequent rule.
- the system may further include a user interface module stored on the memory and executable by the one or more processors, the user interface module configured to provide the user with a user interface that includes options for performing one or more of adding a user-specified media item to the target media, removing one or more media items from the target media, specifying an order of the target media in the video, specifying a title of the video, and selecting the song.
- the user interface module may also be configured to generate a preview of the video from at least one of the first item in the video and subsequent items in the video that are static images.
- the user interface module may also be configured to generate a link to a network location that, when selected by the user, provides the user with access to the video.
- an apparatus to generate a video includes means for grouping media items associated with a user into segments based on a timestamp associated with each media item and a total number of media items, means for selecting target media from the media items for each of the segments based on media attributes associated with the media item, wherein the target media includes two or more media items, means for generating the video that includes the target media for each of the segments by: generating a first animation that illustrates a first transition from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, wherein the first item is illustrated as being attached to the second item in the first animation, means for determining whether the target media includes one or more additional items, responsive to determining that the target media includes one or more additional items, means for generating a second animation that illustrates a second transition from the second item to a first additional item and that further illustrates additional transitions between a remainder of the one or more additional items, and means for adding a song to the video
- the apparatus may further include the timestamp being associated with each media item represents at least one of an upload time associated with uploading each media item, a capture time associated with capturing each media item, a creation time associated with creating each media item, and a last modification made to each media item.
- the apparatus may further include means for selecting the media items from media associated with the user based on the timestamp being indicative of a particular time period.
- Generating the first animation may further include means for illustrating a zoom-out effect on the first item, wherein the zoom-out effect is synchronized to the one or more features of the song.
- the apparatus may further include means for grouping the media item associated with the user into the plurality of segments by grouping media items associated with an event into two or more segments based on an event number of media items associated with the event as compared to the total number of media items.
- the apparatus may further include means for selecting the target media from the media item for each of the plurality of segments based on the media attributes associated with the media item includes selecting based on a share tendency.
- the apparatus may further include a number of target media being a function of the total number of media items as determined prior to the grouping of the media items.
- the system and methods described below advantageously generate videos of media items that a user is expected to enjoy viewing.
- the system and methods increase the chances of creating an interesting video.
- the video may be more captivating to audiences.
- the system and methods advantageously provide coverage of content spanning a long time period and reduce the computational cost of generating the video by selecting target media for each segment independently.
- FIG. 1 illustrates a block diagram of an example system that generates a video.
- FIG. 2 illustrates a block diagram of an example computing device that generates a video.
- FIG. 3A illustrates a graphic representation of a first portion of an animation.
- FIG. 3B illustrates a graphic representation of a second portion of the animation.
- FIG. 3C illustrates a graphic representation of a third portion of the animation.
- FIG. 3D illustrates a graphic representation of a fourth portion of the animation.
- FIG. 4 is a flowchart of an example method to generate a video.
- FIG. 5 is a flowchart of an example method to select target media from media items.
- FIG. 6 is a flowchart of an example method to generate an animation with two or more transitions.
- Some implementations may include a system and method to generate a video.
- Media items associated with a user may be grouped into segments based on a timestamp associated with each media item and a total number of media items. For example, if there are 72 photos available that are evenly distributed over a particular time period, the media items may be grouped into 12 segments where each segment includes six of the media items.
- Target media may be selected from the media items for each of the segments based on media attributes associated with the media item. For example, for each segment, two media items may be selected from the six media items available for that particular segment.
- the target media may be selected based on applying a rule to each media item that is randomly selected from a plurality of rules. For example, the first rule selected may determine whether the media item includes an important entity, such as a birthday cake. If the media item does not satisfy the rule, a subsequent rule may be selected from the plurality of rules. For example, the subsequent rule may determine whether the media item includes several faces including the user's face. If there is no subsequent rule available, the process may proceed to a next media item. Alternatively, the target media may be selected based on a tendency of users to share the images.
- a video may be generated that includes the target media for each of the segments. Generating the video may include generating a first animation that illustrates a first transition from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, where the first item is adjacent to the second item in the first animation.
- the video may include additional transitions with additional items in additional animations.
- a song may be added to the video where one or more features of the song are synchronized with the video. For example, beats of the song may be synchronized with the transitions in the video.
- FIG. 1 illustrates a block diagram of an example system 100 that generates videos.
- the illustrated system 100 includes a server 101 , user devices 115 a , 115 n and a network 105 . Users 125 a - 125 n may be associated with respective user devices 115 a , 115 n .
- the system 100 may include other servers or devices not shown in FIG. 1 .
- the system 100 may include a separate social network server, a music server, etc.
- a letter after a reference number e.g., “ 115 a ,” represents a reference to the element having that particular reference number.
- a reference number in the text without a following letter, e.g., “ 115 ,” represents a general reference to implementations of the element bearing that reference number.
- the entities of the system 100 are communicatively coupled via a network 105 .
- the network 105 may be a conventional type, wired or wireless, and may have numerous different configurations including a star configuration, token ring configuration or other configurations.
- the network 105 may include a local area network (LAN), a wide area network (WAN) (e.g., the Internet), and/or other interconnected data paths across which multiple devices may communicate.
- the network 105 may be a peer-to-peer network.
- the network 105 may also be coupled to or include portions of a telecommunications network for sending data in a variety of different communication protocols.
- the network 105 includes Bluetooth® communication networks or a cellular communications network for sending and receiving data including via short messaging service (SMS), multimedia messaging service (MMS), hypertext transfer protocol (HTTP), direct data connection, wireless application protocol (WAP), email, etc.
- SMS short messaging service
- MMS multimedia messaging service
- HTTP hypertext transfer protocol
- WAP wireless application protocol
- FIG. 1 illustrates one network 105 coupled to the user devices 115 and the server 101 , in practice one or more networks 105 may be coupled to these entities.
- the server 101 may include a processor, a memory and network communication capabilities. In some implementations, the server 101 is a hardware server 101 .
- the server 101 is communicatively coupled to the network 105 via signal line 102 .
- Signal line 102 may be a wired connection, such as Ethernet, coaxial cable, fiber-optic cable, etc., or a wireless connection, such as Wi-Fi, Bluetooth, or other wireless technology.
- the server 101 sends and receives data to and from one or more of the user devices 115 a - 115 n via the network 105 .
- the server 101 may include a video application 103 a and a database 199 .
- the video application 103 may be code and routines configured to generate videos.
- the video application 103 may be implemented using hardware including a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC).
- the video application 103 may be implemented using a combination of hardware and software.
- the database 199 may store videos generated by a video application 103 , such as the video application 103 a stored on the server 101 or the video application 103 b stored on the user device 125 a .
- the database 199 may also store social network data associated with users 125 , songs, etc.
- the user device 115 may be a computing device that includes a memory and a hardware processor, for example a laptop computer, a desktop computer, a tablet computer, a mobile telephone, a wearable device, a head-mounted display, a mobile email device, a portable game player, a portable music player, a reader device, a television with one or more processors embedded therein or coupled thereto, or other electronic device capable of accessing a network 105 .
- a hardware processor for example a laptop computer, a desktop computer, a tablet computer, a mobile telephone, a wearable device, a head-mounted display, a mobile email device, a portable game player, a portable music player, a reader device, a television with one or more processors embedded therein or coupled thereto, or other electronic device capable of accessing a network 105 .
- user device 115 a is coupled to the network 105 via signal line 108 and user device 115 n is coupled to the network 105 via signal line 110 .
- Signal lines 108 and 110 may be a wired connection, such as Ethernet, coaxial cable, fiber-optic cable, etc., or a wireless connection, such as Wi-Fi, Bluetooth, or other wireless technology.
- User devices 115 a , 115 n are accessed by users 125 a , 125 n , respectively.
- the user devices 115 a , 115 n in FIG. 1 are used by way of example. While FIG. 1 illustrates two user devices, 115 a and 115 n , the disclosure applies to a system architecture having one or more user devices 115 .
- the user device 115 can be a mobile device that is included in a wearable device worn by the user 125 .
- the user device 115 is included as part of a clip (e.g., a wristband), part of jewelry, or part of a pair of glasses.
- the user device 115 can be a smart watch.
- the user 125 can view videos from the video application 103 on a display of the device worn by the user 125 .
- the user 125 can view the videos on a display of a smart watch or a smart wristband.
- the video application 103 b may be stored on a user device 115 a .
- the video application 103 may include a thin-client video application 103 b stored on the user device 115 a and a video application 103 a that is stored on the server 101 .
- the video application 103 a may be a web application that generates web pages viewable by the user device 115 a using the video application 103 b .
- the video application 103 may include a mobile application that runs on the user device 115 a and sends information to the video application 103 a stored on the server 101 .
- the user 125 a may capture media using the user device 115 a and transmit the media to the server 101 for the video application 103 a .
- the video application 103 a stored on the server 101 may process the information and send additional information back to the video application 103 b stored on the user device 115 a .
- the video application 103 a may generate a video from the media and transmit the video to the user device 115 a for display.
- the user device 115 a may stream the video directly from the server 101 or store the video on the user device 115 a.
- the video application 103 may be a standalone application stored on the server 101 .
- a user 125 a may access the web pages using a browser or other software on the user device 125 a .
- the video application 103 b stored on the user device 115 a may receive instructions from the video application 103 a stored on the server 101 to display information generated by the video application 103 a .
- the video application 103 may include the same components on the user device 115 a as are included on the server 101 .
- a video may be generated from media items by the server 101 or by the user device 115 .
- a video may be generated from media items by the server 101 or by the user device 115 .
- FIG. 2 illustrates a block diagram of an example computing device 200 that generates videos.
- the computing device 200 may be a server 101 or a user device 115 .
- the computing device 200 may include a processor 235 , a memory 237 , a communication unit 239 , a display 241 , and a storage device 243 .
- a video application 103 may be stored in the memory 237 .
- the components of the computing device 200 may be communicatively coupled by a bus 220 .
- the processor 235 includes an arithmetic logic unit, a microprocessor, a general purpose controller or some other processor array to perform computations and provide instructions to a display device.
- Processor 235 processes data and may include various computing architectures including a complex instruction set computer (CISC) architecture, a reduced instruction set computer (RISC) architecture, or an architecture implementing a combination of instruction sets.
- FIG. 2 includes a single processor 235 , multiple processors 235 may be included.
- Other processors, operating systems, sensors, displays and physical configurations may be part of the computing device 200 .
- the processor 235 is coupled to the bus 220 for communication with the other components via signal line 222 .
- the memory 237 stores instructions that may be executed by the processor 235 and/or data.
- the instructions may include code for performing the techniques described herein.
- the memory 237 may be a dynamic random access memory (DRAM) device, a static RAM, or some other memory device.
- the memory 237 also includes a non-volatile memory, such as a (SRAM) device or flash memory, or similar permanent storage device and media including a hard disk drive, a floppy disk drive, a compact disc read only memory (CD-ROM) device, a DVD-ROM device, a DVD-RAM device, a DVD-RW device, a flash memory device, or some other mass storage device for storing information on a more permanent basis.
- the memory 237 includes code and routines configured to execute the video application 103 , which is described in greater detail below.
- the memory 237 is coupled to the bus 220 for communication with the other components via signal line 224 .
- the communication unit 239 transmits and receives data to and from at least one of the user device 115 and the server 101 depending upon where the video application 103 may be stored.
- the communication unit 239 includes a port for direct physical connection to the network 105 or to another communication channel.
- the communication unit 239 includes a universal serial bus (USB), secure digital (SD), category 5 cable (CAT-5) or similar port for wired communication with the user device 115 or the server 101 , depending on where the video application 103 may be stored.
- the communication unit 239 includes a wireless transceiver for exchanging data with the user device 115 , server 101 , or other communication channels using one or more wireless communication methods, including IEEE 802.11, IEEE 802.16, Bluetooth® or another suitable wireless communication method.
- the communication unit 239 is coupled to the bus 220 for communication with the other components via signal line 226 .
- the communication unit 239 includes a cellular communications transceiver for sending and receiving data over a cellular communications network including via short messaging service (SMS), multimedia messaging service (MMS), hypertext transfer protocol (HTTP), direct data connection, WAP, e-mail or another suitable type of electronic communication.
- SMS short messaging service
- MMS multimedia messaging service
- HTTP hypertext transfer protocol
- the communication unit 239 includes a wired port and a wireless transceiver.
- the communication unit 239 also provides other conventional connections to the network 105 for distribution of files and/or media objects using standard network protocols including, but not limited to, UDP, TCP/IP, HTTP, HTTPS, SMTP, SPDY, QUIC, etc.
- the display 241 may include hardware configured to display graphical data received from the video application 103 .
- the display 241 may render graphics to display a user interface that includes a video.
- the display 241 is coupled to the bus 220 for communication with the other components via signal line 228 .
- Other hardware components that provide information to a user may be included as part of the computing device 200 .
- the computing device 200 may include a speaker for audio interfaces, a vibration or force feedback device, or other types of non-display output devices.
- the display 241 may be optional.
- the computing device 200 may not include all the components.
- the computing device 200 is a wearable device, the computing device 200 may not include storage device 243 .
- the computing device 200 may include other components not listed here, e.g., one or more cameras, sensors, battery, etc.
- the storage device 243 may be a non-transitory computer-readable storage medium that stores data that provides the functionality described herein.
- the storage device 243 may include the database 199 in FIG. 1 .
- the storage device 243 may be a DRAM device, a SRAM device, flash memory or some other memory device.
- the storage device 243 also includes a non-volatile memory or similar permanent storage device and media including a hard disk drive, a floppy disk drive, a CD-ROM device, a DVD-ROM device, a DVD-RAM device, a DVD-RW device, a flash memory device, or some other mass storage device for storing information on a permanent basis.
- the storage device 243 is coupled to the bus 220 for communication with the other components via signal line 230 .
- the video application 103 includes a preselector module 202 , a segmentation module 204 , a media selector module 206 , a video generation module 208 , a music module 210 , a social network module 212 , and a user interface module 214 .
- a preselector module 202 the video application 103 includes a preselector module 202 , a segmentation module 204 , a media selector module 206 , a video generation module 208 , a music module 210 , a social network module 212 , and a user interface module 214 .
- Other modules and/or configurations are possible.
- the preselector module 202 may be configured to receive and transmit data.
- the preselector module 202 may be a set of instructions executable by the processor 235 to filter media items.
- the preselector module 202 may be stored in the memory 237 of the computing device 200 and can be accessible and executable by the processor 235 .
- the preselector module 202 may receive media items.
- the preselector module 202 may receive the media items from a user, such as when the user captures media from a user device 115 , from other users that shared the media items with the user, for example, via a social network operated by the social network module 212 , tagged the user in the media items, etc.
- Exemplary media items may include, but are not limited to, an image, a video, an animated Graphics Interchange Format (GIF) file, a photo burst series of images, a face switching series of images, and a multi-zoom series of images.
- An exemplary photo burst may include an animation of a fast switch between images that create a stop-motion effect.
- An exemplary face switching may be used to show changes in a person's face over a particular period of time. For example, the face switching may use different pictures of a baby to show how the baby has changed over the past year.
- the preselector module 202 may filter the media items to remove duplicates and remove media items that fail to meet preselection criteria.
- the preselection criteria may include, for example, rules that require people or landmarks to be in the images (as opposed to images of documents, bills, receipts, etc.), rules against including screenshots, rules that require a threshold level of image quality, etc.
- the preselector module 202 may determine whether media items are suitable for being used as target media based on cropping the media items. For example, the preselector module 202 may crop portions of images to identify the most salient regions of the image at a particular aspect ratio. The preselector module 202 may crop the images by cutting any combination of the top, bottom, left, or right of the image to highlight the entities or people in the image. In instances where portrait images include people, the preselector module 202 may draw bounding boxes around people in the images and determine whether the portrait images may be cropped to be converted into landscape images while keeping all the people in the image. In some implementations, the preselector module 202 may discard portrait images where the crop cuts out some of the people in the image.
- the segmentation module 204 may be configured to group media items associated with a user into segments.
- the segmentation module 204 may be a set of instructions executable by the processor 235 to segment media items.
- the segmentation module 204 may be stored in the memory 237 of the computing device 200 and can be accessible and executable by the processor 235 .
- the segmentation module 204 receives filtered media items from the preselector module 202 . In some implementations where the preselector module 202 is not included in the video application 103 , the segmentation module 204 receives the media items. For example, the segmentation module 204 may receive the media items as described above with reference to the preselector module 202 .
- the segmentation module 204 may group the media items associated with a user into segments based on a timestamp associated with each media item and a total number of media items. The segmentation module 204 may use the timestamp for each media item and the total number of media items to ensure that there is sufficient time diversity of media items. For example, if the timestamps are evenly dispersed throughout a certain time period, such as a year, the segmentation module 204 may divide the media items into 12 segments where each event is associated with a particular segment. Alternatively, if timestamps for the media items indicate that there are an abundance of media items centered around one event, the segmentation module 204 may divide the media items associated with one event into multiple segments.
- the segmentation module 204 may determine whether to group media items associated with an event into two or more segments based on an event number of media items associated with the event as compared to the total number of media items. For example, the segmentation module 204 may determine that the media items include a “Trip to Paris” event and because 200 of the 300 media items are associated with the trip to Paris, the segmentation module 204 may group some media items associated with the trip to Paris into a first segment that includes the first two days in Paris based on the timestamps and group the other media items associated with the event into a second segment that includes the last two days in Paris based on the timestamps.
- Exemplary timestamps may be associated with, but are not limited to, an upload time associated with uploading each media item, a capture time associated with capturing each media item, a creation time associated with creating each media item, and a last modification made to each media item.
- the segmentation module 204 may select media items from media associated with a user based on the timestamp associated with each media item being indicative of a particular time period.
- Exemplary time periods may include, but are not limited to, a calendar year, a birthday year, a time period of an important event (e.g., a soccer tournament), a time period of a chapter of a person's life (a wedding and the associated celebration), etc.
- the segmentation module 204 may determine a number of target media based on a total number of media items. The segmentation module 204 may determine the number of target media before grouping the media items into segments. For example, the segmentation module 204 may determine that the number of target media is half the total number of media items.
- the media selector module 206 may be configured to select target media from media items.
- the media selector module 206 may be a set of instructions executable by the processor 235 to select target media.
- the media selector module 206 may be stored in the memory 237 of the computing device 200 and can be accessible and executable by the processor 235 .
- the media selector module 206 selects target media for each segment independently, which may advantageously ensure time-diversity and reduce the time cost of selecting target media as compared to selecting target media for each segment sequentially. In some implementations, the media selector module 206 selects one to three images for each segment.
- the media selector module 206 may select the target media from the media items based on media attributes associated with the media item.
- the media selector module 206 may use a training model to determine how to score the media items based on a tendency of users to share the images. For example, the media selector module 206 may compare media that were shared with other users to media that were not shared to identify media attribute differences. Based on the differences, the media selector module 206 may create a model for how to associate a share tendency score to media items.
- the media selector module 206 may apply different rules to determine which media items should be selected as target media based on the media attributes.
- the rules may include different types of image processing steps. For example, in instances where the user consents to the use of such data, the media selector module 206 may perform image recognition to identify people in the media items. In some implementations, the media selector module on the rule. The media selector module 206 may determine whether the score exceeds a threshold, select the target media based on top scores, etc.
- the rules may include an important entity rule that determines whether one or more important entities are present in the media item (e.g., based on experiences particular to the user, such as an image of a wooden box that the user indicates is important or experiences particular to users in general, such as an image of a birthday cake or a wedding picture).
- the rules may further include a landmark rule that determines whether one or more important landmarks are present in the media item (e.g., based on a popularity score associated with the entities, where the Eiffel Tower may be determined to be an important landmark).
- the rules may further include an important person rule that determines whether one or more important people are in the media item (e.g., based on a face frequency score).
- the rules may further include an important event rule that determines whether the media item corresponds to an important event based on whether a predetermined number of photos were taken during a particular time period (e.g., at least five photos were taken in an hour, based on correlating the media with an important date associated with the user, such as a birthday, anniversary, festival, trip, holiday, etc.).
- the rules may further include a high-visual quality rule that determines whether the media item has a visual quality that is within or exceeds a quality threshold (e.g., based on a level of blurriness, dynamic range, etc.).
- the rules may further include a highlight rule that determines whether the media item represents a highlight of an event.
- the rules may further include a socially engaged rule that determines whether one or more people in a media item are socially engaged (e.g., whether multiple people are looking at each other or looking directly at the camera versus people looking at unidentified objects, people that have vacant stares, etc.).
- the media selector module 206 performs facial recognition to identify the people in the media item and determine posture, gaze direction, and emotion of the people (e.g., based on a facial pose that indicates smiling, surprised, crying, etc.).
- the time-diversity rule that determines that target media for a segment include time diversity (e.g., the target media are not selected from a single event), a shared media rule that determines whether the media is shared from a media album, a burst for mix rule that determines a quality of a burst for different types of media, a burst for animation rule that determines a quality of a burst for animation, a burst for face switching that determines a quality of a burst that includes different faces as part of a burst, a face affinity rule that determines an affinity between the user associated with the media item and a person in the media item (e.g., in the instance where the user and the person consent to the use of such data, the affinity may be based on communications between the user and the person, such as a number of emails, texts, chats, etc.
- time diversity e.g., the target media are not selected from a single event
- a shared media rule that determines whether the media is shared
- the media selector module 206 may perform facial recognition to identify the people in the media).
- the media selector module 206 randomly selects one of the rules from the plurality of rules according to a configured probability. For example, the media selector module 206 may use a process that employs a random or pseudorandom number generation technique to determine a rule to select from a group of rules. The media selector module 206 may determine whether the media item satisfies the rule. If the media item satisfies the rule, the media item may be selected as target media. If the media item does not satisfy the rule, the media selector module 206 may randomly select a subsequent rule. The media selector module 206 may continue this process until a threshold number of rules are selected, there are no longer subsequent rules available, or the media item satisfies one of the subsequent rules.
- the random selection process may be advantageous over a process where multiple rules are applied all at once because some of the rules may contradict each other.
- the important event rule may result in selecting multiple media items from the same event.
- the time-diversity rule may prohibit selecting multiple media items from the same event.
- the media selector module 206 may rank the rules based on importance and select rules to apply to a media item based on the importance of the rules. For example, the media selector module 206 may determine, from decreasing importance, that the order of the rules is the important entity rule, the important person rule, the important event rule, the social engagement rule, the time diversity rule, and the high-visual quality rule. In some implementations, the media selector module 206 may assign a score to each rule, compute a total score for each media item that is a weighted sum of the scores, and select the media items that are associated with the best scores (e.g., the highest scores if a higher score is indicative of a media item that satisfies one or more rules).
- the media selector module 206 may assign a score to each rule, compute a total score for each media item that is a weighted sum of the scores, and select the media items that are associated with the best scores (e.g., the highest scores if a higher score is indicative of a media item that sati
- the media selector module 206 selects media items as target media that somewhat satisfy many rules over media items that greatly satisfy one rule, which may result in mediocre results for the target media.
- the media selector module 206 selects a cover image and an ending image for the video. For example, the media selector module 206 may identify media items that satisfy multiple rules mentioned above. In some implementations, the media selector module 206 may apply different criteria when selecting the cover image and the ending image. In some implementations, the criteria when selecting the cover image and the ending image may be stricter than for the rest of the target media. For example, the cover image and the ending image may have to satisfy a first rule and subsequent rules, such as having an important entity or landmark, an important person or a group photo where the user associated with the media items is in the image, people smiling in the image, and the visual quality may include a different quality threshold value.
- a first rule and subsequent rules such as having an important entity or landmark, an important person or a group photo where the user associated with the media items is in the image, people smiling in the image, and the visual quality may include a different quality threshold value.
- the quality threshold value when selecting the cover image and the ending image may be stricter than for the rest of the target media.
- the media selector module 206 selects landscape images or images that may be cropped to form a landscape image for the cover image and the ending image.
- the video generation module 208 may be configured to generate a video from the target media.
- the video generation module 208 may be a set of instructions executable by the processor 235 to generate the video.
- the video generation module 208 may be stored in the memory 237 of the computing device 200 and can be accessible and executable by the processor 235 .
- the video generation module 208 generates portions of a video from target media for each segment and combines the portions in sequence to form the video. In some implementations, the video generation module generates a video that is a sequence of the target media.
- the video generation module 208 may assemble the target media into a video by creating animations of the target media that are arranged into a sequence.
- the video generation module 208 may generate one or more animations that include transitions between different target media items. For example, the video generation module 208 may generate an animation that includes a close-up of a first image, the animation then zooms out of the close-up of the first image to show additional portions of the first image. The animation then zooms out of the first image a second time and then the animation includes a transition from the first image to a second image, where the second image is adjacent to the first image in the animation.
- the animations are discussed in greater detail below with reference to FIGS.
- transitions are possible, such as changing a perspective of the target media by zooming-in to emphasize important regions of the target media or an illustration of the target media moving in a particular direction (left-to-right, top-to-bottom, etc.).
- the video generation module 208 generates a video that includes a first animation that illustrates transitions from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, where the first item is adjacent to the second item.
- the video generation module 208 may determine whether the target media includes one or more additional items and, if there are additional items, the video generation module 208 may generate a subsequent animation that illustrates a subsequent transition from the second item to an additional item. The subsequent transition may further illustrate additional transitions between a remainder of the one or more additional items.
- the video generation module 208 may add a title to the video, such as a title that describes the particular time period represented by the target media. For example, the video may be titled “Year in Review 2014 ,” “Sara Is One,” “Daniel's Graduation,” etc.
- the video generation module 208 may determine a title for the video from a title of an album, in the instance where the user consents to the use of the data, determining a title based on calendar entries, etc.
- the music module 210 may be configured to add a song to the video.
- the music module 210 may be a set of instructions executable by the processor 235 to add the song to the video.
- the music module 210 may be stored in the memory 237 of the computing device 200 and can be accessible and executable by the processor 235 .
- the music module 210 may add a song to the video, where one or more features of the song are synchronized with the video.
- the features of the song may include beats that are matched to the transitions. For example, the transition from a first media item to a second media item may be synchronized with a beat of the video. In another example, a zoom-out effect on a first item in the target media may be synchronized with a beat of the video.
- different types of beats may be matched with the video. For example, transitions between target media items may be matched with a major beat in the song and transitions between different perspectives of a target media item (e.g., zooming-out) may be matched to a minor beat in the song.
- the features may also include different vocal sections, instrumental sections, quiet sections, loud sections, male voice sections, female voice sections, etc.
- the music module 210 may use a database of songs (e.g., the database 199 in FIG. 1 , the storage device 243 of FIG. 2 , etc.) or retrieve songs from a third-party server that are identified to have features that synchronize with transitions in a video. For example, the music module 210 may identify songs for the database that have consistent beats at predetermined intervals, songs that are instrumental, etc. In some implementations, the music module 210 may select different songs based on a theme of the video. For example, the music module 210 may determine that the theme of a video is “Mara's Trip to Paris” based a title associated with the video, landmark recognition of the target media, etc. and match the Parisian theme with a French-inspired song. In some implementations, the music module 210 may synchronize any song selected by a user to the video.
- a database of songs e.g., the database 199 in FIG. 1 , the storage device 243 of FIG. 2 , etc.
- the social network module 212 may be configured to manage a social network.
- the social network module 212 may be a set of instructions executable by the processor 235 to manage the social network.
- the social network module 212 may be stored in the memory 237 of the computing device 200 and may be accessible and executable by the processor 235 .
- the social network module 212 may be excluded from the video application 103 .
- a social network can be a type of social structure where the users may be connected by a common feature.
- the common feature may include relationships/connections, e.g., friendship, family, work, an interest, etc.
- the common features may be provided by one or more social networking systems including explicitly defined relationships and relationships implied by social connections with other online users, where the relationships form a social graph that may be stored in the storage device 243 .
- the social graph can reflect a mapping of these users and how they can be related.
- the social network may encompass, for example, social network applications, mail applications, chat applications, and search applications.
- the social network module 212 may post the video to the user's social feed.
- the social network module 212 may identify other people in the video that are members of the social network. For example, the social network module 212 may receive the identity of the members from the media selector module 206 . The social network module 212 may associate the identity of the members with the members' profiles. The social network module 212 may then tag the members in the video and notify the members that they were tagged in the video.
- the user interface module 214 may be configured to provide information to a user.
- the user interface module 214 can be a set of instructions executable by the processor 235 to provide the functionality described below for providing information to a user.
- the user interface module 214 can be stored in the memory 237 of the computing device 200 and can be accessible and executable by the processor 235 .
- the user interface module 214 may generate a user interface that allows a user to edit the video.
- the user interface may include options that allow a user to edit the video.
- the user may edit the video by adding a user-specified media item to the target media and removing one or more media items from the target media.
- the user may edit the video by specifying an order of the target media in the video, editing the target media (cropping a photo, applying an enhancement to a burst, etc.), and reordering the target media in the video.
- the user may edit the video by changing the song, selecting the song, and modifying the volume of the song.
- the user may edit the video by changing the title.
- the media selector module 206 may select replacement target media (if applicable)
- the video generation module 208 may update the video
- the music module 210 may update synchronization of the song with the updated video.
- the user interface module 214 may generate a link to a network location for a user or other users (e.g., users of the social network) to access the video. For example, the user interface module 214 may generate a uniform resource locator (URL) that the user may share with other users, a landing page that any user may visit to obtain their own personalized video, etc. The user interface module 214 may further generate one or more options to share the network location, such as an email notification, a mobile callout, a share by link, an automatic notification to other users that are directly connected to the user in the social network, a post to be displayed on the user's social network, etc. In some implementations, the user interface module 214 may generate an option to download the video.
- a uniform resource locator URL
- the user interface module 214 may further generate one or more options to share the network location, such as an email notification, a mobile callout, a share by link, an automatic notification to other users that are directly connected to the user in the social network, a post to be displayed on the user'
- the user interface module 214 may generate a preview of the video that includes highlights of the video.
- the preview may include the cover image, the end image, and 12 target media items that are static images and that satisfy the rules.
- the target media for the preview may satisfy a more strict set of criteria than the target media for the video.
- the target media for the preview may be selected from the best scored target media from the video.
- the preview may be automatically added to a user's media library (on the cloud, on their phone, etc.).
- FIGS. 3A-3D illustrates graphic representations of an animation generated by the video generation module 208 .
- FIG. 3A illustrates a graphic representation 300 of a first portion of an animation.
- the graphic representation 300 is a close-up of a user 305 and a friend 307 of the user.
- the graphic representation 300 is a cover image
- the graphic representation 300 satisfies multiple rules as determined by the media selector module 206 .
- the media selector module 206 determines that the cover image includes the user, the cover image includes a person with a high face affinity score based on frequent communications between the person and the user (with consent from the user), the cover image includes an important landmark 309 , and the quality of the image meets a threshold quality value.
- FIG. 3B illustrates a graphic representation 325 of a second portion of the animation.
- the animation includes a first transition from the close-up of the user 305 and the friend 307 as illustrated in FIG. 3A to a zoomed-out version of the cover image as illustrated in FIG. 3B .
- the important landmark 309 is identifiable as the Eiffel Tower.
- FIG. 3C illustrates a graphic representation 350 of a third portion of the animation.
- the animation includes a second transition from the close-up of the user 305 , the friend 307 , and the important landmark 309 as illustrated in FIG. 3B to a zoomed-out version of the cover image as illustrated in FIG. 3C .
- the graphic representation 350 also includes a title for the cover image that indicates that the subject matter of the video is the year in photos.
- FIG. 3D illustrates a graphic representation 375 of a fourth portion of the animation.
- the animation includes a third transition from the cover image 380 to a second item 385 .
- the third transition is illustrated as the cover image 380 being adjacent to the second item 385 with movement from the right to the left as the cover image 380 moves from an onscreen location to an offscreen location.
- the second item is illustrated as including another friend 387 of the user in front of another important landmark 389 , which the media selector module 206 identified as the Zakim Bridge in Boston.
- the first two transitions may be synchronized to a different feature of the song than the third transition. This may be advantageous, for example, for the user to distinguish between different types of transitions, such a zoom-out transition (e.g., the first two transitions) and a horizontal movement (e.g., the third transition).
- a zoom-out transition e.g., the first two transitions
- a horizontal movement e.g., the third transition
- FIG. 4 is a flowchart of an example method 400 to generate a video.
- the method 400 may be implemented by server 101 , a user device 115 or a combination of the server 101 and the user device 115 , using the video application 103 illustrated in FIG. 1 .
- the video application 103 may include the segmentation module 204 , the media selector module 206 , the video generation module 208 , and the music module 210 illustrated in FIG. 2 .
- media items associated with a user are grouped into segments based on a timestamp associated with each media item and a total number of media items.
- the segmentation module 204 may group the media items associated with the user into segments based on the timestamp data and the total number of media items.
- the segmentation module 204 may group the media items into segments where a single event represented in the media items are part of one of the segments. For example, images of a user's birthday party may be associated with a single segment.
- a single event represented in the media items may span two or more segments. For example, a user's trip to Paris may be associated with two segments.
- target media is selected from the media items for each of the segments based on media attributes associated with the media attributes associated with the media item, the target media including two or more media items.
- the media selector module 206 may select the target media.
- the target media may be selected based on rules as described with reference to FIG. 5 .
- a video is generated that includes the target media for each of the segments.
- the video generation module 208 may generate the video.
- the video includes animations as described in greater detail below with reference to FIG. 6 .
- a song is added to the video, where one or more features of the song are synchronized with the video.
- the music module 210 may add the song to the video.
- the one or more features may include beats that are synchronized to transitions in the video.
- blocks 402 to 408 are illustrated in a particular order, other orders are possible with intervening steps. In some implementations, some blocks may be added, skipped, or combined.
- FIG. 5 is a flowchart of an example method 500 to select target media from media items.
- the method 500 may be implemented by server 101 , a user device 115 or a combination of the server 101 and the user device 115 , using the video application 103 illustrated in FIG. 1 .
- the video application 103 may include the media selector module 206 illustrated in FIG. 2 .
- a rule may be randomly selected from a plurality of rules.
- the media selector module 206 may randomly select the rule from the plurality of rules.
- the rule may be randomly selected according to a probability associated with the rule (e.g., a weight associated with each of the rules).
- the media selector module 206 determines whether the media item satisfies the rule.
- the rule may be satisfied by applying a score to the media item based on media attributes being analyzed based on the rule and determining whether the score exceeds a threshold value. Responsive to a determination that the media item does not satisfy the rule, the method 500 proceeds to block 508 where it is determined whether there is a subsequent rule from the plurality of rules. For example, the media selector module 206 determines where there is a subsequent rule. Responsive to a determination that the subsequent rule is present, the method 500 proceeds to block 502 . Responsive to a determination that there is not a subsequent rule from the plurality of rules, the method 500 proceeds to block 510 .
- the method 500 proceeds to block 510 .
- the media selector module 206 determines whether there are additional media items. Responsive to determining that there are additional media items, the method 500 proceeds to block 506 . Responsive to determining that there are not additional media items, the method 500 proceeds to block 406 of FIG. 4 .
- blocks 502 to 510 are illustrated in a particular order, other orders are possible with intervening steps. In addition, blocks may be added, skipped, or combined.
- FIG. 6 is a flowchart of an example method 600 to generate an animation with two or more transitions.
- the method 600 may be implemented using the video application 103 illustrated in FIG. 1 .
- the video application 103 may include the video generation module 208 illustrated in FIG. 2 .
- a first animation is generated that illustrates a first transition from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, where the first item is adjacent to the second item in the first animation.
- the video generation module 208 generates the first animation.
- the video generation module 208 determines whether the target media includes the one or more additional items. Responsive to a determination that the target media does not include the one or more additional items, the method 600 proceeds to block 408 of FIG. 4 . Responsive to the target media including the one or more additional items, the method 600 proceeds to block 606 .
- a subsequent animation is generated that illustrates a subsequent transition from the second item to an additional item.
- the video generation module 208 generates the subsequent animation. The method proceeds to block 604 and continues in a loop until there are no longer additional items in the target media.
- the implementations of the specification can also relate to a processor for performing one or more steps of the methods described above.
- the processor may be a special-purpose processor selectively activated or reconfigured by a computer program stored in the computer.
- a computer program may be stored in a non-transitory computer-readable storage medium, including, but not limited to, any type of disk including floppy disks, optical disks, ROMs, CD-ROMs, magnetic disks, RAMs, EPROMs, EEPROMs, magnetic or optical cards, flash memories including USB keys with non-volatile memory, or any type of media suitable for storing electronic instructions, each coupled to a computer system bus.
- the specification can take the form of some entirely hardware implementations, some entirely software implementations or some implementations containing both hardware and software elements.
- the specification is implemented in software, which includes, but is not limited to, firmware, resident software, microcode, etc.
- a computer-usable or computer-readable medium can be any apparatus that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device.
- a data processing system suitable for storing or executing program code will include at least one processor coupled directly or indirectly to memory elements through a system bus.
- the memory elements can include local memory employed during actual execution of the program code, bulk storage, and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.
- the systems provide users with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location), or control whether and/or how to receive content from the server that may be more relevant to the user.
- user information e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location
- certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be treated on that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.
- the user may have control over how information is collected about the user and used by the server.
Abstract
Description
- People enjoy looking at media items that they have previously posted on websites, such as social networking websites or video websites. Manual curation of media items into a video or compilation may not be scalable with large numbers of media. It may also be difficult to automatically select items from the media to use for a compilation. For example, some events may be overrepresented or underrepresented. In addition, depending on the number of media items associated with the user, it may be computationally expensive to process all the media items according to different media criteria.
- Implementations generally relate to media items and videos. In some implementations, a method to generate a video includes grouping media items associated with a user into segments based on a timestamp associated with each media item and a total number of media items. The method may further include selecting target media from the media items for each of the segments based on media attributes associated with the media item, wherein the target media includes two or more media items. The method may further include generating the video that includes the target media for each of the segments by: generating a first animation that illustrates a first transition from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, wherein the first item is illustrated as being attached to the second item in the first animation, determining whether the target media includes one or more additional items, and responsive to determining that the target media includes one or more additional items, generating a second animation that illustrates a second transition from the second item to a first additional item and that further illustrates additional transitions between a remainder of the one or more additional items. The method may further include adding a song to the video, wherein one or more features of the song are synchronized with the first transition and the second transition.
- In some implementations, the method may further include the timestamp being associated with each media item represents at least one of an upload time associated with uploading each media item, a capture time associated with capturing each media item, a creation time associated with creating each media item, and a last modification made to each media item. The method may further include selecting the media items from media associated with the user based on the timestamp being indicative of a particular time period. Generating the first animation may further include illustrating a zoom-out effect on the first item, wherein the zoom-out effect is synchronized to the one or more features of the song. The method may further include grouping the media item associated with the user into the plurality of segments by grouping media items associated with an event into two or more segments based on an event number of media items associated with the event as compared to the total number of media items. The method may further include selecting the target media from the media item for each of the plurality of segments based on the media attributes associated with the media item includes selecting based on a share tendency. The method may further include a number of target media being a function of the total number of media items as determined prior to the grouping of the media items.
- In some implementations, a system to generate a video includes one or more processors coupled to a memory, a media selector module, and a video generation module. The media selector module may be stored in the memory and executable by the one or more processors to select target media from media items by, for each of the media items: randomly selecting a first rule from a plurality of rules, determining whether the media item satisfies the first rule, responsive to a determination that the media item does not satisfy the first rule, continuing to select a subsequent rule from the plurality of rules until the subsequent rule is satisfied or the plurality of rules are exhausted, and determining that the media item is a target media item if the media item satisfied the first rule or the subsequent rule. The video generation module may be stored in the memory, coupled to the media selector module, and executable by the one or more processors to generate the video that includes the target media.
- In some implementations, the video generation module is configured to generate the video by: generating two or more animations, wherein each animation illustrates a transition between two media items of the target media items and arranging the two or more animations into a sequence. The system may further include a music module stored on the memory and executable by the one or more processors, the music module configured to add a song to the video and synchronize one or more features of the song with the transitions in the two or more animations. The media selector module may be configured to select the first rule from the plurality of rules that include two or more of an important entity rule, a landmark rule, a face frequency rule, a face affinity rule, and a highlights rule. The video generation module may be configured to generate a cover image that is target media that appears first in the video and wherein the video generation module identifies the cover image by determining whether the cover image satisfies at least the first rule and the subsequent rule.
- In some implementations, the system may further include a user interface module stored on the memory and executable by the one or more processors, the user interface module configured to provide the user with a user interface that includes options for performing one or more of adding a user-specified media item to the target media, removing one or more media items from the target media, specifying an order of the target media in the video, specifying a title of the video, and selecting the song. The user interface module may also be configured to generate a preview of the video from at least one of the first item in the video and subsequent items in the video that are static images. The user interface module may also be configured to generate a link to a network location that, when selected by the user, provides the user with access to the video.
- In some implementations, an apparatus to generate a video includes means for grouping media items associated with a user into segments based on a timestamp associated with each media item and a total number of media items, means for selecting target media from the media items for each of the segments based on media attributes associated with the media item, wherein the target media includes two or more media items, means for generating the video that includes the target media for each of the segments by: generating a first animation that illustrates a first transition from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, wherein the first item is illustrated as being attached to the second item in the first animation, means for determining whether the target media includes one or more additional items, responsive to determining that the target media includes one or more additional items, means for generating a second animation that illustrates a second transition from the second item to a first additional item and that further illustrates additional transitions between a remainder of the one or more additional items, and means for adding a song to the video, wherein one or more features of the song are synchronized with the first transition and the second transition.
- In some implementations, the apparatus may further include the timestamp being associated with each media item represents at least one of an upload time associated with uploading each media item, a capture time associated with capturing each media item, a creation time associated with creating each media item, and a last modification made to each media item. The apparatus may further include means for selecting the media items from media associated with the user based on the timestamp being indicative of a particular time period. Generating the first animation may further include means for illustrating a zoom-out effect on the first item, wherein the zoom-out effect is synchronized to the one or more features of the song. The apparatus may further include means for grouping the media item associated with the user into the plurality of segments by grouping media items associated with an event into two or more segments based on an event number of media items associated with the event as compared to the total number of media items. The apparatus may further include means for selecting the target media from the media item for each of the plurality of segments based on the media attributes associated with the media item includes selecting based on a share tendency. The apparatus may further include a number of target media being a function of the total number of media items as determined prior to the grouping of the media items.
- Other aspects may include corresponding methods, systems, apparatus, and computer program products.
- The system and methods described below advantageously generate videos of media items that a user is expected to enjoy viewing. In addition, by randomly selecting a rule to apply to media items instead of applying multiple rules at once, the system and methods increase the chances of creating an interesting video. By synchronizing the video with features of a song, such as beats of a song, the video may be more captivating to audiences. Lastly, the system and methods advantageously provide coverage of content spanning a long time period and reduce the computational cost of generating the video by selecting target media for each segment independently.
- The disclosure is illustrated by way of example, and not by way of limitation in the figures of the accompanying drawings in which like reference numerals are used to refer to similar elements.
-
FIG. 1 illustrates a block diagram of an example system that generates a video. -
FIG. 2 illustrates a block diagram of an example computing device that generates a video. -
FIG. 3A illustrates a graphic representation of a first portion of an animation. -
FIG. 3B illustrates a graphic representation of a second portion of the animation. -
FIG. 3C illustrates a graphic representation of a third portion of the animation. -
FIG. 3D illustrates a graphic representation of a fourth portion of the animation. -
FIG. 4 is a flowchart of an example method to generate a video. -
FIG. 5 is a flowchart of an example method to select target media from media items. -
FIG. 6 is a flowchart of an example method to generate an animation with two or more transitions. - Some implementations may include a system and method to generate a video. Media items associated with a user may be grouped into segments based on a timestamp associated with each media item and a total number of media items. For example, if there are 72 photos available that are evenly distributed over a particular time period, the media items may be grouped into 12 segments where each segment includes six of the media items. Target media may be selected from the media items for each of the segments based on media attributes associated with the media item. For example, for each segment, two media items may be selected from the six media items available for that particular segment.
- In some implementations, the target media may be selected based on applying a rule to each media item that is randomly selected from a plurality of rules. For example, the first rule selected may determine whether the media item includes an important entity, such as a birthday cake. If the media item does not satisfy the rule, a subsequent rule may be selected from the plurality of rules. For example, the subsequent rule may determine whether the media item includes several faces including the user's face. If there is no subsequent rule available, the process may proceed to a next media item. Alternatively, the target media may be selected based on a tendency of users to share the images.
- A video may be generated that includes the target media for each of the segments. Generating the video may include generating a first animation that illustrates a first transition from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, where the first item is adjacent to the second item in the first animation. In some implementations, the video may include additional transitions with additional items in additional animations.
- In some implementations, a song may be added to the video where one or more features of the song are synchronized with the video. For example, beats of the song may be synchronized with the transitions in the video.
-
FIG. 1 illustrates a block diagram of anexample system 100 that generates videos. The illustratedsystem 100 includes aserver 101,user devices 115 a, 115 n and anetwork 105. Users 125 a-125 n may be associated withrespective user devices 115 a, 115 n. In some implementations, thesystem 100 may include other servers or devices not shown inFIG. 1 . For example, thesystem 100 may include a separate social network server, a music server, etc. InFIG. 1 and the remaining figures, a letter after a reference number, e.g., “115 a,” represents a reference to the element having that particular reference number. A reference number in the text without a following letter, e.g., “115,” represents a general reference to implementations of the element bearing that reference number. - In the illustrated implementation, the entities of the
system 100 are communicatively coupled via anetwork 105. Thenetwork 105 may be a conventional type, wired or wireless, and may have numerous different configurations including a star configuration, token ring configuration or other configurations. Furthermore, thenetwork 105 may include a local area network (LAN), a wide area network (WAN) (e.g., the Internet), and/or other interconnected data paths across which multiple devices may communicate. In some implementations, thenetwork 105 may be a peer-to-peer network. Thenetwork 105 may also be coupled to or include portions of a telecommunications network for sending data in a variety of different communication protocols. In some implementations, thenetwork 105 includes Bluetooth® communication networks or a cellular communications network for sending and receiving data including via short messaging service (SMS), multimedia messaging service (MMS), hypertext transfer protocol (HTTP), direct data connection, wireless application protocol (WAP), email, etc. AlthoughFIG. 1 illustrates onenetwork 105 coupled to the user devices 115 and theserver 101, in practice one ormore networks 105 may be coupled to these entities. - The
server 101 may include a processor, a memory and network communication capabilities. In some implementations, theserver 101 is ahardware server 101. Theserver 101 is communicatively coupled to thenetwork 105 via signal line 102. Signal line 102 may be a wired connection, such as Ethernet, coaxial cable, fiber-optic cable, etc., or a wireless connection, such as Wi-Fi, Bluetooth, or other wireless technology. - In some implementations, the
server 101 sends and receives data to and from one or more of the user devices 115 a-115 n via thenetwork 105. Theserver 101 may include avideo application 103 a and adatabase 199. - The
video application 103 may be code and routines configured to generate videos. In some implementations, thevideo application 103 may be implemented using hardware including a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC). In some implementations, thevideo application 103 may be implemented using a combination of hardware and software. - The
database 199 may store videos generated by avideo application 103, such as thevideo application 103 a stored on theserver 101 or thevideo application 103 b stored on theuser device 125 a. Thedatabase 199 may also store social network data associated with users 125, songs, etc. - The user device 115 may be a computing device that includes a memory and a hardware processor, for example a laptop computer, a desktop computer, a tablet computer, a mobile telephone, a wearable device, a head-mounted display, a mobile email device, a portable game player, a portable music player, a reader device, a television with one or more processors embedded therein or coupled thereto, or other electronic device capable of accessing a
network 105. - In the illustrated implementation, user device 115 a is coupled to the
network 105 viasignal line 108 anduser device 115 n is coupled to thenetwork 105 viasignal line 110.Signal lines User devices 115 a, 115 n are accessed byusers user devices 115 a, 115 n inFIG. 1 are used by way of example. WhileFIG. 1 illustrates two user devices, 115 a and 115 n, the disclosure applies to a system architecture having one or more user devices 115. - In some implementations, the user device 115 can be a mobile device that is included in a wearable device worn by the user 125. For example, the user device 115 is included as part of a clip (e.g., a wristband), part of jewelry, or part of a pair of glasses. In another example, the user device 115 can be a smart watch. The user 125 can view videos from the
video application 103 on a display of the device worn by the user 125. For example, the user 125 can view the videos on a display of a smart watch or a smart wristband. - In some implementations, the
video application 103 b may be stored on a user device 115 a. Thevideo application 103 may include a thin-client video application 103 b stored on the user device 115 a and avideo application 103 a that is stored on theserver 101. For example, thevideo application 103 a may be a web application that generates web pages viewable by the user device 115 a using thevideo application 103 b. In various implementations, thevideo application 103 may include a mobile application that runs on the user device 115 a and sends information to thevideo application 103 a stored on theserver 101. For example, theuser 125 a may capture media using the user device 115 a and transmit the media to theserver 101 for thevideo application 103 a. Thevideo application 103 a stored on theserver 101 may process the information and send additional information back to thevideo application 103 b stored on the user device 115 a. For example, thevideo application 103 a may generate a video from the media and transmit the video to the user device 115 a for display. The user device 115 a may stream the video directly from theserver 101 or store the video on the user device 115 a. - In some implementations, the
video application 103 may be a standalone application stored on theserver 101. Auser 125 a may access the web pages using a browser or other software on theuser device 125 a. In this implementation, thevideo application 103 b stored on the user device 115 a may receive instructions from thevideo application 103 a stored on theserver 101 to display information generated by thevideo application 103 a. In some implementations, thevideo application 103 may include the same components on the user device 115 a as are included on theserver 101. In these implementations, a video may be generated from media items by theserver 101 or by the user device 115. In these implementations, a video may be generated from media items by theserver 101 or by the user device 115. -
FIG. 2 illustrates a block diagram of anexample computing device 200 that generates videos. Thecomputing device 200 may be aserver 101 or a user device 115. Thecomputing device 200 may include aprocessor 235, amemory 237, acommunication unit 239, adisplay 241, and astorage device 243. Avideo application 103 may be stored in thememory 237. The components of thecomputing device 200 may be communicatively coupled by abus 220. - The
processor 235 includes an arithmetic logic unit, a microprocessor, a general purpose controller or some other processor array to perform computations and provide instructions to a display device.Processor 235 processes data and may include various computing architectures including a complex instruction set computer (CISC) architecture, a reduced instruction set computer (RISC) architecture, or an architecture implementing a combination of instruction sets. AlthoughFIG. 2 includes asingle processor 235,multiple processors 235 may be included. Other processors, operating systems, sensors, displays and physical configurations may be part of thecomputing device 200. Theprocessor 235 is coupled to thebus 220 for communication with the other components viasignal line 222. - The
memory 237 stores instructions that may be executed by theprocessor 235 and/or data. The instructions may include code for performing the techniques described herein. Thememory 237 may be a dynamic random access memory (DRAM) device, a static RAM, or some other memory device. In some implementations, thememory 237 also includes a non-volatile memory, such as a (SRAM) device or flash memory, or similar permanent storage device and media including a hard disk drive, a floppy disk drive, a compact disc read only memory (CD-ROM) device, a DVD-ROM device, a DVD-RAM device, a DVD-RW device, a flash memory device, or some other mass storage device for storing information on a more permanent basis. Thememory 237 includes code and routines configured to execute thevideo application 103, which is described in greater detail below. Thememory 237 is coupled to thebus 220 for communication with the other components viasignal line 224. - The
communication unit 239 transmits and receives data to and from at least one of the user device 115 and theserver 101 depending upon where thevideo application 103 may be stored. In some implementations, thecommunication unit 239 includes a port for direct physical connection to thenetwork 105 or to another communication channel. For example, thecommunication unit 239 includes a universal serial bus (USB), secure digital (SD), category 5 cable (CAT-5) or similar port for wired communication with the user device 115 or theserver 101, depending on where thevideo application 103 may be stored. In some implementations, thecommunication unit 239 includes a wireless transceiver for exchanging data with the user device 115,server 101, or other communication channels using one or more wireless communication methods, including IEEE 802.11, IEEE 802.16, Bluetooth® or another suitable wireless communication method. Thecommunication unit 239 is coupled to thebus 220 for communication with the other components viasignal line 226. - In some implementations, the
communication unit 239 includes a cellular communications transceiver for sending and receiving data over a cellular communications network including via short messaging service (SMS), multimedia messaging service (MMS), hypertext transfer protocol (HTTP), direct data connection, WAP, e-mail or another suitable type of electronic communication. In some implementations, thecommunication unit 239 includes a wired port and a wireless transceiver. Thecommunication unit 239 also provides other conventional connections to thenetwork 105 for distribution of files and/or media objects using standard network protocols including, but not limited to, UDP, TCP/IP, HTTP, HTTPS, SMTP, SPDY, QUIC, etc. - The
display 241 may include hardware configured to display graphical data received from thevideo application 103. For example, thedisplay 241 may render graphics to display a user interface that includes a video. Thedisplay 241 is coupled to thebus 220 for communication with the other components viasignal line 228. Other hardware components that provide information to a user may be included as part of thecomputing device 200. For example, thecomputing device 200 may include a speaker for audio interfaces, a vibration or force feedback device, or other types of non-display output devices. In some implementations, such as where thecomputing device 200 is aserver 101, thedisplay 241 may be optional. In some implementations, thecomputing device 200 may not include all the components. In implementations where thecomputing device 200 is a wearable device, thecomputing device 200 may not includestorage device 243. In some implementations, thecomputing device 200 may include other components not listed here, e.g., one or more cameras, sensors, battery, etc. - The
storage device 243 may be a non-transitory computer-readable storage medium that stores data that provides the functionality described herein. In implementations where thecomputing device 200 is theserver 101, thestorage device 243 may include thedatabase 199 inFIG. 1 . Thestorage device 243 may be a DRAM device, a SRAM device, flash memory or some other memory device. In some implementations, thestorage device 243 also includes a non-volatile memory or similar permanent storage device and media including a hard disk drive, a floppy disk drive, a CD-ROM device, a DVD-ROM device, a DVD-RAM device, a DVD-RW device, a flash memory device, or some other mass storage device for storing information on a permanent basis. Thestorage device 243 is coupled to thebus 220 for communication with the other components viasignal line 230. - In the illustrated implementation shown in
FIG. 2 , thevideo application 103 includes apreselector module 202, asegmentation module 204, amedia selector module 206, avideo generation module 208, a music module 210, asocial network module 212, and a user interface module 214. Other modules and/or configurations are possible. - The
preselector module 202 may be configured to receive and transmit data. In some implementations, thepreselector module 202 may be a set of instructions executable by theprocessor 235 to filter media items. In some implementations, thepreselector module 202 may be stored in thememory 237 of thecomputing device 200 and can be accessible and executable by theprocessor 235. - In some implementations, the
preselector module 202 may receive media items. Thepreselector module 202 may receive the media items from a user, such as when the user captures media from a user device 115, from other users that shared the media items with the user, for example, via a social network operated by thesocial network module 212, tagged the user in the media items, etc. Exemplary media items may include, but are not limited to, an image, a video, an animated Graphics Interchange Format (GIF) file, a photo burst series of images, a face switching series of images, and a multi-zoom series of images. An exemplary photo burst may include an animation of a fast switch between images that create a stop-motion effect. An exemplary face switching may be used to show changes in a person's face over a particular period of time. For example, the face switching may use different pictures of a baby to show how the baby has changed over the past year. - The
preselector module 202 may filter the media items to remove duplicates and remove media items that fail to meet preselection criteria. The preselection criteria may include, for example, rules that require people or landmarks to be in the images (as opposed to images of documents, bills, receipts, etc.), rules against including screenshots, rules that require a threshold level of image quality, etc. - In some implementations, the
preselector module 202 may determine whether media items are suitable for being used as target media based on cropping the media items. For example, thepreselector module 202 may crop portions of images to identify the most salient regions of the image at a particular aspect ratio. Thepreselector module 202 may crop the images by cutting any combination of the top, bottom, left, or right of the image to highlight the entities or people in the image. In instances where portrait images include people, thepreselector module 202 may draw bounding boxes around people in the images and determine whether the portrait images may be cropped to be converted into landscape images while keeping all the people in the image. In some implementations, thepreselector module 202 may discard portrait images where the crop cuts out some of the people in the image. - The
segmentation module 204 may be configured to group media items associated with a user into segments. In some implementations, thesegmentation module 204 may be a set of instructions executable by theprocessor 235 to segment media items. In some implementations, thesegmentation module 204 may be stored in thememory 237 of thecomputing device 200 and can be accessible and executable by theprocessor 235. - In some implementations, the
segmentation module 204 receives filtered media items from thepreselector module 202. In some implementations where thepreselector module 202 is not included in thevideo application 103, thesegmentation module 204 receives the media items. For example, thesegmentation module 204 may receive the media items as described above with reference to thepreselector module 202. - The
segmentation module 204 may group the media items associated with a user into segments based on a timestamp associated with each media item and a total number of media items. Thesegmentation module 204 may use the timestamp for each media item and the total number of media items to ensure that there is sufficient time diversity of media items. For example, if the timestamps are evenly dispersed throughout a certain time period, such as a year, thesegmentation module 204 may divide the media items into 12 segments where each event is associated with a particular segment. Alternatively, if timestamps for the media items indicate that there are an abundance of media items centered around one event, thesegmentation module 204 may divide the media items associated with one event into multiple segments. Thesegmentation module 204 may determine whether to group media items associated with an event into two or more segments based on an event number of media items associated with the event as compared to the total number of media items. For example, thesegmentation module 204 may determine that the media items include a “Trip to Paris” event and because 200 of the 300 media items are associated with the trip to Paris, thesegmentation module 204 may group some media items associated with the trip to Paris into a first segment that includes the first two days in Paris based on the timestamps and group the other media items associated with the event into a second segment that includes the last two days in Paris based on the timestamps. - Exemplary timestamps may be associated with, but are not limited to, an upload time associated with uploading each media item, a capture time associated with capturing each media item, a creation time associated with creating each media item, and a last modification made to each media item. The
segmentation module 204 may select media items from media associated with a user based on the timestamp associated with each media item being indicative of a particular time period. Exemplary time periods may include, but are not limited to, a calendar year, a birthday year, a time period of an important event (e.g., a soccer tournament), a time period of a chapter of a person's life (a wedding and the associated celebration), etc. - In some implementations, the
segmentation module 204 may determine a number of target media based on a total number of media items. Thesegmentation module 204 may determine the number of target media before grouping the media items into segments. For example, thesegmentation module 204 may determine that the number of target media is half the total number of media items. - The
media selector module 206 may be configured to select target media from media items. In some implementations, themedia selector module 206 may be a set of instructions executable by theprocessor 235 to select target media. In some implementations, themedia selector module 206 may be stored in thememory 237 of thecomputing device 200 and can be accessible and executable by theprocessor 235. - In some implementations, the
media selector module 206 selects target media for each segment independently, which may advantageously ensure time-diversity and reduce the time cost of selecting target media as compared to selecting target media for each segment sequentially. In some implementations, themedia selector module 206 selects one to three images for each segment. - The
media selector module 206 may select the target media from the media items based on media attributes associated with the media item. In some implementations, themedia selector module 206 may use a training model to determine how to score the media items based on a tendency of users to share the images. For example, themedia selector module 206 may compare media that were shared with other users to media that were not shared to identify media attribute differences. Based on the differences, themedia selector module 206 may create a model for how to associate a share tendency score to media items. - In some implementations, the
media selector module 206 may apply different rules to determine which media items should be selected as target media based on the media attributes. The rules may include different types of image processing steps. For example, in instances where the user consents to the use of such data, themedia selector module 206 may perform image recognition to identify people in the media items. In some implementations, the media selector module on the rule. Themedia selector module 206 may determine whether the score exceeds a threshold, select the target media based on top scores, etc. - The rules may include an important entity rule that determines whether one or more important entities are present in the media item (e.g., based on experiences particular to the user, such as an image of a wooden box that the user indicates is important or experiences particular to users in general, such as an image of a birthday cake or a wedding picture). The rules may further include a landmark rule that determines whether one or more important landmarks are present in the media item (e.g., based on a popularity score associated with the entities, where the Eiffel Tower may be determined to be an important landmark). The rules may further include an important person rule that determines whether one or more important people are in the media item (e.g., based on a face frequency score). The rules may further include an important event rule that determines whether the media item corresponds to an important event based on whether a predetermined number of photos were taken during a particular time period (e.g., at least five photos were taken in an hour, based on correlating the media with an important date associated with the user, such as a birthday, anniversary, festival, trip, holiday, etc.). The rules may further include a high-visual quality rule that determines whether the media item has a visual quality that is within or exceeds a quality threshold (e.g., based on a level of blurriness, dynamic range, etc.). The rules may further include a highlight rule that determines whether the media item represents a highlight of an event. The rules may further include a socially engaged rule that determines whether one or more people in a media item are socially engaged (e.g., whether multiple people are looking at each other or looking directly at the camera versus people looking at unidentified objects, people that have vacant stares, etc.). In some implementations, in the instance where the user consents to the use of such data, the
media selector module 206 performs facial recognition to identify the people in the media item and determine posture, gaze direction, and emotion of the people (e.g., based on a facial pose that indicates smiling, surprised, crying, etc.). In some implementations, the time-diversity rule that determines that target media for a segment include time diversity (e.g., the target media are not selected from a single event), a shared media rule that determines whether the media is shared from a media album, a burst for mix rule that determines a quality of a burst for different types of media, a burst for animation rule that determines a quality of a burst for animation, a burst for face switching that determines a quality of a burst that includes different faces as part of a burst, a face affinity rule that determines an affinity between the user associated with the media item and a person in the media item (e.g., in the instance where the user and the person consent to the use of such data, the affinity may be based on communications between the user and the person, such as a number of emails, texts, chats, etc. shared between the user and the person), and a face clustering rule that determines clustering of people in a media album (e.g., in the instance where the user and the person consent to the use of such data, themedia selector module 206 may perform facial recognition to identify the people in the media). - In some implementations, the
media selector module 206 randomly selects one of the rules from the plurality of rules according to a configured probability. For example, themedia selector module 206 may use a process that employs a random or pseudorandom number generation technique to determine a rule to select from a group of rules. Themedia selector module 206 may determine whether the media item satisfies the rule. If the media item satisfies the rule, the media item may be selected as target media. If the media item does not satisfy the rule, themedia selector module 206 may randomly select a subsequent rule. Themedia selector module 206 may continue this process until a threshold number of rules are selected, there are no longer subsequent rules available, or the media item satisfies one of the subsequent rules. The random selection process may be advantageous over a process where multiple rules are applied all at once because some of the rules may contradict each other. For example, the important event rule may result in selecting multiple media items from the same event. Conversely, the time-diversity rule may prohibit selecting multiple media items from the same event. - In some implementations, the
media selector module 206 may rank the rules based on importance and select rules to apply to a media item based on the importance of the rules. For example, themedia selector module 206 may determine, from decreasing importance, that the order of the rules is the important entity rule, the important person rule, the important event rule, the social engagement rule, the time diversity rule, and the high-visual quality rule. In some implementations, themedia selector module 206 may assign a score to each rule, compute a total score for each media item that is a weighted sum of the scores, and select the media items that are associated with the best scores (e.g., the highest scores if a higher score is indicative of a media item that satisfies one or more rules). One disadvantage of the ranking approach is that themedia selector module 206 selects media items as target media that somewhat satisfy many rules over media items that greatly satisfy one rule, which may result in mediocre results for the target media. In addition, it may be difficult to select appropriate weights for scoring the different types of rules. - In some implementations, the
media selector module 206 selects a cover image and an ending image for the video. For example, themedia selector module 206 may identify media items that satisfy multiple rules mentioned above. In some implementations, themedia selector module 206 may apply different criteria when selecting the cover image and the ending image. In some implementations, the criteria when selecting the cover image and the ending image may be stricter than for the rest of the target media. For example, the cover image and the ending image may have to satisfy a first rule and subsequent rules, such as having an important entity or landmark, an important person or a group photo where the user associated with the media items is in the image, people smiling in the image, and the visual quality may include a different quality threshold value. In some implementations, the quality threshold value when selecting the cover image and the ending image may be stricter than for the rest of the target media. In some implementations, themedia selector module 206 selects landscape images or images that may be cropped to form a landscape image for the cover image and the ending image. - The
video generation module 208 may be configured to generate a video from the target media. In some implementations, thevideo generation module 208 may be a set of instructions executable by theprocessor 235 to generate the video. In some implementations, thevideo generation module 208 may be stored in thememory 237 of thecomputing device 200 and can be accessible and executable by theprocessor 235. - In some implementations, the
video generation module 208 generates portions of a video from target media for each segment and combines the portions in sequence to form the video. In some implementations, the video generation module generates a video that is a sequence of the target media. - The
video generation module 208 may assemble the target media into a video by creating animations of the target media that are arranged into a sequence. In particular, instead of having a slideshow that displays one photo and then fades into another photo, thevideo generation module 208 may generate one or more animations that include transitions between different target media items. For example, thevideo generation module 208 may generate an animation that includes a close-up of a first image, the animation then zooms out of the close-up of the first image to show additional portions of the first image. The animation then zooms out of the first image a second time and then the animation includes a transition from the first image to a second image, where the second image is adjacent to the first image in the animation. The animations are discussed in greater detail below with reference toFIGS. 3A-3D . Other transitions are possible, such as changing a perspective of the target media by zooming-in to emphasize important regions of the target media or an illustration of the target media moving in a particular direction (left-to-right, top-to-bottom, etc.). - In some implementations, the
video generation module 208 generates a video that includes a first animation that illustrates transitions from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, where the first item is adjacent to the second item. Thevideo generation module 208 may determine whether the target media includes one or more additional items and, if there are additional items, thevideo generation module 208 may generate a subsequent animation that illustrates a subsequent transition from the second item to an additional item. The subsequent transition may further illustrate additional transitions between a remainder of the one or more additional items. - The
video generation module 208 may add a title to the video, such as a title that describes the particular time period represented by the target media. For example, the video may be titled “Year in Review 2014,” “Sara Is One,” “Daniel's Graduation,” etc. Thevideo generation module 208 may determine a title for the video from a title of an album, in the instance where the user consents to the use of the data, determining a title based on calendar entries, etc. - The music module 210 may be configured to add a song to the video. In some implementations, the music module 210 may be a set of instructions executable by the
processor 235 to add the song to the video. In some implementations, the music module 210 may be stored in thememory 237 of thecomputing device 200 and can be accessible and executable by theprocessor 235. - In some implementations, the music module 210 may add a song to the video, where one or more features of the song are synchronized with the video. In some implementations, the features of the song may include beats that are matched to the transitions. For example, the transition from a first media item to a second media item may be synchronized with a beat of the video. In another example, a zoom-out effect on a first item in the target media may be synchronized with a beat of the video.
- In some implementations, different types of beats may be matched with the video. For example, transitions between target media items may be matched with a major beat in the song and transitions between different perspectives of a target media item (e.g., zooming-out) may be matched to a minor beat in the song. The features may also include different vocal sections, instrumental sections, quiet sections, loud sections, male voice sections, female voice sections, etc.
- In some implementations, the music module 210 may use a database of songs (e.g., the
database 199 inFIG. 1 , thestorage device 243 ofFIG. 2 , etc.) or retrieve songs from a third-party server that are identified to have features that synchronize with transitions in a video. For example, the music module 210 may identify songs for the database that have consistent beats at predetermined intervals, songs that are instrumental, etc. In some implementations, the music module 210 may select different songs based on a theme of the video. For example, the music module 210 may determine that the theme of a video is “Mara's Trip to Paris” based a title associated with the video, landmark recognition of the target media, etc. and match the Parisian theme with a French-inspired song. In some implementations, the music module 210 may synchronize any song selected by a user to the video. - The
social network module 212 may be configured to manage a social network. In some implementations, thesocial network module 212 may be a set of instructions executable by theprocessor 235 to manage the social network. In some implementations, thesocial network module 212 may be stored in thememory 237 of thecomputing device 200 and may be accessible and executable by theprocessor 235. In some implementations, thesocial network module 212 may be excluded from thevideo application 103. - A social network can be a type of social structure where the users may be connected by a common feature. The common feature may include relationships/connections, e.g., friendship, family, work, an interest, etc. The common features may be provided by one or more social networking systems including explicitly defined relationships and relationships implied by social connections with other online users, where the relationships form a social graph that may be stored in the
storage device 243. In some examples, the social graph can reflect a mapping of these users and how they can be related. As a result, the social network may encompass, for example, social network applications, mail applications, chat applications, and search applications. - In the instance where the user consents to the use of such data, the
social network module 212 may post the video to the user's social feed. In addition, where users have consented to the use of such data, thesocial network module 212 may identify other people in the video that are members of the social network. For example, thesocial network module 212 may receive the identity of the members from themedia selector module 206. Thesocial network module 212 may associate the identity of the members with the members' profiles. Thesocial network module 212 may then tag the members in the video and notify the members that they were tagged in the video. - The user interface module 214 may be configured to provide information to a user. In some implementations, the user interface module 214 can be a set of instructions executable by the
processor 235 to provide the functionality described below for providing information to a user. In some implementations, the user interface module 214 can be stored in thememory 237 of thecomputing device 200 and can be accessible and executable by theprocessor 235. - In some implementations, the user interface module 214 may generate a user interface that allows a user to edit the video. For example, the user interface may include options that allow a user to edit the video. In some examples, the user may edit the video by adding a user-specified media item to the target media and removing one or more media items from the target media. In some examples, the user may edit the video by specifying an order of the target media in the video, editing the target media (cropping a photo, applying an enhancement to a burst, etc.), and reordering the target media in the video. In some examples, the user may edit the video by changing the song, selecting the song, and modifying the volume of the song. In some examples, the user may edit the video by changing the title. In instances where the user edits the video, the
media selector module 206 may select replacement target media (if applicable), thevideo generation module 208 may update the video, and the music module 210 may update synchronization of the song with the updated video. - The user interface module 214 may generate a link to a network location for a user or other users (e.g., users of the social network) to access the video. For example, the user interface module 214 may generate a uniform resource locator (URL) that the user may share with other users, a landing page that any user may visit to obtain their own personalized video, etc. The user interface module 214 may further generate one or more options to share the network location, such as an email notification, a mobile callout, a share by link, an automatic notification to other users that are directly connected to the user in the social network, a post to be displayed on the user's social network, etc. In some implementations, the user interface module 214 may generate an option to download the video.
- In some implementations, the user interface module 214 may generate a preview of the video that includes highlights of the video. For example, the preview may include the cover image, the end image, and 12 target media items that are static images and that satisfy the rules. In some implementations, the target media for the preview may satisfy a more strict set of criteria than the target media for the video. For example, the target media for the preview may be selected from the best scored target media from the video. In some implementations, the preview may be automatically added to a user's media library (on the cloud, on their phone, etc.).
-
FIGS. 3A-3D illustrates graphic representations of an animation generated by thevideo generation module 208.FIG. 3A illustrates agraphic representation 300 of a first portion of an animation. In this example, thegraphic representation 300 is a close-up of auser 305 and afriend 307 of the user. Because thegraphic representation 300 is a cover image, thegraphic representation 300 satisfies multiple rules as determined by themedia selector module 206. For example, themedia selector module 206 determines that the cover image includes the user, the cover image includes a person with a high face affinity score based on frequent communications between the person and the user (with consent from the user), the cover image includes animportant landmark 309, and the quality of the image meets a threshold quality value. -
FIG. 3B illustrates agraphic representation 325 of a second portion of the animation. In this example, the animation includes a first transition from the close-up of theuser 305 and thefriend 307 as illustrated inFIG. 3A to a zoomed-out version of the cover image as illustrated inFIG. 3B . In thisgraphic representation 325 theimportant landmark 309 is identifiable as the Eiffel Tower. -
FIG. 3C illustrates agraphic representation 350 of a third portion of the animation. In this example, the animation includes a second transition from the close-up of theuser 305, thefriend 307, and theimportant landmark 309 as illustrated inFIG. 3B to a zoomed-out version of the cover image as illustrated inFIG. 3C . Thegraphic representation 350 also includes a title for the cover image that indicates that the subject matter of the video is the year in photos. -
FIG. 3D illustrates agraphic representation 375 of a fourth portion of the animation. In this example, the animation includes a third transition from thecover image 380 to asecond item 385. The third transition is illustrated as thecover image 380 being adjacent to thesecond item 385 with movement from the right to the left as thecover image 380 moves from an onscreen location to an offscreen location. The second item is illustrated as including anotherfriend 387 of the user in front of anotherimportant landmark 389, which themedia selector module 206 identified as the Zakim Bridge in Boston. - In some implementations, the first two transitions may be synchronized to a different feature of the song than the third transition. This may be advantageous, for example, for the user to distinguish between different types of transitions, such a zoom-out transition (e.g., the first two transitions) and a horizontal movement (e.g., the third transition).
-
FIG. 4 is a flowchart of anexample method 400 to generate a video. Themethod 400 may be implemented byserver 101, a user device 115 or a combination of theserver 101 and the user device 115, using thevideo application 103 illustrated inFIG. 1 . Thevideo application 103 may include thesegmentation module 204, themedia selector module 206, thevideo generation module 208, and the music module 210 illustrated inFIG. 2 . - At block 402, media items associated with a user are grouped into segments based on a timestamp associated with each media item and a total number of media items. For example, the
segmentation module 204 may group the media items associated with the user into segments based on the timestamp data and the total number of media items. In some implementations, if the timestamp data and the total number of media items indicate a time diversity in the media items, thesegmentation module 204 may group the media items into segments where a single event represented in the media items are part of one of the segments. For example, images of a user's birthday party may be associated with a single segment. In other implementations, if there is a lack of time diversity in the media items, a single event represented in the media items may span two or more segments. For example, a user's trip to Paris may be associated with two segments. - At
block 404, target media is selected from the media items for each of the segments based on media attributes associated with the media attributes associated with the media item, the target media including two or more media items. For example, themedia selector module 206 may select the target media. The target media may be selected based on rules as described with reference toFIG. 5 . - At
block 406, a video is generated that includes the target media for each of the segments. For example, thevideo generation module 208 may generate the video. The video includes animations as described in greater detail below with reference toFIG. 6 . - At
block 408, a song is added to the video, where one or more features of the song are synchronized with the video. For example, the music module 210 may add the song to the video. In some examples, the one or more features may include beats that are synchronized to transitions in the video. - While blocks 402 to 408 are illustrated in a particular order, other orders are possible with intervening steps. In some implementations, some blocks may be added, skipped, or combined.
-
FIG. 5 is a flowchart of anexample method 500 to select target media from media items. Themethod 500 may be implemented byserver 101, a user device 115 or a combination of theserver 101 and the user device 115, using thevideo application 103 illustrated inFIG. 1 . Thevideo application 103 may include themedia selector module 206 illustrated inFIG. 2 . - At
block 502, a rule may be randomly selected from a plurality of rules. For example, themedia selector module 206 may randomly select the rule from the plurality of rules. The rule may be randomly selected according to a probability associated with the rule (e.g., a weight associated with each of the rules). - At block 504, it is determined whether the media item satisfies the rule. For example, the
media selector module 206 determines whether the media item satisfies the rule. The rule may be satisfied by applying a score to the media item based on media attributes being analyzed based on the rule and determining whether the score exceeds a threshold value. Responsive to a determination that the media item does not satisfy the rule, themethod 500 proceeds to block 508 where it is determined whether there is a subsequent rule from the plurality of rules. For example, themedia selector module 206 determines where there is a subsequent rule. Responsive to a determination that the subsequent rule is present, themethod 500 proceeds to block 502. Responsive to a determination that there is not a subsequent rule from the plurality of rules, themethod 500 proceeds to block 510. - Responsive to the media item satisfying the rule, the
method 500 proceeds to block 510. Atblock 510, it is determined whether there are additional media items. For example, themedia selector module 206 determines whether there are additional media items. Responsive to determining that there are additional media items, themethod 500 proceeds to block 506. Responsive to determining that there are not additional media items, themethod 500 proceeds to block 406 ofFIG. 4 . - While
blocks 502 to 510 are illustrated in a particular order, other orders are possible with intervening steps. In addition, blocks may be added, skipped, or combined. -
FIG. 6 is a flowchart of anexample method 600 to generate an animation with two or more transitions. Themethod 600 may be implemented using thevideo application 103 illustrated inFIG. 1 . Thevideo application 103 may include thevideo generation module 208 illustrated inFIG. 2 . - At block 602, a first animation is generated that illustrates a first transition from a first item from the target media to a second item from the target media with movement of the first item from an onscreen location to an offscreen location, where the first item is adjacent to the second item in the first animation. For example, the
video generation module 208 generates the first animation. - At
block 604, it is determined whether the target media includes one or more additional items. For example, thevideo generation module 208 determines whether the target media includes the one or more additional items. Responsive to a determination that the target media does not include the one or more additional items, themethod 600 proceeds to block 408 ofFIG. 4 . Responsive to the target media including the one or more additional items, themethod 600 proceeds to block 606. - At
block 606, a subsequent animation is generated that illustrates a subsequent transition from the second item to an additional item. For example, thevideo generation module 208 generates the subsequent animation. The method proceeds to block 604 and continues in a loop until there are no longer additional items in the target media. - In the above description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the specification. It will be apparent, however, to one skilled in the art that the disclosure can be practiced without these specific details. In some instances, structures and devices are shown in block diagram form in order to avoid obscuring the description. For example, the implementations can be described above primarily with reference to user interfaces and particular hardware. However, the implementations can apply to any type of computing device that can receive data and commands, and any peripheral devices providing services.
- Reference in the specification to “some implementations” or “some instances” means that a particular feature, structure, or characteristic described in connection with the implementations or instances can be included in at least one implementation of the description. The appearances of the phrase “in some implementations” in various places in the specification are not necessarily all referring to the same implementations.
- Some portions of the detailed descriptions above are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here, and generally, conceived to be a self-consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical or magnetic data capable of being stored, transferred, combined, compared, and otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to these data as bits, values, elements, symbols, characters, terms, numbers, or the like.
- It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms including “processing” or “computing” or “calculating” or “determining” or “displaying” or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission, or display devices.
- The implementations of the specification can also relate to a processor for performing one or more steps of the methods described above. The processor may be a special-purpose processor selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a non-transitory computer-readable storage medium, including, but not limited to, any type of disk including floppy disks, optical disks, ROMs, CD-ROMs, magnetic disks, RAMs, EPROMs, EEPROMs, magnetic or optical cards, flash memories including USB keys with non-volatile memory, or any type of media suitable for storing electronic instructions, each coupled to a computer system bus.
- The specification can take the form of some entirely hardware implementations, some entirely software implementations or some implementations containing both hardware and software elements. In some implementations, the specification is implemented in software, which includes, but is not limited to, firmware, resident software, microcode, etc.
- Furthermore, the description can take the form of a computer program product accessible from a computer-usable or computer-readable medium providing program code for use by or in connection with a computer or any instruction execution system. For the purposes of this description, a computer-usable or computer-readable medium can be any apparatus that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device.
- A data processing system suitable for storing or executing program code will include at least one processor coupled directly or indirectly to memory elements through a system bus. The memory elements can include local memory employed during actual execution of the program code, bulk storage, and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.
- In situations in which the systems discussed above collect personal information, the systems provide users with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location), or control whether and/or how to receive content from the server that may be more relevant to the user. In addition, certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, a user's identity may be treated on that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined. Thus, the user may have control over how information is collected about the user and used by the server.
Claims (20)
Priority Applications (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/885,285 US9691431B2 (en) | 2015-10-16 | 2015-10-16 | Generating videos of media items associated with a user |
PCT/US2016/056313 WO2017066131A1 (en) | 2015-10-16 | 2016-10-10 | Generating videos of media items associated with a user |
US15/633,011 US10242711B2 (en) | 2015-10-16 | 2017-06-26 | Generating videos of media items associated with a user |
US16/359,928 US10685680B2 (en) | 2015-10-16 | 2019-03-20 | Generating videos of media items associated with a user |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/885,285 US9691431B2 (en) | 2015-10-16 | 2015-10-16 | Generating videos of media items associated with a user |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/633,011 Continuation US10242711B2 (en) | 2015-10-16 | 2017-06-26 | Generating videos of media items associated with a user |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170110154A1 true US20170110154A1 (en) | 2017-04-20 |
US9691431B2 US9691431B2 (en) | 2017-06-27 |
Family
ID=57208370
Family Applications (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/885,285 Active 2035-12-09 US9691431B2 (en) | 2015-10-16 | 2015-10-16 | Generating videos of media items associated with a user |
US15/633,011 Active US10242711B2 (en) | 2015-10-16 | 2017-06-26 | Generating videos of media items associated with a user |
US16/359,928 Active US10685680B2 (en) | 2015-10-16 | 2019-03-20 | Generating videos of media items associated with a user |
Family Applications After (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/633,011 Active US10242711B2 (en) | 2015-10-16 | 2017-06-26 | Generating videos of media items associated with a user |
US16/359,928 Active US10685680B2 (en) | 2015-10-16 | 2019-03-20 | Generating videos of media items associated with a user |
Country Status (2)
Country | Link |
---|---|
US (3) | US9691431B2 (en) |
WO (1) | WO2017066131A1 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180279018A1 (en) * | 2017-03-21 | 2018-09-27 | International Business Machines Corporation | Customizable audio and video formats for digital content consumption |
CN112637622A (en) * | 2020-12-11 | 2021-04-09 | 北京字跳网络技术有限公司 | Live broadcasting singing method, device, equipment and medium |
US11146514B2 (en) * | 2018-05-07 | 2021-10-12 | Apple Inc. | Sharing content in a messaging application |
US20220365990A1 (en) * | 2021-05-11 | 2022-11-17 | Google Llc | Determining a visual theme in a collection of media items |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN110913240B (en) * | 2019-12-02 | 2022-02-22 | 广州酷狗计算机科技有限公司 | Video interception method, device, server and computer readable storage medium |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080123955A1 (en) * | 2006-11-29 | 2008-05-29 | Chia-Hung Yeh | Method for estimating boundary of video segment in video streams |
US20130195427A1 (en) * | 2012-01-27 | 2013-08-01 | Nokia Corporation | Method and apparatus for developing and utilizing multi-track video files |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7694213B2 (en) * | 2004-11-01 | 2010-04-06 | Advanced Telecommunications Research Institute International | Video content creating apparatus |
US8718386B2 (en) | 2010-02-23 | 2014-05-06 | Intellectual Ventures Fund 83 Llc | Adaptive event timeline in consumer image collections |
US9071885B2 (en) * | 2010-08-18 | 2015-06-30 | Demand Media, Inc. | Systems, methods, and machine-readable storage media for presenting animations overlying multimedia files |
US20150268932A1 (en) * | 2013-03-15 | 2015-09-24 | MediaGraph, LLC | Systems and methods to generate a playlist from a media map |
-
2015
- 2015-10-16 US US14/885,285 patent/US9691431B2/en active Active
-
2016
- 2016-10-10 WO PCT/US2016/056313 patent/WO2017066131A1/en active Application Filing
-
2017
- 2017-06-26 US US15/633,011 patent/US10242711B2/en active Active
-
2019
- 2019-03-20 US US16/359,928 patent/US10685680B2/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080123955A1 (en) * | 2006-11-29 | 2008-05-29 | Chia-Hung Yeh | Method for estimating boundary of video segment in video streams |
US20130195427A1 (en) * | 2012-01-27 | 2013-08-01 | Nokia Corporation | Method and apparatus for developing and utilizing multi-track video files |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180279018A1 (en) * | 2017-03-21 | 2018-09-27 | International Business Machines Corporation | Customizable audio and video formats for digital content consumption |
US11146514B2 (en) * | 2018-05-07 | 2021-10-12 | Apple Inc. | Sharing content in a messaging application |
US20220006767A1 (en) * | 2018-05-07 | 2022-01-06 | Apple Inc. | Sharing content in a messaging application |
CN112637622A (en) * | 2020-12-11 | 2021-04-09 | 北京字跳网络技术有限公司 | Live broadcasting singing method, device, equipment and medium |
US20220365990A1 (en) * | 2021-05-11 | 2022-11-17 | Google Llc | Determining a visual theme in a collection of media items |
Also Published As
Publication number | Publication date |
---|---|
US9691431B2 (en) | 2017-06-27 |
US20190252001A1 (en) | 2019-08-15 |
US20170309311A1 (en) | 2017-10-26 |
US10242711B2 (en) | 2019-03-26 |
US10685680B2 (en) | 2020-06-16 |
WO2017066131A1 (en) | 2017-04-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10685680B2 (en) | Generating videos of media items associated with a user | |
US11615131B2 (en) | Method and system for storytelling on a computing device via social media | |
US10896478B2 (en) | Image grid with selectively prominent images | |
US11455093B2 (en) | Capturing and sending multimedia as electronic messages | |
CN109565613B (en) | Method for generating video collages and computer storage medium | |
US9779157B2 (en) | Bundled event memories | |
TWI579838B (en) | Automatic generation of compilation videos | |
US11700220B2 (en) | Generation of electronic media content collections | |
US20170316256A1 (en) | Automatic animation triggering from video | |
TW201545120A (en) | Automatic generation of compilation videos | |
US20150130816A1 (en) | Computer-implemented methods and systems for creating multimedia animation presentations | |
KR102437640B1 (en) | Image selection suggestions | |
CN106462810A (en) | Connecting current user activities with related stored media collections | |
TW201606538A (en) | Image organization by date | |
US11665312B1 (en) | Video reformatting recommendation | |
CN114127779A (en) | Automatic generation of character groups and image-based authoring |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:DAI, SHENGYANG;ST. CLAIR, TIMOTHY SEPKOSKI;ASHIDA, KOJI;AND OTHERS;SIGNING DATES FROM 20151014 TO 20151016;REEL/FRAME:036813/0030 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044097/0658Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |