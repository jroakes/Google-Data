US20230274731A1 - Mixing Heterogeneous Loss Types to Improve Accuracy of Keyword Spotting - Google Patents
Mixing Heterogeneous Loss Types to Improve Accuracy of Keyword Spotting Download PDFInfo
- Publication number
- US20230274731A1 US20230274731A1 US17/652,801 US202217652801A US2023274731A1 US 20230274731 A1 US20230274731 A1 US 20230274731A1 US 202217652801 A US202217652801 A US 202217652801A US 2023274731 A1 US2023274731 A1 US 2023274731A1
- Authority
- US
- United States
- Prior art keywords
- loss
- label
- hotword
- neural network
- encoder
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012549 training Methods 0.000 claims abstract description 121
- 238000013528 artificial neural network Methods 0.000 claims abstract description 110
- 238000000034 method Methods 0.000 claims abstract description 82
- 230000008569 process Effects 0.000 claims abstract description 42
- 230000015654 memory Effects 0.000 claims description 94
- 238000011176 pooling Methods 0.000 claims description 49
- 238000012545 processing Methods 0.000 claims description 31
- 238000009499 grossing Methods 0.000 claims description 14
- 238000000354 decomposition reaction Methods 0.000 claims description 8
- 238000004891 communication Methods 0.000 claims description 4
- 230000006870 function Effects 0.000 description 29
- 210000002569 neuron Anatomy 0.000 description 19
- 238000001914 filtration Methods 0.000 description 15
- 238000013459 approach Methods 0.000 description 12
- 238000001514 detection method Methods 0.000 description 9
- 238000004590 computer program Methods 0.000 description 8
- 230000003287 optical effect Effects 0.000 description 6
- 230000004913 activation Effects 0.000 description 5
- 230000002123 temporal effect Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 230000007958 sleep Effects 0.000 description 3
- 230000006978 adaptation Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000002372 labelling Methods 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 238000007781 pre-processing Methods 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000005236 sound signal Effects 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000004580 weight loss Effects 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 230000001133 acceleration Effects 0.000 description 1
- 230000009471 action Effects 0.000 description 1
- 239000000654 additive Substances 0.000 description 1
- 230000000996 additive effect Effects 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000008014 freezing Effects 0.000 description 1
- 238000007710 freezing Methods 0.000 description 1
- 230000006266 hibernation Effects 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003595 spectral effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 238000005303 weighing Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
- G10L2015/025—Phonemes, fenemes or fenones being the recognition units
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
Definitions
- This disclosure relates to a system for spotting keywords in streaming audio.
- a speech-enabled environment (e.g., home, workplace, school, automobile, etc.) allows a user to speak a query or a command out loud to a computer-based system that fields and answers the query and/or performs a function based on the command.
- the speech-enabled environment can be implemented using a network of connected microphone devices distributed through various rooms or areas of the environment. These devices may use so called “hotwords” to help discern when a given utterance is directed at the system, as opposed to an utterance that is directed to another individual present in the environment. Accordingly, the devices may operate in a sleep state or a hibernation state and wake-up only when a detected utterance includes a hotword.
- the devices in the environment must be able to detect hotwords accurately and efficiently. Neural networks have recently emerged as an attractive solution for training models to detect hotwords spoken by users in streaming audio.
- One aspect of the disclosure provides a method for training a hotword detector using two labels for training data and two loss functions.
- the computer-implemented method when executed on data processing hardware causes the data processing hardware to perform operations including receiving a training input audio sequence including a sequence of input frames. The sequence of input frames defining a hotword that initiates a wake-up process on a user device.
- the operations further include obtaining a first label for the training input audio sequence and a second label for the training input audio sequence, the second label different than the first label.
- the operations also include generating, using a memorized neural network and the training input audio sequence, an output indicating a likelihood the training input audio sequence includes the hotword.
- the operations include determining a first loss based on the first label and the output and a second loss based on the second label and the output.
- the operations further include optimizing the memorized neural network based on the first loss and the second loss associated with the training input audio sequence.
- Implementations of the disclosure may include one or more of the following optional features.
- the memorized neural network includes an encoder and a decoder, with each of the encoder and the decoder of the memorized neural network including sequentially stacked single value decomposition filter (SVDF) layers.
- the output is based on a probability distribution of a logit based on the training input audio sequence.
- the operations further include smoothing the logit prior to determining the first loss.
- determining the first loss includes generating a plurality of encoder windows, each encoder window of the plurality of encoder windows associated with one or more phonemes of the hotword. These implementations further include determining the first loss for each encoder window of the plurality of encoder windows. In these implementations a collective size of the plurality of encoder windows may correspond to an average acoustic length of the hotword.
- optimizing the memorized neural network may include generating a weighted average of the first loss and the second loss.
- the second label is derived from one or more phoneme sequences of the hotword.
- the first label may be based on a position of a last phoneme of the hotword.
- the sequence of input frames each may include one or more respective audio features characterizing phonetic components of the hotword.
- the first label is a max pooling loss label and the second label is a cross entropy label.
- the first loss is a max pooling loss and the second loss is a cross entropy loss.
- the system includes data processing hardware and memory hardware in communication with the data processing hardware.
- the memory hardware stores instructions that when executed on the data processing hardware causes the data processing hardware to perform operations including receiving a training input audio sequence including a sequence of input frames.
- the sequence of input frames defining a hotword that initiates a wake-up process on a user device.
- the operations further include obtaining a first label for the training input audio sequence and a second label for the training input audio sequence, the second label different than the first label.
- the operations also include generating, using a memorized neural network and the training input audio sequence, an output indicating a likelihood the training input audio sequence includes the hotword.
- the operations include determining a first loss based on the first label and the output and a second loss based on the second label and the output.
- the operations further include optimizing the memorized neural network based on the first loss and the second loss associated with the training input audio sequence.
- the memorized neural network includes an encoder and a decoder, with each of the encoder and the decoder of the memorized neural network including sequentially stacked single value decomposition filter (SVDF) layers.
- the output is based on a probability distribution of a logit based on the training input audio sequence.
- the operations further include smoothing the logit prior to determining the first loss.
- determining the first loss includes generating a plurality of encoder windows, each encoder window of the plurality of encoder windows associated with one or more phonemes of the hotword. These implementations further include determining the first loss for each encoder window of the plurality of encoder windows. In these implementations a collective size of the plurality of encoder windows may correspond to an average acoustic length of the hotword.
- optimizing the memorized neural network may include generating a weighted average of the first loss and the second loss.
- the second label is derived from one or more phoneme sequences of the hotword.
- the first label may be based on a position of a last phoneme of the hotword.
- the sequence of input frames each may include one or more respective audio features characterizing phonetic components of the hotword.
- the first label is a max pooling loss label and the second label is a cross entropy label.
- the first loss is a max pooling loss and the second loss is a cross entropy loss
- FIG. 1 is a schematic view of an example system for training a memorized neural network and using the trained memorized neural network to detect a hotword in a spoken utterance.
- FIG. 2 is a schematic view of components of a typical neural network acoustic encoder used by models that detect hotwords.
- FIG. 3 A is a schematic view of example components of the memorized neural network of the system of FIG. 1 .
- FIG. 3 B is a schematic view of example components of a memorized neural network with multiple layers.
- FIGS. 4 A and 4 B are schematic views showing audio feature-label pairs generated from streaming audio for training neural networks.
- FIGS. 5 A and 5 B are schematic views of layers of the memorized neural network of the system of FIG. 1 .
- FIG. 5 C is a schematic view of an example training process for the memorized neural network of the system of FIG. 1 .
- FIG. 6 is a graphical representation of an example of windows used during the training process of FIG. 5 C .
- FIG. 7 is a schematic view of an example training process for the memorized neural network of FIG. 1 using two labels and two loss functions.
- FIG. 8 is a flowchart of an example arrangement of operations for a method of training a neural network to detect a hotword.
- FIG. 9 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- a voice-enabled device e.g., a user device executing a voice assistant
- a voice-enabled device allows a user to speak a query or a command out loud and field and answer the query and/or perform a function based on the command.
- a “hotword” also referred to as a “keyword”, “attention word”, “wake-up phrase/word”, “trigger phrase”, or “voice action initiation command”
- the voice enabled device is able to discern between utterances directed to the system (i.e., to initiate a wake-up process for processing one or more terms following the hotword in the utterance) and utterances directed to an individual in the environment.
- the voice-enabled device operates in a sleep state to conserve battery power and does not process input audio data unless the input audio data follows a spoken hotword. For instance, while in the sleep state, the voice-enabled device captures input audio via a microphone and uses a hotword detector trained to detect the presence of the hotword in the input audio. When the hotword is detected in the input audio, the voice-enabled device initiates a wake-up process for processing the hotword and/or any other terms in the input audio following the hotword.
- Hotword detection is analogous to searching for a needle in a haystack because the hotword detector must continuously listen to streaming audio, and trigger correctly and instantly when the presence of the hotword is detected in the streaming audio. In other words, the hotword detector is tasked with ignoring streaming audio unless the presence of the hotword is detected.
- Neural networks are commonly employed by hotword detectors to address the complexity of detecting the presence of a hotword in a continuous stream of audio.
- a hotword detector typically includes three main components: a signal processing frontend; a neural network acoustic encoder; and a hand-designed decoder.
- the signal processing frontend may convert raw audio signals captured by the microphone of the user device into one or more audio features formatted for processing by the neural network acoustic encoder component.
- the neural network acoustic encoder component may convert these audio features into phonemes and the hand-designed decoder uses a hand-coded algorithm to stitch the phonemes together to provide a probability of whether or not an audio sequence includes the hotword.
- a common method for training a neural network includes providing a labeled training sample to the neural network.
- the training sample is typically a prescreened data input that is labeled based on the desired output of the neural network. For example, for a hotword detector, the training sample is labeled with an indication of the presence of a hotword (e.g., a “1” if a hotword is present in the training sample, and a “0” otherwise).
- the neural network analyzes the training sample and then generates an output or prediction which is compared to the predefined target output (i.e., the label) to determine a loss using a loss function. The loss indicates an accuracy of the output compared to the label. The loss is then fed to the neural network which adjusts one or more weights, values, or parameters based on the loss.
- the training sample may include an audio sequence and the neural network may output an indication or probability that the audio sequence includes a hotword. While this training process appears straightforward, there are many different ways to label the audio sequence, with each labeling convention resulting in a unique loss. In turn, each resulting loss tunes the hotword detector in a specific manner. For example, a simple cross entropy based loss uses labels derived from phoneme sequences. The cross entropy based loss is highly sensitive to positional alignment of all sub-phonemes including the keyword and, thus, the resulting trained hotword detector uses a significant portion of its resources on learning to predict an exact position to signal detection of a pattern.
- a max pooling technique uses labels derived from only the position of the last phoneme of the keyword. Accordingly, unlike the cross entropy example above, the max pooling technique does not rely on positional information of other sub-phonemes and the resulting hotword detector is more stable than a hotword detector trained using cross entropy based loss, as it can ignore the noise in the positional alignment.
- each of the examples above have strengths and weaknesses when compared to each other or to other techniques.
- Implementations herein are directed toward an end-to-end hotword spotting system (also referred to as a ‘keyword spotting system’) that trains a hotword detector using two separate target labels for each sample in a training input set.
- the two target labels are each used to determine two separate losses.
- the hotword detector receives a loss based on both of the two losses and adjusts accordingly.
- the hotword detector can be fine-tuned using multiple techniques. Simultaneously using the loss functions based on, for example, cross entropy and max pooling to train the hotword detector can result in a more accurate and efficient hotword detector than a hotword detector trained on either technique individually.
- an example system 100 includes one or more user devices 102 each associated with a respective user 10 and in communication with a remote system 110 via a network 104 .
- Each user device 102 may correspond to a computing device, such as a mobile phone, computer, wearable device, smart appliance, smart speaker, etc., and is equipped with data processing hardware 103 and memory hardware 105 .
- the remote system 110 may be a single computer, multiple computers, or a distributed system (e.g., a cloud environment) having scalable/elastic computing resources 112 (e.g., data processing hardware) and/or storage resources 114 (e.g., memory hardware).
- the user device 102 receives a trained memorized neural network 300 from the remote system 110 via the network 104 and executes the trained memorized neural network 300 to detect hotwords in streaming audio 118 .
- the trained memorized neural network 300 may reside in a hotword detector 106 (also referred to as a hotworder) of the user device 102 that is configured to detect the presence of a hotword in streaming audio without performing semantic analysis or speech recognition processing on the streaming audio 118 .
- the trained memorized neural network 300 may additionally or alternatively reside in an automatic speech recognizer (ASR) 108 of the user device 102 and/or the remote system 110 to confirm that the hotword detector 106 correctly detected the presence of a hotword in streaming audio 118 .
- ASR automatic speech recognizer
- the data processing hardware 112 trains the memorized neural network 300 using training samples 400 obtained from annotated utterance pools 130 .
- the training samples may include a first label 420 , 420 a and a second label 420 , 420 b . That is, each training sample may be annotated with two separate labels 420 a , 420 b .
- the annotated utterance pools 130 may reside on the memory hardware 114 and/or some other remote memory location(s).
- the memorized neural network 300 executing on the user device 102 is configured to detect the presence of the hotword in the utterance 120 to initiate a wake-up process on the user device 102 for processing the hotword and/or one or more other terms (e.g., query or command) following the hotword in the utterance 120 .
- the user device 102 sends the utterance 120 to the remote system 110 for additional processing or verification (e.g., with another, potentially more computationally-intensive memorized neural network 300 ).
- the memorized neural network 300 includes an encoder portion 310 and a decoder portion 311 each including a layered topology of single value decomposition filter (SVDF) layers 302 .
- the SVDF layers 302 provide the memory for the neural network 300 by providing each SVDF layer 302 with a memory capacity such that the memory capacities of all of the SVDF layers 302 additively make-up the total fixed memory for the neural network 300 to remember only a fixed length of time in the streaming audio 118 necessary to capture audio features 410 ( FIGS. 4 A and 4 B ) that characterize the hotword.
- This memorized neural network 300 architecture is exemplary, and it is understood than any memorized neural network 300 architecture may be substituted.
- the memorized neural network 300 is trained using the multiple labels 420 , 420 a —b to generate a respective loss 710 , 710 a —b for each corresponding label 420 a —b.
- the process of training neural network 300 with multiple labels 420 is described in greater detail below ( FIG. 7 ).
- a typical hotword detector uses a neural network acoustic encoder 200 without memory. Because the network 200 lacks memory, each neuron 212 of the acoustic encoder 200 must accept, as an input, every audio feature of every frame 210 , 210 a —d of a spoken utterance 120 simultaneously. Note that each frame 210 can have any number of audio features, each of which the neuron 212 accepts as an input. Such a configuration requires a neural network acoustic encoder 200 of substantial size that increases dramatically as the fixed length of time increases and/or the number of audio features increases. The output of the acoustic encoder 200 results in a probability of each, for example, phoneme of the hotword that has been detected.
- the acoustic encoder 200 must then rely on a hand-coded decoder to process the outputs of the acoustic encoder 200 (e.g., stitch together the phonemes) in order to generate a score (i.e., an estimation) indicating a presences of the hotword.
- a hand-coded decoder to process the outputs of the acoustic encoder 200 (e.g., stitch together the phonemes) in order to generate a score (i.e., an estimation) indicating a presences of the hotword.
- a single value decomposition filter (SVDF) neural network 300 (also referred to as a memorized neural network) has any number of neurons/nodes 312 , where each neuron 312 accepts only a single frame 210 , 210 a —d of a spoken utterance 120 at a time. That is, if each frame 210 , for example, constitutes 30 ms of audio data, a respective frame 210 is input to the neuron 312 approximately every 30 ms (i.e., Time 1, Time 2, Time 3, Time 4, etc.).
- each neuron 312 including a two-stage filtering mechanism: a first stage 320 (i.e., Stage 1 Feature Filter) that performs filtering on a features dimension of the input and a second stage 340 (i.e., Stage 2 Time Filter) that performs filtering on a time dimension on the outputs of the first stage 320 . Therefore, the stage 1 feature filter 320 performs feature filtering on only the current frame 210 . The result of the processing is then placed in a memory component 330 . In these examples, the size of the memory component 330 is configurable per node or per layer level.
- stage 1 feature filter 320 After the stage 1 feature filter 320 processes a given frame 210 (e.g., by filtering audio features within the frame), the filtered result is placed in a next available memory location 332 , 332 a —d of the memory component 330 . Once all memory locations 332 are filled, the stage 1 feature filter 320 will overwrite the memory location 332 storing the oldest filtered data in the memory component 330 . Note that, for illustrative purposes, FIG.
- FIG. 3 A shows a memory component 330 of size four (four memory locations 332 a —d) and four frames 210 a —d, but due to the nature of hotword detection, the system 100 will typically monitor streaming audio 118 continuously such that each neuron 312 will “slide” along or process frames 210 akin to a pipeline.
- the layer is analogous to computing N x T (T equaling the number of frames 210 in a fixed period of time) convolutions of the feature filters by sliding each of the N filters 320 , 340 on the input feature frames 210 , with a stride the size of the feature frames.
- the stage 1 feature filter 320 would place filtered audio features associated with following Frame 5 (F5) (during a Time 5) into memory 330 by overwriting the filtered audio features associated with Frame 1 (F1) 210 a within memory location 332 a .
- the stage 2 time filter 340 applies filtering to the previous T ⁇ 1 (T again equaling the number of frames 210 in a fixed period of time) filtered audio features output from the stage 1 feature filter 320 .
- the stage 2 time filter 340 then filters each filtered audio feature stored in memory 330 .
- FIG. 3 A shows the stage 2 time filter 340 filtering the audio features in each of the four memory locations 332 every time the stage 1 feature filter 320 stores a new filtered audio feature into memory 330 .
- the stage 2 time filter 340 is always filtering a number of past frames 210 , where the number is proportional to the size of the memory 330 .
- Each neuron 312 is part of a single SVDF layer 302 , and the neural network 300 may include any number of layers 302 .
- the output of each stage 2 time filter 340 is passed to an input of a neuron 312 in the next layer 302 .
- the number of layers 302 and the number of neurons 312 per layer 302 is fully configurable and is dependent upon available resources and desired size, power, and accuracy. This disclosure is not limited to the number of SVDF layers 302 nor the number of neurons 312 in each SVDF layer 302 .
- each SVDF layer 302 , 302 a —n (or simply ‘layer’) of the neural network 300 is connected such that the outputs of the previous layer are accepted as inputs to the corresponding layer 302 .
- the final layer 302 n outputs a probability score 350 indicating the probability that the utterance 120 includes the hotword.
- the layer design derives from the concept that a densely connected layer 302 that is processing a sequence of input frames 210 can be approximated by using a singular value decomposition of each of its nodes 312 .
- the approximation is configurable.
- a rank R approximation signifies extending a new dimension R for the layer's filters: stage 1 occurs independently, and in stage 2, the outputs of all ranks get added up prior to passing through the non-linearity.
- an SVDF decomposition of the nodes 312 of a densely connected layer of matching dimensions can be used to initialize an SVDF layer 302 , which provides a principled initialization and increases the quality of the layer's generalization.
- the “power” of a larger densely connected layer is transferred into a potentially (depending on the rank) much smaller SVDF.
- the SVDF layer 302 does not need the initialization to outperform a densely connected or even convolutional layer with the same or even more operations.
- the system 100 includes a stateful, stackable neural network 300 where each neuron 312 of each SVDF layer 302 includes a first stage 320 , associated with filtering audio features, and a second stage 340 , associated with filtering outputs of the first stage 320 with respect to time.
- the first stage 320 is configured to perform filtering on one or more audio features on one audio feature input frame 210 at a time and output the filtered audio features to the respective memory component 330 .
- the stage 1 feature filter 320 receives one or more audio features associated with a time frame 210 as input for processing and outputs the processed audio features into the respective memory component 330 of the SVDF layer 302 .
- the second stage 340 is configured to perform filtering on all the filtered audio features output from the first stage 320 and residing in the respective memory component 330 . For instance, when the respective memory component 330 is equal to eight (8), the second stage 340 would pull up to the last eight (8) filtered audio features residing in the memory component 330 that were output from the first stage 320 during individual filtering of the audio features within a sequence of eight (8) input frames 210 . As the first stage 320 fills the corresponding memory component 330 to capacity, the memory locations 332 containing the oldest filtered audio features are overwritten (i.e., first in, first out).
- the second stage 340 is capable of remembering a number of past outputs processed by the first stage 320 of the corresponding SVDF layer 302 .
- the memory components 330 at the SVDF layers 302 are additive, the memory component 330 at each SVDF neuron 312 and layer 302 also includes the memory of each preceding SVDF neuron 312 and layer 302 , thus extending the overall receptive field of the memorized neural network 300 .
- the last SVDF layer 302 will include a sequence of up to the last thirty-two (32) audio feature input frames 210 individually filtered by the neural network 300 .
- the amount of memory is configurable per layer 302 or even per node 312 .
- the first layer 302 a may be allotted thirty-two (32) locations 332
- the last layer 302 may be configured with eight (8) locations 332 .
- the stacked SVDF layers 302 allow the neural network 300 to process only the audio features for one input time frame 210 (e.g., 30 milliseconds of audio data) at a time and incorporate a number of filtered audio features into the past that capture the fixed length of time necessary to capture the designated hotword in the streaming audio 118 .
- a neural network 200 without memory would require its neurons 212 to process all of the audio feature frames covering the fixed length of time (e.g., 2 seconds of audio data) at once in order to determine the probability of the streaming audio including the presence of the hotword, which drastically increases the overall size of the network.
- RNNs recurrent neural networks
- LSTM long short-term memory
- RNN-LSTMs cause the neurons to continuously update their state after each processing instance, in effect having an infinite memory, and thereby prevent the ability to remember a finite past number of processed outputs where each new output re-writes over a previous output (once the fixed-sized memory is at capacity).
- SVDF networks do not recur the outputs into the state (memory), nor rewrite all the state with each iteration; instead, the memory keeps each inference run's state isolated from subsequent runs, instead pushing and popping in new entries based on the memory size configured for the layer.
- the memorized neural network 300 is trained on a plurality of training input audio sequences 400 (i.e., training samples) that each include a sequence of input frames 210 , 210 a —n and two or more labels 420 a — b assigned to the input frames 210 .
- Each input frame 210 includes one or more respective audio features 410 characterizing phonetic components 430 of a hotword, and each label 420 indicates a probability that the one or more audio features 410 of a respective input frame 210 include a phonetic component 430 of the hotword.
- the audio features 410 for each input frame 210 are converted from raw audio signals 402 of an audio stream 118 during a pre-processing stage 404 .
- the audio features 410 may include one or more log-filterbanks.
- the pre-processing stage may segment the audio stream 118 (or spoken utterance 120 ) into the sequence of input frames 210 (e.g., 30 ms each), and generate separate log-filterbanks for each frame 210 .
- each frame 210 may be represented by forty log-filterbanks.
- each successive SVDF layer 302 receives, as input, the filtered audio features 410 with respect to time that are output from the immediately preceding SVDF layer 302 .
- each training input audio sequence 400 is associated with a training sample that includes an annotated (i.e., with labels 420 a —b) utterance containing a designated hotword occurring within a fixed length of time (e.g., two seconds).
- the memorized neural network 300 may also optionally be trained on annotated utterances 400 that do not include the designated hotword, or include the designated hotword but spanning a time longer than the fixed length of time, and thus, would not be falsely detected due to the fixed memory forgetting data outside the fixed length of time.
- the fixed length of time corresponds to an amount of time that a typical speaker would take to speak the designated hotword to summon a user device 102 for processing spoken queries and/or voice commands.
- the neural network 300 includes an amount of fixed memory that is proportional to the amount of audio to span the fixed time (e.g., two seconds).
- the fixed memory of the neural network 300 allows neurons 312 of the neural network to filter audio features 410 (e.g., log-filterbanks) from one input frame 210 (e.g., 30 ms time window) of the streaming audio 118 at a time, while storing the most recent filtered audio features 410 spanning the fixed length of time and removing or deleting any filtered audio features 410 outside the fixed length of time from a current filtering iteration.
- audio features 410 e.g., log-filterbanks
- the neural network 300 has, for example, a memory depth of thirty-two (32), the first thirty-two (32) frames processed by the neural network 300 will fill the memory component 330 to capacity, and for each new output after the first 32 , the neural network 300 will remove the oldest processed audio feature from the corresponding memory location 332 of the memory component 330 .
- training input audio sequence 400 a includes labels 420 a that may be applied to each input frame 210 .
- a target label 420 a associated with a target score e.g., ‘1’
- a target score e.g., ‘1’
- each input frame 210 includes a corresponding input feature-label pair 410 , 420 a .
- the input features 410 are typically one-dimensional tensors corresponding to, for example, mel filterbanks or log-filterbanks, computed from the input audio over the input frame 210 .
- the exemplary label 420 a focuses on the position of the last phoneme of the hotword and does not rely on positional information of other sub-phonemes (hence the label “0” for phonetic components that are not “1”). Typically this type of label 420 a is associated with a max pooling loss, which does not depend on the exact location of the target pattern, and instead looks to define an existence of a pattern in a defined interval.
- the labels 420 a are generated from the annotated utterances 400 a , where each input feature tensor 410 is assigned a phonetic class via a force-alignment step (i.e., a label of ‘1’ is given to pairs corresponding to the last class belonging to the hotword, and ‘0’ to all the rest).
- the training input audio sequence 400 a includes binary labels assigned to the sequence of input frames.
- the annotated utterances 400 a , or training input audio sequence 400 a correspond to the training samples 400 obtained from the annotated utterance pools 130 of FIG. 1 .
- FIG. 4 B includes a training input audio sequence 400 b that includes labels 420 b associated with scores that increase along the sequence of input frames 210 as the number of audio features 410 characterizing (matching) phonetic components 430 of the hotword progresses.
- the hotword includes “Ok Google”
- the input frames 210 that include respective audio features 410 that characterize the first phonetic components, ‘o’ and ‘k’ have assigned labels 420 b of ‘1’
- the input frames 210 that include respective audio features 410 characterizing the final phonetic component of ‘1’ have assigned labels 420 b of ‘5’.
- the input frames 210 including respective audio features 410 characterizing the middle phonetic components 430 have assigned labels 420 b of ‘2’, ‘3’, and ‘4’.
- the number of positive labels 420 b increases. For example, a fixed amount of ‘1’ labels 420 b is generated, starting from the first frame 210 including audio features 410 characterizing to the final phonetic component 430 of the hotword.
- a positive label 420 b may be applied to frames 210 that otherwise would have been applied a non-positive label 420 b (e.g., ‘0’).
- the start position of the positive label 420 b is modified.
- the label 420 b may be shifted to start at either a start, mid-point, or end of a segment of frames 210 containing the final keyword phonetic component 430 .
- a weight loss is associated with the input sequence.
- weight loss data is added to the input sequence that allows the training procedure to reduce the loss (i.e. error gradient) caused by small mis-alignment.
- a loss can be caused from either mis-classification or mis-alignment.
- the neural network 300 predicts both the correct label 420 b and correct position (timing) of the label 420 b .
- the network 300 Even if the network 300 detected the keyword at some point, the result can be considered an error if it's not perfectly aligned with the given target label 420 b .
- weighing the loss is particularly useful for frames 210 with high likelihood of mis-alignment during the force-alignment stage.
- the exemplary labels 420 b are typically associated with a cross entropy loss, which results in a model that is highly sensitive to positional alignments of all sub-phonemes of the keyword.
- the neural network 300 is optimized (using a determined loss) to generate outputs 350 indicating whether the hotword(s) are present in the streaming audio 118 .
- the network 300 is trained in two stages. Referring now to FIG. 5 A , schematic view 500 a shows an encoder portion (or simply ‘encoder’) 310 a of the neural network 300 that includes, for example, eight layers, that are trained individually to produce acoustic posterior probabilities.
- the network 300 may, for example, include bottleneck, softmax, and/or other layers.
- label generation assigns distinct classes to all the phonetic components of the hotword (plus silence and “epsilon” targets for all that is not the hotword).
- the decoder portion (or simply ‘decoder’) 311 a of the neural network 300 is trained by creating a topology where the first part (i.e. the layers and connections) matches that of the encoder 310 a , and a selected checkpoint from that encoder 310 a of the neural network 300 is used to initialize it.
- the training is specified to “freeze” (i.e. not update) the parameters of the encoder 310 a , thus tuning just the decoder 311 a portion of the topology. This naturally produces a single spotter neural network, even though it is the product of two staggered training pipelines. Training with this method is particularly useful on models that tend to present overfitting to parts of the training set.
- the neural network 300 is trained end-to-end from the start.
- the neural network 300 accepts features directly (similarly to the encoder 310 a training described previously), but instead uses the binary target label 420 a (i.e., ‘0’ or ‘1’) outputs for use in training the decoder 311 a .
- Such an end-to-end neural network 300 may use any topology.
- schematic view 500 b shows a neural network 300 topology of an encoder 310 b and a decoder 311 b that is similar to the topology of FIG. 5 A except that the encoder 310 b does not include the intermediate softmax layer.
- the topology of FIG. 5 A the topology of FIG.
- 5 B may use a pre-trained encoder checkpoint with an adaptation rate to tune how the decoder 311 b part is adjusted (e.g. if the adaptation rate is set to 0, it is equivalent to the FIG. 5 A topology).
- This end-to-end pipeline where the entirety of the topology's parameters are adjusted, tends to outperform the separately trained encoder 310 a and decoder 311 a of FIG. 5 A , particularly in smaller sized models which do not tend to over fit.
- neural network 300 may avoid the use of a manually tuned decoder.
- Manual tuning the decoder increases the difficulty in changing or adding hotwords.
- the single memorized neural network 300 can be trained to detect multiple different hotwords, as well as the same hotword across two or more locales. Further, detection quality reduces compared to a network optimized specifically for hotword detection trained with potentially millions of examples.
- typical manually tuned decoders are more complicated than a single neural network that performs both encoding and decoding. Traditional systems tend to be over parameterized, consuming significantly more memory and computation than a comparable end-to-end model and they are unable to leverage as much neural network acceleration hardware. Additionally, a manual tuned decoder suffers from accented utterances, and makes it extremely difficult to create detectors that can work across multiple locales and/or languages.
- the memorized neural network 300 outperforms simple fully-connected layers of the same size, but also benefits from optionally initializing parameters from a pre-trained fully connected layer.
- the network 300 allows fine grained control over how much to remember from the past. This results in outperforming RNN-LSTMs for certain tasks that do not benefit (and actually are hurt) from paying attention to theoretically infinite past (e.g. continuously listening to streaming audio).
- network 300 can work in tandem with RNN-LSTMs, typically leveraging SVDF for the lower layers, filtering the noisy low-level feature past, and LSTM for the higher layers.
- the number of parameters and computation are finely controlled, given that several relatively small filters comprise the SVDF. This is useful when selecting a tradeoff between quality and size/computation.
- network 300 allows creating very small networks that outperform other topologies like simple convolutional neural networks (CNNs) which operate at a larger granularity.
- CNNs simple convolutional neural networks
- the neural network 300 is optimized using a smoothed max pooling loss.
- this approach includes jointly training an encoder 310 , 310 c and a decoder 311 , 311 c .
- the neural network 300 may be trained to detect not only parts of a hotword (e.g., with the encoder 310 c ), but also an entire hotword (e.g., with the decoder 311 c ).
- this approach does not depend on frame labels 420 a —b and may lend itself to implementations such as on-device learning (e.g., for user devices 102 ).
- a training input audio sequence 400 often includes intervals of repeated similar or identical frame labels 420 called runs. For instance, both FIGS. 4 A and 4 B include runs of “0.” These runs, when training the network 300 , indicate that the network 300 should make a strong learning association for the generation of outputs 350 .
- a smoothed max pooling approach e.g., as shown in FIGS. 5 C and 6 ) avoids specifying an exact activation position (i.e., specifying timing) using frame labels 420 .
- a smoothed max pooling loss approach for a smoothed max pooling loss approach, in some examples, an initial loss is defined for both the encoder 310 c and the decoder 311 c and then the initial loss of each the encoder 310 c and the decoder 311 c is optimized simultaneously.
- Max pooling refers to a sample-based discretization process where some input is reduced in dimensionality by applying a max filter.
- a training process 500 c using the smoothed max pooling approach includes a smoothing operation 510 , 510 e —d and a max pooling operation 520 , 520 e —d. In these examples, the smoothing operation 510 occurs before the max pooling operation 520 .
- the training process 500 c performs a temporal smoothing on the frames 210 .
- the training process 500 c smooths logits 502 , 502 e —d corresponding to the frames 210 .
- a logit generally refers to a vector or other raw predictive form that is output from the one or more SVDF layers 302 .
- the logit 502 serves as an input into the softmax portion of an encoder 310 and/or a decoder 311 such that the encoder 310 and/or the decoder 311 generates an output probability based on the input of one or more logits 502 .
- the logit 502 is a non-normalized predictive data form and the softmax normalizes the logit 502 into a probability (e.g., a probability of a hotword).
- the training process 500 c trains the network 300 with greater stability for small variation and temporal shifts within the streaming audio 118 .
- This greater stability is in contrast to other training approaches that may use some form of a max pooling operation without a temporal smoothing operation.
- other training approaches may use max pooling in a time domain and determine cross entropy loss with respect to a logit 502 of a frame 210 with maximum activation.
- the training process 500 c of the network 300 may result in smooth activation and stable peak values.
- the training process 500 c determines a smoothed max pooling loss where the loss represents a difference between what the network 300 thinks that the output distribution should theoretically be and what the output distribution actually is.
- the smoothed max pooling loss may be determined by the following equations:
- Loss Loss + + Loss - ( 1 )
- Loss - [ - log ⁇ y c t ( X t , W ) ] ( 5 )
- X t is a spectral feature of d-dimension
- y i (X t , W) stands for an i-th dimension of the neural network's softmax output
- W is the network weight
- c t is a frame label 420 at frame t (e.g., a frame 210 )
- s(t) is a smoothing filter
- ß is a convolution over time
- [ ⁇ i start , ⁇ i end ] defines a start and an end time of an interval of the i-th max pooling window.
- both the encoder 310 c and the decoder 311 c undergo the training process 500 c that uses the smoothed max pooling approach.
- FIG. 5 C illustrates the encoder 310 c including a smoothing operation 510 , 510 e and a max pooling operation 520 , 520 e .
- the encoder 310 c learns a sequence of sound-parts (e.g., phonetic components of audio features 410 ) that define the hotword.
- this learning may occur in a semi-supervised manner.
- the max pooling operation 510 e during training 500 c occurs by dividing a fixed-length hotword (e.g., an expected length of a hotword or an average length of the hotword) into max-pooling windows 310 w , 310 w 1-n .
- a fixed-length hotword e.g., an expected length of a hotword or an average length of the hotword
- FIG. 6 depicts n-sequential windows 310 w over an expected hotword location.
- the max pooling operation 510 e determines a max pooling loss at each window 310 w .
- the max pooling loss at each window 310 w is defined by the following equations:
- ⁇ i e_start ⁇ end +offset e ⁇ win size e *i, i ⁇ [1, . . . , n] (6)
- ⁇ i e_end ⁇ i e_start +win size e , i ⁇ [ 1, . . . , n] (7)
- ⁇ end corresponds to an endpoint for the hotword
- offset refers to a time offset for a window 310 w.
- the number of windows 310 w and/or the size 310 w s of each window 310 w are tunable parameters during the training process 500 c . These parameters may be tuned such that the number of windows 310 w “n” approximates the number of distinguishable sound-parts (e.g., phonemes) and/or the size 310 w s of the windows 310 w multiplied by “n” number of windows 310 w approximately matches the fixed-length of the hotword.
- an encoder offset Offsete that offsets the sequence of windows 310 w from an endpoint Wend of the hotword may also be tunable during the training 500 c of the encoder 310 c.
- the decoder 311 c includes a smoothing operation 510 , 510 d and a max pooling operation 520 , 520 d .
- the training process 500 c trains the decoder 311 c to generate strong activation (i.e., a high probability of detection for a hotword) for input frames 210 that contain audio features 410 at or near the end of the hotword. Due to the nature of max pooling loss, max pooling loss values are not sensitive to an exact value for the endpoint Wend of the hotword as long as a decoder window 311 w includes the actual endpoint Wend of the hotword.
- the training process 500 c determines the max pooling loss for a window 311 w containing the endpoint Wend of the hotword according to the following equations:
- offset d and win size d may be tunable parameters to include the expected endpoint Wend of the hotword.
- the decoder window 311 w is shown as an interval extending from ⁇ 1 d_start to ⁇ 1 d_end .
- the smoothed max pooling loss approach allows the network 300 to learn an optimal position of strongest activation (e.g., in a semi-supervised manner).
- the training process 500 c derives the endpoint Wend of the hotword based on word-level alignment.
- the endpoint Wend of the hotword is determined based on the output of the encoder 310 .
- the smoothed max pooling approach jointly trains the encoder 310 c and decoder 311 c simultaneously without such freezing. Since the encoder 310 c and the decoder 311 c are jointly trained during the training process 500 c using smoothed max pooling loss, the relative importance of each loss may be controlled by a tunable parameter, a. For instance, the total loss referring to the loss at the encoder 310 c and the loss at the decoder 311 c have a relationship as described by the following equation:
- a training process 700 for a memorized neural network 300 includes using a first label 420 a (e.g., a cross entropy label 420 ) and a second label 420 b (e.g., a max pooling loss label) and a first loss function 705 , 705 a and a second loss function 705 , 705 b to generate a first loss 710 , 710 a and a second loss 710 , 710 b , respectively.
- the process 700 begins by feeding a memorized neural network 300 a training input audio sequence 400 .
- the data of the training input audio sequence 400 is labeled using both labels 420 a —b.
- a single training input audio sequence 400 is labeled using the first label 420 a and the second label 420 b as described above with respect to FIGS. 4 A and 4 B .
- the example labels 420 a , 420 b are for illustrative purposes and are not intended to be limiting as any suitable labeling convention applicable for determining a loss 710 can be used in the training process 700 .
- the memorized neural network 300 may generate the output 350 (i.e., the probability score 350 ).
- the memorized neural network 300 may process the training input audio sequence 400 in the manner described with respect to any of FIG. 2 - 6 or any other suitable manner for processing audio data to determine a likelihood a hotword is present in the training input audio sequence 400 .
- the output 350 is used by each of the two loss functions 705 . That is, the first loss function 705 a receives the output 350 and the label 420 a to determine the first loss 710 a . Similarly, the second loss function 705 b receives the output 350 and the label 420 b to determine the second loss 710 b .
- the losses 710 are each determined from the same output 350 by using two different labels 420 a , 420 bb of the same training input audio sequence 400 and two different loss functions 705 a , 705 b .
- the loss functions 705 may determine the losses 710 in any manner as described with respect to any of FIGS. 2 - 6 .
- the first loss function 705 a is a max pooling loss function and the second loss function 705 b is a cross entropy loss function.
- a single loss function 705 receives the output 350 and labels 420 and generates a respective loss 710 based on each label 420 .
- the loss functions 705 may implement any suitable technique such as regression loss, mean squared error, mean squared logarithmic error, mean absolute error, binary classification, binary cross entropy, hinge loss, multi-class loss, etc.
- the losses 710 a , 710 b are fed directly to the memorized neural network 300 during the training process 700 .
- the losses 710 a , 710 b are combined or weighted together to produce a joint loss 710 , 710 c and the joint loss 710 c is processed by the memorized neural network 300 .
- the losses are averaged using a weighted averaging formula.
- the first loss 710 a and the second loss 710 b may be defined as follows:
- X is the output 350
- L1 is the first loss function 705 a
- L2 is the second loss function 705 b
- Y2 is the label 420 b
- the joint loss 710 c is represented by:
- alpha and beta are scalar hyper-parameters.
- the first loss 710 a and the second loss 710 b may be combined in any other manner (e.g., added, multiplied, etc.).
- Examples herein illustrate training a neural network 300 with training input audio sequences 400 annotated with the two labels 420 a,b .
- the first loss function 705 a uses the output 350 and the label 420 a to generate the first loss 710 a .
- the second loss function 705 b uses the output 350 and the label 420 b to generate the second loss 710 b .
- the neural network is trained, updated, or fine-tuned using both the first loss 710 a and the second loss 710 b . It is understood that these examples are non-limiting and any number of labels 420 and any number of respective loss function 705 may generate any number of losses to train any appropriate neural network 300 .
- FIG. 8 is a flowchart of an example arrangement of operations for a method 800 of training a neural network 300 using multiple labels 420 and multiple loss functions 705 .
- the method 800 includes receiving a training input audio sequence 400 including a sequence of input frames.
- the sequence of input frames defines a hotword that initiates a wake-up process on a user device 102 .
- the method 800 includes obtaining a first label 420 a (e.g., a max pooling label) and a second label 420 b (e.g., a cross entropy label) for the training input audio sequence 400 .
- a first label 420 a e.g., a max pooling label
- a second label 420 b e.g., a cross entropy label
- the method 800 includes generating, using a memorized neural network 300 and the training input audio sequence 400 , an output 350 indicating a likelihood the training input audio sequence 400 includes the hotword.
- the method 800 includes determining a first loss 710 a (e.g., a max pooling loss) based on the first label 420 a and the output 350 .
- the method 800 includes determining a second loss 710 b (e.g., a cross entropy loss) based on the second label 420 b and the output 350 .
- the method 800 includes optimizing the memorized neural network 300 based on the first loss 710 a and the second loss 710 b associated with the training input audio sequence 400 .
- a software application may refer to computer software that causes a computing device to perform a task.
- a software application may be referred to as an “application,” an “app,” or a “program.”
- Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- the non-transitory memory may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device.
- the non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- FIG. 9 is schematic view of an example computing device 900 that may be used to implement the systems and methods described in this document.
- the computing device 900 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 900 includes a processor 910 , memory 920 , a storage device 930 , a high-speed interface/controller 940 connecting to the memory 920 and high-speed expansion ports 950 , and a low speed interface/controller 960 connecting to a low speed bus 970 and a storage device 930 .
- Each of the components 910 , 920 , 930 , 940 , 950 , and 960 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 910 can process instructions for execution within the computing device 900 , including instructions stored in the memory 920 or on the storage device 930 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 970 coupled to high speed interface 940 .
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 900 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 920 stores information non-transitorily within the computing device 900 .
- the memory 920 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 920 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 900 .
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 930 is capable of providing mass storage for the computing device 900 .
- the storage device 930 is a computer-readable medium.
- the storage device 930 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 920 , the storage device 920 , or memory on processor 910 .
- the high speed controller 940 manages bandwidth-intensive operations for the computing device 900 , while the low speed controller 960 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 940 is coupled to the memory 920 , the display 980 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 950 , which may accept various expansion cards (not shown).
- the low-speed controller 960 is coupled to the storage device 930 and a low-speed expansion port 990 .
- the low-speed expansion port 990 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 900 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 900 a or multiple times in a group of such servers 900 a , as a laptop computer 900 b , or as part of a rack server system 900 c.
- implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data
- a computer need not have such devices.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
Abstract
A method for training a neural network includes receiving a training input audio sequence including a sequence of input frames defining a hotword that initiates a wake-up process on a user device. The method further includes obtaining a first label and a second label for the training input audio sequence. The method includes generating, using a memorized neural network and the training input audio sequence, an output indicating a likelihood the training input audio sequence includes the hotword. The method further includes determining a first loss based on the first label and the output. The method includes determining a second loss based on the second label and the output. The method further includes optimizing the memorized neural network based on the first loss and the second loss associated with the training input audio sequence.
Description
- This disclosure relates to a system for spotting keywords in streaming audio.
- A speech-enabled environment (e.g., home, workplace, school, automobile, etc.) allows a user to speak a query or a command out loud to a computer-based system that fields and answers the query and/or performs a function based on the command. The speech-enabled environment can be implemented using a network of connected microphone devices distributed through various rooms or areas of the environment. These devices may use so called “hotwords” to help discern when a given utterance is directed at the system, as opposed to an utterance that is directed to another individual present in the environment. Accordingly, the devices may operate in a sleep state or a hibernation state and wake-up only when a detected utterance includes a hotword. For the speech-enabled environment to operate optimally, the devices in the environment must be able to detect hotwords accurately and efficiently. Neural networks have recently emerged as an attractive solution for training models to detect hotwords spoken by users in streaming audio.
- One aspect of the disclosure provides a method for training a hotword detector using two labels for training data and two loss functions. The computer-implemented method when executed on data processing hardware causes the data processing hardware to perform operations including receiving a training input audio sequence including a sequence of input frames. The sequence of input frames defining a hotword that initiates a wake-up process on a user device. The operations further include obtaining a first label for the training input audio sequence and a second label for the training input audio sequence, the second label different than the first label. The operations also include generating, using a memorized neural network and the training input audio sequence, an output indicating a likelihood the training input audio sequence includes the hotword. The operations include determining a first loss based on the first label and the output and a second loss based on the second label and the output. The operations further include optimizing the memorized neural network based on the first loss and the second loss associated with the training input audio sequence.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, the memorized neural network includes an encoder and a decoder, with each of the encoder and the decoder of the memorized neural network including sequentially stacked single value decomposition filter (SVDF) layers. In other implementations, the output is based on a probability distribution of a logit based on the training input audio sequence. In these implementations, the operations further include smoothing the logit prior to determining the first loss.
- In some additional implementations, determining the first loss includes generating a plurality of encoder windows, each encoder window of the plurality of encoder windows associated with one or more phonemes of the hotword. These implementations further include determining the first loss for each encoder window of the plurality of encoder windows. In these implementations a collective size of the plurality of encoder windows may correspond to an average acoustic length of the hotword.
- Further, optimizing the memorized neural network may include generating a weighted average of the first loss and the second loss. In some implementations, the second label is derived from one or more phoneme sequences of the hotword. Alternatively, the first label may be based on a position of a last phoneme of the hotword. Further, the sequence of input frames each may include one or more respective audio features characterizing phonetic components of the hotword.
- In some example implementations, the first label is a max pooling loss label and the second label is a cross entropy label. In these example implementations, the first loss is a max pooling loss and the second loss is a cross entropy loss.
- Another aspect of the disclosure provides a system for training a hotword detector using two labels for training data and two loss functions. The system includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that when executed on the data processing hardware causes the data processing hardware to perform operations including receiving a training input audio sequence including a sequence of input frames. The sequence of input frames defining a hotword that initiates a wake-up process on a user device. The operations further include obtaining a first label for the training input audio sequence and a second label for the training input audio sequence, the second label different than the first label. The operations also include generating, using a memorized neural network and the training input audio sequence, an output indicating a likelihood the training input audio sequence includes the hotword. The operations include determining a first loss based on the first label and the output and a second loss based on the second label and the output. The operations further include optimizing the memorized neural network based on the first loss and the second loss associated with the training input audio sequence.
- This aspect may include one or more of the following optional features. In some implementations, the memorized neural network includes an encoder and a decoder, with each of the encoder and the decoder of the memorized neural network including sequentially stacked single value decomposition filter (SVDF) layers. In other implementations, the output is based on a probability distribution of a logit based on the training input audio sequence. In these implementations, the operations further include smoothing the logit prior to determining the first loss.
- In some additional implementations, determining the first loss includes generating a plurality of encoder windows, each encoder window of the plurality of encoder windows associated with one or more phonemes of the hotword. These implementations further include determining the first loss for each encoder window of the plurality of encoder windows. In these implementations a collective size of the plurality of encoder windows may correspond to an average acoustic length of the hotword.
- Further, optimizing the memorized neural network may include generating a weighted average of the first loss and the second loss. In some implementations, the second label is derived from one or more phoneme sequences of the hotword. Alternatively, the first label may be based on a position of a last phoneme of the hotword. Further, the sequence of input frames each may include one or more respective audio features characterizing phonetic components of the hotword.
- In some example implementations, the first label is a max pooling loss label and the second label is a cross entropy label. In these example implementations, the first loss is a max pooling loss and the second loss is a cross entropy loss
- The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a schematic view of an example system for training a memorized neural network and using the trained memorized neural network to detect a hotword in a spoken utterance. -
FIG. 2 is a schematic view of components of a typical neural network acoustic encoder used by models that detect hotwords. -
FIG. 3A is a schematic view of example components of the memorized neural network of the system ofFIG. 1 . -
FIG. 3B is a schematic view of example components of a memorized neural network with multiple layers. -
FIGS. 4A and 4B are schematic views showing audio feature-label pairs generated from streaming audio for training neural networks. -
FIGS. 5A and 5B are schematic views of layers of the memorized neural network of the system ofFIG. 1 . -
FIG. 5C is a schematic view of an example training process for the memorized neural network of the system ofFIG. 1 . -
FIG. 6 is a graphical representation of an example of windows used during the training process ofFIG. 5C . -
FIG. 7 is a schematic view of an example training process for the memorized neural network ofFIG. 1 using two labels and two loss functions. -
FIG. 8 is a flowchart of an example arrangement of operations for a method of training a neural network to detect a hotword. -
FIG. 9 is a schematic view of an example computing device that may be used to implement the systems and methods described herein. - Like reference symbols in the various drawings indicate like elements.
- A voice-enabled device (e.g., a user device executing a voice assistant) allows a user to speak a query or a command out loud and field and answer the query and/or perform a function based on the command. Through the use of a “hotword” (also referred to as a “keyword”, “attention word”, “wake-up phrase/word”, “trigger phrase”, or “voice action initiation command”), in which by agreement a predetermined term/phrase that is spoken to invoke attention for the voice enabled device is reserved, the voice enabled device is able to discern between utterances directed to the system (i.e., to initiate a wake-up process for processing one or more terms following the hotword in the utterance) and utterances directed to an individual in the environment. Typically, the voice-enabled device operates in a sleep state to conserve battery power and does not process input audio data unless the input audio data follows a spoken hotword. For instance, while in the sleep state, the voice-enabled device captures input audio via a microphone and uses a hotword detector trained to detect the presence of the hotword in the input audio. When the hotword is detected in the input audio, the voice-enabled device initiates a wake-up process for processing the hotword and/or any other terms in the input audio following the hotword.
- Hotword detection is analogous to searching for a needle in a haystack because the hotword detector must continuously listen to streaming audio, and trigger correctly and instantly when the presence of the hotword is detected in the streaming audio. In other words, the hotword detector is tasked with ignoring streaming audio unless the presence of the hotword is detected. Neural networks are commonly employed by hotword detectors to address the complexity of detecting the presence of a hotword in a continuous stream of audio.
- A hotword detector typically includes three main components: a signal processing frontend; a neural network acoustic encoder; and a hand-designed decoder. The signal processing frontend may convert raw audio signals captured by the microphone of the user device into one or more audio features formatted for processing by the neural network acoustic encoder component. For instance, the neural network acoustic encoder component may convert these audio features into phonemes and the hand-designed decoder uses a hand-coded algorithm to stitch the phonemes together to provide a probability of whether or not an audio sequence includes the hotword.
- A common method for training a neural network includes providing a labeled training sample to the neural network. The training sample is typically a prescreened data input that is labeled based on the desired output of the neural network. For example, for a hotword detector, the training sample is labeled with an indication of the presence of a hotword (e.g., a “1” if a hotword is present in the training sample, and a “0” otherwise). The neural network analyzes the training sample and then generates an output or prediction which is compared to the predefined target output (i.e., the label) to determine a loss using a loss function. The loss indicates an accuracy of the output compared to the label. The loss is then fed to the neural network which adjusts one or more weights, values, or parameters based on the loss.
- For training a hotword detector, the training sample may include an audio sequence and the neural network may output an indication or probability that the audio sequence includes a hotword. While this training process appears straightforward, there are many different ways to label the audio sequence, with each labeling convention resulting in a unique loss. In turn, each resulting loss tunes the hotword detector in a specific manner. For example, a simple cross entropy based loss uses labels derived from phoneme sequences. The cross entropy based loss is highly sensitive to positional alignment of all sub-phonemes including the keyword and, thus, the resulting trained hotword detector uses a significant portion of its resources on learning to predict an exact position to signal detection of a pattern. In another example, a max pooling technique uses labels derived from only the position of the last phoneme of the keyword. Accordingly, unlike the cross entropy example above, the max pooling technique does not rely on positional information of other sub-phonemes and the resulting hotword detector is more stable than a hotword detector trained using cross entropy based loss, as it can ignore the noise in the positional alignment. However, each of the examples above have strengths and weaknesses when compared to each other or to other techniques.
- Implementations herein are directed toward an end-to-end hotword spotting system (also referred to as a ‘keyword spotting system’) that trains a hotword detector using two separate target labels for each sample in a training input set. In turn, for each training sample, the two target labels are each used to determine two separate losses. The hotword detector then receives a loss based on both of the two losses and adjusts accordingly. By using two losses determined differently using two different labels, the hotword detector can be fine-tuned using multiple techniques. Simultaneously using the loss functions based on, for example, cross entropy and max pooling to train the hotword detector can result in a more accurate and efficient hotword detector than a hotword detector trained on either technique individually.
- Referring to
FIG. 1 , in some implementations, anexample system 100 includes one ormore user devices 102 each associated with arespective user 10 and in communication with aremote system 110 via anetwork 104. Eachuser device 102 may correspond to a computing device, such as a mobile phone, computer, wearable device, smart appliance, smart speaker, etc., and is equipped withdata processing hardware 103 andmemory hardware 105. Theremote system 110 may be a single computer, multiple computers, or a distributed system (e.g., a cloud environment) having scalable/elastic computing resources 112 (e.g., data processing hardware) and/or storage resources 114 (e.g., memory hardware). Theuser device 102 receives a trained memorizedneural network 300 from theremote system 110 via thenetwork 104 and executes the trained memorizedneural network 300 to detect hotwords in streamingaudio 118. The trained memorizedneural network 300 may reside in a hotword detector 106 (also referred to as a hotworder) of theuser device 102 that is configured to detect the presence of a hotword in streaming audio without performing semantic analysis or speech recognition processing on thestreaming audio 118. Optionally, the trained memorizedneural network 300 may additionally or alternatively reside in an automatic speech recognizer (ASR) 108 of theuser device 102 and/or theremote system 110 to confirm that thehotword detector 106 correctly detected the presence of a hotword in streamingaudio 118. - In some implementations, the
data processing hardware 112 trains the memorizedneural network 300 usingtraining samples 400 obtained from annotated utterance pools 130. The training samples may include afirst label 420, 420 a and asecond label 420, 420 b. That is, each training sample may be annotated with twoseparate labels memory hardware 114 and/or some other remote memory location(s). In the example shown, when theuser 10 speaks anutterance 120 including a hotword (e.g., “Hey Google”) captured as streamingaudio 118 by theuser device 102, the memorizedneural network 300 executing on theuser device 102 is configured to detect the presence of the hotword in theutterance 120 to initiate a wake-up process on theuser device 102 for processing the hotword and/or one or more other terms (e.g., query or command) following the hotword in theutterance 120. In additional implementations, theuser device 102 sends theutterance 120 to theremote system 110 for additional processing or verification (e.g., with another, potentially more computationally-intensive memorized neural network 300). - In the example shown, the memorized
neural network 300 includes anencoder portion 310 and adecoder portion 311 each including a layered topology of single value decomposition filter (SVDF) layers 302. The SVDF layers 302 provide the memory for theneural network 300 by providing eachSVDF layer 302 with a memory capacity such that the memory capacities of all of the SVDF layers 302 additively make-up the total fixed memory for theneural network 300 to remember only a fixed length of time in thestreaming audio 118 necessary to capture audio features 410 (FIGS. 4A and 4B ) that characterize the hotword. This memorizedneural network 300 architecture is exemplary, and it is understood than any memorizedneural network 300 architecture may be substituted. - In some implementations, the memorized
neural network 300 is trained using themultiple labels 420, 420 a—b to generate arespective loss corresponding label 420 a—b. The process of trainingneural network 300 with multiple labels 420 is described in greater detail below (FIG. 7 ). - Referring now to
FIG. 2 , a typical hotword detector uses a neural networkacoustic encoder 200 without memory. Because thenetwork 200 lacks memory, eachneuron 212 of theacoustic encoder 200 must accept, as an input, every audio feature of everyframe utterance 120 simultaneously. Note that eachframe 210 can have any number of audio features, each of which theneuron 212 accepts as an input. Such a configuration requires a neural networkacoustic encoder 200 of substantial size that increases dramatically as the fixed length of time increases and/or the number of audio features increases. The output of theacoustic encoder 200 results in a probability of each, for example, phoneme of the hotword that has been detected. Theacoustic encoder 200 must then rely on a hand-coded decoder to process the outputs of the acoustic encoder 200 (e.g., stitch together the phonemes) in order to generate a score (i.e., an estimation) indicating a presences of the hotword. - Referring now to
FIGS. 3A and 3B , in some implementations, a single value decomposition filter (SVDF) neural network 300 (also referred to as a memorized neural network) has any number of neurons/nodes 312, where eachneuron 312 accepts only asingle frame utterance 120 at a time. That is, if eachframe 210, for example, constitutes 30 ms of audio data, arespective frame 210 is input to theneuron 312 approximately every 30 ms (i.e.,Time 1,Time 2,Time 3,Time 4, etc.).FIG. 3A shows eachneuron 312 including a two-stage filtering mechanism: a first stage 320 (i.e.,Stage 1 Feature Filter) that performs filtering on a features dimension of the input and a second stage 340 (i.e.,Stage 2 Time Filter) that performs filtering on a time dimension on the outputs of thefirst stage 320. Therefore, thestage 1feature filter 320 performs feature filtering on only thecurrent frame 210. The result of the processing is then placed in amemory component 330. In these examples, the size of thememory component 330 is configurable per node or per layer level. After thestage 1feature filter 320 processes a given frame 210 (e.g., by filtering audio features within the frame), the filtered result is placed in a nextavailable memory location memory component 330. Once allmemory locations 332 are filled, thestage 1feature filter 320 will overwrite thememory location 332 storing the oldest filtered data in thememory component 330. Note that, for illustrative purposes,FIG. 3A shows amemory component 330 of size four (fourmemory locations 332 a—d) and fourframes 210 a—d, but due to the nature of hotword detection, thesystem 100 will typically monitor streamingaudio 118 continuously such that eachneuron 312 will “slide” along or process frames 210 akin to a pipeline. Put another way, if each stage includes N feature filters 320 and N time filters 340 (each matching the size of the input feature frame 210), the layer is analogous to computing N x T (T equaling the number offrames 210 in a fixed period of time) convolutions of the feature filters by sliding each of the N filters 320, 340 on the input feature frames 210, with a stride the size of the feature frames. For example, since the example shows thememory component 330 at capacity after thestage 1 feature filter outputs the filtered audio features associated with Frame 4 (F4) 210 d (during Time 4), thestage 1feature filter 320 would place filtered audio features associated with following Frame 5 (F5) (during a Time 5) intomemory 330 by overwriting the filtered audio features associated with Frame 1 (F1) 210 a withinmemory location 332 a. In this way, thestage 2time filter 340 applies filtering to the previous T−1 (T again equaling the number offrames 210 in a fixed period of time) filtered audio features output from thestage 1feature filter 320. - The
stage 2time filter 340 then filters each filtered audio feature stored inmemory 330. For example,FIG. 3A shows thestage 2time filter 340 filtering the audio features in each of the fourmemory locations 332 every time thestage 1feature filter 320 stores a new filtered audio feature intomemory 330. In this way, thestage 2time filter 340 is always filtering a number ofpast frames 210, where the number is proportional to the size of thememory 330. Eachneuron 312 is part of asingle SVDF layer 302, and theneural network 300 may include any number oflayers 302. The output of eachstage 2time filter 340 is passed to an input of aneuron 312 in thenext layer 302. The number oflayers 302 and the number ofneurons 312 perlayer 302 is fully configurable and is dependent upon available resources and desired size, power, and accuracy. This disclosure is not limited to the number of SVDF layers 302 nor the number ofneurons 312 in eachSVDF layer 302. - Referring now to
FIG. 3B , eachSVDF layer neural network 300, in some implementations, is connected such that the outputs of the previous layer are accepted as inputs to thecorresponding layer 302. In some examples, thefinal layer 302 n outputs aprobability score 350 indicating the probability that theutterance 120 includes the hotword. - In an
SVDF network 300 of the illustrated example, the layer design derives from the concept that a densely connectedlayer 302 that is processing a sequence of input frames 210 can be approximated by using a singular value decomposition of each of itsnodes 312. The approximation is configurable. For example, a rank R approximation signifies extending a new dimension R for the layer's filters:stage 1 occurs independently, and instage 2, the outputs of all ranks get added up prior to passing through the non-linearity. In other words, an SVDF decomposition of thenodes 312 of a densely connected layer of matching dimensions can be used to initialize anSVDF layer 302, which provides a principled initialization and increases the quality of the layer's generalization. In essence, the “power” of a larger densely connected layer is transferred into a potentially (depending on the rank) much smaller SVDF. Note, however, theSVDF layer 302 does not need the initialization to outperform a densely connected or even convolutional layer with the same or even more operations. - In some implementations, the
system 100 includes a stateful, stackableneural network 300 where eachneuron 312 of eachSVDF layer 302 includes afirst stage 320, associated with filtering audio features, and asecond stage 340, associated with filtering outputs of thefirst stage 320 with respect to time. Specifically, thefirst stage 320 is configured to perform filtering on one or more audio features on one audiofeature input frame 210 at a time and output the filtered audio features to therespective memory component 330. Here, thestage 1feature filter 320 receives one or more audio features associated with atime frame 210 as input for processing and outputs the processed audio features into therespective memory component 330 of theSVDF layer 302. Thereafter, thesecond stage 340 is configured to perform filtering on all the filtered audio features output from thefirst stage 320 and residing in therespective memory component 330. For instance, when therespective memory component 330 is equal to eight (8), thesecond stage 340 would pull up to the last eight (8) filtered audio features residing in thememory component 330 that were output from thefirst stage 320 during individual filtering of the audio features within a sequence of eight (8) input frames 210. As thefirst stage 320 fills thecorresponding memory component 330 to capacity, thememory locations 332 containing the oldest filtered audio features are overwritten (i.e., first in, first out). Thus, depending on the capacity of thememory component 330 at theSVDF neuron 312 orlayer 302, thesecond stage 340 is capable of remembering a number of past outputs processed by thefirst stage 320 of thecorresponding SVDF layer 302. Moreover, since thememory components 330 at the SVDF layers 302 are additive, thememory component 330 at eachSVDF neuron 312 andlayer 302 also includes the memory of each precedingSVDF neuron 312 andlayer 302, thus extending the overall receptive field of the memorizedneural network 300. For instance, in aneural network 300 topology with fourSVDF layers 302, each having asingle neuron 312 with amemory component 330 equal to eight (8), thelast SVDF layer 302 will include a sequence of up to the last thirty-two (32) audio feature input frames 210 individually filtered by theneural network 300. Note, however, the amount of memory is configurable perlayer 302 or even pernode 312. For example, thefirst layer 302 a may be allotted thirty-two (32)locations 332, while thelast layer 302 may be configured with eight (8)locations 332. As a result, the stacked SVDF layers 302 allow theneural network 300 to process only the audio features for one input time frame 210 (e.g., 30 milliseconds of audio data) at a time and incorporate a number of filtered audio features into the past that capture the fixed length of time necessary to capture the designated hotword in thestreaming audio 118. By contrast, aneural network 200 without memory (as shown inFIG. 2 ) would require itsneurons 212 to process all of the audio feature frames covering the fixed length of time (e.g., 2 seconds of audio data) at once in order to determine the probability of the streaming audio including the presence of the hotword, which drastically increases the overall size of the network. Moreover, while recurrent neural networks (RNNs) using long short-term memory (LSTM) provide memory, RNN-LSTMs cause the neurons to continuously update their state after each processing instance, in effect having an infinite memory, and thereby prevent the ability to remember a finite past number of processed outputs where each new output re-writes over a previous output (once the fixed-sized memory is at capacity). Put another way, SVDF networks do not recur the outputs into the state (memory), nor rewrite all the state with each iteration; instead, the memory keeps each inference run's state isolated from subsequent runs, instead pushing and popping in new entries based on the memory size configured for the layer. - Referring now to
FIGS. 4A and 4B , in some implementations, the memorizedneural network 300 is trained on a plurality of training input audio sequences 400 (i.e., training samples) that each include a sequence of input frames 210, 210 a—n and two ormore labels 420 a— b assigned to the input frames 210. Eachinput frame 210 includes one or more respective audio features 410 characterizingphonetic components 430 of a hotword, and each label 420 indicates a probability that the one or more audio features 410 of arespective input frame 210 include aphonetic component 430 of the hotword. In some examples, the audio features 410 for eachinput frame 210 are converted from rawaudio signals 402 of anaudio stream 118 during apre-processing stage 404. The audio features 410 may include one or more log-filterbanks. Thus, the pre-processing stage may segment the audio stream 118 (or spoken utterance 120) into the sequence of input frames 210 (e.g., 30 ms each), and generate separate log-filterbanks for eachframe 210. For example, eachframe 210 may be represented by forty log-filterbanks. Moreover, eachsuccessive SVDF layer 302 receives, as input, the filtered audio features 410 with respect to time that are output from the immediately precedingSVDF layer 302. - In the example shown, each training
input audio sequence 400 is associated with a training sample that includes an annotated (i.e., withlabels 420 a—b) utterance containing a designated hotword occurring within a fixed length of time (e.g., two seconds). The memorizedneural network 300 may also optionally be trained on annotatedutterances 400 that do not include the designated hotword, or include the designated hotword but spanning a time longer than the fixed length of time, and thus, would not be falsely detected due to the fixed memory forgetting data outside the fixed length of time. In some examples, the fixed length of time corresponds to an amount of time that a typical speaker would take to speak the designated hotword to summon auser device 102 for processing spoken queries and/or voice commands. For instance, if the designated hotword includes the phrase “Hey Google” or “Ok Google”, a fixed length of time set equal to two seconds is likely sufficient since even a slow speaker would generally not take more than two seconds to speak the designated phrase. Accordingly, since it is only important to detect the occurrence of the designated hotword within streamingaudio 118 during the fixed length of time, theneural network 300 includes an amount of fixed memory that is proportional to the amount of audio to span the fixed time (e.g., two seconds). Thus, the fixed memory of theneural network 300 allowsneurons 312 of the neural network to filter audio features 410 (e.g., log-filterbanks) from one input frame 210 (e.g., 30 ms time window) of thestreaming audio 118 at a time, while storing the most recent filtered audio features 410 spanning the fixed length of time and removing or deleting any filtered audio features 410 outside the fixed length of time from a current filtering iteration. Thus, if theneural network 300 has, for example, a memory depth of thirty-two (32), the first thirty-two (32) frames processed by theneural network 300 will fill thememory component 330 to capacity, and for each new output after the first 32, theneural network 300 will remove the oldest processed audio feature from thecorresponding memory location 332 of thememory component 330. - Referring to
FIG. 4A , for end-to-end training, training input audio sequence 400 a includeslabels 420 a that may be applied to eachinput frame 210. In some examples, when a training sample 400 a contains the hotword, atarget label 420 a associated with a target score (e.g., ‘1’) is applied to one or more input frames 210 that containaudio features 410 characterizingphonetic components 430 at or near the end of the hotword. For example, if thephonetic components 430 of the hotword “OK Google” are broken into: “ou”, ‘k’, “eI”, “<silence>”, ‘g’, ‘u’, ‘g’, ‘@’, ‘1’, then target labels of the number ‘1’ are applied to all input frames 210 that correspond to the letter ‘1’ (i.e. thelast component 430 of the hotword), which are part of the required sequence ofphonetic components 430 of the hotword. In this scenario, all other input frames 210 (not associated with the last phonetic component 430) are assigned a different label (e.g., ‘0’). Thus, eachinput frame 210 includes a corresponding input feature-label pair input frame 210. - The
exemplary label 420 a focuses on the position of the last phoneme of the hotword and does not rely on positional information of other sub-phonemes (hence the label “0” for phonetic components that are not “1”). Typically this type oflabel 420 a is associated with a max pooling loss, which does not depend on the exact location of the target pattern, and instead looks to define an existence of a pattern in a defined interval. Thelabels 420 a are generated from the annotated utterances 400 a, where eachinput feature tensor 410 is assigned a phonetic class via a force-alignment step (i.e., a label of ‘1’ is given to pairs corresponding to the last class belonging to the hotword, and ‘0’ to all the rest). Thus, the training input audio sequence 400 a includes binary labels assigned to the sequence of input frames. The annotated utterances 400 a, or training input audio sequence 400 a, correspond to thetraining samples 400 obtained from the annotated utterance pools 130 ofFIG. 1 . - In another example,
FIG. 4B includes a training input audio sequence 400 b that includeslabels 420 b associated with scores that increase along the sequence of input frames 210 as the number ofaudio features 410 characterizing (matching)phonetic components 430 of the hotword progresses. For instance, when the hotword includes “Ok Google”, the input frames 210 that include respective audio features 410 that characterize the first phonetic components, ‘o’ and ‘k’, have assignedlabels 420 b of ‘1’, while the input frames 210 that include respective audio features 410 characterizing the final phonetic component of ‘1’ have assignedlabels 420 b of ‘5’. The input frames 210 including respective audio features 410 characterizing the middlephonetic components 430 have assignedlabels 420 b of ‘2’, ‘3’, and ‘4’. - In additional implementations, the number of
positive labels 420 b increases. For example, a fixed amount of ‘1’labels 420 b is generated, starting from thefirst frame 210 including audio features 410 characterizing to the finalphonetic component 430 of the hotword. In this implementation, when the configured number ofpositive labels 420 b (e.g., ‘1’) is large, apositive label 420 b may be applied toframes 210 that otherwise would have been applied anon-positive label 420 b (e.g., ‘0’). In other examples, the start position of thepositive label 420 b is modified. For example, thelabel 420 b may be shifted to start at either a start, mid-point, or end of a segment offrames 210 containing the final keywordphonetic component 430. Still yet in other examples, a weight loss is associated with the input sequence. For example, weight loss data is added to the input sequence that allows the training procedure to reduce the loss (i.e. error gradient) caused by small mis-alignment. Specifically, with frame-based loss functions, a loss can be caused from either mis-classification or mis-alignment. To reduce the loss, theneural network 300 predicts both thecorrect label 420 b and correct position (timing) of thelabel 420 b. Even if thenetwork 300 detected the keyword at some point, the result can be considered an error if it's not perfectly aligned with the giventarget label 420 b. Thus, weighing the loss is particularly useful forframes 210 with high likelihood of mis-alignment during the force-alignment stage. Theexemplary labels 420 b are typically associated with a cross entropy loss, which results in a model that is highly sensitive to positional alignments of all sub-phonemes of the keyword. - As a result of training using either of the training input audio sequences 400 a, 400 b of
FIGS. 4A and 4B , theneural network 300 is optimized (using a determined loss) to generateoutputs 350 indicating whether the hotword(s) are present in thestreaming audio 118. In some examples, thenetwork 300 is trained in two stages. Referring now toFIG. 5A , schematic view 500 a shows an encoder portion (or simply ‘encoder’) 310 a of theneural network 300 that includes, for example, eight layers, that are trained individually to produce acoustic posterior probabilities. In addition to the SVDF layers, thenetwork 300 may, for example, include bottleneck, softmax, and/or other layers. For training theencoder 310 a, label generation assigns distinct classes to all the phonetic components of the hotword (plus silence and “epsilon” targets for all that is not the hotword). Then, the decoder portion (or simply ‘decoder’) 311 a of theneural network 300 is trained by creating a topology where the first part (i.e. the layers and connections) matches that of theencoder 310 a, and a selected checkpoint from thatencoder 310 a of theneural network 300 is used to initialize it. The training is specified to “freeze” (i.e. not update) the parameters of theencoder 310 a, thus tuning just the decoder 311 a portion of the topology. This naturally produces a single spotter neural network, even though it is the product of two staggered training pipelines. Training with this method is particularly useful on models that tend to present overfitting to parts of the training set. - Alternatively, the
neural network 300 is trained end-to-end from the start. For example, theneural network 300 accepts features directly (similarly to theencoder 310 a training described previously), but instead uses thebinary target label 420 a (i.e., ‘0’ or ‘1’) outputs for use in training the decoder 311 a. Such an end-to-endneural network 300 may use any topology. For example, as shown inFIG. 5B , schematic view 500 b shows aneural network 300 topology of anencoder 310 b and adecoder 311 b that is similar to the topology ofFIG. 5A except that theencoder 310 b does not include the intermediate softmax layer. As with the topology ofFIG. 5A , the topology ofFIG. 5B may use a pre-trained encoder checkpoint with an adaptation rate to tune how thedecoder 311 b part is adjusted (e.g. if the adaptation rate is set to 0, it is equivalent to theFIG. 5A topology). This end-to-end pipeline, where the entirety of the topology's parameters are adjusted, tends to outperform the separately trainedencoder 310 a and decoder 311 a ofFIG. 5A , particularly in smaller sized models which do not tend to over fit. - Thus,
neural network 300 may avoid the use of a manually tuned decoder. Manual tuning the decoder increases the difficulty in changing or adding hotwords. The single memorizedneural network 300 can be trained to detect multiple different hotwords, as well as the same hotword across two or more locales. Further, detection quality reduces compared to a network optimized specifically for hotword detection trained with potentially millions of examples. Further, typical manually tuned decoders are more complicated than a single neural network that performs both encoding and decoding. Traditional systems tend to be over parameterized, consuming significantly more memory and computation than a comparable end-to-end model and they are unable to leverage as much neural network acceleration hardware. Additionally, a manual tuned decoder suffers from accented utterances, and makes it extremely difficult to create detectors that can work across multiple locales and/or languages. - The memorized
neural network 300 outperforms simple fully-connected layers of the same size, but also benefits from optionally initializing parameters from a pre-trained fully connected layer. Thenetwork 300 allows fine grained control over how much to remember from the past. This results in outperforming RNN-LSTMs for certain tasks that do not benefit (and actually are hurt) from paying attention to theoretically infinite past (e.g. continuously listening to streaming audio). However,network 300 can work in tandem with RNN-LSTMs, typically leveraging SVDF for the lower layers, filtering the noisy low-level feature past, and LSTM for the higher layers. The number of parameters and computation are finely controlled, given that several relatively small filters comprise the SVDF. This is useful when selecting a tradeoff between quality and size/computation. Moreover, because of this quality,network 300 allows creating very small networks that outperform other topologies like simple convolutional neural networks (CNNs) which operate at a larger granularity. - Referring to
FIGS. 5C and 6 , in some configurations, instead of optimizing theneural network 300 to generate theprobability score 350 indicating a likelihood the hotword(s) are present in thestreaming audio 118 using cross entropy loss, theneural network 300 is optimized using a smoothed max pooling loss. Here, similar to the examples shown inFIGS. 5A and 5B , this approach includes jointly training anencoder decoder neural network 300 may be trained to detect not only parts of a hotword (e.g., with theencoder 310 c), but also an entire hotword (e.g., with thedecoder 311 c). By using a smoothed max pooling loss approach, this approach does not depend onframe labels 420 a—b and may lend itself to implementations such as on-device learning (e.g., for user devices 102). - In hotword detection, the exact positon of the hotword is generally not as important as the actual presence of the hotword. Therefore, the alignment of frame labels 420 may cause hotword detection errors (i.e., potentially compromising hotword detection). This alignment may be particularly problematic when frame labels 420 have inherent uncertainty caused by noise or a particular speech accent. With frame labels 420, a training
input audio sequence 400 often includes intervals of repeated similar or identical frame labels 420 called runs. For instance, bothFIGS. 4A and 4B include runs of “0.” These runs, when training thenetwork 300, indicate that thenetwork 300 should make a strong learning association for the generation ofoutputs 350. In contrast, a smoothed max pooling approach (e.g., as shown inFIGS. 5C and 6 ) avoids specifying an exact activation position (i.e., specifying timing) using frame labels 420. - For a smoothed max pooling loss approach, in some examples, an initial loss is defined for both the
encoder 310 c and thedecoder 311 c and then the initial loss of each theencoder 310 c and thedecoder 311 c is optimized simultaneously. Max pooling refers to a sample-based discretization process where some input is reduced in dimensionality by applying a max filter. In some examples, a training process 500 c using the smoothed max pooling approach includes a smoothing operation 510, 510 e—d and a max pooling operation 520, 520 e—d. In these examples, the smoothing operation 510 occurs before the max pooling operation 520. Here, during the smoothing operation 510, the training process 500 c performs a temporal smoothing on theframes 210. For instance, the training process 500 c smooths logits 502, 502 e—d corresponding to theframes 210. A logit generally refers to a vector or other raw predictive form that is output from the one or more SVDF layers 302. The logit 502 serves as an input into the softmax portion of anencoder 310 and/or adecoder 311 such that theencoder 310 and/or thedecoder 311 generates an output probability based on the input of one or more logits 502. For instance, the logit 502 is a non-normalized predictive data form and the softmax normalizes the logit 502 into a probability (e.g., a probability of a hotword). - By having a smoothing operation 510 prior to a max pooling operation 520, the training process 500 c trains the
network 300 with greater stability for small variation and temporal shifts within thestreaming audio 118. This greater stability is in contrast to other training approaches that may use some form of a max pooling operation without a temporal smoothing operation. For instance, other training approaches may use max pooling in a time domain and determine cross entropy loss with respect to a logit 502 of aframe 210 with maximum activation. By introducing the temporal smoothing operation 510 before the max pooling operation 520, the training process 500 c of thenetwork 300 may result in smooth activation and stable peak values. - During the max pooling operation 520, the training process 500 c determines a smoothed max pooling loss where the loss represents a difference between what the
network 300 thinks that the output distribution should theoretically be and what the output distribution actually is. Here, the smoothed max pooling loss may be determined by the following equations: -
- where Xt is a spectral feature of d-dimension, yi (Xt, W) stands for an i-th dimension of the neural network's softmax output, W is the network weight, ct is a frame label 420 at frame t (e.g., a frame 210), s(t) is a smoothing filter, ß is a convolution over time, and [τi start, τi end] defines a start and an end time of an interval of the i-th max pooling window.
- With continued reference to
FIG. 5C , both theencoder 310 c and thedecoder 311 c undergo the training process 500 c that uses the smoothed max pooling approach. For instance,FIG. 5C illustrates theencoder 310 c including a smoothing operation 510, 510 e and a max pooling operation 520, 520 e. During the max pooling operation 520 e of the training 500 c, theencoder 310 c learns a sequence of sound-parts (e.g., phonetic components of audio features 410) that define the hotword. Here, this learning may occur in a semi-supervised manner. In some examples, the max pooling operation 510 e during training 500 c occurs by dividing a fixed-length hotword (e.g., an expected length of a hotword or an average length of the hotword) into max-pooling windows 310 w, 310 w 1-n. - For instance,
FIG. 6 depicts n-sequential windows 310 w over an expected hotword location. The max pooling operation 510 e then determines a max pooling loss at each window 310 w. In some implementations, the max pooling loss at each window 310 w is defined by the following equations: -
τi e_start=ωend+offsete−winsize e *i, i∈[1, . . . , n] (6) -
τi e_end=τi e_start+winsize e , i∈[1, . . . , n] (7) - where “e” corresponds to a variable of the
encoder 310 c, ω end corresponds to an endpoint for the hotword, and offset refers to a time offset for a window 310 w. - In some examples, the number of windows 310 w and/or the size 310 w s of each window 310 w are tunable parameters during the training process 500 c. These parameters may be tuned such that the number of windows 310 w “n” approximates the number of distinguishable sound-parts (e.g., phonemes) and/or the size 310 w s of the windows 310 w multiplied by “n” number of windows 310 w approximately matches the fixed-length of the hotword. In addition to the number of windows 310 w and the size 310 w s of each window 310 w being tunable, a variable referred to as an encoder offset Offsete that offsets the sequence of windows 310 w from an endpoint Wend of the hotword may also be tunable during the training 500 c of the
encoder 310 c. - Similar to the
encoder 310 c, in the training process 500 c, thedecoder 311 c includes a smoothing operation 510, 510 d and a max pooling operation 520, 520 d. Generally speaking, the training process 500 c trains thedecoder 311 c to generate strong activation (i.e., a high probability of detection for a hotword) for input frames 210 that containaudio features 410 at or near the end of the hotword. Due to the nature of max pooling loss, max pooling loss values are not sensitive to an exact value for the endpoint Wend of the hotword as long as adecoder window 311 w includes the actual endpoint Wend of the hotword. During the max pooling operation 520 d for thedecoder 311 c, the training process 500 c determines the max pooling loss for awindow 311 w containing the endpoint Wend of the hotword according to the following equations: -
τi d_start=ωend+offsetd (8) -
τi d_end=τi d_start+winsize d (9) - where offsetd and winsize d may be tunable parameters to include the expected endpoint Wend of the hotword.
- With continued reference to
FIG. 6 , thedecoder window 311 w is shown as an interval extending from τ1 d_start to τ1 d_end. When the interval is large enough to include the actual endpoint Wend of the hotword, the smoothed max pooling loss approach allows thenetwork 300 to learn an optimal position of strongest activation (e.g., in a semi-supervised manner). In some examples, the training process 500 c derives the endpoint Wend of the hotword based on word-level alignment. In some implementations, the endpoint Wend of the hotword is determined based on the output of theencoder 310. - In contrast to some end-to-
end networks 300 with joint training where anencoder 310 may be trained first and then adecoder 311 may be trained while model weights of theencoder 310 are frozen, the smoothed max pooling approach jointly trains theencoder 310 c anddecoder 311 c simultaneously without such freezing. Since theencoder 310 c and thedecoder 311 c are jointly trained during the training process 500 c using smoothed max pooling loss, the relative importance of each loss may be controlled by a tunable parameter, a. For instance, the total loss referring to the loss at theencoder 310 c and the loss at thedecoder 311 c have a relationship as described by the following equation: -
Total Loss=α*Losse+Lossd (10) - Referring now to
FIG. 7 , atraining process 700 for a memorizedneural network 300 includes using afirst label 420 a (e.g., a cross entropy label 420) and asecond label 420 b (e.g., a max pooling loss label) and a first loss function 705, 705 a and a second loss function 705, 705 b to generate afirst loss second loss process 700 begins by feeding a memorized neural network 300 a traininginput audio sequence 400. Here, the data of the traininginput audio sequence 400 is labeled using bothlabels 420 a—b. For example, a single traininginput audio sequence 400 is labeled using thefirst label 420 a and thesecond label 420 b as described above with respect toFIGS. 4A and 4B . The example labels 420 a, 420 b are for illustrative purposes and are not intended to be limiting as any suitable labeling convention applicable for determining aloss 710 can be used in thetraining process 700. - Upon receiving the training
input audio sequence 400, the memorizedneural network 300 may generate the output 350 (i.e., the probability score 350). The memorizedneural network 300 may process the traininginput audio sequence 400 in the manner described with respect to any ofFIG. 2-6 or any other suitable manner for processing audio data to determine a likelihood a hotword is present in the traininginput audio sequence 400. In some implementations, theoutput 350 is used by each of the two loss functions 705. That is, the first loss function 705 a receives theoutput 350 and thelabel 420 a to determine thefirst loss 710 a. Similarly, the second loss function 705 b receives theoutput 350 and thelabel 420 b to determine thesecond loss 710 b. Notably, thelosses 710 are each determined from thesame output 350 by using twodifferent labels 420 a, 420 bb of the same traininginput audio sequence 400 and two different loss functions 705 a, 705 b. The loss functions 705 may determine thelosses 710 in any manner as described with respect to any ofFIGS. 2-6 . In some examples, the first loss function 705 a is a max pooling loss function and the second loss function 705 b is a cross entropy loss function. In other implementations, a single loss function 705 receives theoutput 350 and labels 420 and generates arespective loss 710 based on each label 420. The loss functions 705 may implement any suitable technique such as regression loss, mean squared error, mean squared logarithmic error, mean absolute error, binary classification, binary cross entropy, hinge loss, multi-class loss, etc. - In some implementations the
losses neural network 300 during thetraining process 700. In other implementations, thelosses joint loss 710, 710 c and the joint loss 710 c is processed by the memorizedneural network 300. In some implementations, the losses are averaged using a weighted averaging formula. For example, thefirst loss 710 a and thesecond loss 710 b may be defined as follows: -
First Loss=L1[f(X,),Y1] (11) -
Second Loss=L2[f(X,),Y2] (12) - Here, X is the
output 350, L1 is the first loss function 705 a, Y/is thelabel 420 a, L2 is the second loss function 705 b, Y2 is thelabel 420 b. In these examples, the joint loss 710 c is represented by: -
Joint Loss=alpha*L1[f(X,theta), Y2]+beta*L2[f(X,theta), Y2] (13) - Here, alpha and beta are scalar hyper-parameters. The
first loss 710 a and thesecond loss 710 b may be combined in any other manner (e.g., added, multiplied, etc.). - Examples herein illustrate training a
neural network 300 with traininginput audio sequences 400 annotated with the twolabels 420 a,b. The first loss function 705 a uses theoutput 350 and thelabel 420 a to generate thefirst loss 710 a. The second loss function 705 b uses theoutput 350 and thelabel 420 b to generate thesecond loss 710 b. The neural network is trained, updated, or fine-tuned using both thefirst loss 710 a and thesecond loss 710 b. It is understood that these examples are non-limiting and any number of labels 420 and any number of respective loss function 705 may generate any number of losses to train any appropriateneural network 300. -
FIG. 8 is a flowchart of an example arrangement of operations for amethod 800 of training aneural network 300 using multiple labels 420 and multiple loss functions 705. Atoperation 802, themethod 800 includes receiving a traininginput audio sequence 400 including a sequence of input frames. Here, the sequence of input frames defines a hotword that initiates a wake-up process on auser device 102. Atoperation 804, themethod 800 includes obtaining afirst label 420 a (e.g., a max pooling label) and asecond label 420 b (e.g., a cross entropy label) for the traininginput audio sequence 400. Atoperation 806, themethod 800 includes generating, using a memorizedneural network 300 and the traininginput audio sequence 400, anoutput 350 indicating a likelihood the traininginput audio sequence 400 includes the hotword. Atoperation 808, themethod 800 includes determining afirst loss 710 a (e.g., a max pooling loss) based on thefirst label 420 a and theoutput 350. Atoperation 810, themethod 800 includes determining asecond loss 710 b (e.g., a cross entropy loss) based on thesecond label 420 b and theoutput 350. At operation 814, themethod 800 includes optimizing the memorizedneural network 300 based on thefirst loss 710 a and thesecond loss 710 b associated with the traininginput audio sequence 400. - As used herein, a software application (i.e., a software resource) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an “application,” an “app,” or a “program.” Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- The non-transitory memory may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
-
FIG. 9 is schematic view of anexample computing device 900 that may be used to implement the systems and methods described in this document. Thecomputing device 900 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 900 includes aprocessor 910,memory 920, astorage device 930, a high-speed interface/controller 940 connecting to thememory 920 and high-speed expansion ports 950, and a low speed interface/controller 960 connecting to a low speed bus 970 and astorage device 930. Each of thecomponents processor 910 can process instructions for execution within thecomputing device 900, including instructions stored in thememory 920 or on thestorage device 930 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 970 coupled tohigh speed interface 940. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 900 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 920 stores information non-transitorily within thecomputing device 900. Thememory 920 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). Thenon-transitory memory 920 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by thecomputing device 900. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes. - The
storage device 930 is capable of providing mass storage for thecomputing device 900. In some implementations, thestorage device 930 is a computer-readable medium. In various different implementations, thestorage device 930 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 920, thestorage device 920, or memory onprocessor 910. - The
high speed controller 940 manages bandwidth-intensive operations for thecomputing device 900, while thelow speed controller 960 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 940 is coupled to thememory 920, the display 980 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 950, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 960 is coupled to thestorage device 930 and a low-speed expansion port 990. The low-speed expansion port 990, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 900 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 900 a or multiple times in a group ofsuch servers 900 a, as alaptop computer 900 b, or as part of arack server system 900 c. - Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” and “computer-readable medium” refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (22)
1. A computer-implemented method when executed by data processing hardware causes the data processing hardware to perform operations comprising:
receiving a training input audio sequence comprising a sequence of input frames, the sequence of input frames defining a hotword that initiates a wake-up process on a user device;
obtaining a first label for the training input audio sequence;
obtaining a second label for the training input audio sequence, the second label different than the first label;
generating, using a memorized neural network and the training input audio sequence, an output indicating a likelihood the training input audio sequence includes the hotword;
determining a first loss based on the first label and the output;
determining a second loss based on the second label and the output; and
optimizing the memorized neural network based on the first loss and the second loss associated with the training input audio sequence.
2. The method of claim 1 , wherein the memorized neural network comprises an encoder and a decoder, each of the encoder and the decoder of the memorized neural network comprising sequentially stacked single value decomposition filter (SVDF) layers.
3. The method of claim 1 , wherein the output is based on a probability distribution of a logit based on the training input audio sequence.
4. The method of claim 3 , wherein the operations further comprise smoothing the logit prior to determining the first loss.
5. The method of claim 1 , wherein determining the first loss comprises:
generating a plurality of encoder windows, each encoder window of the plurality of encoder windows associated with one or more phonemes of the hotword; and
determining the first loss for each encoder window of the plurality of encoder windows.
6. The method of claim 5 , wherein a collective size of the plurality of encoder windows corresponds to an average acoustic length of the hotword.
7. The method of claim 1 , wherein optimizing the memorized neural network comprises generating a weighted average of the first loss and the second loss.
8. The method of claim 1 , wherein the second label is derived from one or more phoneme sequences of the hotword.
9. The method of claim 1 , wherein the first label is based on a position of a last phoneme of the hotword.
10. The method of claim 1 , wherein the sequence of input frames each comprise one or more respective audio features characterizing phonetic components of the hotword.
11. The method of claim 1 , wherein:
the first label comprises a max pooling loss label;
the second label comprises a cross entropy label;
the first loss comprises a max pooling loss; and
the second loss comprises a cross entropy loss.
12. A system comprising:
data processing hardware; and
memory hardware in communication with the data processing hardware, the memory hardware storing instructions that when executed on the data processing hardware cause the data processing hardware to perform operations comprising:
receiving a training input audio sequence comprising a sequence of input frames, the sequence of input frames defining a hotword that initiates a wake-up process on a user device;
obtaining a first label for the training input audio sequence;
obtaining a second label for the training input audio sequence, the second label different than the first label;
generating, using a memorized neural network and the training input audio sequence, an output indicating a likelihood the training input audio sequence includes the hotword;
determining a first loss based on the first label and the output;
determining a second loss based on the second label and the output; and
optimizing the memorized neural network based on the first loss and the second loss associated with the training input audio sequence.
13. The system of claim 12 , wherein the memorized neural network comprises an encoder and a decoder, each of the encoder and the decoder of the memorized neural network comprising sequentially stacked single value decomposition filter (SVDF) layers.
14. The system of claim 12 , wherein the output is based on a probability distribution of a logit based on the training input audio sequence.
15. The system of claim 14 , wherein the operations further comprise smoothing the logit prior to determining the first loss.
16. The system of claim 12 , wherein determining the first loss comprises:
generating a plurality of encoder windows, each encoder window of the plurality of encoder windows associated with one or more phonemes of the hotword; and
determining the first loss for each encoder window of the plurality of encoder windows.
17. The system of claim 16 , wherein a collective size of the plurality of encoder windows corresponds to an average acoustic length of the hotword.
18. The system of claim 12 , wherein optimizing the memorized neural network comprises generating a weighted average of the first loss and the second loss.
19. The system of claim 12 , wherein the second label is derived from one or more phoneme sequences of the hotword.
20. The system of claim 12 , wherein the first label is based on a position of a last phoneme of the hotword.
21. The system of claim 12 , wherein the sequence of input frames each comprise one or more respective audio features characterizing phonetic components of the hotword.
22. The system of claim 12 , wherein:
the first label comprises a max pooling loss label;
the second label comprises a cross entropy label;
the first loss comprises a max pooling loss; and
the second loss comprises a cross entropy loss.
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/652,801 US20230274731A1 (en) | 2022-02-28 | 2022-02-28 | Mixing Heterogeneous Loss Types to Improve Accuracy of Keyword Spotting |
PCT/US2023/062518 WO2023164380A1 (en) | 2022-02-28 | 2023-02-13 | Mixing heterogeneous loss types to improve accuracy of keyword spotting |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/652,801 US20230274731A1 (en) | 2022-02-28 | 2022-02-28 | Mixing Heterogeneous Loss Types to Improve Accuracy of Keyword Spotting |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230274731A1 true US20230274731A1 (en) | 2023-08-31 |
Family
ID=85570223
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/652,801 Pending US20230274731A1 (en) | 2022-02-28 | 2022-02-28 | Mixing Heterogeneous Loss Types to Improve Accuracy of Keyword Spotting |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230274731A1 (en) |
WO (1) | WO2023164380A1 (en) |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10304440B1 (en) * | 2015-07-10 | 2019-05-28 | Amazon Technologies, Inc. | Keyword spotting using multi-task configuration |
WO2020013946A1 (en) * | 2018-07-13 | 2020-01-16 | Google Llc | End-to-end streaming keyword spotting |
-
2022
- 2022-02-28 US US17/652,801 patent/US20230274731A1/en active Pending
-
2023
- 2023-02-13 WO PCT/US2023/062518 patent/WO2023164380A1/en unknown
Also Published As
Publication number | Publication date |
---|---|
WO2023164380A1 (en) | 2023-08-31 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11967310B2 (en) | End-to-end streaming keyword spotting | |
US11556793B2 (en) | Training multiple neural networks with different accuracy | |
US10762891B2 (en) | Binary and multi-class classification systems and methods using connectionist temporal classification | |
US9646634B2 (en) | Low-rank hidden input layer for speech recognition neural network | |
US9715660B2 (en) | Transfer learning for deep neural network based hotword detection | |
US11676625B2 (en) | Unified endpointer using multitask and multidomain learning | |
US10762417B2 (en) | Efficient connectionist temporal classification for binary classification | |
US11100932B2 (en) | Robust start-end point detection algorithm using neural network | |
Hou et al. | Region proposal network based small-footprint keyword spotting | |
US11087213B2 (en) | Binary and multi-class classification systems and methods using one spike connectionist temporal classification | |
US20230274731A1 (en) | Mixing Heterogeneous Loss Types to Improve Accuracy of Keyword Spotting | |
US20230022800A1 (en) | Small Footprint Multi-Channel Keyword Spotting | |
Frikha et al. | Advanced classification approach for neuronal phoneme recognition system based on efficient constructive training algorithm |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:PARK, HYUN JIN;MORENO, IGNACIO LOPEZ;PARK, ALEX SEUNGRYONG;REEL/FRAME:059142/0987Effective date: 20220228 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |