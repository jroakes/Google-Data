CN108156441A - Visual is stablized - Google Patents
Visual is stablized Download PDFInfo
- Publication number
- CN108156441A CN108156441A CN201711275264.8A CN201711275264A CN108156441A CN 108156441 A CN108156441 A CN 108156441A CN 201711275264 A CN201711275264 A CN 201711275264A CN 108156441 A CN108156441 A CN 108156441A
- Authority
- CN
- China
- Prior art keywords
- gaze
- hmd
- plane
- user
- video
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000000007 visual effect Effects 0.000 title description 9
- 230000033001 locomotion Effects 0.000 claims abstract description 89
- 238000000034 method Methods 0.000 claims abstract description 34
- 230000006870 function Effects 0.000 claims description 35
- 238000012545 processing Methods 0.000 claims description 17
- 230000015654 memory Effects 0.000 claims description 16
- 230000000295 complement effect Effects 0.000 abstract description 7
- 210000001508 eye Anatomy 0.000 description 134
- 238000004891 communication Methods 0.000 description 54
- 238000003860 storage Methods 0.000 description 30
- 238000005516 engineering process Methods 0.000 description 22
- 239000011521 glass Substances 0.000 description 19
- 238000013500 data storage Methods 0.000 description 15
- 230000006641 stabilisation Effects 0.000 description 8
- 238000011105 stabilization Methods 0.000 description 8
- 230000008859 change Effects 0.000 description 7
- 210000005252 bulbus oculi Anatomy 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 238000005259 measurement Methods 0.000 description 6
- 238000004458 analytical method Methods 0.000 description 5
- 230000001413 cellular effect Effects 0.000 description 5
- 210000003128 head Anatomy 0.000 description 5
- 230000003068 static effect Effects 0.000 description 5
- 238000009826 distribution Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 210000000162 simple eye Anatomy 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 238000012937 correction Methods 0.000 description 3
- 239000004973 liquid crystal related substance Substances 0.000 description 3
- 238000013507 mapping Methods 0.000 description 3
- 239000000463 material Substances 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 210000004556 brain Anatomy 0.000 description 2
- 238000007906 compression Methods 0.000 description 2
- 230000006835 compression Effects 0.000 description 2
- 238000010276 construction Methods 0.000 description 2
- 125000004122 cyclic group Chemical group 0.000 description 2
- 238000013144 data compression Methods 0.000 description 2
- 230000001815 facial effect Effects 0.000 description 2
- 210000003644 lens cell Anatomy 0.000 description 2
- 230000005389 magnetism Effects 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 230000008569 process Effects 0.000 description 2
- 210000001525 retina Anatomy 0.000 description 2
- 241000264877 Hippospongia communis Species 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000000712 assembly Effects 0.000 description 1
- 238000000429 assembly Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000002490 cerebral effect Effects 0.000 description 1
- 230000001427 coherent effect Effects 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 230000006837 decompression Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000005538 encapsulation Methods 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 230000004424 eye movement Effects 0.000 description 1
- 210000000887 face Anatomy 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 230000005057 finger movement Effects 0.000 description 1
- 230000004927 fusion Effects 0.000 description 1
- 230000004886 head movement Effects 0.000 description 1
- 238000005286 illumination Methods 0.000 description 1
- 239000004615 ingredient Substances 0.000 description 1
- 238000009434 installation Methods 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 230000002045 lasting effect Effects 0.000 description 1
- 230000007787 long-term memory Effects 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000008447 perception Effects 0.000 description 1
- 239000004033 plastic Substances 0.000 description 1
- 229920003023 plastic Polymers 0.000 description 1
- 230000002035 prolonged effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 238000012163 sequencing technique Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B27/0172—Head mounted characterised by optical features
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/0093—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00 with means for monitoring data relating to the user, e.g. head-tracking, eye-tracking
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B27/0176—Head mounted characterised by mechanical features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/013—Eye tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/246—Analysis of motion using feature-based methods, e.g. the tracking of corners or segments
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/204—Image signal generators using stereoscopic image cameras
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/332—Displays for viewing with the aid of special glasses or head-mounted displays [HMD]
- H04N13/344—Displays for viewing with the aid of special glasses or head-mounted displays [HMD] with head-mounted left-right displays
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/366—Image reproducers using viewer tracking
- H04N13/383—Image reproducers using viewer tracking for tracking with gaze detection, i.e. detecting the lines of sight of the viewer's eyes
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6812—Motion detection based on additional sensors, e.g. acceleration sensors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/682—Vibration or motion blur correction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/682—Vibration or motion blur correction
- H04N23/683—Vibration or motion blur correction performed by a processor, e.g. controlling the readout of an image memory
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/0132—Head-up displays characterised by optical features comprising binocular systems
- G02B2027/0134—Head-up displays characterised by optical features comprising binocular systems of stereoscopic type
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/0138—Head-up displays characterised by optical features comprising image capture systems, e.g. camera
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/014—Head-up displays characterised by optical features comprising information/image processing systems
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B2027/0178—Eyeglass type
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0179—Display position adjusting means not related to the information to be displayed
- G02B2027/0187—Display position adjusting means not related to the information to be displayed slaved to motion of at least a part of the body of the user, e.g. head, eye
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
Abstract
Disclose the system and method that a kind of vision for video image is stablized.When the video image in the capture forward field of view of video camera forward using wearable headset equipment (HMD), the eye-tracking device of HMD can be used to obtain the left eye of user of HMD and the binocular fixation direction of right eye.The left eye of user based on HMD and the binocular fixation direction obtained of right eye can determine function of the convergence direction of gaze of user as the time during the time interval of the capture with video image simultaneously.The movement for assembling direction of gaze and the forward video camera of intersection point complementary area of the plane of delineation of video camera forward using user is may then pass through, to stablize the video image of capture.
Description
Technical field
It is calculated the present invention relates to a kind of wearable headset equipment (HMD) and in wearable headset equipment (HMD)
The method that machine is realized.
Background technology
Unless otherwise indicated, otherwise the material described in this part be not the claim in the application existing skill
Art, and not because being recognized as the prior art included in this part.
The electronic access to the data in communication network and service, Yi Jizhi can be used various techniques to provide a user
Hold the communication between user.For example, such as computer, phone and personal digital assistant (personal digital
Assistant, PDA) equipment can be used for exchanging information by including the communication network of internet.Communication network is in turn
It can be provided to communication path and the link of server, which can be via communication equipment by user with trustship (host)
Application, content and the service for accessing or using.Content can include text, video data, audio data and/or other types of
Data.
Invention content
In one aspect, example embodiment presented herein is in wearable headset equipment (head-mountable
Device, HMD) in provide a method of computer implementation, this method includes：Using HMD's and it is attached to the face of HMD
Forward video camera in forward field of view (field of view, FOV) while capture video image, utilizes HMD's
And be attached to HMD obtains the left eye of user of HMD and the three-dimensional video-frequency of right eye towards the video camera of left eye and right eye
Image；The left eye of user based on HMD and the stereoscopic video images obtained of right eye, in the capture with video image simultaneously
During time interval, determine that function of (gaze) direction as the time is watched in the convergence (convergent) of user attentively；And pass through
It is taken the photograph using the convergence direction of gaze of user and the forward video of intersection point complementary area of the plane of delineation of video camera forward
Camera is relative to the movement for assembling direction of gaze, to stablize the video image of capture.
On the other hand, example embodiment presented herein provides one kind in wearable headset equipment (HMD)
Computer implemented method, this method include：Using HMD's and the video camera forward of HMD is attached in forward direction
In visual field (FOV) while capture video image, using HMD's and the eye-tracking device of HMD is attached to obtain HMD's
The left eye of user and the eye gaze direction of at least one of right eye；Based on the eye gaze direction obtained, with video
Function of the direction of gaze of user as the time is determined during the time interval of the capture of image simultaneously；And by using user
Direction of gaze and the plane of delineation of video camera forward the forward video camera of intersection point complementary area relative to note
The movement of apparent direction, to stablize captured video image.
On the other hand, example embodiment presented herein provides wearable headset equipment (HMD), including：
Video camera forward；Eye-tracking device；Processor；And the memory of store instruction, it instructs and is transported by processor
HMD is made to perform operation during row, which includes：It is regarded being captured in forward field of view (FOV) using video camera forward
While frequency image, the left eye of user of HMD and the binocular fixation direction of right eye are obtained using eye-tracking device；Based on being obtained
The left eye of user of HMD obtained and the binocular fixation direction of right eye, during the time interval of the capture with video image simultaneously,
Determine function of the convergence direction of gaze of user as the time；And by using user assemble direction of gaze with forward
The forward video camera of intersection point complementary area of the plane of delineation of video camera comes steady relative to the movement for assembling direction of gaze
Fixed captured video image.
By read it is following be described in detail, appropriate refer to the attached drawing, in terms of these and other, advantage and alternative solution pair
It will become obvious in those of ordinary skill in the art.Moreover, it should be understood that provided herein is invention content and other descriptions
It is only intended to be illustrative embodiment by way of example with attached drawing, in this way, many variations are all possible.For example, it can weigh
New arrangement, distribution, is eliminated or changes structural detail and processing step in other ways, while be maintained at claimed combination
Embodiment in the range of.
Description of the drawings
Figure 1A is the first view of the wearable headset equipment of example according to example embodiment.
Figure 1B is the second view of the wearable headset equipment of example of Figure 1A according to example embodiment.
Fig. 1 C show the wearable headset equipment of another example according to example embodiment.
Fig. 1 D illustrate the wearable headset equipment of another example according to example embodiment.
Fig. 2 is the block diagram of wearable headset equipment according to example embodiment.
Fig. 3 is the simplified block diagram of communication network according to example embodiment.
Fig. 4 A are the block diagrams of computing device according to example embodiment.
Fig. 4 B depict the cluster (cluster) with the computing device of type shown in Fig. 4 A according to example embodiment
Network.
Fig. 5 is the exemplary concept map of the video stabilization moved based on eyes according to example embodiment.
Fig. 6 is the flow chart for the exemplary method for showing the video stabilization moved based on eyes according to example embodiment.
Specific embodiment
1. general view
Headset equipment (HMD) can include video camera forward, be configured in the user of HMD or wear
Capture video image or video data (or only " video ") in the forward field of view (FOV) of wearer.By headset equipment (HMD)
Video potentially unstable that video camera forward captures and shake, particularly if when user moves around.For
The known technology for stablizing shake and/or unstable video image is usually directed to instruction movement of video camera during capture
Subsidiary, the measurement obtained from motion sensor etc..Other technologies can be related to some form of motion analysis modeling
Shake and/or unstable video image are corrected with approximation.However, computationally (computationally) can for these technologies
It can be expensive, and lack the reliability index of the actual motion of video camera during video captures.
Compared with sensor-based measurement of video camera movement or based on the approximation of model, the eye motion of user
Natural basis can be provided with direction of gaze to correct the movement of the video camera of HMD for being fixed to and being dressed by user.It is logical
Often, the eyes of user or eyes are fixed at the certain objects in FOV or point, and brain by eyes against head movement with
Keep the focusing to surrounding objects.Therefore, video capture during user eyes movement and direction of gaze can provide before to
The instruction of one or more of visual field (FOV) point-of-interest, can be used to determine in turn for video camera forward
It machine movement reliable and precisely compensates for so that video stabilization is natural and computationally cheap.
In the exemplary embodiment, it can be determined to use by " collision (collie) " by tracking the information of watching attentively of Extract eyes
Watch the point of convergence in the forward direction FOV at family attentively.Then convergent point can be mapped to the plane of delineation of video camera forward.
For example, mapping can identify the pixel or pixel groups of the convergence direction of gaze corresponding to user.Doing so can be in the convergence of user
Instant coordinate is provided in the video frame of direction of gaze.As the function of time, therefore technology offer is taken the photograph across video forward
The track of pixel in the plane of delineation of camera represents video camera forward relative to the substantially fixed of user
The movement of direction of gaze.Therefore track corresponds to the motion path of compensation video camera movement.Apply the track as opposite
The correction in the instruction direction of forward video camera, can make the video image stabilization captured by video camera forward.
In the exemplary embodiment, wearable HMD can include for capture HMD wearer (user) forward direction FOV
The video camera forward of video image, and can also include for obtain wearer left eye and right eye eyes movement with
And two video cameras towards eyes of direction of gaze.More specifically, a wearer left side can be captured towards the video camera of left eye
The video image of eye and video camera towards right eye can capture the video image of wearer's right eye.Left eye and right eye regard
Frequency capture can be captured with the video of the forward direction FOV by video camera forward and is performed simultaneously.Left video image and the right side
Video image can be combined to create or generate the stereoscopic video images of wearer's eyes.It as described above, will be in three-dimensional video-frequency
The corresponding direction of gaze fusion of left eye and right eye captured in image provides the convergence direction of gaze of wearer, then can incite somebody to action
It is mapped to the plane of delineation of video camera forward for image stabilization.
In the exemplary embodiment, correction or the benefit of the movement for video camera forward can be applied in real time
It repays, because the stereoscopic video images of the eyes of the video image forward and wearer of FOV are captured simultaneously.Additional or
In optional example embodiment, can while Video Image Capturing after time apply for video camera forward
The correction or compensation of the movement of machine.
In the exemplary embodiment, more than technology can be applied in the one eye eyeball for the wearer for only keeping track HMD.This
In the case of, identified direction of gaze may not necessarily correspond to assemble direction of gaze.However, it still can be provided in towards
Track in the plane of delineation of preceding video camera, the movement that can be used for the forward video camera in opposite are entangled
Just.When direction of gaze is towards very remote point (for example, " unlimited focal length "), which is applied to one eye eyeball or two eyes
Difference between eyeball may become insignificant.
2. example system and network
In general, example embodiment in the form of wearable computer or can take wearable computer
Form realize, and will in this way be shown herein by exemplary mode.Specifically, example embodiment can be with wear-type
Equipment (HMD) is associated or the form of headset equipment (HMD) is taken to realize, which communicably connects
It is connected to and the computing systems of data is exchanged from HMD (such as may be a part for network or be connected to the service based on cloud of network
Device).
However, example embodiment in the form of miscellaneous equipment (mobile phone etc.) or can also take other set
The form of standby (mobile phone etc.) is realized.Example embodiment can also include non-transitory computer-readable medium (or
Multiple non-transitory computer-readable mediums) and/or can take non-transitory computer-readable medium (or multiple non-transitories
Computer-readable medium) form, the non-transitory computer-readable medium (or multiple non-transitory computer-readable mediums)
With it is being stored thereon, can be run to provide the program instruction of functions described herein by one or more processors.In addition,
Example embodiment can further include the equipment of such as wearable computer or mobile phone or the subsystem of this equipment
It unites and/or takes the equipment of such as wearable computer or mobile phone or the form of the subsystem of this equipment, this sets
It is standby to include with this non-transitory computer-readable medium for having stored thereon this program instruction.
A. example wearable computing system
According to example embodiment, wearable computing system can include various assemblies, which is included at one or more
Manage device, the memory of one or more forms, one or more sensors equipment, one or more I/O equipment, one or more
Communication equipment and interface and headset equipment (HMD), it is all to be arranged jointly in a manner of making user wearable.Wearable meter
Calculation system can also include being stored in the memory of one or another kind of forms and can by system one or the other
The machine language logic (for example, software, firmware and/or hardware instruction) of device operation is managed, to realize one or more programs, to appoint
Business, using etc..Wearable computing system factor (form factor) can be configured in a variety of manners, the various forms factor
Including but not limited to, be integrated in HMD as unified data packet or it is distributed, have and be integrated in one or more in HMD
A element and the wearable one or more of the other element of separation (for example, as clothes, in garment pocket, as ornaments
Deng).
Although being described above as the component of wearable computing system, it is (or at least table to be sometimes thought of as HMD
Show) wearable computing system is convenient.It therefore, (or " can unless otherwise specified, term " wearable headset equipment "
Wearing HMD ") or only " headset equipment " (or " HMD ") will be used herein to refer to wearable computing system, have integrated
(unified data packet) form, distribution (or part is distributed) form or other wearable forms.
Figure 1A shows to receive, transmit and the exemplary wearable computing system 100 of display data.According to exemplary
Embodiment, wearable computing system 100 are depicted as taking the wearable HMD of the form of glasses 102, in figure 1A facing external
View in show.It will be understood, however, that other types of wearable computing devices can be used additionally or alternatively, wrap
Including only tool, there are one the simple eye display configurations of lens display element.
As shown in Figure 1A, glasses 102 include frame element, lens element 110 and 112 and extension side arm 114 and 116,
The frame element includes lens-mount 104 and 106 and central frame support 108.Central frame support 108 and extension side
Arm 114 and 116 is individually configured the face that glasses 102 are fixed to user for the nose via user and ear.Frame element
104th, 106 and 108 and extension side arm 114 and 116 in each can be formed by the solid construction of plastics or metal or
It can be formed that circuit and component is allowed to interconnect with by 102 internal wiring of glasses by the hollow structure with similar material.Thoroughly
Each material that can include showing image or figure directly or by reflecting surface in mirror element 110 and 112.
In addition, each at least part in lens element 110 and 112 can be it is sufficiently transparent with allow user have an X-rayed lens cells
Part.The two features of lens member can combine；Such as augmented reality or head-up display are provided, wherein the image that projects or
Figure can be superimposed upon on the real world-view that user is perceived by lens element or be perceived with user by lens element
Real world-view be provided together.
It is each the protrusion extended respectively from frame element 104 and 106, and be located in use to extend side arm 114 and 116
Behind the ear at family with by glasses 102 be fixed to user.Extension side arm 114 and 116 can be prolonged by surrounding the rear portion of user's head
It stretches and glasses 102 is further fixed to user.Additionally or alternatively, wearable computing system 100, which may be coupled to, wears
Formula helmet structure is integral with wear-type helmet structure.Other possibilities there is also.
Wearable computing system 100 can also include onboard computing systems 118, video camera 120 forward, sensing
Device 122, finger operable touch pad 124 and communication interface 126.Show that onboard computing systems 118 are located in prolonging for glasses 102
It stretches on side arm 114；However, onboard computing systems 118 may be provided in the other parts of glasses 102.Onboard computing systems
118 can include for example, the memory of one or more processors and one or more forms.Onboard computing systems 118 can be with
It is configured as receiving and analyze from video camera 120, sensor 122, finger operable touch pad 124 and wireless communication interface
The data of 126 (and may be from other sensing equipments and/or user interface) and image is generated for being output to lens cells
Part 110 and 112.
Show that video camera 120 forward is located on the extension side arm 114 of glasses 102；However, regarding forward
Frequency video camera 120 may be provided in the other parts of glasses 102.Video camera 120 forward may be configured to
With various resolution ratio or with different frame per second video image is captured in the forward direction FOV of the wearer of HMD.Miniature video images
Machine, those such as used in cellular phone or web camera can be integrated into the example of wearable system 100.
Although Figure 1A shows a video camera 120 forward, can use more video cameras, and each can by with
It is set to the identical view of capture or captures different views.For example, video camera 120 forward can capture user institute
At least part of the real world view of perception.This image forward captured by video camera 120 forward
Then it can be used for the reality of generation enhancing, the image of Computer generation seems the real world that is perceived with user
View interacts.
For example, sensor 122 can be used for measuring and/or determine position, direction and movable information.While depicted as installation
Single component on the extension side arm 116 of glasses 102, but sensor 122 can essentially include being provided at glasses 102
One or more different pieces on more than one type sensor device or element.
Unrestricted in an illustrative manner, sensor 122 can include one or more motion detectors (for example, one
Or multiple gyroscopes and/or accelerometer), one or more magnetometers and location determining device (for example, GPS device).Top
Spiral shell instrument, accelerometer and magnetometer are desirably integrated into usually said " Inertial Measurement Unit " (inertial measurement
Unit, IMU) in.IMU can be in turn possible be with location information (for example, from GPS device) together from IMU sensor numbers
According to " posture the heading reference system " (attitude for the pointing direction for calculating (for example, using onboard computing systems 118) HMD
Heading reference system, AHRS) a part.Correspondingly, sensor 122 can include the one of either AHRS
Part.Other sensor devices or element can be included in sensor 122 and other sensing functions can be by sensor 122
It performs.
The finger operable touch pad 124 being shown mounted on the extension side arm 114 of glasses 102 can be used by the user to
Input order.However, finger operable touch pad 124 can be positioned in the other parts of glasses 102.Moreover, in glasses
There may be more than one finger operable touch pad on 102.Finger operable touch pad 124 can be used by a user to input
Order.Other than other possibilities, finger operable touch pad 124 can be via capacitance sensing, resistance sensing or surface sound
At least one of the position and movement of wave processing to sense finger.Finger operable touch pad 124 can be parallel to
Finger movement is sensed on the direction of pad surfaces, on the direction perpendicular to pad surfaces or in the two, and can be with
The stress level of application can be sensed.Finger operable touch pad 124 can be by one or more translucent or transparent insulating layers
And one or more translucent or transparent conductive layers are formed.The edge of finger operable touch pad 124 can be formed as having
Protrusion, recess or coarse surface, during so as to reach the edge of finger operable touch pad 124 when the finger of user to
Family provides touch feedback.Although being not shown in figure 1A, it is operable that glasses 102 can include one or more additional fingers
Touch tablet, such as extension side arm 116 is attached to, it can operate to provide duplication independently of finger operable touch pad 124
And/or different function.
Communication interface 126 can include supporting between wearable computing system 100 and remote equipment or communication network
Wired and or wireless communications antenna and transceiver apparatus.For example, communication interface 126 can be supported and 3G and/or 4G honeycombs
Radiotechnics (such as CDMA, EVDO, GSM, UMTS, LTE, WiMAX) and such as bluetooth, Zigbee and WiFi (for example,
802.11A, 802.11B, 802.11g) wireless local or any one of personal area network's technology or whole wireless
Communication.Other types of wireless access technology can also be supported.Communication interface 126 can enable wearable computing system 100
With the computer in such as another wireless telecom equipment (for example, cellular phone or another wearable computing devices), communication network
User or server in communication network or one or more terminal devices of server system between communication.It is for example, logical
Letter interface 126 can also support the access in radio being connect with Ethernet or USB to communicate.
Figure 1B shows the view towards inside of the wearable computing system 100 of Figure 1A, represents to be presented on the wearing of HMD
The view of HMD in the forward direction FOV of person.Other than multiple components shown in Figure 1A, glasses 102 are also portrayed as packet by Figure 1B
It includes the inner surface for being coupled to central frame support 108 and is configured as the real-time of the left eye of the wearer of capture HMD 102
The first of video and/or quick static image is towards the video camera 128 of eyes.In addition, second takes the photograph towards the videos of eyes
Camera 130 is shown coupled to the inner surface of central frame support 108 and is configured as the wearer of capture HMD 102
Left eye real-time video and/or quick static image.As described below, the realtime graphic of the left eye of wearer and right eye can group
It closes ground and generates the real time tridimensional video figure that can be handled and analyzed the accurate direction of gaze of the wearer to provide HMD 102
Picture and/or quick static image.The processing and analysis can be done in real time to be watched attentively and eye movement information in real time with generating,
And/or generate the same or similar information in the subsequent subsequent time.It in other exemplary embodiments, can be by can be with
Track eyes are mobile and/or obtain the equipment of the other forms of eye tracking data to replace or enhance the video camera towards eyes
Machine 128 and 130, the eye tracking data can be analyzed to determine function of the direction of gaze as the time of user.
Though not necessarily in being shown in Figure 1A or Figure 1B, but HMD 102 may include before being configured in wearer
One or more display elements and/or component of static and/or video (movement) image are shown into FOV.Display element and/or
Component can be including optical waveguide etc. projection element, the wearer for virtual image to be directed to HMD can see
In their forward direction FOV.When HMD 102 is dressed by user or wearer, may then pass through with projection or display
The lens element 110 and 112 of image (such as showing image 132 and 134) while see forward field of view.This passes through a left side in fig. ib
The identical FOV objects 136-R in visual field (FOV) object 136-L and right lens element 110 in lens element 112 is represented.Institute
The combination of the image of display and the real-world object observed in FOV may be the one side of augmented reality above-mentioned.
Can be the generation of right side and left side lens element in addition, when right side and left side image is synthesized together by the wearer of HMD
Image generates virtual three-dimensional space.Then dummy object can be made to seem to be located at and occupies the reality by lens transparent watched
Border three dimensions.
In alternative embodiments, other types of display element can also be used.For example, lens element 110,112 can be with
Including：The transparent or semitransparent matrix display of such as electroluminescent display or liquid crystal display；For image to be passed to
One or more waveguides of the eyes of user；And/or can by focus be aligned the image close to eyes pass to user its
Its optical element.Corresponding display driver can be arranged in frame element 104 and 106 that this matrix is driven to show
Device.Alternatively, or in addition, the scanning laser equipment of such as low power laser or LED source and adjoint scanning system can incite somebody to action
Grating is shown on the retina for the one or more eyes for being drawn directly into user.Then user can be based on arrival retina
Light shows to perceive grating.
Although being not shown in Figure 1A and Figure 1B, wearable system 100 can also be included for the one of audio output
A or multiple components.For example, wearable computing system 100 can be equipped with (multiple) loud speaker, (multiple) earphone and/or (more
It is a) earphone jack.There is also other possibilities.
Although the wearable computing system 100 of the example embodiment shown in Figure 1A and Figure 1B is configured as being integrated in HMD
Unified data packet in component, but other configurations are also possible.For example, although without clearly in Figure 1A and Figure 1B
It shows, but wearable computing system 100 can be configured as with all or part of wherein onboard computing systems 118 far from eye
The distributed structure/architecture of mirror 102 is realized.For example, some or all onboard computing systems 118 can be made into as attachment
It may be worn in clothes or on clothes, such as, in pocket or on belt clamp.Similarly, it is retouched in Figure 1A and/or Figure 1B
The other components being integrated in glasses 102 painted can also be from HMD component Remote configurations.It is certain in this distributed structure/architecture
Component may be still integrated in HMD components.For example, one or more sensors (for example, magnetometer, gyroscope etc.) can be by
It is integrated in glasses 102.
Example distribution formula configuration in, HMD components (including other integrated packages) can via communication interface 126 (or
Via the special connection different from communication interface 126) it communicates with remote component.For example, wired (such as USB or Ethernet)
Or wireless (such as WiFi or bluetooth) connection can support the communication between remote computing system and HMD components.In addition, for example, this
The communication link of sample can be realized between HMD components and other remote equipments of such as laptop computer or mobile phone.
Fig. 1 C show another wearable computing system accoding to exemplary embodiment, take the form of HMD 152.
HMD 152 may include frame element and side arm, such as those for Figure 1A and Figure 1B descriptions.HMD 152 can additionally include
The forward video camera 156 of 154 knead dough of onboard computing systems, such as Figure 1A and Figure 1B it is described those.Video is taken the photograph
Camera 156 is shown as on the frame of HMD 152.However, video camera 156 can also be mounted on other positions.Although
It is not explicitly shown, but the HMD in each in Fig. 1 C and Fig. 1 D can include being configured as the video camera towards eyes
Machine and/or miscellaneous equipment or element for tracking the movement of the eyes of the wearer of HMD.
As shown in Figure 1 C, HMD 152 can include maying be coupled to the individual monitor 158 of equipment.Display 158 can be with
It is formed on one in the lens element of HMD 152, the lens element such as described for Figure 1A and Figure 1B, and can be by
It is configured to the figure that coverage machine generates in the visual field of the physical world of user.Show that display 158 is provided at HMD
The center of 152 lens, however, display 158 may be provided in other positions.Display 158 can be via passing through optics wave
160 computing systems 154 for being coupled to display 158 are led to control.
Fig. 1 D show another wearable computing system accoding to exemplary embodiment, take the form of HMD 172.HMD
172 may include side arm 173, central frame support 174 and bridge joint (bridge) part with nose support 175.Shown in Fig. 1 D
In example, 174 connection side arm 173 of central frame support.HMD 172 does not include the lens-mount comprising lens element.HMD 172
Onboard computing systems 176 and video camera 178 can additionally be included, such as Figure 1A and Figure 1B it is described those.
HMD 172 can include the member of the single lens of one maying be coupled in side arm 173 or central frame support 174
Part 180.Lens element 180 can include such as the display of Figure 1A and Figure 1B display described, and can by with
It is set to the figure that computer generation is covered on the view of the physical world of user.In one example, single lens element 180
The inside (that is, the side of a part for user's head is exposed to when being dressed by user) of extension side arm 173 can be coupled to.
Single lens element 180 can be positioned in when HMD 172 is dressed by user before the eyes of user or near.For example, such as
Shown in Fig. 1 D, single lens element 180 can be positioned at the lower section of central frame support 174.
Fig. 2 is the block diagram for the functional unit for describing example wearable computing system 202 according to example embodiment.Such as Fig. 2
Shown, example wearable computing system 202 includes one or more processing units 204, data set 206, transceiver 212, leads to
Believe interface 214, user's input/output (I/O) equipment 216 and sensor device 228, can all pass through system bus
238 or other communication interconnection equipments are coupled.These components can be arranged to support (all according to wearable computing system
System 100 as shown in Figure 1A and 1B or other wearable HMD) example embodiment operation.
One or more processing units 204 can include one or more general processors (for example, INTEL microprocessors)
And/or one or more application specific processors (such as dedicated digital signal processor, application-specific integrated circuit etc.).In turn, data
Storage device 206 can include one or more volatibility and/or non-volatile storage components, such as magnetic memory or optical storage
Device or disk storage device.Data storage device 206 can be wholly or partly integrated for example as height with processing unit 204
Fast buffer storage or register.As further shown, data storage device 206 is provided as save routine logic 208 and journey
Ordinal number is according to 210.
Programmed logic 208 can be run described herein to perform including definition by one or more processing units 204
The machine language instruction (for example, software code, firmware code etc.) of the routine of various functions.Program data 210 can include by
The data that the one or more application or program that one or more processors can be run are used or manipulated.Except the data of other forms
Outside, such data can be included specific to the data of program, user data, input/output data, sensing data or one
Other data and information are received, store, retrieve, transmit, analyze or change in a or multiple programs or the operational process of application.
Transceiver 212 and communication interface 214 can be configured as support wearable computing system 202 with it is such as another wireless
Communication equipment (for example, cellular phone or another wearable computing devices), computer or communication network in telex network network
In server or server system one or more terminal devices between communication.Transceiver 212 can be with one or more
A antenna coupling is with enabled wireless communication, for example, described as described above for the wireless communication interface 126 shown in Figure 1A.
Transceiver 212 can also be coupled with one or more wired connectors (such as Ethernet or USB) for wire communication.Transmitting-receiving
Device 212 and communication interface 214 can be used for supporting the communication in distributed structure/architecture, the wearable computing in the distributed structure/architecture
The various components of system 202 be located at away from each other.In this sense, system bus 238 can include supporting this distribution
The element of communication between formula component and/or part.
As shown in the figure, user I/O equipment 216 includes video camera 218, display 220, loud speaker 222,224 and of microphone
Touch tablet 226.Video camera 218 can correspond in the discussion of video camera 120 and/or fig. 1 above A and Figure 1B forward retouch
The video camera 128 and 130 towards eyes stated.Similarly, display 220 can correspond to image procossing and display is
System is used for the user (wearer) for making HMD it can be seen that image.Other than other elements, display 220 can also include dividing
The first projecting apparatus 128 and the second projecting apparatus 130 not coupled with lens element 112 and 110, for generating as described above for Figure 1B institutes
The image of description is shown.Touch tablet 226 can correspond to as being directed to the described finger operable touch pads 124 of Figure 1A.It raises one's voice
Device 422 and microphone 224 can correspond similarly to reference to figure 1A and Figure 1B it is described above in component.User's I/O equipment
Each in 216 can also include the logical order run of device controller and storage and for total via system
The interface that line 238 communicates.
It can correspond to include position sensor above for the sensor device 228 of the described sensors 122 of Figure 1A
230th, motion sensor 232, one or more magnetometers 234 and aspect sensor 236.Position sensor 230 can correspond to
Equipment (for example, mobile telephone system triangulation equipment etc.) is determined in global positioning system (GPS) equipment or other positions.
Motion sensor 232 can correspond to one or more accelerometers and/or one or more gyroscopes.For example, typically match
Put three accelerometers that can include along three mutually orthogonal axis orientations (orient).Three magnetometers can also be used
Similar configuration.
Aspect sensor 236 can include an AHRS either parts of AHRS, and AHRS is used to provide to determine HMD phases
For the function of the similar theodolite of the angle direction of the reference pointing direction of local ground coordinate system.For example, sensing directional
Device can determine the forward direction pointing direction of HMD relative to horizontal elevation angle and relative to such as geographical (or geodesy
) north reference direction azimuth.Other angles and coordinate system can also be used to determine direction.
Each in sensor device 228 can also include device controller and the logical order run stored,
And the interface for communicating via system bus 238.
It should be understood that it is (all wearable as shown in Figure 2 to there may be wearable computing system or wearable HMD
Computing system 202) many specific embodiments.In addition, it should be appreciated by those skilled in the art that how to design and build in this way
Embodiment.
B. example network
In the exemplary embodiment, HMD can support communication with network and with it is in network or with network communication
The communication of the equipment of connection.This communication can include another equipment of HMD and such as another HMD connecting, mobile meter
Calculate the information exchange between equipment (for example, mobile phone or smart phone) or server.Information exchange can support service and/
Or application or be service and/or application a part, include but not limited to upload and/or download content (for example, music,
Video etc.) and client-server communication etc..
Fig. 3 is shown in which that one or more HMD may participate in a view of the network 300 of communication.As depicted, net
Network 300 includes being connected to every in radio access network (RAN) 304, Radio Access Network 306 and cable access network 308
The data network 302 of one.Data network 302 can represent such as internet or one or more interconnection including internet
Communication network.Radio access network 304 can represent support for example 3G and/or 4G cellular radio technologies (such as CDMA,
EVDO, GSM, UMTS, LTE, WiMAX) service provider cellular radio network.Radio Access Network 306 can represent
Support such as bluetooth, ZigBee and the house of WiFi (for example, 802.11A, 802.11B, 802.11g) or hot spot radio area network
Network.Cable access network 308 can represent to support the house of such as Ethernet or business LAN.
Network 300 further includes the server system 310 for being connected to data network 302.Server system 310 can represent to use
In the website for the service for providing a user one or another kind of types or other network-based facilities.For example, according to example reality
Example is applied, server system 310 can be with the online social networking service of trustship or website.As another example, server system 310
Network-based information search service can be provided.As another example, server system 310 can receive eyes from HMD
Tracking data, and analysis result is returned into HMD.
Fig. 3 is also illustrated via a various terminals user and/or client for being connected to network 300 in three access networks
End equipment.For example, HMD 312 is connected to RAN 304, and HMD via air interface 313 (for example, 3G or 4G technologies)
314 are connected to RAN 304 via air interface 315 (for example, 3G or 4G technologies).Also by way of example, HMD 316 via connecing in the air
317 (for example, WiFi technologies) of mouth are connected to Radio Access Network 306.In addition and as an example, mobile phone 318 is shown
RAN 304 is connected to via air interface 319, shows that smart phone 320 is connected to wireless access network via air interface 321
Network 306, and show that laptop computer 322 is connected to cable access network 308 via wireline interface 323.Terminal user
Each in equipment can connect to communicate with one or the other networked devices via it with the respective of network.These
Some equipment in end user device can also directly communicate with (or other unshowned end user devices) each other.
Each in HMD 312,314 and 316 is depicted as by different user (each user is represented by cartoon face)
Wearing is so that show may the possible variable related with user associated with each HMD, situation and application.For example, HMD
312 can be once by content uploading to online social networking service, and HMD 314 can be in same time or another time to base
Request is sent in the information search service of network.User can via their corresponding HMD with each other and/or and network interaction.
Other examples are also possible.For the purpose largely discussed of this paper, usually only with reference to HMD without reference to the user of HMD
It is sufficient that (or wearer).By make as needed to the user (or wearer) of HMD clearly refer to or the user of HMD
The discussion of (or wearer).
C. example server system
The network server of server system 310 in such as Fig. 3 can take various forms and with one or more
Different mode is realized.Fig. 4 A and Fig. 4 B show two example embodiments of server system：It is set including representativeness calculating
For the integrated system of (Fig. 4 A) and including being communicatively coupled to multiple representative calculating equipments together and other system member
The distributed system (Fig. 4 B) of part.
Fig. 4 A are the block diagrams of computing device 400 according to example embodiment.Computing device 400 can include user interface mould
Block 401, network communication interface module 402, one or more processors 403 and data storage device 404, it is all these can
It is linked together via system bus, network or other bindiny mechanisms 405.Computing device 400 can receive data simultaneously
Any kind of equipment of information displayed in association with for the data with reception is provided.For example, can take can for equipment 400
It dresses the form of computing device or is included as a part for wearable computing devices, such as referring to figs. 1A to described by Fig. 1 D
HMD 102, HMD 152 or HMD 172.In addition, as described above, computing device 400 can also take integrating server system
Form or be included in integrating server system.Computing device 400 can also take other systems other forms and/or
It is included as a part for other systems.
Subscriber Interface Module SIM 401 can be used to external user input-output apparatus transmission data and/or be used from outside
Family input-output apparatus receives data.For example, Subscriber Interface Module SIM 401 can be configured as to/from such as keyboard, keypad,
Touch screen, computer mouse, tracking ball, control stick and/or other similar equipment currently known or the user developed later
Input equipment sends/receives data.Subscriber Interface Module SIM 401 can be additionally configured to such as one or more cathode-ray tubes
(cathode ray tube, CRT), liquid crystal display (liquid crystal display, LCD), light emitting diode
(light emitting diode, LED), digital light processing (digital light processing, DLP) technology is used
Display, printer, light bulb, and/or other similar equipment user that is currently known or developing later show that equipment provides
Output.Subscriber Interface Module SIM 401 can be additionally configured to generation audible output, such as loud speaker, speaker receptacle, audio output
Port, audio output apparatus, earphone and/or other similar devices that are currently known or developing later.
Network communication interface module 402 can include being configured as the network progress via network 302 such as shown in Fig. 3
One or more wireless interfaces 407 of communication and/or wireline interface 408.Wireless interface 407 can include one or more nothings
Line transceiver, such as bluetooth transceiver, perhaps according to 802.11 standards of IEEE (for example, 802.11A, 802.11B, 802.11g)
The Wi-Fi transceiver of operation, perhaps according to the WiMAX transceiver of 802.16 standard operations of IEEE, and/or can be configured to via
The other types of wireless transceiver that wireless network communicates.Wireline interface 408 can include one or more wired transmitting-receivings
Device, such as ethernet transceiver, universal serial bus (Universal Serial Bus, USB) transceiver can be configured as
It is similar with wired network communication via conducting wire, twisted-pair feeder, coaxial cable, optical link, fiber link or other physical connections
Transceiver.
In some embodiments, network communication interface module 402, which can be configured as, provides reliable, safety, compression
And/or certification communication.For each communication described herein, reliable communication can be provided for ensuring that (for example, guaranteed
Message transmission) information perhaps as message header and/or a part for footer (for example, packets/messages sequencing information, (more
It is a) encapsulation header and/or footer, size/temporal information and such as cyclic redundancy check (cyclic redundancy
Check, CRC) and/or parity values transmission verification information).Communication can use one or more compression algorithms and/or
(such as, but not limited to one or more lossless (lossless) data compressions are calculated for agreement, and/or decompression algorithm and/or agreement
Method and/or one or more damage (lossy) data compression algorithm) it compresses and decompresses.It can use one or more close
Code agreement and/or algorithm (such as, but not limited to DES, AES, RSA, Diffie-Hellman and/or DSA) make communication become safety
It (for example, be encoded or be encrypted) and/or decrypted/or is decoded.Can also use other cipher protocols and/or algorithm or
In addition to the cipher protocol and/or algorithm listed herein, to protect (and then decryption/decoding) communication.
One or more processors 403 can include one or more general processors and/or one or more special places
Manage device (for example, digital signal processor, application-specific integrated circuit etc.).One or more processors 403 can be configured as operation packet
The computer-readable program instructions 406 being contained in data storage device 404 and/or other instructions as described herein.
Data storage device 404 can include can by least one of processor 403 read or access one or
Multiple computer readable storage mediums.One or more computer readable storage mediums can include volatibility and/or non-volatile
Property storage assembly, such as optics, magnetism, organic or other memories or disc memory device, which can be whole
Body or it is partly integrated at least one of one or more processors 403.In some embodiments, data storage device
404 can be come using single physical equipment (for example, an optics, magnetism, organic or other memories or disk storage unit)
It realizes, and in other embodiments, data storage device 404 can be realized using two or more physical equipments.
It is associated computer-readable with data storage device 404 described herein and/or other computer-readable mediums
Storage medium can also include non-transitory computer-readable medium, such as register memory, processor cache and random
Access the computer-readable medium of memory (RAM) equally short time period storage data.With data storage device described herein
404 and/or the associated computer readable storage medium of other computer-readable mediums can also include storage a longer period of time
Program code and/or data non-transitory computer-readable medium, such as secondary or lasting long term memory, such as
As read-only memory (ROM), CD or disk, compact disc read-only memory (CD-ROM).With data storage 404 described herein
And/or other associated computer readable storage mediums of computer-readable medium can also be any other volatibility or non-volatile
Property storage system.It is associated computer-readable with data storage device 404 described herein and/or other computer-readable mediums
Storage medium is considered such as computer readable storage medium or tangible storage device.
Data storage device 404 can include computer-readable program instructions 406 and perhaps include additional data.One
In a little embodiments, data storage device 404 can additionally include performing technique described herein, at least part of method
And/or the storage device required by least part of equipment described herein and the function of network.
Fig. 4 B depict the network 406 with computing cluster 409a, 409b and 409c according to example embodiment.In Fig. 4 B
In, the function of the network server of the server system 310 in such as Fig. 3 can be distributed in three computing clusters 409a, 409b
Between 408c.Computing cluster 409a can include being set by one or more calculate that local cluster network 412a links together
Standby 400a, cluster-based storage array 410a and cluster routers 411a.Similarly, computing cluster 409b can include by local cluster
One or more computing device 400b, cluster-based storage array 410b and the cluster routers 411b that network 412b links together.
Similarly, computing cluster 409c can include the one or more computing devices to be linked together by local cluster network 412c
400c, cluster-based storage array 410c and cluster routers 411c.
In some embodiments, each in computing cluster 409a, 409b and 409c can have the calculating of identical quantity
The cluster routers of equipment, the cluster-based storage array of identical quantity and identical quantity.However, it in other embodiments, calculates
Some or all of cluster 409a, 409b and 409c can have the computing device of different number, the cluster of different number to deposit
Store up the cluster routers of array and/or different number.Computing device, cluster-based storage array and cluster road in each computing cluster
It can depend on distributing to the calculating task or multiple tasks of each computing cluster by the quantity of device.
Cluster-based storage array 410a, 410b and 410c of computing cluster 409a, 409b and 409c can include being configured
Data storage array for the disk array controller for managing the reading and write-access to hard disk drive group.Disk array control
Device processed combine individually or with its respective computing device to be configured as managing and is stored in data in cluster-based storage array
Backup or redundant copy to prevent disc driver or other cluster-based storage array failures and/or network failure, the failure and/
Or network failure prevents one or more computing devices from accessing one or more cluster-based storage arrays.
Cluster routers 411a, 411b and 411c in computing cluster 409a, 409b and 409c can include being configured as
The network equipment of internal and external communication is provided for computing cluster.For example, the cluster routers 411a in computing cluster 409a can
To include one or more internets exchanges and/or routing device, which exchanges and/or routing device quilt
It is configured to provide between (i) computing device 400a and cluster storage array 410a via the local Netcom of local cluster network 412a
413a is connected to network 406 via Wide Area Network between letter, and/or (ii) computing cluster 409a and computing cluster 409b and 409c
Wan communication.Cluster routers 411b and 411c can include the network equipment similar to cluster routers 411a, and
Cluster routers 411b and 411c can be that computing cluster 409b and 409b execution cluster routers 411a are computing cluster 409a
The similar network function performed.
3. the video stabilization with direction of gaze is moved based on eye
The user of HMD can obtain video data using preceding while HMD is dressed to video camera.For such as Figure 1A and
Configuration shown in Figure 1B, video camera (such as video camera 120 forward) forward is with fixed relative to HMD
FOV directions, HMD will remain fixed direction usually relative to wearer in itself.In this way, the FOV side of video camera forward
To will be moved together with HMD, HMD is on the contrary as wearer moves.Therefore the video image that is captured is by the fortune including wearer
It is dynamic.The movement for the wearer being passed in video image can be so that video image seems not when being watched during playback
It is coherent or other forms unstable.
In general, when people move relative to set point or assigned direction, the eye gaze direction of people will be inclined to
In tracking set point or assigned direction in its FOV.At least in the duration comparable duration of the mobile generation with people
On, and for less arrive be more than wherein eye gaze direction can remain fixed in set point eye motion range
Head and/or body kinematics, this is generally maintained at very.It is this be capable of effective compensation body kinematics be relatively fixed the ability watched attentively
It is a natural functions of the brain when handling vision data and controlling eye motion with trend.It may include it is conscious and
Subconscious ingredient --- watch attentively for example, remaining fixed and non-autonomous watch the fixed decision recognized attentively.For most of
Actual purpose, between the direction of gaze of the wearer of forward direction FOV and HMD of the video camera fixed to HMD are discussed
Relative motion and/or towards when, only consider wearer face movement it is sufficient that.Unless otherwise noted, otherwise HMD is worn
The movement of wearer and the movement of the face of wearer (for example, translation and/or change of direction) will draw interchangeably herein
With.
According to example embodiment, the headset equipment of the wearable computing system such as shown in Figure 1A to Fig. 1 D can profit
It is tracked with the eye gaze of HMD wearers and determines eye during the forward video camera of using face carries out video capture
Function of the eyeball direction of gaze as the time, to stablize the video image of capture.Eye gaze tracking can use one of HMD or
Multiple eyes track components or equipment (such as towards the video camera of eyes 128 and 130) to realize to track and measure eye
Eyeball moves.Although the acquisition and analysis of eyes tracking measurement and data can use the physical resource of HMD (for example, towards eyes
Camera operation, processing capacity etc.), but direction of gaze measurement represents the information to any HMD components offer free of charge in itself,
More precisely by " free " offer of the natural cerebral function of HMD wearers.Direction of gaze measured value can also be recognized
To be " self reference ", because they directly refer to the preceding point of interest into FOV or interest region.
In the typical feelings that the direction forward of the face of HMD wearers is moved relative to the inceptive direction in forward direction FOV
Under condition, when the eyes of wearer against the facial movement movement of wearer on inceptive direction by remaining fixed (or almost
When so), the eye motion of wearer can be applied to the video data of capture to compensate video image plane relative to first
The movement in beginning direction.More specifically, vedio data quilt usually in the plane of delineation of (that is, vertical) orthogonal with FOV directions
Detection and capture.Therefore, video camera forward will cause FOV according to the movement across this relative to the movement of inceptive direction
The plane of delineation moves.When video data is detected and is captured, particularly if the movement of video camera forward is nothing
It is meaning or other only with the movement of wearer, then it can generate bounce, fluctuating across this movement of the plane of delineation
Or other unstable video image.
According to example embodiment, the eye motion of wearer can be mapped to the plane of delineation to compensate across the plane of delineation
FOV movement.More specifically, when the video data of capture is handled, can be come using the eye motion of mapping effective
Video image is repositioned onto the position that they will occur in the plane of delineation in the case of the movement of no wearer by ground.
This repositioning can be done in real time or be completed in capture post-processes.No matter which kind of situation, be as a result all stable video.
Since motion compensation uses function of the eye gaze direction as the time, so this technology is referred to as visual stabilization.
Fig. 5 is the conceptual representation that visual according to example embodiment is stablized.It is being used as an example, Fig. 5 is shown
Three continuous time instances during the video capture of video camera 502 forward；Three times from left to right arrange
On three panels in figure, it is respectively labeled as 1,2 and 3.Each Display panel insubstantial eyes of a pair seen from above
501-L (left eye) and 501-R (right eye).Eyes have the binocular fixation for being fixed on the preceding point 507 into FOV.It shows each
Insubstantial cartoon nose 505 in panel is to indicate direction of associated (but sightless) face relative to point 507
Direction and eyes relative to face angle direction.Different " nose direction " directions between panel can be used to table
Show movement (for example, change direction) of the face from a panel to another panel.Equally in each panel, towards eyes
The movement of video camera 504-L tracking left eyes 501-L, towards the fortune of the video camera 504-R tracking right eyes 501-R of eyes
It is dynamic.As indicated, the angle direction profit of video camera 502 and video camera 504-L and 505-R towards eyes forward
It is tracked with " nose direction " direction.This corresponds to approximately fixed directions of the HMD relative to wearer head.In order to succinctly rise
See, these digital labels are only present in first panel.
Apparently and by way of example, the direction of nose 505 (and sightless face) and and face
Forward video camera 502 and towards eyes video camera 504-L and 505-R together, in Figure 5 by from left to right
Sequence changes to the next panel from a panel.Meanwhile the direction of gaze of left eye and right eye is remained fixed in a little on 507.This
By left and right the direction of gaze 503-L-1 and 503-R-1 in panel 1, the 503-L-2 in panel 2 and 503-R-2 and panel 3
In 503-L-3 and 503-R-1 represent.
Point 507 not necessarily corresponds to the spatial position (although possible) of actual physics object, and corresponds to left eye and the right side
The convergent point watched attentively of eye.In this way, it is considered the geometrical construction that can be determined according to trigon General Principle,
In such as the distance between two formation baselines, and the direction of gaze of each eye of wearer be considered relative to
The angle of plane comprising baseline.Then the line-of-sight distance of convergent point can be determined.For the actual purpose of calculating, definition
The each eye of the wearer of HMD is more convenient relative to the direction of gaze of one or more fixed positions on HMD.For example,
It can define and be used for the sight of the central point in the plane of delineation towards the video camera 504-L of eyes from left eye 501-L
The reference direction of the direction of left eye is measured, and similarly for the right eye 504-R and video camera 504-R towards eyes.
It note that, for very remote point, direction of gaze may not be assembled or be intersected.Under any circumstance, the wearer's of HMD is double
Eye fixation direction is considered the center of the FOV of wearer.Therefore, tracking pairs of eyes direction of gaze can be provided and be can be used for
The reference direction that the movement of FOVs of the FOV far from wearer of the forward video camera in opposite is corrected is (for example, the FOV of wearer
Center).
Fig. 5 further depicts the plane of delineation of the video camera 502 forward for each in panel 1,2 and 3
In point 507 position and according to example embodiment as how compensated the movement of position in the plane of delineation.Specifically,
In panel 1, plane of delineation 506-1 is displayed directly the lower section of exemplary eyes 501-L and 501-R.The plane of delineation is retouched
It is depicted as two-dimensional grid, it is intended that represent pel array；The size (quantity of the pixel in each dimension) of grid is as shown
Arbitrarily, it is intended merely to convey conceptual illustration.The stain in the pixel near the heart represents the picture position of point 507 within a grid.
That is, the stain expression at the center of plane of delineation 506-1 is mapped to the eyes note of the wearer of the location of pixels in the plane of delineation
Apparent direction.
The identical plane of delineation is shown in panel 2, but the 506-2 that is re-flagged is to indicate it relative to point 507
Direction changes with the direction of the change of video camera 502 forward.Due to towards change as a result, showing again
The picture position for being shown as the point 507 of stain is moved into the pixel different from panel 1.Ash point represents the previous picture of point 507
Plain position and the movement locus that the image to the point 507 in the arrow expression plane of delineation of the curve of stain is put from ash.
In panel 3, the identical plane of delineation is shown again, but the 506-3 re-flagged now is to indicate with face
The direction further changed of forward video camera 502, relative to the further change of the direction of point 507.As direction
It is further changing as a result, point 507 picture position (still appearing as stain) be moved into the pixel different from panel 2.
The previous pixel position of two ash point display points 507, and the curve arrow from grey point to stain represents the point in the plane of delineation
Another track of the movement of 507 image.Therefore, track in panel 2 and 3 represent in capture video data forward by
Across the movement of the point 507 of the plane of delineation caused by the movement of wearer's face.That is, the video of track following forward is taken the photograph
The FOV of camera is relative to as the movement at the center of the FOV of wearer determined from the binocular fixation direction of wearer.
Across the image of the point 507 of the plane of delineation movement compensation the plane of delineation represent 506-1,506-2 and 506-3 in
Each under conceptually illustrate in the plane of delineation 508-1,508-2 and 508-3 of repositioning for describing respectively.Citing
For, panel 1 is used to represent the forward video camera 502 of knead dough of the face of wearer relative to the initial FOV of point 507
Direction.Accordingly, with respect to plane of delineation 506-1, the plane of delineation 508-1 of repositioning is deviated with zero (nothing).
However, in panel 2, the plane of delineation 508-2 of repositioning is relative to putting in 507 plane of delineation 506-2
The corresponding plane of delineation 506-2 in track be shifted by.As indicated, when shown movement starts, which appear in invocation point 507
In center pixel position in the plane of delineation 508-2 of repositioning, with the initial pixel locations in plane of delineation 506-1
It is identical.Dashed rectangle in panel 2 represents original (unmigrated) position of the plane of delineation.
In panel 3, the plane of delineation 508-3 of repositioning is relative to putting the track in 507 plane of delineation 506-3
Corresponding plane of delineation 506-3 is shifted by.As indicated, when shown movement starts, it is fixed which makes invocation point 507 appear in again
Position plane of delineation 508-3 in center pixel position in, again with the initial pixel locations phase in plane of delineation 506-1
Together.Dashed rectangle in plane 3 represents original (unmigrated) position of the plane of delineation.
According to example embodiment, such as the image represented in the plane of delineation 508-2,508-2 and 508-3 of repositioning
Offset is applied in all pixels of each plane of delineation 506-2,506-2 and 506-3.In this way, it is captured in vedio data
Entire FOV can be relocated to be aligned with as the center of FOV of the wearer determined during video captures.In forward direction
FOV videos are captured while track the eye gaze direction that the movement of the eyes of wearer can be used for determining left eye and right eye,
And the function for generating eyes or three-dimensional direction of gaze as the time is assembled in the analysis (geometry) in eye gaze direction.Figure in Fig. 5
Track shown in image plane example represents the mapping in the eyes eye gaze direction on plane of delineation example.Therefore, institute is deviated
The video image forward of capture effectively will be as the video image captured by mobile video camera 502 forward
Be converted to the video image as the capture of video camera forward for being carried out at the same time eyes mobile tracking.The result is that vision
Stable video.
In the exemplary embodiment, the video camera 504-L and 505-R towards eyes can be used to track eyes movement conduct
The function of time.Other eye-tracking devices and technology can also be utilized.For example, the continuous static of left eye and right eye can be handled
The quick obtaining of image is to determine the individually function with three-dimensional eye gaze direction as the time.Other technology can be related to
The reflection of the controlled point illumination of track eyes.These and other technology can be used for catching by the video camera forward of HMD
Function of the eye gaze direction as the time is determined during obtaining video image.
Accoding to exemplary embodiment, the vedio data of capture offset or repositioning can with video capture simultaneously
Ground performs in real time.Additionally or alternatively, identified three-dimensional direction of gaze can be recorded and be applied to after video capture
Time offset operation in.In real-time conditions, same to hour offset, reposition operation can be by the one or more of HMD
Processor ((multiple) processor 204 in vehicle based computing system 118 or Fig. 2 in such as Figure 1A) performs.For deviating afterwards,
Processing can by HMD one or more processors and/or pass through teleprocessing system (this server in such as network)
To perform.
As described above, the propensity in the eye gaze direction of the people of tracking object is between direction of gaze and object
The response of relative motion.Therefore, the movement of the object in the FOV of the video camera forward of HMD can be by following generation：
Movement of the face of wearer relative to the FOV position of fixed object in space, the facial movement by wherein wearer/
One of movement in movement towards fixed reference system or the collective reference system in both faces of physics and wearer
A little combinations.Illustrative embodiments above is to be retouched according to wearer's face relative to the movement of the object of fixed position or point
It states.This is convenient to a certain extent, in part because these technologies may have centainly with the movement of the face of wearer
Correlation because this type of sports will be directly transferred to before to video camera, and therefore arrive what may be notably observed that
FOV is moved.However, the relative motion that described principle can be applied to for any of the above described reason and occur.
For example, the point 507 in Fig. 5 can be relative to HMD (on the object of wearer's movement relative to HMD)
Wearer movement object, such as automobile or aircraft.If wearer by the eyes of (main) mobile s/he rather than
The head of s/he tracks object, then the diagram of Fig. 5 is still set up.That is, the FOV of the plane of delineation is remained fixed, and object
The movement of body is converted to the movement across the plane of delineation, and eye gaze direction tracking mobile object, by its eyes in eyes
Keep (at least approximately) fixed on FOV directions.
Further according to example embodiment, the object such as in the plane of delineation of plane of delineation 506-1,506-2,506-3
Movement can be divided into the movement of the face from wearer or the movement of the object in space (or be at least mainly one
Or another).Specifically, above-mentioned technology can within the interested period be applied to two or more it is different by with
Track object.In the context of Fig. 5, this can correspond to a little 507 two or more examples, each example from it is different with
Track example is associated.Determine the related coefficient between different tracking then can be used to determine as plane of delineation 506-1,506-2,
Shown in 506-3 in the plane of delineation across the movement of pixel whether be direction due to HMD in space change or due to
The movement of the object that one or more is watched attentively in space.
Further according to example embodiment, the correlation as the direction of gaze of the function of time and location of pixels can be used
The specific interested object of wearer is just being look in the wearer for speculating HMD rather than more random is being watched attentively.For example,
The wearer of driving can look about as conventional driving convention and observe adjacent track, the vehicle to come head-on etc..So
And in driving procedure, wearer is it can also be noted that specific attention object, such as billboard or terrestrial reference.It is various to watch attentively
Duration and its can be used for that the interested object of HMD wearers will be included with the correlation of the location of pixels in the plane of delineation
Pixel and wearer more conventional behavior or the relevant pixel of attention distinguish.
Although Yi Shang technology is especially suitable for binocular fixation direction, the line-of-sight distance including determining object or point into space
From ability, but the principle is applicable to only keep track the embodiment of the direction of gaze of one eye eyeball.In such embodiments,
Distance determines it is impossible, but very close towards simple eye camera shooting by the way that video camera forward is configured to
Machine, as determined by the video camera towards eyes, the center of the plane of delineation of video camera forward (or the plane of delineation
Reference position) it can be aligned to arbitrarily close to (or almost such) simple eye reference direction of gaze.It for example, can with reference to direction of gaze
To be the direction of (vertical) orthogonal with the center of the plane of delineation of eye cameras.In this, as calibration, by regarding towards eyes
The monocular fixation direction that frequency video camera measures can be mapped directly into the side of watching attentively in the FOV of video camera forward
To.Although the fixation eyes technology that differs is equally accurate, at least in involved hardware and power consumption, simple eye tracking is used
Image stabilization may be lower than eyes technology cost.
Fig. 6 is the method 600 that visual stability is used in the wearable computing system for show such as wearable HMD
The flow chart of example embodiment, such as the description of conceptual operational term.Exemplary method 600 can equipped with towards
Preceding video camera, towards left eye knead dough to the video camera of right eye, one or more processors and store instruction
It is realized on the wearable HMD of memory, which can operate such that HMD performs this method by one or more processors
Various operations, as described below.Memory can be some form of non-transitory tangible computer readable storage medium, such as magnetic
Disk or CD etc., and in manufacture, configuration or for preparing wearable headset equipment and/or the server for operation
During other processes, the instruction can be provided for being transmitted to the memory of the memory of wearable headset equipment, server
Or the two.The various pieces of this method can also be realized on the server (or other computing devices or platform) outside HMD.
The example embodiment of this method is described below.
As shown, at step 602, when the video camera forward of HMD is regarded in the forward direction of video forward
In field (FOV) while capture video image, the left eye of the user of HMD and the stereoscopic video images of right eye can use the face of HMD
It is obtained to left eye and right-eye camera.
At step 604, the left eye of the user of HMD and the stereoscopic video images obtained of right eye are caught with video image
Function of the convergence direction of gaze of user as the time is used to determine during time interval while obtaining.
At step 606, by using the plane of delineation for assembling direction of gaze and video camera forward of user
The forward video camera of intersection point complementary area relative to assemble direction of gaze movement, to stablize captured video figure
Picture.
Accoding to exemplary embodiment, with determining that the convergence of user is watched attentively during capturing the time interval of video image simultaneously
Function of the direction as the time, it may be necessary to from the left eye and the stereoscopic video images of right eye obtained, determine the left eye of user
Corresponding gaze angle is the function of time with right eye, and then determines the phase of the corresponding angle of direction before projecting on FOV
Intersection point.
Also according to example embodiment, the plane of delineation for assembling direction of gaze and video camera forward of user is utilized
Intersection point complementary area forward screen video camera relative to the movement for assembling direction of gaze stablize captured video image,
It (such as, is regarded at may needing each time in two or more a series of times what two or more were captured
At the time of frequency frame) determine the convergence direction of gaze of user and the intersection point of the plane of delineation.Then each friendship of sequence can be determined
Plane coordinates in the plane of delineation of point, and the reference point that an intersection point of the sequence can be appointed as in the plane of delineation.
Then the video image captured at each corresponding time of one or more sequence times can be deviated, at the corresponding time
The plane coordinates of intersection point be aligned with the plane coordinates of reference point.
Further according to example embodiment, the plane of delineation can include the pel array of rule so that determine the every of sequence
Plane coordinates in the plane of delineation of a intersection point is it needs to be determined that the pixel coordinate of each intersection point for sequence.In the configuration,
The video image that offset captures at each corresponding time of one or more sequence times can make the intersection point at the corresponding time
Plane coordinates be aligned with the plane coordinates of reference point, it may be necessary to deviate the video image that is captured in the corresponding time with will be corresponding
The pixel coordinate of the intersection point of time is aligned with the pixel coordinate of reference point.
In the exemplary embodiment, the intersection point for assembling direction of gaze and the plane of delineation of video camera forward of user is utilized
The forward video camera of complementary area may be needed relative to the movement for assembling direction of gaze, determine to assemble note during time interval
The intercept curve of apparent direction and the plane of delineation is captured as the function of time then along course deviation during time interval
Video image is aligned by video image with each moment during time interval with the intersection point in the plane of delineation.
Also according to example embodiment, the video image captured during time interval along course deviation may need, just
The video image captured during time interval along course deviation in real time when determining and assembling direction of gaze.Additionally or replace
Dai Di, the video image captured during time interval along course deviation may be needed after convergence direction of gaze is determined
The video image that time captures along course deviation during time interval.
It will also be understood that the step of shown in Fig. 6, is intended to show that the operation of example embodiment.Therefore, thus it is possible to vary or modification
Various steps, thus it is possible to vary the sequence of certain steps, and additional step can be increased, while still realize whole desired
Operation.
Conclusion
Illustrative embodiment is described by example herein.However, it will be appreciated by those skilled in the art that
The present embodiment can be changed and modified, the element that is directed to without departing from the embodiment being defined by the claims, product and
The true scope and spirit of method.
Claims (20)
1. a kind of method realized in wearable headset equipment (HMD) Computer includes：
It HMD's and is attached to the video camera forward of the HMD using described in forward field of view (FOV) and captures
While video image, using it is described HMD's and be attached to the HMD towards the video camera of left eye and right eye obtain
The left eye of the user of the HMD and the stereoscopic video images of right eye；
The left eye of user based on the HMD and the stereoscopic video images obtained of right eye are caught with the video image
Obtain at the same time interval during, determine function of the convergence direction of gaze of the user as the time；And
It is mended by using the intersection point for assembling direction of gaze and the plane of delineation of the video camera forward of the user
The video camera forward is repaid relative to the movement for assembling direction of gaze, to stablize captured video image.
2. the method for claim 1, wherein during the time interval captured simultaneously with the video image, really
The convergence direction of gaze of the fixed user includes as the function of time：
Determine that the left eye of the user and the corresponding of right eye are watched attentively from the stereoscopic video images obtained of the left eye and right eye
Function of the angle as the time；And
Determine to project to the intersection point of the corresponding gaze angle on the forward direction FOV.
3. the method for claim 1, wherein direction of gaze and the video forward are assembled using the user
Video camera described in the intersection point compensation of the plane of delineation of video camera forward is relative to the movement for assembling direction of gaze
Including：
At a series of each time in two or more times, the convergence direction of gaze and described image of the user are determined
The intersection point of plane；
Determine the plane coordinates in the plane of delineation of each intersection point of the sequence；
The reference point one intersection point of the sequence being appointed as in described image plane；And
The video image of the capture each corresponding time of one or more of sequence times at is deviated with will be described
The plane coordinates of the intersection point at the corresponding time is aligned with the plane coordinates of the reference point.
4. method as claimed in claim 3, wherein, described image plane includes the regular array of pixel,
Wherein it is determined that the plane coordinates in the plane of delineation of each intersection point of the sequence includes determining for the sequence
The pixel coordinate of each intersection point of row；
And wherein, deviate at each corresponding time of one or more of sequence times capture the video image with
The plane coordinates of the intersection point at the corresponding time is aligned with the plane coordinates of the reference point including deviating in institute
The video image that is captured at each corresponding time of one or more sequence times is stated with will be described at the corresponding time
The pixel coordinate of intersection point is aligned with the pixel coordinate of the reference point.
5. the method for claim 1, wherein direction of gaze and the video forward are assembled using the user
Video camera described in the intersection point compensation of the plane of delineation of video camera forward is relative to the movement for assembling direction of gaze
Including：
During the time interval, letter of the track of the intersection point for assembling direction of gaze and the plane of delineation as the time is determined
Number；And
The video image captured during the time interval along the course deviation, with during the time interval
By the public point alignment in the video image and described image plane at each moment.
6. method as claimed in claim 5, wherein, captured during the time interval along the course deviation described in
Video image includes, the video figure captured during the time interval along the course deviation during processing time
Picture, the processing time are at least one of the following：(i) determining that real-time or (ii) for assembling direction of gaze is true
The fixed time assembled after direction of gaze.
7. a kind of method realized in wearable headset equipment (HMD) Computer, including：
It HMD's and is attached to the video camera forward of the HMD using described in forward field of view (FOV) and captures
While video image, HMD's and it is attached to the eye-tracking device of the HMD using described and obtains the user of the HMD
Left eye and at least one of right eye eye gaze direction；
Based on the eye gaze direction obtained, determined during the time interval of the capture with the video image simultaneously described
Function of the direction of gaze of user as the time；And
Institute is compensated by using the intersection point of the plane of delineation of the direction of gaze and video camera forward of the user
Movement of the video camera forward relative to the direction of gaze is stated, to stablize captured video image.
8. the method for claim 7, wherein, utilize the left eye of the user of the eye-tracking device acquisition HMD
Include with the eye gaze direction of at least one of right eye：Obtain the user's of the HMD using the eye-tracking device
The binocular fixation direction of left eye and right eye；
And wherein, in the direction of gaze conduct with determining the user during capturing the time interval of the video image simultaneously
The function of time includes：
The corresponding gaze angle conduct of the left eye and right eye of the user is determined from the binocular fixation direction of the left eye and right eye
The function of time；And
Determine to project to the intersection point of the corresponding gaze angle on the forward direction FOV.
9. the method for claim 7, wherein, utilize direction of gaze and the video camera forward of the user
Video camera described in the intersection point compensation of the plane of delineation of machine forward includes relative to the movement of the direction of gaze：
At a series of each time in two or more times, the direction of gaze of the user and described image plane are determined
Intersection point；
Determine the plane coordinates in the plane of delineation of each intersection point of the sequence；
The reference point one intersection point of the sequence being appointed as in described image plane；And
The video image captured at each corresponding time of one or more of sequence times is deviated, by the phase
The plane coordinates of the intersection point between seasonable is aligned with the plane coordinates of the reference point.
10. method as claimed in claim 9, wherein, described image plane includes the regular array of pixel,
Wherein it is determined that the plane coordinates in the plane of delineation of each intersection point of the sequence includes determining for the sequence
The pixel coordinate of each intersection point of row；
And wherein, deviate at each corresponding time of one or more of sequence times capture the video image with
The plane coordinates of the intersection point at the corresponding time is aligned with the plane coordinates of the reference point including deviating in institute
The video image captured at each corresponding time in one or more sequence times is stated with by the institute at each corresponding time
The pixel coordinate for stating intersection point is aligned with the pixel coordinate of the reference point.
11. it the method for claim 7, wherein, is taken the photograph using the direction of gaze and the video forward of the user
Video camera described in the intersection point compensation of the plane of delineation of camera forward includes relative to the movement of the direction of gaze：
Function of the intercept curve of the determining direction of gaze and the plane of delineation as the time during the time interval；And
The video image captured during the time interval along the course deviation, with during the time interval
Each moment is by the public point alignment in the video image and described image plane.
12. method as claimed in claim 11, wherein, the institute captured during the time interval along the course deviation
It states video image to include, the video captured during the time interval along the course deviation during processing time
Image, the processing time are at least one of the following：(i) real-time or (ii) for assembling direction of gaze is being determined
Determine the time after the convergence direction of gaze.
13. method as claimed in claim 8, wherein, the eye-tracking device includes taking the photograph towards the video of left eye and right eye
Camera,
And wherein, the left eye of user of the HMD and the binocular fixation direction of right eye are obtained using the eye-tracking device
Including：The left eye of user and the three-dimensional video-frequency figure of right eye of the HMD is obtained using the video camera towards left eye and right eye
Picture.
14. a kind of headset equipment (HMD), including：
Video camera forward；
Eye-tracking device；
Processor；And
Memory, store instruction, described instruction make the HMD execution include following operation when being run by the processor：
While video image is captured in forward field of view (FOV) using the video camera forward, using described
Eye-tracking device obtains the left eye of user of the HMD and the binocular fixation direction of right eye；
The left eye of user based on the HMD and the binocular fixation direction obtained of right eye are caught with the video image
Obtain at the same time interval during, determine function of the convergence direction of gaze of the user as the time；And
It is mended by using the intersection point for assembling direction of gaze and the plane of delineation of the video camera forward of the user
The video camera forward is repaid relative to the movement for assembling direction of gaze, to stablize captured video image.
15. HMD as claimed in claim 14, wherein, during the time interval of the capture with the video image simultaneously really
The convergence direction of gaze of the fixed user includes as the function of time：
From the binocular fixation direction obtained of left eye and right eye, the left eye of the user and the corresponding gaze angle of right eye are determined
Function as the time；And
Determine to project to the intersection point of the corresponding angle of direction on the forward direction FOV.
16. HMD as claimed in claim 14, wherein, utilize convergence direction of gaze and the regarding forward of the user
Video camera described in the intersection point compensation of the plane of delineation of frequency video camera forward is relative to the fortune for assembling direction of gaze
It is dynamic to include：
At a series of each time in two or more times, the convergence direction of gaze and described image of the user are determined
The intersection point of plane；
Determine the plane coordinates in the plane of delineation of each intersection point of the sequence；
Specify an intersection point of the sequence for the reference point in described image plane and
The video image captured at each corresponding time of one or more of sequence times is deviated, by the phase
The plane coordinates of the intersection point between seasonable is aligned with the plane coordinates of the reference point.
17. HMD as claimed in claim 16, wherein, described image plane includes the regular array of pixel,
Wherein it is determined that the plane coordinates in the plane of delineation of each intersection point of the sequence includes determining for the sequence
The pixel coordinate of each intersection point of row；
And wherein, deviate at each corresponding time of one or more of sequence times capture the video image with
The plane coordinates of the intersection point at the corresponding time is aligned with the plane coordinates of the reference point including deviating in institute
The video image captured at the corresponding time of one or more sequence times is stated with by the picture of the intersection point at the corresponding time
Plain coordinate is aligned with the pixel coordinate of the reference point.
18. HMD as claimed in claim 14, wherein, utilize convergence direction of gaze and the regarding forward of the user
Video camera described in the intersection point compensation of the plane of delineation of frequency video camera forward is relative to the fortune for assembling direction of gaze
It is dynamic to include：
During the time interval, letter of the track of the intersection point for assembling direction of gaze and the plane of delineation as the time is determined
Number；And
Along the video image that the course deviation captures during the time interval, during with each during time interval
By the public point alignment in the video image and described image plane at quarter.
19. HMD as claimed in claim 18, wherein, the institute captured during the time interval along the course deviation
It states video image to include, the video captured during the time interval along the course deviation during processing time
Image, the processing time be it is following in it is at least one：(i) determining that real-time or (ii) for assembling direction of gaze is true
The fixed time assembled after direction of gaze.
20. HMD as claimed in claim 14, wherein, the eye-tracking device includes taking the photograph towards the video of left eye and right eye
Camera,
And wherein, the left eye of user of the HMD and the binocular fixation direction of right eye are obtained using the eye-tracking device
Including：Utilize the left eye of user and the stereopsis of right eye that the HMD is obtained towards the video camera of left eye and right eye
Frequency image.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662430595P | 2016-12-06 | 2016-12-06 | |
US62/430,595 | 2016-12-06 |
Publications (2)
Publication Number | Publication Date |
---|---|
CN108156441A true CN108156441A (en) | 2018-06-12 |
CN108156441B CN108156441B (en) | 2021-07-30 |
Family
ID=60382036
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201711275264.8A Active CN108156441B (en) | 2016-12-06 | 2017-12-06 | Head-mounted device and computer-implemented method therein |
Country Status (6)
Country | Link |
---|---|
US (1) | US10591731B2 (en) |
EP (1) | EP3334148A1 (en) |
CN (1) | CN108156441B (en) |
DE (2) | DE102017126116A1 (en) |
GB (1) | GB2559237B (en) |
WO (1) | WO2018106390A1 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112740654A (en) * | 2018-09-19 | 2021-04-30 | 高途乐公司 | System and method for stabilizing video |
US20210199431A1 (en) * | 2019-12-26 | 2021-07-01 | Stmicroelectronics, Inc. | Method, device, and system of measuring eye convergence angle |
CN114531951A (en) * | 2019-09-27 | 2022-05-24 | 美国斯耐普公司 | Automatic video capture and compositing system |
CN114721144A (en) * | 2021-01-04 | 2022-07-08 | 宏碁股份有限公司 | Naked-eye stereoscopic display and control method thereof |
Families Citing this family (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10810773B2 (en) * | 2017-06-14 | 2020-10-20 | Dell Products, L.P. | Headset display control based upon a user's pupil state |
US11145124B2 (en) | 2017-08-30 | 2021-10-12 | Ronald H. Winston | System and method for rendering virtual reality interactions |
EP3750028B1 (en) * | 2018-02-09 | 2022-10-19 | Pupil Labs GmbH | Devices, systems and methods for predicting gaze-related parameters |
WO2019154511A1 (en) * | 2018-02-09 | 2019-08-15 | Pupil Labs Gmbh | Devices, systems and methods for predicting gaze-related parameters using a neural network |
EP3749172B1 (en) | 2018-02-09 | 2022-03-30 | Pupil Labs GmbH | Devices, systems and methods for predicting gaze-related parameters |
US11733517B2 (en) * | 2018-04-23 | 2023-08-22 | Sony Corporation | Ocular optical system, medical viewer, and medical viewer system |
US10833945B2 (en) * | 2018-11-13 | 2020-11-10 | International Business Machines Corporation | Managing downloading of content |
US11676422B2 (en) | 2019-06-05 | 2023-06-13 | Pupil Labs Gmbh | Devices, systems and methods for predicting gaze-related parameters |
EP3786767B1 (en) * | 2019-07-29 | 2023-11-08 | HTC Corporation | Eye tracking method, head-mounted display, and computer readable storage medium |
CN112949370A (en) * | 2019-12-10 | 2021-06-11 | 托比股份公司 | Eye event detection |
US20220269896A1 (en) * | 2020-04-13 | 2022-08-25 | Google Llc | Systems and methods for image data management |
US20220103757A1 (en) * | 2020-09-30 | 2022-03-31 | Snap Inc. | Multi-purpose cameras for simultaneous capture and cv on wearable ar devices |
WO2022132171A1 (en) * | 2020-12-18 | 2022-06-23 | Google Llc | Steerable camera array for head-mounted display devices |
US11567569B2 (en) | 2021-04-08 | 2023-01-31 | Google Llc | Object selection based on eye tracking in wearable device |
US20230031871A1 (en) * | 2021-07-29 | 2023-02-02 | Meta Platforms Technologies, Llc | User interface to select field of view of a camera in a smart glass |
WO2023009853A1 (en) * | 2021-07-29 | 2023-02-02 | Meta Platforms Technologies, Llc | User interface to select field of view of a camera in smart glasses |
WO2023183337A1 (en) * | 2022-03-21 | 2023-09-28 | Headvantage Corporation | Body worn camera, sensor and content delivery system |
US11822736B1 (en) * | 2022-05-18 | 2023-11-21 | Google Llc | Passive-accessory mediated gesture interaction with a head-mounted device |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101238486A (en) * | 2005-07-05 | 2008-08-06 | 汉特罗产品有限公司 | Apparatuses, computer program product, and method for digital image processing |
US8736692B1 (en) * | 2012-07-09 | 2014-05-27 | Google Inc. | Using involuntary orbital movements to stabilize a video |
CN104170370A (en) * | 2012-01-16 | 2014-11-26 | 谷歌公司 | Methods and systems for processing video for stablization using dynamic crop |
US20150235355A1 (en) * | 2014-02-19 | 2015-08-20 | Daqri, Llc | Active parallax correction |
CN105052129A (en) * | 2013-03-15 | 2015-11-11 | 谷歌公司 | Cascaded camera motion estimation, rolling shutter detection, and camera shake detection for video stabilization |
CN105718036A (en) * | 2014-12-04 | 2016-06-29 | 中芯国际集成电路制造(上海)有限公司 | Information interaction methods and systems of wearable device |
CN105830427A (en) * | 2013-10-11 | 2016-08-03 | 脸谱公司 | Applying video stabilization to a multimedia clip |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110228051A1 (en) * | 2010-03-17 | 2011-09-22 | Goksel Dedeoglu | Stereoscopic Viewing Comfort Through Gaze Estimation |
US8888287B2 (en) | 2010-12-13 | 2014-11-18 | Microsoft Corporation | Human-computer interface system having a 3D gaze tracker |
JP5998863B2 (en) | 2012-11-09 | 2016-09-28 | 株式会社Ｊｖｃケンウッド | Gaze detection device and gaze detection method |
US10136810B2 (en) | 2013-10-03 | 2018-11-27 | Neuroscience Research Australia (Neura) | Systems and methods for diagnosis and therapy of vision stability dysfunction |
JP6929644B2 (en) | 2013-12-31 | 2021-09-01 | グーグル エルエルシーＧｏｏｇｌｅ ＬＬＣ | Systems and methods for gaze media selection and editing |
US10395428B2 (en) * | 2016-06-13 | 2019-08-27 | Sony Interactive Entertainment Inc. | HMD transitions for focusing on specific content in virtual-reality environments |
-
2017
- 2017-10-25 US US15/793,023 patent/US10591731B2/en active Active
- 2017-11-08 EP EP17200621.5A patent/EP3334148A1/en active Pending
- 2017-11-08 DE DE102017126116.8A patent/DE102017126116A1/en active Pending
- 2017-11-08 WO PCT/US2017/060563 patent/WO2018106390A1/en active Application Filing
- 2017-11-08 DE DE202017106773.4U patent/DE202017106773U1/en active Active
- 2017-11-27 GB GB1719623.9A patent/GB2559237B/en active Active
- 2017-12-06 CN CN201711275264.8A patent/CN108156441B/en active Active
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101238486A (en) * | 2005-07-05 | 2008-08-06 | 汉特罗产品有限公司 | Apparatuses, computer program product, and method for digital image processing |
CN104170370A (en) * | 2012-01-16 | 2014-11-26 | 谷歌公司 | Methods and systems for processing video for stablization using dynamic crop |
US8736692B1 (en) * | 2012-07-09 | 2014-05-27 | Google Inc. | Using involuntary orbital movements to stabilize a video |
CN105052129A (en) * | 2013-03-15 | 2015-11-11 | 谷歌公司 | Cascaded camera motion estimation, rolling shutter detection, and camera shake detection for video stabilization |
CN105830427A (en) * | 2013-10-11 | 2016-08-03 | 脸谱公司 | Applying video stabilization to a multimedia clip |
US20150235355A1 (en) * | 2014-02-19 | 2015-08-20 | Daqri, Llc | Active parallax correction |
CN105718036A (en) * | 2014-12-04 | 2016-06-29 | 中芯国际集成电路制造(上海)有限公司 | Information interaction methods and systems of wearable device |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112740654A (en) * | 2018-09-19 | 2021-04-30 | 高途乐公司 | System and method for stabilizing video |
CN112740654B (en) * | 2018-09-19 | 2023-12-19 | 高途乐公司 | System and method for stabilizing video |
CN114531951A (en) * | 2019-09-27 | 2022-05-24 | 美国斯耐普公司 | Automatic video capture and compositing system |
US20210199431A1 (en) * | 2019-12-26 | 2021-07-01 | Stmicroelectronics, Inc. | Method, device, and system of measuring eye convergence angle |
US11720170B2 (en) * | 2019-12-26 | 2023-08-08 | Stmicroelectronics, Inc. | Method, device, and system of measuring eye convergence angle |
CN114721144A (en) * | 2021-01-04 | 2022-07-08 | 宏碁股份有限公司 | Naked-eye stereoscopic display and control method thereof |
Also Published As
Publication number | Publication date |
---|---|
EP3334148A1 (en) | 2018-06-13 |
CN108156441B (en) | 2021-07-30 |
US10591731B2 (en) | 2020-03-17 |
DE102017126116A1 (en) | 2018-06-07 |
DE202017106773U1 (en) | 2018-03-07 |
WO2018106390A1 (en) | 2018-06-14 |
US20180157045A1 (en) | 2018-06-07 |
GB201719623D0 (en) | 2018-01-10 |
GB2559237A (en) | 2018-08-01 |
GB2559237B (en) | 2021-03-10 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108156441A (en) | Visual is stablized | |
US20210110615A1 (en) | Cross reality system supporting multiple device types | |
US8970495B1 (en) | Image stabilization for color-sequential displays | |
US10269139B2 (en) | Computer program, head-mounted display device, and calibration method | |
US11269402B1 (en) | User interface interaction paradigms for eyewear device with limited field of view | |
US11314323B2 (en) | Position tracking system for head-mounted displays that includes sensor integrated circuits | |
US20160117864A1 (en) | Recalibration of a flexible mixed reality device | |
JP6732617B2 (en) | Information processing apparatus and image generation method | |
AU2014281726A1 (en) | Virtual object orientation and visualization | |
EP4073618A1 (en) | Content stabilization for head-mounted displays | |
CN104536579A (en) | Interactive three-dimensional scenery and digital image high-speed fusing processing system and method | |
KR102653796B1 (en) | Generation of shockwaves in 3D depth videos and images | |
CN112655202B (en) | Reduced bandwidth stereoscopic distortion correction for fisheye lenses of head-mounted displays | |
KR20210052570A (en) | Determination of separable distortion mismatch | |
WO2022005734A1 (en) | Rolling shutter camera pipeline exposure timestamp error determination | |
US11982808B2 (en) | Extended field-of-view capture of augmented reality experiences | |
US20220373796A1 (en) | Extended field-of-view capture of augmented reality experiences | |
US20230217007A1 (en) | Hyper-connected and synchronized ar glasses | |
US11899204B2 (en) | Soft follow and pitch angle effects for VR/AR interface | |
US11902107B2 (en) | Eyewear experience hub for network resource optimization | |
US20240094822A1 (en) | Ar glasses as iot remote control |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |