US9280742B1 - Conceptual enhancement of automatic multimedia annotations - Google Patents
Conceptual enhancement of automatic multimedia annotations Download PDFInfo
- Publication number
- US9280742B1 US9280742B1 US13/604,590 US201213604590A US9280742B1 US 9280742 B1 US9280742 B1 US 9280742B1 US 201213604590 A US201213604590 A US 201213604590A US 9280742 B1 US9280742 B1 US 9280742B1
- Authority
- US
- United States
- Prior art keywords
- classifier
- semantic tags
- media clip
- suggested
- feature vector
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 239000013598 vector Substances 0.000 claims abstract description 60
- 238000000034 method Methods 0.000 claims abstract description 36
- 230000015654 memory Effects 0.000 claims description 18
- 238000012545 processing Methods 0.000 claims description 13
- 238000012549 training Methods 0.000 description 16
- 238000010586 diagram Methods 0.000 description 11
- 238000012706 support-vector machine Methods 0.000 description 8
- 230000005291 magnetic effect Effects 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 5
- 241000167854 Bourreria succulenta Species 0.000 description 4
- 241000282326 Felis catus Species 0.000 description 4
- 238000013528 artificial neural network Methods 0.000 description 4
- 238000004422 calculation algorithm Methods 0.000 description 4
- 235000019693 cherries Nutrition 0.000 description 4
- 238000003066 decision tree Methods 0.000 description 4
- 238000004590 computer program Methods 0.000 description 3
- 239000000284 extract Substances 0.000 description 3
- 238000013507 mapping Methods 0.000 description 3
- 241000234295 Musa Species 0.000 description 2
- 235000018290 Musa x paradisiaca Nutrition 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 238000004519 manufacturing process Methods 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 241000220225 Malus Species 0.000 description 1
- 235000011430 Malus pumila Nutrition 0.000 description 1
- 235000015103 Malus silvestris Nutrition 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 230000003203 everyday effect Effects 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000008569 process Effects 0.000 description 1
- 239000000126 substance Substances 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 230000008685 targeting Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
- G11B27/28—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/25—Fusion techniques
- G06F18/254—Fusion techniques of classification results, e.g. of results related to same input data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/74—Image or video pattern matching; Proximity measures in feature spaces
- G06V10/75—Organisation of the matching processes, e.g. simultaneous or sequential comparisons of image or video features; Coarse-fine approaches, e.g. multi-scale approaches; using context analysis; Selection of dictionaries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
Definitions
- aspects and implementations of the present disclosure relate to data processing, and more specifically, to managing video clips or other types of data.
- Video is becoming pervasive on the World Wide Web.
- professional content providers e.g., news organizations, media companies, etc.
- users of such websites may subscribe to be notified of or receive video clips uploaded by or recommended by other users and conveniently view the video clips uploaded by or recommended by these other users.
- User-generated video clips are typically recorded with digital video cameras and digital still cameras that have video capability, and increasingly, using wireless terminals (e.g., smartphones, etc.) that have still image and video capabilities.
- Users may apply one or more “semantic tags” to video clips that characterize the video clips and enable other users to find the video clip when submitting keyword searches to the content hosting website. For example, a user who uploads a video clip of an amusing incident between her cat and her baby might apply the semantic tags “cat,” “baby,” and “funny” to the video clip.
- a method for suggesting semantic tags for media content includes identifying a media clip (e.g., a video clip, an audio clip, a clip containing a combination of video and audio, a video frame, a picture, etc.) that is provided by a user, and generating a first set of semantic tags for the media clip based on a feature vector associated with the media clip.
- a media clip e.g., a video clip, an audio clip, a clip containing a combination of video and audio, a video frame, a picture, etc.
- the method may then include providing the first set of semantic tags to a classifier (e.g., a support vector machine [SVM], AdaBoost, a neural network, a decision tree, etc.) that is trained based on user selection of semantic tags, obtaining a second set of semantic tags from the classifier, and suggesting to the user the second set of semantic tags for the media clip.
- a classifier e.g., a support vector machine [SVM], AdaBoost, a neural network, a decision tree, etc.
- an apparatus to suggest semantic tags for media content includes a memory to store a media clip and a processing device, coupled to the memory, to input a first set of semantic tags for the media clip to a classifier, where the first set of semantic tags is generated based on a feature vector associated with the media clip, to run the classifier to obtain a second set of semantic tags, and to present the second set of semantic tags to a user as suggestions for the media clip.
- a computer readable storage medium has instructions that cause a computer system to perform operations including: training a classifier on a plurality of input-output mappings, where each of the input-output mappings is associated with a respective media clip and each of the input-output mappings includes: (1) a respective input based on a first set of semantic tags suggested for the respective media clip, the first set of semantic tags generated based on a feature vector associated with the respective media clip, and (2) a respective output based on which one or more semantic tags of the first set of semantic tags are selected; and running the trained classifier to obtain a first set of semantic tags from a second set of semantic tags associated with a media clip.
- FIG. 1 illustrates an exemplary system architecture, in accordance with one implementation of the present disclosure.
- FIG. 2 is a block diagram of one implementation of a media clip manager.
- FIG. 3 depicts a flow diagram of aspects of a method for training a classifier.
- FIG. 4 depicts a flow diagram of aspects of a method for suggesting semantic tags for a media clip.
- FIG. 5 is a block diagram illustrating an example flow illustrating aspects of this disclosure.
- FIG. 6 depicts a block diagram of an illustrative computer system operating in accordance with aspects and implementations of the present disclosure.
- a media clip can include a video clip, an audio clip, a clip containing a combination of video and audio, a video frame, a picture, etc.
- the server generates a feature vector based on the media clip.
- the feature vector can include one or more values based on one or more features of the media clip (e.g., color, texture, intensity, etc.).
- the feature vector is then input to a set of one or more classifiers (e.g., support vector machines [SVMs], AdaBoost, neural networks, decision trees, etc.), referred to herein as a “stage 1 classifier set,” to obtain a first set of semantic tags based on the feature vector.
- the server uses another classifier, referred to herein as the “stage 2 classifier,” to generate an improved, second set of semantic tags from the first set of semantic tags.
- the stage 2 classifier may be a classifier that has been trained on multiple input-output pairs, where the input represents a set of semantic tags that previously have been suggested to one or more users for a particular media clip, and the output represents which of these tags were selected.
- the stage 2 classifier learns from past user selections of tags and is able to improve the set of semantic tags generated by the stage 1 classifier set by confirming relevant tags, eliminating irrelevant tags, and predicting additional relevant tags. Consequently, a more appropriate set of tags for the media clip can be suggested to a user.
- stage 1 classifier set generates the following set of semantic tags based on a feature vector for a particular media clip:
- the computer system presents the second set of semantic tags generated by the stage 2 classifier to the user as suggestions for the media clip, and allows the user to select one or more of the tags. Upon receiving the user's selections, the computer system can then apply the selected tags to the media clip and store the media clip in a data store, as permitted by the user.
- aspects and implementations of the present disclosure are thus capable of suggesting appropriate semantic tags to users who upload media clips to a content hosting website.
- This facility enables users to easily select semantic tags for the media clip, thereby increasing the likelihood that other users will find the media clip when submitting keyword searches, and facilitating the delivery of, for example, relevant advertisements to viewers of the media clip.
- users may not bother to tag the media clips, or may apply tags that are inappropriate or irrelevant.
- aspects and implementations of the present disclosure can thus improve the quality of content descriptions, which in turn can improve the quality of searches and the targeting of advertisements. While aspects and implementations of the present disclosure are described with reference to media clips, aspects and implementations of the present disclosure also apply to other types of content items, such as webpages, textual documents, and so forth.
- FIG. 1 illustrates an example system architecture 100 , in accordance with one implementation of the present disclosure.
- the system architecture 100 includes a server machine 115 , a media clip store 120 and client machines 102 A- 102 N connected to a network 104 .
- Network 104 may be a public network (e.g., the Internet), a private network (e.g., a local area network (LAN) or wide area network (WAN)), or a combination thereof.
- LAN local area network
- WAN wide area network
- the client machines 102 A- 102 N may be wireless terminals (e.g., smartphones, etc.), personal computers (PC), laptops, tablet computers, or any other computing or communication devices.
- the client machines 102 A- 102 N may run an operating system (OS) that manages hardware and software of the client machines 102 A- 102 N.
- a browser (not shown) may run on the client machines (e.g., on the OS of the client machines).
- the browser may be a web browser that can access content served by a web server.
- the browser may issue image and/or video search queries to the web server or may enable browsing of images and/or videos that have previously been searched.
- the client machines 102 A- 102 N may also upload images and/or video to the web server for storage and/or classification.
- Server machine 115 may be a rackmount server, a router computer, a personal computer, a portable digital assistant, a mobile phone, a laptop computer, a tablet computer, a camera, a video camera, a netbook, a desktop computer, a media center, or any combination of the above.
- Server machine 115 includes a web server 140 and a media clip manager 125 .
- the web server 140 and media clip manager 125 may run on different machines.
- Media clip store 120 is a persistent storage that is capable of storing media clips (e.g., video clips, audio clips, clips containing both video and audio, images, etc.) and other types of content items (e.g., webpages, text-based documents, etc.), as well as data structures to tag, organize, and index the media clips and other types of content.
- Media clip store 210 may be hosted by one or more storage devices, such as main memory, magnetic or optical storage based disks, tapes or hard drives, NAS, SAN, and so forth.
- media clip store 120 might be hosted by a network-attached file server, while in other implementations media clip store 120 may be hosted by some other type of persistent storage such as that of the server machine 115 or one or more different machines coupled to the server machine 115 via the network 104 .
- the media clips stored in the media clip store 120 may include user-generated content that is uploaded by client machines.
- the media clips may additionally or alternatively include content provided by service providers such as news organizations, publishers, libraries and so on.
- Web server 140 may serve content from media clip store 120 to clients 102 A- 102 N. Web server 140 may receive search queries and perform searches on the contents of the media clip store 120 to identify content items that satisfy the search query. Web server 140 may then send to a client 102 A- 102 N those content items that match the search query.
- media clip manager 125 extracts features (e.g., color, texture, intensity, etc.) from uploaded media clips to generate feature vectors, creates semantic tags based on the feature vectors, and suggests the semantic tags to users.
- Media clip manager 125 can further receive input from users specifying user selection among the suggested semantic tags, apply selected tags to media clips, store the media clips and the semantic tags in media clip store 120 as permitted by the users, and index the media clips in media clip store 120 using the semantic tags.
- An implementation of media clip manager 125 is described in detail below with respect to FIGS. 2 through 5 .
- FIG. 2 is a block diagram of one implementation of a media clip manager 200 .
- the media clip manager 200 may be the same as the media clip manager 125 of FIG. 1 and may include a feature extractor 202 , a stage 1 classifier set 204 , a stage 2 classifier 206 , an input/output manager 208 , and a data store 210 .
- the components can be combined together or separated in further components, according to a particular implementation.
- the data store 210 may be the same as media clip store 120 or a different data store (e.g., a temporary buffer or a permanent data store) to hold one or more media clips that are to be processed, one or more data structures for indexing media clips in media clip store 120 , semantic tags generated by stage 1 classifier set 204 and stage 2 classifier 206 , user selections of semantic tags, or some combination of these data.
- Data store 210 may be hosted by one or more storage devices, such as main memory, magnetic or optical storage based disks, tapes or hard drives, and so forth.
- the media clip manager 200 notifies users of the types of information that are stored in the data store 210 and/or media clip store 120 , and provides the users the opportunity to opt-out of having such information collected and/or shared with the media clip manager 200 ,
- the feature extractor 202 obtains a feature vector for a media clip using one or more techniques such as principal components analysis, semidefinite embeddings, Isomaps, partial least squares, and so forth.
- the computations associated with extracting features of a media clip are performed by feature extractor 302 itself, while in some other aspects these computations are performed by another entity (e.g., an executable library of image processing routines [not depicted in the Figures] hosted by server machine 115 , etc.) and the results are provided to feature extractor 202 .
- the stage 1 classifier set 204 is a set of one or more classifiers (e.g., support vector machines [SVMs], AdaBoost, neural networks, decision trees, etc.) that accepts as input a feature vector associated with a media clip, and outputs a set of semantic tags for the media clip.
- stage 1 classifier set 204 consists of a single classifier, while in some other implementations, stage 1 classifier set 204 includes multiple classifiers (e.g., a classifier for every type of semantic tag in a tag dictionary, etc.).
- the feature extractor 202 assembles, for each semantic tag in a tag dictionary, a set of positive examples and a set of negative examples.
- the set of positive examples for a semantic tag may include feature vectors for video clips that have been tagged with that particular semantic tag.
- the set of negative examples for a semantic tag may include feature vectors for video clips that have not been tagged with that particular semantic tag.
- the larger set is randomly sampled to match the size of the smaller set.
- the stage 1 classifier set 204 may then be trained for each tag using the feature vectors of corresponding positive and negative examples.
- the output of stage 1 classifier set 204 comprises a numerical score for each semantic tag indicating how strongly the tag is applicable to the media clip (e.g., a real number between 0 and 1 inclusive, etc.).
- the set of semantic tags may then be obtained from the output of stage 1 classifier set 204 by applying a minimum threshold to the numerical scores (e.g., by considering all tags that have a score of at least, say, 0.3, as being a member of the set).
- the output of stage 1 classifier set 204 comprises a binary value for each semantic tag in the dictionary which indicates that the tag is either definitely in or definitely out of the set.
- Input/output manager 208 may then present the resulting set of semantic tags (“suggested semantic tags”) for the media clip to a user (e.g., the owner of the media clip). The user may then select one or more semantic tags from the set and add the selected tags to the description of the media clip. The suggested semantic tags and the selected semantic tags may be stored in the data store 210 .
- the stage 2 classifier 206 is a classifier (e.g., support vector machine [SVM], AdaBoost, neural network, decision tree, etc.) that accepts as input a first set of semantic tags, and, if available, respective scores for the first set of semantic tags, and outputs a second set of semantic tags, and, optionally, respective scores for the second set of semantic tags.
- stage 2 classifier 206 is trained on multiple input-output pairs, where the input includes a set of semantic tags that previously have been suggested to a user for a particular media clip, and where the output comprises the tags that were selected by the user.
- stage 2 classifier 206 includes multiple classifiers, as is the case for stage 1, rather than a single classifier.
- the input/output manager 208 identifies when users have uploaded media clips to server 115 , presents suggested semantic tags to users, receives users' selections, applies selected tags to media clips, and stores tagged media clips in the data store 210 and/or media clip store 120 .
- FIG. 3 depicts a flow diagram of aspects of a method 300 for training stage 2 classifier 206 .
- the method is performed by processing logic that may comprise hardware (circuitry, dedicated logic, etc.), software (such as is run on a general purpose computer system or a dedicated machine), or a combination of both.
- processing logic may comprise hardware (circuitry, dedicated logic, etc.), software (such as is run on a general purpose computer system or a dedicated machine), or a combination of both.
- the methods are depicted and described as a series of acts. However, acts in accordance with this disclosure can occur in various orders and/or concurrently, and with other acts not presented and described herein. Furthermore, not all illustrated acts may be required to implement the methods in accordance with the disclosed subject matter.
- those skilled in the art will understand and appreciate that the methods could alternatively be represented as a series of interrelated states via a state diagram or events.
- a training set of input-output pairs is generated, where the input is based on one or more semantic tags that previously have been suggested to a user for a particular media clip, and where the output is based on the tag(s) that were selected by the user.
- stage 2 classifier 206 is trained on the training set generated at block 301 .
- a notification is received that indicates that one or more input-output pairs have been added to the training set. (As described in detail below, the adding of input-output pairs to the training set is performed at block 409 of the method of FIG. 4 .)
- stage 2 classifier 206 is trained on the updated training set. After block 304 , execution continues back at block 303 .
- FIG. 4 depicts a flow diagram of aspects of a method 400 for suggesting semantic tags for a media clip.
- the method is performed by processing logic that may comprise hardware (circuitry, dedicated logic, etc.), software (such as is run on a general purpose computer system or a dedicated machine), or a combination of both.
- the method is performed by the server machine 115 of FIG. 1 , while in some other implementations, one or more of blocks 401 through 409 are performed by another machine.
- various components of media clip manager 200 run on separate machines (e.g., feature extractor 202 and stage 1 classifier set 204 might run on one machine while stage 2 classifier 206 and input/output manager 208 might run on another machine, etc.).
- block 401 a media clip that is uploaded by a user to server 115 is identified.
- block 401 is performed by input/output manager 208 .
- a feature vector is generated for the media clip.
- block 402 is performed by feature extractor 202 .
- a first set of semantic tags for the media clip is generated based on the feature vector.
- respective scores for each of the tags are also generated, while in some other aspects, no such scores are generated. (It should be noted that the case in which no scores are generated can be considered a special case in which all of the scores are equal.)
- block 403 is performed by stage 1 classifier set 204 .
- an input for stage 1 classifier set 204 may be generated based on the feature vector generated at block 402 ; stage 1 classifier set 204 may generate an output based on this input; and the first set of semantic tags, along with respective scores, may be generated based on the output.
- the input may be the values of the feature vector itself, or alternatively the input values may be derived from the feature vector in some fashion (e.g., by normalizing the values, by applying some function to the values, by combining some of the values in some manner, etc.).
- the output of stage 1 classifier set 204 may be scores for each of the semantic tags in a dictionary, and the first set of semantic tags may be generated by including only those tags whose scores are above a threshold. Alternatively, the first set of semantic tags may be obtained from the output of stage 1 classifier set 204 in some other fashion. It should also be noted that in some other aspects, an alternative technique is employed at block 403 to generate the set of semantic tags (e.g., a rule-based system that does not rely on a classifier, etc.).
- a second set of semantic tags for the media clip is generated based on the first set of semantic tags, and optionally based on the scores for each of the first set of tags (if generated at block 403 ) as well.
- respective scores for each of the second set of tags are also generated at block 404 , while in some other aspects, no such scores are generated.
- the tags may be treated equally [i.e., as though their scores are equal]).
- block 404 is performed by stage 2 classifier 206 .
- the scores of semantic tags from the first set i.e., those generated at block 403
- stage 2 classifier 206 may generate a new set of scores, each corresponding to a respective semantic tag in the dictionary.
- the second set of semantic tags can then be generated by including only those tags whose scores are above a threshold.
- a classifier may be used in an alternative fashion at block 404 to generate the second set of semantic tags (e.g., representing the first set of semantic tags without associated scores, generating binary outputs for membership in the second set of semantic tags, etc.).
- a technique other than a classifier e.g., a rule-based system, etc. may be employed at block 404 to generate the second set of semantic tags.
- the second set of semantic tags generated at block 404 (possibly ordered by scores, if scores were generated at block 404 ) is suggested to the user, and at block 406 the user's selection(s) from the second set of semantic tags are received.
- blocks 405 and 406 are performed by input/output manager 208 .
- the semantic tags selected by the user and received at block 406 are associated with (e.g., applied or added to) the media clip, and at block 408 the tagged media clip is stored in media clip store 120 .
- blocks 407 and 408 are performed by input/output manager 208 .
- an input-output pair based on the second set of semantic tags and the tags selected by the user is added to a training set for future re-training of stage 2 classifier 206 .
- the input is a representation of the second set of semantic tags
- the output is a representation of the user-selected tags.
- re-training of the stage 2 classifier 206 may occur at defined time intervals (e.g., periodically, etc.), while in some other aspects re-training may occur whenever a certain number of input-output pairs have been added to the training set, while in yet other aspects stage 2 classifier 206 may be continually re-trained as input-output pairs are added to the training set.
- users may be randomly presented with tags from stage 1 classifier set 204 ; this increases the diversity of the training set, which can reduce self-reinforcement learning and overfitting.
- FIG. 5 is a block diagram illustrating an example flow to further illustrate aspects of this disclosure. While the flow is described below with reference to a video, the flow can be applicable to other content as well, such as an image, music, etc.
- the system 500 includes a stage 1 classifier (content classifier 504 ) and a stage 2 classifier (meta classifier 506 ).
- System 500 can also include a data store 508 that stores a tag dictionary 509 .
- System 500 can also include a data store 510 that stores videos 511 , such as previously uploaded by one or more users.
- Data store 508 and data store 510 can be different data stores or parts of the same data store.
- the data store 508 and 510 are operatively coupled to or otherwise accessible by the content classifier 504 and meta classifier 506 .
- the content classifier 504 extracts feature vectors from video content such as color, texture, audio, etc. For each tag (or label) in the tag dictionary 509 , feature vectors for videos stored in data store 510 that are associated with the tag are identified for use as positive examples (a positive example set). In one implementation, also for each tag in the tag dictionary 509 , features vectors for videos stored in data store 510 that are not associated with the tag are identified for use as negative examples (a negative set). When the size of the negative example set is large, a random sample of the negative example set can be used instead. In one implementation, the size of the random sample is selected to match the size of the positive example set. The content classifier 504 is trained for each tag using the video content features of corresponding positive and negative examples.
- a user 502 transmits a video 503 to the content classifier 504 .
- the content classifier extracts one or more feature vectors from the video 503 content such as color, texture, audio, etc.
- the trained content classifier uses the extracted feature vector(s) to identify one or more tags. For example, the trained content classifier can identify tags associated with videos stored in the data store 510 that have similar feature vectors.
- the trained classifier can score each identified tag based on a calculation of the similarity of the feature vector(s) already associated with the tag to the extracted feature vector of the video 503 , generating a similarity score for each identified tag.
- the content classifier 504 outputs suggested tags 505 .
- the suggested tags 505 can be the identified tags, or a subset thereof based on, for example, a threshold similarity score. In one implementation, the content classifier 504 outputs suggested tags in rank order, based on the similarity scores.
- the suggested tags can be presented to the user 502 . For example, a display can be presented to the user with an option to accept zero or more of the suggested tags as a description of the video.
- the suggested tags 505 and accepted tags 507 can be recorded for the video 503 , as well as for other videos similarly processed.
- the meta classifier 506 can be trained using the suggested tags 505 as feature vectors and the accepted tags 507 as labels.
- the meta classifier 506 can be used to re-estimate scores for tags in the tag dictionary 509 given tags which are identified by the content classifier based on the feature vectors.
- the meta classifier 506 can filter out those tags identified by the content classifier 504 that are conceptually irrelevant and can predict conceptually relevant tags given the context of the tags 505 suggested by the content classifier 504 . Accordingly, the meta classifier 506 can present users with conceptually enhanced suggested tags 512 .
- FIG. 6 illustrates an exemplary computer system within which a set of instructions, for causing the machine to perform any one or more of the methodologies discussed herein, may be executed.
- the machine may be connected (e.g., networked) to other machines in a LAN, an intranet, an extranet, or the Internet.
- the machine may operate in the capacity of a server machine in client-server network environment.
- the machine may be a personal computer (PC), a tablet computer, a mobile device, a set-top box (STB), a server, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine.
- PC personal computer
- STB set-top box
- STB set-top box
- server a network router, switch or bridge
- any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine.
- the term “machine” shall also be taken to include any collection
- the exemplary computer system 600 includes a processing system (processor) 602 , a main memory 604 (e.g., read-only memory (ROM), flash memory, dynamic random access memory (DRAM) such as synchronous DRAM (SDRAM)), a static memory 606 (e.g., flash memory, static random access memory (SRAM)), and a data storage device 616 , which communicate with each other via a bus 608 .
- processor processing system
- main memory 604 e.g., read-only memory (ROM), flash memory, dynamic random access memory (DRAM) such as synchronous DRAM (SDRAM)
- DRAM dynamic random access memory
- SDRAM synchronous DRAM
- static memory 606 e.g., flash memory, static random access memory (SRAM)
- SRAM static random access memory
- Processor 602 represents one or more general-purpose processing devices such as a microprocessor, central processing unit, or the like. More particularly, the processor 602 may be a complex instruction set computing (CISC) microprocessor, reduced instruction set computing (RISC) microprocessor, very long instruction word (VLIW) microprocessor, or a processor implementing other instruction sets or processors implementing a combination of instruction sets.
- the processor 602 may also be one or more special-purpose processing devices such as an application specific integrated circuit (ASIC), a field programmable gate array (FPGA), a digital signal processor (DSP), network processor, or the like.
- the processor 602 is configured to execute instructions 626 for performing the operations and steps discussed herein.
- the computer system 600 may further include a network interface device 622 .
- the computer system 600 also may include a video display unit 610 (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)), an alphanumeric input device 612 (e.g., a keyboard), a cursor control device 614 (e.g., a mouse), and a signal generation device 620 (e.g., a speaker).
- a video display unit 610 e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)
- an alphanumeric input device 612 e.g., a keyboard
- a cursor control device 614 e.g., a mouse
- a signal generation device 620 e.g., a speaker
- the data storage device 616 may include a computer-readable medium 624 on which is stored one or more sets of instructions 626 (e.g., instructions executed by media clip manager 125 and corresponding to blocks 601 through 605 and 601 through 606 , etc.) embodying any one or more of the methodologies or functions described herein. Instructions 626 may also reside, completely or at least partially, within the main memory 604 and/or within the processor 602 during execution thereof by the computer system 600 , the main memory 604 and the processor 602 also constituting computer-readable media. Instructions 626 may further be transmitted or received over a network via the network interface device 622 .
- instructions 626 e.g., instructions executed by media clip manager 125 and corresponding to blocks 601 through 605 and 601 through 606 , etc.
- While the computer-readable storage medium 624 is shown in an exemplary implementation to be a single medium, the term “computer-readable storage medium” should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions.
- the term “computer-readable storage medium” shall also be taken to include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present disclosure.
- the term “computer-readable storage medium” shall accordingly be taken to include, but not be limited to, solid-state memories, optical media, and magnetic media.
- Implementations of the disclosure also relate to an apparatus for performing the operations herein.
- This apparatus may be specially constructed for the required purposes, or it may include a general purpose computer selectively activated or reconfigured by a computer program stored in the computer.
- a computer program may be stored in a computer readable storage medium, such as, but not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, and magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, or any type of media suitable for storing electronic instructions.
- example or “exemplary” are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example’ or “exemplary” is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the words “example” or “exemplary” is intended to present concepts in a concrete fashion.
- the term “or” is intended to mean an inclusive “or” rather than an exclusive “or”. That is, unless specified otherwise, or clear from context, “X includes A or B” is intended to mean any of the natural inclusive permutations.
Abstract
Description
-
- cat
- kitten
- singing
- yodeling
- funny
which are then input to thestage 2 classifier. Thestage 2 classifier might then generate the following tags to be suggested to the user for the media clip: - cat
- singing
- talking
- funny
Claims (17)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/604,590 US9280742B1 (en) | 2012-09-05 | 2012-09-05 | Conceptual enhancement of automatic multimedia annotations |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/604,590 US9280742B1 (en) | 2012-09-05 | 2012-09-05 | Conceptual enhancement of automatic multimedia annotations |
Publications (1)
Publication Number | Publication Date |
---|---|
US9280742B1 true US9280742B1 (en) | 2016-03-08 |
Family
ID=55410471
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/604,590 Active 2033-04-16 US9280742B1 (en) | 2012-09-05 | 2012-09-05 | Conceptual enhancement of automatic multimedia annotations |
Country Status (1)
Country | Link |
---|---|
US (1) | US9280742B1 (en) |
Cited By (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105677735A (en) * | 2015-12-30 | 2016-06-15 | 腾讯科技（深圳）有限公司 | Video search method and apparatus |
US20170270406A1 (en) * | 2016-03-18 | 2017-09-21 | Qualcomm Incorporated | Cloud-based processing using local device provided sensor data and labels |
US20170300533A1 (en) * | 2016-04-14 | 2017-10-19 | Baidu Usa Llc | Method and system for classification of user query intent for medical information retrieval system |
US20170344637A1 (en) * | 2016-05-31 | 2017-11-30 | International Business Machines Corporation | Dynamically tagging webpages based on critical words |
US20180089497A1 (en) * | 2016-09-27 | 2018-03-29 | Apical Ltd | Image processing |
US20190037270A1 (en) * | 2017-07-31 | 2019-01-31 | Zhilabs S.L. | Determination of qoe in encrypted video streams using supervised learning |
CN110399505A (en) * | 2018-04-17 | 2019-11-01 | 华为技术有限公司 | Semantic label generation method and equipment, computer storage medium |
US10534981B2 (en) * | 2015-08-12 | 2020-01-14 | Oath Inc. | Media content analysis system and method |
US10635750B1 (en) * | 2014-04-29 | 2020-04-28 | Google Llc | Classification of offensive words |
CN111414494A (en) * | 2020-02-17 | 2020-07-14 | 北京达佳互联信息技术有限公司 | Multimedia work display method and device, electronic equipment and storage medium |
US10860860B1 (en) * | 2019-01-03 | 2020-12-08 | Amazon Technologies, Inc. | Matching videos to titles using artificial intelligence |
US20210209147A1 (en) * | 2020-01-06 | 2021-07-08 | Strengths, Inc. | Precision recall in voice computing |
WO2021136058A1 (en) * | 2019-12-31 | 2021-07-08 | 华为技术有限公司 | Video processing method and device |
US11109103B2 (en) * | 2019-11-27 | 2021-08-31 | Rovi Guides, Inc. | Systems and methods for deep recommendations using signature analysis |
US11170003B2 (en) * | 2008-08-15 | 2021-11-09 | Ebay Inc. | Sharing item images based on a similarity score |
CN113711616A (en) * | 2019-05-23 | 2021-11-26 | 谷歌有限责任公司 | Cross-platform content muting |
US11297388B2 (en) | 2019-11-27 | 2022-04-05 | Rovi Guides, Inc. | Systems and methods for deep recommendations using signature analysis |
US20220132222A1 (en) * | 2016-09-27 | 2022-04-28 | Clarifai, Inc. | Prediction model training via live stream concept association |
US20230056418A1 (en) * | 2018-07-05 | 2023-02-23 | Movidius Limited | Video surveillance with neural networks |
-
2012
- 2012-09-05 US US13/604,590 patent/US9280742B1/en active Active
Non-Patent Citations (3)
Title |
---|
Alepidou et al ("A Semantic Tag Recommendation Framework for Collaborative Tagging Systems" IEEE Oct. 9-Oct. 11, 2011). * |
Li et al ("Personalizing Automated Image Annotation using Cross-Entropy" 2011). * |
Toderici et al ("Finding Meaning on YouTube: Tag Recommendation and Category Discovery" 2010). * |
Cited By (34)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11170003B2 (en) * | 2008-08-15 | 2021-11-09 | Ebay Inc. | Sharing item images based on a similarity score |
US10635750B1 (en) * | 2014-04-29 | 2020-04-28 | Google Llc | Classification of offensive words |
US10534981B2 (en) * | 2015-08-12 | 2020-01-14 | Oath Inc. | Media content analysis system and method |
CN105677735A (en) * | 2015-12-30 | 2016-06-15 | 腾讯科技（深圳）有限公司 | Video search method and apparatus |
US20180025079A1 (en) * | 2015-12-30 | 2018-01-25 | Tencent Technology (Shenzhen) Company Limited | Video search method and apparatus |
US10642892B2 (en) * | 2015-12-30 | 2020-05-05 | Tencent Technology (Shenzhen) Company Limited | Video search method and apparatus |
US20170270406A1 (en) * | 2016-03-18 | 2017-09-21 | Qualcomm Incorporated | Cloud-based processing using local device provided sensor data and labels |
US20170300533A1 (en) * | 2016-04-14 | 2017-10-19 | Baidu Usa Llc | Method and system for classification of user query intent for medical information retrieval system |
US20170344637A1 (en) * | 2016-05-31 | 2017-11-30 | International Business Machines Corporation | Dynamically tagging webpages based on critical words |
US11275805B2 (en) | 2016-05-31 | 2022-03-15 | International Business Machines Corporation | Dynamically tagging webpages based on critical words |
US10459994B2 (en) * | 2016-05-31 | 2019-10-29 | International Business Machines Corporation | Dynamically tagging webpages based on critical words |
US10489634B2 (en) * | 2016-09-27 | 2019-11-26 | Apical Limited And University Of Leicester | Image processing |
GB2554435B (en) * | 2016-09-27 | 2019-10-23 | Univ Leicester | Image processing |
GB2554435A (en) * | 2016-09-27 | 2018-04-04 | Univ Leicester | Image processing |
US11917268B2 (en) * | 2016-09-27 | 2024-02-27 | Clarifai, Inc. | Prediction model training via live stream concept association |
CN107871130B (en) * | 2016-09-27 | 2023-04-18 | Arm有限公司 | Image processing |
US20220132222A1 (en) * | 2016-09-27 | 2022-04-28 | Clarifai, Inc. | Prediction model training via live stream concept association |
CN107871130A (en) * | 2016-09-27 | 2018-04-03 | 顶级公司 | Image procossing |
US20180089497A1 (en) * | 2016-09-27 | 2018-03-29 | Apical Ltd | Image processing |
US20190037270A1 (en) * | 2017-07-31 | 2019-01-31 | Zhilabs S.L. | Determination of qoe in encrypted video streams using supervised learning |
US11234048B2 (en) * | 2017-07-31 | 2022-01-25 | Zhilabs S.L. | Determination of QOE in encrypted video streams using supervised learning |
CN110399505A (en) * | 2018-04-17 | 2019-11-01 | 华为技术有限公司 | Semantic label generation method and equipment, computer storage medium |
US20230056418A1 (en) * | 2018-07-05 | 2023-02-23 | Movidius Limited | Video surveillance with neural networks |
US10860860B1 (en) * | 2019-01-03 | 2020-12-08 | Amazon Technologies, Inc. | Matching videos to titles using artificial intelligence |
CN113711616A (en) * | 2019-05-23 | 2021-11-26 | 谷歌有限责任公司 | Cross-platform content muting |
US11586663B2 (en) | 2019-05-23 | 2023-02-21 | Google Llc | Cross-platform content muting |
US20210360321A1 (en) * | 2019-11-27 | 2021-11-18 | Rovi Guides, Inc. | Systems and methods for deep recommendations using signature analysis |
US11109103B2 (en) * | 2019-11-27 | 2021-08-31 | Rovi Guides, Inc. | Systems and methods for deep recommendations using signature analysis |
US11297388B2 (en) | 2019-11-27 | 2022-04-05 | Rovi Guides, Inc. | Systems and methods for deep recommendations using signature analysis |
US11509963B2 (en) * | 2019-11-27 | 2022-11-22 | Rovi Guides, Inc. | Systems and methods for deep recommendations using signature analysis |
CN113128285A (en) * | 2019-12-31 | 2021-07-16 | 华为技术有限公司 | Method and device for processing video |
WO2021136058A1 (en) * | 2019-12-31 | 2021-07-08 | 华为技术有限公司 | Video processing method and device |
US20210209147A1 (en) * | 2020-01-06 | 2021-07-08 | Strengths, Inc. | Precision recall in voice computing |
CN111414494A (en) * | 2020-02-17 | 2020-07-14 | 北京达佳互联信息技术有限公司 | Multimedia work display method and device, electronic equipment and storage medium |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9280742B1 (en) | Conceptual enhancement of automatic multimedia annotations | |
Alam et al. | Processing social media images by combining human and machine computing during crises | |
Nguyen et al. | Personalized deep learning for tag recommendation | |
US20170098165A1 (en) | Method and Apparatus for Establishing and Using User Recommendation Model in Social Network | |
Liu et al. | Textual query of personal photos facilitated by large-scale web data | |
Liu et al. | Learning domain representation for multi-domain sentiment classification | |
Zhang et al. | Semi-relaxation supervised hashing for cross-modal retrieval | |
Amato et al. | VISIONE at video browser showdown 2023 | |
US20180053097A1 (en) | Method and system for multi-label prediction | |
US9870376B2 (en) | Method and system for concept summarization | |
US20130018827A1 (en) | System and method for automated labeling of text documents using ontologies | |
Zhao et al. | Multi-modal microblog classification via multi-task learning | |
US20180101617A1 (en) | Ranking Search Results using Machine Learning Based Models | |
Kumar et al. | Image sentiment analysis using convolutional neural network | |
CN112703495A (en) | Inferring topics using entity links and ontology data | |
Tous et al. | Automated curation of brand-related social media images with deep learning | |
Rashid et al. | Analysis of streaming data using big data and hybrid machine learning approach | |
Ła̧giewka et al. | Distributed image retrieval with colour and keypoint features | |
US11328218B1 (en) | Identifying subjective attributes by analysis of curation signals | |
Gupta et al. | A matrix factorization framework for jointly analyzing multiple nonnegative data sources | |
CN111382262A (en) | Method and apparatus for outputting information | |
Leksin et al. | Semi-supervised tag extraction in a web recommender system | |
JP5972096B2 (en) | Apparatus, method and program for extracting posts related to contents | |
Al-Barhamtoshy et al. | A data analytic framework for unstructured text | |
Khan et al. | Multimodal rule transfer into automatic knowledge based topic models |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SARGIN, MEHMET EMRE;TODERICI, GEORGE DAN;SIGNING DATES FROM 20120902 TO 20120905;REEL/FRAME:028903/0449 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044566/0657Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |