KR20230118959A - Providing a specific reason for the fulfillment of an Assistant order - Google Patents
Providing a specific reason for the fulfillment of an Assistant order Download PDFInfo
- Publication number
- KR20230118959A KR20230118959A KR1020237023808A KR20237023808A KR20230118959A KR 20230118959 A KR20230118959 A KR 20230118959A KR 1020237023808 A KR1020237023808 A KR 1020237023808A KR 20237023808 A KR20237023808 A KR 20237023808A KR 20230118959 A KR20230118959 A KR 20230118959A
- Authority
- KR
- South Korea
- Prior art keywords
- assistant
- automated assistant
- specific
- command
- user
- Prior art date
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
- G06N5/045—Explanation of inference; Explainable artificial intelligence [XAI]; Interpretable artificial intelligence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/55—Rule-based translation
- G06F40/56—Natural language generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
- G06N5/041—Abduction
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/02—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail using automatic reactions or user delegation, e.g. automatic replies or chatbot-generated messages
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Abstract
여기에 설명된 구현은 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행 및/또는 대체 이행을 수행한(또는 수행하지 않은) 이유에 대한 특정 추론을 유발하는 것과 관련된다. 예를 들어 구현은 어시스턴트 명령을 포함하는 사용자 입력을 수신하고, 어시스턴트 명령의 특정 이행 또는 대체 이행의 수행에 사용될 데이터를 결정하기 위해 사용자 입력을 프로세싱하고, 자동화된 어시스턴트가 데이터를 사용하여 어시스턴트 명령의 특정 이행 또는 대체 이행을 수행하도록 한다. 일부 구현에서, 특정 추론을 포함하는 출력은 특정 추론을 요청하는 추가 사용자 입력에 응답하여 사용자에게 프레젠테이션하기 위해 제공될 수 있다. 일부 구현에서, 선택 가능한 요소는 시각적으로 렌더링될 수 있고, 사용자에 의해 선택될 때 특정 추론을 포함하는 출력이 사용자에게 프레젠테이션하기 위해 제공될 수 있다.Implementations described herein involve inducing specific inferences about why an automated assistant performed (or did not perform) specific implementations and/or alternative implementations of assistant commands. For example, an implementation may receive user input that includes an assistant command, process the user input to determine data to be used for performing a specific or alternative implementation of an assistant command, and allow an automated assistant to use the data to complete an assistant command. to perform a specific or alternative implementation; In some implementations, an output comprising a particular inference can be provided for presentation to a user in response to additional user input requesting the particular inference. In some implementations, a selectable element can be visually rendered, and when selected by a user, output containing specific inferences can be provided for presentation to the user.
Description
본 명세서는 어시스턴트 명령의 이행과 관련하여 특정한 이유를 제공하는 것에 관한 것이다.This specification is directed to providing specific reasons regarding the implementation of an assistant command.
인간은 여기에서 "자동화된 어시스턴트"("챗봇", "대화형 개인 어시스턴트", "지능형 개인 어시스턴트", "개인 음성 어시스턴트", "대화 에이전트" 등으로도 불림)라고 하는 대화형 소프트웨어 애플리케이션을 사용하여 인간 대 컴퓨터 대화에 참여할 수 있다. 예를 들어, 인간(자동화된 어시스턴트와 상호 작용할 때 "사용자"라고 부를 수 있음)은 음성 자연어 입력(즉, 음성 발화)을 자동화된 어시스턴트에 제공할 수 있으며, 이는 경우에 따라 텍스트로 변환된 다음 프로세싱될 수 있고 및/또는 텍스트(예: 타이핑된) 자연어 입력 또는 터치 입력을 제공함으로써 가능하다. 자동화된 어시스턴트는 일반적으로 반응형 사용자 인터페이스 출력(예: 청각적 및/또는 시각적 사용자 인터페이스 출력)을 제공하고, 스마트 네트워크 장치를 제어하고/하거나 다른 작업을 수행함으로써 어시스턴트 명령을 포함하는 이러한 사용자 입력에 응답한다.Humans use interactive software applications referred to herein as "automated assistants" (also called "chatbots", "interactive personal assistants", "intelligent personal assistants", "personal voice assistants", "conversational agents", etc.) to engage in human-to-computer conversations. For example, a human (what we might call a "user" when interacting with an automated assistant) can provide spoken natural language input (i.e., spoken utterances) to an automated assistant, which in some cases is converted to text and then can be processed and/or by providing textual (eg, typed) natural language input or touch input. An automated assistant typically responds to these user inputs, including assistant commands, by providing responsive user interface output (e.g., audible and/or visual user interface output), controlling smart networked devices, and/or performing other tasks. Respond.
자동화된 어시스턴트는 일반적으로 어시스턴트 명령을 포함하는 이러한 사용자 입력을 해석하고 이에 응답하는 파라미터의 파이프라인에 의존한다. 예를 들어, 자동 음성 인식(ASR) 엔진은 음성 발화의 전사(즉, 용어 및/또는 기타 토큰의 시퀀스)와 같은 ASR 출력을 생성하기 위해 사용자의 음성 발화에 대응하는 오디오 데이터를 프로세싱할 수 있다. 또한, 자연어 이해(NLU) 엔진은 음성 발화를 제공하는 사용자의 의도 및 선택적으로 의도와 연관된 파라미터(들)에 대한 슬롯 값(들)과 같은 NLU 출력을 생성하기 위해 ASR 출력을 프로세싱할 수 있다. 더욱이, 이행 엔진은 NLU 출력을 프로세싱하고, 음성 발화에 응답하는 콘텐츠를 얻기 위한 및/또는 음성 발화에 응답하는 동작을 수행하기 위한 구조화된 요청과 같은 이행 출력(fulfillment output)을 생성하는 데 사용될 수 있다. An automated assistant typically relies on a pipeline of parameters to interpret and respond to these user inputs, including assistant commands. For example, an automatic speech recognition (ASR) engine may process audio data corresponding to a user's spoken utterance to produce an ASR output, such as a transcription of the spoken utterance (ie, a sequence of terms and/or other tokens). . Further, a natural language understanding (NLU) engine may process the ASR output to generate NLU output, such as the user's intent and optionally slot value(s) for the parameter(s) associated with the intent providing the spoken utterance. Moreover, the fulfillment engine may be used to process the NLU output and generate fulfillment output, such as structured requests to obtain content in response to spoken utterances and/or to perform actions in response to spoken utterances. there is.
경우에 따라 사용자는 자동화된 어시스턴트가 특정 응답 콘텐츠를 제공하고/하거나 어시스턴트 명령을 포함하는 이러한 사용자 입력 수신에 대한 응답으로 특정 액션을 수행하는 이유를 이해하지 못할 수 있다. 이러한 오해는 자동화된 어시스턴트가 특정 응답 콘텐츠를 제공하지 않거나 이러한 사용자가 사용자 입력을 제공할 때 의도한 특정 액션을 수행하지 않을 때 악화될 수 있다. 예를 들어, 특정 사용자가 자동화된 어시스턴트가 음악을 재생하도록 요청하는 자동화된 어시스턴트로 향하는 사용자 입력을 제공하는 경우, 그러나 자동화된 어시스턴트가 원하지 않는 소프트웨어 애플리케이션을 사용하여 음악을 재생하거나 사용자 입력에 응답하여 검색 결과를 제공하는 경우, 특정 사용자는 자동화된 어시스턴트가 원하는 방식으로 음악을 재생하지 않은 이유에 대해 혼란스러워할 수 있다. 그 결과, 특정 사용자는 동일한 사용자 입력의 다른 인스턴스 또는 원하는 소프트웨어 애플리케이션을 사용하여 음악이 재생되도록 하는 다른 사용자 입력을 포함하는 추가 사용자 입력을 제공할 수 있다. 따라서 특정 사용자와 자동화된 어시스턴트 사이의 사람과 컴퓨터 간의 대화 시간을 연장한다. 또한 자동화된 어시스턴트가 원하는 소프트웨어 애플리케이션을 사용하여 음악을 재생하도록 한 경우에도, 특정 사용자는 자동화된 어시스턴트가 원하는 소프트웨어 애플리케이션을 사용하여 음악이 재생되도록 할 수 있었던 방법을 사용자가 알지 못할 수 있고 자신의 데이터 보안에 대해 우려할 수 있기 때문에 자동화된 어시스턴트를 염려할 수 있다. 따라서, 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행하게 하는 이유를 특정 사용자가 이해할 수 있는 메커니즘을 제공하는 것이 유익할 수 있다.In some cases, the user may not understand why the automated assistant presents certain response content and/or performs certain actions in response to receiving such user input, including assistant commands. This misunderstanding can be exacerbated when automated assistants do not provide specific response content or perform specific intended actions when such users provide user input. For example, if a particular user provides user input directed to the automated assistant requesting that the automated assistant play music, but the automated assistant uses an unsolicited software application to play music or respond to user input. When providing search results, certain users may be confused as to why the automated assistant didn't play the music the way they wanted. As a result, a particular user may provide additional user input, including another instance of the same user input or another user input that causes music to be played using a desired software application. Thus, it extends the human-computer conversation time between a specific user and an automated assistant. Also, even if you have your automated assistant use your preferred software application to play music, a particular user may not know how the automated assistant was able to cause music to play using your preferred software application, and your data You may be concerned about automated assistants because you may be concerned about security. Accordingly, it may be beneficial to provide a mechanism by which a particular user can understand why an automated assistant performs a particular implementation of an assistant command.
본 명세서에 개시된 구현은 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행 및/또는 대체 이행을 수행한(또는 수행하지 않은) 이유에 대한 특정 추론을 야기하는 것에 관한 것이다. 예를 들어, 구현은 어시스턴트 명령을 포함하는 사용자 입력을 수신하고, 어시스턴트 명령의 특정 이행 또는 대체 이행의 수행에 사용될 데이터를 결정하기 위해 사용자 입력을 프로세싱하고, 자동화된 어시스턴트가 데이터를 사용하여 어시스턴트 명령의 특정 이행 또는 대체 이행을 수행하도록 한다. 일부 구현에서, 특정 추론을 포함하는 출력은 특정 추론을 요청하는 추가 사용자 입력에 응답하여 사용자에게 프레젠테이션하기 위해 제공될 수 있다. 일부 구현에서, 하나 이상의 선택 가능한 요소가 시각적으로 렌더링될 수 있고, 하나 이상의 선택 가능한 요소 중 특정 하나가 사용자에 의해 선택될 때, 특정 추론을 포함하는 출력이 사용자에게 프레젠테이션하기 위해 제공될 수 있다. Implementations disclosed herein relate to causing specific inferences about why an automated assistant performed (or did not perform) specific implementations and/or alternative implementations of assistant commands. For example, an implementation may receive user input that includes an assistant command, process the user input to determine data to be used for performing a specific or alternative implementation of an assistant command, and allow an automated assistant to use the data to perform an assistant command. to carry out specific or alternative implementations of In some implementations, an output comprising a particular inference can be provided for presentation to a user in response to additional user input requesting the particular inference. In some implementations, one or more selectable elements may be visually rendered, and when a particular one of the one or more selectable elements is selected by a user, an output containing a particular inference may be provided for presentation to the user.
예를 들어, 클라이언트 장치의 사용자가 "play rock music(록 음악 재생)"라는 음성 발화를 제공한다고 가정한다. 이 예에서, 자동화된 어시스턴트는 음성 발화를 캡처하는 오디오 데이터가 자동 음성 인식(ASR) 모델을 사용하여 프로세싱되도록 하고, 음성 발화에 대응하는 것으로 예측된 음성 가설, 음성 발화에 대응하는 것으로 예측되는 예측 음소와 같은 ASR 출력 및/또는 다른 ASR 출력 및 선택적으로 각각의 음성 가설, 예측된 음소 및/또는 다른 ASR 출력과 관련된 ASR 메트릭을 생성할 수 있다(예: 특정 음성 가설 또는 특정 예상 음소가 음성 발화에 해당할 가능성을 나타냄). 또한, 자동화된 어시스턴트는 자연어 이해(NLU) 모델을 사용하여 ASR 출력을 프로세싱하여 음성 발화를 제공할 때 사용자의 하나 이상의 예상된 의도, 하나 이상의 예측된 의도 각각과 연관된 대응하는 파라미터에 대한 하나 이상의 슬롯 값과 같은 NLU 출력을 생성하거나, 및/또는 다른 NLU 출력 및 선택적으로 각각의 의도, 슬롯 값 및/또는 다른 NLU 출력과 연관된 NLU 메트릭을 생성한다(예를 들어, 특정 의도 및/또는 특정 슬롯 값이 음성 발화를 제공할 때 실제 의도 및/또는 원하는 슬롯 값에 대응할 가능성을 나타내는 것). 이 예에서 자동화된 어시스턴트는 사용자가 특정 음악 장르(예: 록)만 지정했기 때문에 음악 의도와 연관된 아티스트 파라미터에 대한 아티스트 슬롯 값, 음악 의도와 연관된 노래 파라미터에 대한 노래 슬롯 값, 소프트웨어 애플리케이션에 대한 소프트웨어 애플리케이션 또는 스트리밍 서비스 슬롯 값 또는 음악과 연관된 스트리밍 서비스 파라미터 의도 등과 같이, 하나 이상의 슬롯 값을 추론할 수 있다. 유추된 슬롯 값의 변형으로 인해 음성 발화에 대한 하나 이상의 해석이 발생할 수 있다. 다양한 구현에서, 그리고 자동화된 어시스턴트가 클라이언트 장치 사용자의 하나 이상의 사용자 프로필에 액세스할 수 있다고 가정하면, 자동화된 어시스턴트는 하나 이상의 슬롯 값을 추론하는 데 사용자 프로필 데이터를 사용할 수 있다. 그렇지 않으면 자동화된 어시스턴트가 하나 이상의 기본 슬롯 값을 사용할 수 있다. For example, it is assumed that the user of the client device provides a voice utterance "play rock music". In this example, an automated assistant causes audio data capturing a spoken utterance to be processed using an Automatic Speech Recognition (ASR) model, a predicted voice hypothesis corresponding to a spoken utterance, a prediction predicted to correspond to a spoken utterance ASR outputs such as phonemes and/or other ASR outputs and, optionally, ASR metrics associated with each speech hypothesis, predicted phoneme, and/or other ASR outputs (e.g., a specific speech hypothesis or a specific expected phoneme may be associated with a speech utterance). represents the probability of corresponding). In addition, the automated assistant uses a natural language understanding (NLU) model to process the ASR output to provide spoken utterances with one or more expected intents of the user, one or more slots for corresponding parameters associated with each of the one or more predicted intents. generates NLU outputs such as values, and/or other NLU outputs and optionally NLU metrics associated with respective intents, slot values, and/or other NLU outputs (e.g., specific intents and/or specific slot values). indicating actual intent and/or likelihood of corresponding desired slot value when presenting this spoken utterance). In this example, the automated assistant uses the Artist Slot value for the Artist parameter associated with the Music Intent, the Song Slot value for the Song parameter associated with the Music Intent, and the Software Application for the Software Intent because the user has specified only a specific music genre (e.g. Rock). One or more slot values may be inferred, such as application or streaming service slot values or streaming service parameter intents associated with music. Variation of the inferred slot value may result in more than one interpretation of the speech utterance. In various implementations, and assuming that the automated assistant has access to one or more user profiles of the user of the client device, the automated assistant can use the user profile data to infer one or more slot values. Otherwise, the automated assistant may use one or more default slot values.
또한, 자동화된 어시스턴트는 음성 발화를 만족시킬 수 있는 하나 이상의 이행자(예: 소프트웨어 애플리케이션, 서버 등)에게 전송될 하나 이상의 구조화된 요청과 같은, 하나 이상의 이행 규칙(들) 및/또는 이행 모델(들)을 사용하여 NLU 출력이 프로세싱되도록 하여 이행 출력을 생성할 수 있다. 하나 이상의 구조화된 요청을 전송할 때, 하나 이상의 이행자는 하나 이상의 이행 후보를 생성할 수 있고, 하나 이상의 이행 후보를 자동화된 어시스턴트로 다시 전송할 수 있다. 자동화된 어시스턴트는 Additionally, the automated assistant may have one or more fulfillment rule(s) and/or fulfillment model(s), such as one or more structured requests to be sent to one or more fulfillers (e.g., software applications, servers, etc.) capable of satisfying a spoken utterance. ) to allow the NLU output to be processed to generate the transitive output. When sending one or more structured requests, one or more fulfillment candidates can generate one or more fulfillment candidates, and send the one or more fulfillment candidates back to the automated assistant. automated assistants
자동화된 어시스턴트가 클라이언트 장치의 사용자의 하나 이상의 사용자 프로파일에 액세스할 수 있다고 가정하는 사용자 프로파일 데이터에 기초하여 수신되는 하나 이상의 이행 후보에 응답하여 하나 이상의 이행 후보 각각과 연관된 이행 메트릭을 생성할 수 있다(예를 들어, 하나 이상의 이행 후보 중 특정 이행 후보가 구현된 경우 음성 발화를 만족시킬 가능성을 나타냄). 자동화된 어시스턴트는 ASR 메트릭, NLU 메트릭 및/또는 이행 메트릭에 기초하여 하나 이상의 이행 후보의 순위를 매길 수 있고, 그 순위에 따라 특정 이행 후보를 선택한다. 또한, 자동화된 어시스턴트는 음성 발화를 만족시키려는 시도에서 선택된 특정 이행 후보가 구현되도록 할 수 있다. The automated assistant may generate a adherence metric associated with each of the one or more transition candidates in response to the one or more transition candidates received based on user profile data assuming that the automated assistant has access to one or more user profiles of the user of the client device ( eg, indicating the likelihood of satisfying a speech utterance if a particular transition candidate among one or more transition candidates is implemented). The automated assistant may rank one or more transition candidates based on the ASR metric, NLU metric, and/or fulfillment metric, and selects a particular transition candidate according to the ranking. In addition, the automated assistant may cause specific implementation candidates selected in an attempt to satisfy a spoken utterance to be implemented.
예를 들어, 이 예에서, 자동화된 어시스턴트가 음악 의도와 연관된 아티스트 파라미터에 대한 "아티스트 1"의 아티스트 슬롯 값, 음악 의도와 연관된 노래 파라미터에 대한 "노래 1"의 노래 슬롯 값, 음악 의도와 관련된 소프트웨어 애플리케이션 또는 스트리밍 서비스 파라미터에 대한 "애플리케이션 1"의 소프트웨어 애플리케이션 또는 스트리밍 서비스 슬롯 값을 갖는 음악 의도를 포함하는 "play rock music(록 음악 재생)"의 음성 발화의 제1 해석을 결정했다고 가정한다. 또한 자동화된 어시스턴트가 음악 의도와 연관된 아티스트 파라미터에 대해 "아티스트 1"의 아티스트 슬롯 값, 음악 의도와 연관된 노래 파라미터에 대한 "노래 1"의 노래 슬롯 값, 음악 의도와 연관된 소프트웨어 애플리케이션 또는 스트리밍 서비스 파라미터에 대한 "애플리케이션 2"의 소프트웨어 애플리케이션 또는 스트리밍 서비스 슬롯 값을 갖는 음악 의도를 포함하는 "록 음악 재생"의 음성 발화의 제1 해석을 결정했다고 가정한다. 이 경우 "애플리케이션 1" 및 "애플리케이션 2"는 발화를 만족시킬 수 있는 하나 이상의 이행자로 간주될 수 있다. 따라서, 자동화된 어시스턴트는 하나 이상의 이행 후보를 얻기 위해 "애플리케이션 1" 및 "애플리케이션 2"(및 선택적으로 음성 발화를 만족시킬 수 있는 다른 이행자)에 하나 이상의 구조화된 요청을 전송할 수 있다. 또한, 자동화된 어시스턴트는 하나 이상의 이행 후보의 순위를 매기고 특정 이행 후보를 선택하여 음성 발화에 응답하여 특정 이행이 수행되도록 할 수 있다. 이 예에서, 또한 자동화된 어시스턴트가 제1 해석과 연관된 이행 후보를 선택한다고 가정한다. 따라서, 자동화된 어시스턴트는 "아티스트 1"의 "노래 1"이 음성 발화의 특정 이행으로서 클라이언트 장치(또는 클라이언트 장치와 통신하는 추가 클라이언트 장치)의 스피커(들)를 통해 "애플리케이션 1"을 사용하여 재생되게 할 수 있다. For example, in this example, an automated assistant may use the Artist slot value of “Artist 1” for the Artist parameter associated with a musical intent, the Song slot value for “Song 1” for a song parameter associated with a musical intent, and the Song slot value associated with a musical intent. Suppose you have determined a first interpretation of a spoken utterance of “play rock music” that includes a musical intent with a software application or streaming service slot value of “application 1” for the software application or streaming service parameters. In addition, the automated assistant will automatically set the artist slot value of “Artist 1” for the artist parameter associated with the musical intent, the song slot value for “Song 1” for the song parameter associated with the musical intent, and the software application or streaming service parameter associated with the musical intent. Suppose we have determined a first interpretation of the spoken utterance of “Play rock music” that includes a music intent with a software application or streaming service slot value of “Application 2” for In this case, "Application 1" and "Application 2" can be considered as one or more fulfillers capable of satisfying the utterance. Thus, the automated assistant can send one or more structured requests to "Application 1" and "Application 2" (and optionally other fulfillers that can satisfy the spoken utterance) to obtain one or more fulfillment candidates. Additionally, the automated assistant may rank one or more transition candidates and select a particular transition candidate to have a particular transition performed in response to a spoken utterance. In this example, it is also assumed that the automated assistant selects the transition candidate associated with the first interpretation. Accordingly, the automated assistant can play "Song 1" of "Artist 1" using "Application 1" through the speaker(s) of the client device (or additional client devices communicating with the client device) as a specific implementation of the spoken utterance. can make it
일부 구현에서, 특정 이행이 수행되도록 하는 자동화된 어시스턴트에 이어, 클라이언트 장치의 사용자는 자동화된 어시스턴트가 특정 이행이 수행된 이유 및/또는 대체 이행이 수행되지 않은 이유에 대한 특정 추론을 제공하도록 요청하는 추가 사용자 입력을 제공할 수 있다. 이러한 구현의 일부 버전에서 특정 추론에 대한 요청은 특정 추론에 대한 일반적인 요청일 수 있으며(예: "why did you do that?(왜 그랬어?)"), 반면 다른 구현에서 특정 추론에 대한 요청은 특정 추론에 대한 특정 요청일 수 있다(예: "why did you play the music using application 1?(어플리케이션 1을 사용하여 음악을 재생한 이유는 무엇입니까?)", "why didn't you use application 2?(어플리케이션 2를 사용하지 않은 이유는 무엇입니까?)", "why did you select artist 1?(아티스트 1을 선택한 이유는 무엇입니까?)", 등). 예를 들어, 사용자가 "why did you do that?(왜 그렇게 했습니까?)"라는 추가 음성 발화를 제공한다고 가정한다. 이 예에서, 요청은 특정 추론에 대한 일반적인 요청이며, 자동화된 어시스턴트는 일반 요청에 응답하는 출력을 생성하기 위해 음성 발화의 제1 해석과 연관된 추가 데이터를 결정할 수 있다(예: "You use application 1 the most to listen to music, you have listened to artist 1 in the past, and song 1 is artist 1's most popular song(당신은 음악을 듣기 위해 애플리케이션 1을 가장 많이 사용하고, 과거에 아티스트 1을 들었고, 노래 1은 아티스트 1의 가장 인기 있는 노래입니다)" 등). 대조적으로, 사용자가 "애플리케이션 2를 사용하지 않은 이유는 무엇입니까?"라는 추가 음성 발화를 제공한다고 가정한다. 이 예에서, 요청은 특정 추론에 대한 특정 요청이며, 자동화된 어시스턴트는 일반적인 요청에 응답하는 출력을 생성하기 위해 음성 발화의 제1 해석 및/또는 제2 해석과 연관된 추가 데이터를 결정할 수 있다(예: "음악을 듣기 위해 애플리케이션 2보다 애플리케이션 1을 더 많이 사용한다" 등). 그러나 특정 요청이 포함된 이 예에서는 사용자가 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스 권한을 부여하지 않았다고 가정한다. 이 예에서, 자동화된 어시스턴트는 추가로 또는 대안적으로 추천된 액션과 연관된 추천 데이터를 결정할 수 있고, 추천된 액션을 포함하는 추천 데이터에 기초하여 프롬프트를 생성할 수 있다(예: "애플리케이션 2를 사용할 수 있는 액세스 권한을 부여하지 않았습니다. 액세스 권한을 부여하시겠습니까?" 등). 따라서, 자동화된 어시스턴트는 이행의 특정 에스펙트(aspects)에 대한 특정 추론을 제공할 수 있을 뿐만 아니라, 자동화된 어시스턴트가 어시스턴트 명령을 포함하는 사용자 입력 수신에 응답하여 사용자가 현재 및/또는 미래 이행을 적응하도록 프롬프트할 수도 있다. In some implementations, following an automated assistant to cause a particular transition to be performed, a user of the client device requests the automated assistant to provide a particular inference as to why a particular transition was performed and/or why an alternative transition was not performed. Additional user input can be provided. In some versions of these implementations, requests for specific inferences may be general requests for specific inferences (e.g., "why did you do that?"), while in other implementations, requests for specific inferences may be specific. This can be a specific request for inference, e.g. "why did you play the music using application 1?", "why didn't you use application 2?" (Why didn't you use Application 2?)", "Why did you select artist 1?", etc.). For example, it is assumed that the user provides an additional voice utterance "why did you do that?" In this example, the request is a general request for a specific inference, and the automated assistant can determine additional data associated with the first interpretation of the spoken utterance to generate output responsive to the general request (e.g., "You use application 1 the most to listen to music, you have listened to artist 1 in the past, and song 1 is artist 1's most popular song is the most popular song by artist 1)" etc.). In contrast, suppose the user provides an additional spoken utterance "Why didn't you use application 2?" In this example, the request is a specific request for a specific inference, and the automated assistant can determine additional data associated with the first and/or second interpretation of the spoken utterance to generate output responsive to the general request (e.g. : "I use Application 1 more than Application 2 to listen to music", etc.). However, this example with a specific request assumes that the user has not granted automated assistant access to "Application 2". In this example, the automated assistant may additionally or alternatively determine recommendation data associated with the recommended action, and generate a prompt based on the recommendation data that includes the recommended action (eg, "application 2 You have not granted access to use it. Would you like to grant access?" etc.). Thus, an automated assistant may not only provide specific inferences about specific aspects of a transition, but the automated assistant may respond to receiving user input, including assistant commands, so that the user can make current and/or future transitions. You may be prompted to adapt.
추가 또는 대체 구현에서 그리고 특정 이행을 수행하도록 하는 자동화된 어시스턴트 이후에, 자동화된 어시스턴트는 특정 추론과 연관된 하나 이상의 선택 가능한 요소가 클라이언트 장치의 디스플레이를 통해 사용자에게 제공되도록 능동적으로 야기할 수 있다. 예를 들어, 특정 추론에 대한 일반적인 요청과 연관된 제1 선택 가능한 요소는 사용자에게 프레젠테이션하기 위해 제공될 수 있으며, 선택되면 자동화된 어시스턴트가 일반 요청에 응답하는 특정 추론을 제공하게 할 수 있다. 또한, 특정 추론에 대한 제1 특정 요청과 연관된 제2 선택 가능 요소는 추가로 또는 대안적으로 사용자에게 프레젠테이션하기 위해 제공될 수 있고, 선택될 때 자동화된 어시스턴트로 하여금 제1 특정 요청에 응답하는 특정 추론을 제공하게 할 수 있다. 더욱이, 특정 추론에 대한 제2 특정 요청과 연관된 제3 선택가능 요소는 추가로 또는 대안적으로 사용자에게 프레젠테이션하기 위해 제공될 수 있고, 선택될 때 자동화된 어시스턴트로 하여금 제2 특정 요청에 응답하는 특정 추론을 제공하게 할 수 있다. 이러한 구현의 일부 버전에서, 자동화된 어시스턴트는 ASR 메트릭, NLU 메트릭 및/또는 이행 메트릭이 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행에 대해 그다지 확신하지 못한다는 것을 나타내는 임계값을 만족하지 못하는 결정에 응답하여 하나 이상의 선택 가능한 요소가 사용자에게 제공되도록 할 수 있다. 그러한 구현의 일부 버전에서, 자동화된 어시스턴트는 ASR 메트릭, NLU 메트릭 및/또는 이행 메트릭에 관계없이 사용자에게 프레젠테이션하기 위해 하나 이상의 선택 가능한 요소가 제공되도록 할 수 있다. In additional or alternative implementations, and after the automated assistant prompts the particular implementation to perform, the automated assistant can actively cause one or more selectable elements associated with the particular inference to be presented to the user via the display of the client device. For example, a first selectable element associated with a general request for a specific inference may be presented for presentation to the user and, when selected, may cause an automated assistant to present the specific inference in response to the general request. Additionally, a second selectable element associated with a first particular request for a particular inference may additionally or alternatively be provided for presentation to the user and, when selected, causes the automated assistant to respond to the first particular request. inference can be provided. Moreover, a third selectable element associated with a second particular request for a particular inference may additionally or alternatively be provided for presentation to the user and, when selected, causes the automated assistant to respond to the second particular request. inference can be provided. In some versions of these implementations, the automated assistant responds to a determination that the ASR metric, NLU metric, and/or fulfillment metric do not satisfy a threshold indicating that the automated assistant is not very confident about the specific fulfillment of an assistant instruction. One or more selectable elements may be presented to the user. In some versions of such implementations, the automated assistant may cause one or more selectable elements to be presented for presentation to the user regardless of the ASR metric, NLU metric, and/or performance metric.
위의 예는 자동화된 어시스턴트가 음악이 재생되도록 하기 위해 특정 소프트웨어 애플리케이션(예: "애플리케이션 1")을 선택한 이유에 대한 특정 추론을 제공하는 것과 관련하여 설명되었지만, 이는 예시를 위한 것이며 제한하려는 의도가 아님을 이해해야 한다. 본 명세서에 기술된 바와 같이, 본 명세서에 기술된 기술은 특정 컴퓨팅 장치가 어시스턴트 명령의 이행에 사용되도록 선택된 이유, 해당 파라미터에 대한 특정 슬롯 값이 선택된 이유, 자동화된 어시스턴트가 대체 이행을 수행할 수 없는 이유, 및/또는 여기에 설명된 임의의 다른 에스펙트와 같은 이행의 임의의 에스펙트에 대한 특정 추론을 제공하기 위해 사용될 수 있다. 또한, 위의 예에서 설명된 추천된 액션에는 특정 소프트웨어 애플리케이션(예: "애플리케이션 2")에 대한 자동화된 어시스턴트 액세스를 허용하는 사용자가 포함되지만, 이는 또한 예시를 위한 것이며 제한하려는 의도가 아님을 이해해야 한다. 여기에 설명된 기술은 클라이언트 장치에서 소프트웨어 애플리케이션을 다운로드하는 것, 추가 클라이언트 장치를 네트워크를 통해 클라이언트 장치와 통신적으로 연결하는 것, 및/또는 여기에 설명된 임의의 다른 추천된 액션와 같은, 어시스턴트 명령의 이행에 적응하기 위해 임의의 추천된 액션을 제공하는 데 사용될 수 있다. While the example above was described in terms of providing a specific inference as to why an automated assistant chose a particular software application (eg "Application 1") to cause music to play, this is for illustrative purposes only and is not intended to be limiting. You must understand that it is not As described herein, the techniques described herein can be used to determine why a particular computing device has been selected to be used for fulfillment of an assistant command, why a particular slot value for that parameter has been selected, and for an automated assistant to perform an alternative fulfillment. may be used to provide specific inferences about any aspect of an implementation, such as why not, and/or any other aspect described herein. It should also be understood that the recommended actions described in the example above include allowing the user to allow automated assistant access to a specific software application (e.g. "Application 2"), but this is also illustrative and not intended to be limiting. do. Techniques described herein include downloading software applications on client devices, communicatively connecting additional client devices with client devices over a network, and/or assisting with assistant commands, such as any other recommended action described herein. can be used to provide any recommended action to adapt to the implementation of
여기에 설명된 기술을 사용함으로써 하나 이상의 기술적 이점을 획득할 수 있다. 비제한적인 예로서, 여기에 설명된 기술은 자동화된 어시스턴트가 이행의 특정 에스펙트(또는 이행 부족)에 대한 특정 추론을 제공할 수 있도록 한다. 따라서 클라이언트 장치의 사용자는 자신의 데이터가 언제 사용되고 있으며 특정 추론을 제공함으로써 자신의 데이터가 어떻게 사용되고 있는지 이해할 수 있다. 더 나아가, 여기에 설명된 기술은 자동화된 어시스턴트가 사용자 데이터의 개인 정보 설정을 빠르고 효율적으로 조정할 수 있도록 한다. 사용자가 다양한 인터페이스를 탐색하여 사용자 데이터의 개인 정보 설정을 수동으로 변경해야 할 필요성을 제거하여 사용자 입력의 양을 줄인다. 결과적으로 사용자 데이터의 보안이 강화되고 클라이언트 장치의 계산 리소스가 절약될 수 있다. 또 다른 비제한적 예로서, 수행할 추천된 액션을 제공하고 사람과 컴퓨터 간의 대화를 계속함으로써 자동화된 어시스턴트가 그렇지 않으면 낭비될 사용자 상호 작용을 구제할 수 있다. 예를 들어, One or more technical advantages may be obtained by using the techniques described herein. As a non-limiting example, the technology described herein allows an automated assistant to provide specific inferences about specific aspects of fulfillment (or lack of fulfillment). Thus, users of client devices can understand when their data is being used and how their data is being used by providing specific inferences. Further, the technology described herein allows automated assistants to quickly and efficiently adjust the privacy settings of user data. It reduces the amount of user input by eliminating the need for users to manually change privacy settings for user data by navigating through various interfaces. As a result, the security of user data can be enhanced and computational resources of the client device can be saved. As another non-limiting example, an automated assistant can salvage otherwise wasted user interaction by providing a recommended action to perform and continuing the human-computer conversation. for example,
사용자가 하나 이상의 스마트 조명을 제어하기 위해 "turn on the lights(조명 켜)"라는 음성 발화를 제공하지만 사용자가 스마트 조명을 제어하기 위한 소프트웨어 애플리케이션에 대한 자동화된 어시스턴트 액세스 권한을 부여하지 않은 경우, 자동화된 어시스턴트는 자동화된 어시스턴트가 특정 시간에 조명을 제어할 수 없다는 것을 단순히 나타내기보다는 스마트 조명을 제어하기 위한 소프트웨어 애플리케이션에 대한 액세스 권한을 사용자에게 프롬프트할 수 있다. 그 결과, 계산 및/또는 네트워크 리소스는 여기에 설명된 기술을 사용하여 보존될 수 있다. If the user provides the spoken utterance "turn on the lights" to control one or more smart lights, but the user has not given the automated assistant access to the software application to control the smart lights, the automation The automated assistant may prompt the user for access to a software application to control the smart lighting, rather than simply indicating that the automated assistant cannot control the lighting at a particular time. As a result, computational and/or network resources may be conserved using the techniques described herein.
상기 설명은 본 명세서에 개시된 일부 구현의 개요로서 제공된다. 이러한 구현 및 기타 구현은 여기에서 추가로 자세히 설명된다.The above description is provided as an overview of some implementations disclosed herein. These and other implementations are described in further detail herein.
전술한 개념 및 본 명세서에 더 상세히 설명된 추가 개념의 모든 조합은 본 명세서에 개시된 주제의 일부인 것으로 고려된다는 것을 이해해야 한다. 예를 들어, 본 개시의 말미에 나타나는 청구된 주제의 모든 조합은 본 명세서에 개시된 주제의 일부인 것으로 간주된다.It should be understood that all combinations of the foregoing concepts and additional concepts described in greater detail herein are considered to be part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered to be part of the subject matter disclosed herein.
도 1은 본 명세서에 개시된 구현이 구현될 수 있는 본 개시의 다양한 에스펙트를 입증하는 예시적인 환경의 블록도를 도시한다.
도 2는 사용자 입력에 포함되고 자동화된 어시스턴트가 수행되도록 하는 어시스턴트 명령의 특정 이행을 야기하고, 다양한 구현에 따라 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 제공하게 한 이유에 대한 특정 추론을 야기하는 예시적인 방법을 예시하는 흐름도를 도시한다.
도 3은 사용자 입력에 포함되고 자동화된 어시스턴트로 향하는 어시스턴트 명령의 특정 이행이 수행될 수 없다고 결정하고, 다양한 구현에 따라 자동화된 어시스턴트가 제공될 특정 추론을 수행할 수 없는 이유에 대해 특정 추론을 야기하는 예시적인 방법을 예시하는 흐름도를 도시한다.
도 4a, 도 4b 및 도 4c는 다양한 구현에 따라 제공될 어시스턴트 명령의 이행에 관한 특정 추론을 유발하는 다양한 비제한적 예를 도시한다.
도5a 및 5b는 다양한 구현에 따라 제공될 어시스턴트 명령의 이행에 관한 특정 추론을 유발하는 다양한 추가적인 비제한적 예를 도시한다.
도 6은 다양한 구현에 따른 컴퓨팅 장치의 예시적인 아키텍처를 도시한다.1 shows a block diagram of an example environment demonstrating various aspects of the present disclosure in which implementations disclosed herein may be implemented.
2 is an example of what is contained in user input and causes specific implementation of an assistant command that causes the automated assistant to be performed and, according to various implementations, causes specific inference as to why the automated assistant provides a specific implementation of an assistant command. A flow diagram illustrating the method is shown.
3 determines that certain implementations of the assistant commands contained in the user input and directed to the automated assistant cannot be performed and, according to various implementations, cause certain inferences as to why the automated assistant cannot perform certain inferences to be presented. A flow diagram illustrating an example method of doing so is shown.
4A, 4B and 4C show various non-limiting examples of triggering particular inferences about the implementation of assistant commands that will be provided according to various implementations.
5A and 5B show various additional non-limiting examples of triggering particular inferences about the implementation of assistant commands that will be provided according to various implementations.
6 illustrates an example architecture of a computing device according to various implementations.
도 1을 참조하면, 본 발명의 다양한 양태를 나타내고 본 명세서에 개시된 구현이 구현될 수 있는 예시적인 환경의 블록도가 도시되어 있다. 예시적인 환경은 클라이언트 장치(110), 하나 이상의 클라우드 기반 자동화된 어시스턴트 컴포넌트(115), 하나 이상의 제1-파티 서버(191) 및 하나 이상의 제3-파티 서버(192)를 포함한다. Referring to FIG. 1 , there is shown a block diagram of an exemplary environment in which various aspects of the invention and implementations disclosed herein may be implemented. An exemplary environment includes a client device 110 , one or more cloud-based automated assistant components 115 , one or more first-party servers 191 and one or more third-party servers 192 .
클라이언트 장치(110)는 자동화된 어시스턴트 클라이언트(113)를 실행할 수 있다. 자동화된 어시스턴트 클라이언트(113)는 클라이언트 장치(110)의 운영 체제(예: 운영 체제 "탑(on top)"에 설치됨)와 별개인 애플리케이션일 수 있거나 대안적으로 클라이언트 장치(110)의 운영 체제에 의해 직접 구현될 수 있다. 아래에서 추가로 설명되는 바와 같이, 자동화된 어시스턴트 클라이언트(113)는 클라이언트 장치(110)의 사용자 인터페이스 컴포넌트(들)(112)에 의해 수신된 다양한 요청에 응답하여 클라우드 기반 자동화된 어시스턴트 컴포넌트(115) 중 하나 이상과 선택적으로 상호작용할 수 있다. 또한, 또한 아래에서 설명되는 바와 같이, 클라이언트 장치(110)의 다른 엔진(들)은 하나 이상의 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)와 선택적으로 상호작용할 수 있다. The client device 110 may run an automated assistant client 113 . Automated assistant client 113 may be a separate application from the operating system of client device 110 (eg, installed “on top” of the operating system) or, alternatively, integrated into the operating system of client device 110 . can be implemented directly by As described further below, automated assistant client 113 responds to various requests received by user interface component(s) 112 of client device 110 to provide cloud-based automated assistant component 115 can optionally interact with one or more of them. Additionally, as also described below, other engine(s) of client device 110 may optionally interact with one or more cloud-based automated assistant components 115 .
하나 이상의 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)는 하나 이상의 로컬 영역 네트워크(Wi-Fi LAN, Bluetooth 네트워크, 근거리 통신 네트워크, 메시 네트워크 등을 포함한 "LAN"), 광역 네트워크(인터넷 등을 포함한 "WAN") 및/또는 다른 네트워크를 통해 클라이언트 장치(110)에 통신 가능하게 연결된 하나 이상의 컴퓨팅 시스템(예: 집합적으로 "클라우드" 또는 "원격" 컴퓨팅 시스템이라고 하는 서버)에서 구현될 수 있다. 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)와 클라이언트 장치(110)의 통신 연결은 일반적으로 도 1의 1991로 표시된다. 또한, 일부 구현에서, 클라이언트 장치(110)는 하나 이상의 네트워크(예를 들어, LAN 및/또는 WAN)를 통해 본 명세서에 설명된 다른 클라이언트 장치(미도시)와 통신 가능하게 결합될 수 있다.One or more cloud-based automated assistant components 115 may be connected to one or more local area networks ("LAN" including Wi-Fi LANs, Bluetooth networks, local area networks, mesh networks, etc.), wide area networks ("WAN" including Internet, etc.) ) and/or one or more computing systems (eg, servers, collectively referred to as “cloud” or “remote” computing systems) communicatively coupled to client device 110 via other networks. The communication connection between the cloud-based automated assistant component 115 and the client device 110 is generally indicated at 199 1 in FIG. 1 . Additionally, in some implementations, client device 110 may be communicatively coupled with other client devices (not shown) described herein over one or more networks (eg, LANs and/or WANs).
하나 이상의 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)는 또한 하나 이상의 네트워크(예: LAN, WAN 및/또는 기타 네트워크)를 통해 하나 이상의 제1-파티 서버(191) 및/또는 하나 이상의 제3-파티 서버(192)와 통신 가능하게 결합될 수 있다. 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)와 제1-파티(first-party) 서버(191) 중 하나 이상의 통신 연결은 일반적으로 도 1의 1992로 표시된다. 또한, 하나 이상의 제3-파티(third-party) 서버(192)와 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)의 통신 연결은 일반적으로 도 1의 1993으로 표시된다. 일부 구현에서, 비록 도 1에 명시적으로 도시되지는 않았지만. 클라이언트 장치(110)는 추가로 또는 대안적으로 하나 이상의 네트워크(예: LAN, WAN 및/또는 기타 네트워크)를 통해 하나 이상의 제1-파티 서버(191) 및/또는 하나 이상의 제3-파티 서버(192)와 통신 가능하게 결합될 수 있다. 또한, 하나 이상의 네트워크(1991, 1992, 1993)는 간략화를 위해 이하 "네트워크(들)(199)"로 총칭한다.One or more cloud-based automated assistant components 115 may also connect to one or more first-party servers 191 and/or one or more third-party servers via one or more networks (eg, LANs, WANs, and/or other networks). (192) and communicatively coupled. A communication connection between one or more of the cloud-based automated assistant component 115 and the first-party server 191 is indicated generally at 199 2 in FIG. 1 . Also, communication connections between one or more third-party servers 192 and the cloud-based automated assistant component 115 are generally indicated at 199 3 in FIG. 1 . In some implementations, although not explicitly shown in FIG. 1 . Client device 110 may additionally or alternatively connect to one or more first-party servers 191 and/or one or more third-party servers (eg, LAN, WAN, and/or other networks) via one or more networks (eg, LAN, WAN, and/or other networks). 192) and communicatively coupled. In addition, one or more networks 1991, 1992, and 1993 are collectively referred to as "network(s) 199" hereinafter for simplicity.
자동화된 어시스턴트 클라이언트(113)는 클라우드 기반 자동화된 어시스턴트 컴포넌트(115) 중 하나 이상과의 상호 작용을 통해 사용자 관점에서 클라이언트 장치(110)의 사용자가 인간 대 컴퓨터 대화에 참여할 수 있는 자동화된 어시스턴트(120)의 논리적 인스턴스인 것처럼 보이는 것을 형성할 수 있다. 예를 들어, 점선으로 둘러싸인 자동화된 어시스턴트(120)의 인스턴스는 클라이언트 장치(110)의 자동화된 어시스턴트 클라이언트(113) 및 하나 이상의 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)를 포함한다. 따라서 클라이언트 장치(110)에서 실행되는 자동화된 어시스턴트 클라이언트(113)와 관여하는 각각의 사용자는 사실상 자동화된 어시스턴트(120)의 그 자신의 논리적 인스턴스와 관여할 수 있다는 것을 이해해야 한다(또는 가정 또는 다른 사용자 그룹 간에 공유되는 및/또는 복수의 자동화된 어시스턴트 클라이언트(113) 간에 공유되는 자동화된 어시스턴트(120)의 논리적 인스턴스). 도 1에는 클라이언트 장치(110)만이 도시되어 있지만, 하나 이상의 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)가 추가로 클라이언트 장치의 많은 추가 그룹에 서비스를 제공할 수 있다는 것이 이해된다. 더욱이, 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)가 도 1에 도시되어 있지만, 다양한 구현에서, 자동화된 어시스턴트(120)는 클라이언트 장치(110)에서만 구현될 수 있음을 이해해야 한다.The automated assistant client 113 is an automated assistant 120 that allows the user of the client device 110 to engage in human-to-computer conversations from the user perspective through interaction with one or more of the cloud-based automated assistant components 115. ) can form what appears to be a logical instance of For example, an instance of automated assistant 120 enclosed by a dotted line includes an automated assistant client 113 on a client device 110 and one or more cloud-based automated assistant components 115 . Accordingly, it should be understood that each user engaging with the automated assistant client 113 running on the client device 110 may in fact be engaging with its own logical instance of the automated assistant 120 (or home or other user). A logical instance of an automated assistant 120 shared between groups and/or shared among multiple automated assistant clients 113). Although only client devices 110 are shown in FIG. 1 , it is understood that one or more cloud-based automated assistant components 115 may additionally serve many additional groups of client devices. Moreover, although a cloud-based automated assistant component 115 is shown in FIG. 1 , it should be understood that in various implementations, automated assistant 120 may be implemented only on a client device 110 .
본 명세서에서 사용되는 바와 같이, 제1-파티 장치 또는 시스템(예를 들어, 하나 이상의 제1-파티 서버(191), 하나 이상의 제1-파티 소프트웨어 애플리케이션 등)은 본 명세서에서 참조되는 자동화된 어시스턴트(120)를 제어하는 파티(party)와 동일한 파티에 의해 제어되는 시스템을 참조한다. 예를 들어, 하나 이상의 제1-파티 서버(191)는 검색 엔진 서비스, 통신 서비스(예: 이메일, SMS 메시징 등), 내비게이션 서비스, 음악 서비스, 문서 편집 또는 공유 서비스, 및/또는 본 명세서에서 참조되는 자동화된 어시스턴트(120)를 제어하는 제1-파티와 동일한 제1-파티가 제어하는 다른 서비스를 호스팅하는 시스템을 참조할 수 있다. 대조적으로, 제3-파티 장치(디바이스) 또는 시스템(예를 들어, 하나 이상의 제3-파티 서버(192), 하나 이상의 제3-파티 소프트웨어 애플리케이션 등)은 본 명세서에서 참조되는 자동화된 어시스턴트(120)를 제어하는 제1-파티와 별개의 제1-파티에 의해 제어되는 시스템을 참조한다. 예를 들어, 하나 이상의 제3-파티 서버(192)는 동일한 서비스를 호스팅하는 시스템을 참조할 수 있지만, 이러한 서비스는 본 명세서에서 참조되는 자동화된 어시스턴트(120)를 제어하는 제1-파티와 다른 제1-파티에 의해 제어된다.As used herein, a first-party device or system (eg, one or more first-party servers 191, one or more first-party software applications, etc.) refers to an automated assistant as referred to herein. Refers to a system controlled by the same party as the party controlling (120). For example, one or more first-party servers 191 may be search engine services, communication services (eg, email, SMS messaging, etc.), navigation services, music services, document editing or sharing services, and/or as referenced herein. It may refer to a system that hosts other services controlled by the same first-party as the first-party that controls automated assistant 120 . In contrast, a third-party device (device) or system (eg, one or more third-party servers 192, one or more third-party software applications, etc.) is referred to herein as an automated assistant 120. ) refers to a system controlled by a first-party separate from the first-party that controls . For example, one or more third-party servers 192 may refer to systems hosting the same service, but these services may be different from the first-party that controls the automated assistant 120 referred to herein. Controlled by the 1st-party.
클라이언트 장치(110)는 예를 들어, 데스크탑 컴퓨팅 장치, 랩탑 컴퓨팅 장치, 태블릿 컴퓨팅 장치, 휴대폰 컴퓨팅 장치, 사용자 차량의 컴퓨팅 장치(예: 차량 내 통신 시스템, 차량 내 엔터테인먼트 시스템, 차량 내 내비게이션 시스템), 대화형 독립형 스피커(예: 디스플레이 포함 또는 제외), 스마트 기기, 스마트 텔레비전, 스마트 조명 또는 스마트 세탁기/건조기 등과 같은 스마트 네트워크 장치, 컴퓨팅 장치, 및/또는 자동화된 어시스턴트(120)로 향하는 사용자 입력을 수신할 수 있는 임의의 IoT 장치를 포함하는 사용자의 웨어러블 장치(예를 들어, 컴퓨팅 장치를 가진 사용자의 시계, 컴퓨팅 장치를 가진 사용자의 안경, 가상 또는 증강 현실 컴퓨팅 장치) 중 하나 이상을 포함할 수 있다. 추가 및/또는 대체 클라이언트 장치가 제공될 수 있다. The client device 110 may be, for example, a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (eg, an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), Receive user input directed to an interactive stand-alone speaker (eg, with or without a display), smart appliance, smart television, smart lighting, or smart network device, computing device, and/or automated assistant 120, such as a smart washer/dryer; It may include one or more of a user's wearable device (eg, a user's watch with a computing device, a user's glasses with a computing device, a virtual or augmented reality computing device), including any IoT device capable of . Additional and/or replacement client devices may be provided.
다양한 구현에서, 클라이언트 장치(110)는 클라이언트 장치(110)의 사용자로부터의 승인으로 검출된 존재, 특히 인간 존재를 나타내는 신호를 제공하도록 구성되는 하나 이상의 존재 센서(111)를 포함할 수 있다. 그러한 구현들 중 일부에서, 자동화된 어시스턴트(120)는 클라이언트 장치(110)(또는 클라이언트 장치(110)의 사용자와 연관된 다른 컴퓨팅 장치에서)에서의 사용자의 존재의 적어도 일부에 기초하여 음성 발화(또는 자동화된 어시스턴트(120)로 향하는 다른 입력)를 만족시키기 위해 클라이언트 장치(110)(또는 클라이언트 장치(110)의 사용자와 연관된 다른 컴퓨팅 장치)를 식별할 수 있다. 음성 발화(또는 자동화된 어시스턴트(120)로 향하는 다른 입력)는 클라이언트 장치(110) 및/또는 클라이언트 장치(110)의 사용자와 연관된 다른 컴퓨팅 장치(들)가 제어되게 함으로써, 및/또는 클라이언트 장치(110) 및/또는 클라이언트 장치(110)의 사용자와 연관된 다른 컴퓨팅 장치(들)로 하여금 음성 발화(또는 자동화된 어시스턴트(120)로 향하는 다른 입력)를 만족시키기 위해 임의의 다른 동작을 수행하게 함으로써, 클라이언트 장치(110) 및/또는 클라이언트 장치(110)의 사용자와 연관된 다른 컴퓨팅 장치(들)에서 반응형 콘텐츠를 렌더링(예: 청각 및/또는 시각적으로)함으로써 충족될 수 있다. 본 명세서에 기술된 바와 같이, 자동화된 어시스턴트(120)는 사용자가 어디에 가까이 있는지 또는 최근에 근처에 있었는지에 기초하여 클라이언트 장치(110)(또는 다른 컴퓨팅 장치)를 결정할 때 존재 센서(111)에 기초하여 결정된 데이터를 사용할 수 있고, 대응하는 명령을 클라이언트 장치(110)(또는 다른 컴퓨팅 장치)에만 제공할 수 있다. 일부 추가 또는 대체 구현에서, 자동화된 어시스턴트(120)는 임의의 사용자(들) 여부를 결정할 때 존재 센서(111)에 기초하여 결정된 데이터를 사용할 수 있고, 선택적으로 클라이언트 장치(110)에 근접한 사용자(들)(임의의 사용자 또는 특정 사용자)에 기초하여 클라이언트 장치(110)(또는 다른 컴퓨팅 장치)로의 및/또는 클라이언트 장치(또는 다른 컴퓨팅 장치)로부터의 데이터 제공을 억제할 수 있다. In various implementations, the client device 110 may include one or more presence sensors 111 configured to provide a signal indicative of a detected presence, particularly a human presence, with authorization from a user of the client device 110 . In some such implementations, automated assistant 120 makes a vocal utterance (or utterance) based at least in part on the user's presence at client device 110 (or on another computing device associated with the user of client device 110). It may identify the client device 110 (or other computing device associated with the user of the client device 110) to satisfy other input directed to the automated assistant 120. A spoken utterance (or other input directed to automated assistant 120) causes the client device 110 and/or other computing device(s) associated with the user of client device 110 to be controlled, and/or the client device ( 110) and/or other computing device(s) associated with the user of client device 110 to perform any other action to satisfy a spoken utterance (or other input directed to automated assistant 120); This may be accomplished by rendering (eg, audibly and/or visually) the responsive content on the client device 110 and/or other computing device(s) associated with the user of the client device 110 . As described herein, automated assistant 120 responds to presence sensor 111 when determining client device 110 (or other computing device) based on where the user is or has recently been nearby. The data determined based thereon may be used and corresponding commands may be provided only to the client device 110 (or other computing device). In some additional or alternative implementations, automated assistant 120 may use the data determined based on presence sensor 111 when determining whether any user(s) are, optionally, users in proximity of client device 110 ( s) (any user or a specific user) may inhibit the provision of data to and/or from the client device 110 (or other computing device).
존재 센서(111)는 다양한 형태를 가질 수 있다. 예를 들어, 클라이언트 장치(110)는 그들의 시야에서 검출된 움직임을 나타내는 신호(들)를 캡처하고 제공하도록 구성된 하나 이상의 비전 컴포넌트(예: 디지털 카메라 및/또는 기타 비전 파라미터)를 구비할 수 있다. 추가적으로 또는 대안적으로, 클라이언트 장치(110)는 시야 내에 있는 물체로부터 방사되는 적외선("IR") 광을 측정하는 수동 적외선("PIR") 센서와 같은 다른 유형의 광 기반 존재 센서(111)를 구비할 수 있다. 추가적으로 또는 대안적으로 클라이언트 장치(110)는 하나 이상의 마이크로폰과 같은 음향(또는 압력) 파동을 검출하는 존재 센서(111)를 구비할 수 있다. The presence sensor 111 may have various forms. For example, client devices 110 may have one or more vision components (eg, digital cameras and/or other vision parameters) configured to capture and provide signal(s) indicative of detected motion in their field of view. Additionally or alternatively, the client device 110 may include another type of light-based presence sensor 111, such as a passive infrared ("PIR") sensor that measures infrared ("IR") light emitted from objects within its field of view. can be provided Additionally or alternatively, the client device 110 may be equipped with a presence sensor 111 that detects acoustic (or pressure) waves, such as one or more microphones.
추가로 또는 대안적으로, 일부 구현에서, 존재 센서(111)는 인간 존재 또는 장치 존재와 연관된 다른 현상을 검출하도록 구성될 수 있다. 예를 들어, 일부 실시예에서, 클라이언트 장치(110)는 예를 들어 사용자에 의해 휴대/작동되는 다른 컴퓨팅 장치(예: 모바일 장치, 웨어러블 컴퓨팅 장치 등) 및/또는 다른 컴퓨팅 장치에 의해 방출되는 다양한 유형의 무선 신호(예: 라디오, 초음파, 전자파 등의 전파)를 검출하는 존재 센서(111)를 구비할 수 있다. 예를 들어, 클라이언트 장치(110)는 다른 컴퓨팅 장치(들)에 의해 검출될 수 있는 초음파 또는 적외선과 같이 인간이 인지할 수 없는 파동을 방출하도록 구성될 수 있다(예: 초음파 가능 마이크로폰와 같은 초음파/적외선 수신기를 통해). Additionally or alternatively, in some implementations, presence sensor 111 can be configured to detect other phenomena associated with human presence or device presence. For example, in some embodiments, the client device 110 may be, for example, another computing device carried/operated by a user (eg, a mobile device, wearable computing device, etc.) and/or various other computing devices emitted by the other computing device. A presence sensor 111 may be provided to detect tangible wireless signals (eg, radio waves, ultrasonic waves, electromagnetic waves, etc.). For example, the client device 110 may be configured to emit waves that are non-human perceptible, such as ultrasound or infrared, that may be detected by other computing device(s) (e.g., ultrasound/sound waves such as an ultrasound-capable microphone). via an infrared receiver).
추가적으로 또는 대안적으로, 클라이언트 장치(110)는 사용자에 의해 휴대/작동되는 다른 컴퓨팅 장치(들)(예: 모바일 장치, 웨어러블 컴퓨팅 장치 등)에 의해 검출될 수 있고 사용자의 특정 위치를 결정하는 데 사용될 수 있는 전파(예: Wi-Fi, Bluetooth, 셀룰러 등)와 같은 다른 유형의 사람이 인지할 수 없는 파동을 방출할 수 있다. 일부 구현에서, GPS 및/또는 Wi-Fi 삼각 측량은 예를 들어 클라이언트 장치(110)로/로부터의 GPS 및/또는 Wi-Fi 신호에 기초하여 사람의 위치를 검출하는 데 사용될 수 있다. 다른 구현에서, TOF(time-of-flight), 신호 강도 등과 같은 다른 무선 신호 특성은 사용자에 의해 휴대/작동되는 다른 컴퓨팅 장치(들)에 의해 방출된 신호에 기초하여 특정 사람의 위치를 결정하기 위해 클라이언트 장치(110)에 의해 단독으로 또는 집합적으로 사용될 수 있다.Additionally or alternatively, the client device 110 may be detected by other computing device(s) carried/operated by the user (eg, mobile device, wearable computing device, etc.) and may be used to determine the specific location of the user. Other types of radio waves that can be used (e.g. Wi-Fi, Bluetooth, cellular, etc.) may emit waves imperceptible to humans. In some implementations, GPS and/or Wi-Fi triangulation may be used to detect a person's location based on GPS and/or Wi-Fi signals to/from client device 110, for example. In other implementations, other radio signal characteristics such as time-of-flight (TOF), signal strength, etc. may be used to determine the location of a particular person based on signals emitted by other computing device(s) carried/operated by the user. may be used singly or collectively by the client device 110 for
추가로 또는 대안적으로, 일부 구현에서, 클라이언트 장치(110)는 음성 인식을 수행하여 음성으로부터 사용자를 인식할 수 있다. 예를 들어, 자동화된 어시스턴트(120)의 일부 인스턴스는 예를 들어 다양한 리소스에 대한 액세스를 제공/제한하기 위해 사용자의 프로필에 음성을 일치시키도록 구성될 수 있다. 일부 구현에서, 화자의 움직임은 예를 들어 클라이언트 장치(110)의 존재 센서(111)(및 선택적으로 GPS 센서 및/또는 가속도계)에 의해 결정될 수 있다. 일부 구현에서, 이렇게 검출된 움직임에 기초하여, 사용자의 위치가 예측될 수 있고, 이 위치는 사용자의 위치에 대한 클라이언트 장치(110) 및/또는 다른 컴퓨팅 장치(들)에서 적어도 부분적으로 근접성에 기초하여 클라이언트 장치(110) 및/또는 다른 컴퓨팅 장치(들)에서 임의의 콘텐츠가 렌더링되게 할 때 사용자의 위치로 가정될 수 있다. 일부 구현에서, 사용자는 특히 마지막 참여 이후 많은 시간이 경과하지 않은 경우 자동화된 어시스턴트(120)와 참여했던 마지막 위치에 있는 것으로 간단하게 가정될 수 있다.Additionally or alternatively, in some implementations, client device 110 can perform voice recognition to recognize a user from voice. For example, some instances of automated assistant 120 may be configured to match a voice to a user's profile, for example, to provide/restrict access to various resources. In some implementations, movement of the speaker can be determined, for example, by the presence sensor 111 (and optionally a GPS sensor and/or accelerometer) of the client device 110 . In some implementations, based on such detected movement, a location of the user may be predicted, which location is based at least in part on proximity in client device 110 and/or other computing device(s) to the location of the user. so that the user's location may be assumed when causing any content to be rendered on the client device 110 and/or other computing device(s). In some implementations, the user may simply be assumed to be at the last location where they engaged with automated assistant 120, especially if not much time has elapsed since their last engagement.
클라이언트 장치(110)는 하나 이상의 사용자 인터페이스 입력 장치 (예: 마이크로폰, 터치스크린, 키보드 및/또는 기타 입력 장치) 및/또는 하나 이상의 사용자 인터페이스 출력 장치(예: 디스플레이, 스피커, 프로젝터 및/또는 기타 출력 장치)를 포함할 수 있는 사용자 인터페이스 컴포넌트(들)(112)를 더 포함한다. 더 나아가, 클라이언트 장치(110) 및/또는 임의의 다른 컴퓨팅 장치(들)는 데이터 및 소프트웨어 애플리케이션의 저장을 위한 하나 이상의 메모리, 데이터 액세스 및 애플리케이션 실행을 위한 하나 이상의 프로세서, 네트워크(들)(199)를 통한 통신을 용이하게 하는 기타 파라미터를 포함한다. 일부 구현에서, 클라이언트 장치(110), 다른 컴퓨팅 장치(들) 및/또는 자동화된 어시스턴트(120)에 의해 수행되는 동작은 여러 컴퓨터 시스템에 걸쳐 분산될 수 있으며, 다른 구현에서, 본 명세서에 기술된 동작은 클라이언트 장치(110) 또는 원격 시스템에서 배타적으로 수행될 수 있다. 자동화된 어시스턴트(120)는 예를 들어 네트워크(예를 들어, 도 1의 네트워크(들)(199))를 통해 서로 연결된 하나 이상의 위치에 있는 하나 이상의 컴퓨터에서 실행되는 컴퓨터 프로그램으로 구현될 수 있다. Client device 110 may include one or more user interface input devices (eg, a microphone, touchscreen, keyboard, and/or other input device) and/or one or more user interface output devices (eg, a display, speaker, projector, and/or other output device). and user interface component(s) 112, which may include a device). Further, the client device 110 and/or any other computing device(s) may include one or more memories for storage of data and software applications, one or more processors for accessing data and executing applications, and network(s) 199. and other parameters that facilitate communication via In some implementations, operations performed by client device 110, other computing device(s), and/or automated assistant 120 may be distributed across multiple computer systems, and in other implementations, the operations described herein Operations may be performed exclusively on the client device 110 or on a remote system. Automated assistant 120 may be implemented as a computer program running on one or more computers at one or more locations that are connected to each other via a network (eg, network(s) 199 in FIG. 1 ), for example.
전술한 바와 같이, 다양한 구현에서, 클라이언트 장치(110)는 자동화된 어시스턴트 클라이언트(113)를 작동시킬 수 있다. 다양한 실시예에서, 자동화된 어시스턴트 클라이언트(113)는 음성 캡처/자동 음성 인식(ASR: automatic speech recognition)/자연어 이해(NLU: natural language understanding)/TTS(text-to-speech)/이행 모듈(fulfillment module)(114)을 포함할 수 있다. 다른 구현에서, 각각의 음성 캡처/ASR/NLU/TTS/이행 모듈(114)의 하나 이상의 에스펙트(aspect)는 자동화된 어시스턴트 클라이언트(113)와 별도로 구현될 수 있다(예를 들어, 하나 이상의 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)에 의해). As noted above, in various implementations, the client device 110 may operate an automated assistant client 113. In various embodiments, automated assistant client 113 is a voice capture/automatic speech recognition (ASR)/natural language understanding (NLU)/text-to-speech/fulfillment module. module) (114). In other implementations, one or more aspects of each voice capture/ASR/NLU/TTS/fulfillment module 114 may be implemented separately from the automated assistant client 113 (e.g., one or more cloud by the based automated assistant component 115).
음성 캡처/ASR/NLU/TTS/이행 모듈(114)은 예를 들어 사용자의 음성을 캡처(음성 캡처, 예를 들어 각각의 마이크로폰(들)을 통해)하고(어떤 경우에는 존재 센서(111) 중 하나 이상을 포함할 수 있음)); 기계 학습(ML) 모델(들) 데이터베이스(120A)에 저장된 ASR 모델(들)을 사용하여 캡처된 오디오를 인식된 텍스트 및/또는 다른 표현 또는 임베딩으로 변환하고; ML 모델(들) 데이터베이스(120A)에 저장된 NLU 모델(들)을 사용하여 인식된 텍스트를 파싱 및/또는 주석을 달고(annotate); 및/또는 데이터를 얻고 및/또는 ML 모델(들) 데이터베이스(120A)에 저장된 하나 이상의 이행 규칙(들) 및/또는 이행 모델(들)을 사용하여 사용자의 말에 응답하여 ㅇo액션(들)이 수행되게 하기 위해 구조화된 요청을 생성하는 데 사용할 이행 데이터(fulfillment data)를 결정하는 것을 포함하는 하나 이상의 기능을 수행하도록 구성될 수 있다. 또한, 음성 캡처/ASR/NLU/TTS/이행 모듈(114)은 ML 모델(들) 데이터베이스(120A)에 저장된 TTS 모델(들)을 사용하여 텍스트를 음성으로 변환하고, TTS 변환에 기초하여 합성된 음성을 캡처하는 합성된 음성 오디오 데이터는 클라이언트 장치(110)의 스피커(들)을 통해 클라이언트 장치(110)의 사용자에게 가청적인 프레젠테이션을 위해 제공될 수 있다. 이러한 ML 모델(들)의 인스턴스(들)는 클라이언트 장치(110)에 로컬로 저장될 수 있고 및/또는 도 1의 네트워크(들)(199)를 통해 클라이언트 장치(110)에 의해 액세스 가능할 수 있다. 일부 구현에서, 클라이언트 장치(110)는 컴퓨팅 리소스(예: 프로세서 주기, 메모리, 배터리 등) 측면에서 상대적으로 제약을 받을 수 있으므로, 클라이언트 장치(110)에 로컬인 각각의 음성 캡처/ASR/NLU/TTS/이행 모듈(114)은 음성 인식 모델(들)을 사용하여 한정된 수의 서로 다른 음성 발화를 텍스트(또는 저차원 임베딩(lower dimensionality embedding)과 같은 다른 형식으로)로 변환하도록 구성될 수 있다. The Voice Capture/ASR/NLU/TTS/Fulfillment module 114 captures, for example, the user's voice (voice capture, e.g., via respective microphone(s)) and (in some cases, the presence sensor 111) may contain one or more)); convert captured audio to recognized text and/or other representations or embeddings using ASR model(s) stored in machine learning (ML) model(s) database 120A; parse and/or annotate the recognized text using the NLU model(s) stored in the ML model(s) database 120A; o Action(s) in response to user speech by obtaining data and/or using one or more transitive rule(s) and/or transitive model(s) stored in the ML model(s) database 120A may be configured to perform one or more functions including determining fulfillment data to use to generate a structured request to cause the request to be performed. In addition, the voice capture/ASR/NLU/TTS/fulfillment module 114 converts text to speech using the TTS model(s) stored in the ML model(s) database 120A, and synthesizes based on the TTS conversion. Synthesized voice audio data that captures voice may be presented for audible presentation to a user of the client device 110 via the speaker(s) of the client device 110 . Instance(s) of such ML model(s) may be stored locally on client device 110 and/or accessible by client device 110 via network(s) 199 of FIG. 1 . . In some implementations, the client device 110 may be relatively constrained in terms of computing resources (eg, processor cycles, memory, battery, etc.), so that each voice capture/ASR/NLU/ local to the client device 110 The TTS/Fulfillment module 114 may be configured to convert a limited number of different speech utterances into text (or into another format, such as a lower dimensionality embedding) using speech recognition model(s).
일부 음성 입력은 하나 이상의 클라우드 기반 자동화된 어시스턴트 파라미터(115)로 전송될 수 있으며, 이는 클라우드 기반 ASR 모듈(116), 클라우드 기반 NLU 모듈(117), 클라우드 기반 TTS 모듈(118) 및/또는 클라우드 기반 이행 모듈(119)을 포함할 수 있다. 이러한 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)는 클라이언트 장치(110)에 로컬인 음성 캡처/ASR/NLU/TTS/이행 모듈(114)과 관련하여 설명된 동일하거나 유사한 기능을 수행하기 위해 클라우드의 거의 무한한 리소스를 사용하고, 그러나 음성 캡처/ASR/NLU/TTS/이행 모듈(114)은 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)와 상호 작용하지 않고 클라이언트 장치(110)에서 로컬로 이 기능을 수행할 수 있다는 점에 유의해야 한다. Some speech inputs may be sent to one or more cloud-based automated assistant parameters 115, which may include a cloud-based ASR module 116, a cloud-based NLU module 117, a cloud-based TTS module 118, and/or a cloud-based TTS module 118. A fulfillment module 119 may be included. This cloud-based automated assistant component 115 can be used to perform the same or similar functions described with respect to the voice capture/ASR/NLU/TTS/fulfillment module 114 that is local to the client device 110, and has an almost infinite number of capabilities in the cloud. resources, but that the voice capture/ASR/NLU/TTS/fulfillment module 114 can perform this function locally on the client device 110 without interacting with the cloud-based automated assistant component 115. point should be noted.
도 1은 단일 사용자를 갖는 단일 클라이언트 장치에 대해 설명되며, 이는 예시를 위한 것이며 제한하려는 의도가 아님을 이해해야 한다. 예를 들어, 사용자의 하나 이상의 추가 클라이언트 장치는 또한 여기에 설명된 기술을 구현할 수 있다. 이러한 추가적인 클라이언트 장치는 (예를 들어, 네트워크(들)(199)를 통해) 클라이언트 장치(110)와 통신할 수 있다. 다른 예로서, 클라이언트 장치(110)는 공유 설정(예를 들어, 사용자 그룹, 가정, 호텔 방, 회사의 공유 공간 등)에서 복수의 사용자에 의해 사용될 수 있다.1 is described for a single client device with a single user, it should be understood that this is for illustrative purposes and is not intended to be limiting. For example, one or more additional client devices of a user may also implement the techniques described herein. These additional client devices may communicate with client device 110 (eg, via network(s) 199 ). As another example, the client device 110 may be used by multiple users in a shared setting (eg, a group of users, a home, a hotel room, a shared space at work, etc.).
일부 구현에서, 클라이언트 장치(110)는 사용자 입력에 포함되고 그리고 특정 추론을 요청하는 추가 사용자 입력에 대한 응답으로 클라이언트 장치(110)의 사용자에게 프레젠테이션하기 위해 제공될 자동화된 어시스턴트(120)로 향하는 어시스턴트 명령의 이행에 대한 특정 추론(reasoning)을 야기하는 데 사용되는 다양한 엔진을 더 포함할 수 있다. 예를 들어, 도 1에 도시된 바와 같이, 클라이언트 장치(110)는 요청 엔진(130) 및 추론 엔진(140)을 더 포함할 수 있다. 클라이언트 장치(110)는 사용자 프로파일(들) 데이터베이스(110A), ML 모델(들) 데이터베이스(120A) 및 메타데이터 데이터베이스(140A)를 포함하는 온-디바이스 메모리를 더 포함할 수 있다. 일부 구현에서, 이러한 다양한 엔진은 클라이언트 장치(110)에서만 독점적으로 실행될 수 있다. 추가 또는 대체 구현에서, 이들 다양한 엔진 중 하나 이상이 클라이언트 장치(110)로부터 원격으로 실행될 수 있다(예를 들어, 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)의 일부로서). 예를 들어, 어시스턴트 명령이 클라이언트 장치(110)에서 로컬로 수행되는 구현에서, 이들 다양한 엔진의 온-디바이스 인스턴스가 여기에서 설명된 동작을 수행하는 데 사용될 수 있다. 그러나, 어시스턴트 명령(assistant command)이 (예를 들어, 클라우드 기반 자동화된 어시스턴트 컴포넌트(115)의 일부로서) 클라이언트 장치(110)로부터 원격으로 이행되는 구현에서, 이러한 다양한 엔진의 원격 인스턴스가 본 명세서에 설명된 동작을 수행하기 위해 사용될 수 있다. In some implementations, the client device 110 includes the user input and directs the assistant to the automated assistant 120 to be presented for presentation to the user of the client device 110 in response to additional user input requesting certain inferences. It may further include various engines used to cause specific reasoning about the execution of instructions. For example, as shown in FIG. 1 , the client device 110 may further include a request engine 130 and an inference engine 140 . The client device 110 may further include on-device memory containing a user profile(s) database 110A, an ML model(s) database 120A and a metadata database 140A. In some implementations, these various engines may run exclusively on the client device 110 . In additional or alternative implementations, one or more of these various engines may be executed remotely from the client device 110 (eg, as part of a cloud-based automated assistant component 115). For example, in implementations where assistant commands are performed locally on the client device 110, on-device instances of these various engines may be used to perform the operations described herein. However, in implementations where assistant commands are executed remotely from client device 110 (eg, as part of cloud-based automated assistant component 115), remote instances of these various engines are described herein. Can be used to perform the described operations.
일부 구현에서, 클라이언트 장치의 사용자 인터페이스 컴포넌트(들)(112)를 통해 검출되고 자동화된 어시스턴트(120)로 향하는 사용자 입력에 포함된 어시스턴트 명령을 이행한 후, 요청 엔진(130)은 추가 사용자 입력이 요청을 포함하는지 여부를 결정하기 위해 추가 사용자 입력이 프로세싱되게 할 수 있다(예를 들어, 음성 캡처/ASR/NLU/TTS/이행 모듈(114)의 하나 이상의 에스펙트를 사용하여). 예를 들어, 클라이언트 장치(110)의 사용자가 클라이언트 장치(110) 사용자의 집에 있는 조명이 꺼진 상태에서 켜진 상태로 켜지도록 "turn on the lights(조명 켜)"라는 음성 발화를 제공한다고 가정한다. 도 2 및 도 3과 관련하여 더 상세히 기술된 바와 같이, 음성 발화를 캡처하는 오디오 데이터는 ASR 출력(선택적으로 ASR 메트릭 포함)을 생성하기 위해 ML 모델(들) 데이터베이스(120A)에 저장된 ASR 모델(들)을 사용하여 프로세싱될 수 있고, ASR 출력은 NLU 출력(선택적으로 NLU 메트릭 포함)을 생성하기 위해 ML 모델(들) 데이터베이스(120A)에 저장된 NLU 모델(들)을 사용하여 프로세싱될 수 있고, NLU 출력은 이행 출력(fulfillment output)을 생성하기 위해 ML 모델(들) 데이터베이스(120A)에 저장된 이행 규칙(들) 및/또는 이행 모델(들)을 사용하여 프로세싱될 수 있다. 이행 출력과 연관된 구조화된 요청(들)은 클라이언트 장치(110)에서 로컬로 및/또는 제1-파티 서버(들)(191) 및/또는 제3-파티 서버(들)(192)에서 원격으로 실행되는 다양한 소프트웨어 애플리케이션과 같은 하나 이상의 이행자(fulfiller)에게 전송될 수 있고 그리고 하나 이상의 이행 후보들이 구조화된 요청(들)에 응답하여 생성될 수 있다(각 이행 메트릭은 하나 이상의 해당 이행 측정과 연관될 수 있음). 자동화된 어시스턴트(120)는 하나 이상의 이행 후보가 ASR 메트릭, NLU 메트릭 및/또는 이행 메트릭에 기초하여 순위가 매겨지게 할 수 있다. ASR 메트릭, NLU 메트릭, 이행 메트릭, 및/또는 음성 발화의 이행과 연관된 임의의 다른 데이터는 메타데이터 데이터베이스(140A)에 저장될 수 있다. 이 데이터는 이후에 (예를 들어, 도 2, 3, 4a-4c 및 5a-5b와 관련하여) 본 명세서에 기술된 바와 같이 어시스턴트 명령의 이행을 위한 특정 추론을 제공하는 것과 연관된 데이터를 결정하기 위해 자동화된 어시스턴트(120)에 의해 액세스될 수 있다. In some implementations, after fulfilling an assistant command contained in user input detected via user interface component(s) 112 on the client device and directed to automated assistant 120, request engine 130 determines that additional user input is Additional user input may be processed to determine whether or not to include the request (eg, using one or more aspects of the Voice Capture/ASR/NLU/TTS/Fulfillment module 114). For example, it is assumed that the user of the client device 110 provides a voice utterance "turn on the lights" so that the lights in the user's home of the client device 110 are turned on from an off state. . As described in more detail with respect to FIGS. 2 and 3 , audio data that captures spoken utterances is stored in the ML model(s) database 120A to generate ASR outputs (optionally including ASR metrics) (ASR models ( s), the ASR output may be processed using the NLU model(s) stored in the ML model(s) database 120A to generate an NLU output (optionally including NLU metrics), The NLU output may be processed using fulfillment rule(s) and/or fulfillment model(s) stored in ML model(s) database 120A to generate fulfillment output. The structured request(s) associated with the fulfillment output can be sent locally on client device 110 and/or remotely on first-party server(s) 191 and/or third-party server(s) 192. It can be sent to one or more fulfillers, such as various software applications that run, and one or more fulfillment candidates can be generated in response to the structured request(s) (each fulfillment metric can be associated with one or more corresponding fulfillment measures). can). Automated assistant 120 may cause one or more transition candidates to be ranked based on an ASR metric, an NLU metric, and/or a compliance metric. ASR metrics, NLU metrics, fulfillment metrics, and/or any other data associated with the fulfillment of spoken utterances may be stored in the metadata database 140A. This data is then used to determine data associated with providing specific inferences for implementation of assistant commands as described herein (eg, with respect to FIGS. 2, 3, 4a-4c and 5a-5b). can be accessed by automated assistant 120 for
그러나, 이 예에서, 클라이언트 장치(110)의 사용자가 클라이언트 장치 사용자의 거주지에서 조명을 제어하는 것과 연관된 소프트웨어 애플리케이션 또는 서비스에 대한 자동화된 어시스턴트(120) 액세스를 허가하지 않았다고 가정한다. 따라서, 이 예에서 하나 이상의 이행 후보는 조명을 켜는 데 사용되는 데이터기 이러한 이행자 중 하나 이상에 대한 액세스 부족으로 인해 적어도 결정할 수 없기 때문에 클라이언트 장치(110)에서 또는 음성 발화를 만족시킬 수 있는 서버(예를 들어, 제1-파티 서버(들)(191) 및/또는 제3-파티 서버)에서 액세스 가능한 소프트웨어 애플리케이션이 없음을 나타낼 수 있다. 그 결과, 자동화된 어시스턴트(120)는 클라이언트 장치(110)의 사용자에게 자동화된 어시스턴트(120)가 음성 발화를 만족시킬 수 없음을 통지하는데 사용될 대체 데이터를 결정할 수 있다. 따라서, 자동화된 어시스턴트(120)는 음성 캡처/ASR/NLU/TTS/이행 모듈(114)을 사용하여 대체 데이터를 기반으로 예를 들어 "Sorry, but I cannot turn on the lights(미안하지만 조명을 켤 수 없습니다)"와 같은 합성된 음성을 포함하는 합성된 음성 오디오 데이터를 생성하고 그리고 클라이언트 장치(110)의 스피커를 통한 가청적인 프레젠테이션을 위해 합성된 음성이 제공되도록 한다. However, in this example, it is assumed that the user of client device 110 has not authorized automated assistant 120 access to a software application or service associated with controlling the lighting at the client device user's residence. Thus, the one or more transition candidates in this example are either on the client device 110 or on the server that can satisfy the voice utterance (because the data used to turn on the lights cannot be determined at least due to lack of access to one or more of these transitions). For example, it may indicate that no software applications are accessible on first-party server(s) 191 and/or third-party servers. As a result, automated assistant 120 can determine alternative data to be used to notify the user of client device 110 that automated assistant 120 cannot satisfy the spoken utterance. Thus, automated assistant 120 uses voice capture/ASR/NLU/TTS/fulfillment module 114 to, for example, "Sorry, but I cannot turn on the lights" based on alternative data. can't)" and cause the synthesized voice to be presented for audible presentation through the speaker of the client device 110.
일부 구현에서, 클라이언트 장치(110)의 사용자가 자동화된 어시스턴트(120)가 어시스턴트 명령의 특정 이행이 수행되게 한 이유에 대한 특정 추론을 제공하도록 요청하는 추가 사용자 입력을 제공한다고 가정하면, 요청 엔진(130)은 요청이 이행에 대한 특정 추론에 대한 일반적인 요청인지 또는 이행에 대한 특정 추론에 대한 특정 요청인지를 결정할 수 있다. 요청 엔진(130)은 추가 사용자 입력 프로세싱에 기초하여 생성된 적어도 NLU 출력에 기초하여 요청이 이행에 대한 특정 추론에 대한 일반 요청인지 또는 이행에 대한 특정 추론에 대한 특정 요청인지를 결정할 수 있고, 추론 엔진(140)은 요청 유형에 기초하여 특정 추론을 제공하는 데 사용되는 것으로 결정된 추가 데이터를 적응시킬 수 있다(예를 들어, 도 2, 3, 4a-4c 및 5a-5b와 관련하여 기술된 바와 같이). 추가적인 또는 대안적인 구현에서, 자동화된 어시스턴트(120)는 수행될 때 자동화된 어시스턴트(120)가 어시스턴트 명령의 특정 이행을 야기할 수 있게 하는 추천된 액션을 결정하기 위해 추천 데이터를 획득할 수 있다. 이 예에서, 추천된 액션은 자동화된 어시스턴트(120)가 조명을 제어하게 할 수 있는 소프트웨어 애플리케이션 또는 서비스에 대한 액세스를 승인하는 것을 포함할 수 있다. 따라서, 자동화된 어시스턴트(120)는 음성 캡처/ASR/NLU/TTS/이행 모듈(114)을 사용하여 추천 데이터를 기반으로 예를 들어 "I would be able to control the lights if you granted me access to lights application(당신이 허락했다면 조명을 제어할 수 있을 것입니다.)"와 같은 합성된 음성을 포함하는 추가 합성 음성 오디오 데이터를 생성할 수 있으며, 합성된 음성이 클라이언트 장치(110)의 스피커(들)를 통해 가청적인 프레젠테이션을 위해 제공되도록 한다. 위의 설명은 예시를 위해 제공되고 제한하려는 의도가 아님을 이해해야 하며, 여기에 설명된 기술의 추가 설명이 도 2, 3, 4a-4c 및 5a-5b와 관련하여 아래에 제공된다는 점에 유의해야 한다. In some implementations, assuming that the user of client device 110 provides additional user input requesting that automated assistant 120 provide a particular inference as to why a particular implementation of an assistant command was performed, the request engine ( 130) may determine whether the request is a general request for a specific inference about a fulfillment or a specific request for a specific inference about a fulfillment. The request engine 130 may determine whether a request is a general request for a specific inference for a fulfillment or a specific request for a specific inference for a fulfillment based on at least the NLU output generated based on further user input processing, and the inference Engine 140 may adapt additional data determined to be used to provide a particular inference based on the type of request (e.g., as described with respect to FIGS. 2, 3, 4a-4c and 5a-5b). together). In additional or alternative implementations, automated assistant 120 may obtain recommendation data to determine recommended actions that, when performed, enable automated assistant 120 to cause specific fulfillment of assistant commands. In this example, the recommended action may include granting access to a software application or service that may allow automated assistant 120 to control the lighting. Accordingly, automated assistant 120 uses voice capture/ASR/NLU/TTS/fulfillment module 114 to provide, for example, "I would be able to control the lights if you granted me access to lights" based on the recommendation data. application (which will be able to control the lights if you've allowed it)", where the synthesized voice is sent to the speaker(s) of the client device 110. to be provided for audible presentation. It should be understood that the above description is provided for purposes of illustration and is not intended to be limiting, and it should be noted that further description of the techniques described herein is provided below with respect to FIGS. 2, 3, 4a-4c and 5a-5b. do.
도 2를 참조하면, 사용자 입력에 포함되고 수행될 자동화된 어시스턴트로 향하는 어시스턴트 명령의 특정 이행을 야기하는 예시적인 방법(200)을 예시하는 흐름도가 묘사된다. 편의상, 방법(200)의 동작은 동작을 수행하는 시스템을 참조하여 설명된다. 방법(200)의 이러한 시스템은 하나 이상의 프로세서, 메모리, 및/또는 컴퓨팅 장치(들)의 다른 컴포넌트(들)를 포함한다(예를 들어, 도 1의 클라이언트 장치(110), 도 4a-4c의 클라이언트 장치(410), 도 5a-5b의 클라이언트 장치(510), 도 6의 컴퓨팅 장치(610), 하나 이상의 서버 및/또는 다른 컴퓨팅 장치). 더욱이, 방법(200)의 동작이 특정 순서로 도시되어 있지만, 이는 제한을 의미하지 않는다. 하나 이상의 작업을 재정렬, 생략 및/또는 추가할 수 있다. Referring to FIG. 2 , a flow diagram illustrating an example method 200 for causing specific implementation of assistant commands contained in user input and destined for an automated assistant to be performed is depicted. For convenience, the operation of method 200 is described with reference to a system that performs the operation. This system of method 200 includes one or more processors, memory, and/or other component(s) of computing device(s) (e.g., client device 110 in FIG. 1, FIGS. 4A-4C). client device 410, client device 510 of FIGS. 5A-5B, computing device 610 of FIG. 6, one or more servers and/or other computing devices). Moreover, while operations of method 200 are shown in a particular order, this is not meant to be limiting. You can rearrange, omit, and/or add one or more tasks.
블록(252)에서, 시스템은 어시스턴트 명령을 포함하고 자동화된 어시스턴트로 향하는 사용자 입력을 클라이언트 장치의 사용자로부터 수신한다. 일부 구현에서, 사용자 입력은 클라이언트 장치의 마이크로폰(들)에 의해 생성된 오디오 데이터에서 캡처된 음성 발화에 대응할 수 있다. 추가적인 또는 대안적인 구현에서, 사용자 입력은 클라이언트 장치의 디스플레이 또는 클라이언트 장치의 다른 입력 장치(예를 들어, 키보드 및/또는 마우스)를 통해 수신된 터치 입력 또는 타이핑된 입력에 대응할 수 있다. At block 252, the system receives user input from a user of the client device that includes assistant commands and is directed to the automated assistant. In some implementations, user input may correspond to spoken utterances captured in audio data generated by the microphone(s) of the client device. In additional or alternative implementations, user input may correspond to touch input or typed input received through a display of the client device or another input device (eg, a keyboard and/or mouse) of the client device.
블록(254)에서, 시스템은 어시스턴트 명령의 특정 이행을 수행하는데 사용될 데이터를 결정하기 위해 사용자 입력을 프로세싱한다. 사용자 입력이 음성 발화에 해당하는 구현에서, 음성 발화를 캡처하는 오디오 데이터는 ASR 모델(들)을 사용하여 프로세싱되어 ASR 출력(예: 음성 가설(들), 음소(들) 및/또는 기타 ASR 출력) 및 선택적으로 ASR 출력과 연관된 ASR 메트릭을 생성할 수 있다. 또한 NLU 모델을 사용하여 ASR 출력을 프로세싱하여 NLU 출력(예: ASR 출력에 기초하여 결정된 의도, ASR 출력에 기초하여 결정된 의도와 연관된 파라미터에 대한 슬롯 값 등) 및 선택적으로 NLU 출력과 연관된 NLU 메트릭을 생성할 수 있다. 또한, NLU 출력은 어시스턴트 명령의 이행에 사용되는 데이터를 얻기 위해 전송될 요청을 생성하는데 사용되는 이행 출력을 생성하기 위해 및/또는 이행 출력 및 선택적으로 이행 데이터와 연관된 이행 메트릭에 기초하여 액션(들)이 수행되도록(예를 들어, 도 1의 제1-파티 서버(들)(191), 도 1의 제3-파티 서버(들)(192), 클라이언트 장치에서 로컬로 구현된 제1-파티 소프트웨어 애플리케이션(들), 제3-파티 클라이언트 장치 등에서 로컬로 구현된 소프트웨어 애플리케이션) 이행 규칙 및/또는 이행 모델을 사용하여 프로세싱될 수 있다. 사용자 입력이 터치 입력 또는 타이핑된 입력에 해당하는 구현에서, 터치 입력 또는 타이핑된 입력에 해당하는 텍스트는 NLU 모델을 사용하여 NLU 출력 및 선택적으로 NLU 출력과 연관된 NLU 메트릭을 생성하여 프로세싱될 수 있다. 또한 NLU 출력은, 어시스턴트 명령의 이행 수행에 사용되는 데이터를 얻기 위해 전송될 요청을 생성하는 데 사용되는 이행 출력을 생성하기 위해 및/또는 어시스턴트 명령의 이행 수행 시 이행 출력에 따라 액션(들)이 수행되도록, 이행 규칙 및/또는 이행 모델을 사용하여 프로세싱될 수 있다. At block 254, the system processes the user input to determine data to be used to perform a particular implementation of the assistant command. In implementations where the user input corresponds to a spoken utterance, the audio data capturing the spoken utterance is processed using the ASR model(s) to produce an ASR output (e.g., speech hypothesis(s), phoneme(s), and/or other ASR outputs. ) and optionally an ASR metric associated with the ASR output. The NLU model is also used to process the ASR output to obtain NLU outputs (e.g., intents determined based on the ASR outputs, slot values for parameters associated with intents determined based on the ASR outputs, etc.) and, optionally, NLU metrics associated with the NLU outputs. can create In addition, the NLU output may be used to generate a request to be sent to obtain data used in the fulfillment of an assistant command, and/or an action(s) based on a fulfillment metric associated with the fulfillment output and/or optionally the fulfillment data. ) is performed (e.g., first-party server(s) 191 of FIG. 1, third-party server(s) 192 of FIG. 1, locally implemented first-party server(s) 192 on the client device. software application(s), software applications locally implemented on third-party client devices, etc.) may be processed using compliance rules and/or compliance models. In implementations where the user input corresponds to touch input or typed input, the text corresponding to the touch input or typed input may be processed using the NLU model to generate an NLU output and optionally an NLU metric associated with the NLU output. NLU outputs may also be used to generate transitional outputs used to generate requests to be sent to obtain data used in performing fulfillment of an assistant command and/or when performing a fulfillment of an assistant command, an action(s) based on a transitional output may be performed. To be performed, it may be processed using transitive rules and/or transitive models.
블록(256)에서, 시스템은 자동화된 어시스턴트가 데이터를 사용하여 어시스턴트 명령의 특정 이행을 수행하게 한다. 특히, 이행 출력은 결과적으로 하나 이상의 이행 후보를 생성하는, 도 1의 제1-파티 서버(들)(191), 도 1의 제3-파티 서버(들)(192), 클라이언트 장치에서 로컬로 구현된 제1-파티 소프트웨어 애플리케이션(들), 클라이언트 장치에 로컬로 구현된 제3-파티 소프트웨어 애플리케이션 중 하나 이상에 전송된 데이터를 포함할 수 있다. 시스템은 예를 들어 ASR 메트릭, NLU 메트릭 및/또는 이행 메트릭에 기초하여 어시스턴트 명령의 특정 이행을 수행하기 위해 하나 이상의 이행 후보 중에서 특정 이행 후보를 선택할 수 있다. 예를 들어, 클라이언트 장치의 사용자가 "play rock music(록 음악 재생)"라는 음성 발화를 제공한다고 가정한다. 이 예에서, 음성 발화를 캡처하는 오디오 데이터는 ASR 모델(들)을 사용하여 프로세싱되어, ASR 출력으로서, 제1 ASR 메트릭과 연관된 "록 음악 재생"의 제1 음성 가설(예를 들어, 제1 음성 가설이 확률, 이진 값, 로그 우도 등과 같이, 음성 발화에 포함된 용어(들) 및/또는 구(들)에 대응할 가능성) 및 제2 ASR 메트릭과 연관된 "play Bach music(바흐 음악 재생)"의 제2 음성 가설, 및/또는 다른 음성 가설 및 해당 메트릭을 생성할 수 있다. 또한, 각각의 음성 가설은 NLU 모델(들)을 사용하여 프로세싱되어, NLU 데이터로서, 제1 NLU 메트릭과 연관된 장르 파라미터에 대해 "록"의 슬롯 값을 갖는 "play music(음악 재생)"의 제1 의도(intent)(예를 들어, 제1 의도 및 슬롯 값(들)이 확률, 이진 값, 로그 우도 등과 같은 사용자의 원하는 의도에 대응할 가능성) 및 제2 NLU 메트릭과 연관된 아티스트 파라미터에 대한 슬롯 값("Bach")을 갖는 "play music(음악 재생)"의 제2 의도를 생성할 수 있다. At block 256, the system causes the automated assistant to use the data to perform specific implementations of the assistant commands. In particular, the fulfillment output is locally on the first-party server(s) 191 of FIG. 1, the third-party server(s) 192 of FIG. 1, the client device, which in turn generates one or more fulfillment candidates. may include data transferred to one or more of an implemented first-party software application(s), a third-party software application implemented locally on the client device. The system may select a particular transition candidate from among one or more transition candidates to perform a particular transition of an assistant command based on, for example, an ASR metric, an NLU metric, and/or a transition metric. For example, it is assumed that the user of the client device provides a voice utterance "play rock music". In this example, audio data capturing a spoken utterance is processed using the ASR model(s) so that, as an ASR output, a first voice hypothesis of “rock music playing” associated with the first ASR metric (e.g., the first probability that the negative hypothesis corresponds to the term(s) and/or phrase(s) contained in the spoken utterance, such as probability, binary value, log likelihood, etc.) and "play Bach music" associated with the second ASR metric. second negative hypotheses of , and/or other negative hypotheses and corresponding metrics. In addition, each voice hypothesis is processed using the NLU model(s) so that, as NLU data, the category of “play music” with a slot value of “rock” for the genre parameter associated with the first NLU metric 1 Intent (e.g., the likelihood that the first intent and the slot value(s) correspond to the user's desired intent, such as probability, binary value, log likelihood, etc.) and the slot value for the artist parameter associated with the second NLU metric You can create a second intent of “play music” with (“Bach”).
더욱이, 이 예에서, 하나 이상의 이행 후보는 예를 들어, 제1 이행 메트릭과 연관된 제1-파티 미디어 애플리케이션을 사용하여 록 음악을 재생하는 제1 이행 후보(예: 시스템의 이행 요청에 응답하는 제1-파티 미디어 애플리케이션에 의해 시스템으로 반환됨), 제2 이행 메트릭과 연관된 제3-파티 미디어 애플리케이션을 사용하여 록 음악을 재생하는 제2 이행 후보(예: 시스템의 이행 요청에 응답하는 제3-파티 미디어 애플리케이션에 의해 시스템으로 반환됨), 제3 이행 메트릭과 연관된 제1-파티 미디어 애플리케이션을 사용하여 "바흐(Bach)" 음악을 재생하는 제3 이행 후보(예: 시스템의 이행 요청에 응답하는 제1-파티 미디어 애플리케이션에 의해 시스템으로 반환됨), 제4 이행 메트릭과 연관된 제3-파티 미디어 애플리케이션을 사용하여 바흐 음악을 재생하는 제4 이행 후보(예: 시스템의 이행 요청에 응답하는 제3-파티 미디어 애플리케이션에 의해 시스템으로 반환됨) 및/또는 다른 이행 후보를 포함할 수 있다. 이 예에서, ASR 메트릭, NLU 메트릭 및/또는 이행 메트릭이 제1-파티 미디어 애플리케이션을 사용하여 록 음악을 재생하는 제1 이행 후보가 음성 발화를 충족할 가능성이 가장 높다는 것을 나타낸다고 가정하면, 자동화된 어시스턴트는 어시스턴트 명령의 특정 이행으로 제1-파티 미디어 애플리케이션이 록 음악을 재생하도록 할 수 있다. 이 예에서, 시스템은, "음악 재생" 의도와 연관된 다른 파라미터 및 두 가지 해석(예: NLU 출력의 일부)에 대한 슬롯 값을 추론할 수 있다. 예를 들어, "록 음악 재생"의 제1 해석과 관련하여, 시스템은, 시스템에서 액세스할 수 있는 경우 사용자 프로필 데이터(예를 들어, 도 1의 클라이언트 장치(110)의 사용자 프로파일(들) 데이터베이스(110A)에 저장됨)를 기반으로, 아티스트 파라미터(예: 사용자가 가장 많이 듣는 록 아티스트)에 대한 아티스트 슬롯 값을 추론하고, 소프트웨어 애플리케이션 파라미터(예: 사용자가 음악을 듣기 위해 가장 많이 사용하는 애플리케이션)에 대한 소프트웨어 애플리케이션 슬롯 값을 추론하고, 노래 파라미터 등(예: 사용자가 가장 많이 들은 록 곡 또는 가장 많이 들은 바흐 작곡)에 대한 노래 슬롯 값을 추론할 수 있다. 그렇지 않으면 시스템이 이러한 파라미터 중 하나 이상에 대해 기본 슬롯 값을 사용할 수 있다. 다른 예에서, 사용자는 특정 아티스트, 특정 아티스트, 특정 소프트웨어 애플리케이션 등과 같은 이러한 파라미터 중 하나 이상에 대한 슬롯 값을 지정할 수 있다. Furthermore, in this example, the one or more transition candidates may include, for example, a first transition candidate that plays rock music using a first-party media application associated with a first transition metric (e.g., a first transition candidate responding to a transition request from the system). returned to the system by the first-party media application), and a second transition candidate playing rock music using a third-party media application associated with the second fulfillment metric (e.g., a third-party media application responding to the system's fulfillment request). returned to the system by the party media application), and a third fulfillment candidate that plays "Bach" music using the first-party media application associated with the third fulfillment metric (e.g., responding to the system's fulfillment request). returned to the system by the first-party media application), and a fourth transition candidate that plays Bach music using the third-party media application associated with the fourth fulfillment metric (e.g., a third transition response that responds to the system's fulfillment request). -returned to the system by the party media application) and/or other transition candidates. In this example, assuming that the ASR metric, the NLU metric and/or the fulfillment metric indicate that the first transitional candidate playing rock music using the first-party media application is most likely to satisfy the spoken utterance, the automated The assistant may cause the first-party media application to play rock music with specific implementation of the assistant command. In this example, the system can infer slot values for two interpretations (eg, part of NLU output) and other parameters associated with the "play music" intent. For example, with respect to the first interpretation of “play rock music,” the system may, if accessible by the system, use user profile data (e.g., a database of user profile(s) of client device 110 of FIG. 1 ). 110A), infer artist slot values for artist parameters (e.g. rock artist the user listens to the most), and software application parameters (e.g. the application the user uses most to listen to music). ), infer song slot values for song parameters, etc. (e.g., the user's most listened to rock songs or most listened to Bach compositions). Otherwise, the system may use default slot values for one or more of these parameters. In another example, a user may specify a slot value for one or more of these parameters, such as a specific artist, a specific artist, a specific software application, and the like.
일부 구현에서, 블록(256)은 서브-블록(256A)을 포함할 수 있다. 포함된다면, 서브-블록(256A)에서, 시스템은 특정 이행에 대한 특정 추론과 연관된 하나 이상의 선택 가능한 요소가 사용자에게 제공되도록 한다. 특정 이행에 대한 특정 추론은 예를 들어, 사용자 입력에 응답하는 것으로서 하나 이상의 이행 후보 중에서 특정 이행 후보가 선택된 하나 이상의 이유를 포함할 수 있다. 클라이언트 장치의 사용자로부터, 특정 선택 가능한 요소, 하나 이상의 선택 가능한 요소에 대한 사용자 선택을 수신하는 것에 응답하여, 시스템은 클라이언트 장치의 사용자에게 프레젠테이션하기 위해 특정 추론이 제공되도록 할 수 있다. 이러한 구현의 일부 버전에서, 시스템은 ASR 메트릭, NLU 메트릭, 및/또는 선택된 특정 이행 후보와 연관된 이행 메트릭이 메트릭 임계값을 충족하지 못한다는 결정에 응답하여 특정 이행에 대한 특정 추론과 관련된 하나 이상의 선택 가능한 요소가 사용자에게 제공되도록 할 수 있다. 달리 말하면, 선택된 특정 이행 후보가 사용자가 의도한 것일 가능성이 가장 높지만 시스템이 선택된 특정 이행 후보에 대해 그다지 확신하지 못한다고 시스템이 결정하는 것에 응답하여, 시스템은 특정 이행에 대한 특정 추론과 관련된 하나 이상의 선택 가능한 요소가 사용자에게 제공되도록 할 수 있다. 이러한 구현의 다른 버전에서, 시스템은 ASR 메트릭, NLU 메트릭 및/또는 선택된 특정 이행 후보와 연관된 이행 메트릭에 관계없이 특정 이행에 대한 특정 추론과 관련된 하나 이상의 선택 가능한 요소가 사용자에게 제공되도록 할 수 있다. 여기에 설명된 바와 같이(예를 들어, 도 5b와 관련하여), 하나 이상의 선택 가능한 요소는 일반적인 요청(general request) 또는 하나 이상의 대응하는 특정 요청(particular request)과 연관될 수 있다. In some implementations, block 256 may include sub-block 256A. If included, at sub-block 256A, the system causes the user to be presented with one or more selectable elements associated with a particular inference for a particular implementation. A particular inference for a particular transition may include one or more reasons why a particular transition candidate was selected from among one or more transition candidates, for example, as responsive to user input. In response to receiving a particular selectable element, or a user selection of one or more selectable elements, from the user of the client device, the system may cause the particular inference to be presented for presentation to the user of the client device. In some versions of these implementations, the system may select one or more choices associated with a particular inference for a particular transition in response to a determination that an ASR metric, an NLU metric, and/or a performance metric associated with a particular transition candidate selected do not satisfy a metric threshold. Possible elements can be made available to the user. In other words, in response to the system determining that the particular transition candidate selected is most likely what the user intended, but the system is not very certain about the particular transition candidate selected, the system makes one or more selections related to a particular inference about the particular transition. Possible elements can be made available to the user. In another version of this implementation, the system may cause the user to be presented with one or more selectable elements related to a particular inference for a particular transition, regardless of the ASR metric, the NLU metric, and/or the performance metric associated with the particular transition candidate selected. As described herein (eg, with respect to FIG. 5B ), one or more selectable elements may be associated with a general request or with one or more corresponding particular requests.
블록(258)에서, 시스템은 자동화된 어시스턴트가 특정 이행을 수행하게 한 이유에 대한 특정 추론의 요청이 클라이언트 장치에서 수신되는지 여부를 결정한다. 일부 구현에서, 자동화된 어시스턴트가 특정 이행을 수행하게 한 이유에 대한 특정 추론의 요청은 클라이언트 장치에서 수신되는 추가 사용자 입력에 포함될 수 있다. 이러한 구현의 일부 버전에서, 시스템은 추가 사용자 입력이 특정 추론에 대한 요청을 포함하는지 여부를 결정하기 위해 블록(252)과 관련하여 설명된 것과 동일하거나 유사한 방식으로, ASR 모델(들)을 사용하여 추가 사용자 입력을 프로세싱하여 ASR 출력을 생성하고, NLU 모델(들)을 사용하여 NLU 데이터를 생성하고, 이행 규칙(들) 또는 모델(들)을 사용하여 이행 출력을 생성할 수 있다. 예를 들어, 추가 사용자 입력이 음성 발화 또는 타이핑된 입력인지 여부에 관계없이 추가 사용자를 프로세싱하여 NLU 출력을 생성할 수 있으며, 시스템은 자동화된 어시스턴트가 NLU 출력(예: 특정 추론에 대한 요청과 연관된 의도)에 기초하여 특정 이행을 수행하게 한 이유에 대한 특정 추론의 요청이 추가 사용자 입력에 포함되어 있는지 여부를 결정할 수 있다. 이러한 구현의 추가 또는 대체 버전에서, 자동화된 어시스턴트가 특정 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청은 클라이언트 장치에서 수신되는 하나 이상의 선택 가능한 요소의 사용자 선택에 포함될 수 있다(예를 들어, 서브-블록(256A)에 대해 전술한 바와 같이). 블록(258)의 반복에서, 시스템은 자동화된 어시스턴트가 특정 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청이 클라이언트 장치에서 수신되지 않는다고 결정하면, 시스템은 자동화된 어시스턴트가 블록 258에서 특정 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청을 계속 모니터링한다(그리고 선택적으로 특정 이행이 수행되게 한 후 임계 기간 동안(예를 들어, 5초, 10초, 15초, 및/또는 임의의 다른 임계 기간 동안)). 블록(258)의 반복에서, 자동화된 어시스턴트가 특정 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청이 클라이언트 장치에서 수신되었다고 시스템이 결정하면, 시스템은 블록(260)으로 진행한다. At block 258, the system determines whether a request for a particular inference as to why the automated assistant performed a particular action was received at the client device. In some implementations, a request for a specific inference as to why the automated assistant performed a specific action may be included in additional user input received at the client device. In some versions of such implementations, the system may use the ASR model(s), in the same or similar manner as described with respect to block 252, to determine whether additional user input includes a request for a particular inference. Additional user input may be processed to generate ASR output, NLU model(s) may be used to generate NLU data, and transitive rule(s) or model(s) may be used to generate transitive output. For example, additional user inputs may be processed to generate NLU outputs, whether those additional user inputs are spoken utterances or typed inputs, and the system may allow an automated assistant to generate NLU outputs (e.g., associated with a request for a particular inference). intent), it may be determined whether the additional user input includes a request for a specific inference as to why a specific action was performed. In additional or alternative versions of these implementations, a request for a particular inference as to why the automated assistant performed a particular action could be included in the user's selection of one or more selectable elements received at the client device (e.g., as described above for sub-block 256A). In repetition of block 258, if the system determines that no request for a particular inference as to why it caused the automated assistant to perform a particular action was received from the client device, the system determines that the automated assistant performed the particular action in block 258. Continue monitoring requests for specific inferences as to why a particular action was performed (and optionally for a threshold period after a specific action was caused to be performed (e.g., 5 seconds, 10 seconds, 15 seconds, and/or any other threshold for a period)). In iterations of block 258, if the system determines that a request was received from the client device for a particular inference as to why the automated assistant performed a particular action, the system proceeds to block 260.
블록(260)에서, 시스템은 특정 추론(reasoning)을 제공하는 데 사용될 추가 데이터를 결정하기 위한 요청을 포함하는 추가 사용자 입력을 프로세싱한다. 특정 추론을 제공하는 데 사용될 것으로 결정된 추가 데이터는 예를 들어 추가 사용자 입력에 포함된 요청의 유형에 기초할 수 있다. 따라서, 블록(262)에서, 시스템은 특정 추론에 대한 요청 유형을 결정한다. 요청 유형은 예를 들어, 자동화된 어시스턴트가 특정 이행(fulfillment)을 수행하게 한 이유에 관한 특정 추론에 대한 일반적인 요청(예: "why did you do that?(왜 그랬어?)") 또는 자동화된 어시스턴트가 특정 이행을 수행하게 한 이유에 관한 특정 추론에 대한 특정 요청(예: "why did you select the first-party music application?(제1-파티 음악 애플리케이션을 선택한 이유는 무엇입니까?)", "why didn’t you select the third-party music application?(제3-파티 음악 애플리케이션을 선택하지 않은 이유는 무엇입니까?)", "why did you select the certain artist?(특정 아티스트를 선택한 이유는 무엇입니까?)", "why did you select the certain artist’s most popular song?(특정 아티스트의 가장 인기 있는 곡을 선택한 이유는 무엇입니까?)) 및/또는 기타 특정 요청이 될 수 있다. 예를 들어, 추가 사용자는 NLU 출력을 생성하도록 프로세싱될 수 있으며, 시스템은 자동화된 어시스턴트가 NLU 출력에 기초하여 특정 이행을 수행하게 한 이유와 관련하여 추가 사용자 입력이 특정 추론에 대한 요청을 포함하는지 여부를 결정할 수 있다(예: 특정 추론에 대한 일반적인 요청과 연관된 의도 및/또는 특정 추론에 대한 특정 요청과 연관된 의도). At block 260, the system processes additional user input including a request to determine additional data to be used to provide a particular reasoning. The additional data determined to be used to provide a particular inference may be based, for example, on the type of request included in the additional user input. Accordingly, at block 262, the system determines the request type for the particular speculation. A request type can be, for example, a general request for a specific inference about why you have an automated assistant perform a particular fulfillment (e.g. "why did you do that?") or an automated assistant specific requests for specific inferences about what caused a particular fulfillment, e.g. "why did you select the first-party music application?", "why did you select the first-party music application?" didn't you select the third-party music application?", "why did you select the certain artist? )", "why did you select the certain artist's most popular song?", and/or other specific requests. For example, additional users can be processed to generate an NLU output, and the system can determine whether additional user input includes a request for a particular inference as to why it caused the automated assistant to perform a particular action based on the NLU output (e.g. : an intent associated with a general request for a specific inference and/or an intent associated with a specific request for a specific inference).
블록 262의 반복에서, 시스템이 특정 추론에 대한 요청 유형이 특정 추론에 대한 일반적인 요청이라고 결정하면 시스템은 블록 264로 진행한다. 블록 264에서, 시스템은 특정 추론을 제공할 때 추가 데이터로서 제1 추가 데이터를 결정한다. 위의 "play rock music(록 음악 재생)"의 예에서, 특정 추론에 대한 일반적인 요청은 예를 들어 "why did you do that?"라는 요청으로 구체화될 수 있으며, 여기서 "that"은 특정 록 아티스트 및 선택된 특정 록 노래와 함께 제1-파티 미디어 애플리케이션을 사용하여 록 음악을 재생하는 것과 같이 선택된 특정 이행 후보를 나타낸다. 요청 유형을 일반 요청으로 결정하는 응답으로, 시스템은 예를 들어 "I selected the first-party application because you share your application usage with me and use it the most to listen to music(애플리케이션 사용을 공유하고 음악을 듣는 데 가장 많이 사용하기 때문에 퍼스트 파티 애플리케이션을 선택했습니다.)", "I selected a certain artist because you share your music preferences with me and you listen to the certain artist the most(음악 취향이 비슷하시고 특정 아티스트의 음악을 가장 많이 들으셔서 특정 아티스트를 선택했습니다.)", "I selected a certain song by the certain artist because it the certain artist’s most popular song(특정 아티스트의 노래가 가장 인기가 많아서 특정 아티스트의 노래를 선택했습니다.)"의 출력에 해당하는 추가 데이터로서 특정 이행 후보와 연관된 제1 데이터를 획득하거나 및/또는 사용자 입력에 응답하여 시스템이 특정 이행 후보(또는 추론된 특정 슬롯 값)를 선택한 이유와 연관된 다른 추론을 획득할 수 있다. In repetition of block 262, if the system determines that the request type for a specific speculation is a general request for a specific speculation, then the system proceeds to block 264. At block 264, the system determines first additional data as the additional data when providing the particular inference. In the "play rock music" example above, a general request for a specific inference could be embodied, for example, as a request for "why did you do that?", where "that" is a specific rock artist. and a particular selected transition candidate, such as playing rock music using a first-party media application with a particular rock song selected. In response to determine the type of request as a generic request, the system may say, for example, "I selected the first-party application because you share your application usage with me and use it the most to listen to music." I chose the first-party application because I use it the most for music.", "I selected a certain artist because you share your music preferences with me and you listen to the certain artist the most. I selected a certain artist because I listened to it the most)", "I selected a certain song by the certain artist because it the certain artist's most popular song. Obtain first data associated with a particular transition candidate as additional data corresponding to the output of "and/or obtain other inferences associated with why the system selected a particular transition candidate (or inferred particular slot value) in response to a user input. can do.
블록 262의 반복에서, 시스템이 특정 추론에 대한 요청 유형이 특정 추론에 대한 특정 요청이라고 결정하면 시스템은 블록 266으로 진행한다. 블록 266에서, 시스템은 특정 추론을 제공하는 데 있어 추가 데이터로 제2 추가 데이터를 결정한다. 위의 "록 음악 재생"의 예에서, 특정 추론에 대한 특정 요청은 예를 들어 "why did you select the first-party music application?(제1-파티 음악 애플리케이션을 선택한 이유는 무엇입니까?", "why didn’t you select the third-party music application?(왜 제3-파티 음악 애플리케이션을 선택하지 않았습니까?)", "why did you select the certain artist?(특정 아티스트를 선택한 이유는?)"와 같은 요청으로 구체화되거나, 및/또는 기타 특정 요청으로 구체화될 수 있다. 요청 유형이 특정 요청이라는 결정에 응답하여, 시스템은 예를 들어, "I selected the first-party application because you share your application usage with me and use it the most to listen to music(애플리케이션 사용을 공유하고 음악을 듣는 데 가장 많이 사용하기 때문에 제1-파티 애플리케이션을 선택했습니다)", "I did not select the third-party application because you have not provided me with access to the third-party application("제3-파티 애플리케이션에 대한 액세스 권한을 제공하지 않았기 때문에 제3-파티 애플리케이션을 선택하지 않았습니다.)", "I selected a certain artist because you share your music preferences with me and you listen to the certain artist the most(음악 취향이 비슷하시고 특정 아티스트의 음악을 가장 많이 들으셔서 특정 아티스트를 선택했습니다.)", " selected a certain song by the certain artist because it the certain artist’s most popular song(“어떤 가수의 노래가 그 가수의 가장 인기 있는 곡이라 그 가수의 노래를 선택했습니다)"의 출력에 해당하는 추가 데이터로서 특정 이행 후보 또는 하나 이상의 이행 후보에 포함된 대체 이행 후보와 연관된 제2 데이터를 획득할 수 있거나, 및/또는 사용자 입력에 응답하여 시스템이 특정 이행 후보를 선택하거나 특정 대체 이행 후보를 선택하지 않은 이유와 연관된 기타 추론을 획득할 수 있다. In repetition of block 262, if the system determines that the type of request for a specific speculation is a specific request for a specific speculation, the system proceeds to block 266. At block 266, the system determines a second additional data as the additional data in providing the particular inference. In the example of "playing rock music" above, a specific request for a specific inference would be, for example, "why did you select the first-party music application?", " Why didn't you select the third-party music application?", "why did you select the certain artist?" In response to determining that the request type is a specific request, the system may state, for example, "I selected the first-party application because you share your application usage with me and use it the most to listen to music", "I did not select the third-party application because you have not provided me with access to the third-party application", "I selected a certain artist because you share your music preferences with me and you listen to the certain artist the most", " selected a certain song by the certain artist because it the certain artist's additional data corresponding to the output of the most popular song ("I chose a singer's song because that singer's song is that singer's most popular song)", which contains a specific transition candidate or an alternative transition candidate contained in one or more transition candidates; Associated second data may be obtained, and/or other inference associated with why the system selected a particular transition candidate or did not select a particular alternative transition candidate in response to a user input.
블록(268)에서, 시스템은 자동화된 어시스턴트가 특정 추론을 포함하는 출력을 사용자에게 프레젠테이션하기 위해 추가 데이터를 사용하도록 한다. 일부 구현에서, 특정 추론을 포함하는 출력은 추가 데이터를 특징으로 하는 특정 추론을 캡처하는 합성된 음성을 포함하는 합성된 음성 오디오 데이터를 포함할 수 있다. 이러한 구현의 일부 버전에서, 합성된 음성 오디오 데이터를 생성하기 위해 TTS 모델(들)(예를 들어, 도 1의 ML 모델(들) 데이터베이스(120A)에 저장됨)을 사용하여 하나 이상의 이행 후보와 연관된 메타데이터를 기반으로 시스템에 의해 생성되는 특정 추론에 해당하는 텍스트가 프로세싱될 수 있으며, 합성된 음성 오디오 데이터는 클라이언트 장치 또는 클라이언트 장치와 통신하는 추가 클라이언트 장치의 스피커(들)를 통해 사용자에게 프레젠테이션하기 위해 가청적으로 렌더링될 수 있다. 추가적인 또는 대안적인 구현에서, 특정 추론을 포함하는 출력은 클라이언트 장치 또는 클라이언트 장치와 통신하는 추가 클라이언트 장치의 디스플레이를 통해 사용자에게 프레젠테이션하기 위해 시각적으로 렌더링되는 텍스트 또는 기타 그래픽 콘텐츠를 포함할 수 있다. 시스템은 블록(252)으로 돌아가서 추가 어시스턴트 명령을 포함하고 자동화된 어시스턴트로 향하는 추가 사용자 입력을 수신하는 것에 응답하여 도 2의 방법(200)의 추가적인 반복을 수행할 수 있다. At block 268, the system causes the automated assistant to use the additional data to present output containing the particular inference to the user. In some implementations, the output that includes the specific speculation can include synthesized speech audio data that includes synthesized voice that captures the specific speculation that is characterized by additional data. In some versions of these implementations, the TTS model(s) (eg, stored in the ML model(s) database 120A of FIG. Text corresponding to certain inferences generated by the system based on associated metadata may be processed and the synthesized speech audio data presented to the user through the speaker(s) of the client device or additional client devices communicating with the client device. can be rendered audibly for In additional or alternative implementations, output containing certain inferences may include visually rendered text or other graphical content for presentation to a user via a display of the client device or additional client devices in communication with the client device. The system may return to block 252 and perform additional iterations of method 200 of FIG. 2 in response to receiving additional user input directed to the automated assistant, including additional assistant commands.
도 3을 참조하면, 사용자 입력에 포함되고 자동화된 어시스턴트로 향하는 어시스턴트 명령의 특정 이행이 수행될 수 없다고 결정하는 예시적인 방법(300)을 예시하는 흐름도가 묘사된다. 편의상, 방법(300)의 동작은 동작을 수행하는 시스템을 참조하여 설명된다. 방법(300)의 이 시스템은 하나 이상의 프로세서, 메모리, 및/또는 컴퓨팅 장치(들)(예를 들어, 도 1의 클라이언트 장치(110), 도 4a-4c의 클라이언트 장치(410), 도 5a-5b의 클라이언트 장치(510), 도 6의 컴퓨팅 장치(610), 하나 이상의 서버 및/또는 다른 컴퓨팅 장치)의 다른 컴포넌트(들)를 포함한다. 더욱이, 방법(300)의 동작이 특정 순서로 도시되어 있지만, 이는 제한을 의미하지 않는다. 하나 이상의 동작은 재정렬, 생략 및/또는 추가될 수 있다. Referring to FIG. 3 , a flow diagram illustrating an exemplary method 300 of determining that a particular implementation of an assistant command contained in user input and directed to an automated assistant cannot be performed is depicted. For convenience, the operation of method 300 is described with reference to a system that performs the operation. This system of method 300 may include one or more processors, memory, and/or computing device(s) (e.g., client device 110 of FIG. 1, client device 410 of FIGS. 4A-4C, FIG. 5A-4C). client device 510 of 5b, computing device 610 of FIG. 6, one or more servers and/or other computing devices). Moreover, while the operations of method 300 are shown in a particular order, this is not meant to be limiting. One or more actions may be rearranged, omitted, and/or added.
블록(352)에서, 시스템은 어시스턴트 명령을 포함하고 자동화된 어시스턴트로 향하는 사용자 입력을 클라이언트 장치의 사용자로부터 수신한다. 일부 구현에서, 사용자 입력은 클라이언트 장치의 마이크로폰(들)에 의해 생성된 오디오 데이터에서 캡처된 음성 발화에 대응할 수 있다. 추가적인 또는 대안적인 구현에서, 사용자 입력은 클라이언트 장치의 디스플레이 또는 클라이언트 장치의 다른 입력 장치(예를 들어, 키보드 및/또는 마우스)를 통해 수신된 터치 입력 또는 타이핑된 입력에 대응할 수 있다. At block 352, the system receives user input from the user of the client device that includes assistant commands and is directed to the automated assistant. In some implementations, user input may correspond to spoken utterances captured in audio data generated by the microphone(s) of the client device. In additional or alternative implementations, user input may correspond to touch input or typed input received through a display of the client device or another input device (eg, a keyboard and/or mouse) of the client device.
블록(354)에서, 시스템은 어시스턴트 명령의 특정 이행을 수행하는데 사용될 데이터가 결정될 수 있는지 여부를 결정한다. 사용자 입력이 음성 발화에 해당하는 구현에서, 음성 발화를 캡처하는 오디오 데이터는 ASR 모델(들)을 사용하여 프로세싱되어 ASR 출력(예: 음성 가설(들), 음소(들) 및/또는 기타 ASR 출력) 및 선택적으로 ASR 출력과 연관된 ASR 메트릭을 생성할 수 있다. 또한 NLU 모델을 사용하여 ASR 출력을 프로세싱하여 NLU 출력(예: ASR 출력에 기초하여 결정된 의도(intent), ASR 출력에 기초하여 결정된 의도와 연관된 파라미터에 대한 슬롯 값 등) 및 선택적으로 NLU 출력과 연관된 NLU 메트릭을 생성할 수 있다. 또한, 이행 규칙(들) 및/또는 이행 모델(들)을 사용하여 NLU 출력이 프로세싱되어 어시스턴트 명령의 이행 수행에 사용될 이행 데이터를 생성하고, 선택적으로 이행 데이터와 연관된 이행 메트릭을 생성할 수 있다. 사용자 입력이 터치 입력 또는 타이핑된 입력에 해당하는 구현에서, 터치 입력 또는 타이핑된 입력에 해당하는 텍스트는 NLU 모델을 사용하여 NLU 출력 및 선택적으로 NLU 출력과 연관된 NLU 메트릭을 생성하기 위해 프로세싱될 수 있다. 또한 NLU 출력은 어시스턴트 명령의 이행에 사용되는 데이터를 얻기 위해 전송될 요청을 생성하는 데 이용되는 이행 출력을 생성하기 위해 및/또는 어시스턴트의 이행을 수행할 때 이행 출력에 기초하여 액션이 수행되게 하도록 하기 위해, 이행 규칙 및/또는 이행 모델을 사용하여 프로세싱될 수 있다. At block 354, the system determines whether data to be used to perform a particular implementation of the assistant command can be determined. In implementations where the user input corresponds to a spoken utterance, the audio data capturing the spoken utterance is processed using the ASR model(s) to produce an ASR output (e.g., speech hypothesis(s), phoneme(s), and/or other ASR outputs. ) and optionally an ASR metric associated with the ASR output. The NLU model may also be used to process the ASR output to obtain NLU outputs (e.g., intents determined based on the ASR outputs, slot values for parameters associated with the intents determined based on the ASR outputs, etc.) and optionally associated NLU outputs. NLU metrics can be generated. Further, the NLU output may be processed using the adherence rule(s) and/or adherence model(s) to generate adherence data to be used in the fulfillment of the assistant instruction, and optionally generate adherence metrics associated with the adherence data. In implementations where the user input corresponds to a touch input or typed input, the text corresponding to the touch input or typed input may be processed using the NLU model to generate an NLU output and optionally an NLU metric associated with the NLU output. . NLU outputs may also be used to generate fulfillment outputs used to generate requests to be sent to obtain data used in the fulfillment of assistant commands and/or to cause actions to be performed based on fulfillment outputs when performing the assistant's fulfillment. To do so, it may be processed using transitive rules and/or transitive models.
시스템은 어시스턴트 명령의 이행 수행에서 이행 출력에 기초하여 액션(들)이 수행되게 하기 위해 및/또는 어시스턴트 명령의 이행 수행에 사용되는 데이터를 획득하기 위해 전송되는 이행 출력에 응답하여 수신된 데이터에 기초하여 어시스턴트 명령의 특정 이행의 수행에 사용될 데이터가 결정될 수 있는지 여부를 결정할 수 있다. 도 2에 도시된 바와 같이, 이행 출력에 응답하여 획득할 수 있는 데이터는 하나 이상의 이행 후보를 포함할 수 있다. 시스템은 예를 들어, ASR 메트릭, NLU 메트릭, 및/또는 사용자 입력과 연관된 이행 메트릭에 기초하여 어시스턴트 명령의 특정 이행을 수행하기 위해, 하나 이상의 이행 후보 중에서 특정 이행 후보를 선택할 수 있다. 예를 들어, 클라이언트 장치의 사용자가 "turn on the lights(조명을 켜)"라는 음성 발화를 제공한다고 가정한다. 이 예에서 음성 발화를 캡처하는 오디오 데이터는 ASR 메트릭과 연관된 "turn on the lights"라는 음성 가설을 ASR 출력으로 생성하기 위해, ASR 모델(들)을 사용하여 프로세싱될 수 있다(예를 들어, 제1 음성 가설이 음성 발화에 포함된 용어(들) 및/또는 구(들)에 대응할 가능성, 예를 들어 확률, 이진 값, 로그 우도 등). 또한, 각각의 음성 가설은 NLU 메트릭과 연관된 "turn on lights"의 의도를 NLU 데이터로서 생성하기 위해 NLU 모델(들)을 사용하여 프로세싱될 수 있다(예를 들어, 제1 의도 및 슬롯 값(들)이 확률, 이진 값, 로그 우도 등과 같은 사용자의 원하는 의도에 대응할 가능성). 또한 이행 출력에는 예를 들어 하나 이상의 소프트웨어 애플리케이션(예: 조명을 켤 수 있는 제1-파티 소프트웨어 애플리케이션 및/또는 제3-파티 소프트웨어 애플리케이션)에 대한 요청이 포함될 수 있다. 그러나 이 예에서는 자동화된 어시스턴트가 조명을 제어하는 데 사용되는 소프트웨어 애플리케이션에 액세스할 수 없다고 가정한다. 따라서, 이 예에서, 시스템은 이행 출력을 소프트웨어 애플리케이션으로 전송하지 못할 수 있으며 하나 이상의 이행 후보는 자동화된 어시스턴트가 소프트웨어 애플리케이션과 상호 작용하여 조명을 제어할 수 없기 때문에 음성 발화를 이행할 수 없음을 나타내는 널(null) 이행 후보만 포함한다. 반대로, 자동화된 어시스턴트가 조명을 제어하는 데 사용되는 소프트웨어 애플리케이션에 액세스할 수 있다고 가정하면, 시스템은 자동화된 어시스턴트가 조명을 제어하기 위해 소프트웨어 애플리케이션과 상호작용할 수 있기 때문에 어시스턴트 명령의 특정 이행의 수행에 사용될 데이터가 결정될 수 있다고 결정할 수 있고, 하나 이상의 실행 후보에는 실행될 때 조명이 제어되도록 하는 하나 이상의 어시스턴트 명령이 포함될 수 있다. The system may, based on data received in response to transitional outputs transmitted to cause action(s) to be performed based on the transitional outputs in the transitional performance of the assistant command and/or to obtain data used in the transitional performance of the assistant command. so that it can be determined whether the data to be used for carrying out a specific implementation of the assistant command can be determined. As shown in FIG. 2 , data that can be obtained in response to a transition output may include one or more transition candidates. The system may select a particular transition candidate from among one or more transition candidates to perform a particular transition of an assistant command based, for example, on an ASR metric, an NLU metric, and/or a transition metric associated with a user input. For example, it is assumed that the user of the client device provides a voice utterance “turn on the lights”. The audio data that captures the speech utterance in this example can be processed using the ASR model(s) to generate as an ASR output a speech hypothesis of “turn on the lights” associated with the ASR metric (e.g., 1 Probability that the negative hypothesis corresponds to the term(s) and/or phrase(s) contained in the negative utterance, e.g. probability, binary value, log likelihood, etc.). Additionally, each negative hypothesis can be processed using NLU model(s) to generate as NLU data an intent to “turn on lights” associated with an NLU metric (e.g., a first intent and slot value(s)). ) the probability that it corresponds to the user's desired intent, such as probability, binary value, log-likelihood, etc.). The fulfillment output may also include, for example, a request for one or more software applications (eg, a first-party software application that can turn on a light and/or a third-party software application). However, this example assumes that the automated assistant does not have access to the software application used to control the lights. Thus, in this example, the system may not be able to send fulfillment output to the software application and one or more fulfillment candidates indicate that the voice utterance cannot be fulfilled because the automated assistant cannot interact with the software application to control the lights. Contains only null transition candidates. Conversely, assuming that the automated assistant has access to the software application used to control the light, the system is responsible for performing certain implementations of the assistant's commands because the automated assistant can interact with the software application to control the light. It can be determined that the data to be used can be determined, and the one or more execution candidates can include one or more assistant commands that, when executed, cause the lighting to be controlled.
블록 354의 반복에서, 시스템이 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터가 결정될 수 있다고 결정하면, 시스템은 도 2의 블록(256)으로 진행하며, 도 2의 방법(200)의 반복을 계속한다. 예를 들어, 자동화된 어시스턴트가 조명을 제어하는 데 사용되는 소프트웨어 애플리케이션에 액세스할 수 있는 위의 예에서, 시스템은 도 2의 블록(256)으로 진행할 수 있고, 블록(256)으로부터 전술한 도 2의 방법(200)의 반복을 계속하여 어시스턴트 명령의 특정 이행이 수행되게 하고, 그리고 도 2와 관련하여 전술한 바와 같이 요구되는 경우 특정 이행에 대한 특정 추론을 제공한다. 블록(354)의 반복에서, 시스템이 어시스턴트 명령의 특정 이행을 수행하는데 사용될 데이터가 결정될 수 없다고 결정하면, 시스템은 블록(356)으로 진행한다. 예를 들어, 자동화된 어시스턴트가 조명을 제어하는 데 사용되는 소프트웨어 애플리케이션에 액세스할 수 있는 위의 예에서, 시스템은 블록(356)으로 진행할 수 있다. In iterations of block 354, if the system determines that the data to be used to perform a particular implementation of the assistant command can be determined, the system proceeds to block 256 of FIG. 2 and continues the iteration of method 200 of FIG. do. For example, in the above example where the automated assistant can access the software application used to control the lighting, the system can proceed to block 256 of FIG. 2, from which block 256 the previously described FIG. continues the iteration of method 200 of the assistant command to be performed, and to provide specific inferences for the particular implementation, if required, as described above with respect to FIG. 2 . If, in iterations of block 354, the system determines that the data to be used to perform a particular implementation of the assistant command cannot be determined, the system proceeds to block 356. For example, in the example above where the automated assistant can access a software application used to control the lights, the system can proceed to block 356 .
블록(356)에서, 시스템은 어시스턴트 명령의 대체 이행을 수행하는데 사용될 대체 데이터가 결정될 수 있는지 여부를 결정한다. 예를 들어, 시스템은 하나 이상의 대체 이행 후보가 있는지 여부를 결정하기 위해 하나 이상의 이행 후보를 분석할 수 있다. 블록(356)의 반복에서, 시스템이 어시스턴트 명령의 대체 이행을 수행하는데 사용될 대체 데이터가 결정될 수 없다고 결정하면, 시스템은 블록(358)으로 진행한다. 예를 들어, 클라이언트 장치의 사용자가 "turn on the lights"라는 음성 발화를 제공하고 자동화된 어시스턴트가 조명을 제어하기 위한 소프트웨어 애플리케이션에 액세스할 수 없는 위의 예를 계속 진행하면, 시스템은 하나 이상의 이행 후보에 포함된 대체 이행 후보가 없음을 결정할 수 있다(예: 널(null) 이행 후보에만 해당). 이 경우, 시스템은 블록 358로 진행하기로 결정할 수 있다. At block 356, the system determines whether replacement data to be used to perform replacement implementation of the assistant command can be determined. For example, the system may analyze one or more transition candidates to determine whether there are one or more alternative transition candidates. If, in iterations of block 356, the system determines that the replacement data to be used to perform the replacement implementation of the assistant command cannot be determined, the system proceeds to block 358. For example, if the user of the client device gives the spoken utterance "turn on the lights" and proceeding with the example above where the automated assistant does not have access to the software application to control the lights, the system will perform one or more It may be determined that there are no alternative transition candidates included in the candidate (eg only null transition candidates). In this case, the system may decide to proceed to block 358.
블록(358)에서, 시스템은 자동화된 어시스턴트가 특정 이행을 수행할 수 있는 방법에 대한 추천된 액션을 생성하는데 사용될 추천 데이터를 결정하기 위해 사용자 입력을 프로세싱한다. 위의 예에서 계속해서, 시스템은 "turn on the lights"라는 음성 발화에 대한 조명 응답을 제어하는 특정 이행이 수행될 수 있지만 사용자가 조명을 제어하는 데 사용되는 소프트웨어 애플리케이션에 대한 자동화된 어시스턴트 액세스 권한을 부여하지 않았다는 사실을 결정할 수 있다. 따라서, 이 예에서, 추천된 액션은 사용자가 조명을 제어하는데 사용되는 소프트웨어 애플리케이션에 대한 자동화된 어시스턴트 액세스를 승인해야 한다는 표시를 포함할 수 있다. 또 다른 예로서, 조명을 제어하기 위한 클라이언트 장치에 소프트웨어 애플리케이션이 설치되어 있지 않다고 가정하면, 시스템은 "turn on the lights"라는 음성 발화에 대한 조명 제어의 특정 이행이 수행될 수 있지만 사용자가 조명을 제어하는 데 사용되는 소프트웨어 애플리케이션을 설치하지 않았다는 사실을 결정할 수 있고, 사용자는 조명을 제어하는 데 사용되는 소프트웨어 애플리케이션에 대한 자동화된 어시스턴트 액세스 권한을 부여해야 한다고 결정할 수 있다. At block 358, the system processes the user input to determine recommended data to be used to generate recommended actions for how the automated assistant can perform a particular implementation. Continuing from the example above, the system allows the user to have automated assistant access to the software application used to control the lights, although certain actions can be taken to control the lights response to the spoken utterance of "turn on the lights." can determine the fact that it has not been granted. Thus, in this example, the recommended action may include an indication that the user should authorize the automated assistant access to the software application used to control the lighting. As another example, assuming that no software application is installed on the client device for controlling the lights, the system may perform certain implementations of controlling the lights in response to a voice utterance of "turn on the lights", but the user may turn on the lights. It may be determined that the software application used to control is not installed, and the user should give the automated assistant access to the software application used to control the light.
블록(360)에서, 시스템은 자동화된 어시스턴트가 추천 데이터를 사용하여 사용자에게 프레젠테이션하기 위해 추천된 액션을 포함하는 출력을 제공하게 한다. 추천된 액션을 포함하는 출력은 (예를 들어, 도 2의 블록 268과 관련하여 설명된 바와 같이) 사용자에게 프레젠테이션하기 위해 청각적으로 및/또는 시각적으로 렌더링될 수 있다. 일부 구현에서, 추천된 액션을 포함하는 출력은 자동화된 어시스턴트가 추천된 액션을 자동으로 수행하게 하는 추가 입력을 사용자가 제공할 수 있게 하는 프롬프트(prompt)를 포함할 수 있다. 위의 예에서 시스템은 "I cannot turn on the lights right now, but I can if you grant me access to the software application utilized to control the lights, would you like to grant me access?(지금은 조명을 켤 수 없지만 조명을 제어하는 데 사용되는 소프트웨어 애플리케이션에 대한 액세스 권한을 부여하면 조명을 켤 수 있습니다. 액세스 권한을 부여하시겠습니까?)"라는 출력을 생성할 수 있다. 따라서, 추천된 액션을 포함하는 사용자에게 프레젠테이션하기 위해 제공되는 출력은 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행할 수 있는 방법 중 하나 이상을 나타낼 수 있고(예: "I cannot turn on the lights right now, but I can if you grant me access to the software application utilized to control the lights(지금은 조명을 켤 수 없지만 조명을 제어하는 데 사용되는 소프트웨어 애플리케이션에 대한 액세스 권한을 부여하면 할 수 있다.)"), 어시스턴트 명령의 특정 이행을 수행하도록 사용자에게 프롬프트할 수 있다(예: "would you like to grant me access?(접근 권한을 부여하시겠습니까?)"). 추가 또는 대체 구현에서, 추천된 액션을 포함하는 출력에는 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행할 수 있도록 사용자가 따라야 하는 단계별 명령어가 포함될 수 있다(예: "(1) 설정 열기, (2) 소프트웨어 애플리케이션 공유 설정 열기, (3) 조명 애플리케이션에 대한 소프트웨어 애플리케이션 설정 공유"). 시스템은 블록(352)으로 돌아가서 추가 어시스턴트 명령을 포함하고 자동화된 어시스턴트로 향하는 추가 사용자 입력을 수신하는 것에 응답하여 도 3의 방법(300)의 추가적인 반복을 수행할 수 있다. At block 360, the system causes the automated assistant to use the recommendation data to provide output containing recommended actions for presentation to the user. Output containing the recommended action may be rendered aurally and/or visually for presentation to the user (eg, as described with respect to block 268 of FIG. 2 ). In some implementations, the output containing the recommended action may include a prompt allowing the user to provide additional input that causes the automated assistant to automatically perform the recommended action. In the example above, the system might say "I cannot turn on the lights right now, but I can if you grant me access to the software application utilized to control the lights, would you like to grant me access?" Granting access to the software application used to control the lights will turn them on. Do you want to grant access?)". Thus, the output provided for presentation to the user, including the recommended action, may indicate one or more of the ways in which an automated assistant may perform a particular implementation of an assistant command (e.g., "I cannot turn on the lights right now, but I can if you grant me access to the software application utilized to control the lights") , may prompt the user to perform a specific implementation of an assistant command (e.g. "would you like to grant me access?"). In additional or alternative implementations, the output containing the recommended action may include step-by-step instructions the user must follow in order for the automated assistant to perform a specific implementation of the assistant's command (e.g., "(1) open settings; (2) ) Open Software Application Sharing Settings, (3) Share Software Application Settings for Lighting Applications"). The system may return to block 352 and perform additional iterations of method 300 of FIG. 3 in response to receiving additional user input directed to the automated assistant, including additional assistant commands.
블록(356)의 반복에서, 시스템이 어시스턴트 명령의 대체 이행의 수행에 사용될 대체 데이터가 결정될 수 있다고 결정하면, 시스템은 블록(362)으로 진행한다. 사용자가 "turn on the lights"라는 음성 발화를 제공하는 예와 대조적으로, 사용자가 블록 352에서 수신되는 "play rock music using application 2(애플리케이션 2를 사용하여 록 음악 재생)"이라는 음성 발화를 제공한다고 가정한다. 또한 사용자가 "application 2(애플리케이션 2)"에 대한 자동화된 어시스턴트 액세스 권한을 부여하지 않았다고 가정한다. 따라서, 이 예에서, "애플리케이션 2"에서 록 음악 재생의 특정 이행은 블록 354의 인스턴스에서 수행될 수 없다. 그러나, 블록(356)의 인스턴스에서, 시스템은 이전 예에서와 달리 대체 이행 후보가 있다고 결정할 수 있다. 예를 들어, 이 예에서 사용자가 자동화된 어시스턴트가 록 음악을 재생하기 위해 "애플리케이션 1"을 대안적으로 사용할 수 있도록 "애플리케이션 1"에 대한 자동화된 어시스턴트 액세스를 승인했다고 가정한다.In iterations of block 356, if the system determines that replacement data can be determined to be used in performing the replacement implementation of the assistant command, the system proceeds to block 362. In contrast to the example where the user provides the spoken utterance "turn on the lights", say that the user provides the spoken utterance "play rock music using application 2" which is received at block 352. Assume. Assume also that the user has not granted automated assistant access to "application 2". Thus, in this example, the specific implementation of playing rock music in “Application 2” cannot be performed in the instance of block 354. However, in an instance of block 356, the system may determine that there is an alternate transition candidate, unlike in the previous example. For example, assume in this example that the user has granted the automated assistant access to "Application 1" so that the automated assistant can alternatively use "Application 1" to play rock music.
블록(362)에서, 시스템은 어시스턴트 명령의 대안적인 이행(대체 이행)에 사용될 대안적인 데이터(대체 데이터)를 결정하기 위해 사용자 입력을 프로세싱한다. 사용자가 "애플리케이션 2를 사용하여 록 음악 재생"이라는 음성 발화를 제공하는 위의 예를 계속하면, 록 음악 재생에 대한 구조화된 요청을 포함하는 이행 출력이 처음에 적어도 "애플리케이션 2"로 전송될 수 있다. 일부 구현에서, 이행 출력을 "애플리케이션 2"로 전송하는 것에 응답하여, 시스템은 사용자가 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스를 승인하지 않았기 때문에 널 이행 후보(null fulfillment candidate)의 표시를 수신할 수 있다. 추가적인 또는 대안적인 구현에서, 시스템은 사용자가 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스를 승인하지 않았음을 결정할 수 있고, 시스템은 "애플리케이션 2"에 대한 요청 전송을 보류하고 사용자가 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스 권한을 부여하지 않았기 때문에 널 이행 후보를 결정할 수 있다. 그러나 음악 재생 요청을 이행하려는 시도에서, 시스템은 이행 출력을 "애플리케이션 1"로 전송할 수 있으며(그리고 선택적으로 "애플리케이션 2"가 널 이행 후보와 연관되어 있다는 결정에 대한 응답으로), 사용자가 "애플리케이션 1"에 대한 자동화된 어시스턴트 액세스 권한을 부여했기 때문에 "애플리케이션 1"을 사용하여 록 음악을 재생하는 대체 이행 후보와 연관된 대체 데이터를 결정한다. 따라서 대체 이행 후보가 사용자 입력에 포함된 어시스턴트 명령의 특정 이행이 아니더라도(예: 대체 이행 후보는 사용자 요청에 따라 "애플리케이션 2"가 아닌 "애플리케이션 1"과 연결되기 때문에), 시스템은 대체 이행 후보를 사용하여 어시스턴트자 명령 이행을 시도할 수 있다.At block 362, the system processes the user input to determine alternative data (replacement data) to be used for alternative implementations of the assistant command (replacement implementations). Continuing the above example where the user gives the spoken utterance "Play rock music using application 2", the output of the transition containing a structured request to play rock music can initially be sent to at least "application 2". there is. In some implementations, in response to sending the fulfillment output to “Application 2,” the system may receive an indication of a null fulfillment candidate because the user has not authorized automated assistant access to “Application 2.” can In additional or alternative implementations, the system may determine that the user has not authorized automated assistant access to “application 2,” and the system withholds sending the request for “application 2,” and the user returns to “application 2.” Since you haven't given automated assistant access to , you can determine the null transition candidate. However, in an attempt to fulfill a music play request, the system may send a fulfillment output to "Application 1" (and optionally in response to determining that "Application 2" is associated with a null fulfillment candidate), and the user may send "Application Since we gave the automated assistant access to "1", we use "Application 1" to determine the alternative data associated with the alternate transition candidate playing rock music. Thus, even if the alternative transition candidate is not a specific implementation of the assistant command contained in the user input (e.g., because the alternative transition candidate is associated with "Application 1" rather than "Application 2" as per the user's request), the system determines the alternative transition candidate. You can use it to try to fulfill the assistant's command.
블록(364)에서, 시스템은 자동화된 어시스턴트가 어시스턴트 명령의 대체 이행을 수행하기 위해 대체 데이터를 사용하게 한다. 위의 예에서 계속해서, 시스템은 "애플리케이션 1"이 클라이언트 장치의 스피커(들) 또는 클라이언트 장치와 통신하는 추가 컴퓨팅 장치의 스피커(들)(예: 클라이언트 장치, 다른 클라이언트 장치 등과 통신하는 스마트 스피커)를 통해 록 음악 재생을 시작하게 할 수 있다. 일부 구현에서, 그리고 도 2의 서브-블록(256A)과 유사하게, 시스템은 특정 추론과 연관된 하나 이상의 선택 가능한 요소(selectable elements)가 사용자에게 제공되도록 할 수 있다. 그러나, 도 2의 서브-블록(256A)의 상기 동작과는 대조적으로, 대체 이행이 수행된 이유(예: "애플리케이션 1"을 사용하여 록 음악이 재생된 이유") 또는 특정 이행이 수행되지 않은 이유(예: "애플리케이션 2"를 사용하여 록 음악이 재생되지 않은 이유)에 대한 특정 추론이 제공될 수 있다. 이러한 구현에서, 대체 이행에 대한 특정 추론은 예를 들어 대체 이행 후보가 하나 이상의 이행 후보 중에서 사용자 입력에 응답하는 것으로 선택된 하나 이상의 이유를 포함할 수 있다. At block 364, the system causes the automated assistant to use the substitute data to perform the substitute fulfillment of the assistant command. Continuing from the example above, the system would have “Application 1” use the speaker(s) of the client device or speaker(s) of an additional computing device that communicates with the client device (e.g., a smart speaker that communicates with the client device, another client device, etc.) to start playing rock music. In some implementations, and similar to sub-block 256A of FIG. 2 , the system can cause one or more selectable elements associated with a particular inference to be presented to the user. However, in contrast to the above operation of sub-block 256A of FIG. 2, the reason why the alternative transition was performed (e.g., why rock music was played using “application 1”) or why a particular transition was not performed Specific inferences may be provided as to why (eg, why rock music was not played using “Application 2.”) In such implementations, specific inferences about alternative transitions may be provided, for example, if alternative transition candidates are one or more transitions. It may include one or more reasons selected as responding to user input among the candidates.
블록 366에서, 시스템은 자동화된 어시스턴트가 대체 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청이 클라이언트 장치에서 수신되는지 여부를 결정한다. 일부 구현에서, 자동화된 어시스턴트가 대체 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청은 클라이언트 장치에서 수신되는 추가 사용자 입력에 포함될 수 있다. 이러한 구현의 일부 버전에서, 시스템은 ASR 모델을 사용하여 추가 사용자 입력을 프로세싱하여 ASR 출력을 생성하고, NLU 모델을 사용하여 NLU 데이터를 생성하고, 이행 규칙 또는 모델을 사용하여 추가 사용자 입력이 특정 추론에 대한 요청을 포함하는지 여부를 결정하기 위해 블록(252)과 관련하여 설명된 것과 동일하거나 유사한 방식으로 이행 출력을 생성할 수 있다. 예를 들어, 추가 사용자 입력이 음성 발화 또는 타이핑된 입력인지 여부에 관계없이 추가 사용자를 프로세싱하여 NLU 출력을 생성할 수 있으며, 시스템은 자동화된 어시스턴트가 NLU 출력에 기초하여 대체 이행을 수행하게 한 이유와 관련하여 추가 사용자 입력에 특정 추론에 대한 요청이 포함되어 있는지 여부를 결정할 수 있다(예: 특정 추론에 대한 요청과 연관된 의도). 이러한 구현의 추가 또는 대체 버전에서, 자동화된 어시스턴트가 대체 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청은 클라이언트 장치에서 수신되는 하나 이상의 선택 가능한 요소의 사용자 선택에 포함될 수 있다(예를 들어, 서브-블록(256A)에 대해 전술한 바와 같이). 만일, 블록 366의 반복에서, 시스템이 자동화된 어시스턴트가 대체 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청이 클라이언트 장치에서 수신되지 않는다고 결정하면, 시스템은 자동화된 어시스턴트가 블록 366에서 대체 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청을 계속 모니터링한다(선택적으로 특정 이행이 수행된 후 임계 시간(예를 들어, 15초, 20초, 30초 및/또는 임의의 다른 임계 지속 시간) 동안). 블록(366)의 반복에서, 자동화된 어시스턴트가 대체 이행을 수행하게 한 이유에 관한 특정 추론에 대한 요청이 클라이언트 장치에서 수신되었다고 시스템이 결정하면, 시스템은 블록(368)으로 진행한다. 일부 구현에서, 그리고 도 2의 블록 262와 유사하게, 시스템은 특정 추론에 대한 요청 유형(예: 일반 요청, 제1 특정 요청, 제2 특정 요청 등)을 결정할 수 있다. At block 366, the system determines whether a request for a particular inference as to what caused the automated assistant to perform the replacement transition is received at the client device. In some implementations, a request for a specific inference as to what caused the automated assistant to perform the alternative transition may be included in additional user input received at the client device. In some versions of these implementations, the system uses an ASR model to process additional user input to produce an ASR output, an NLU model to generate NLU data, and a transitive rule or model to use a transitive rule or model to infer that additional user input is specific. The implementation output may be generated in the same or similar manner as described with respect to block 252 to determine whether or not to include a request for . For example, the reason why the additional user input could be processed to generate NLU outputs by processing additional user inputs, whether they were spoken utterances or typed inputs, and the system had an automated assistant perform alternate transitions based on the NLU outputs. With respect to , it may be possible to determine whether additional user input includes a request for a particular inference (e.g. an intent associated with a request for a particular inference). In additional or alternative versions of this implementation, a request for a specific inference as to what caused the automated assistant to perform the alternative implementation could be included in the user's selection of one or more selectable elements received at the client device (e.g., as described above for sub-block 256A). If, in iterations at block 366, the system determines that no request for a particular inference as to why the automated assistant performed the replacement transition was received from the client device, the system determines that the automated assistant performs the replacement transition at block 366. Continue to monitor requests for specific inferences as to why the action was performed (optionally for a threshold amount of time (e.g., 15 seconds, 20 seconds, 30 seconds, and/or any other threshold duration) after the specific action was performed. ). In repetition of block 366, if the system determines that a request for a specific inference as to why the automated assistant performed the replacement transition was received from the client device, the system proceeds to block 368. In some implementations, and similar to block 262 of FIG. 2 , the system can determine a request type for a particular inference (eg, a general request, a first particular request, a second particular request, etc.).
블록(368)에서, 시스템은 특정 추론을 제공하는데 사용될 추가 데이터를 결정하기 위한 요청을 포함하는 추가 사용자 입력을 프로세싱한다. 위의 "play rock music using application 2(애플리케이션 2를 사용하여 록 음악 재생)" 예를 계속 진행하면, 특정 추론에 대한 일반적인 요청은 예를 들어 "why did you do that?(왜 그렇게 했습니까?)"라는 요청으로 구체화될 수 있으며, 여기서 "that"은 특정 록 아티스트 및 선택된 특정 록 노래(예를 들어, 도 2와 관련하여 전술한 바와 같은 추론된 슬롯 값)와 함께 "애플리케이션 1"(사용자가 요청한 "애플리케이션 2" 대신)을 사용하여 록 음악을 재생하는 것과 같이 선택된 대체 이행 후보를 나타낸다. 요청 유형이 일반 요청이라는 결정에 응답하여, 시스템은 일반 요청에 응답하는 출력에 대응하는 추가 데이터와 연관된 추가 데이터를 획득할 수 있다(예를 들어, 도 2와 관련하여 설명됨). 또한 위의 "play rock music(록 음악 재생)"의 예를 계속 사용하면, 특정 추론에 대한 특정 요청은 예를 들어 "why did you use application 1 instead of application 2 like I requested?(내가 요청한 애플리케이션 2 대신 애플리케이션 1을 사용한 이유는 무엇입니까?)", "why did you select the certain artist?(특정 아티스트를 선택한 이유는?)", "why did you select the certain artist's most popular song?(왜 특정 아티스트의 가장 인기 있는 노래를 선택했습니까?)"와 같은 요청 및/또는 기타 특정 요청으로 구체화될 수 있다. 요청 유형이 특정 요청이라는 결정에 응답하여, 시스템은 특정 요청에 응답하는 출력에 대응하는 추가 데이터를 획득할 수 있다(예를 들어, 도 2와 관련하여 설명됨).At block 368, the system processes additional user input including a request to determine additional data to be used to provide a particular inference. Continuing the "play rock music using application 2" example above, a common request for a particular reasoning would be, for example, "why did you do that?" ", where "that" is "application 1" (where the user is Indicates an alternative transition candidate selected, such as playing rock music using the requested "Application 2" instead. In response to determining that the request type is a general request, the system may obtain additional data associated with additional data corresponding to an output responsive to the general request (eg, as described with respect to FIG. 2 ). Also, continuing with the "play rock music" example above, a specific request for a specific inference would be, for example, "why did you use application 1 instead of application 2 like I requested?" Why did you use application 1 instead?)", "why did you select the certain artist?", "why did you select the certain artist's most popular song? Did you pick the most popular song?" and/or other specific requests. In response to determining that the request type is a specific request, the system may obtain additional data corresponding to outputs responsive to the specific request (eg, as described with respect to FIG. 2 ).
블록(370)에서, 시스템은 자동화된 어시스턴트가 특정 추론을 포함하는 출력을 사용자에게 프레젠테이션하기 위해 추가 데이터를 사용하게 한다. 일부 구현에서, 특정 추론을 포함하는 출력은 추가 데이터를 특징으로 하는 특정 추론을 캡처하는 합성된 음성을 포함하는 합성된 음성 오디오 데이터를 포함할 수 있다. 이러한 구현의 일부 버전에서, 하나 이상의 이행 후보와 연관된 메타데이터를 기반으로 시스템에 의해 생성된 특정 추론에 해당하는 텍스트는 합성된 음성 오디오 데이터를 생성하기 위해 TTS 모델(들)(예를 들어, 도 1의 ML 모델(들) 데이터베이스(120A)에 저장됨)을 사용하여 프로세싱될 수 있으며, 합성된 음성 오디오 데이터는 클라이언트 장치 또는 클라이언트 장치와 통신하는 추가 클라이언트 장치의 스피커(들)를 통해 사용자에게 프레젠테이션하기 위해 가청적으로 렌더링될 수 있다. 추가적인 또는 대안적인 구현에서, 특정 추론을 포함하는 출력은 클라이언트 장치 또는 클라이언트 장치와 통신하는 추가 클라이언트 장치의 디스플레이를 통해 사용자에게 프레젠테이션하기 위해 시각적으로 렌더링되는 텍스트 또는 기타 그래픽 콘텐츠를 포함할 수 있다. 시스템은 블록(352)으로 돌아가서 추가 어시스턴트 명령을 포함하고 자동화된 어시스턴트로 향하는 추가 사용자 입력을 수신하는 것에 응답하여 도 3의 방법(300)의 추가적인 반복을 수행할 수 있다. At block 370, the system causes the automated assistant to use the additional data to present output containing the particular inference to the user. In some implementations, the output that includes the specific speculation can include synthesized speech audio data that includes synthesized voice that captures the specific speculation that is characterized by additional data. In some versions of these implementations, text corresponding to particular inferences generated by the system based on metadata associated with one or more transition candidates is used in the TTS model(s) (e.g., diagrams) to generate synthesized speech audio data. 1 ML model(s) stored in database 120A), and the synthesized speech audio data is presented to the user through the speaker(s) of the client device or additional client devices communicating with the client device. can be rendered audibly for In additional or alternative implementations, output containing certain inferences may include visually rendered text or other graphical content for presentation to a user via a display of the client device or additional client devices in communication with the client device. The system may return to block 352 and perform additional iterations of method 300 of FIG. 3 in response to receiving additional user input directed to the automated assistant, including additional assistant commands.
도 4a 내지 도 4c에서, 제공될 어시스턴트 명령의 이행에 대한 특정 추론을 유발하는 다양한 비제한적 예가 도시된다. 클라이언트 장치(410)(예를 들어, 도 1의 클라이언트 장치(110)의 인스턴스)는 예를 들어, 음성 발화 및/또는 다른 가청 입력에 기초하여 오디오 데이터를 생성하는 마이크로폰(들) 및/또는 합성된 음성 및/또는 다른 가청 출력을 들을 수 있게 렌더링하는 스피커(들)를 포함하는 다양한 사용자 인터페이스 컴포넌트를 포함한다. 도 4a 내지 도 4c에 도시된 클라이언트 장치(410)는 디스플레이가 없는 독립형 스피커이며, 이는 예시를 위한 것이며 제한하려는 의도가 아님을 이해해야 한다. 예를 들어, 클라이언트 장치(410)는 디스플레이가 있는 독립형 스피커, 휴대폰(예를 들어, 도 5a 및 5b와 관련하여 설명된 바와 같이), 홈 오토메이션 장치, 차량 내 시스템, 랩탑, 데스크탑 컴퓨터 및/또는 클라이언트 장치(410)의 사용자(401)와 인간 대 컴퓨터 대화 세션에 참여하기 위해 자동화된 어시스턴트를 실행할 수 있는 임의의 다른 장치일 수 있다. In Figures 4a to 4c, various non-limiting examples are shown which lead to specific inferences about the implementation of the assistant command to be presented. Client device 410 (e.g., instance of client device 110 in FIG. 1) may include, for example, a microphone(s) and/or synthesizer that generates audio data based on spoken speech and/or other audible input. and various user interface components including speaker(s) that render audibly rendered speech and/or other audible output. It should be understood that the client device 410 shown in FIGS. 4A-4C is a stand-alone speaker without a display, which is for illustrative purposes and is not intended to be limiting. For example, client device 410 may be a stand-alone speaker with a display, a mobile phone (eg, as described with respect to FIGS. 5A and 5B ), a home automation device, an in-vehicle system, a laptop, a desktop computer, and/or It may be any other device capable of running an automated assistant to engage in a human-to-computer conversation session with user 401 of client device 410 .
구체적으로 도 4a에서, 클라이언트 장치(410)의 사용자(401)가 "Assistant, play rock music(어시스턴트, 록 음악 틀어 줘)"라는 음성 발화(452A)를 제공한다고 가정한다. 음성 발화(452A)를 수신하는 것에 응답하여, 자동화된 어시스턴트는 ASR 모델(들)을 사용하여 음성 발화(452A)를 캡처하는 오디오 데이터가 프로세싱되도록 하여, 예를 들어 음성 발화(452A)에 대응하는 것으로 예측되는 하나 이상의 음성 가설(예: 용어 가설(들) 및/또는 전사 가설(들)), 음성(452A)에 대응하는 것으로 예측되는 하나 이상의 예측된 음소를 포함하는 ASR 출력 및/또는 다른 ASR 출력을 생성할 수 있다. ASR 출력을 생성 시, ASR 모델(들)은 선택적으로 하나 이상의 음성 가설, 예측 음소와 연관된 ASR 메트릭을 생성하고, 및/또는 하나 이상의 음성 가설, 예측된 음소, 및/또는 다른 ASR 출력이 음성 발화(452A)에 대응할 가능성을 나타내는 다른 ASR 출력을 생성할 수 있다. 더 나아가, ASR 출력은 NLU 모델을 사용하여 프로세싱되어, 예를 들어 ASR 출력에 기초하여 결정된 하나 이상의 의도, ASR 출력에 기초하여 결정된 하나 이상의 의도 각각과 연관된 하나 이상의 대응 파라미터에 대한 하나 이상의 슬롯 값을 포함하는 NLU 출력 및/또는 다른 NLU 출력을 생성할 수 있다. NLU 출력 생성 시, NLU 모델(들)은 선택적으로 하나 이상의 의도 각각과 연관된 NLU 메트릭, 의도와 연관된 해당 파라미터에 대한 하나 이상의 슬롯 값, 및/또는 하나 이상의 의도, 의도와 관련된 해당 파라미터에 대한 하나 이상의 슬롯 값 및/또는 다른 NLU 출력이 음성 발화(452A)를 제공할 때 사용자(401)의 실제 의도에 대응할 가능성을 나타내는 다른 NLU 출력을 생성할 수 있다. Specifically, in FIG. 4A , it is assumed that the user 401 of the client device 410 provides a voice utterance 452A, “Assistant, play rock music.” In response to receiving spoken utterance 452A, the automated assistant causes audio data that captures spoken utterance 452A to be processed using the ASR model(s), e.g., the output corresponding to spoken utterance 452A. one or more phonetic hypotheses (e.g., term hypothesis(s) and/or transcription hypothesis(s)) predicted to be true, an ASR output comprising one or more predicted phonemes predicted to correspond to voice 452A, and/or other ASRs. output can be generated. In generating ASR outputs, the ASR model(s) optionally generate ASR metrics associated with one or more speech hypotheses, predicted phones, and/or one or more speech hypotheses, predicted phones, and/or other ASR outputs associated with speech utterances. Another ASR output indicating the possibility of responding to (452A) may be generated. Further, the ASR output may be processed using the NLU model to obtain, for example, one or more intents determined based on the ASR output, one or more slot values for one or more corresponding parameters associated with each of the one or more intents determined based on the ASR output. NLU outputs that contain and/or other NLU outputs. In generating the NLU output, the NLU model(s) optionally include an NLU metric associated with each of the one or more intents, one or more slot values for corresponding parameters associated with the intents, and/or one or more intents, one or more values for corresponding parameters associated with the intents. The slot values and/or other NLU outputs may be generated to indicate a likelihood of corresponding to the actual intent of the user 401 when giving the spoken utterance 452A.
특히, 자동화된 어시스턴트는 하나 이상의 의도 각각과 연관된 대응하는 파라미터에 대한 슬롯 값 중 하나 이상을 추론할 수 있으며, 그 결과 음성 발화(452A)의 하나 이상의 해석(interpretation)이 발생하며, 여기서 하나 이상의 해석 각각은 특정 대응 파라미터에 대한 적어도 하나의 고유한 슬롯 값을 포함한다. 따라서, 도 4a의 예에서, 제1 해석은 "play music(음악 재생)" 의도와 연관된 애플리케이션 파라미터에 대한 "애플리케이션 1"의 슬롯 값, "음악 재생" 의도와 연관된 아티스트 파라미터에 대한 "artist 1(아티스트 1)"의 슬롯 값, 및 "음악 재생" 의도와 연관된 노래 파라미터에 대한 "song 1(노래 1)"의 슬롯 값을 갖는 "음악 재생" 의도를 포함할 수 있고; 제2 해석은 "음악 재생" 의도와 연관된 애플리케이션 파라미터에 대한 "애플리케이션 2"의 슬롯 값, "음악 재생" 의도와 연관된 아티스트 파라미터에 대한 "아티스트 1"의 슬롯 값 및 "음악 재생" 의도와 연관된 노래 파라미터에 대한 "노래 1"의 슬롯 값을 갖는 "음악 재생" 의도를 포함할 수 있고; 제3 해석은 "음악 재생" 의도와 연관된 애플리케이션 파라미터에 대한 "애플리케이션 1"의 슬롯 값, "음악 재생" 의도와 연관된 아티스트 파라미터에 대한 "아티스트 2"의 슬롯 값, 및 "음악 재생" 의도와 연관된 노래 파라미터에 대한 "노래 2"의 슬롯 값을 갖는 "음악 재생" 의도를 포함할 수 있다. In particular, the automated assistant can infer one or more of the slot values for corresponding parameters associated with each of the one or more intents, resulting in one or more interpretations of spoken utterance 452A, wherein one or more interpretations occur. Each contains at least one unique slot value for a particular corresponding parameter. Thus, in the example of FIG. 4A , the first interpretation is the slot value of “application 1” for the application parameter associated with the “play music” intent, the “artist 1” for the artist parameter associated with the “play music” intent. may include a “play music” intent with a slot value of “artist 1)” and a slot value of “song 1” for the song parameter associated with the “play music” intent; The second interpretation is the slot value of "Application 2" for the application parameter associated with the "play music" intent, the slot value of "Artist 1" for the Artist parameter associated with the "play music" intent, and the song associated with the "play music" intent. may contain an intent "play music" with a slot value of "song 1" for the parameter; A third interpretation is the slot value of "Application 1" for the application parameter associated with the "play music" intent, the slot value of "Artist 2" for the Artist parameter associated with the "play music" intent, and the slot value associated with the "play music" intent. It may contain a “play music” intent with a slot value of “song 2” for the song parameter.
더욱이, 자동화된 어시스턴트는 NLU 출력이 이행 규칙(들) 및/또는 이행 모델(들)을 사용하여 프로세싱되도록 하여 이행 출력을 생성할 수 있다. 이행 출력은 예를 들어 복수의 해석에 기초하여(예: NLU 출력에 따라 결정됨) 생성되고, 도 1의 제1-파티 서버(들)(191), 도 1의 제3-파티 서버(들)(192), 클라이언트 장치(410)에서 액세스 가능한 제1-파티 소프트웨어 애플리케이션(들), 클라이언트 장치(410)에서 액세스 가능한 제3-파티 소프트웨어 애플리케이션(들), 및/또는 음성 발화(452A)를 이행할 수 있는 임의의 다른 이행자와 같은 하나 이상의 이행자(fulfillers)로 전송될 하나 이상의 구조화된 요청을 포함할 수 있다. 도 4a의 예에서, 자동화된 어시스턴트는 대응하는 구조화된 요청이 NLU 출력에 의해 표시되는 음성 발화(452A)를 만족시킬 수 있는 것으로 식별되는 이들 소프트웨어 애플리케이션에 기초하여 적어도 "애플리케이션 1" 및 "애플리케이션 2"로 전송되게 할 수 있다. 이러한 구조화된 요청 전송에 대한 응답으로 자동화된 어시스턴트는 "애플리케이션 1" 및 "애플리케이션 2"로부터 하나 이상의 이행 후보를 수신할 수 있다. 예를 들어, 자동화된 어시스턴트는 "애플리케이션 1"이 복수의 해석에 기초하여 생성된 하나 이상의 구조화된 요청을 이행할 수 있는지 여부를 나타내는 하나 이상의 이행 후보를 "애플리케이션 1"로부터 수신할 수 있고, "애플리케이션 2"가 복수의 해석에 기초하여 생성된 하나 이상의 구조화된 요청을 이행할 수 있는지 여부를 나타내는 하나 이상의 이행 후보를 "애플리케이션 2"로부터 수신할 수 있다. 하나 이상의 이행 후보는 선택적으로 하나 이상의 이행 후보 각각이 음성 발화(452A)를 만족시킬 가능성을 나타내는 이행 메트릭(fulfillment metric)을 포함할 수 있다. Moreover, the automated assistant can cause the NLU output to be processed using the transitive rule(s) and/or transitive model(s) to generate the transitive output. The fulfillment output is generated, for example, based on a plurality of interpretations (eg, determined according to the NLU output), the first-party server(s) 191 of FIG. 1, the third-party server(s) of FIG. 192, implements first-party software application(s) accessible on client device 410, third-party software application(s) accessible on client device 410, and/or voice utterance 452A. It can contain one or more structured requests to be sent to one or more fulfillers, such as any other fulfillers that can. In the example of FIG. 4A , the automated assistant has at least "application 1" and "application 2" based on those software applications identified as being capable of satisfying spoken utterance 452A, whose corresponding structured request is indicated by the NLU output. " can be sent to In response to sending this structured request, the automated assistant may receive one or more fulfillment candidates from “Application 1” and “Application 2”. For example, an automated assistant can receive from "Application 1" one or more fulfillment candidates indicating whether "Application 1" can fulfill one or more structured requests generated based on a plurality of interpretations; Application 2" may receive one or more fulfillment candidates from "Application 2" indicating whether or not it can fulfill the one or more structured requests generated based on the plurality of interpretations. The one or more fulfillment candidates may optionally include a fulfillment metric that indicates the likelihood that each of the one or more fulfillment candidates will satisfy the spoken utterance 452A.
자동화된 어시스턴트는 ASR 메트릭, NLU 메트릭 및/또는 이행에 기초하여 하나 이상의 이행 후보의 순위를 매길 수 있고, 순위를 기반으로 하나 이상의 이행 후보 중에서 특정 이행 후보를 선택할 수 있다. 예를 들어, 도 4a의 예에서, 자동화된 어시스턴트가 순위를 기반으로, 애플리케이션 파라미터에 대한 "애플리케이션 1"의 슬롯 값, 아티스트 파라미터에 대한 "아티스트 1"의 슬롯 값 및 위에서 설명한 대로 노래 파라미터에 대한 "노래 1"의 슬롯 값을 갖는 "음악 재생" 의도를 포함하는 제1 해석과 연관된 특정 이행 후보를 선택한다고 가정한다. 또한 사용자(401)가 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스를 허용하지 않았고, 그 결과, "애플리케이션 1"로 전송된 하나 이상의 구조화된 요청에 기초하여 결정된 하나 이상의 이행 후보가 널 이행 후보라고 가정한다. 추가 또는 대체 구현에서, 자동화된 어시스턴트는 The automated assistant may rank one or more transition candidates based on the ASR metric, NLU metric, and/or transition, and may select a particular transition candidate from among the one or more transition candidates based on the ranking. For example, in the example of FIG. 4A , the automated assistant, based on the ranking, has a slot value of “Application 1” for the Application parameter, a slot value of “Artist 1” for the Artist parameter, and a Song parameter as described above. Suppose we select a particular transition candidate associated with a first interpretation that includes the intent "play music" with a slot value of "song 1". Assume also that user 401 has not allowed automated assistant access to "Application 2", and as a result, one or more transition candidates determined based on one or more structured requests sent to "Application 1" are null transition candidates. do. In additional or alternative implementations, the automated assistant may
사용자(401)가 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스를 승인하지 않았다는 것을 알고 있기 때문에 컴퓨팅 및/또는 네트워크 리소스를 절약하기 위해 "애플리케이션 2"에 대한 임의의 구조화된 요청 전송을 보류할 수 있고, "애플리케이션 2"로 전송될 수 있는 잠재적 구조적 요청에 대한 널 이행 후보를 자동으로 결정할 수 있다. 제1 해석과 연관된 특정 이행 후보 선택에 대한 응답으로, 자동화된 어시스턴트는 클라이언트 장치(410)의 스피커(들)을 통해 사용자(401)에게 가청적인 프레젠테이션을 위해 "Okay, playing rock music using application 1(좋아, 애플리케이션 1을 사용하여 록 음악을 재생해)"의 합성된 음성(454A1)을 제공하게 할 수 있고, 음성 발화(452A)를 만족시키기 위해 특정 이행 후보와 연관된 제1 해석에 기초하여 결정된 어시스턴트 명령이 454A2(예: "plays song 1 by artist 1 using application 1(애플리케이션 1을 사용하여 아티스트 1의 노래 1 재생)")에 의해 표시된 바와 같이 구현되게 한다. Because it knows that user 401 has not authorized automated assistant access to "application 2", it may withhold sending any structured requests to "application 2" to conserve computing and/or network resources; , can automatically determine null fulfillment candidates for potential structured requests that can be sent to “application 2”. In response to selecting a particular transition candidate associated with the first interpretation, the automated assistant displays "Okay, playing rock music using application 1( OK, use application 1 to play rock music) and provide the synthesized voice 454A1 of" the assistant determined based on the first interpretation associated with the particular transition candidate to satisfy the spoken utterance 452A. Causes the command to be implemented as indicated by 454A2 (eg, "plays song 1 by artist 1 using application 1").
그러나, 추가로 사용자(401)가 "Why did you do that?(왜 그렇게 했습니까?)"라는 추가 음성 발화(456A)를 제공한다고 가정한다. 추가 음성 발화(456A) 수신에 응답하여, 자동화된 어시스턴트는 음성 발화(452A)를 프로세싱하는 것과 관련하여 전술한 것과 동일하거나 유사한 방식으로 ASR 출력을 생성하기 위해 ASR 모델(들)을 사용하여 추가 음성(456A)을 캡처하는 오디오 데이터가 프로세싱되도록 할 수 있다. 또한, ASR 출력은 NLU 모델(들)을 사용하여 프로세싱되어 음성 발화(452A)를 프로세싱하는 것과 관련하여 위에서 설명한 것과 동일하거나 유사한 방식으로 NLU 출력을 생성할 수 있다. 자동화된 어시스턴트는 ASR 출력 및/또는 NLU 출력에 기초하여, 추가 음성 발화(456A)가 음성 발화(452A)에 포함된 어시스턴트 명령의 특정 이행이 수행된 이유에 대한 특정 추론을 제공하기 위한 자동화된 어시스턴트에 대한 요청을 포함하는지 여부를 결정할 수 있다. 일부 구현에서, 자동화된 어시스턴트는 ASR 출력 및/또는 NLU 출력에 기초하여, 특정 추론에 대한 요청이 특정 추론에 대한 일반적인 요청인지 또는 특정 추론에 대한 하나 이상의 특정 요청인지 여부를 추가로 또는 대안적으로 결정할 수 있다. However, it is further assumed that the user 401 provides an additional voice utterance 456A, “Why did you do that?”. In response to receiving the additional voice utterance 456A, the automated assistant uses the ASR model(s) to generate an ASR output in the same or similar manner as described above in connection with processing the voice utterance 452A. Audio data capturing 456A may be processed. Further, the ASR output may be processed using the NLU model(s) to generate the NLU output in the same or similar manner as described above in connection with processing the spoken utterance 452A. The automated assistant, based on the ASR output and/or the NLU output, further voice utterance 456A is an automated assistant to provide a specific inference as to why a specific implementation of the assistant command contained in spoken utterance 452A was performed. You can decide whether to include a request for . In some implementations, the automated assistant additionally or alternatively determines whether the request for a particular inference is a general request for a particular inference or one or more specific requests for a particular inference based on the ASR output and/or the NLU output. can decide
예를 들어, 도 4a의 예에서, 자동화된 어시스턴트는 추가 음성 발화(456A)의 프로세싱에 기초하여 생성된 ASR 출력 및/또는 NLU 출력에 기초하여, 특정 추론에 대한 요청이 사용자가 특정 이행의 특정 에스펙트에 대해 문의하지 않았기 때문에 일반적인 요청이라고 결정할 수 있다(예를 들어, 도 4b와 관련하여 아래에 설명됨). 오히려, 추가 음성 발화(456A)는 일반적으로 자동화된 어시스턴트가 왜 "애플리케이션 1"이 "아티스트 1"에 의한 "노래 1"을 재생하게 했는지 설명할 것을 요청한다. 따라서, 자동화된 어시스턴트는 추가 음성 발화(456A)에 응답하여 출력을 생성하는데 사용될 추가적인 데이터를 결정하기 위해 선택된 특정 이행 후보와 연관된 메타데이터를 획득할 수 있다. 추가 데이터를 기반으로 자동화된 어시스턴트는 "You share your application usage with me, and it looks like you use application 1 the most for listening to music, you have listened to artist 1 in the past, and song 1 is a new song by artist 1(당신은 저와 당신의 애플리케이션 사용을 공유했고, 당신은 음악을 듣기 위해 애플리케이션 1을 가장 많이 사용하는 것 같습니다. 당신은 과거에 아티스트 1을 들었고, 노래 1은 아티스트 1의 신곡입니다.)"의 추가 합성된 음성(458A1)이 클라이언트 장치(410)의 스피커(들)를 통해 사용자(401)에게 가청적인 프레젠테이션을 제공하도록 할 수 있다. 일부 구현에서, 자동화된 어시스턴트는 예를 들어 선택적으로 사용자(401)에게 "Use application 2, but only for rock music(록 음악에만 애플리케이션 2를 사용해)"라는 추가 음성 발화(460A)를 제공하도록 요청하는 것과 같이 사용자(401)가 자동화된 어시스턴트가 일부 대체 이행 후보를 수행하기를 원하는 경우, 사용자(401)에게 추가 음성 발화를 제공하도록 요청하는 클라이언트 장치(410)의 스피커(들)를 통해 사용자(401)에게 가청적인 프레젠테이션을 위해 "Would you prefer me to do something else?(내가 다른 일을 하길 원하십니까?)"라는 프롬프트(458A2)가 제공되도록 할 수 있다. For example, in the example of FIG. 4A , the automated assistant, based on the ASR output and/or NLU output generated based on processing of additional spoken utterance 456A, requests for specific inferences may be used by the user to specify certain implementations. Since we did not inquire about the aspect, we can decide that it is a general request (eg, described below with respect to FIG. 4B ). Rather, additional voice utterance 456A generally requests that the automated assistant explain why "Application 1" caused "Artist 1" to play "Song 1". Accordingly, the automated assistant can obtain metadata associated with the particular transition candidate selected to determine additional data to be used to generate output in response to additional spoken utterance 456A. Based on the additional data, the automated assistant responds with "You share your application usage with me, and it looks like you use application 1 the most for listening to music, you have listened to artist 1 in the past, and song 1 is a new song by artist 1 (You shared your application usage with me, and it seems that you use application 1 the most to listen to music. You have listened to artist 1 in the past, and song 1 is a new release by artist 1. )" may cause the additional synthesized voice 458A1 to provide an audible presentation to the user 401 through the speaker(s) of the client device 410 . In some implementations, the automated assistant optionally requests user 401 to provide an additional spoken utterance 460A, for example, “Use application 2, but only for rock music.” As such, if the user 401 wants the automated assistant to perform some alternative transition candidates, the user 401 via the speaker(s) of the client device 410 requesting the user 401 to provide additional spoken utterances. ) to be presented with the prompt 458A2 "Would you prefer me to do something else?" for an audible presentation.
특히, 추가 음성 발화(460A)는 음성 발화(452A) 및 자동화된 어시스턴트가 록 음악을 재생하기 위한 어시스턴트 명령을 포함하는 음성 발화의 미래 인스턴스에 응답하여 사용될 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스를 암시적으로 부여할 수 있다. 일부 구현에서, 추가 음성 발화(460A)는 암묵적으로 록 음악을 재생하기 위해서만 사용될 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스를 허용할 수 있다. 추가적인 또는 대안적인 구현에서, 추가 음성 발화(460A)는 임의의 장르의 음악을 재생하기 위해 사용될 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스를 암시적으로 허용할 수 있다. 또한, 일부 구현에서, 자동화된 어시스턴트는 자동화된 어시스턴트에게 "애플리케이션 2"에 대한 액세스를 허용하는 추가 음성 발화(460A)를 수신하는 것에 응답하여 "애플리케이션 1"을 사용하여 "아티스트 1"에 의한 "노래 1"을 재생하는 것에서(예: 위에서 설명한 제1 해석에 따른 어시스턴트 명령) "애플리케이션 2"를 사용하여 "아티스트 1"에 의한 "노래 1"을 재생하는 것으로(예: 위에서 설명한 제2 해석에 따른 어시스턴트 명령) 전환할 수 있다. In particular, additional spoken utterance 460A provides automated assistant access to "Application 2" that will be used in response to spoken utterance 452A and future instances of the spoken utterance that include an assistant command for the automated assistant to play rock music. can be given implicitly. In some implementations, additional voice utterance 460A may implicitly allow automated assistant access to “Application 2” to be used only to play rock music. In additional or alternative implementations, additional voice utterance 460A may implicitly allow automated assistant access to “Application 2” to be used to play any genre of music. Further, in some implementations, the automated assistant uses "Application 1" in response to receiving additional spoken utterance 460A that grants the automated assistant access to "Application 2" to "Artist 1" From playing "Song 1" (e.g. Assistant command per the first interpretation described above) to playing "Song 1" by "Artist 1" using "Application 2" (e.g. according to the second interpretation described above) Assistant command) can be switched.
구체적으로 도 4b와 관련하여, 도 4a에서 설명된 특정 추론에 대한 일반적인 요청과 대조적으로, 다시 사용자가 "Assistant, play rock music(어시스턴트, 록 음악 재생해)"라는 동일한 음성 발화(452B)를 제공한다고 가정하고, 자동화된 어시스턴트는 클라이언트 장치(410)의 스피커(들)을 통해 사용자(401)에게 가청적인 프레젠테이션을 위해 "Okay, playing rock music using application 1(좋아, 애플리케이션 1을 사용하여 록 음악을 재생해)"의 합성된 음성(454B1)을 제공하게 하고, Specifically with respect to FIG. 4B , again in contrast to the general request for specific inference described in FIG. 4A , the user provides the same spoken utterance 452B of “Assistant, play rock music”. , the automated assistant sends "Okay, playing rock music using application 1" for an audible presentation to user 401 via the speaker(s) of client device 410. play)" to provide the synthesized voice 454B1;
특정 이행 후보와 연관된 제1 해석에 기초하여 결정된 어시스턴트 명령이 454B2로 표시된 바와 같이(예: "plays song 1 by artist 1 using application 1(애플리케이션 1을 사용하여 아티스트 1의 노래 1 재생해)") 구현되어 발화(45BA)를 만족시키게 한다. 그러나, 도 4b의 예에서, 사용자(401)가 "Why did you use application 1?(왜 애플리케이션 1을 사용했습니까?)"라는 추가 음성 발화를 제공한다고 가정한다. 이 예에서, 자동화된 어시스턴트는, 추가 음성 발화(456B)의 프로세싱에 기초하여 생성된 ASR 출력 및/또는 NLU 출력에 기초하여, 사용자가 특정 이행의 특정 에스펙트에 대해 문의하고 있기 때문에 특정 추론에 대한 요청이 특정 요청임을 결정할 수 있다(예: why the automated assistant selected "application 1" to play "song 1" by "artist 1(자동화된 어시스턴트가 "아티스트 1"의 "노래 1"을 재생하기 위해 "애플리케이션 1"을 선택한 이유)). 따라서, 자동화된 어시스턴트는 추가 음성 발화(456A)에 응답하여 출력을 생성하는데 사용될 추가적인 데이터를 결정하기 위해 선택된 특정 이행 후보와 연관된 메타데이터를 획득할 수 있다. 특히, 도 4b의 예에서, 추가 데이터는 도 4b의 예에서 사용된 추가 데이터가 특히 "어플리케이션 1"이 선택된 이유를 묻는 사용자(401)의 특정 요청에 맞춰지거나 적응될 수 있다는 점에서 도 4b의 예의 추가 데이터와 다를 수 있다. 추가 데이터를 기반으로, 자동화된 어시스턴트는 "You share your application usage with me, and it looks like you use application 1 the most for listening to music(저와 애플리케이션 사용량을 공유하셨는데, 음악 감상에 애플리케이션 1을 가장 많이 사용하시는 것 같습니다.))"와 같은 추가 합성된 음성(458B1)을 야기할 수 있으며, 이는 클라이언트 장치(410)의 스피커(들)를 통해 사용자(401)에게 가청적인 프레젠테이션을 위해 제공되는 음악 듣기를 위해 애플리케이션 1을 가장 많이 사용하는 것처럼 보인다. 일부 구현에서, 자동화된 어시스턴트는 선택적으로 예를 들어 사용자(401)에게 "Use application 2, but only for rock music(애플리케이션 2를 사용하되 록 음악에만 사용)"라는 추가 음성 발화(460B)를 제공하도록 요청하는 것과 같이, 사용자(401)가 일부 대체 이행 후보를 자동화된 어시스턴트가 수행하기를 원하는 경우, 사용자(401)에게 추가 음성 발화를 제공하도록 요청하는 클라이언트 장치(410)의 스피커(들)를 통해 사용자(401)에게 가청적인 프레젠테이션을 위해 "Would you prefer me to do something else?(내가 다른 일을 하길 원하십니까?)"라는 프롬프트(458B2)(예: 추천 데이터를 기반으로 결정됨)가 제공되게 할 수 있다. 도 4a에 관하여 전술한 바와 유사하게, 추가 음성 발화(460B)는 음성 발화(452B) 및 자동화된 어시스턴트가 록 음악을 재생하기 위한 어시스턴트 명령을 포함하는 음성 발화의 미래 인스턴스에 응답하여 사용될 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스를 암시적으로 부여할 수 있다. Implementation of the assistant command determined based on the first interpretation associated with the particular transition candidate as indicated by 454B2 (e.g., "plays song 1 by artist 1 using application 1") to satisfy the utterance (45BA). However, in the example of FIG. 4B , it is assumed that the user 401 provides an additional voice utterance “Why did you use application 1?”. In this example, the automated assistant makes a particular inference because the user is inquiring about a particular aspect of a particular implementation, based on the NLU output and/or the ASR output generated based on the processing of the additional spoken utterance 456B. can determine that a request for is a specific request, e.g. why the automated assistant selected "application 1" to play "song 1" by "artist 1" reason for choosing application 1")). Accordingly, the automated assistant can obtain metadata associated with the particular transition candidate selected to determine additional data to be used to generate the output in response to the additional spoken utterance 456A. In particular, 4b, in the example of FIG. 4b, in that the additional data used in the example of FIG. 4b may be specifically tailored or adapted to the specific request of the user 401 asking why “Application 1” was selected. This may be different from the additional data: Based on the additional data, the automated assistant responds with "You share your application usage with me, and it looks like you use application 1 the most for listening to music." It seems that application 1 is used the most for listening. Application 1 appears to be the most used for listening to the music presented for the presentation.In some implementations, the automated assistant may optionally, for example, prompt the user 401 to "Use application 2, but only for rock music." If the user 401 wants the automated assistant to perform some alternate fulfillment candidates, such as requesting that the user 401 provide an additional spoken utterance 460B that reads, "but only for rock music", the user 401 can add "Would you prefer me to do something else?" for an audible presentation to the user 401 via the speaker(s) of the client device 410 requesting to provide an audio utterance. prompt 458B2 (eg, determined based on the recommendation data) may be provided. Similar to what was described above with respect to FIG. 4A, additional spoken utterance 460B is "Application 2" that will be used in response to future instances of spoken utterance that include voice utterance 452B and an assistant command for an automated assistant to play rock music. You can implicitly grant automated assistant access to ".
구체적으로 도 4c를 참조하면, 도 4a 및 도 4b와 대조적으로, 사용자가 "Assistant, play rock music(어시스턴트, 록 음악 재생해)"라는 동일한 음성 발화(452C)를 제공한다고 가정한다. 그러나, 도 4c의 예에서, 사용자(401)가 음악을 재생할 수 있는 임의의 소프트웨어 애플리케이션 또는 서비스(예를 들어, 도 1의 제1-파티 서버(들)(191) 및/또는 도 1의 제3-파티 서버(들)(192) 중 하나 이상에 의해 구현되는 스트리밍 서비스)에 대한 자동화된 어시스턴트 액세스를 승인하지 않았다고 가정한다. 따라서 "애플리케이션 1", "애플리케이션 2" 또는 기타 소프트웨어 애플리케이션이나 서비스와 연관된 하나 이상의 이행 후보에 포함된 이행 후보는 널 이행 후보와 연관될 수 있다. 그럼에도 불구하고 발화(452C)를 만족시키려는 시도에서, 자동화된 어시스턴트는 상호작용을 위한 연산 자원의 낭비를 피하기 위해 "rock music(록 음악)"에 대한 검색 결과와 같은 음성 발화(452C)에 응답하는 콘텐츠를 얻기 위해 예를 들어 웹 브라우저에 구조화된 요청을 전송할 수 있다. 도 4c에 도시된 바와 같이, 자동화된 어시스턴트는 클라이언트 장치(410)의 스피커(들)를 통해 "Rock music is a broad genre of popular music that originated as ‘rock and roll’ …(록 음악은 '로큰롤'에서 유래한 광범위한 대중 음악 장르입니다...)"의 합성된 음성(454C)이 사용자(401)에게 가청적인 프레젠테이션을 위해 제공되게 할 수 있다. Referring specifically to FIG. 4C , in contrast to FIGS. 4A and 4B , it is assumed that the user provides the same voice utterance 452C as “Assistant, play rock music”. However, in the example of FIG. 4C , any software application or service through which user 401 can play music (eg, first-party server(s) 191 of FIG. 1 and/or third party server(s) 191 of FIG. 1 ). Assume that you have not authorized automated assistant access to the streaming service implemented by one or more of the 3-party server(s) 192 . Accordingly, a transition candidate included in one or more transition candidates associated with "Application 1", "Application 2", or other software application or service may be associated with a null transition candidate. Nevertheless, in an attempt to satisfy utterance 452C, the automated assistant responds to spoken utterance 452C, such as a search result for “rock music,” to avoid wasting computational resources for interaction. You can send a structured request to, for example, a web browser to get content. As shown in FIG. 4C , the automated assistant announces "Rock music is a broad genre of popular music that originated as 'rock and roll'..." via the speaker(s) of the client device 410. The synthesized voice 454C of "may be presented for an audible presentation to the user 401.
그러나, 추가로 사용자(401)가 "Why didn't you play music?(왜 음악을 재생하지 않았습니까?)"라는 추가 음성 발화(456C)를 제공한다고 가정한다. 추가 발화(456C)를 수신하는 것에 응답하여, 자동화된 어시스턴트는 ASR 모델(들)을 사용하여 추가 음성 발화(456C)를 캡처하는 오디오 데이터가 프로세싱되도록 하여 도 4의 음성 발화(452A)를 프로세싱하는 것과 관련하여 전술한 동일하거나 유사한 방식으로 ASR 출력을 생성하게 할 수 있다. 또한, ASR 출력은 NLU 모델(들)을 사용하여 프로세싱되어 도 4a의 음성 발화(452A)를 프로세싱하는 것과 관련하여 위에서 설명한 것과 동일하거나 유사한 방식으로 NLU 출력을 생성할 수 있다. 자동화된 어시스턴트는 ASR 출력 및/또는 NLU 출력에 기초하여, 추가 음성 발화(456C)가 음성 발화(452A)에 포함된 어시스턴트 명령의 특정 이행이 수행된 이유에 대한 특정 추론을 제공하라는 자동화된 어시스턴트에 대한 요청을 포함하는지 여부를 결정할 수 있다. 특히, 도 4c의 예에서, 사용자(401)가 자동화된 어시스턴트가 특정 이행을 수행하게 한 이유에 대해 문의하는 대신, 사용자(401)는 자동화된 어시스턴트가 특정 이행을 수행하지 않은 이유에 대해 질문하고 있다.However, it is further assumed that the user 401 provides an additional voice utterance 456C of “Why didn't you play music?”. In response to receiving additional utterance 456C, the automated assistant causes the audio data that captures additional spoken utterance 456C to be processed using the ASR model(s) to process spoken utterance 452A of FIG. 4 . In relation to this, it is possible to generate an ASR output in the same or similar manner described above. Further, the ASR output may be processed using the NLU model(s) to generate the NLU output in the same or similar manner as described above with respect to processing the spoken utterance 452A of FIG. 4A. The automated assistant tells the automated assistant that, based on the ASR output and/or NLU output, additional spoken utterance 456C provides a specific inference as to why a specific implementation of the assistant command contained in spoken utterance 452A was performed. You can decide whether to include a request for In particular, in the example of FIG. 4C , instead of asking why user 401 caused the automated assistant to perform a certain action, user 401 asks why the automated assistant did not perform a certain action and there is.
따라서, 도 4c의 예에서, 자동화된 어시스턴트는, 추가 음성 발화(456A)의 프로세싱에 기초하여 생성된 ASR 출력 및/또는 NLU 출력에 기초하여, 사용자가 특정 이행의 특정 에스펙트에 대해 문의하고 있기 때문에 특정 추론에 대한 요청이 특정 요청이라고 결정할 수 있다(예: 자동화된 어시스턴트가 음악을 재생하지 않는 이유). 따라서, 자동화된 어시스턴트는 추가 음성 발화(456C)에 응답하여 출력을 생성하는 데 사용될 추가 데이터를 결정하기 위해 선택되지 않은 하나 이상의 대체 이행 후보와 연관된 메타데이터를 획득할 수 있다. 특히, 도 4c의 예에서, 추가 데이터는 도 4c의 예에서 사용된 추가 데이터가 특히 음악이 재생되지 않은 이유를 묻는 사용자(401)의 특정 요청에 맞춰지거나 적응될 수 있다는 점에서 도 4a 및 4b의 예의 추가 데이터와 다를 수 있다. 추가 데이터를 기반으로, 자동화된 어시스턴트는 클라이언트 장치(410)의 스피커(들)을 통해 사용자(401)에게 가청적인 프레젠테이션을 위해 “You have not granted me access to any applications or services that I can use to play music(음악을 재생하는 데 사용할 수 있는 애플리케이션이나 서비스에 대한 액세스 권한을 부여하지 않았습니다)”와 같은 추가 합성된 음성(458C1)이 제공되게 할 수 있다. 일부 구현에서, 자동화된 어시스턴트는, 예를 들어 사용자(401)에게 "Yes, use application 2, but only for rock music(예, 애플리케이션 2를 사용하지만 록 음악에만 해당됩니다)"와 같은 추가 음성 발화(460C)를 제공하도록 요청하는 것과 같이, 사용자(401)가 하나 이상의 소프트웨어 애플리케이션 또는 서비스를 사용하여 음악을 재생하기를 원하는 경우, 클라이언트 장치(410)의 스피커(들)를 통해 사용자(401)에게 추가 음성 발화를 제공하도록 요청하는 "Would you like to grant me permission to an application or service?(애플리케이션 또는 서비스에 대한 권한을 부여하시겠습니까?)"와 같은 프롬프트(458C2)(예: 추천 데이터를 기반으로 결정됨)가 사용자(401)에게 청각적으로 프레젠테이션을 위해 제공되도록 선택적으로 할 수 있다. 도 4c의 예에서, 추가 음성 발화(460C)는 음성 발화(452C) 및 자동화된 어시스턴트가 록 음악을 재생하기 위한 어시스턴트 명령을 포함하는 음성 발화의 미래 인스턴스(future instances)에 응답하여 사용될 "애플리케이션 2"에 대한 자동화된 어시스턴트 액세스를 명시적으로 부여할 수 있다. Thus, in the example of FIG. 4C , the automated assistant may, based on the NLU output and/or ASR output generated based on the processing of additional spoken utterance 456A, be aware that the user is inquiring about a particular aspect of a particular implementation. Because of this, it can decide that a request for a specific inference is a specific request (e.g. why the automated assistant isn't playing music). Accordingly, the automated assistant may obtain metadata associated with one or more alternate transition candidates that were not selected to determine additional data to be used to generate an output in response to additional spoken utterance 456C. In particular, in the example of FIG. 4C , the additional data is similar to that of FIGS. 4A and 4B in that the additional data used in the example of FIG. 4C may be tailored or adapted specifically to the specific request of the user 401 asking why music was not playing. may differ from the additional data in the example of Based on the additional data, the automated assistant reads “You have not granted me access to any applications or services that I can use to play” for an audible presentation to the user 401 via the speaker(s) of the client device 410. music (you have not given permission to access any application or service that can be used to play music)”. In some implementations, the automated assistant may say an additional spoken utterance (e.g., to user 401, "Yes, use application 2, but only for rock music"). 460C), if the user 401 wishes to play music using one or more software applications or services, the additional A prompt (458C2) such as "Would you like to grant me permission to an application or service?" requesting to provide a spoken utterance (e.g. determined based on referral data) may optionally be presented for presentation audibly to the user 401 . In the example of FIG. 4C , additional spoken utterance 460C is "Application 2" that will be used in response to future instances of spoken utterance 452C and the spoken utterance that includes an assistant command for an automated assistant to play rock music. You can explicitly grant automated assistant access to ".
도 5a 내지 도 5b는 제공될 어시스턴트 명령의 이행에 대한 특정 추론을 유발하는 다양한 추가적인 비제한적 예를 도시한다. 클라이언트 장치(510)(예를 들어, 도 1의 클라이언트 장치(110)의 인스턴스)는 예를 들어 음성 발화 및/또는 기타 가청 입력을 기반으로 오디오 데이터를 생성하는 마이크로폰(들), 합성된 음성 및/또는 기타 가청 출력을 들을 수 있게 렌더링하는 스피커(들) 및/또는 시각적 출력을 시각적으로 렌더링하는 디스플레이(580)을 포함하는 다양한 사용자 인터페이스 컴포넌트를 포함할 수 있다. 또한, 클라이언트 장치(510)의 디스플레이(580)는 클라이언트 장치(510)가 하나 이상의 동작을 수행하게 하기 위해 클라이언트 장치(510)의 사용자와 상호 작용할 수 있는 다양한 시스템 인터페이스 요소(581, 582, 583)(예: 하드웨어 및/또는 소프트웨어 인터페이스 요소)를 포함할 수 있다. 클라이언트 장치(510)의 디스플레이(580)는 사용자가 터치 입력(예를 들어, 사용자 입력을 디스플레이(580) 또는 그 일부(예를 들어, 텍스트 입력 상자(미도시), 키보드(미도시), 또는 디스플레이(580)의 다른 부분으로 지시함으로써) 및/또는 음성 입력(예를 들어, 마이크로폰 인터페이스 요소(584)를 선택함으로써 또는 클라이언트 장치(510)에서 반드시 마이크로폰 인터페이스 요소(584)를 선택하지 않고 말함으로써(즉, 자동화된 어시스턴트는 하나 이상의 용어 또는 구, 제스처(들) 응시(들), 입 움직임(들), 입술 움직임(들) 및/또는 음성 입력을 활성화하기 위한 기타 조건을 모니터링할 수 있다))에 의해 디스플레이(580) 상에 렌더링된 콘텐츠와 상호작용할 수 있게 한다. 도 5a-5b에 도시된 클라이언트 장치(510)는 이동 전화기이며, 이는 예시를 위한 것이며 제한하려는 의도가 아님을 이해해야 한다. 예를 들어, 클라이언트 장치(510)는 디스플레이가 있는 독립형 스피커, 디스플레이가 없는 독립형 스피커(예를 들어, 도 4a-4c와 관련하여 설명된 바와 같이), 홈 오토메이션 장치, 차량 내 시스템, 랩탑, 데스크탑 컴퓨터 및/또는 클라이언트 장치(510)의 사용자와 인간 대 컴퓨터 대화 세션에 참여하기 위해 자동화된 어시스턴트를 실행할 수 있는 임의의 다른 장치일 수 있다. 5A-5B show various additional non-limiting examples of triggering specific inferences about the implementation of an assistant command to be presented. Client device 510 (e.g., instance of client device 110 in FIG. 1) may include, for example, a microphone(s) that generates audio data based on spoken speech and/or other audible input, synthesized voice and It may include various user interface components including speaker(s) that audibly render other audible output and/or display 580 that visually renders visual output. In addition, the display 580 of the client device 510 includes various system interface elements 581, 582, 583 that can interact with the user of the client device 510 to cause the client device 510 to perform one or more actions. (e.g. hardware and/or software interface elements). The display 580 of the client device 510 allows a user to receive touch input (e.g., user input on the display 580) or a portion thereof (e.g., a text input box (not shown), a keyboard (not shown), or By pointing to another part of the display 580) and/or voice input (eg, by selecting the microphone interface element 584 or speaking without necessarily selecting the microphone interface element 584 on the client device 510). (That is, the automated assistant may monitor one or more terms or phrases, gesture(s) gaze(s), mouth movement(s), lip movement(s), and/or other conditions to activate voice input) ) to interact with the content rendered on the display 580. It should be understood that the client device 510 shown in Figures 5A-5B is a mobile phone, which is for illustration purposes and is not intended to be limiting. For example, the client device 510 may be a standalone speaker with a display, a standalone speaker without a display (eg, as described with respect to FIGS. 4A-4C ), a home automation device, an in-vehicle system, a laptop, a desktop It may be a computer and/or any other device capable of executing an automated assistant to engage in a human-to-computer conversation session with a user of client device 510 .
구체적으로 도 5a에 도시된 바와 같이, 클라이언트 장치(510)의 사용자가 "Play some rock music(록 음악 좀 틀어 줘)"라는 음성 발화(552A)를 제공한다고 가정하자. 음성 발화(552A)를 수신하는 것에 응답하여, 자동화된 어시스턴트는 ASR 출력을 생성하기 위해 ASR 모델(들)을 사용하여 음성 발화(552A)를 캡처하는 오디오 데이터가 프로세싱되도록 할 수 있고, NLU 모델(들)을 사용하여 ASR 출력이 NLU 출력을 생성하도록 프로세싱되도록 할 수 있고, 이행 규칙 및/또는 이행 모델을 사용하여 NLU 출력을 프로세싱하여 동일하거나 유사한 방식으로 이행 출력을 생성한다. 또한, 사용자의 클라이언트 장치(510)가 클라이언트 장치(510)보다 더 강력한 스피커를 갖고 록 음악을 재생할 수 있는 스마트 스피커(예: 거실 스피커)와 통신적으로 결합되어 있다고 자동화된 어시스턴트가 결정한다고 가정한다. 따라서, 음성 발화(552A)를 프로세싱하는 것에 기초하여, 자동화된 어시스턴트는 록 음악이 거실 스피커에서 재생되도록 하고 그리고 “Okay, playing rock music on the living room speaker(알겠습니다. 거실 스피커에서 록 음악을 재생합니다)"와 같은 합성된 음성(554A)이 클라이언트 장치(510)의 스피커(들)를 통해 사용자에게 가청적인 프레젠테이션을 위해 제공되도록 결정할 수 있고 및/또는 클라이언트 장치(510)의 디스플레이(580)를 통해 사용자에게 시각적으로 제시하기 위해 거실 스피커에서 록 음악을 재생하도록 결정할 수 있다. Specifically, as shown in FIG. 5A , it is assumed that the user of the client device 510 provides a voice utterance 552A, “Play some rock music.” In response to receiving the spoken utterance 552A, the automated assistant can cause the audio data capturing the spoken utterance 552A to be processed using the ASR model(s) to generate an ASR output, and the NLU model ( s) can be used to cause ASR outputs to be processed to produce NLU outputs, and processing NLU outputs using transitive rules and/or transitive models to generate transitive outputs in the same or similar manner. Also assume that the automated assistant determines that the user's client device 510 is communicatively coupled to a smart speaker (eg living room speaker) that has more powerful speakers than the client device 510 and can play rock music. . Thus, based on processing voice utterance 552A, the automated assistant causes rock music to be played on the living room speakers and reads “Okay, playing rock music on the living room speakers. )” may be presented for audible presentation to the user via the speaker(s) of the client device 510 and/or via the display 580 of the client device 510. You may decide to play rock music on your living room speakers for visual presentation to the user.
그러나, 추가로 클라이언트 장치(510)의 사용자가 "Why did you decide to play the music on the living room speaker?(거실 스피커에서 음악을 재생하기로 결정한 이유는 무엇입니까?)"라는 추가 음성 발화(556A)를 제공한다고 가정한다. 이 예에서, 자동화된 어시스턴트는, 추가 음성 발화(556A)의 프로세싱에 기초하여 생성된 ASR 출력 및/또는 NLU 출력에 기초하여, 특정 추론에 대한 요청이 추가 음성 발화에 포함되어 있고 그리고 사용자가 특정 이행의 특정 에스펙트에 대해 문의하고 있기 때문에 요청이 특정 요청이라는 것을 결정할 수 있다(예: 자동화된 어시스턴트가 록 음악을 재생하기 위해 "거실 스피커"를 선택한 이유). 이 예에서, 자동화된 어시스턴트는 클라이언트 장치(510) 또는 클라이언트 장치(510)에 통신 가능하게 결합되고 음악을 재생할 수 있는 다른 컴퓨팅 장치가 아니라(예: 주방 스피커, 서재 스피커 등), 자동화된 어시스턴트가 거실 스피커에서 음악을 재생하기로 결정한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 선택된 이행 후보와 연관된 메타데이터를 획득할 수 있다. 추가 데이터를 기반으로, 자동화된 어시스턴트는 “The living room speaker is more robust than your phone’s speaker("거실 스피커는 휴대폰 스피커보다 더 로버스트합니다)”와 같은 추가 합성된 음성(558A1)이 (그리고 선택적으로 (예를 들어, 도 1의 클라이언트 장치(110)의 존재 센서(들)(111)를 통해) 거실에서 클라이언트 장치(510)의 사용자의 검출된 존재에 기초하여) 클라이언트 장치(510)의 스피커(들)를 통해 사용자에게 가청적으로 제공되도록 및/또는 클라이언트 장치(510)의 디스플레이(580)를 통해 사용자에게 시각적으로 제공되도록 할 수 있다. 일부 구현에서, 자동화된 어시스턴트는, 예를 들어 록 음악을 재생하기 위해 클라이언트 장치(510)를 사용하는 것, 음악을 재생하기 위해 클라이언트 장치(510)에 의해 액세스 가능한 상이한 소프트웨어 애플리케이션을 사용하는 것, 자동화된 어시스턴트가 아티스트/노래를 전환하게 하는 것 등과 같이 사용자가 대체 이행 후보를 자동화된 어시스턴트가 수행하기를 원하는 경우, 선택적으로 클라이언트 장치(510)의 스피커(들)를 통해 사용자에게 청각적으로 프레젠테이션하기 위해 및/또는 클라이언트 장치(510)의 디스플레이(580)를 통해 사용자에게 시각적으로 프레젠테이션하기 위해 사용자가 추가 음성 발화를 제공하도록 요청하는 “Would you prefer me to play the rock music on your phone or another device?(휴대전화로 록 음악을 틀어드릴까요, 아니면 다른 기기에서 틀어드릴까요?)”와 같은 프롬프트(558A2)(예: 추천 데이터를 기반으로 결정됨)가 제공되도록 할 수 있다. 사용자가 추가 입력을 제공하면 자동화된 어시스턴트가 그에 따라 음악 재생을 조정할 수 있다. However, in addition, the user of the client device 510 makes an additional voice utterance (556A) "Why did you decide to play the music on the living room speaker?" ) is assumed to provide In this example, the automated assistant determines that, based on the ASR output and/or NLU output generated based on processing of the additional spoken utterance 556A, a request for a specific inference is included in the additional spoken utterance and the user has specified a specific Since we are inquiring about a specific aspect of the fulfillment, we can determine that the request is a specific request (e.g. why the automated assistant chose "living room speakers" to play rock music). In this example, the automated assistant is not client device 510 or another computing device communicatively coupled to client device 510 and capable of playing music (eg, kitchen speakers, study speakers, etc.) Metadata associated with the selected implementation candidate may be obtained to determine additional data that will be used to provide specific inferences as to why a decision was made to play music on the living room speakers. Based on the additional data, the automated assistant produces an additional synthesized speech (558A1) such as “The living room speaker is more robust than your phone's speaker” The speaker of the client device 510 (based on the detected presence of the user of the client device 510 in the living room (e.g., via the presence sensor(s) 111 of the client device 110 of FIG. 1)) s) and/or visually presented to the user via the display 580 of the client device 510. In some implementations, the automated assistant may provide, for example, rock music. such as using the client device 510 to play music, using different software applications accessible by the client device 510 to play music, having an automated assistant switch artists/songs, etc. If the user wants the automated assistant to perform alternative transition candidates, optionally for audible presentation to the user through the speaker(s) of the client device 510 and/or the display 580 of the client device 510. ) that asks the user to provide additional spoken utterances for a visual presentation to the user: “Would you prefer me to play the rock music on your phone or another device? Would you like to play it on your device?” prompt 558A2 (e.g., determined based on recommendation data) is presented. If the user provides additional input, the automated assistant can adjust music playback accordingly. .
추가적인 또는 대안적인 구현에서, 클라이언트 장치(510)의 사용자가 특정 추론에 대한 요청을 포함하는 임의의 추가 사용자 입력(556A)을 제공하기를 기다리는 것과 대조적으로, 자동화된 어시스턴트는 특정 추론과 연관된 하나 이상의 선택 가능한 요소를 능동적으로 제공할 수 있다. 예를 들어, 특히 도 5b를 참조하면, 클라이언트 장치(510)의 사용자가 "Play some rock music(록 음악 좀 틀어 줘)"라는 음성 발화(552B)를 제공한다고 가정하자. 음성 발화(552B)를 수신하는 것에 응답하여, 자동화된 어시스턴트는 음성 발화(552B)를 캡처하는 오디오 데이터가 ASR 모델(들)을 사용하여 프로세싱되게 하여 ASR 출력을 생성하게 하고, ASR 출력이 NLU 모델(들)을 사용하여 프로세싱되게 하여 NLU 출력을 생성하게 하고, 이행 규칙 및/또는 이행 모델을 사용하여 NLU 출력을 프로세싱하여 동일하거나 유사한 방식으로 이행 출력을 생성한다. 사용자의 클라이언트 장치(510)가 클라이언트 장치(510)보다 더 로버스트한 스피커를 갖고 록 음악을 재생할 수 있는 스마트 스피커(예: 거실 스피커)와 통신적으로 결합되어 있다고 자동화된 어시스턴트가 결정한다고 가정한다. 따라서, 발화(552B) 프로세싱에 기초하여, 자동화된 어시스턴트는 거실 스피커에서 록 음악을 재생하도록 결정할 수 있고 그리고 클라이언트 장치(510)의 스피커(들)를 통해 사용자에게 가청적 프레젠테이션을 위해 및/또는 클라이언트 장치(510)의 디스플레이(580)를 통해 사용자에게 시각적 프레젠테이션을 위해 "Okay, playing rock music on the living room speaker(알겠습니다. 거실 스피커에서 록 음악을 재생합니다)”와 같은 합성된 음성(554B)이 제공되도록 결정할 수 있고, 거실 스피커에서 록 음악이 재생되도록 할 수 있다. In additional or alternative implementations, as opposed to waiting for the user of client device 510 to provide any additional user input 556A that includes a request for a particular inference, the automated assistant may perform one or more actions associated with a particular inference. Selectable elements can be actively provided. For example, referring specifically to FIG. 5B , assume that the user of the client device 510 provides a voice utterance 552B “Play some rock music.” In response to receiving spoken utterance 552B, the automated assistant causes audio data capturing spoken utterance 552B to be processed using the ASR model(s) to generate an ASR output, which ASR output is an NLU model to be processed using the (s) to produce NLU outputs, and to process the NLU outputs using transitive rules and/or transitive models to produce transitive outputs in the same or similar manner. Suppose the automated assistant determines that the user's client device 510 is communicatively coupled to a smart speaker (eg living room speaker) that has more robust speakers than the client device 510 and can play rock music. . Accordingly, based on the processing of utterance 552B, the automated assistant may determine to play rock music on the living room speakers and for audible presentation to the user through the speaker(s) of the client device 510 and/or the client device 510. Synthesized voice 554B such as “Okay, playing rock music on the living room speaker” is output for visual presentation to the user via display 580 of device 510. You can decide to be served, and you can have rock music playing on your living room speakers.
그러나, 도 5b의 예에서, 또한 클라이언트 장치(510)의 사용자가 임의의 추가적인 음성 발화 또는 다른 사용자 입력을 제공하지 않고 특정 추론과 연관된 하나 이상의 선택 가능한 요소를 능동적으로 자동화된 어시스턴트가 제공한다고 가정한다. 예를 들어, 도 5b에 도시된 바와 같이, 자동화된 어시스턴트는 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 일반적으로 수행한 이유에 대한 일반 요청과 연관된 "Why did you do that?(왜 그랬어?)"라는 제1 선택 가능한 요소(556B1)를 야기할 수 있고, 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 거실 스피커를 사용하여 수행한 이유에 대한 특정 요청과 연관된 “Why did you use the living room speaker?(거실 스피커는 왜 사용했습니까?)”와 같은 제2 선택 가능 요소(556B2)를 야기할 수 있다. 클라이언트 장치의 사용자로부터, 제1 선택가능 요소(556B1)의 사용자 선택(예: 터치 입력 또는 음성 입력을 통해)에 응답하여, 자동화된 어시스턴트는 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 사용자 선택(예를 들어, 제1 선택 가능 요소(556B1)의 사용자 선택 또는 제2 선택 가능 요소(556B2)의 사용자 선택)에 기초하여 어시스턴트 명령의 특정 이행과 연관된 메타데이터를 획득할 수 있다. 예를 들어, 제1 선택가능 요소(556B1)의 사용자 선택을 수신하는 것에 응답하여, 자동화된 어시스턴트는 자동화된 어시스턴트가 록 음악을 재생하기 위해 특정 애플리케이션을 왜 선택했는지(예를 들어, 도 4a-4c와 관련하여 전술한 바와 같이 "애플리케이션 1" 대 "애플리케이션 2"), 특정 아티스트를 왜 선택했는지(예를 들어, 도 4a-4c와 관련하여 전술한 바와 같이), 특정 아티스트의 특정 노래를 왜 선택했는지(예를 들어, 도 4a-4c와 관련하여 전술한 바와 같이), 거실 스피커에서 음악을 왜 재생했는지(예를 들어, 도 5a와 관련하여 전술한 바와 같이), 및/또는 특정 이행의 다른 에스펙트과 연관된 특정 추론에 대한 추가 데이터를 결정할 수 있다. 또한, 예를 들어, 제2 선택 가능 요소(556B2)의 사용자 선택을 수신하는 것에 응답하여, 자동화된 어시스턴트는 클라이언트 장치(510) 또는 클라이언트 장치(510)에 통신 가능하게 연결되고 음악을 재생할 수 있는 다른 컴퓨팅 장치(예: 주방 스피커, 서재 스피커 등)에서가 아니라 거실 스피커에서 음악을 재생하기로 결정한 이유에 대한 추가 데이터를 결정할 수 있다.However, in the example of FIG. 5B , also assume that the automated assistant actively presents one or more selectable elements associated with a particular inference without the user of the client device 510 providing any additional spoken utterances or other user input. . For example, as shown in FIG. 5B , an automated assistant may respond with a "Why did you do that?" associated with a general request as to why the automated assistant typically performed a particular implementation of an assistant command. "Why did you use the living room speaker? ( Why did you use living room speakers?)” may result in a second selectable element 556B2. In response to user selection (eg, via touch input or voice input) of first selectable element 556B1 , from the user of the client device, the automated assistant determines additional data to be used to provide a particular inference to the user. Based on the selection (eg, user selection of the first selectable element 556B1 or user selection of the second selectable element 556B2 ), metadata associated with a specific implementation of the assistant command may be obtained. For example, in response to receiving a user selection of first selectable element 556B1, the automated assistant can determine why the automated assistant has selected a particular application to play rock music (e.g., FIG. 4A- “Application 1” versus “Application 2” as described above with respect to 4c), why a particular artist was chosen (eg, as described above with respect to FIGS. 4A-4C), and why a particular song by a particular artist. (e.g., as described above with respect to FIGS. 4A-4C), why the music was played on the living room speakers (e.g., as described above with respect to FIG. 5A), and/or the Additional data may be determined for certain inferences associated with other aspects. Also, for example, in response to receiving a user selection of second selectable element 556B2 , the automated assistant can communicatively connect to client device 510 or client device 510 and play music. Additional data may be determined as to why the decision was made to play music on living room speakers rather than on other computing devices (e.g., kitchen speakers, study speakers, etc.).
예를 들어, 도 5b의 예에서, 클라이언트 장치(510)의 사용자가 제2 선택 가능 요소(556B2)의 사용자 선택을 제공한다고 가정한다. 제2 선택 가능 요소(556B2)의 사용자 선택에 기초하여 결정된 추가 데이터에 기초하여, 자동화된 어시스턴트는, (그리고 선택적으로 (예를 들어, 도 1의 클라이언트 장치(110)의 존재 센서(들)(111)를 통해) 거실에서 클라이언트 장치(510)의 사용자의 검출된 존재에 기초하여) “The living room speaker is more robust than your phone’s speaker(거실 스피커는 휴대폰 더 로버스트합니다)”와 같은 추가 합성된 음성(558B1)이 클라이언트 장치(510)의 스피커(들)를 통해 사용자에게 청각적으로 제공되도록 및/또는 클라이언트 장치(510)의 디스플레이(580)를 통해 사용자에게 시각적으로 제공되도록 할 수 있다. 일부 구현에서, 자동화된 어시스턴트는 록 음악을 재생하기 위해 클라이언트 장치(510)를 사용하는 것, 음악을 재생하기 위해 클라이언트 장치(510)에 의해 액세스 가능한 상이한 소프트웨어 애플리케이션을 사용하는 것, 자동 어시스턴트가 아티스트/노래를 전환하게 하는 것 등과 같이 사용자가 자동화된 어시스턴트가 대체 이행 후보를 수행하기를 원하는 경우, 클라이언트 장치(510)의 스피커(들)를 통해 사용자에게 청각적으로 프레젠테이션하기 위해 및/또는 클라이언트 장치(510)의 디스플레이(580)를 통해 사용자에게 시각적으로 프레젠테이션하기 위해 “Would you prefer me to play the rock music on your phone or another device?(휴대전화로 록 음악을 틀어드릴까요, 아니면 다른 기기에서 틀어드릴까요?)”와 같은 프롬프트(558B2)(예: 추천 데이터를 기반으로 결정됨)가 선택적으로 제공되도록 할 수 있으며, 이는 사용자에게 추가 음성 발화를 제공하도록 요청한다. 사용자가 추가 입력을 제공하면 자동화된 어시스턴트가 그에 따라 음악 재생을 조정할 수 있다. For example, in the example of FIG. 5B , assume that the user of the client device 510 provides a user selection of the second selectable element 556B2. Based on the additional data determined based on the user selection of second selectable element 556B2, the automated assistant: (and optionally (e.g., the presence sensor(s) of client device 110 of FIG. 1) ( 111) based on the detected presence of the user of the client device 510 in the living room) an additional synthesized message such as “The living room speaker is more robust than your phone's speaker”. Voice 558B1 may be presented aurally to the user through the speaker(s) of client device 510 and/or visually presented to the user through display 580 of client device 510 . In some implementations, the automated assistant uses the client device 510 to play rock music, uses a different software application accessible by the client device 510 to play the music, the automated assistant is an artist /to make an audible presentation to the user through the speaker(s) of the client device 510 and/or if the user wants the automated assistant to perform an alternate transition candidate, such as to have it switch songs; To visually present to the user via the display 580 of the 510, “Would you prefer me to play the rock music on your phone or another device?” May I have one?)” prompt 558B2 (e.g., determined based on recommendation data) may be optionally presented, which asks the user to provide additional spoken utterances. If the user provides additional input, the automated assistant can adjust music playback accordingly.
비록 도 4a-4c 및 5a-5b의 예들은 자동화된 어시스턴트가 미디어 애플리케이션 또는 미디어 서비스에 대한 특정 음성 발화에 기초하여 이행을 수행하게 하고 추가적인 특정 음성 발화에 응답하여 이행을 위한 특정 추론을 제공하는 것과 관련하여 설명되었지만, 이는 예시를 위한 것이며 제한하려는 의도가 아님을 이해해야 한다. 예를 들어, 본 명세서에 기술된 기술은 자동화된 어시스턴트에 의해 수행되는 임의의 이행의 임의의 에스펙트에 대해 및/또는 자동화된 어시스턴트에 의해 선택되거나 선택되지 않는 임의의 이행 후보에 대해 특정 추론을 제공하기 위해 사용될 수 있다. 더욱이, 도 4a-4c 및 5a-5b의 상기 예들이 비록 특정 추천 데이터에 기초하여 결정된 특정 추천된 액션을 제공하는 것과 관련하여 설명되지만(예: 미디어 재생을 위한 특정 소프트웨어 애플리케이션에 대한 자동화된 어시스턴트 액세스 권한 부여), 이는 또한 예시를 위한 것이며 제한하려는 의도가 아님을 이해해야 한다. 일부 비제한적 예로서, 본 명세서에 기술된 추천된 액션은 임의의 소프트웨어 애플리케이션, 임의의 사용자 계정, 사용자와 연관된 임의의 컴퓨팅 장치, 이력 쿼리 활동, 및/또는 어시스턴트 명령을 이행하는 방법을 결정할 때 자동화된 어시스턴트가 활용할 수 있는 기타 모든 사용자 데이터에 대한 자동화된 어시스턴트 액세스 권한 부여를 포함할 수 있다.Although the examples of FIGS. 4A-4C and 5A-5B are different from having an automated assistant perform an action based on a specific voice utterance for a media application or media service and providing specific inference for the action in response to additional specific voice utterances. Although described in this regard, it should be understood that this is for illustrative purposes and is not intended to be limiting. For example, the techniques described herein can make specific inferences about any aspect of any transition performed by an automated assistant and/or for any transition candidate that may or may not be selected by an automated assistant. can be used to provide Moreover, although the above examples of FIGS. 4A-4C and 5A-5B are described in terms of providing a specific recommended action determined based on specific recommendation data (e.g., automated assistant access to a specific software application for media playback) authorization), it should also be understood that this is for illustrative purposes only and is not intended to be limiting. As some non-limiting examples, the recommended actions described herein can be automated in determining how to fulfill any software application, any user account, any computing device associated with the user, historical query activity, and/or assistant commands. This may include granting automated assistant access to any other user data that the customized assistant may utilize.
도 6은 본 명세서에 기술된 기술의 하나 이상의 양태를 수행하기 위해 선택적으로 사용될 수 있는 예시적인 컴퓨팅 장치(610)의 블록도이다. 일부 구현에서, 클라이언트 장치, 클라우드 기반 자동화된 어시스턴트 컴포넌트(들) 및/또는 다른 컴포넌트(들) 중 하나 이상은 예시적인 컴퓨팅 장치(610)의 하나 이상의 컴포넌트를 포함할 수 있다.6 is a block diagram of an example computing device 610 that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client device, cloud-based automated assistant component(s), and/or other component(s) may include one or more components of the example computing device 610 .
컴퓨팅 장치(610)는 전형적으로 버스 서브시스템(612)을 통해 복수의 주변 장치와 통신하는 적어도 하나의 프로세서(614)를 포함한다. 이들 주변 장치는, 예를 들어, 메모리 서브시스템(625) 및 파일 저장 서브시스템(626), 사용자 인터페이스 출력 장치들(620), 사용자 인터페이스 입력 장치들(622) 및 네트워크 인터페이스 서브시스템(616)을 포함하는 저장 서브시스템(624)을 포함한다. 입력 및 출력 장치는 사용자가 컴퓨팅 장치(610)와 상호 작용할 수 있도록 한다. 네트워크 인터페이스 서브시스템(616)은 외부 네트워크에 대한 인터페이스를 제공하고 다른 컴퓨팅 장치의 대응하는 인터페이스 장치에 결합된다.Computing device 610 typically includes at least one processor 614 that communicates with a plurality of peripheral devices via a bus subsystem 612 . These peripherals include, for example, memory subsystem 625 and file storage subsystem 626, user interface output devices 620, user interface input devices 622, and network interface subsystem 616. A storage subsystem 624 including Input and output devices allow a user to interact with computing device 610 . Network interface subsystem 616 provides an interface to external networks and is coupled to corresponding interface devices of other computing devices.
사용자 인터페이스 입력 장치들(622)은, 키보드, 마우스, 트랙볼, 터치패드 또는 그래픽 태블릿과 같은 포인팅 장치, 스캐너, 디스플레이에 통합된 터치스크린, 음성 인식 시스템과 같은 오디오 입력 장치, 마이크로폰 및/또는 기타 유형의 입력 장치를 포함할 수 있다. 일반적으로, 입력 장치라는 용어의 사용은 정보를 컴퓨팅 장치(610) 또는 통신 네트워크에 입력하기 위한 모든 가능한 유형의 장치 및 방법을 포함하도록 의도된다.User interface input devices 622 may include a keyboard, mouse, trackball, pointing device such as a touchpad or graphics tablet, a scanner, a touch screen integrated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. may include an input device of In general, use of the term input device is intended to include all possible types of devices and methods for entering information into computing device 610 or a communications network.
사용자 인터페이스 출력 장치들(620)은 디스플레이 서브시스템, 프린터, 팩스 또는 오디오 출력 장치와 같은 비시각적 디스플레이를 포함할 수 있다. 디스플레이 서브시스템은 음극선관(CRT), 액정 디스플레이(LCD)와 같은 평면 패널 장치, 프로젝션 장치 또는 가시 이미지를 생성하기 위한 기타 메커니즘을 포함할 수 있다. 디스플레이 서브시스템은 오디오 출력 장치와 같은 비시각적 디스플레이를 제공할 수도 있다. 일반적으로, 출력 장치라는 용어의 사용은 컴퓨팅 장치(610)로부터 사용자 또는 다른 기계 또는 컴퓨팅 장치로 정보를 출력하기 위한 모든 가능한 유형의 장치 및 방법을 포함하도록 의도된다.User interface output devices 620 may include a display subsystem, a non-visual display such as a printer, fax or audio output device. The display subsystem may include a flat panel device such as a cathode ray tube (CRT), a liquid crystal display (LCD), a projection device, or other mechanism for producing a visible image. The display subsystem may provide a non-visual display, such as an audio output device. In general, use of the term output device is intended to include all possible types of devices and methods for outputting information from computing device 610 to a user or other machine or computing device.
저장 서브시스템(624)은 본 명세서에 기술된 일부 또는 모든 모듈의 기능을 제공하는 프로그래밍 및 데이터 구성을 저장한다. 예를 들어, 저장 서브시스템(624)은 도 1 및 도2에 도시된 다양한 컴포넌트를 구현하는 것뿐만 아니라 본 명세서에 개시된 방법의 선택된 양상을 수행하기 위한 로직(logic)을 포함할 수 있다.Storage subsystem 624 stores programming and data configurations that provide the functionality of some or all of the modules described herein. For example, storage subsystem 624 may include logic to implement the various components shown in FIGS. 1 and 2 as well as to perform selected aspects of the methods disclosed herein.
이들 소프트웨어 모듈은 일반적으로 프로세서(614) 단독으로 또는 다른 프로세서와 조합하여 실행된다. 저장 서브시스템(624)에 포함된 메모리 서브시스템(625)은 프로그램 실행 동안 명령 및 데이터 저장을 위한 주 RAM(random access memory)(630) 및 고정 명령이 저장되는 ROM(Read Only Memory)(632)를 포함하는 복수의 메모리를 포함할 수 있다. 파일 저장 서브시스템(626)은 프로그램 및 데이터 파일을 위한 영구 저장 장치를 제공할 수 있고, 하드 디스크 드라이브, 관련 이동식 매체와 함께 플로피 디스크 드라이브, CD-ROM 드라이브, 광학 드라이브 또는 이동식 매체 카트리지를 포함할 수 있다. 특정 구현의 기능을 구현하는 모듈들은 저장 서브시스템(624)의 파일 저장 서브시스템(626), 또는 프로세서(들)(614)에 의해 액세스 가능한 다른 머신들에 저장될 수 있다.These software modules are typically executed by processor 614 alone or in combination with another processor. The memory subsystem 625 included in the storage subsystem 624 includes a main random access memory (RAM) 630 for storing instructions and data during program execution and a read only memory (ROM) 632 for storing fixed instructions. It may include a plurality of memories including. File storage subsystem 626 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive with associated removable media, a CD-ROM drive, an optical drive, or a removable media cartridge. can Modules implementing functionality of a particular implementation may be stored in file storage subsystem 626 of storage subsystem 624 or other machines accessible by processor(s) 614 .
버스 서브시스템(612)은, 컴퓨팅 장치(610)의 다양한 컴포넌트 및 서브시스템이, 의도한 대로 서로 통신하게 하는 메커니즘을 제공한다. 버스 서브시스템(612)이 개략적으로 단일 버스로 도시되어 있지만, 버스 서브시스템의 대안적인 구현은 다중 버스를 사용할 수 있다.Bus subsystem 612 provides a mechanism for the various components and subsystems of computing device 610 to communicate with each other as intended. Although bus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses.
컴퓨팅 장치(610)는 워크스테이션, 서버, 컴퓨팅 클러스터, 블레이드 서버, 서버 팜 또는 임의의 다른 데이터 프로세싱 시스템 또는 컴퓨팅 장치를 포함하는 다양한 유형일 수 있다. 컴퓨터 및 네트워크의 끊임없이 변화하는 특성으로 인해, 도 6에 도시된 컴퓨팅 장치(610)의 설명은 일부 구현을 설명하기 위한 특정 예로서만 의도된 것이다. 컴퓨팅 장치(610)의 많은 다른 구성이 도 6에 도시된 컴퓨팅 장치보다 더 많거나 더 적은 컴포넌트를 갖는 것이 가능하다.Computing device 610 may be of various types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 610 shown in FIG. 6 is intended only as a specific example to describe some implementations. It is possible that many other configurations of computing device 610 have more or fewer components than the computing device shown in FIG. 6 .
여기에 설명된 시스템이 사용자에 대한 개인 정보를 수집 또는 모니터링하거나 개인 및/또는 모니터링된 정보를 사용할 수 있는 상황에서), 사용자는 프로그램 또는 기능이 사용자 정보(예: 사용자의 소셜 네트워크, 소셜 활동 또는 활동, 직업, 사용자의 선호도 또는 사용자의 현재 지리적 위치에 대한 정보)를 수집하는지 여부를 제어하거나 사용자와 더 관련이 있을 수 있는 콘텐츠 서버로부터 콘텐츠를 수신할지 여부 및/또는 방법을 제어할 수 있는 기회를 제공받을 수 있다. 또한 특정 데이터는 저장되거나 사용되기 전에 하나 이상의 방식으로 프로세싱되어 개인 식별 정보가 제거될 수 있다. 예를 들어, 사용자에 대한 개인 식별 정보가 결정될 수 없도록 사용자의 신원이 프로세싱될 수 있거나, 지리적 위치 정보(예: 도시, 우편번호(ZIP code) 또는 주 레벨(state level))가 획득되는 경우 사용자의 지리적 위치가 일반화되어 사용자의 특정 지리적 위치가 결정될 수 없다. 따라서 사용자는 사용자에 대한 정보 수집 및/또는 사용 방법을 제어할 수 있다.In situations where the systems described herein may collect or monitor personal information about users, or may use personal and/or monitored information), users may be aware that programs or features may use user information (e.g., users' social networks, social activities or activity, profession, user preferences, or information about the user's current geographic location), or control whether and/or how content is received from content servers that may be more relevant to the user. can be provided. Also, certain data may be processed in one or more ways before it is stored or used to remove personally identifiable information. For example, the user's identity may be processed so that personally identifiable information about the user cannot be determined, or where geographic location information (eg city, ZIP code, or state level) is obtained. The geographic location of the user is generalized so that the specific geographic location of the user cannot be determined. Thus, users can control how information about them is collected and/or used.
일부 구현에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되며, 방법은, 클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계; 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계; 상기 자동화된 어시스턴트로 하여금 상기 데이터를 사용하여 상기 어시스턴트 명령의 상기 특정 이행을 수행하게 하는 단계; 상기 클라이언트 장치의 사용자로부터, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 요청을 포함하는 추가 사용자 입력을 수신하는 단계; 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계; 그리고 상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력을 제공하도록 상기 추가 데이터를 사용하도록 하는 단계를 포함한다.In some implementations, a method implemented by one or more processors is provided, the method comprising receiving user input from a user of a client device directed to an automated assistant comprising an assistant command and executing at least in part on the client device. step; processing the user input to determine data to be used to perform a particular implementation of the assistant command; causing the automated assistant to perform the particular implementation of the assistant command using the data; receiving additional user input from the user of the client device, the request comprising a request to the automated assistant to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command; processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command; and to cause the automated assistant to use the additional data to provide output that includes a specific inference as to why the automated assistant performed a specific implementation of the assistant command, for presentation to a user of the client device. It includes steps to
본 명세서에 개시된 기술의 이들 및 다른 구현은 선택적으로 다음 특징 중 하나 이상을 포함할 수 있다.These and other implementations of the technology disclosed herein may optionally include one or more of the following features.
일부 구현에서, 상기 어시스턴트 명령의 특정 이행을 수행하는데 사용될 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계는, 자동 음성 인식(ASR) 모델을 사용하여, 상기 어시스턴트 명령을 포함하는 상기 사용자 입력을 캡처한 오디오 데이터를 프로세싱하여 ASR 출력을 생성하는 단계; 자연어 이해(NLU) 모델을 사용하여, 상기 ASR 출력을 프로세싱하여 NLU 출력을 생성하는 단계; 그리고 상기 NLU 출력에 기초하여 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 결정하는 단계를 포함할 수 있다. In some implementations, processing the user input to determine data to be used to perform specific implementation of the assistant command captures the user input, including the assistant command, using an automatic speech recognition (ASR) model. processing the audio data to generate an ASR output; processing the ASR output to generate an NLU output, using a natural language understanding (NLU) model; and determining data to be used for performing a specific implementation of the assistant command based on the NLU output.
일부 구현들에서, 상기 어시스턴트 명령의 특정 이행을 수행하는데 사용될 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계는, 자연어 이해(NLU) 모델을 사용하여, 상기 타이핑된 입력을 프로세싱하여 NLU 출력을 생성하는 단계; 그리고 상기 NLU 출력에 기초하여 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 생성하는 단계를 포함한다. In some implementations, processing the user input to determine data to be used to perform a specific implementation of the assistant command may include processing the typed input to generate an NLU output, using a natural language understanding (NLU) model. doing; and generating data to be used to perform a specific implementation of the assistant command based on the NLU output.
일부 구현에서, 상기 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 요청은, 상기 자동화된 어시스턴트가 상기 특정 이행을 수행하는 데 사용된, 복수의 서로 다른 소프트웨어 애플리케이션들 중에서, 특정 소프트웨어 애플리케이션을 선택한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 특정 요청이다. 이러한 구현의 일부 버전에서, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계는, 상기 특정 이행을 수행하는 데 사용된 상기 특정 소프트웨어 애플리케이션과 연관된 메타데이터를 획득하는 단계; 그리고 상기 특정 소프트웨어 애플리케이션과 연관된 메타데이터에 기초하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 상기 추가 데이터를 결정하는 단계를 포함한다. In some implementations, a request to the automated assistant to provide a particular inference as to why the automated assistant performed a particular implementation of an assistant command may include: A specific request to the automated assistant to provide a specific inference as to why a specific software application was selected from among a plurality of different software applications. In some versions of these implementations, processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command comprises: obtaining metadata associated with the particular software application used to perform; and determining, based on the metadata associated with the specific software application, the additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command.
일부 구현에서, 상기 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 자동화된 어시스턴트에 대한 요청은, 상기 자동화된 어시스턴트가 특정 이행을 수행하는 데 사용된, 상기 사용자 입력의 복수의 서로 다른 해석(disparate interpretations) 중에서, 상기 사용자 입력의 특정 해석을 선택한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 특정 요청이다. 이러한 구현의 일부 버전에서, 상기 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계는, 상기 특정 이행을 수행하는 데 사용된 상기 사용자 입력의 특정 해석과 연관된 메타데이터를 획득하는 단계; 그리고 상기 사용자 입력의 특정 해석과 연관된 메타데이터에 기초하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 상기 추가 데이터를 결정하는 단계를 포함한다. In some implementations, a request to the automated assistant to provide a particular inference as to why the automated assistant performed a particular implementation of an assistant's command may cause the automated assistant to perform a particular implementation, the user A specific request to the automated assistant to provide a specific inference as to why a particular interpretation of the user input was chosen, out of a plurality of disparate interpretations of input. In some versions of these implementations, processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command may include performing the specific implementation. obtaining metadata associated with a particular interpretation of the user input used to perform; and based on the metadata associated with the specific interpretation of the user input, determining the additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command.
일부 구현에서, 상기 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 자동화된 어시스턴트에 대한 요청은, 상기 자동화된 어시스턴트가 특정 이행을 수행하는 데 사용된, 상기 사용자의 클라이언트 장치 대신에, 상기 사용자의 추가 클라이언트 장치를 선택한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 특정 요청이다. 이러한 구현의 일부 버전에서, 상기 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 추가 사용자 입력을 프로세싱하는 단계는, 상기 특정 이행을 수행하는 데 사용된 상기 추가 클라이언트 장치와 연관된 메타데이터를 획득하는 단계; 그리고 상기 추가 클라이언트 장치와 연관된 메타데이터에 기초하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 상기 추가 데이터를 결정하는 단계를 포함한다. In some implementations, a request to the automated assistant to provide a particular inference as to why the automated assistant performed a particular implementation of an assistant's command may cause the automated assistant to perform a particular implementation, the user A specific request to the automated assistant to provide a specific inference as to why the user selected an additional client device instead of the user's client device. In some versions of these implementations, processing additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a particular implementation of an assistant command comprises performing the particular implementation. obtaining metadata associated with the additional client device used to do so; and based on the metadata associated with the additional client device, determining the additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command.
일부 구현에서, 상기 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 자동화된 어시스턴트에 대한 요청은, 상기 자동화된 어시스턴트가 상기 특정 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 일반적인 요청을 포함한다. 이러한 구현의 일부 버전에서, 상기 자동화된 어시스턴트가 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 추가 사용자 입력을 프로세싱하는 단계는, (i) 상기 특정 이행을 수행하는 데 사용된, 복수의 서로 다른 소프트웨어 애플리케이션 중에서, 특정 소프트웨어 애플리케이션, (ii) 상기 특정 이행을 수행하는 데 사용된, 상기 사용자 입력에 대한 복수의 서로 다른 해석 중에서, 상기 사용자 입력에 대한 특정 해석, 또는 (iii) 상기 특정 이행을 수행하는 데 사용된, 상기 사용자의 클라이언트 장치 대신에, 상기 사용자의 추가 클라이언트 장치 중 하나 이상과 연관된 해당 메타데이터를 획득하는 단계: 그리고 상기 해당 메타데이터에 기초하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 상기 추가 데이터를 결정하는 단계를 포함한다. In some implementations, a request to the automated assistant to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command may cause the automated assistant to provide a specific inference as to why the specific implementation was performed. includes a general request to the automated assistant to provide In some versions of these implementations, processing additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command comprises: (i) the specific implementation; a particular software application, among a plurality of different software applications used to perform the implementation; (ii) a user input, among a plurality of different interpretations of the user input, obtaining corresponding metadata associated with one or more of the user's additional client devices, instead of the user's client device, used to perform the specific interpretation, or (iii) the specific implementation; and based on that, determining the additional data to be used to provide a particular inference as to why the automated assistant performed a particular implementation of the assistant command.
일부 구현에서, 상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력을 제공하도록 상기 추가 데이터를 사용하도록 하는 단계는, TTS(text-to-speech) 모델을 사용하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론에 대응하는 합성된 음성을 포함하는 합성된 음성 오디오 데이터를 생성하도록 상기 추가 데이터를 프로세싱하는 단계를 포함한다.In some implementations, for presentation to a user of the client device, the additional data to cause the automated assistant to provide output that includes a particular inference as to why the automated assistant performed a particular implementation of the assistant command. Using a synthesized speech comprising synthesized speech corresponding to a particular inference as to why the automated assistant performed a particular implementation of the assistant command, using a text-to-speech (TTS) model. processing the additional data to generate voice audio data.
일부 구현에서, 상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력을 제공하도록 상기 추가 데이터를 사용하도록 하는 단계는, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력이 상기 클라이언트 장치의 디스플레이에서 시각적으로 렌더링되게 하는 단계를 포함한다. 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계는 상기 추가 사용자 입력에 포함된 요청에 기초하여, 미리 생성된 데이터의 복수의 서로 다른 인스턴스 중에서 상기 추가 데이터를 선택하는 단계를 포함한다. In some implementations, for presentation to a user of the client device, the additional data to cause the automated assistant to provide output that includes a particular inference as to why the automated assistant performed a particular implementation of the assistant command. Causing to use includes causing output to be visually rendered on a display of the client device that includes a particular inference as to why the automated assistant performed a particular implementation of the assistant command. Processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command may be based on a request contained in the additional user input. , selecting the additional data from among a plurality of different instances of pre-generated data.
일부 구현들에서, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계는 상기 추가 사용자 입력에 포함된 요청에 기초하여, 상기 추가 데이터를 생성하는 단계를 포함한다.In some implementations, processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command is included in the additional user input. Based on the received request, generating the additional data.
일부 구현에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되며, 방법은, 클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계; 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터가 결정될 수 있는지 여부를 결정하는 단계; 상기 어시스턴트 명령의 특정 이행을 수행하는데 사용될 데이터가 결정될 수 없다는 결정에 응답하여: 상기 어시스턴트 명령의 대체 이행을 수행하는 데 사용될 대체 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계; 그리고 상기 자동화된 어시스턴트로 하여금 상기 어시스턴트 명령의 대체 이행을 수행하기 위해 상기 대체 데이터를 사용하게 하는 단계; 상기 클라이언트 장치의 사용자로부터, 상기 자동화된 어시스턴트가 상기 어시스턴트의 특정 이행 대신에 상기 어시스턴트 명령의 상기 대체 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 요청을 포함하는 추가 사용자 입력을 수신하는 단계; 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계; 그리고 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력을 제공하도록 상기 추가 데이터를 사용하도록 하는 단계를 포함한다.In some implementations, a method implemented by one or more processors is provided, the method comprising receiving user input from a user of a client device directed to an automated assistant comprising an assistant command and executing at least in part on the client device. step; determining whether data to be used for performing a specific implementation of the assistant command can be determined; In response to determining that data to be used to perform a particular implementation of the assistant command cannot be determined: processing the user input to determine replacement data to be used to perform the replacement implementation of the assistant command; and causing the automated assistant to use the substitute data to perform a substitute fulfillment of the assistant command; A further user comprising a request from a user of the client device to the automated assistant to provide a specific inference as to why the automated assistant performed the alternative implementation of the assistant command instead of the specific implementation of the assistant. receiving an input; processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed an alternate implementation of the assistant command instead of a specific implementation of the assistant command; and, for presentation to a user of a client device, cause the automated assistant to provide output that includes a specific inference as to why the automated assistant performed a substitute implementation of the assistant command instead of the specific implementation of the assistant command. and using the additional data to do so.
본 명세서에 개시된 기술의 이들 및 다른 구현은 선택적으로 다음 특징 중 하나 이상을 포함할 수 있다.These and other implementations of the technology disclosed herein may optionally include one or more of the following features.
일부 구현들에서, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계는 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행할 수 있는 방법에 대한 추천된 액션을 생성하는 데 사용된 추천 데이터를 생성하도록 상기 추가 사용자 입력을 프로세싱하는 단계를 더 포함한다. 이러한 구현의 일부 버전에서, 상기 출력은 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 가능하게 할 수 있는 방법에 대한 상기 추천된 액션을 더 포함할 수 있다. 이러한 구현의 일부 추가 버전에서, 상기 추천된 액션은 선택될 때 상기 자동화된 어시스턴트로 하여금 상기 추천된 액션을 수행하게 하는 프롬프트를 포함할 수 있다.In some implementations, processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed an alternative implementation of the assistant command instead of a specific implementation of the assistant command. The step further includes processing the additional user input to generate recommendation data used to generate a recommended action for how the automated assistant may perform a particular implementation of the assistant command. In some versions of these implementations, the output may further include the recommended action for how the automated assistant may enable specific fulfillment of the assistant command. In some further versions of this implementation, the recommended action may include a prompt that, when selected, causes the automated assistant to perform the recommended action.
일부 구현에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되며, 방법은, 클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계; 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터가 결정될 수 있는지 여부를 결정하는 단계; 상기 어시스턴트 명령의 특정 이행을 수행하는데 사용될 데이터가 결정될 수 없다는 결정에 응답하여: 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행할 수 있는 방법에 대한 추천된 액션을 생성하는데 사용될 추천 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계; 그리고 상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 가능하게 할 수 있는 방법에 대한 추천된 액션을 포함하고, 그리고 선택될 때 상기 자동화된 어시스턴트가 상기 추천된 액션을 수행하도록 하는 프롬프트를 포함하는 출력을 제공하도록 추천 데이터를 사용하도록 하는 단계; 그리고 상기 클라이언트 장치의 사용자로부터, 상기 프롬프트의 사용자 선택을 포함하는 추가 사용자 입력을 수신하는 것에 응답하여: 상기 자동화된 어시스턴트로 하여금 상기 어시스턴트 명령의 특정 이행을 수행할 수 있도록 상기 추천된 액션을 수행하도록 하는 단계를 포함한다. In some implementations, a method implemented by one or more processors is provided, the method comprising receiving user input from a user of a client device directed to an automated assistant comprising an assistant command and executing at least in part on the client device. step; determining whether data to be used for performing a specific implementation of the assistant command can be determined; In response to determining that data to be used to perform a particular implementation of the assistant command cannot be determined: determine recommended data to be used to generate a recommended action for how the automated assistant can perform a particular implementation of the assistant command. processing the user input to do so; and include a recommended action for how the automated assistant may enable the specific implementation of the assistant command, and when selected, the automated assistant, for presentation to a user of the client device. causing an automated assistant to use the recommendation data to provide output comprising a prompt to perform the recommended action; and in response to receiving additional user input from the user of the client device, including user selection of the prompt: to perform the recommended action to enable the automated assistant to perform a specific implementation of the assistant command. It includes steps to
본 명세서에 개시된 기술의 이들 및 다른 구현은 선택적으로 다음 특징 중 하나 이상을 포함할 수 있다. These and other implementations of the technology disclosed herein may optionally include one or more of the following features.
일부 구현에서, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행할 수 있는 방법에 대한 추천된 액션을 생성하는데 사용될 추천 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 것은 상기 어시스턴트 명령의 대체 이행이 없다는 결정에 응답할 수 있다.In some implementations, processing the user input to determine recommended data to be used to generate recommended actions for how the automated assistant can perform a particular implementation of the assistant command is an alternative implementation of the assistant command. You can respond to a decision that there is no.
일부 구현에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되며, 방법은, 클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계; 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계; 상기 자동화된 어시스턴트로 하여금 상기 데이터를 사용하여 상기 어시스턴트 명령의 특정 이행을 수행하게 하는 단계; 상기 클라이언트 장치의 사용자로부터, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행하지 않은 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 요청을 포함하는 추가 사용자 입력을 수신하는 단계; 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행하지 않은 이유에 대한 상기 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계; 그리고 상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행하지 않은 이유에 대한 특정 추론을 포함하는 출력을 제공하도록 상기 추가 데이터를 사용하도록 하는 단계를 포함할 수 있다. In some implementations, a method implemented by one or more processors is provided, the method comprising receiving user input from a user of a client device directed to an automated assistant comprising an assistant command and executing at least in part on the client device. step; processing the user input to determine data to be used to perform a particular implementation of the assistant command; causing the automated assistant to perform a specific implementation of the assistant command using the data; Further comprising a request from a user of the client device to the automated assistant to provide a specific inference as to why the automated assistant did not perform an alternative implementation of the assistant command instead of a specific implementation of the assistant command. receiving user input; processing the additional user input to determine additional data to be used to provide the specific inference as to why the automated assistant did not perform an alternate implementation of the assistant command instead of the specific implementation of the assistant command; and cause the automated assistant to, for presentation to a user of the client device, output comprising a specific inference as to why the automated assistant did not perform a substitute implementation of the assistant command instead of the specific implementation of the assistant command. It may include using the additional data to provide.
일부 구현에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되며, 방법은, 클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계; 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계; 상기 자동화된 어시스턴트로 하여금 상기 데이터를 사용하여 상기 어시스턴트 명령의 특정 이행을 수행하게 하는 단계; 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행하는 동안: 상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 선택될 때 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하게 하는 선택 가능한 요소를 시각적으로 렌더링하게 하는 단계; 그리고 상기 선택 가능한 요소의 사용자 선택을 포함하는 추가 사용자 입력을 상기 클라이언트 장치의 사용자로부터 수신하는 것에 응답하여: 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행하지 않은 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계; 그리고 상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력을 제공하게 하는 단계를 포함할 수 있다. In some implementations, a method implemented by one or more processors is provided, the method comprising receiving user input from a user of a client device directed to an automated assistant comprising an assistant command and executing at least in part on the client device. step; processing the user input to determine data to be used to perform a specific implementation of the assistant command; causing the automated assistant to perform a specific implementation of the assistant command using the data; While the automated assistant performs specific implementations of the assistant commands: Causes the automated assistant to perform specific implementations of the assistant commands when selected, to present to the user of the client device. causing a visually rendered selectable element to provide a particular inference as to why; and in response to receiving additional user input from a user of the client device comprising a user selection of the selectable element: the automated assistant has not performed a substitute implementation of the assistant command instead of the specific implementation of the assistant command. processing the additional user input to determine additional data to be used to provide a specific inference as to why; and causing the automated assistant to provide output comprising a particular inference as to why the automated assistant performed a particular implementation of the assistant command, for presentation to a user of the client device. there is.
본 명세서에 개시된 기술의 이들 및 다른 구현은 선택적으로 다음 특징 중 하나 이상을 포함할 수 있다.These and other implementations of the technology disclosed herein may optionally include one or more of the following features.
일부 구현에서, 자동화된 어시스턴트로 향하는 어시스턴트 명령을 포함하는 사용자 입력은 클라이언트 장치의 하나 이상의 마이크로폰에 의해 생성된 오디오 데이터에서 캡처될 수 있다. 이러한 구현의 일부 버전에서, 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계는, 자동 음성 인식(ASR) 모델을 사용하여, 상기 어시스턴트 명령을 포함하는 상기 사용자 입력을 캡처한 오디오 데이터를 프로세싱하여 ASR 출력을 생성하는 단계; 자연어 이해(NLU) 모델을 사용하여, 상기 ASR 출력을 프로세싱하여 NLU 출력을 생성하는 단계; 그리고 상기 NLU 출력에 기초하여 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 상기 데이터를 결정하는 단계를 포함할 수 있다. 이러한 구현의 일부 추가 버전에서, 상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 선택될 때 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하게 하는 선택 가능한 요소를 시각적으로 렌더링하게 하는 것은 상기 NLU 출력과 연관된 NLU 메트릭이 NLU 메트릭 임계값을 충족하지 못한다는 결정에 응답할 수 있다.In some implementations, user input including assistant commands directed to the automated assistant may be captured in audio data generated by one or more microphones of the client device. In some versions of these implementations, processing the user input to determine data to be used to perform a specific implementation of the assistant command may include, using an automatic speech recognition (ASR) model, the process comprising the assistant command. processing the audio data that captured the user input to generate an ASR output; processing the ASR output to generate an NLU output, using a natural language understanding (NLU) model; and determining the data to be used for performing a specific implementation of the assistant command based on the NLU output. In some further versions of this implementation, to present to a user of the client device, cause the automated assistant to, when selected, provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command. The visually rendering of a selectable element may respond to a determination that an NLU metric associated with the NLU output does not meet an NLU metric threshold.
또한, 일부 구현에는 하나 이상의 프로세서(예: 중앙 프로세싱 장치(들)(CPU(들)), 그래픽 프로세싱 장치(들)(GPU(들) 및/또는 텐서 프로세싱 장치(들))가 포함되며, 하나 이상의 컴퓨팅 장치에서 하나 이상의 프로세서는 관련 메모리에 저장된 명령어를 실행하도록 동작 가능하고 명령어는 전술한 방법 중 임의의 방법의 성능을 유발하도록 구성된다. 일부 구현은 또한 전술한 방법 중 임의의 것을 수행하기 위해 하나 이상의 프로세서에 의해 실행가능한 컴퓨터 명령어를 저장하는 하나 이상의 비일시적 컴퓨터 판독가능 저장 매체를 포함한다. 일부 구현은 또한 전술한 방법 중 임의의 것을 수행하기 위해 하나 이상의 프로세서에 의해 실행가능한 명령어를 포함하는 컴퓨터 프로그램 제품을 포함한다. Additionally, some implementations include one or more processors (e.g., central processing unit(s) (CPU(s)), graphics processing unit(s) (GPU(s) and/or tensor processing unit(s)); The one or more processors in the above computing devices are operable to execute instructions stored in associated memory and the instructions are configured to cause the performance of any of the methods described above Some implementations may also implement any of the methods described above One or more non-transitory computer readable storage media storing computer instructions executable by one or more processors Some implementations also include instructions executable by one or more processors to perform any of the methods described above. Includes computer program products.
Claims (30)
클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계;
상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계;
상기 자동화된 어시스턴트로 하여금 상기 데이터를 사용하여 상기 어시스턴트 명령의 상기 특정 이행을 수행하게 하는 단계;
상기 클라이언트 장치의 사용자로부터, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 요청을 포함하는 추가 사용자 입력을 수신하는 단계;
상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계; 그리고
상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력을 제공하도록 상기 추가 데이터를 사용하도록 하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.A method implemented by one or more processors, comprising:
receiving user input from a user of a client device to an automated assistant comprising assistant commands and executing at least in part on the client device;
processing the user input to determine data to be used to perform a specific implementation of the assistant command;
causing the automated assistant to perform the particular implementation of the assistant command using the data;
receiving additional user input from the user of the client device, the request comprising a request to the automated assistant to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command;
processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command; and
and, for presentation to a user of the client device, cause the automated assistant to use the additional data to provide output that includes a specific inference as to why the automated assistant performed a specific implementation of the assistant command. A method implemented by one or more processors comprising the steps of:
자동 음성 인식(ASR) 모델을 사용하여, 상기 어시스턴트 명령을 포함하는 상기 사용자 입력을 캡처한 오디오 데이터를 프로세싱하여 ASR 출력을 생성하는 단계;
자연어 이해(NLU) 모델을 사용하여, 상기 ASR 출력을 프로세싱하여 NLU 출력을 생성하는 단계; 그리고
상기 NLU 출력에 기초하여 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 결정하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.3. The method of claim 2, wherein processing the user input to determine data to be used to perform a particular implementation of the assistant command comprises:
processing audio data that captured the user input including the assistant command to generate an ASR output, using an automatic speech recognition (ASR) model;
processing the ASR output to generate an NLU output, using a natural language understanding (NLU) model; and
and determining data to be used to perform a specific implementation of the assistant command based on the NLU output.
자연어 이해(NLU) 모델을 사용하여, 상기 타이핑된 입력을 프로세싱하여 NLU 출력을 생성하는 단계; 그리고
상기 NLU 출력에 기초하여 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 생성하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.5. The method of claim 4, wherein processing the user input to determine data to be used to perform a particular implementation of the assistant command comprises:
processing the typed input to generate NLU output, using a natural language understanding (NLU) model; and
and generating data to be used to perform a specific implementation of the assistant command based on the NLU output.
상기 자동화된 어시스턴트가 상기 특정 이행을 수행하는 데 사용된, 복수의 서로 다른 소프트웨어 애플리케이션들 중에서, 특정 소프트웨어 애플리케이션을 선택한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 특정 요청인 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.The method of claim 1 , wherein a request to the automated assistant to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command comprises:
a specific request to the automated assistant to provide a specific inference as to why the automated assistant selected a specific software application, out of a plurality of different software applications, used to perform the specific implementation. A method implemented by one or more processors that do.
상기 특정 이행을 수행하는 데 사용된 상기 특정 소프트웨어 애플리케이션과 연관된 메타데이터를 획득하는 단계; 그리고
상기 특정 소프트웨어 애플리케이션과 연관된 메타데이터에 기초하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 상기 추가 데이터를 결정하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.7. The method of claim 6, wherein processing additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command comprises:
obtaining metadata associated with the specific software application used to perform the specific implementation; and
determining, based on metadata associated with the specific software application, the additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command. A method implemented by one or more processors.
상기 자동화된 어시스턴트가 특정 이행을 수행하는 데 사용된, 상기 사용자 입력의 복수의 서로 다른 해석(disparate interpretations) 중에서, 상기 사용자 입력의 특정 해석을 선택한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 특정 요청인 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.A request for an automated assistant to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command according to any one of the preceding claims.
The automated assistant causing the automated assistant to provide a particular inference as to why, among a plurality of disparate interpretations of the user input, a particular interpretation of the user input was selected that was used to perform the particular action. A method implemented by one or more processors, characterized in that it is a specific request for.
상기 특정 이행을 수행하는 데 사용된 상기 사용자 입력의 특정 해석과 연관된 메타데이터를 획득하는 단계; 그리고
상기 사용자 입력의 특정 해석과 연관된 메타데이터에 기초하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 상기 추가 데이터를 결정하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.9. The method of claim 8, wherein processing additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command comprises:
obtaining metadata associated with a particular interpretation of the user input used to perform the particular implementation; and
determining, based on metadata associated with the specific interpretation of the user input, the additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command. A method implemented by one or more processors that
상기 자동화된 어시스턴트가 특정 이행을 수행하는 데 사용된, 상기 사용자의 클라이언트 장치 대신에, 상기 사용자의 추가 클라이언트 장치를 선택한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 특정 요청인 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.A request for an automated assistant to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command according to any one of the preceding claims.
a specific request to the automated assistant to provide a specific inference as to why the automated assistant has selected the user's additional client device, rather than the user's client device, used to perform the specific transition. A method implemented by one or more processors that
상기 특정 이행을 수행하는 데 사용된 상기 추가 클라이언트 장치와 연관된 메타데이터를 획득하는 단계; 그리고
상기 추가 클라이언트 장치와 연관된 메타데이터에 기초하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 상기 추가 데이터를 결정하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.11. The method of claim 10, wherein processing additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command comprises:
obtaining metadata associated with the additional client device used to perform the particular implementation; and
determining, based on metadata associated with the additional client device, the additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command. A method implemented by one or more processors.
상기 자동화된 어시스턴트가 상기 특정 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 일반적인 요청인 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.A request for an automated assistant to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command according to any one of the preceding claims.
and a general request to the automated assistant to provide a specific inference as to why the automated assistant performed the specific action.
(i) 상기 특정 이행을 수행하는 데 사용된, 복수의 서로 다른 소프트웨어 애플리케이션 중에서, 특정 소프트웨어 애플리케이션, (ii) 상기 특정 이행을 수행하는 데 사용된, 상기 사용자 입력에 대한 복수의 서로 다른 해석 중에서, 상기 사용자 입력에 대한 특정 해석, 또는 (iii) 상기 특정 이행을 수행하는 데 사용된, 상기 사용자의 클라이언트 장치 대신에, 상기 사용자의 추가 클라이언트 장치 중 하나 이상과 연관된 해당 메타데이터를 획득하는 단계: 그리고
상기 해당 메타데이터에 기초하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 상기 추가 데이터를 결정하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.13. The method of claim 12, wherein processing additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of an assistant command comprises:
(i) a particular software application, among a plurality of different software applications used to perform the particular transition; (ii) among a plurality of different interpretations of the user input used to perform the particular transition; obtaining a particular interpretation of the user input, or (iii) corresponding metadata associated with one or more of the user's additional client devices, instead of the user's client device, used to perform the particular implementation; and
determining, based on the corresponding metadata, the additional data to be used to provide a particular inference as to why the automated assistant performed a particular implementation of the assistant command. method implemented by
TTS(text-to-speech) 모델을 사용하여, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론에 대응하는 합성된 음성을 포함하는 합성된 음성 오디오 데이터를 생성하도록 상기 추가 데이터를 프로세싱하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.The method according to claim 1 , which causes the automated assistant to, for presentation to a user of the client device, output comprising a specific inference as to why the automated assistant performed a specific implementation of the assistant command. The step of using the additional data to provide
generate synthesized speech audio data comprising synthesized speech corresponding to a particular inference as to why the automated assistant performed a particular implementation of the assistant command, using a text-to-speech (TTS) model; A method implemented by one or more processors comprising processing additional data.
상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력이 상기 클라이언트 장치의 디스플레이에서 시각적으로 렌더링되게 하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.The method according to claim 1 , which causes the automated assistant to, for presentation to a user of the client device, output comprising a specific inference as to why the automated assistant performed a specific implementation of the assistant command. The step of using the additional data to provide
and causing the automated assistant to visually render on the display of the client device output comprising a specific inference as to why the specific implementation of the assistant command. method.
상기 추가 사용자 입력에 포함된 요청에 기초하여, 미리 생성된 데이터의 복수의 서로 다른 인스턴스 중에서 상기 추가 데이터를 선택하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.10. The method of any one of the preceding claims, wherein processing additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command comprises:
and selecting the additional data from among a plurality of different instances of pre-generated data based on a request contained in the additional user input.
상기 추가 사용자 입력에 포함된 요청에 기초하여, 상기 추가 데이터를 생성하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.10. The method of any one of the preceding claims, wherein processing additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command comprises:
and generating the additional data based on a request included in the additional user input.
클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계;
상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터가 결정될 수 있는지 여부를 결정하는 단계;
상기 어시스턴트 명령의 특정 이행을 수행하는데 사용될 데이터가 결정될 수 없다는 결정에 응답하여:
상기 어시스턴트 명령의 대체 이행을 수행하는 데 사용될 대체 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계; 그리고
상기 자동화된 어시스턴트로 하여금 상기 어시스턴트 명령의 대체 이행을 수행하기 위해 상기 대체 데이터를 사용하게 하는 단계;
상기 클라이언트 장치의 사용자로부터, 상기 자동화된 어시스턴트가 상기 어시스턴트의 특정 이행 대신에 상기 어시스턴트 명령의 상기 대체 이행을 수행한 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 요청을 포함하는 추가 사용자 입력을 수신하는 단계;
상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행한 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계; 그리고
클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력을 제공하도록 상기 추가 데이터를 사용하도록 하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.A method implemented by one or more processors, comprising:
receiving user input from a user of a client device to an automated assistant comprising assistant commands and executing at least in part on the client device;
determining whether data to be used for performing a specific implementation of the assistant command can be determined;
In response to determining that data to be used to perform a particular implementation of the assistant command cannot be determined:
processing the user input to determine replacement data to be used to perform replacement implementation of the assistant command; and
causing the automated assistant to use the substitute data to perform a substitute fulfillment of the assistant command;
A further user comprising a request from a user of the client device to the automated assistant to provide a specific inference as to why the automated assistant performed the alternative implementation of the assistant command instead of the specific implementation of the assistant. receiving an input;
processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed an alternate implementation of the assistant command instead of a specific implementation of the assistant command; and
For presentation to a user of a client device, cause the automated assistant to provide output that includes a specific inference as to why the automated assistant performed an alternative implementation of the assistant command instead of the specific implementation of the assistant command. and enabling the additional data to be used.
상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행할 수 있는 방법에 대한 추천된 액션을 생성하는 데 사용된 추천 데이터를 생성하도록 상기 추가 사용자 입력을 프로세싱하는 단계를 더 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.19. The method of claim 18, wherein processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant performed an alternate implementation of the assistant command instead of a specific implementation of the assistant command. The steps are,
processing the additional user input to generate recommendation data used to generate a recommended action for how the automated assistant may perform a particular implementation of the assistant command. A method implemented by one or more processors.
클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계;
상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터가 결정될 수 있는지 여부를 결정하는 단계;
상기 어시스턴트 명령의 특정 이행을 수행하는데 사용될 데이터가 결정될 수 없다는 결정에 응답하여:
상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행할 수 있는 방법에 대한 추천된 액션을 생성하는데 사용될 추천 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계; 그리고
상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 가능하게 할 수 있는 방법에 대한 추천된 액션을 포함하고, 그리고 선택될 때 상기 자동화된 어시스턴트가 상기 추천된 액션을 수행하도록 하는 프롬프트를 포함하는 출력을 제공하도록 추천 데이터를 사용하도록 하는 단계; 그리고
상기 클라이언트 장치의 사용자로부터, 상기 프롬프트의 사용자 선택을 포함하는 추가 사용자 입력을 수신하는 것에 응답하여:
상기 자동화된 어시스턴트로 하여금 상기 어시스턴트 명령의 특정 이행을 수행할 수 있도록 상기 추천된 액션을 수행하도록 하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.A method implemented by one or more processors, comprising:
receiving user input from a user of a client device to an automated assistant comprising assistant commands and executing at least in part on the client device;
determining whether data to be used for performing a specific implementation of the assistant command can be determined;
In response to determining that data to be used to perform a particular implementation of the assistant command cannot be determined:
processing the user input to determine recommended data to be used to generate recommended actions for how the automated assistant may perform a particular implementation of the assistant command; and
Include a recommended action on how the automated assistant can enable the specific fulfillment of the assistant command, and, when selected, the automated assistant, for presentation to the user of the client device. causing the recommended assistant to use the recommended data to provide output including a prompt to perform the recommended action; and
In response to receiving additional user input from the user of the client device, including user selection of the prompt:
and causing the automated assistant to perform the recommended action to enable the specific implementation of the assistant command.
상기 어시스턴트 명령의 대체 이행이 없다는 결정에 응답하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.23. The method of claim 22, wherein processing the user input to determine recommended data to be used to generate a recommended action for how the automated assistant can perform a particular implementation of the assistant command comprises:
and responding to a determination that there is no alternative implementation of the assistant instruction.
클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계;
상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계;
상기 자동화된 어시스턴트로 하여금 상기 데이터를 사용하여 상기 어시스턴트 명령의 특정 이행을 수행하게 하는 단계;
상기 클라이언트 장치의 사용자로부터, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행하지 않은 이유에 대한 특정 추론을 제공하도록 하는 상기 자동화된 어시스턴트에 대한 요청을 포함하는 추가 사용자 입력을 수신하는 단계;
상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행하지 않은 이유에 대한 상기 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계; 그리고
상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행하지 않은 이유에 대한 특정 추론을 포함하는 출력을 제공하도록 상기 추가 데이터를 사용하도록 하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.A method implemented by one or more processors, comprising:
receiving user input from a user of a client device to an automated assistant comprising assistant commands and executing at least in part on the client device;
processing the user input to determine data to be used to perform a specific implementation of the assistant command;
causing the automated assistant to perform a specific implementation of the assistant command using the data;
Further comprising a request from a user of the client device to the automated assistant to provide a specific inference as to why the automated assistant did not perform an alternative implementation of the assistant command instead of a specific implementation of the assistant command. receiving user input;
processing the additional user input to determine additional data to be used to provide the specific inference as to why the automated assistant did not perform an alternate implementation of the assistant command instead of the specific implementation of the assistant command; and
For presentation to a user of the client device, causes the automated assistant to produce output containing a specific inference as to why the automated assistant did not perform a substitute implementation of the assistant command instead of a specific implementation of the assistant command. A method implemented by one or more processors comprising the step of using said additional data to provide
클라이언트 장치의 사용자로부터, 어시스턴트 명령을 포함하고 그리고 상기 클라이언트 장치에서 적어도 부분적으로 실행하는 자동화된 어시스턴트로 향하는 사용자 입력을 수신하는 단계;
상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 데이터를 결정하기 위해 상기 사용자 입력을 프로세싱하는 단계;
상기 자동화된 어시스턴트로 하여금 상기 데이터를 사용하여 상기 어시스턴트 명령의 특정 이행을 수행하게 하는 단계;
상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행하는 동안:
상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 선택될 때 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 제공하게 하는 선택 가능한 요소를 시각적으로 렌더링하게 하는 단계; 그리고
상기 선택 가능한 요소의 사용자 선택을 포함하는 추가 사용자 입력을 상기 클라이언트 장치의 사용자로부터 수신하는 것에 응답하여:
상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행 대신에 상기 어시스턴트 명령의 대체 이행을 수행하지 않은 이유에 대한 특정 추론을 제공하는 데 사용될 추가 데이터를 결정하기 위해 상기 추가 사용자 입력을 프로세싱하는 단계; 그리고
상기 클라이언트 장치의 사용자에게 프레젠테이션하기 위해, 상기 자동화된 어시스턴트로 하여금, 상기 자동화된 어시스턴트가 상기 어시스턴트 명령의 특정 이행을 수행한 이유에 대한 특정 추론을 포함하는 출력을 제공하게 하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.A method implemented by one or more processors, comprising:
receiving user input from a user of a client device to an automated assistant comprising assistant commands and executing at least in part on the client device;
processing the user input to determine data to be used to perform a specific implementation of the assistant command;
causing the automated assistant to perform a specific implementation of the assistant command using the data;
While the automated assistant performs certain implementations of the assistant commands:
For presentation to the user of the client device, visually rendering a selectable element that causes the automated assistant to, when selected, provide a specific inference as to why the automated assistant performed a specific implementation of the assistant command. the step of making; and
In response to receiving additional user input from the user of the client device comprising a user selection of the selectable element:
processing the additional user input to determine additional data to be used to provide a specific inference as to why the automated assistant did not perform a specific implementation of the assistant command instead of performing an alternate implementation of the assistant command; and
and causing the automated assistant to provide output comprising a particular inference as to why the automated assistant performed a particular implementation of the assistant command, for presentation to a user of the client device. A method implemented by one or more processors that
자동 음성 인식(ASR) 모델을 사용하여, 상기 어시스턴트 명령을 포함하는 상기 사용자 입력을 캡처한 오디오 데이터를 프로세싱하여 ASR 출력을 생성하는 단계;
자연어 이해(NLU) 모델을 사용하여, 상기 ASR 출력을 프로세싱하여 NLU 출력을 생성하는 단계; 그리고
상기 NLU 출력에 기초하여 상기 어시스턴트 명령의 특정 이행을 수행하는 데 사용될 상기 데이터를 결정하는 단계를 포함하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.27. The method of claim 26, wherein processing the user input to determine data to be used to perform a particular implementation of the assistant command comprises:
processing audio data that captured the user input including the assistant command to generate an ASR output, using an automatic speech recognition (ASR) model;
processing the ASR output to generate an NLU output, using a natural language understanding (NLU) model; and
and determining the data to be used to perform a specific implementation of the assistant command based on the NLU output.
상기 NLU 출력과 연관된 NLU 메트릭이 NLU 메트릭 임계값을 충족하지 못한다는 결정에 응답하는 것을 특징으로 하는 하나 이상의 프로세서에 의해 구현되는 방법.28. The method of claim 27, wherein the selection causes the automated assistant to provide, when selected, a specific inference as to why the automated assistant performed a specific implementation of the assistant command, for presentation to a user of the client device. What makes it possible to visually render an element is,
and responding to a determination that an NLU metric associated with the NLU output does not satisfy an NLU metric threshold.
적어도 하나의 프로세서; 그리고
실행될 때 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제28항 중 어느 한 항에 대응하는 동작을 수행하게 하는 명령어를 저장한 메모리를 포함하는 것을 특징으로 하는 시스템.As a system,
at least one processor; and
A system comprising a memory storing instructions that, when executed, cause the at least one processor to perform an operation corresponding to any one of claims 1 to 28.
실행될 때 적어도 하나의 프로세서로 하여금 제1항 내지 제28항 중 어느 한 항에 대응하는 동작을 수행하게 하는 명령어를 저장한 것을 특징으로 하는 비일시적 컴퓨터 판독 가능 저장 매체.As a non-transitory computer-readable storage medium,
A non-transitory computer-readable storage medium characterized by storing instructions that, when executed, cause at least one processor to perform an operation corresponding to any one of claims 1 to 28.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163226961P | 2021-07-29 | 2021-07-29 | |
US63/226,961 | 2021-07-29 | ||
US17/532,759 | 2021-11-22 | ||
US17/532,759 US20230031461A1 (en) | 2021-07-29 | 2021-11-22 | Providing certain reasoning with respect to fulfillment of an assistant command |
PCT/US2021/060986 WO2023009156A1 (en) | 2021-07-29 | 2021-11-29 | Providing certain reasoning with respect to fulfillment of an assistant command |
Publications (1)
Publication Number | Publication Date |
---|---|
KR20230118959A true KR20230118959A (en) | 2023-08-14 |
Family
ID=79165039
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020237023808A KR20230118959A (en) | 2021-07-29 | 2021-11-29 | Providing a specific reason for the fulfillment of an Assistant order |
Country Status (4)
Country | Link |
---|---|
EP (1) | EP4150536A1 (en) |
JP (1) | JP2024508209A (en) |
KR (1) | KR20230118959A (en) |
WO (1) | WO2023009156A1 (en) |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11069351B1 (en) * | 2018-12-11 | 2021-07-20 | Amazon Technologies, Inc. | Vehicle voice user interface |
US11170774B2 (en) * | 2019-05-21 | 2021-11-09 | Qualcomm Incorproated | Virtual assistant device |
US20210125612A1 (en) * | 2019-10-24 | 2021-04-29 | Capital One Services, Llc | Systems and methods for automated discrepancy determination, explanation, and resolution with personalization |
US20220058347A1 (en) * | 2020-08-21 | 2022-02-24 | Oracle International Corporation | Techniques for providing explanations for text classification |
-
2021
- 2021-11-29 KR KR1020237023808A patent/KR20230118959A/en unknown
- 2021-11-29 EP EP21835012.2A patent/EP4150536A1/en active Pending
- 2021-11-29 JP JP2023537164A patent/JP2024508209A/en active Pending
- 2021-11-29 WO PCT/US2021/060986 patent/WO2023009156A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
JP2024508209A (en) | 2024-02-26 |
WO2023009156A1 (en) | 2023-02-02 |
EP4150536A1 (en) | 2023-03-22 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3788620B1 (en) | Supplementing voice inputs to an automated assistant according to selected suggestions | |
US11763813B2 (en) | Methods and systems for reducing latency in automated assistant interactions | |
US20230377580A1 (en) | Dynamically adapting on-device models, of grouped assistant devices, for cooperative processing of assistant requests | |
US20240046935A1 (en) | Generating and/or utilizing voice authentication biasing parameters for assistant devices | |
KR20240007261A (en) | Use large-scale language models to generate automated assistant response(s) | |
JP2023549015A (en) | Enabling natural conversations about automated assistants | |
US20230031461A1 (en) | Providing certain reasoning with respect to fulfillment of an assistant command | |
US20230061929A1 (en) | Dynamically configuring a warm word button with assistant commands | |
KR20230118959A (en) | Providing a specific reason for the fulfillment of an Assistant order | |
KR20230147157A (en) | Contextual suppression of assistant command(s) | |
KR20230156929A (en) | Robotic computing device with adaptive user interaction | |
CN116711283A (en) | Providing deterministic reasoning about fulfilling assistant commands | |
US20230053341A1 (en) | Enabling natural conversations with soft endpointing for an automated assistant | |
US20240078374A1 (en) | System(s) and method(s) for causing contextually relevant emoji(s) to be visually rendered for presentation to user(s) in smart dictation | |
US20240111811A1 (en) | Selecting a device to respond to device-agnostic user requests | |
US20230197072A1 (en) | Warm word arbitration between automated assistant devices | |
KR20230158615A (en) | Enable natural conversations using soft endpointing for automated assistants | |
CN117121100A (en) | Enabling natural conversations with soft endpoints for automated assistants | |
WO2024054271A1 (en) | System(s) and method(s) for causing contextually relevant emoji(s) to be visually rendered for presentation to user(s) in smart dictation | |
KR20230153450A (en) | Device arbitration for local implementation of automatic speech recognition | |
JP2024505794A (en) | Dynamic adaptation of graphical user interface elements by an automated assistant as the user repeatedly provides an audio utterance or sequence of audio utterances | |
CN116670637A (en) | Dynamic adaptation of graphical user interface elements by an automated assistant when a user iteratively provides a spoken utterance or sequence of spoken utterances | |
WO2023114087A1 (en) | Warm word arbitration between automated assistant devices |