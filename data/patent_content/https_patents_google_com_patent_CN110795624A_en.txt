CN110795624A - Analyzing a personalization framework - Google Patents
Analyzing a personalization framework Download PDFInfo
- Publication number
- CN110795624A CN110795624A CN201911011979.1A CN201911011979A CN110795624A CN 110795624 A CN110795624 A CN 110795624A CN 201911011979 A CN201911011979 A CN 201911011979A CN 110795624 A CN110795624 A CN 110795624A
- Authority
- CN
- China
- Prior art keywords
- user
- interest
- machine learning
- learning model
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000003993 interaction Effects 0.000 claims abstract description 192
- 238000012549 training Methods 0.000 claims abstract description 116
- 238000010801 machine learning Methods 0.000 claims abstract description 59
- 238000000034 method Methods 0.000 claims abstract description 48
- 238000012544 monitoring process Methods 0.000 claims abstract description 7
- 230000015654 memory Effects 0.000 claims description 18
- 230000004044 response Effects 0.000 claims description 18
- 238000007477 logistic regression Methods 0.000 claims description 6
- 230000007774 longterm Effects 0.000 claims description 4
- 230000006403 short-term memory Effects 0.000 claims description 4
- 238000004458 analytical method Methods 0.000 abstract description 45
- 238000013528 artificial neural network Methods 0.000 description 20
- 230000009471 action Effects 0.000 description 15
- 238000012545 processing Methods 0.000 description 15
- 230000000007 visual effect Effects 0.000 description 13
- 238000004590 computer program Methods 0.000 description 12
- 230000008569 process Effects 0.000 description 12
- 238000004891 communication Methods 0.000 description 10
- 238000013515 script Methods 0.000 description 7
- 238000010586 diagram Methods 0.000 description 6
- 230000000694 effects Effects 0.000 description 5
- 239000000284 extract Substances 0.000 description 5
- 238000000605 extraction Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000008520 organization Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 2
- 238000004422 calculation algorithm Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000007405 data analysis Methods 0.000 description 2
- 238000013135 deep learning Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000014509 gene expression Effects 0.000 description 2
- 230000001939 inductive effect Effects 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000010295 mobile communication Methods 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 238000009877 rendering Methods 0.000 description 2
- 238000012552 review Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000010200 validation analysis Methods 0.000 description 2
- 229920002334 Spandex Polymers 0.000 description 1
- 230000003044 adaptive effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 239000002131 composite material Substances 0.000 description 1
- 235000014510 cooky Nutrition 0.000 description 1
- 238000012517 data analytics Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 230000003111 delayed effect Effects 0.000 description 1
- 238000005553 drilling Methods 0.000 description 1
- 230000002068 genetic effect Effects 0.000 description 1
- 230000003116 impacting effect Effects 0.000 description 1
- 238000012417 linear regression Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000005065 mining Methods 0.000 description 1
- 238000010926 purge Methods 0.000 description 1
- 238000007637 random forest analysis Methods 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000003997 social interaction Effects 0.000 description 1
- 239000004759 spandex Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 230000026676 system process Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000003442 weekly effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/34—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment
- G06F11/3409—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment for performance assessment
- G06F11/3414—Workload generation, e.g. scripts, playback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/34—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment
- G06F11/3438—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment monitoring of user actions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/34—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment
- G06F11/3447—Performance evaluation by modeling
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/248—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/31—Indexing; Data structures therefor; Storage structures
- G06F16/313—Selection or weighting of terms for indexing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/958—Organisation or management of web site content, e.g. publishing, maintaining pages or automatic linking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/18—Complex mathematical operations for evaluating statistical data, e.g. average values, frequency distributions, probability functions, regression analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/10—Office automation; Time management
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/50—Network services
- H04L67/535—Tracking the activity of the user
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
- G06N20/10—Machine learning using kernel methods, e.g. support vector machines [SVM]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
- G06N20/20—Ensemble learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/01—Dynamic search techniques; Heuristics; Dynamic trees; Branch-and-bound
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/02—Knowledge representation; Symbolic representation
- G06N5/022—Knowledge engineering; Knowledge acquisition
- G06N5/025—Extracting rules from data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/01—Probabilistic graphical models, e.g. probabilistic networks
Abstract
Methods, systems, and computer-readable media for personalizing an analysis user interface. The method includes generating a training data set from received user interaction data, inputting the training data set to a machine learning model to train the model, generating a set of user interest scores for particular users, wherein each user interest score indicates a user's interest in accessing information corresponding to a UI element of an application, determining from the user interest scores that the user is interested in particular UI elements that are not included in the initial UI and that have at least a threshold score, dynamically modifying the initial UI to include the particular UI elements, presenting an updated UI, monitoring for further user interactions, updating the model based on the further user interactions, and modifying the updated UI based on the updated model.
Description
Technical Field
This document relates to providing a personalized framework within an analysis environment.
Background
This document relates to providing a personalized framework within an analysis environment. Users interacting with content on the internet generate interaction data that can be viewed and analyzed through different User Interface (UI) elements. The user interface and the content presented by the user interface (such as the analysis data) may be customized through an automated framework.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in a method that includes receiving, by a computing device, user interaction data indicative of interactions of a first user with one or more User Interface (UI) elements of an initial UI of an analytics reporting application presented to the first user. The method includes generating a training data set from received user interaction data and inputting the training data set to a machine learning model. In response to training the data set, the method includes training, by the computing device, a machine learning model, and generating, using the trained machine learning model, a set of user interest scores for the second user, wherein each of the user interest scores indicates the second user's interest in accessing information corresponding to a UI element of the analytics reporting application. The method includes determining, based on a threshold score associated with a particular UI element that is not included in the initial UI, that at least one of the sets of user interest scores positively indicates a second user's interest in accessing information corresponding to the particular UI element, and in response to determining that at least one of the sets of user interest scores positively indicates a second user's interest in accessing information corresponding to the particular UI element, dynamically modifying the initial UI, including incorporating the particular UI element into the initial UI, to obtain an updated UI that includes the particular UI element that is not included in the initial UI. The method includes presenting an updated UI including a particular UI to a second user, monitoring for further user interaction with the updated UI after presenting the updated UI to the second user, and updating the machine learning model based on the further user interaction. The method includes selecting an updated set of UI elements to present in the UI based on the updated machine learning model.
Each of these and other embodiments may optionally include one or more of the following features. In some embodiments, the machine learning model is a logistic regression model. In some implementations, the machine learning model performs depth and breadth learning. In some embodiments, the machine learning model is a long term short term memory model.
The method may also include determining that a score in the set of user interest scores associated with the other UI element is less than a threshold score associated with the other UI element; wherein the other UI element is not displayed to the second user based on the determination. The method may also include classifying the second user based on the second user's interactions and characteristics.
In some implementations, the received user interaction data relates to two or more different topics, and each of the two or more different topics is assigned a different weight based on the importance of the corresponding topic to the user as determined from the user interest score for the topic.
The method may also include receiving a user interaction dataset indicative of interactions of a group of multiple different users with the one or more UI elements, generating a user group training dataset from the received user interaction dataset, inputting the user group training dataset to the machine learning model, and training, by the computing device, the machine learning model in response to the user group training dataset.
In some embodiments, the first user and the second user are the same user. In some embodiments, the first user and the second user are different users.
Other embodiments of this aspect include corresponding systems, apparatus, and computer programs configured to perform the actions of the methods encoded on computer storage devices.
Particular embodiments of the subject matter described in this document can be implemented to realize one or more of the following advantages. In some environments, there has not been a previous office to provide a "warm start" to a new user, or to be immediately personalized to provide an interface for relevant content or UI elements. Users who are not familiar with a particular application or interface, especially applications or interfaces that are complex or provide multiple functions or content, may be deterred or frustrated by the UI or their experience and are less likely to continue using the application. For example, in some environments, a user may start to use a new application to analyze data, and because the application is too complex, provides too much content to sort for the user before finding content that is useful or interesting to the user, or is difficult to browse (navigator), the user may give up and return to using their existing application, or continue to use the application inefficiently. By customizing content to meet the needs of new users who are unfamiliar with the application or interface to make the user experience easier, faster, and more efficient, the system increases the likelihood that the user will return to using the application again and increases the value the user derives from the content presented to the user. Based on the user's interactions, the system may determine, select, and present content that is most relevant to the user, and in some cases, may avoid presenting content that is not of interest to the user or that is not useful for use. By avoiding the presentation of content that is not of interest to the user, effective personalization of content can reduce computer resources (e.g., processing power, memory, etc.) used within an analysis environment in which a user interface is adapted to display user-related data to the user without displaying user-unrelated data to the user. In other words, it requires less interaction (e.g., menu selections, network requests, etc.) for the user to be directed to the content they need, thereby providing a more efficient and effective user interface relative to conventional systems and user interfaces. Furthermore, providing data that will not be used by the user may be avoided and also data transmission, e.g. from a remote server, may be avoided, thereby reducing unnecessary bandwidth utilization.
As the amount/type of data collected and the number of different reports available grows, the ability to reduce the number of interactions to reach the desired data set and customized user interface (and in particular data analysis user interface) becomes increasingly important, as this growth in data and available reports makes it difficult for users to reach the data they are looking for. Furthermore, users in one particular role may only be interested in a subset of the data and/or reports, while users in a different role may only be interested in a different subset of the data, such that a standard user interface will not be useful to all user groups. The techniques described in this document enable automatic customization of a user interface presented to each user such that information relevant to the user is presented in the user interface. Such customization may be performed by utilizing a machine learning model based on user interaction with various user interface elements.
The system may generate training data for a user interest model by mining user data, which learns user interests based on previous user interaction data with content or UI elements, automatically organize content and modify the presentation of the content (e.g., by visual or auditory effects such as physical placement on a page, visual highlighting, visual effects, sound effects, etc.) to adjust the content to a particular user. The previous user interaction data may be indicative of the particular user's own previous interactions and used to train a user interest model specific to the particular user. In some implementations, the previous user interaction data may indicate previous interactions by other users and be used to train a user interest model specific to a particular user — the other users may have a threshold similarity to the particular user over several user characteristics, and the previous user interaction data may include the particular user. In some implementations, the previous user interaction data may indicate previous interactions by other users and be used to train a user interest model for users having particular characteristics. Thus, the user interface may be an adaptive user interface that adapts to the user, e.g., based on attributes of the user or based on input received from a user associated with the user interface.
Moreover, utilizing this method of selecting content for a user allows new users to experience similar utility and comfort when interacting with a particular application or environment as users already familiar with the particular application or environment. The method allows for optimizing the content presented to the user and for providing content to the user based on the user's daily interactions. Using this system, users do not have to do anything different from what they have done to receive more relevant content-this approach uses less processing resources, as users do not have to separately provide feedback on what users think is most relevant. In some applications or environments, the user does not have any customization options. Thus, the described techniques provide users with more relevant content in an easy-to-use format while requiring less input and less computing resources than currently available approaches.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example environment in which user interaction with digital components distributed for presentation with electronic documents is used to generate training data for a user interest model that dynamically customizes a user interface that presents analysis of the interaction data.
FIG. 2 is an example data flow for customizing content in an application based on previous user interaction data.
FIG. 3 is an example application user interface that may be customized based on previous user interaction data.
FIG. 4 is a flow diagram of an example process for determining an opportunity to submit a user to send a digital component.
FIG. 5 is a block diagram of an example computing system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This document describes methods, systems, and devices that improve content relevance and ease of use and browsing for users of a particular application or environment, and allow content providers to provide more useful data to users. In some cases, when the digital component is presented to a user, the user interacts with the digital component. For example, the user may interact, for example, by selecting a digital component, by scrolling through (scroll past) the digital component, by closing the digital component, by selecting to view more content related to the digital component, and so forth. Some user interactions with digital components may be classified as positive (positive) interactions. A positive interaction is an indication that a particular user (or a user sharing a threshold amount of characteristics with a particular user) is interested in a digital component or content similar to a digital component. Some user interactions may be classified as negative (negative) interactions. A user's interaction with a digital component classified as a negative interaction indicates that the particular user or a user sharing a threshold amount of characteristics with the particular user is not interested in the digital component or content similar to the digital component. Based on the collected user interaction data, the system may generate training data for a user interest model that determines whether a particular user, or users with similar characteristics, may be interested in a particular type or presentation of content. The system may perform this personalization process in two steps: feature extraction, in which user interaction data is processed to determine relevant features within the user interaction data; and user interest scoring (rating) or prediction. The system then uses the user interest scores to personalize the UI elements by presenting the UI elements that are most relevant to the particular user (e.g., from among a large number of possible UI elements that may be presented). The system provides a similarly optimized experience for each user in different user groups, even if some users have less interaction or experience with the system.
The server may perform feature extraction by examining interaction data generated when the user interacts with the "digital component". The digital components may be presented to the user, for example, through a user device. In this example, the distribution system receives a request for digital components to be presented to a particular user through a particular user device associated with the user. When a request is received, the distribution system communicates with the server to determine the digital components that are most likely of interest to the user. The server receives user interaction data and generates training examples provided to the machine learning model by extracting features from the user interaction data to generate a training data set from the received user interaction data. The training data may contain, for example, training examples that indicate a set of conditions and whether those conditions result in a positive or negative result. The server then trains the user interest model by, for example, inputting the generated training data set to the machine learning model.
Multiple levels of personalization may occur. User interaction data may include both user identifiers and profile identifiers-the UI and content personalization of the system may be applied on a user-specific basis, or may be generalized (generalized) to the profile level across characteristics shared by multiple users.
The system uses the captured user interactions with the UI elements to infer the user's interest in a particular feature. As discussed in more detail below, the features may include, for example, metrics, dimensions, filters, reports, and the like.
Each of the user's interactions with a particular feature or feature set may be used to generate a training data set for the user interest model. For example, if a user often generates reports with default metrics and selects a particular dimension of data, the system may determine that the user is interested in the particular dimension of data, and thus the user interactions that generate reports with the particular dimension of data may be a positive example of the user's interest in the particular dimension of data. For simplicity, throughout this document, the dimensions of data are referred to as dimensions.
In some embodiments, the system obtains data from the environment. A web page or application has attributes that may indicate that a user is interested in certain metrics, dimensions, filters, and the like. For example, a user generating a report for an application shipping groceries within the state of wisconsin may be interested in a metric for the state of wisconsin and indicating the number of new users that have begun using the application. These features of the application may be extracted and input as training features to inform personalization of future user experiences. The system may extract features from data such as the URL of a web page, a persistent identifier interaction log, and the like.
Note that the techniques described in this document may also be implemented in situations where a user is browsing available applications (e.g., in an online analysis application or a web browser), or in other environments (e.g., on a publisher's web page). For the sake of brevity, much of the description below will refer to a data analysis environment.
As used throughout this document, the phrase "digital component" refers to a discrete unit of digital content or digital information (e.g., a video clip, an audio clip, a multimedia clip, an image, text, or another unit of content). The digital components may be electronically stored in the physical memory device as a single file or in a collection of files, and the digital components may be in the form of video files, audio files, multimedia files, image files, or text files, and include advertising information such that the advertisement is one type of digital component. Typically, the digital components are defined (or provided) by a single provider or source (e.g., advertiser, publisher, or other content provider), but may be a combination of content from multiple sources. Digital components from multiple different sources may be combined into a single electronic document (e.g., a collection of various different digital components), and portions of the various digital components from the different sources may be combined with information extracted from the search results (or other portions of the content) into a single digital component.
FIG. 1 is a block diagram of an example environment 100 in which digital components are distributed for presentation with an electronic document. User interactions with these digital components are collected and used to generate training data. The generated data is used to train a user interest model that dynamically modifies the analysis user interface. The example environment 100 includes a network 102, such as a Local Area Network (LAN), Wide Area Network (WAN), the internet, or a combination thereof. The network 102 connects an electronic document server 120, user devices 104, third party devices 106, and a Digital Component Distribution System (DCDS) 112 (also referred to as a distribution system). The example environment 100 may include many different electronic document servers 120, different types of user devices 104, and third party devices 106.
The user device 104 or the third party device 106 is an electronic device capable of requesting and receiving resources over the network 102. Example user devices 104 or third party devices 106 include personal computers, mobile communication devices, and other devices capable of sending and receiving data over the network 102. The user device 104 or third party device 106 typically includes a user application, such as a web browser, to facilitate sending and receiving data over the network 102, although a native application executed by the user device 104 or third party device 106 may also facilitate sending and receiving data over the network 102.
An electronic document is data that presents a set of content on a user device 104 or a third party device 106. Examples of electronic documents include analysis reports, web pages, word processing documents, Portable Document Format (PDF) documents, images, videos, search results pages, and feeds (feed sources). Native applications (e.g., "apps"), such as applications installed on mobile, tablet, or desktop computing devices, are also examples of electronic documents. Electronic documents 121 ("Electronic Docs") may be provided by Electronic document server 120 ("Electronic Doc Servers") to user devices 104 or third party devices 106. For example, electronic document server 120 may include a server hosting (host) a publisher's website. In this example, the user device 104 or the third party device 106 may initiate a request for a resource, such as a given publisher web page, and the electronic document server 120 hosting the given publisher web page may respond to the request by sending machine-executable instructions that initiate presentation of the given web page at the user device 104 or the third party device 106.
In another example, the electronic document server 120 may include an application server from which the user device 104 or the third party device 106 may download applications. In this example, the user device 104 or the third party device 106 may request resources, such as files needed to install the application, download the files, and then execute the downloaded application locally.
In some cases, a given electronic document 121 may include one or more digital component tags or digital component scripts that reference the DCDS 112. In these cases, the digital component tag or digital component script is executed by the user device 104 when a given electronic document 121 is processed by the user device 104 or the third party device 106. Execution of the digital component tag or digital component script configures the user device 104 to generate a request 114 (referred to as a "component request") for a resource that includes one or more digital components, which request 114 is transmitted to the DCDS 112 over the network 102. For example, a digital component tag or digital component script may enable the user device 104 or third party device 106 to generate a packetized data request including a header and payload data. The component request 114 may include event data specifying characteristics such as the name (or network location) of the server from which the digital component was requested, the name (or network location) of the requesting device (e.g., the user device 104 or the third party device 106), and/or information that the DCDS 112 may use to select one or more digital components to provide in response to the request. The component request 114 is transmitted by the user device 104 or the third party device 106 to a server of the DCDS 112 over the network 102 (e.g., a telecommunications network).
The component request 114 may include event data specifying other event characteristics such as characteristics of the requested electronic document and the location of the electronic document where the digital component may be presented. For example, the DCDS 112 may be provided with event data that specifies a reference (e.g., a URL) to an electronic document (e.g., a web page) in which the digital component is to be presented, available locations of the electronic document that are available for presenting the digital component, a size of the available locations, and/or media types that are eligible for presentation in the locations. Similarly, event data specifying keywords associated with the electronic document ("document keywords") or entities (e.g., people, places, or things) referenced by the electronic document may also be included in the component request 114 (e.g., as payload data) and provided to the DCDS 112 to facilitate identification of digital components that are eligible for presentation with the electronic document. The event data may also include a search query submitted from the user device 104 or the third party device 106 to retrieve a search results page, and/or data specifying search results and/or text, auditory, or other visual content included in the search results.
The component request 114 may also include event data related to other information, such as information that a user of the client device has provided, geographic information indicating a status or area from which the component request was submitted, or other information that provides context for the environment in which the digital component is to be displayed (e.g., time of day of the component request, day of the week of the component request, type of device (such as a mobile device or tablet device) in which the digital component is to be displayed). The component request 114 may be transmitted, for example, over a packetized network, and the component request 114 itself may be formatted as packetized data with a header and payload data. The header may specify the destination of the packet and the payload data may include any of the information discussed above.
The DCDS 112 selects a digital component to be presented with a given electronic document in response to receiving the component request 114 and/or using information included in the component request 114. In some implementations, the digital components are selected (using the techniques described herein) in less than one second to avoid errors that may result from delayed selection of the digital components. For example, delays in providing digital components in response to the component request 114 may result in page loading errors at the user device 104, or result in portions of the electronic document remaining unfilled even after portions of the electronic document are presented at the user device 104 or the third party device 106. Furthermore, as the delay in providing digital components to the user device 104 or third party device 106 increases, the electronic document is more likely to no longer be presented at the user device 104 when the digital components are delivered to the user device 104 or third party device 106, thereby negatively impacting the user's experience with the electronic document. Further, for example, if the digital component is provided while the electronic document 121 is no longer being presented at the user device 104 or the third party device 106, the delay in providing the digital component may cause the delivery of the digital component to fail. This delay is reduced when the digital components are not provided with the content presented to the user. The user can obtain the content faster without digital components and with less data transferred and no delay between retrieving the component's database and providing the component's server, improving the user's experience.
The feature extractor 109 analyzes the user interaction data to extract features of the interaction, including user characteristics and electronic document characteristics. User interaction data may be included, for example, in request 112 for electronic document 121. In some implementations, when a user interacts with an electronic document served through the user device 104 or the third party device 106, the interaction generates separate data indicating the characteristics of the interaction. User interactions may include, for example, clicking on a UI element within electronic document 121, viewing a particular portion of electronic document 121, clicking to view more data related to a particular subject matter or portion of electronic document 121, and so forth.
The feature extractor 109 collects user interaction data by, for example, accessing stored user interaction data or intercepting communications between the user device 104 or third party device 106 and the electronic document server 120. In some implementations, the user interaction data can be transmitted directly from the user device 104 or the third party device 106 to the feature extractor 109. The user interaction data may be provided in various formats and, depending on the format, may indicate different characteristics. The user interaction data in the form of an access request URL may indicate whether the user has entered terms, narrowed down his search using filters, selected to view data with a particular dimensional value, and so on. For example, the access request URL "http: com/data analytics report/report/visitors-demographics generator/drill ═ analytics generator: who, analytics. age: com/dataanytics report and have gone down (hill down) two dimensions (gender and age) to see analysis for women 25-34 years old. User interaction data may occur in other forms, such as data stored in association with a persistent identifier (e.g., a cookie), event data (e.g., application listener tracking events such as mouse clicks, scrolling, zooming, touch gestures, etc.), or other types of interaction data that may be collected from devices such as the user device 104 or third party device 106, the electronic document server 120, the DCDS 112, or collected through a communication interface such as the network 102.
Once the user interaction data is collected and the features extracted, the system analyzes the extracted features to predict the user's interest in additional content, information, or analysis reports. To analyze the extracted features, the system generates training data for input to the model. For example, the system may generate training examples for a neural network. The trained neural network may then predict the user's interest in various features and provide recommendation data to the DCDS 112. The recommendation data may indicate features of interest to a particular user or a set of users of a particular profile that satisfy the user's characteristics. The trained neural network may also provide recommendation data in the form of user interface generated data. For example, the user interface generation data may be instructions that, when executed, present content to a user in a particular format. For example, if a user routinely selects a graphical view of an analysis report rather than a tabular view, the system may determine from the user interaction data of this user that this user-and other users of a particular profile that satisfy user characteristics that match the particular profile of user characteristics of this user-is interested in viewing the analysis report in the graphical view. Similarly, if the user historically requested that a particular analysis report be presented after presenting a different analysis report, the system may modify the user interface to present a link to the particular analysis report at the time the different analysis report is presented, even if the link to the particular analysis report was not predefined in the presentation of the different analysis report, and even if the particular analysis report is in an un-launched (un-launched) state.
The training data generator 108 receives the extracted features of the user interaction data and generates one or more training data sets for the machine learning model. The training data generator 108 may use the user interactions and the extracted features to generate training examples. For example, the training data generator 108 may generate negative training examples for neural networks based on user interactions, where corporate users cancel suggested queries that include a metric indicative of the number of new users in Michigan, e.g., by selecting a UI element that causes the suggested queries to be cancelled (dismisss). The generated training examples may additionally include data indicative of one or more attributes of the user associated with the user interaction.
A User interest model 110 (UIM 110) that receives training data from the training data generator 108 trains the machine learning model and predicts User interests. UIM110 may be, for example, a neural network that receives training examples and user or profile information as inputs and outputs predictions of content that may be of interest to the user (or to the user that fits the profile information). UIM110 may be applicable to a particular user for which user-specific data is received. For example, UIM110 may be used to predict whether electronic SalesManager 12 is interested in a metric such as the number of new users within a few dimensional values (female, 18-24 years old, Spain). If the confidence level in the personalized recommendation generated by UIM110 is at least the confidence threshold level, UIM110 may provide the recommendation to DCDS 112 or user device 104 to generate a personalized UI or to present personalized content, such as UI elements predicted to be of most interest to a particular user.
UIM110 may also apply to a particular user for which only profile level data is available. For example, a new user may not interact with a particular application or environment to a level sufficient for the system to generate personalized content or a content presentation format. The system may then provide profile level personalized recommendations for the content or content presentation format. For example, if a new (or rare) user is the sales operations director of family and garden magazines located in new york, the system may determine a profile for that user that indicates that she is the operations director, that her department is the sales department, that her company is located in new york, and that she is interested in metrics and dimensions related to sales growth.
In this particular example, the system does not collect enough user-specific data for the new user through the user device 104. UIM110 may receive user interaction data from third party devices 106 of other users, UIM110 may analyze the user interaction data to match at least a threshold amount of the new user's profile. For example, the system may analyze user interaction data that matches at least three of the five profile characteristics of the new user. In some implementations, particular profile characteristics must match (e.g., users must have the same title, users must be in the same state, etc.). Once the system determines and selects user interaction data from third party device 106 that matches a threshold amount of the new user's profile, UIM110 may train the machine learning model using training examples generated by training data generator 108 from the selected user interaction data. Trained UIM110 may then be used to predict or select content or content presentation formats that may be of interest to the new user at the profile level. This enables a customized user interface to be generated for a user even if little or no historical interaction data is available for the user. In this manner, a user may be directed to data of interest more quickly, for example, by modifying the user interface to incorporate UI elements that initiate reports of data of interest when those reports are in an un-initiated state, and without requiring the user to navigate through a large number of user interfaces or menus to initiate a report of data of interest.
In some implementations, if the confidence level in the personalized content or content presentation format recommendation for the user level or profile level does not meet the confidence threshold level, then the system does not provide the personalized recommendation.
For the cases where the systems discussed herein collect personal information about a user or perhaps utilize personal information, the user may be provided with an opportunity to control whether programs or features collect personal information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current location), or to control whether and/or how content is received from a content server that may be more relevant to the user. In addition, certain data may be anonymized in one or more ways before it is stored or used, such that personally identifiable information is removed. For example, the identity of the user may be anonymized so that personally identifiable information cannot be determined for the user, or the geographic location of the user may be summarized where the location information is obtained (e.g., at the city, ZIP code, or state level) so that the particular location of the user cannot be determined. Thus, the user may control how the server collects and uses information about him or her.
FIG. 2 is an example data flow 200 for implementing the personalization framework in the example environment of FIG. 1. The operations of data flow 200 are performed by a UIM or server 110 in communication with user device 104 or third party device 106. In some embodiments, the stages of flow 200 are performed within a network environment, such as environment 100.
The system collects user interaction data and extracts features of those user interactions. The user interaction data may be stored in the user interaction database 206. Such user interaction data may include user profile data that may be stored in user profile 208. Once the system has collected the characteristics of the user interaction, the system generates a training data set to train a user interest model that predicts the content or content presentation format that a particular user may be interested in. The training data is provided to a machine learning model, and the model provides an output vector that predicts values of content that may be of interest to the user.
In this particular example, the communication performed as a phase of flow 200 occurs over a network 102, which network 102 may be the internet.
The user device 104 includes a display that presents content to a user of the user device 104 within the UI. The viewable portion of the content is within a viewport (viewport)202 of the display. In some implementations, the viewport 202 is a visual portion of a particular application or web page being accessed by a user of the user device 104. For example, if the digital component is presented within an application that does not cover the entire viewable area of the display of the user device 104, the viewport 202 is determined to be the viewable portion of the application. In some applications, there may be more content than can be displayed within the viewport 202 at one time. Such content may be on the same application portion as the content within the viewport 202 of the user device 104, but is effectively invisible to the user, and within the invisible portion 203 of the user device 104. For example, content 204a and content 204b are visible within viewport 202, but content 204c is within portion 203 that is not visible to a user of user device 104. Content 204 (i.e., content 204a, content 204b, and content 204c) may be various types of digital components as described above with reference to fig. 1. Analysis applications support a large number of options for presenting data, and not all reports, statistics, and metrics of available types can be presented in the visual portion of the analysis UI (analytical UI) at once. UIM110 collects user interaction data to determine and predict user information that is most likely to be of interest to a particular user. UIM110 then modifies the analysis UI to present links to relevant reports, UI elements displaying particular metrics, etc. to provide the most relevant information to the particular user when the available options cannot be displayed due to the limitations of the display and may be overwhelmed.
The user of the user device 104 interacts with the content 204 displayed by the user device 104 and generates and collects user interaction data indicative of these interaction characteristics. Details of the user interaction will be described in detail below and with reference to fig. 3.
User interaction data may be collected in various forms, and all of this user interaction data may be parsed and processed by the feature extractor 109. Entities that can have a user interest score (score) include metrics, dimensions, dimension filters, reports, audience/segment markets (segments), goals, and the like. The report may include analysis data about a particular data set, and the particular data set may be aggregated or analyzed. The target audience or segment market indicates the type of users that are interested in viewing the analysis. The user's goals may indicate a target metric that the user wishes to see when monitoring the analysis.
Metrics measure performance, behavior, and/or activity. For example, the performance metrics for a particular application may include newUsers, a metric that indicates the number of new users in a particular time period. Other metrics may indicate, for example, the percentage of sessions for the newly created session, the number of unique page views, the average amount of time spent on the web page, the number of unique social interactions occurring on the web page, the average server connection time, the average page load time, and the like. Different environments may have different metrics-an application may have a metric that indicates an amount of time spent on a particular screen of the application, a site may have a metric that tracks events, a search engine may have a metric that indicates a number of clicks a particular result obtained within a predetermined period of time, and other environments may have various other metrics including user-defined customized metrics.
Customized or user-defined user metrics may be weighted more than features that have no default settings or are directly selected by the user. For example, if a user creates a customized metric to be included in a report or report template, the system may apply a higher weight to the customized metric within the training examples provided to the user interest model.
The dimensions represent particular categories or characteristics of user interaction data. For example, the dimensions of the user interaction data set may include the type of device on which the user interaction is performed, the country in which the user interaction is performed, the type of interaction performed, and so forth. Different environments may have different dimensions available, and users may define custom dimensions to focus their analysis on or classify data into dimensions of their interest.
A filter may be a particular feature according to which data is classified. For example, a filter may be used to show data only within the last 30 days, or all data from georgia, nebraska, and new york, or only data from a particular profile of user characteristics. Different environments may have different filters, and users may define custom filters based on their interest in particular data being analyzed.
In some examples, the user device 104 is a mobile device, such as a cellular telephone, a virtual reality device (e.g., implemented in a headset or other device such as a combination of speakers and a display), a smartphone, a personal digital assistant (e.g., implemented in a desktop speaker or other device such as a combination of speakers and a display), or a tablet computer, and communicates over a wireless network. Training data generator 108 and UIM110 may communicate over a wired network that is independent of the wireless network over which user device 104 communicates. For example, user equipment 104 may be a smartphone communicating over a wireless cellular network, and training data generator 108 and UIM110 may be remote servers communicating over a wired network.
UIM110 receives user interaction data from user device 104 via network 102 and predicts a user's interest in digital components. UIM110 then utilizes the user interest score to modify the UI elements for presentation at user device 104. For example, UIM110 may determine that user device 104 has interacted with a digital component provided by a digital component provider through a digital component server that indicates a positive interest in reporting on the number of users making more than one purchase at a clothing store per day. UIM110 identifies a unique identifier associated with the interaction data. The unique identifier may be, for example, a user or device identifier. In some implementations, the unique identifier can be a user identifier carried across platforms and devices. For example, the unique identifier may be a user account used across browsers and devices, and may be used regardless of whether the user is using a software application or a web application.
The unique identifier may be protected when sent to a third party. For example, the unique identifier may be encrypted when sent in a forwarded opportunity (forwarded opportunity) to the rendered content. In some examples, the opportunity to forward and the unique identifier may be transmitted over a Secure Sockets Layer (SSL). In some examples, the unique identifier associated with the user of the user device 104 may be reset by various methods, including a factory reset via the user device 104. The user may also choose not to send their unique identifier in any content request at all.
In some implementations, the environment in which the system operates is a web application, and the user interaction data can be extracted from a URL of the web application. For example, the URL may indicate a hit (hit) page count and the number of get page requests. Each hit has a page request URL that indicates, in addition to the user-selected portion of the page, the metric the user is selecting, the user has selected a UI element in depth down, etc., the name of the profile and/or user identifier or the network location from which the request is made.
The feature extractor 109 may determine features at a user-specific level and a profile level. Exemplary profile level fields include quality of service (QOS) level (e.g., whether the user has premium service), time at which the profile was created (e.g., 12 months, 20 days, 13: 20: 12 in 2013), network location of the requested page, name of the page (e.g., bird Cool-web site and store), time zone, or the most recent time to see the profile. Such profile data may be stored as, for example, user profile 208.
The user interaction data may be collected from a data log collected from the user interface or from user interaction events. In some examples, the front end of the application may create custom events to track these interactions.
The dimensions may be determined by explicit user interaction or by default settings. In some implementations, the user has explicitly selected the dimension, and these user explicit operations (e.g., selecting the dimension from a drop down menu, drilling down into the dimension, etc.) may be recorded. The dimensions may also be determined according to default settings, such as default dimensions selected when generating content such as a report. For example, if users in italy generate a weekly report on the number of active (at least one user interaction) users in 12-17 years old, the feature extractor 109 may determine a default dimension (e.g., italian country) as well as an explicit dimension (e.g., users in 12-17 years old, active users, etc.). In some implementations, the report may be too general and include too many dimensions to analyze for user interest. For example, in an overview report, a default dimension may not be generated, as that would include every dimension available to the system.
The feature extractor 109 may also determine features from, for example, snapshots of the content that a particular user is viewing when collecting data. If the user is viewing a particular portion of the report for a monthly session, the feature extractor 109 may record the configuration of the report and the relevant features when the user views the report.
In some analysis environments, content known as "insight" may be provided for display to a user. The insight provides analytical insight into user interaction data that may be of interest to the user. For example, a pop-up window (or card) within the reported monthly review tab (monthly review tab) may be presented as an insight indicating up to a year snapshot of the monthly progress of the metrics of most interest to the user. Metrics for insights can include whether insights were provided, how many times insights were provided, whether insights were viewed, when insights were viewed, whether insights were bookmarked for future reference, when insights were bookmarked, whether insights were discarded, when insights were discarded, how many times insights were shared, whether insights were marked as helpful, how many times insights were marked as helpful within a predetermined time period (e.g., 30 days), how many times insights were marked as not helpful within a predetermined time period, or any action available to interact with insights. These and other metrics may be applied to other applications or content depending on the context of the content. Each metric may have a default value. In some embodiments, the default value may be specified by a user.
The users may interact with each other or with the environment through, for example, a question and answer interface. The dimensions, metrics and filters used by the user in asking questions or selecting questions related to their interests are recorded. For example, if the user asks "how many new customers viewed the landing page 5 before making at least one purchase? The system may determine that the user is interested in metrics indicating first customer, customer who purchased at least once, and dimension landing page 5.
The user may also create custom alerts for the data. For example, a regional tacticist may be interested in monitoring user traffic levels at each of a plurality of regional product pages, and may create a customized alert when a user traffic level increases (positive or negative) by a threshold amount to determine when a region is popular. In another example, a user may be identified as a new user responsible for global promotion of a range of products, and a customized alert may be created when a threshold number of customers purchase products from the range at an aggregate level. UIM110 may use, for example, machine learning techniques to determine custom reports, metrics, dimensions, alerts, etc. that may be of interest to the user.
For example, the user interaction data may be stored by linking the user identifier and the profile identifier with the extracted features. The user identifier is linked to a particular user, while the profile identifier is linked to a profile of user characteristics that may be associated with multiple users having the same interests, having the same role in an organization, and so forth. In generating the training data, additional data, such as frequency of use for each dimension, metric, filter, etc., may be stored and analyzed to determine which metrics are most useful. In some embodiments, the system may perform such analysis to learn the best allocation of computing resources to provide the best service on its user basis.
User interaction data, user interest scores, frequency of use data, etc. may be stored in different formats for use depending on the environment and the needs of the user. For example, the system may store the user interaction data in a column-oriented format, such that the value of each field is stored separately, and the computational overhead is proportional to the number of fields actually read from the storage device. The system can only access the relevant fields and thus avoids reading the entire record to access, for example, a single field; thus, the system may use resources in this format more efficiently than other storage formats. Other formats that allow for different levels of efficiency for other environments include storing data in a spandex table, in a matrix factorization, etc. In some embodiments, the system stores different types of data in different formats depending on the most frequently used environment of the data set or data type.
Data may be purged at regular intervals, and in some cases, data is only stored for a predetermined period of time to maintain privacy for the user. The system may determine a minimum time period for which an accuracy level is maintained in predicting user interest, and may store user interest data for the determined minimum time period. In some implementations, the user may indicate a period of time that they will allow the system to store their data. For example, users interested in a more personalized experience may allow the system to store their data for a longer period of time. The system may store the user interest scores in the user's profile while purging the user interaction data to maintain the user's personalization level.
Once the feature extractor 109 has processed the user interaction data and extracted the relevant features, the training data generator 108 generates training examples. For example, training data generator 108 may format user interaction data features into pairs of inputs and expected outputs. These input-output pairs may be provided to UIM110 to train the model.
In this particular example, the training data generator 108 samples the user interaction data to generate training data that is input to the neural network 111 a. The training data generator 108 may select random samples to obtain negative examples to train, for example, a logistic regression model. The training data generator 108 may also obtain the most frequent samples of the unviewed metrics to maximize margin (margin). The training data generator 108 may obtain multiple samples of user interaction data to generate one or more training data sets.
When generating the training data set, training data generator 108 selects training features for use within the training data set from the user's historical interactions with the report. The training data generator 108 determines the training example weights in one of several ways. In some implementations, the training data generator 108 may weight each training example equally and sample only data points within a predetermined time period. In some implementations, the training data generator 108 may use a decay function to weight the training examples differently depending on their recency.
The system may divide training, validation, and generation test data into different percentages of modeling time and resources. The system may also cross-validate the output of the model.
UIM110 receives the training data and trains the machine learning model to select digital components and/or content presentation formats. The machine learning model may use any of a variety of techniques, such as decision trees, linear regression models, logistic regression models, neural networks, classifiers, support vector machines, inductive logic programming, model combinations (e.g., using techniques such as bagging, boosting, random forests, etc.), genetic algorithms, bayesian networks, etc., and may be trained using a variety of methods, such as deep learning, perceptrons, association rules, inductive logic, clustering, maximum entropy classification, learning classification, etc. In some examples, the machine learning model uses supervised learning. In some examples, the machine learning model uses unsupervised learning. The machine learning model may also use wide and deep (wide and deep) learning, long term short term memory modeling, boosting, matrix factorization, user embedding, or item embedding.
Extensive and deep learning trains extensive linear memory models for memory and deep neural networks for generalization across many features to combine the advantages of both models, particularly useful for general, large-scale regression and classification problems with sparse inputs. For example, breadth and depth learning is very useful for the problem of classification features with a large number of possible feature values.
For ease of explanation, the operation of the neural network 111a is described with respect to logistic regression, but it should be understood that the neural network 111a may perform any of a number of other techniques as described above.
The neural network 111a is used to predict the probability that a particular user/profile is interested in a given feature. For example, given a (user, profile) pair and a metric newUsers, the neural network 111a may be trained to predict a probability that the (user, profile) pair is interested in the given metric newUsers. In this example, the neural network 111a uses training labels that indicate (user, profile) whether or not a newUsers is interested in. For a single entity (e.g., a metric, dimension filter, audience/segment market, goal, etc.), UIM11 calculates a user interest score for the entity. When a particular digital component includes two or more entities, UIM110 may calculate a separate user interest score and confidence score for each entity and aggregate these scores to generate an overall interest and confidence score.
The output of the neural network 111a is a prediction of the probability of a particular user, either the user for whom previous user interaction data was collected or the user matching profile data to some extent. UIM110 may provide personalized digital component recommendations, personalized UI elements, and presentation formats, and/or personalized suggestions for user queries, such as search queries, form fields, and the like.
In some implementations, UIM110 may provide suggestions for digital components, such as particular metrics to be included in the report, dimensional values of interest to the user, particular dimensional filters to apply, and so on. UIM110 may determine how much content is visible within viewport 202 of the UI. In some implementations, UIM110 may assign a visual presentation location to a particular digital component according to a user interest score for the component if more content is presented in the UI than a visual asset (real asset) in viewport 202 and content must be presented in non-visible viewport 203.
In some implementations, UIM110 determines the presentation format of the digital component that is most likely of interest to the user. For example, if a user prefers to generate a report with listing information, she may sort the data by herself, UIM110 may know the user preferences listing information rather than graphical information, and may generate an appropriate user interface, a table, in which digital components that may be of interest to the user are presented. UIM110 generates UI elements using user interface generator 111 b. The user interface generator 111b generates instructions that, when executed, cause the selected digital component to be presented in a particular visual format that may be of interest to the user.
In some implementations, UIM110 may determine content that may be of interest to the user for the query. For example, when the user enters a search query of "how many repeat guests … …," UIM110 may determine that the user may be interested in the query "how many repeat guests were seen this month by chinese and japanese branches" because the user is an account manager of chinese and japanese accounts, UIM110 may provide suggestions to the user based on previously recorded user interactions, such as autofill suggestions for the query, form fields, drill-down paths, and so forth. In providing a suggested question, UIM110 evaluates whether a particular user is interested in the metrics and dimensions that the suggested question encompasses; suggested questions with higher overall interest scores are more likely to be shown within the visual area of the analysis UI.
UIM110 calculates an overall user interest score for the particular feature, which may be a time-decaying frequency score. This overall score may be used most frequently when considering whether to personalize the user experience around a particular metric/dimension. UIM110 also calculates a confidence score that indicates the accuracy of the personalization data for a particular user or user profile. In some implementations, the system may instead use profile level personalization if the user-specific personalization score does not meet the threshold score. Profile level personalization is less specific but still useful because a particular user may have similar interests to other users with the same profile, or a threshold amount of similarity to the profile. For example, in the case of a new user, if this is her first browsing analysis application and the system does not have enough user-specific user interaction data to generate a personalization where the confidence score meets the threshold score, UIM110 may provide a personalization report on the profile level if the profile level personalization has a confidence score that meets the threshold score.
The system may also conserve computing resources by determining not to personalize the data/UI for a particular user if there is insufficient information, but instead providing default information. For example, if both the user-specific personalized confidence score and the profile level personalized confidence score of the user do not satisfy the threshold confidence level. Instead of providing personalized data that may not be of interest to the user, the system may provide default information and learn from the user's interactions with the default data to determine what data to provide next/for similar users with similar profiles.
For example, for each user, potential digital components in the form of insights may be scored by UIM110 based on each metric and dimension in the component. UIM110 may then utilize the user interest scores to rank and score insights more accurately and then rank this higher or lower for the user. In some implementations, if there is a limit to the number of digital components that may be presented within viewable portion 202 of the UI, UIM110 may determine the visual presentation location of the digital components such that components that may be of interest to the user are presented within viewable portion 202 of the UI and components that may be less of interest to the user are presented toward non-viewable portion 203 of the UI. For example, if the insight into user profile jane1234 includes a metric rebound Rate (Bounce Rate) and dimension japan, UIM110 calculates scores for the metric and the dimension and determines a degree of interest by jane1234 for each of these entities to take the interest scores for the individual entities into account in the overall interest score for the insight.
Applications may have different user bases in terms of interest and skill level; to serve as much of the user base as possible, many applications are developed that are powerful and feature rich. As the application feature set expands, the complexity and variety of options becomes overwhelming for many users. For example, while a user may access an application and see hundreds of reports, market segments, switches, and actions, most of her interest lies in several metrics (e.g., the number of target users and publisher revenue within the market segment 5A), a certain set of dimensions (e.g., device categories and countries), and a certain value (e.g., mobile, china, and japan). Without information about the topics of interest to the user, the application cannot be optimized for her needs to provide a useful, simple UI.
By providing a user interest score for an entity in an analysis environment, the system allows personalization to be used in the scoring algorithm for scoring metrics of interest to the user that are higher than metrics of disinterest to the user. The user interest scores may be used to identify goals and intentions of the user (e.g., to obtain more active revenue, optimize product sales), and to sort the metrics by intention to present growing opportunities to the correct user. The user interest scores may be used to generate suggested queries or content for user searches — for example, suggestions for particular metrics/dimensions/filters that may be included in a query may be suggested based on the user's context.
The context includes the entity (e.g., report, data point, insight, etc.) the user is looking at or the questions the user has previously presented to the system. When suggesting entities (such as metrics, dimensions, or dimension filters) that are not within the user's current context, the system may select a metric, dimension, or dimension filter with a high user interest score based on the user's previous interactions. For example, if the user has previously requested "revenue growth last year", the follow-up suggestion may be "revenue growth in the United states last year"
By understanding the interests of users at multiple levels, the system can develop and tailor features for users and other users who share characteristics with those users. For example, applications of the system may include servicing corporate or enterprise customers having a particular role in an organization. By understanding the metrics, dimensions, and values of interest to the user (e.g., e-commerce revenue, specific activities, data collected from Alabama, etc.), the system can present the relevant digital components to the user in a manner that is easily understood by the user. Such an improved machine-learning efficient UI generation system reduces the computational resources for determining and selecting digital components and generating UI elements without wasting resources on content or formats that are less likely to be of interest to the user. Understanding the topics of interest to the user or the role of the user in the enterprise may be used as input or context for the user interest learning model. For example, if a user is at an email marketing location, the user may be interested in content delivered by email as well as e-commerce revenue. Understanding a user's business intent or goal may be used as an input to a user interest learning model. For example, if the user's intent is to receive revenue for at least 20 ten thousand dollars from mobile content (mobile content) in the next month, the system may determine that the user is interested in mobile devices and marketing revenue. UIM110 may then weight these metrics more heavily in its user interest scoring model.
FIG. 3 is an example UI 300 for a user to interact with a UI presenting digital components having particular features. The process shown in flow 200 collects data about interactions between a user and a digital component being presented and populates a user interaction database 206 to improve personalization and selection of digital components and digital component presentation formats by maintaining a record of user interactions, extracting features from the user interaction data, generating training examples from the extracted features, and training a user interest model using the training examples. In some implementations, a resource (e.g., a web page or application) can contain more content than is displayed at once in a viewport of a client device. For example, an analysis report may contain several large graphs and tables, and if the user does not scroll down, the user may not be able to view all of the content. If the content is provided outside of a viewport (e.g., non-viewable portion 203) of the UI, the user device 104 can avoid rendering the data, thereby saving processing and memory resources. Accordingly, UIM110 may reduce and/or optimize the use of computing resources by modifying analysis reports and UI elements to show the UI elements in the viewport of the UI that are most likely to be of interest to the user. Flow 200 relates to training data generator 108, UIM110, and user equipment 104.
Each of the user actions takes into account the user interest score. For example, when a user views a browser report, UIM110 is trained to take into account whether she shows the default primary dimensions in viewing the report, or whether she selects custom metrics and dimensions. Further, visual aspects of the report (e.g., showing which metrics, which metrics to scroll to, which metrics the user spends most time looking at) are taken into account in the training example.
The system may determine whether to use particular user interaction data and which features of the selected user interaction data set to use in generating training examples for the user interest model. In one example, when a user views a summary report, which may include too many metrics to be useful for the relevant training example, the system may record the user's view of the report and analyze only the portion of the report that is visible to the user and that the user spent a threshold amount of time viewing. In another example, when a user is viewing a real-time report, the report is dynamically updated and it is difficult to determine which values to record. The system may take a snapshot of the report at a particular time when the user views the report and extract features from the snapshot.
There are many default values for the entities in the report and analysis content presented to the user. For example, the overview report may include all available metrics and dimensions, and the monthly sales summary report may include default metrics related to sales and revenue, and there are metrics, dimensions, filters, etc. that may not be of interest to the user, but are included by default. Typically, all dimensions, metrics, filters, and other entities indicated in the user interaction data are recorded, even though it is simply viewing the overview report. Thus, it is advantageous to weight user operations and interactions, as these are explicit actions of the user's body parts, and these interactions can be interpreted more confidently as intentional expressions of interest.
For example, drill-down to see more specific information may be recorded as a user's interest in more specific information, as well as a measure of drill-down interest. Other actions that may be interpreted as intentional expressions by interested or uninteresting persons may include changing, removing or adding metrics, sorting by metrics, changing primary dimensions, changing, removing or adding secondary dimensions, changing, removing or adding filters, interacting with insights beyond scrolling through digital component presentations, viewing and creating custom reports, manually selecting and manipulating data within reports, tabs, UI elements, manually selecting (e.g., selecting a graph, selecting a high-level filter in a table, selecting a particular target), applying market segmentation — custom or default, performing high-level analysis.
Meaningful feature interactions by the user in the context of the event indications contribute to the system improving the analysis and the user's application experience. By using previously collected user interaction data, the system may recommend new features and personalize the user's reporting experience (e.g., by suggesting reports to view, providing a filtered view of existing reports, creating an audience, creating a session, adding new users in an administrator, etc.). When an action is associated with some data (e.g., a set of metrics/dimensions associated with the action), the system's records are kept with the intent of keeping links and the time and order in which the user took the action.
User interaction with UI elements presenting content may provide positive or negative training examples for machine learning models. For example, within the viewport 202, the content item 204a includes a UI element 302. The UI element 302 represents a UI element with which a user may cancel, reject, or generally provide negative feedback to content or UI elements. For example, if the user selects the UI element 302 to cancel the content item 204a, the user interaction data may indicate that the content item 204a is cancelled and the user is not interested in the content item 204 a. In this way, the system allows the user to provide feedback through her daily interaction with the application without additional interaction.
In some implementations, UIM110 may immediately modify the analysis UI based on the user interaction data. For example, once the user cancels a particular UI element containing state-level aggregated statistics instead of country-level aggregated statistics, UIM110 may determine that the user may not be interested in state-wide statistics and may not present those statistics within the viewport of the UI.
In some implementations, the UI 300 can present a feedback UI element through which the user can provide further details as to why she chose to cancel the content item 204 a. For example, the user may select or enter a free response from among predetermined options. If the user indicates that she is not interested in the content item 204a, because she is not interested in the manner in which the content item 204 is presented. By providing additional feedback, the user may receive better, more relevant, and more understandable content. UIM110 may analyze the feedback to determine, for example, whether the user is temporarily not interested in the analysis data but wants to see it again at a different time, whether the user is interested in the analysis data but wants to see it in a different format, whether the user has turned to a different role within her organization and has a different statistical reporting requirement, and so on.
Within the viewport 202, the content item 204b includes a UI element 304. The UI element 304 represents a UI element with which a user who wants to see a wider range of content related to the content item 204b can see additional content and provide a positive indication of interest in the content item 204 b. For example, if the user selects UI element 304 to see "other content within subsection B," the user interaction data may indicate that content item 204B is cancelled and that the user is interested in seeing different or more content related to content item 204B. The user may be viewing a UI element presenting user statistics for sub-portion B, the first county within the new jersey state, and selecting UI element 304 to view other content within the new jersey sub-portion, such as hadamard county. Such user interaction may generate user interaction data to indicate that the user is interested in content within the dimension "New Jersey State" but not in Pekinet county.
Within the viewport 202, the content item 204a includes a UI element 306. The UI element 306 represents a UI element with which a user who wants to narrow the content associated with the content item 204a can see additional content and provide a positive indication of interest in the content item 204 a. For example, if the user selects a UI element to drill down into and see more of the content of Category 3. In some implementations, the UI element 306 allows the user to select different categories among the categories in which the content item 204a is located. The user may be viewing a UI element presenting statistics of the user owning the vehicle and selecting UI element 306 to drill down to see the user owning the minivan. Such user interaction may generate user interaction data to indicate that the user is interested in the dimensions of the user owning the vehicle, and further interested in the dimensions of the minivan. UIM110 may utilize such user interaction data to predict that, for example, a user may be interested in a dimension of a user of a household having more than two people.
Within the hidden portion 203, the content item 204c is rendered, but may not be rendered (render). For example, if a threshold portion of the content item 204c is not visible from the user's current location within the UI 300, the system 100 may refrain from rendering the content item 204c to save computing resources. In some implementations, if the user scrolls down to view the content item 204c, the action of causing the scrolling of the content item 204c within the viewport 202 for at least the predetermined period of time can be considered a positive indication of interest in the content item 204 c. UIM110 may then generate a positive training example from the scrolling action. The user may see the top of content item 204c and determine that content item 204c is of his interest; such user interaction data may indicate to UIM110 that content item 204c is of interest to the user if the user scrolls such that content item 204c is within viewport 202 for at least 20 seconds. However, if the user scrolls so that the content item 204c is within the viewport 202, and it is determined that the content item 204c is not of interest, he may scroll away; user interaction data indicating that content item 204c is not within viewport 202 for at least 20 seconds may be interpreted by UIM110 as a negative training example.
Other user interactions, such as enlarging a portion of a page or a particular UI element, marking a content item or presentation format as helpful, spending at least a predetermined period of time viewing or interacting with a content item or UI element, querying for particular content, etc., may also be a positive representation of interest.
FIG. 4 is a flow diagram of an example process 400 for personalizing an analysis environment and user experience based on previously collected user interaction data. In some implementations, process 400 may be implemented by one or more servers. For example, process 400 may be implemented by training data generator 108 and UIM110 of fig. 1-2. In some implementations, the process 400 may be implemented as instructions stored on a non-transitory computer-readable medium, and when executed by one or more servers, the instructions may cause the one or more servers to perform the operations of the process 400.
The system receives user interaction data indicating interactions of a particular user with one or more UI elements of an initial UI of an analytics reporting application presented to the particular user (402). For example, the training data generator 108 may collect user interaction data, and the feature extractor 109 may extract features of the recorded user interactions. As described above, these interactions may include, for example, clicking on a UI element to indicate interest, canceling a UI element, viewing other UI elements related to a UI element, and so forth.
The system may build a database that tracks user interactions. The user interaction data may represent, for example, characteristics of content of interest to a particular user. The data is processed through a pipeline of data processing tasks, e.g., a system processes the data to extract features of the data, such as metrics, dimensions, filters, and so forth. For example, the database may track the frequency and recent times that a particular user has interacted with a particular metric, dimension filter, and so forth. In this particular example, the database may be user interaction database 206.
The system converts the received user interaction data into a training data set (404). Once the system has extracted features of the user interaction data, the system generates training examples to provide to the machine learning model to predict user interests. In this particular example, the feature extractor 109 provides the extracted features to the training data generator 108, which training data generator 108 uses the recorded user interaction data and the extracted features to generate training examples. The training examples may include pairs of inputs and expected outputs. The training examples may be influenced by user interaction data and used by UIM110 to identify analysis report items and UI elements that may be of interest to the user.
The system inputs a training data set to a machine learning model (406). In this particular example, training data generator 108 provides training examples to UIM110, and in particular to neural network 111 a. As described above, other types of machine learning models may be used.
In response to the training data set, the system trains a machine learning model (408). In this particular example, UIM110 trains neural network 111a using training examples from training data generator 108. In some implementations, UIM110 undergoes multiple iterations of training, validation, and feature extraction. For example, UIM110 may continue to train neural network 111a until a threshold confidence score for a particular user or a particular feature of a profile has been reached.
The system generates a set of user interest scores for a particular user using a trained machine learning model, wherein each of the user interest scores indicates the user's interest in accessing information corresponding to the UI element of the application (410). In this particular example, UIM110 generates a user interest score for each entity referenced by a UI element using neural network 111a to provide an overall interest score for the UI element.
The system may then determine that at least one of the set of user interest scores positively indicates the user's interest in accessing information corresponding to a particular UI element that is not included in the initial UI and has at least a threshold score (412). In this particular example, UIM110 may determine that at least one of the user interest scores indicates that the user is interested in content corresponding to the UI element (e.g., includes different content, or includes the same content in different presentation formats).
In response to determining that the user interest score in the particular UI element is at least the threshold score, the system dynamically modifies the initial UI, including incorporating the particular UI element into the initial UI, to obtain an updated UI that includes the particular UI element that is not included in the initial UI (414). In this particular example, if UIM110 determines that the user is interested in a particular content presentation format, user interface generator 111b may generate UI instructions that, when executed, cause the content to be presented in the particular content presentation format. If UIM110 determines that the user is interested in content that was not previously presented, UIM110 uses the user interest score to select the content that the user is most likely interested in. UIM110 may rank or select content and modify the analysis UI to present content within a viewport of the UI that is most likely of interest to the user, thereby improving the user experience and helping the user browse through a large number of options and UI elements available within the analysis UI.
The system presents the user with an updated UI that includes the particular UI element (416). In this particular example, the user interface generator 111b provides UI instructions to the user device 104 to cause presentation of particular UI elements. UIM110 modifies particular UI elements based on the generated user interest scores to present content that is most likely of interest to the user.
The system monitors the user for further interaction with the updated UI after presenting the updated UI to the user (418). In this particular example, training data generator 108, and more specifically feature extractor 109, continuously collects user interaction data and extracts features, records the user interaction data in user interaction database 206, and records the user profile data in user profile 208.
The system updates the machine learning model based on further user interaction (420). In this particular example, training data generator 108 generates updated training examples and provides the updated training examples to UIM 110. UIM110 may continuously retrain neural network 111a based on current user interaction data.
The system selects an updated set of UI elements to present in the UI based on the updated machine learning model (422). In this particular example, user interface generator 111b may continually update the UI instructions provided to user device 104, and neural network 111a may continually update the user interest scores that UIM110 uses to dynamically modify the user's environment and analysis experience based on recorded user interaction data.
FIG. 5 is a block diagram of an example computer system 500 that may be used to perform the operations described above. The system 500 includes a processor 510, a memory 520, a storage device 530, and an input/output device 540. Each of the components 510, 520, 530, and 540 may be interconnected, for example, using a system bus 550. Processor 510 is capable of processing instructions for execution within system 500. In one implementation, the processor 510 is a single-threaded processor. In another implementation, the processor 510 is a multi-threaded processor. The processor 510 is capable of processing instructions stored in the memory 520 or the storage device 530.
The storage device 530 is capable of providing mass storage for the system 500. In one implementation, the storage device 530 is a computer-readable medium. In various different embodiments, storage device 530 may comprise, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices (e.g., cloud storage devices) over a network, or some other mass storage device.
The input/output device 540 provides input/output operations for the system 500. In one embodiment, the input/output device 540 may include one or more network interface devices, such as an Ethernet card, a serial communication device, such as an RS-232 port, and/or a wireless interface device, such as an 802.11 card. In another embodiment, the input/output devices may include driver devices configured to receive input data and transmit output data to other input/output devices, such as a keyboard, a printer, and a display device 560. However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and so forth.
Although an example processing system has been described in fig. 5, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
An electronic document (for brevity, simply referred to as a document) does not necessarily correspond to a file. A document may be stored in a portion of a file that holds other documents, in a single file dedicated to the document in question, or in multiple coordinated files.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus. The computer storage media may be or be embodied in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be included in one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification may be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" includes all types of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or a plurality or combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field-programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer does not require such a device. Furthermore, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable memory device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) display screen, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on the user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), internetworks (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the client device (e.g., for displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., as a result of user interaction) may be received at the server from the client device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (20)
1. A method performed by a computing device, the method comprising:
receiving, by a computing device, user interaction data indicative of interactions by a first user with one or more User Interface (UI) elements of an initial UI of an analytics reporting application presented to the first user;
generating a training data set according to the received user interaction data;
inputting the training data set to a machine learning model;
training, by a computing device, the machine learning model in response to the training dataset;
generating a set of user interest scores for a second user using a trained machine learning model, wherein each of the user interest scores indicates the second user's interest in accessing information corresponding to a UI element of the analytics reporting application;
based on a threshold score associated with a particular UI element that is not included in the initial UI, determining that at least one of the set of user interest scores positively indicates a second user's interest in accessing information corresponding to the particular UI element;
in response to determining that at least one of the set of user interest scores positively indicates a second user's interest in accessing information corresponding to the particular UI element, dynamically modifying the initial UI, including incorporating the particular UI element into the initial UI to obtain an updated UI that includes particular UI elements that are not included in the initial UI; and
presenting an updated UI comprising the particular UI element to a second user;
after presenting the updated UI to a second user, monitoring for further user interaction with the updated UI;
updating the machine learning model based on further user interaction; and
an updated set of UI elements presented in the UI is selected based on the updated machine learning model.
2. The method of claim 1, wherein the machine learning model is a logistic regression model.
3. The method of claim 1, wherein the machine learning model performs depth and breadth learning.
4. The method of claim 1, wherein the machine learning model is a long term short term memory model.
5. The method of claim 1, further comprising determining that a score in a set of user interest scores associated with another UI element is less than a threshold score associated with the other UI element; wherein the other UI element is not displayed to the second user based on the determination.
6. The method of claim 1, further comprising classifying the second user based on the second user's interactions and characteristics.
7. The method of claim 1, wherein the received user interaction data relates to two or more different topics, and
wherein each of the two or more different topics is assigned a different weight based on the importance of the corresponding topic to the user as determined from the user interest score for the topic.
8. The method of claim 1, further comprising receiving a user interaction dataset indicating interactions with one or more UI elements by a set of multiple different users;
generating a user group training data set according to the received user interaction data set;
inputting the user group training dataset to the machine learning model; and
training, by a computing device, the machine learning model in response to the user group training dataset.
9. The method of claim 1, wherein the first user and the second user are the same user.
10. The method of claim 1, wherein the first user and the second user are different users.
11. A system, comprising:
at least one processor; and
a memory communicatively coupled to the at least one processor, the memory storing instructions that, when executed by the at least one processor, cause the at least one processor to perform operations comprising:
receiving, by a computing device, user interaction data indicative of interactions by a first user with one or more User Interface (UI) elements of an initial UI of an analytics reporting application presented to the first user;
generating a training data set according to the received user interaction data;
inputting the training data set to a machine learning model;
training, by a computing device, the machine learning model in response to the training dataset;
generating a set of user interest scores for a second user using a trained machine learning model, wherein each of the user interest scores indicates the second user's interest in accessing information corresponding to a UI element of the analytics reporting application;
based on a threshold score associated with a particular UI element that is not included in the initial UI, determining that at least one of the set of user interest scores positively indicates a second user's interest in accessing information corresponding to the particular UI element;
in response to determining that at least one of the set of user interest scores positively indicates a second user's interest in accessing information corresponding to a particular UI element, dynamically modifying the initial UI, including incorporating the particular UI element into the initial UI to obtain an updated UI including particular UI elements that are not included in the initial UI; and
presenting an updated UI comprising the particular UI element to a second user;
after presenting the updated UI to a second user, monitoring for further user interaction with the updated UI;
updating the machine learning model based on further user interaction; and
an updated set of UI elements presented in the UI is selected based on the updated machine learning model.
12. The system of claim 11, wherein the machine learning model is a logistic regression model.
13. The system of claim 11, wherein the machine learning model performs depth and breadth learning.
14. The system of claim 11, wherein the machine learning model is a long term short term memory model.
15. The system of claim 11, further comprising determining that a score in the set of user interest scores associated with another UI element is less than a threshold score associated with the other UI element; wherein the other UI element is not displayed to the second user based on the determination.
16. The system of claim 11, further comprising classifying the second user based on the second user's interactions and characteristics.
17. The system of claim 11, wherein the first user and the second user are the same user.
18. The system of claim 11, wherein the first user and the second user are different users.
19. A computer-readable storage device storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
receiving, by a computing device, user interaction data indicative of interactions by a first user with one or more User Interface (UI) elements of an initial UI of an analytics reporting application presented to the first user;
generating a training data set according to the received user interaction data;
inputting the training data set to a machine learning model;
training, by a computing device, the machine learning model in response to the training dataset;
generating a set of user interest scores for a second user using a trained machine learning model, wherein each of the user interest scores indicates the second user's interest in accessing information corresponding to a UI element of the analytics reporting application;
based on a threshold score associated with a particular UI element that is not included in the initial UI, determining that at least one of the set of user interest scores positively indicates a second user's interest in accessing information corresponding to the particular UI element;
in response to determining that at least one of the set of user interest scores positively indicates a second user's interest in accessing information corresponding to the particular UI element, dynamically modifying the initial UI, including incorporating the particular UI element into the initial UI to obtain an updated UI that includes particular UI elements that are not included in the initial UI; and
presenting an updated UI comprising the particular UI element to a second user;
after presenting the updated UI to a second user, monitoring for further user interaction with the updated UI;
updating the machine learning model based on further user interaction; and
an updated set of UI elements presented in the UI is selected based on the updated machine learning model.
20. The computer-readable storage device of claim 19, the operations further comprising:
receiving a user interaction dataset indicative of interactions of a group of multiple different users with one or more UI elements;
generating a user group training data set according to the received user interaction data set;
inputting the user group training dataset to the machine learning model; and
training, by a computing device, the machine learning model in response to the user group training dataset.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/245,920 | 2019-01-11 | ||
US16/245,920 US11669431B2 (en) | 2019-01-11 | 2019-01-11 | Analytics personalization framework |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110795624A true CN110795624A (en) | 2020-02-14 |
CN110795624B CN110795624B (en) | 2023-11-07 |
Family
ID=68138788
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201911011979.1A Active CN110795624B (en) | 2019-01-11 | 2019-10-23 | Analysis personalization framework |
Country Status (4)
Country | Link |
---|---|
US (2) | US11669431B2 (en) |
EP (1) | EP3861466A1 (en) |
CN (1) | CN110795624B (en) |
WO (1) | WO2020146014A1 (en) |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111736940A (en) * | 2020-06-24 | 2020-10-02 | 中国银行股份有限公司 | Business interface display method and device for intelligent counter |
CN112417271A (en) * | 2020-11-09 | 2021-02-26 | 杭州讯酷科技有限公司 | Intelligent construction method of system with field recommendation |
CN112507186A (en) * | 2020-11-27 | 2021-03-16 | 北京数立得科技有限公司 | Webpage element classification method |
CN113572889A (en) * | 2020-06-26 | 2021-10-29 | 谷歌有限责任公司 | Simplified user interface generation |
WO2021262316A1 (en) * | 2020-06-25 | 2021-12-30 | Google Llc | Pattern-based classification |
WO2022116422A1 (en) * | 2020-12-01 | 2022-06-09 | 平安科技（深圳）有限公司 | Product recommendation method and apparatus, and electronic device and computer-readable storage medium |
CN115191006A (en) * | 2020-02-28 | 2022-10-14 | 奇跃公司 | 3D model for displayed 2D elements |
EP4150546A4 (en) * | 2020-05-15 | 2023-10-18 | Telefonaktiebolaget LM ERICSSON (PUBL) | Method of filtering data traffic sent to a user device |
Families Citing this family (29)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10867338B2 (en) | 2019-01-22 | 2020-12-15 | Capital One Services, Llc | Offering automobile recommendations from generic features learned from natural language inputs |
US10489474B1 (en) | 2019-04-30 | 2019-11-26 | Capital One Services, Llc | Techniques to leverage machine learning for search engine optimization |
US10565639B1 (en) * | 2019-05-02 | 2020-02-18 | Capital One Services, Llc | Techniques to facilitate online commerce by leveraging user activity |
US20200364508A1 (en) * | 2019-05-14 | 2020-11-19 | Nvidia Corporation | Using decay parameters for inferencing with neural networks |
US11232110B2 (en) | 2019-08-23 | 2022-01-25 | Capital One Services, Llc | Natural language keyword tag extraction |
US11347756B2 (en) * | 2019-08-26 | 2022-05-31 | Microsoft Technology Licensing, Llc | Deep command search within and across applications |
US20210125068A1 (en) * | 2019-10-28 | 2021-04-29 | MakinaRocks Co., Ltd. | Method for training neural network |
US10796355B1 (en) | 2019-12-27 | 2020-10-06 | Capital One Services, Llc | Personalized car recommendations based on customer web traffic |
US11847106B2 (en) * | 2020-05-12 | 2023-12-19 | Hubspot, Inc. | Multi-service business platform system having entity resolution systems and methods |
US11900046B2 (en) | 2020-08-07 | 2024-02-13 | Microsoft Technology Licensing, Llc | Intelligent feature identification and presentation |
EP4007960A1 (en) * | 2020-10-14 | 2022-06-08 | Google LLC | Privacy preserving machine learning predictions |
WO2022093690A1 (en) * | 2020-10-27 | 2022-05-05 | Xgenesis Inc. | Methods and systems for automated personalization |
US11599831B2 (en) * | 2020-11-03 | 2023-03-07 | Kpn Innovations, Llc. | Method and system for generating an alimentary element prediction machine-learning model |
US20220138903A1 (en) * | 2020-11-04 | 2022-05-05 | Nvidia Corporation | Upsampling an image using one or more neural networks |
CN112527296A (en) * | 2020-12-21 | 2021-03-19 | Oppo广东移动通信有限公司 | User interface customizing method and device, electronic equipment and storage medium |
EP4330814A1 (en) * | 2021-04-26 | 2024-03-06 | Nostra, Inc. | Artificial intelligence-based personalized content creation workflow |
US20220353304A1 (en) * | 2021-04-30 | 2022-11-03 | Microsoft Technology Licensing, Llc | Intelligent Agent For Auto-Summoning to Meetings |
US11792143B1 (en) | 2021-06-21 | 2023-10-17 | Amazon Technologies, Inc. | Presenting relevant chat messages to listeners of media programs |
US11792467B1 (en) | 2021-06-22 | 2023-10-17 | Amazon Technologies, Inc. | Selecting media to complement group communication experiences |
US11687576B1 (en) | 2021-09-03 | 2023-06-27 | Amazon Technologies, Inc. | Summarizing content of live media programs |
US11785299B1 (en) | 2021-09-30 | 2023-10-10 | Amazon Technologies, Inc. | Selecting advertisements for media programs and establishing favorable conditions for advertisements |
US11785272B1 (en) | 2021-12-03 | 2023-10-10 | Amazon Technologies, Inc. | Selecting times or durations of advertisements during episodes of media programs |
US11916981B1 (en) | 2021-12-08 | 2024-02-27 | Amazon Technologies, Inc. | Evaluating listeners who request to join a media program |
US11791920B1 (en) | 2021-12-10 | 2023-10-17 | Amazon Technologies, Inc. | Recommending media to listeners based on patterns of activity |
US20230252549A1 (en) * | 2022-02-09 | 2023-08-10 | Maplebear Inc. (Dba Instacart) | Search Relevance Model Using Self-Adversarial Negative Sampling |
US11880546B2 (en) * | 2022-04-22 | 2024-01-23 | Capital One Services, Llc | Presentation and control of user interactions with a user interface element |
US20240061687A1 (en) * | 2022-08-17 | 2024-02-22 | Stripe, Inc. | Dynamic generation and customization of user interfaces |
CN115774816B (en) * | 2023-02-10 | 2023-04-11 | 成都萌想科技有限责任公司 | Content elimination method, system, device and storage medium based on user value |
CN117556192A (en) * | 2023-11-16 | 2024-02-13 | 南京小裂变网络科技有限公司 | User growth type operation system and method based on artificial intelligence |
Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1967533A (en) * | 2006-07-17 | 2007-05-23 | 北京航空航天大学 | Gateway personalized recommendation service method and system introduced yuan recommendation engine |
CN101339562A (en) * | 2008-08-15 | 2009-01-07 | 北京航空航天大学 | Portal personalized recommendation service system introducing into interest model feedback and update mechanism |
CN103988161A (en) * | 2011-12-09 | 2014-08-13 | 微软公司 | Adjusting user interface screen order and composition |
US20160239737A1 (en) * | 2015-02-13 | 2016-08-18 | Yahoo!, Inc. | Future event detection |
US20160360336A1 (en) * | 2015-05-27 | 2016-12-08 | Apple Inc. | Systems and Methods for Proactively Identifying and Surfacing Relevant Content on a Touch-Sensitive Device |
US20170032257A1 (en) * | 2015-07-29 | 2017-02-02 | Google Inc. | Modeling personal entities |
US20170155631A1 (en) * | 2015-12-01 | 2017-06-01 | Integem, Inc. | Methods and systems for personalized, interactive and intelligent searches |
US20180189367A1 (en) * | 2013-11-11 | 2018-07-05 | Amazon Technologies, Inc. | Data stream ingestion and persistence techniques |
CN108563755A (en) * | 2018-04-16 | 2018-09-21 | 辽宁工程技术大学 | A kind of personalized recommendation system and method based on bidirectional circulating neural network |
US20180285756A1 (en) * | 2017-03-28 | 2018-10-04 | International Business Machines Corporation | Cross-User Dashboard Behavior Analysis and Dashboard Recommendations |
US20180330248A1 (en) * | 2017-05-12 | 2018-11-15 | Adobe Systems Incorporated | Context-aware recommendation system for analysts |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2008191748A (en) * | 2007-02-01 | 2008-08-21 | Oki Electric Ind Co Ltd | Inter-user communication method, inter-user communication program and inter-user communication device |
US9405427B2 (en) | 2012-09-12 | 2016-08-02 | Facebook, Inc. | Adaptive user interface using machine learning model |
US20140236875A1 (en) | 2012-11-15 | 2014-08-21 | Purepredictive, Inc. | Machine learning for real-time adaptive website interaction |
US20140358916A1 (en) | 2013-05-29 | 2014-12-04 | Microsoft Corporation | Personalized prioritization of integrated search results |
US10795647B2 (en) * | 2017-10-16 | 2020-10-06 | Adobe, Inc. | Application digital content control using an embedded machine learning module |
US10579632B2 (en) * | 2017-12-18 | 2020-03-03 | Microsoft Technology Licensing, Llc | Personalized content authoring driven by recommendations |
-
2019
- 2019-01-11 US US16/245,920 patent/US11669431B2/en active Active
- 2019-09-19 WO PCT/US2019/051960 patent/WO2020146014A1/en unknown
- 2019-09-19 EP EP19783184.5A patent/EP3861466A1/en not_active Withdrawn
- 2019-10-23 CN CN201911011979.1A patent/CN110795624B/en active Active
-
2023
- 2023-04-21 US US18/305,025 patent/US20230385171A1/en active Pending
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1967533A (en) * | 2006-07-17 | 2007-05-23 | 北京航空航天大学 | Gateway personalized recommendation service method and system introduced yuan recommendation engine |
CN101339562A (en) * | 2008-08-15 | 2009-01-07 | 北京航空航天大学 | Portal personalized recommendation service system introducing into interest model feedback and update mechanism |
CN103988161A (en) * | 2011-12-09 | 2014-08-13 | 微软公司 | Adjusting user interface screen order and composition |
US20180189367A1 (en) * | 2013-11-11 | 2018-07-05 | Amazon Technologies, Inc. | Data stream ingestion and persistence techniques |
US20160239737A1 (en) * | 2015-02-13 | 2016-08-18 | Yahoo!, Inc. | Future event detection |
US20160360336A1 (en) * | 2015-05-27 | 2016-12-08 | Apple Inc. | Systems and Methods for Proactively Identifying and Surfacing Relevant Content on a Touch-Sensitive Device |
US20170032257A1 (en) * | 2015-07-29 | 2017-02-02 | Google Inc. | Modeling personal entities |
CN107851092A (en) * | 2015-07-29 | 2018-03-27 | 谷歌有限责任公司 | Personal entity models |
US20170155631A1 (en) * | 2015-12-01 | 2017-06-01 | Integem, Inc. | Methods and systems for personalized, interactive and intelligent searches |
CN108604237A (en) * | 2015-12-01 | 2018-09-28 | 英特吉姆公司股份有限公司 | personalized interactive intelligent search method and system |
US20180285756A1 (en) * | 2017-03-28 | 2018-10-04 | International Business Machines Corporation | Cross-User Dashboard Behavior Analysis and Dashboard Recommendations |
US20180330248A1 (en) * | 2017-05-12 | 2018-11-15 | Adobe Systems Incorporated | Context-aware recommendation system for analysts |
CN108563755A (en) * | 2018-04-16 | 2018-09-21 | 辽宁工程技术大学 | A kind of personalized recommendation system and method based on bidirectional circulating neural network |
Cited By (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN115191006A (en) * | 2020-02-28 | 2022-10-14 | 奇跃公司 | 3D model for displayed 2D elements |
US11824952B2 (en) | 2020-05-15 | 2023-11-21 | Telefonaktiebolaget Lm Ericsson (Publ) | Method of filtering data traffic sent to a user device |
EP4150546A4 (en) * | 2020-05-15 | 2023-10-18 | Telefonaktiebolaget LM ERICSSON (PUBL) | Method of filtering data traffic sent to a user device |
CN111736940A (en) * | 2020-06-24 | 2020-10-02 | 中国银行股份有限公司 | Business interface display method and device for intelligent counter |
CN115280314A (en) * | 2020-06-25 | 2022-11-01 | 谷歌有限责任公司 | Pattern-based classification |
WO2021262316A1 (en) * | 2020-06-25 | 2021-12-30 | Google Llc | Pattern-based classification |
US11704560B2 (en) | 2020-06-25 | 2023-07-18 | Google Llc | Pattern-based classification |
CN113572889A (en) * | 2020-06-26 | 2021-10-29 | 谷歌有限责任公司 | Simplified user interface generation |
CN113572889B (en) * | 2020-06-26 | 2024-03-12 | 谷歌有限责任公司 | Simplifying user interface generation |
CN112417271B (en) * | 2020-11-09 | 2023-09-01 | 杭州讯酷科技有限公司 | Intelligent system construction method with field recommendation |
CN112417271A (en) * | 2020-11-09 | 2021-02-26 | 杭州讯酷科技有限公司 | Intelligent construction method of system with field recommendation |
CN112507186A (en) * | 2020-11-27 | 2021-03-16 | 北京数立得科技有限公司 | Webpage element classification method |
WO2022116422A1 (en) * | 2020-12-01 | 2022-06-09 | 平安科技（深圳）有限公司 | Product recommendation method and apparatus, and electronic device and computer-readable storage medium |
Also Published As
Publication number | Publication date |
---|---|
EP3861466A1 (en) | 2021-08-11 |
US20200226418A1 (en) | 2020-07-16 |
US11669431B2 (en) | 2023-06-06 |
WO2020146014A1 (en) | 2020-07-16 |
US20230385171A1 (en) | 2023-11-30 |
CN110795624B (en) | 2023-11-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110795624B (en) | Analysis personalization framework | |
US10133812B2 (en) | System and method for finding and prioritizing content based on user specific interest profiles | |
US8762302B1 (en) | System and method for revealing correlations between data streams | |
RU2580516C2 (en) | Method of generating customised ranking model, method of generating ranking model, electronic device and server | |
US20140129331A1 (en) | System and method for predicting momentum of activities of a targeted audience for automatically optimizing placement of promotional items or content in a network environment | |
US8914382B2 (en) | System and method for generation of a dynamic social page | |
US10198744B2 (en) | User-targeted advertising | |
US11526905B1 (en) | Systems and methods for preserving privacy | |
US20190347287A1 (en) | Method for screening and injection of media content based on user preferences | |
US20120078725A1 (en) | Method and system for contextual advertisement recommendation across multiple devices of content delivery | |
US20120030018A1 (en) | Systems And Methods For Managing Electronic Content | |
US11551281B2 (en) | Recommendation engine based on optimized combination of recommendation algorithms | |
US11887156B2 (en) | Dynamically varying remarketing based on evolving user interests | |
EP2783337A1 (en) | Systems and methods for recommending advertisement placement based on cross network online activity analysis | |
US11915256B2 (en) | Customized merchant price ratings | |
US11237693B1 (en) | Provisioning serendipitous content recommendations in a targeted content zone | |
JP7223164B2 (en) | Data integrity optimization | |
US20230011804A1 (en) | Customized Merchant Price Ratings | |
EP4367623A1 (en) | Customized merchant price ratings | |
WO2023283116A1 (en) | Customized merchant price ratings | |
KR20160107101A (en) | Electronic device and Method for filtering content in an electronic device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |