CN117150079A - Language-based search of digital content in a network - Google Patents
Language-based search of digital content in a network Download PDFInfo
- Publication number
- CN117150079A CN117150079A CN202311062769.1A CN202311062769A CN117150079A CN 117150079 A CN117150079 A CN 117150079A CN 202311062769 A CN202311062769 A CN 202311062769A CN 117150079 A CN117150079 A CN 117150079A
- Authority
- CN
- China
- Prior art keywords
- digital component
- data processing
- processing system
- component
- digital
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000005236 sound signal Effects 0.000 claims abstract description 123
- 238000012545 processing Methods 0.000 claims description 160
- 238000000034 method Methods 0.000 claims description 95
- 230000015654 memory Effects 0.000 claims description 18
- 230000004044 response Effects 0.000 claims description 16
- 230000007704 transition Effects 0.000 claims description 12
- 238000003058 natural language processing Methods 0.000 description 43
- 230000008569 process Effects 0.000 description 13
- 238000004590 computer program Methods 0.000 description 11
- 230000009471 action Effects 0.000 description 10
- 238000004891 communication Methods 0.000 description 9
- 238000010586 diagram Methods 0.000 description 6
- 230000000007 visual effect Effects 0.000 description 6
- 230000005540 biological transmission Effects 0.000 description 5
- 238000010801 machine learning Methods 0.000 description 5
- 230000008439 repair process Effects 0.000 description 5
- 230000008859 change Effects 0.000 description 4
- 230000003993 interaction Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000009877 rendering Methods 0.000 description 4
- 230000000153 supplemental effect Effects 0.000 description 4
- 238000001914 filtration Methods 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 230000000670 limiting effect Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000013515 script Methods 0.000 description 3
- 238000006243 chemical reaction Methods 0.000 description 2
- 238000003384 imaging method Methods 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 230000033001 locomotion Effects 0.000 description 2
- 230000000877 morphologic effect Effects 0.000 description 2
- 238000007781 pre-processing Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- IRLPACMLTUPBCL-KQYNXXCUSA-N 5'-adenylyl sulfate Chemical compound C1=NC=2C(N)=NC=NC=2N1[C@@H]1O[C@H](COP(O)(=O)OS(O)(=O)=O)[C@@H](O)[C@H]1O IRLPACMLTUPBCL-KQYNXXCUSA-N 0.000 description 1
- 101100286571 Mus musculus Pyhin1 gene Proteins 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000011143 downstream manufacturing Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 239000004615 ingredient Substances 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000011045 prefiltration Methods 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7844—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using original textual content or text extracted from visual content or transcript of audio data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/74—Browsing; Visualisation therefor
- G06F16/745—Browsing; Visualisation therefor the internal structure of a single video sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/73—Querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/74—Browsing; Visualisation therefor
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/422—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS]
- H04N21/42203—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS] sound input device, e.g. microphone
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/472—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/472—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content
- H04N21/47217—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content for controlling playback functions for recorded or on-demand content, e.g. using progress bars, mode or play-point indicators or bookmarks
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/845—Structuring of content, e.g. decomposing content into time segments
- H04N21/8455—Structuring of content, e.g. decomposing content into time segments involving pointers to the content, e.g. pointers to the I-frames of the video stream
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
Abstract
The present application relates to language-based searching of digital content in a network. The solution may enable a user to interact with video and other content through a touch interface and through voice commands. In addition to inputs such as stop and play, the present solution may also automatically generate annotations for the displayed video file. From the annotations, the solution may identify one or more breakpoints associated with different scenes, video portions, or operational steps in the video. The digital assistant may receive an input audio signal and parse the input audio signal to identify semantic entities within the input audio signal. The digital assistant may map the identified semantic entities to annotations to select portions of the video corresponding to user requests in the input audio signal.
Description
Description of the division
The application belongs to a divisional application of Chinese application patent application No.201980005354.6 with the application date of 2019, 4 months and 8 days.
Cross Reference to Related Applications
The present application claims priority from U.S. non-provisional patent application No.15/973,447 entitled "MULTI-MODAL INTERFACE IN A VOICE-ACTIVATED NETWORK (MULTI-MODAL interface in VOICE activated network)" filed on 5/7 of 2018, the entire contents of which are incorporated herein by reference.
Background
The computing device may provide digital content to a networked client device. Excessive network transmissions may be required to find a particular location or content in the provided digital content. In addition, voice-based interfaces may not be able to move to a particular location within the digital content, which may result in wasted network resources when an entity of the digital content is sent to a network client device.
Disclosure of Invention
In accordance with at least one aspect of the present disclosure, a system for controlling digital components in a voice activated system may include a data processing system. The data processing system may include one or more processors and memory. The data processing system can execute a natural language processor ("NLP") component, an annotation component, and a parsing component. The natural language processor component may receive a first input audio signal detected by a sensor at the client computing device. The natural language processor component may parse the first input audio signal to identify a first digital component request in the first input audio signal. The annotation component can generate a first set of annotations for the first digital component based at least on speech recognized in the first digital component. The parsing component can identify a plurality of breakpoints based at least on the first set of annotations. The natural language processor component may receive a second input audio signal detected by a sensor at the client computing device. The natural language processor component may parse the second input audio signal to identify terms in the second input audio signal. The parsing component may select a breakpoint from a plurality of breakpoints based on the term. The parsing component may send a portion of the first digital component corresponding to the breakpoint.
The system may further include a content selection component for selecting a second digital component based on a breakpoint selected from the plurality of breakpoints and transmitting the second digital component with a portion of the first digital component corresponding to the breakpoint to the client computing device. The annotation component can generate a second set of annotations for the first digital component based on the image in the digital component, and the parsing component can identify a plurality of breakpoints based on the second set of annotations.
The annotation component can generate a second set of annotations for the first digital component based on closed caption data in the digital component. The annotation component can receive a second set of annotations of the first digital component from a second client computing device. The parsing component may identify a plurality of breakpoints based on the second set of annotations. The parsing component can identify a scene transition in the digital component and identify a plurality of break points based on the scene transition.
The natural language processor component may receive a third input audio signal detected by a sensor at the client computing device, parse the third input audio signal to identify an indication of a second breakpoint of the plurality of breakpoints, and the parsing component may send a second portion of the first digital component corresponding to the second breakpoint to the client computing device. The natural language processor component may parse the first input audio signal to identify a first semantic representation in the first input audio signal, and the parsing component may select a breakpoint from the plurality of breakpoints based on the first semantic meaning. The parsing component may generate a plurality of portions of the first digital component based on the plurality of break points and may determine a semantic representation of each of the plurality of portions of the first digital component.
The annotation component can generate a second set of annotations for the first digital component based on the semantic representation of each of the plurality of portions of the first digital component, and the parsing component can identify a plurality of breakpoints based on the second set of annotations. Each breakpoint of the plurality of breakpoints may correspond to a start of a different scene. The parsing component may generate a second digital component that includes an indication of each of the plurality of breakpoints and send the second digital component to the client computing device for presentation with the portion of the first digital component corresponding to the breakpoint. The natural language processor component may receive a third input audio signal from the client computing device that includes a selection of one of the plurality of breakpoints, and the parsing component may select a breakpoint from the plurality of breakpoints based on the selection of the one of the plurality of breakpoints.
In accordance with at least one aspect of the present disclosure, a method for controlling a digital component in a voice activated system may include: a first input audio signal detected by a sensor at a client computing device is received by a natural language processor component executed by a data processing system and via an interface of the data processing system. The method may include: the first input audio signal is parsed by the natural language processor component to identify a first digital component request in the first input audio signal. The method may include: a first set of annotations for the first digital component is generated by an annotation component executed by the data processing system based at least on speech recognized in the first digital component. The method may include: a plurality of breakpoints are identified by a parsing component executed by the data processing system based at least on the first set of annotations. The method may include: a second input audio signal detected by a sensor at the client computing device is received by the natural language processor component. The method may include: the second input audio signal is parsed by the natural language processor component to identify terms in the second input audio signal. The method may include: a break point is selected from a plurality of break points based on the term by a parsing component. The method may include: the portion of the first digital component corresponding to the breakpoint is sent by the parsing component to the client computing device.
The method may further comprise: the method further includes selecting, by a content selection component executed by the data processing system, a second digital component based on a breakpoint selected from the plurality of breakpoints, and transmitting, by the content selection component, the second digital component and a portion of the first digital component corresponding to the breakpoint to the client computing device. The method may further comprise: the method further includes generating, by the annotation component, a second set of annotations for the first digital component based on the image in the digital component, and identifying, by the parsing component, a plurality of breakpoints based on the second set of annotations.
The method may further comprise: a second set of annotations for the first digital component is generated by the annotation component based on the closed caption data in the digital component. The method may further comprise: the method further includes receiving, by the annotation component, a second set of annotations of the first digital component from the second client computing device, and identifying, by the parsing component, a plurality of breakpoints based on the second set of annotations. The method may further comprise: the method includes identifying, by a parsing component, a scene transition in a digital component, and identifying, by the parsing component, a plurality of break points based at least on the scene transition.
The method may further comprise: the method includes receiving, by a natural language processor component, a third input audio signal detected by a sensor at the client computing device, parsing, by the natural language processor component, the third input audio signal to identify an indication of a second breakpoint of the plurality of breakpoints, and transmitting, by the parsing component, a second portion of the first digital component to the client computing device that corresponds to the second breakpoint. The method may further comprise: the first input audio signal is parsed by the natural language processor component to identify a first semantic representation in the first input audio signal, and a breakpoint is selected from the plurality of breakpoints based on the first semantic meaning by the parsing component.
The method may further comprise: the plurality of portions of the first digital component are generated by the parsing component based on the plurality of breakpoints, and a semantic representation of each of the plurality of portions of the first digital component is determined by the parsing component. The method may further comprise: the method further includes generating, by the annotation component, a second set of annotations for the first digital component based on the semantic representation of each of the plurality of portions of the first digital component, and identifying, by the parsing component, a plurality of breakpoints based at least on the second set of annotations.
Each breakpoint of the plurality of breakpoints may correspond to a start of a different scene. The method may further comprise: generating, by the parsing component, a second digital component comprising an indication of each of the plurality of breakpoints; the second digital component is sent by the parsing component to the client computing device for presentation with a portion of the first digital component corresponding to the breakpoint. The method may further comprise: a third input audio signal is received by the natural language processor component from the client computing device, the third input audio signal including a selection of one of the plurality of breakpoints, and a breakpoint is selected by the parsing component from the plurality of breakpoints based on the selection of the one of the plurality of breakpoints.
These and other aspects and embodiments are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and embodiments, and provide an overview or framework for understanding the nature and character of the claimed aspects and embodiments. The accompanying drawings provide a description and a further understanding of various aspects and embodiments, and are incorporated in and constitute a part of this specification. Aspects and embodiments of the disclosed subject matter can be combined where appropriate.
Drawings
The drawings are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing. In the drawings:
fig. 1 illustrates an example system for controlling digital components in a voice activated system according to examples of this disclosure.
Fig. 2 illustrates a block diagram of an example representation of a digital component over time, according to an example of the present disclosure.
Fig. 3 illustrates a block diagram of an example method for controlling digital components in a voice activated system in accordance with an example of the present disclosure.
Fig. 4 illustrates a client computing device at a first point in time and during a second point in time during the method illustrated in fig. 3, according to an example of the present disclosure.
FIG. 5 illustrates a block diagram of an example computer system in accordance with examples of this disclosure.
Detailed Description
The following is a more detailed description of various concepts related to methods, apparatuses, and systems for transmitting data in a secure processing environment, and implementations thereof. The various concepts introduced above and discussed in more detail below may be implemented in any of a number of ways.
The present disclosure relates generally to controlling digital components in a voice activated system. The interface for presenting the video may include inputs mimicking an electromechanical playback device (e.g., VCR) and include buttons such as stop, play, forward, and rewind. Interaction with a voice activated system using these limited inputs can be difficult. In addition, the difficulty of interacting with video content makes it difficult for a user to select and view a portion of the video content. The difficulty of finding and viewing only the desired portion of the video content may result in computational and network wastage because the end user may frequently view or download the entire video content rather than just viewing or downloading the desired portion.
The system and method of the present technical solution enables a multimodal interface for voice-based devices such as digital assistants. The solution may enable a user to interact with video and other content through a touch interface and through voice commands. In addition to inputs such as stop and play, the solution may also automatically generate annotations for the displayed video file. From the annotations, the solution may identify one or more breakpoints associated with different scenes, video portions, or operational steps in the video. The digital assistant may receive an input audio signal and parse the input audio signal to identify semantic entities within the input audio signal. The digital assistant may map the identified semantic entities to annotations to select portions of the video corresponding to user requests in the input audio signal. The digital assistant may then jump to the selected portion of the video. Enabling a user of a voice-based digital assistant to search for specific content in a video can reduce computing and network resources by enabling the user to skip to a requested portion of the video, enabling only a portion of the video to be transmitted over the network instead of the entire video. The present solution also provides a new and improved user interface for interacting with video on a voice-based device to improve the human-machine interaction process.
FIG. 1 illustrates an example system 100 for controlling digital components in a voice activated system. The system 100 may include a digital component selection infrastructure. System 100 may include a data processing system 102. The data processing system 102 may communicate with one or more of a digital component provider device 106 (e.g., a content provider device) or a client computing device 104 via a network 105. The network 105 may include a computer network such as the internet, a local area network, a wide area network, a metropolitan area network, or other area network, an intranet, a satellite network, and other communication networks such as a voice or data mobile phone network. The network 105 may be used to access information resources, such as web pages, websites, domain names, or uniform resource locators, that may be presented, output, rendered, or displayed on at least one computing device 104, such as a laptop computer, desktop computer, tablet, digital assistant, personal digital assistant, smart watch, wearable device, smart phone, portable computer, or speaker. For example, a user of the client computing device 104 may access information or data provided by the digital component provider device 106 via the network 105. The client computing device 104 may or may not include a display. For example, the client computing device 104 may include a limited type of user interface, such as a microphone and speaker, (e.g., the client computing device 104 may include a voice-driven or audio-based interface). The primary user interfaces of the computing device 104 may be a microphone and a speaker.
The network 105 may include or constitute a display network, such as a subset of information resources available on the internet that are associated with content placement or search engine result systems or that are entitled to include third party digital components. The data processing system 102 can use the network 105 to access information resources such as web pages, websites, domain names, or uniform resource locators that can be presented, output, rendered, or displayed by the client computing device 104. For example, a user of the client computing device 104 may access information or data provided by the digital component provider device 106 via the network 105.
The network 105 may be any type or form of network and may include any of the following: point-to-point networks, broadcast networks, wide area networks, local area networks, telecommunication networks, data communication networks, computer networks, ATM (asynchronous transfer mode) networks, SONET (synchronous optical network) networks, SDH (synchronous digital hierarchy) networks, wireless networks, and wired networks. The network 105 may include wireless links such as infrared channels or satellite bands. The topology of the network 105 may include a bus, star, or ring network topology. The network may include a mobile telephone network that uses any one or more protocols for communicating between mobile devices, including advanced mobile phone protocol (AMPS), time division multiple access ("TDMA"), code division multiple access ("CDMA"), global system for mobile communications ("GSM"), general packet radio service ("GPRS"), or universal mobile telecommunications system ("UMTS"). Different types of data may be transmitted via different protocols, or the same type of data may be transmitted via different protocols.
System 100 may include at least one data processing system 102. The data processing system 102 may include at least one logic device, such as a computing device having a processor to communicate with the computing device 104 or the digital component provider device 106 via the network 105, for example. Data processing system 102 may include at least one computing resource, server, processor, or memory. For example, data processing system 102 may include multiple computing resources or servers located in at least one data center. Data processing system 102 may include multiple servers grouped logically and facilitate distributed computing techniques. The logical group of servers may be referred to as a data center, a server group, or a machine group. Servers may also be geographically dispersed. The data center or machine group may be managed as a single entity or the machine group may include multiple machine groups. The servers within each computer group may be heterogeneous, and one or more servers or machines may operate in accordance with one or more types of operating system platforms.
Servers in a machine group may be stored in a high-density rack system along with associated storage systems, and may be located in an enterprise data center. Integrating servers in this manner may improve system manageability, data security, physical security of the system, and system performance, for example, by locating the servers and high-performance storage systems on a localized high-performance network. Centralizing all or some of the components of data processing system 102, including servers and storage systems, and coupling them with advanced system management tools allows for more efficient use of server resources, which saves power and processing requirements and reduces bandwidth usage.
The client computing device 104 may include, execute, dock, or otherwise communicate with at least one local digital assistant 134, at least one sensor 138, at least one transducer 140, at least one audio driver 142, or at least one display 144. The sensor 138 may include, for example, a camera, an ambient light sensor, a proximity sensor, a temperature sensor, an accelerometer, a gyroscope, a motion detector, a GPS sensor, a position sensor, a microphone, a video, image detection, or a touch sensor. Transducer 140 may include or be part of a speaker or microphone. The audio driver 142 may provide a software interface to the hardware transducer 140. The audio driver 142 may execute audio files or other instructions provided by the data processing system 102 to control the transducer 140 to generate corresponding sound waves or waveforms. The display 144 may include one or more hardware components or software components configured to provide a visual indication or optical output, such as a light emitting diode, an organic light emitting diode, a liquid crystal display, a laser, or a display.
The local digital assistant 134 may include or be executed by one or more processors, logic arrays, or memories. The local digital assistant 134 may detect the keyword and perform an action based on the keyword. Local digital assistant 134 may be an instance of remote digital assistant component 112 executing at data processing system 102, or may perform any of the functions of remote digital assistant component 112. The local digital assistant 134 can filter out or modify one or more terms before sending the terms as data to the data processing system 102 (e.g., the remote digital assistant component 112) for further processing. Local digital assistant 134 may convert the analog audio signals detected by transducer 140 into digital audio signals and send one or more data packets carrying the digital audio signals to data processing system 102 via network 105. In response to detecting an instruction to perform such a transmission, the local digital assistant 134 may transmit a signal carrying some or all of the input audio signal. The instructions may include, for example, triggering a keyword or other keyword or sending approval of a data packet comprising the input audio signal to the data processing system 102.
The local digital assistant 134 may perform pre-filtering or pre-processing on the input audio signal to remove certain frequencies of the audio. The pre-filter may comprise a filter such as a low pass filter, a high pass filter or a band pass filter. The filter may be applied in the frequency domain. The filters may be applied using digital signal processing techniques. The filter may be configured to maintain frequencies corresponding to human speech or human voice while eliminating frequencies beyond typical frequencies of human voice. For example, the band pass filter may be configured to remove frequencies below a first threshold (e.g., 70Hz, 75Hz, 80Hz, 85Hz, 90Hz, 95Hz, 100Hz, or 105 Hz) and above a second threshold (e.g., 200Hz, 205Hz, 210Hz, 225Hz, 235Hz, 245Hz, or 255 Hz). Applying a bandpass filter may reduce computational resource utilization in downstream processing. The local digital assistant 134 on the computing device 104 may apply a band pass filter before sending the input audio signal to the data processing system 102, thereby reducing network bandwidth utilization. However, based on the computing resources available to computing device 104 and the available network bandwidth, it may be more efficient to provide input audio signals to data processing system 102 to allow data processing system 102 to perform filtering.
The local digital assistant 134 may apply additional preprocessing or pre-filtering techniques, such as noise reduction techniques, to reduce the level of ambient noise that may interfere with the natural language processor. Noise reduction techniques may increase the accuracy and speed of the natural language processor, thereby increasing the performance of the data processing system 102 and managing the rendering of the graphical user interface provided via the display 144.
The client computing device 104 may be associated with an end user that inputs the voice query as audio input to the client computing device 104 (via the sensor 138 or transducer 140) and receives audio (or other) output from the data processing system 102 or digital component provider device 106 for presentation, display, or rendering to the end user of the client computing device 104. The digital components may include computer generated speech that may be provided from the data processing system 102 or the digital component provider device 106 to the client computing device 104. The client computing device 104 may render the computer-generated speech to an end user via a transducer 140 (e.g., a speaker). The computer-generated speech may include a sound recording from a real person or a computer-generated language. The client computing device 104 may provide visual output via a display device 144 communicatively coupled to the computing device 104.
An end user entering a voice query to a client computing device 104 may be associated with multiple client computing devices 104. For example, an end user may be associated with a first client computing device 104 that may be a speaker-based digital assistant device, a second client computing device 104 that may be a mobile device (e.g., a smart phone), and a third client computing device 104 that may be a desktop computer. The data processing system 102 may associate each of the client computing devices 104 with a common login, location, network, or other linking data. For example, an end user may log into each of the client computing devices 104 with the same account user name and password.
The client computing device 104 may receive an input audio signal detected by a sensor 138 (e.g., a microphone) of the computing device 104. The input audio signal may include, for example, queries, questions, commands, instructions, or other statements provided in a language. The input audio signal may include an identifier or name of a third party (e.g., digital component provider device 106) for which the question or request is directed.
The client computing device 104 may include, execute, or be referred to as a digital assistant device. The digital assistant device may include one or more components of the computing device 104. The digital assistant device may include a graphics driver that may receive display output from the data processing system 102 and render the display output on the display 132. The graphics driver may include hardware components or software components that control or enhance how graphics or visual output is displayed on the display 144. The graphics driver may include, for example, a program that controls how the graphics components work with the rest of the computing device 104 (or digital assistant). The local digital assistant 134 may filter the input audio signal to create a filtered input audio signal, convert the filtered input audio signal into data packets, and send the data packets to a data processing system that includes one or more processors and memory.
The digital assistant device may include an audio driver 142 and a speaker assembly (e.g., transducer 140). The preprocessor component 140 receives an indication of the display output and instructs the audio driver 142 to generate an output audio signal to cause the speaker component (e.g., transducer 140) to transmit an audio output corresponding to the indication of the display output.
The system 100 may include at least a digital component provider device 106, access at least the digital component provider device 106, or otherwise interact with at least the digital component provider device 106. Digital component provider device 106 may include one or more servers that may provide digital components to client computing device 104 or data processing system 102. The digital component provider device 106 or components thereof may be integrated with the data processing system 102 or at least partially executed by the data processing system 102. Digital component provider device 106 may include at least one logic device, such as a computer device having a processor to communicate with computing device 104, data processing system 102, or digital component provider device 106 via network 105, for example. The digital component provider device 106 may include at least one computing resource, server, processor, or memory. For example, the digital component provider device 106 may include a plurality of computing resources or servers located in at least one data center.
The digital component provider device 106 may provide audio, visual, or multimedia-based digital components for presentation by the client computing device 104 as audio output digital components, visual output digital components, or a mixture thereof. The digital component may be or include digital content. The digital component may be or include a digital object. The digital components may include subscription-based content or paid content. The digital component may comprise a plurality of digital content items. For example, the digital component may be a data stream from a streaming music service (e.g., digital component provider device 106). The digital component may include or may be a digital movie, a website, a song, an application (e.g., a smart phone or other client device application) or other text-based content, audio-based content, image-based content, or video-based content. For example, the digital component may be an operational video, movie, or other video provided by the digital content provider device 106 to the client computing device 104. The digital content provider device 106 may provide digital components generated by the digital content provider device, digital components uploaded by a user, or sources from other digital content provider devices 106.
Digital component provider device 106 may provide digital components to client computing device 104 via network 105 and bypass data processing system 102. Digital component provider device 106 may provide digital components to client computing device 104 via network 105 and data processing system 102. For example, the digital component provider device 106 may provide digital components to the data processing system 102, which data processing system 102 may store and provide digital components to the client computing device 104 upon request by the client computing device 104.
Data processing system 102 may include at least one computing resource or server. Data processing system 102 may include at least one interface 110, interface with at least one interface 110, or otherwise communicate with at least one interface 110. The data processing system 102 can include at least one remote digital assistant component 112, interface with the at least one remote digital assistant component 112, or otherwise communicate with the at least one remote digital assistant component 112. The remote digital assistant component 112 can include at least one natural language processor component 114, interface with the at least one natural language processor component 114, or otherwise communicate with the at least one natural language processor component 114. Data processing system 102 may include at least one digital component selector 120, interface with at least one digital component selector 120, or otherwise communicate with at least one digital component selector 120. The data processing system 102 can include at least one annotation component 135, interface with the at least one annotation component 135, or otherwise communicate with the at least one annotation component 135. The data processing system 102 can include at least one parsing component 116, interface with the at least one parsing component 116, or otherwise communicate with the at least one parsing component 116. Data processing system 102 may include at least one data store 124, interface with at least one data store 124, or otherwise communicate with at least one data store 124. The at least one data store 124 may include or store a collection of annotations 126, breakpoints 128, subtitle data 130, and content data 132 in one or more data structures or databases. The data store 124 may include one or more local or distributed databases and may include database management.
The interface 110, remote digital assistant component 112, natural language processor component 114, digital component selector 120, annotation component 135, and parsing component 116 may each include at least one processing unit or other logic device such as a programmable logic array engine, or module configured to communicate with a library stock or database 124. The interface 110, remote digital assistant component 112, natural language processor component 114, digital component selector 120, annotation component 135, parsing component, and data repository 124 can be separate components, a single component, or a portion of multiple data processing systems 102. System 100 and its components, such as data processing system 102, may include hardware elements such as one or more processors, logic devices, or circuits.
Data processing system 102 may include an interface 110. Interface 110 may be configured, constructed, or operable to receive and transmit information using, for example, data packets. Interface 110 may use one or more protocols, such as network protocols, to receive and transmit information. The interface 110 may include a hardware interface, a software interface, a wired interface, or a wireless interface. The interface 110 may facilitate converting or formatting data from one format to another. For example, the interface 110 may include an application programming interface that includes definitions for communication between various components, such as software components.
The data processing system 102 may include an application, script, or program, such as local digital assistant 134, installed on the client computing device 104 to communicate input audio signals to the interface 110 of the data processing system 102 and to drive components of the client computing device to render output audio signals or visual output. The data processing system 102 may receive data packets, digital files, or other signals that include or identify an input audio signal (or signals). The computing device 104 may detect the audio signal via the transducer 140 and convert the analog audio signal to a digital file via an analog-to-digital converter. For example, the audio driver 142 may include an analog-to-digital converter component. The preprocessor component 140 can convert the audio signal into a digital file that can be transmitted via data packets over the network 105.
The remote digital assistant component 112 of the data processing system 102 can execute or run the NLP component 114 to receive or obtain data packets that include an input audio signal detected by the sensor 138 of the computing device 104. Client computing device 104 may also execute instances of NLP component 114 to process language and text at client computing device 104. The data packets may provide a digital file. The NLP component 114 can receive or obtain digital files or data packets that include audio signals and parse the audio signals. For example, the NLP component 114 can provide interactions between a person and a computer. NLP component 114 can be configured with techniques for understanding natural language and enabling data processing system 102 to obtain meaning from human or natural language input. The NLP component 114 may include or be configured with machine learning based techniques, such as statistical machine learning. The NLP component 114 may parse the input audio signal using a decision tree, statistical model, or probabilistic model.
The NLP component 114 may perform, for example, the following functions: such as named entity recognition (e.g., given a stream of text, determining which terms in the text map to appropriate names such as people or places, and what type of each such name is such as people, places, or organizations), natural language generation (e.g., converting information from a computer database or semantic intent to a human language that is understandable), natural language understanding (e.g., converting text to a more formal representation, such as a first-order logical structure that a computer module can manipulate), machine translation (e.g., automatically translating text from one human language to another human language, morphological segmentation (e.g., separating words into individual morphemes and identifying classes of morphemes), this may be challenging based on the morphological or structural complexity of the word of the considered language), question answers (e.g., an answer to determine a human language question, which may be specific or open), and semantic processing (e.g., processing that may be performed after identifying the word and encoding its meaning to associate the identified word with other words having similar meanings). NLP component 114 may identify the semantic representation of the identified word.
The NLP component 114 can convert the input audio signal to recognized text by comparing the input signal to a representative set of audio waveforms stored (e.g., in the data store 124) and selecting the closest match. The set of audio waveforms may be stored in a data repository 124 or other database accessible to the data processing system 102. A representative waveform may be generated across a large set of users and then enhanced with voice samples from the users. After the audio signal is converted to recognized text, the NLP component 114 matches the text with words that are associated with actions that the data processing system 102 can service, for example, via training across users or through manual designation. The NLP component 114 can convert image input or video input into text or digital files. For example, the NLP component 114 can detect speech in a video file, convert the speech to text, and then process the text. The NLP component 114 can identify or receive closed caption data in a video file and process the closed caption data to identify text or perform semantic analysis on the closed caption data. The NLP component 114 can store closed caption data for each of the digital components as caption data 130 in the data store 124. The NLP component 114 can convert the NLP component 114 can process, analyze, or interpret image input or video input to perform actions, generate requests, or select data structures, or identify data structures.
The data processing system 102 may receive image input signals or video input signals in addition to or in lieu of input audio signals. The data processing system 102 may process the image input signal or the video input signal using, for example, image interpretation techniques, computer vision, machine learning engines, or other techniques to identify or interpret the image or video to convert the image or video to a digital file. One or more image interpretation techniques, computer vision techniques, or machine learning techniques may be collectively referred to as imaging techniques. In addition to or in lieu of audio processing techniques, the data processing system 102 (e.g., the NLP component 114) may be configured with imaging techniques.
The NLP component 114 may obtain an input audio signal. From the input audio signal, the NLP component 114 can identify at least one request or at least one trigger key corresponding to the request. The request may indicate an intent, digital component, or theme of the input audio signal. The trigger key may indicate the type of action that may be taken. For example, the NLP component 114 may parse an input audio signal to identify at least one request to skip to a particular portion of a video file. The trigger key may include at least one word, phrase, root or partial word, or derivative indicating an action to be taken. For example, the keywords "go", "go to" or "skip" are triggered to indicate that the end user wants to view a particular portion of the video file.
The NLP component 114 can parse the input audio signal to identify, determine, retrieve, or otherwise obtain a request for a digital component. The digital component may be a video-based file such as a streaming movie, program, or other video file. For example, the NLP component 114 may apply semantic processing techniques to the input audio signal to identify the requested digital component. The NLP component 114 can apply semantic processing techniques to the input audio signal to identify trigger phrases that include one or more trigger keywords, such as a first trigger keyword and a second trigger keyword. For example, the input audio signal may include the sentence "Play a video of fixing a bike (play video for repairing a bicycle)". The NLP component 114 may determine that the input audio signal includes a trigger keyword "play". The NLP component 114 can determine that the request is for a digital component (e.g., video) of the repaired bicycle.
Remote digital assistant component 112 of data processing system 102 can execute or run an instance of annotation component 135 to generate an annotated set of digital components. The annotation component 135 can generate an annotation collection of digital components that are sent to the client computing device 104 for presentation. The annotations collection may include one or more annotations of the entire digital component (e.g., video file), or of each of the identified scenes or steps in the digital component. Parsing component 116 can use the annotations collection to determine meaning, semantic meaning, or content contained within a digital component or a scene of a digital component. Parsing component 116 may use the set of annotations to match a request in an input audio signal with a scene or step identified in a digital component. The annotation component 135 can store annotations as an annotation collection 126 in a data store. The annotations collection 126 may be stored in a data structure or database that identifies digital components, breakpoints, scenes, video portions, or any combination thereof associated with the annotations collection 126.
The annotation component 135 can generate an annotation collection based on the voice or text recognized in the digital component. For example, the digital component may be video and the NLP component 114 may extract and process speech from the video. Based on the voice content of the video, the annotation component can determine the content of the video and tag keywords. For example, in an operation video, the annotation component may mark the name of a tool or a particular material. In this example, if the end user provides the input audio signal "what tools do I need to perform this task (i'm what tools are needed to perform the task)" using annotations while viewing the operational video, the data processing system may present the end user with portions of the video discussion tools in the operational video. The annotation component 135 can generate an annotation collection based upon closed caption data associated with the digital component. The annotation collection can include a list of words spoken during the numeric component or portion thereof. The parsing component 116 can perform a keyword search to match keywords identified in the input audio signal with terms spoken during the numeric component or portion thereof. The annotations collection may include semantic meanings or semantic representations of terms or phrases in the numeric component. The annotations collection may indicate semantic meaning for each scene or portion of the digital component.
The annotation component 135 can generate an annotation collection based upon images in the digital component. The annotation component 135 can extract frames or images from image-based or video-based digital components. The annotation component 135 can perform image recognition on the image. The image-based annotation collection may include a database of objects identified in the digital component and points in time at which the identified objects appear in the digital component. The annotation component 135 can also detect transitions in video-based digital components. The transition may be, for example, a scene change or a fade to black. The transition may represent a change from a first scene to a second scene. The annotations collection may indicate what type of conversion is identified in the digital component and the point in time at which the conversion occurred in the digital component.
The annotation component 135 can generate an annotation collection based upon input from the second client computing device 104 or the digital content provider device 106. For example, the digital component may be provided by the digital content provider device 106 or by an end user of the second client computing device 104. The provider of the digital components may annotate the digital components and send the annotations as a collection of annotations to the data processing system 102. The annotations collection from the provider may include points in time in the digital component indicating the start of a scene or step in the video, keywords or tags assigned to different parts of the digital component, or the location of defined break points. For example, an owner or creator of a digital component (e.g., video) may set the location of a breakpoint in metadata of the digital component to identify each step discussed in the digital component.
Remote digital assistant component 112 of data processing system 102 can execute or run an instance of parsing component 116 to parse the digital component into distinct portions based on the annotations collection generated by annotation component 135. Parsing the digital components may include dividing the digital components into separate digital components. For example, the parsing component 116 can divide the video into a plurality of smaller videos. Each of the smaller videos may include a single scene or step included in the original video. Parsing the digital component may include determining breakpoints in the digital component. A portion of a digital component may be defined as being between two breakpoints (or a first breakpoint of a beginning of a file and a first portion of the digital component, and a last breakpoint of an ending of the file and a last portion of the digital component). The parsing component 116 can set a breakpoint based on the annotations collection. For example, the parsing component 116 can set a breakpoint at a transition between scenes. The parsing component 116 may set multiple breakpoints within a single scene. For example, a single scene may cover a single theme in an operation video. A portion of an operational video of a scene containing a single theme may be defined by two breakpoints. The parsing component 116 can also include a plurality of breakpoints within a scene that indicate different steps taken during the scene. Parsing component 116 can use machine learning and/or natural language processing to set breakpoints to identify locations in the digital component that can correspond to different steps in the video, transformations in the video, or useful phrases. For example, the parsing component 116 can identify notes that may be useful or identifiable in the digital component, such as ingredient listings, tool listings, or certain types of scenes (e.g., car chase scenes), and set breakpoints at these locations. The resolution component 116 can also set breakpoints based on the viewing history of the digital component. For example, whenever a user typically views a sub-portion of a video, parsing component 116 may identify the sub-portion as an important or relevant portion of the video and may set a breakpoint near the beginning of the sub-portion. The parsing component 116 can determine or identify each breakpoint of the digital component. The resolution component 116 can save the breakpoint as breakpoint 128 in the data store 124. The breakpoints 128 may be a database storing the point in time of each of the breakpoints in association with an indication of the digital component. Breakpoints may be set at set intervals within the digital assembly. For example, the resolution component 116 can set breakpoints every 5, 10, 15, 20, 25, 30, or 60 minutes of the digital component.
The digital component selector 120 may select digital components including text, character strings, characters, video files, image files, or audio files that may be processed by the client computing device 104 and presented to a user via the display 144 or transducer 140 (e.g., a speaker). Digital component selector 120 may select a digital component that responds to a request in the input audio signal identified by NLP component 114. For a given request, digital component selector 120 may select a supplemental digital component that may also be provided with a primary digital component. The primary digital component may be a digital component that is directly selected in response to a request. For example, the primary digital component may be a user requested operation video. The supplemental digital components may be additional digital components that provide additional information or are related to the primary digital component.
The digital component selector 120 may select which digital component provider device 106 should or may satisfy the request and may forward the request to the digital component provider device 106. For example, the data processing system 102 can initiate a session between the digital component provider device 106 and the client computing device 104 to enable the digital component provider device 106 to transmit digital components to the client computing device 104. The digital component selector 120 may request one or more digital components from the digital component provider device 106. Digital component provider device 106 may provide digital components to data processing system 102, and data processing system 102 may store the digital components in data store 124. In response to a request for a digital component, digital component selector 120 may retrieve the digital component from data store 124. In response to the request for the digital component, the digital component selector 120 may select a portion or all of the digital component to provide to the client computing device 104 in response to the request.
The digital component selector 120 may select a plurality of digital components via a real-time content selection process. The digital component selector 120 may score and rank the digital components and provide the plurality of digital components to the output merge component 120 to allow the output merge component 120 to select the highest ranked digital component. The digital component selector 120 may select one or more additional digital components to be sent to the client computing device 104 based on the input audio signal (or keywords and requests contained therein). In one example, the input audio signal may include a request to initiate streaming video. The digital component selector 120 may select additional digital components (e.g., advertisements). When the digital component selector 120 transmits the operational video stream to the client computing device 104, the additional digital components may be sent to the client computing device 104. The additional digital component may notify the end user of an additional or related digital component provider device 106 that may satisfy the request from the first client computing device 104.
The digital component selector 120 may provide the selected digital component selected in response to a request identified in the input audio signal to the computing device 104 or a local digital assistant 134 or application executing on the computing device 104 for presentation. Accordingly, the digital component selector 120 may receive a content request from the client computing device 104, select a digital component in response to the content request, and send the digital component to the client computing device 104 for presentation. The digital component selector 120 may send the selected digital component to the local digital assistant 134 for presentation by the local digital assistant 134 itself or by a third party application executed by the client computing device 104. For example, the local digital assistant 134 may play or output an audio signal corresponding to the selected digital component.
The data store 124 stores content data 132, which content data 132 may include digital components, for example, provided by the digital component provider device 106 or obtained or determined by the data processing system 102 to facilitate content selection. The content data 132 may include, for example, digital components (or digital component objects) that may include, for example, content items, online documents, audio, images, video, multimedia content, or third party content. Digital component provider device 106 may provide full-length digital components to data processing system 102 for storage as content data 132. Digital component provider device 106 may provide portions of digital components to data processing system 102.
Fig. 2 shows a block diagram of an exemplary representation of a digital component 200 over time. The digital component 200 may be a video-based digital component, such as an operating video. The data processing system may identify a plurality of portions 201 (1) -201 (5), which may be generally referred to as portion 201. The data processing system may identify a plurality of break points 202 (1) -202 (4), which may be generally referred to as break points 202. The data processing system may also define break points 202 at the beginning and end of the digital component.
Each of the portions 201 may be defined between two break points 202. For example, portion 201 (2) is defined as the time between breakpoint 202 (1) and breakpoint 202 (2). The data processing system can select the time at which each of the breakpoints 202 is located based on the annotations collection generated by the annotation component. Breakpoint 202 (1) may correspond to the beginning of the instruction portion of digital component 200. Breakpoint 202 (2) may correspond to a first operational step of digital component 200.
FIG. 3 illustrates a block diagram of an example method 300 for controlling digital components in a voice activated system. Method 300 may include receiving an input signal (act 302). Method 300 may include parsing the input signal (act 304). The method 300 may include generating an annotation collection (act 306). Method 300 may include identifying a breakpoint (act 308). Method 300 may include receiving an input signal (act 310) and analyzing the input signal (act 312). The method 300 may include selecting a breakpoint (act 314). Method 300 may include transmitting a portion of the digital component (act 316).
As described above, method 300 may include receiving an input signal (act 302). The method 300 may include receiving an input signal by a natural language processor component executed by a data processing system. The input signal may be an input audio signal detected by a sensor at the first client device. The sensor may be a microphone of the first client device. For example, a digital assistant component, executed at least in part by a data processing system comprising one or more processors and memory, may receive an input audio signal. The input audio signal may include a dialog facilitated by a digital assistant. A dialog may include one or more inputs and outputs. The dialog may be audio-based, text-based, or a combination of audio and text. The input audio signal may include text input, or other types of input that may provide session information. The data processing system may receive audio input for a conversation corresponding to the conversation. The data processing system may receive one or more portions of the audio input, or upload as a batch or batch (e.g., portions of a conversation uploaded in a single transmission to reduce the number of transmissions).
Method 300 may include parsing the input signal (act 304). The NLP component of the data processing system may parse the input signal to identify a digital component request. The NLP component can identify a trigger key in the input signal. For example, the input audio signal may include "OK, show me how to fix my bike" (good, tell me how to repair a bicycle) ". The NLP component may parse the input signal to determine a request to display an operational video of how to repair the bicycle. The NLP component may determine that a trigger key is to be played, which may indicate that an end user wants to start streaming video and playing the video to a client computing device. The client computing device may also provide the digital component request to the data processing system in text form. For example, an end user may use a physical or numeric keyboard associated with a client computing device to enter a request for a particular video or other file.
Referring also to fig. 4, wherein fig. 4 shows the client computing device 104 at a first point in time 401 and during a second point in time 402 during an action of the method 300. The client computing device 104 shown in fig. 4 illustrates an example user interface presented to an end user on the display 144. The user interface is a conversational, voice-based interface. For example, input from the user is shown as beginning toward the right side of the display 144, and input (or response) from the data processing system is shown as beginning toward the left side of the display 144. New inputs or responses are added to the bottom of the dialog-near the bottom of the display 144. As new inputs or responses are added, older inputs and responses scroll toward the top of the display 144. For example, at time 402, a portion of the input visible at time 401 has scrolled out of the visible portion of display 144.
As shown in fig. 4, the client computing device 104 may detect that the phrase "how do Ifix my bike" is included? (how does i repair my bike. The input audio signal may be processed by the NLP component 114 to extract text of the input audio signal. Text 403 of the input audio signal may be displayed to the user as a confirmation that the client computing device 104 (and data processing system 102) has understood and properly processed the input audio signal. The data processing system may select the digital component 404 in response to a request identified in the input audio signal. In the example shown in fig. 4, the digital component 404 includes a video component 405 and a text component 406.
The method 300 may include generating an annotation collection (act 306). The annotation component 135 can generate one or more annotation collections for the selected digital components. The NLP component 114 can process voice contained in or closed caption data in or associated with the digital component to generate one or more annotation sets. The annotation component 135 can generate one or more annotations collections based upon the objects identified in the digital component. For example, data processing system 102 can perform object recognition on video frames in a digital component to identify objects in the digital component. The annotation component 135 can generate an annotation collection based upon a transition (e.g., video fade-in or video fade-out) within the digital component. NLP component 114 can generate semantic representations of speech or text within the digital component. The annotation component 135 can generate an annotation collection based upon the semantic representation. The annotation component 135 can cluster semantic representations together to determine which portions of the digital component are relevant to a particular topic or step. For example, for a digital component of an operating video of how to repair a bicycle, the annotation component 135 can use the semantic representation to identify the portion of the operating video that shows the replacement of a tire and the portion of the operating video that shows the replacement of a chain of the bicycle. The annotation component 135 can generate an annotation collection that indicates the portion of the operational video in which a tire change is discussed and the portion of the operational video in which a chain change is discussed.
Method 300 may include identifying a breakpoint (act 308). Data processing system 102 may identify breakpoints based on one or more of the generated annotations collections. The breakpoint may identify a point in time of a key frame in the video. A breakpoint may identify a scene transition, the beginning of each step in operating the video, points of interest in the video, entry or exit of objects in the video.
An indication of the one or more breakpoints may be sent to the client computing device 104. Referring also to fig. 4, wherein an indication of the breakpoint may be included in digital component 407 and sent to client computing device 104. Client computing device 104 can render digital component 407 to display a list or indication of breakpoints determined by data processing system 102 to be presented in video component 405. The annotation component 135 can generate a tag for each of the breakpoints. For example, as shown in FIG. 4, data processing system 102 sends a digital component including an indication of three breakpoints to client computing device 104. The break points are labeled "step1 (step 1)", "step2 (step 2)", and "step3 (step 3)", and may correspond to the start time points of step1, step2, and step3, respectively, in the video component 405.
The method 300 may also include the data processing system 102 selecting one or more supplemental or additional digital components in addition to the primary digital component identified in response to the input audio signal. For example, the supplemental digital components may be related to the same topic, provide other information related to the primary digital component, or may prompt the end user for further input. Referring also to FIG. 4, wherein the data processing system 102 selects the additional digital component 408 and sends the additional digital component 408 to the client computing device 104. The client computing device 104 can render and present the additional digital component 408 and the primary digital component 404. The additional digital component 308 may be an advertisement of a service provider or content related to the digital component 404.
Method 300 may include receiving an input signal (act 310). The input signal may be a second input audio signal. The second input audio signal may be detected by a sensor (e.g., a microphone) of the client computing device 104. Method 300 may include parsing the input signal (act 312). The NLP component 114 may parse the second input audio signal to identify keywords, terms, or semantic representations within the second input audio signal. For example, and referring also to fig. 4, the second input audio signal may include the phrase "show fixing the chain (display repair chain)". The client computing device 104 may display text 409 from the second input audio signal. The NLP component 114 may parse the second input audio signal to identify keywords, such as "show" in the example phrase above. NLP component 114 can identify a particular portion of video component 405 that the user wants based on the keyword "show". The NLP component 114 can identify the portion of the video component 405 that the end user wants to view a step in the video corresponding to repairing the chain of a bicycle.
The method 300 may include selecting a breakpoint (act 314). Data processing system 102 can select a breakpoint from a plurality of breakpoints generated based on one or more annotations collections of the digital component. The data processing system 102 may select the breakpoint based on keywords, terms, or semantic representations identified in the second input audio signal. Data processing system 102 can match or associate a keyword, term, or semantic representation with an annotation corresponding to each portion of the digital component or each breakpoint of the plurality of breakpoints.
In the example shown in fig. 4, the user provides a second input audio signal comprising the phrase "show fixing the chain". The second input audio signal may comprise a request for a particular breakpoint. For example, the client computing device 104 renders a digital component 407 that presents multiple breakpoints to the user. The second input audio signal may identify or be associated with a breakpoint that was not previously represented to the end user. For example, as shown in fig. 4, the second input audio signal "show fixing the chain" is not a selection of one of the following breakpoints: "step1 (step 1)", "step2 (step 2)", and "step3 (step 3)". When the second input audio signal does not include a particular selection of a breakpoint, the data processing system 102 can match or associate a keyword, term, or semantic representation identified in the second input audio signal with an annotation corresponding to one of the one or more breakpoints in the portion of the digital component. For example, the term "chain" may be matched or associated with a portion of the video component 405 in which the annotation component 135 recognizes the chain as an object in the video or a textual or semantic meaning of voice in the portion of the video corresponds to the chain.
Method 300 may include transmitting a portion of the digital component (act 316). The transmitted portion of the digital component may correspond to the selected breakpoint. The portion of the digital component may be transmitted to the client computing device 104 with instructions that cause the client computing device 104 to automatically begin playing or rendering the transmitted portion of the digital component. For example, and referring to FIG. 4, the data processing system 102 can select a breakpoint that corresponds to the beginning of a portion of the chain that shows how to replace on the bicycle. The client data processing system 102 can send the selected portion to the data processing system and the data processing system 102 can begin rendering the portion of the video component that shows how to replace the chain of the bicycle. Transmitting a portion of the digital component may also include transmitting an indication of a time associated with the selected breakpoint to the client computing device 104. For example, the client computing device 104 may have previously received an entity of the digital component. When a breakpoint is selected, the time associated with the breakpoint may be sent to the client computing device 104, and the client computing device 104 may skip to the time associated with the breakpoint in the video.
Fig. 5 illustrates a block diagram of an example computer system 500. Computer system or computing device 500 may include or be used to implement system 100 or its components such as data processing system 102. Data processing system 102 may include an intelligent personal assistant or a voice-based digital assistant. Computing system 500 includes a bus 505 or other communication component for communicating information, and a processor 510 or processing circuit coupled to bus 505 for processing information. Computing system 500 may also include one or more processors 510 or processing circuits coupled to a bus for processing information. Computing system 500 also includes main memory 515, such as Random Access Memory (RAM) or other dynamic storage device, coupled to bus 505 for storing information, and instructions to be executed by processor 510. Main memory 515 may be data store 124 or include data store 124. Main memory 515 may also be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 510. Computing system 500 may further include a Read Only Memory (ROM) 520 or other static storage device coupled to bus 505 for storing static information and instructions for processor 510. A storage device 525, such as a solid state device, magnetic disk, or optical disk, may be coupled to bus 505 to persistently store information and instructions 510. Storage device 525 may include data store 124 or be part of data store 124.
The computing system 500 may be coupled via bus 505 to a display 535, such as a liquid crystal display or an active matrix display, to display information to a user. An input device 530, such as a keyboard including alphanumeric and other keys, may be coupled to bus 505 for communicating information and commands to processor 510. The input device 530 may include a touch screen display 535. The input device 530 may also include a cursor control, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to the processor 510 and for controlling cursor movement on the display 535. Display 535 may be part of, for example, data processing system 102, client computing device 104, or another component of fig. 1.
The processes, systems, and methods described herein may be implemented by computing system 500 in response to processor 510 executing an arrangement of instructions contained in main memory 515. Such instructions may be read into main memory 515 from another computer-readable device, such as storage device 525. Execution of the arrangement of instructions contained in main memory 515 causes computing system 500 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 515. Hardwired circuitry may be used in place of or in combination with software instructions in combination with the systems and methods described herein. The systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
Although an example computing system has been described in FIG. 5, the subject matter including the operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them.
For the case where the system discussed herein collects personal information about a user or may utilize personal information, the user may be provided with an opportunity to control whether programs or functions of personal information may be collected (e.g., information about the user's social network, social actions or activities, user preferences, or user's location), or whether or how to receive content from a content server or other data processing system that is more relevant to the user. In addition, certain data may be anonymized in one or more ways prior to storage or use to remove personal identification information when generating parameters. For example, the identity of a user may be anonymized so that personally identifiable information of the user cannot be determined, or the geographic location of the user may be generalized (e.g., to a city, zip code, or state level) where location information is obtained so that a particular location of the user cannot be determined. Thus, the user can control how information about him or her is collected and how it is used by the content server.
The subject matter and operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs (e.g., one or more circuits of computer program instructions) encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium may be or be included in a computer readable storage device, a computer readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded with an artificially generated propagated signal. The computer storage media may also be or be included in one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification may be implemented as operations performed by a data processing apparatus on data stored on one or more computer readable storage devices or received from other sources.
The terms "data processing system," "computing device," "component" or "data processing apparatus" encompass a variety of devices, apparatus, and machines for processing data, including, for example, programmable processors, computers, systems-on-a-chip, a plurality or combination of the foregoing. The device may comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may include code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures. For example, interface 110, digital component selector 120, NLP component 114, annotation component 135, parsing component 116, and other data processing system components may include or share one or more data processing apparatuses, systems, computing devices, or processors.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object or other unit suitable for use in a computing environment. The computer program may correspond to a file in a file system. A computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of data processing system 102) to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including by way of example semiconductor memory devices (e.g., EPROM, EEPROM, and flash memory devices), magnetic disks (e.g., internal hard disk or removable disks); magneto-optical disks, and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
The subject matter described herein may be implemented in a computing system that includes a back-end component (e.g., as a data server) or that includes a middleware component (e.g., an application server) or that includes a front-end component (e.g., having a graphical user interface or a web browser through which a user interacts with an implementation of the subject matter described herein) or a combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), internets (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
A computing system, such as system 100 or system 500, may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network (e.g., network 105). The relationship between client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, the server sends data (e.g., data packets representing digital components) to the client device (e.g., for the purpose of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., results of user interactions) may be received from the client device at the server (e.g., received by the data processing system 102 from the client computing device 104 or the digital component provider device 106).
Although operations are depicted in the drawings in a particular order, such operations need not be performed in the particular order shown or in sequential order, and not all illustrated operations need be performed. The acts described herein may be performed in a different order.
The separation of individual system components does not require separation in all embodiments, and the described program components can be included in a single hardware or software product. For example, NLP component 114 or digital component selector 120 may be a single component, application, or program, or a logic device having one or more processing circuits, or part of one or more servers of data processing system 102.
Having now described some illustrative embodiments, it should be apparent that the foregoing has been presented by way of example and not limitation. In particular, although many of the examples presented herein involve specific combinations of method acts or system elements, those acts and those elements may be combined in other ways to achieve the same objectives. Acts, elements and features discussed in connection with one embodiment are not intended to be excluded from a similar role in other embodiments or embodiments.
The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of "including," "comprising," "having," "containing," "involving," "characterized by …," and variations thereof herein, is meant to encompass the items listed thereafter and equivalents thereof as well as additional items as well as alternative embodiments consisting of items specifically listed thereafter. In one embodiment, the systems and methods described herein consist of one, each combination of more than one, or all of the elements, acts, or components described.
Any reference to an implementation or element or act of a system and method referred to herein in the singular may also encompass an implementation comprising a plurality of such elements, and any reference to any implementation or element or act in the plural may also encompass an implementation comprising only the singular element. Reference to singular or plural forms are not intended to limit the presently disclosed systems or methods, their components, acts, or elements to either the singular or plural configurations. References to any act or element based on any information, action or element may include where the act or element is based at least in part on any information, action or element.
Any embodiment disclosed herein may be combined with any other embodiment or example, and references to "one embodiment," "some embodiments," "one embodiment," etc., are not necessarily mutually exclusive, and are intended to indicate that a particular feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment or example. Such terms as used herein do not necessarily all refer to the same embodiment. Any embodiment may be included or exclusive in combination with any other embodiment in any manner consistent with aspects and embodiments disclosed herein.
Reference to "or" may be construed as inclusive such that any term described using "or" may indicate any one of the singular, the plural, and all of the described terms. For example, a reference to "at least one of a 'and B' may include only" a ", only" B ", and both" a "and" B ". Such references used in conjunction with "comprising" or other open terms may include additional terms.
Where technical features in the drawings, detailed description, or any claim are followed by reference numerals, the reference numerals have been included to increase the intelligibility of the drawings, detailed description, and claims. Thus, neither the reference signs nor their absence have any limiting effect on the scope of any claim elements.
The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. For example, the computing device 104 may generate a packaged data object when the application is launched and forward it to a third party application. The foregoing embodiments are illustrative and not limiting of the systems and methods described. The scope of the systems and methods described herein are, therefore, indicated by the appended claims rather than by the foregoing description, and all changes which come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.
Claims (20)
1. A system for controlling digital components in a voice activated environment, comprising:
a data processing system comprising one or more processors coupled with a memory, the data processing system to:
after identifying a digital component having a plurality of portions corresponding to the annotations collection, receiving an input audio signal acquired via a sensor of the client device;
parsing the input audio signal to identify keywords from the input audio signal;
identifying, from the set of annotations, annotations associated with the keywords identified from the input audio signal;
Selecting a portion from the plurality of portions of the digital component that corresponds to the annotation identified as being associated with the keyword; and
the portion selected from the plurality of portions is provided to the client device.
2. The system of claim 1, comprising the data processing system to:
receiving the input audio signal after presenting, via the client device, at least one frame of the digital component and a plurality of indicators corresponding to the plurality of portions of the digital component; and
the portion selected from the plurality of portions is provided for presentation via the client device.
3. The system of claim 1, comprising the data processing system to:
determining a plurality of breakpoints defining the plurality of portions within the digital component based on at least one of the audio-visual content of the digital component or the set of annotations; and
one breakpoint is identified from the plurality of breakpoints based on the keyword and the set of annotations, the one breakpoint corresponding to the portion of the plurality of portions from the digital component.
4. The system of claim 1, comprising the data processing system to:
Generating a second digital component corresponding to the portion selected from the plurality of portions of the digital component; and
the second digital component is provided to present the portion via the client device.
5. The system of claim 1, comprising the data processing system to:
identifying a start time within the digital component at which the portion begins based on the annotation; and
an indication of the start time is provided to the client device to present a portion beginning at the start time in the digital component.
6. The system of claim 1, comprising the data processing system to:
determining that the keyword identified from the input audio signal does not include a selection of one of the plurality of portions of the digital component; and
in response to determining that the keyword does not include the selection, the annotation associated with the keyword semantic is identified.
7. The system of claim 1, comprising the data processing system to:
determining the keywords identified from the input audio signal includes a selection of the portion of the plurality of portions of the digital component; and
The portion is selected for the annotation determined to be associated with the keyword.
8. The system of claim 1, comprising the data processing system to identify the plurality of portions from the digital component based on one or more scene transitions detected in the audiovisual content of the digital component.
9. The system of claim 1, comprising the data processing system generating the set of annotations for the digital component based on at least one of audiovisual content in the digital component or a set of subtitle data associated with the digital component, each of the set of annotations corresponding to a semantic representation corresponding to the plurality of portions.
10. The system of claim 1, comprising the data processing system to:
receiving a second input audio signal acquired via the sensor of the client device prior to receiving the input audio signal;
parsing the second input audio signal to identify a request for content; and
the digital component is identified from a plurality of digital components based on a request for content identified from the second input audio signal.
11. A method for controlling digital components in a voice activated environment, comprising:
after identifying a digital component having a plurality of portions corresponding to the annotations collection, receiving, by the data processing system, an input audio signal acquired via a sensor of the client device;
parsing, by the data processing system, the input audio signal to identify keywords from the input audio signal;
identifying, by the data processing system, from the set of annotations, annotations associated with the keywords identified from the input audio signal;
selecting, by the data processing system, a portion from the plurality of portions of the digital component that corresponds to the annotation identified as being associated with the keyword; and
the portion selected from the plurality of portions is provided to the client device by the data processing system.
12. The method of claim 11, comprising:
receiving, by the data processing system, the input audio signal after presenting, via the client device, at least one frame of the digital component and a plurality of indicators corresponding to the plurality of portions of the digital component;
the portion selected from the plurality of portions is provided by the data processing system for presentation via the client device.
13. The method of claim 11, comprising:
determining, by the data processing system, a plurality of break points defining the plurality of portions within the digital component based on at least one of the audio-visual content of the digital component or the set of annotations; and
a breakpoint is identified by the data processing system from the plurality of breakpoints based on the keyword and the set of annotations, the breakpoint corresponding to the portion of the plurality of portions from the digital component.
14. The method of claim 11, comprising:
generating, by the data processing system, a second digital component corresponding to the portion selected from the plurality of portions of the digital component; and
the second digital component is provided by the data processing system to present the portion via the client device.
15. The method of claim 11, comprising:
identifying, by the data processing system, a start time within the digital component based on the annotation, at which the portion begins; and
an indication of the start time is provided by the data processing system to the client device to present a portion beginning at the start time in the digital component.
16. The method of claim 11, comprising:
determining, by the data processing system, that the keyword identified from the input audio signal does not include a selection of one of the plurality of portions of the digital component; and
in response to determining that the keyword does not include the selection, the annotation associated with the keyword semantic is identified by the data processing system.
17. The method of claim 11, comprising:
determining, by the data processing system, that the keyword identified from the input audio signal includes a selection of the portion of the plurality of portions of the digital component; and
the portion is selected by the data processing system for the annotation determined to be associated with the keyword.
18. The method of claim 11, comprising identifying, by the data processing system, the plurality of portions from the digital component based on one or more scene transitions detected in the audiovisual content of the digital component.
19. The method of claim 11, comprising generating, by the data processing system, the set of annotations for the digital component based on at least one of audiovisual content in the digital component or a set of subtitle data associated with the digital component, each of the set of annotations corresponding to a semantic representation corresponding to the plurality of portions.
20. The method of claim 11, comprising:
receiving, by the data processing system, a second input audio signal acquired via the sensor of the client device prior to receiving the input audio signal;
parsing, by the data processing system, the second input audio signal to identify a request for content; and
the digital component is identified by the data processing system from a plurality of digital components based on a request for content identified from the second input audio signal.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/973,447 | 2018-05-07 | ||
US15/973,447 US10733984B2 (en) | 2018-05-07 | 2018-05-07 | Multi-modal interface in a voice-activated network |
PCT/US2019/026345 WO2019217018A1 (en) | 2018-05-07 | 2019-04-08 | Voice based search for digital content in a network |
CN201980005354.6A CN111279333B (en) | 2018-05-07 | 2019-04-08 | Language-based search of digital content in a network |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980005354.6A Division CN111279333B (en) | 2018-05-07 | 2019-04-08 | Language-based search of digital content in a network |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117150079A true CN117150079A (en) | 2023-12-01 |
Family
ID=66248778
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311062769.1A Pending CN117150079A (en) | 2018-05-07 | 2019-04-08 | Language-based search of digital content in a network |
CN201980005354.6A Active CN111279333B (en) | 2018-05-07 | 2019-04-08 | Language-based search of digital content in a network |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980005354.6A Active CN111279333B (en) | 2018-05-07 | 2019-04-08 | Language-based search of digital content in a network |
Country Status (6)
Country | Link |
---|---|
US (3) | US10733984B2 (en) |
EP (1) | EP3685280A1 (en) |
JP (2) | JP7021368B2 (en) |
KR (2) | KR102433255B1 (en) |
CN (2) | CN117150079A (en) |
WO (1) | WO2019217018A1 (en) |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10885903B1 (en) * | 2018-12-10 | 2021-01-05 | Amazon Technologies, Inc. | Generating transcription information based on context keywords |
CN109951743A (en) * | 2019-03-29 | 2019-06-28 | 上海哔哩哔哩科技有限公司 | Barrage information processing method, system and computer equipment |
US11687588B2 (en) * | 2019-05-21 | 2023-06-27 | Salesforce.Com, Inc. | Weakly supervised natural language localization networks for video proposal prediction based on a text query |
JP7216175B1 (en) | 2021-11-22 | 2023-01-31 | 株式会社Albert | Image analysis system, image analysis method and program |
Family Cites Families (33)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6263507B1 (en) * | 1996-12-05 | 2001-07-17 | Interval Research Corporation | Browser for use in navigating a body of information, with particular application to browsing information represented by audiovisual data |
US6643620B1 (en) | 1999-03-15 | 2003-11-04 | Matsushita Electric Industrial Co., Ltd. | Voice activated controller for recording and retrieving audio/video programs |
JP2002007478A (en) * | 2000-06-19 | 2002-01-11 | Fuji Xerox System Service Co Ltd | Audiovisual contents providing device and method |
JP2002049625A (en) * | 2000-08-04 | 2002-02-15 | Telecommunication Advancement Organization Of Japan | Image retrieval device and image retrieval method |
US7996232B2 (en) | 2001-12-03 | 2011-08-09 | Rodriguez Arturo A | Recognition of voice-activated commands |
US6889191B2 (en) | 2001-12-03 | 2005-05-03 | Scientific-Atlanta, Inc. | Systems and methods for TV navigation with compressed voice-activated commands |
US20080193016A1 (en) * | 2004-02-06 | 2008-08-14 | Agency For Science, Technology And Research | Automatic Video Event Detection and Indexing |
CN100524457C (en) * | 2004-05-31 | 2009-08-05 | 国际商业机器公司 | Device and method for text-to-speech conversion and corpus adjustment |
JP4251634B2 (en) * | 2004-06-30 | 2009-04-08 | 株式会社東芝 | Multimedia data reproducing apparatus and multimedia data reproducing method |
NO320758B1 (en) * | 2004-07-23 | 2006-01-23 | Telenor Asa | Device and method for reproducing audiovisual content |
JP2007013320A (en) * | 2005-06-28 | 2007-01-18 | Funai Electric Co Ltd | Video recording apparatus, content recording apparatus, content retrieval control method, and content retrieval program |
US20080046406A1 (en) | 2006-08-15 | 2008-02-21 | Microsoft Corporation | Audio and video thumbnails |
JP2008276340A (en) * | 2007-04-26 | 2008-11-13 | Hitachi Ltd | Retrieving device |
KR100966651B1 (en) * | 2008-01-16 | 2010-06-29 | 재단법인서울대학교산학협력재단 | Ontology-based Semantic Annotation System and Method thereof |
US8487984B2 (en) | 2008-01-25 | 2013-07-16 | At&T Intellectual Property I, L.P. | System and method for digital video retrieval involving speech recognition |
US20090307741A1 (en) * | 2008-06-09 | 2009-12-10 | Echostar Technologies L.L.C. | Methods and apparatus for dividing an audio/video stream into multiple segments using text data |
WO2011050280A2 (en) * | 2009-10-22 | 2011-04-28 | Chintamani Patwardhan | Method and apparatus for video search and delivery |
US9443518B1 (en) * | 2011-08-31 | 2016-09-13 | Google Inc. | Text transcript generation from a communication session |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US9304648B2 (en) | 2013-06-26 | 2016-04-05 | Google Inc. | Video segments for a video related to a task |
US20160300020A1 (en) * | 2013-12-03 | 2016-10-13 | 3M Innovative Properties Company | Constraint-based medical coding |
EP3192273A4 (en) * | 2014-09-08 | 2018-05-23 | Google LLC | Selecting and presenting representative frames for video previews |
US9305530B1 (en) * | 2014-09-30 | 2016-04-05 | Amazon Technologies, Inc. | Text synchronization with audio |
US11182431B2 (en) * | 2014-10-03 | 2021-11-23 | Disney Enterprises, Inc. | Voice searching metadata through media content |
US9633262B2 (en) * | 2014-11-21 | 2017-04-25 | Microsoft Technology Licensing, Llc | Content interruption point identification accuracy and efficiency |
US10372819B2 (en) * | 2015-03-23 | 2019-08-06 | International Business Machines Corporation | Determining off-topic questions in a question answering system using probabilistic language models |
US20170004139A1 (en) * | 2015-06-30 | 2017-01-05 | Coursera, Inc. | Searchable annotations-augmented on-line course content |
US20170092278A1 (en) | 2015-09-30 | 2017-03-30 | Apple Inc. | Speaker recognition |
US9747926B2 (en) | 2015-10-16 | 2017-08-29 | Google Inc. | Hotword recognition |
US9928840B2 (en) | 2015-10-16 | 2018-03-27 | Google Llc | Hotword recognition |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10043517B2 (en) * | 2015-12-09 | 2018-08-07 | International Business Machines Corporation | Audio-based event interaction analytics |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
-
2018
- 2018-05-07 US US15/973,447 patent/US10733984B2/en active Active
-
2019
- 2019-04-08 WO PCT/US2019/026345 patent/WO2019217018A1/en unknown
- 2019-04-08 KR KR1020207028940A patent/KR102433255B1/en active IP Right Grant
- 2019-04-08 CN CN202311062769.1A patent/CN117150079A/en active Pending
- 2019-04-08 CN CN201980005354.6A patent/CN111279333B/en active Active
- 2019-04-08 KR KR1020227027672A patent/KR102619568B1/en active IP Right Grant
- 2019-04-08 EP EP19719046.5A patent/EP3685280A1/en active Pending
- 2019-04-08 JP JP2020555514A patent/JP7021368B2/en active Active
-
2020
- 2020-07-08 US US16/923,416 patent/US11776536B2/en active Active
-
2022
- 2022-02-02 JP JP2022014861A patent/JP7311653B2/en active Active
-
2023
- 2023-07-27 US US18/360,367 patent/US20240062749A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20200342856A1 (en) | 2020-10-29 |
JP7311653B2 (en) | 2023-07-19 |
KR20220116361A (en) | 2022-08-22 |
KR20200130400A (en) | 2020-11-18 |
EP3685280A1 (en) | 2020-07-29 |
JP2021521525A (en) | 2021-08-26 |
US10733984B2 (en) | 2020-08-04 |
WO2019217018A1 (en) | 2019-11-14 |
JP2022070886A (en) | 2022-05-13 |
US11776536B2 (en) | 2023-10-03 |
US20240062749A1 (en) | 2024-02-22 |
JP7021368B2 (en) | 2022-02-16 |
KR102433255B1 (en) | 2022-08-18 |
CN111279333B (en) | 2023-09-05 |
CN111279333A (en) | 2020-06-12 |
US20190341028A1 (en) | 2019-11-07 |
KR102619568B1 (en) | 2023-12-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN111279333B (en) | Language-based search of digital content in a network | |
KR102603717B1 (en) | Generation of domain-specific models in networked systems | |
US11848009B2 (en) | Adaptive interface in a voice-activated network | |
US11514907B2 (en) | Activation of remote devices in a networked system | |
JP2023062173A (en) | Video generation method and apparatus of the same, and neural network training method and apparatus of the same | |
CN110720098B (en) | Adaptive interface in voice activated networks | |
CN111213136B (en) | Generation of domain-specific models in networked systems | |
KR20230014680A (en) | Bit vector based content matching for 3rd party digital assistant actions | |
Maybury | 1 Co-operative Multimedia Interfaces More effective, efficient and natural human computer or computer mediated human-human interaction will require both automated understanding and generation of multimedia. Fluent conversational interaction demands explicit models of the user, discourse, task and context. It will also require a richer |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |