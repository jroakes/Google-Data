CN110720098B - Adaptive interface in voice activated networks - Google Patents
Adaptive interface in voice activated networks Download PDFInfo
- Publication number
- CN110720098B CN110720098B CN201980002074.XA CN201980002074A CN110720098B CN 110720098 B CN110720098 B CN 110720098B CN 201980002074 A CN201980002074 A CN 201980002074A CN 110720098 B CN110720098 B CN 110720098B
- Authority
- CN
- China
- Prior art keywords
- candidate
- request
- interface
- audio signal
- input audio
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000003044 adaptive effect Effects 0.000 title description 3
- 238000012545 processing Methods 0.000 claims abstract description 193
- 238000000034 method Methods 0.000 claims abstract description 84
- 230000005236 sound signal Effects 0.000 claims description 161
- 230000009471 action Effects 0.000 claims description 122
- 230000004044 response Effects 0.000 claims description 118
- 239000013598 vector Substances 0.000 claims description 15
- 230000005540 biological transmission Effects 0.000 abstract description 10
- 230000015654 memory Effects 0.000 description 16
- 230000008569 process Effects 0.000 description 13
- 238000004590 computer program Methods 0.000 description 10
- 238000004891 communication Methods 0.000 description 9
- 238000009877 rendering Methods 0.000 description 6
- 230000000007 visual effect Effects 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 5
- 238000013515 script Methods 0.000 description 5
- 239000003795 chemical substances by application Substances 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- 238000001914 filtration Methods 0.000 description 4
- 230000000670 limiting effect Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000007781 pre-processing Methods 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 108090000623 proteins and genes Proteins 0.000 description 3
- 230000006870 function Effects 0.000 description 2
- 238000003384 imaging method Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 230000033001 locomotion Effects 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000003062 neural network model Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000000153 supplemental effect Effects 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- IRLPACMLTUPBCL-KQYNXXCUSA-N 5'-adenylyl sulfate Chemical compound C1=NC=2C(N)=NC=NC=2N1[C@@H]1O[C@H](COP(O)(=O)OS(O)(=O)=O)[C@@H](O)[C@H]1O IRLPACMLTUPBCL-KQYNXXCUSA-N 0.000 description 1
- 241001573881 Corolla Species 0.000 description 1
- 230000003213 activating effect Effects 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000011143 downstream manufacturing Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000011045 prefiltration Methods 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
Abstract
The systems and methods of the present disclosure generally relate to a data processing system capable of identifying and visualizing alternative requests when presented with requests that are ambiguous, unclear, or that other data processing systems may not be responsive. The data processing system is capable of improving network transmission efficiency by selecting alternative requests that are responsive to the intent of the original request to reduce network bandwidth occupancy and processor utilization.
Description
Cross Reference to Related Applications
The present application claims priority from U.S. non-provisional patent application No. 15/977,699 entitled "ADAPTIVE INTERFACE IN A VOICE-ACTIVATED NETWORK," filed on 5.11 in 2018, the entire contents of which are incorporated herein by reference.
Background
Excessive network transmission of packet-based or otherwise network traffic data between computing devices may prevent the computing devices from properly processing, completing operations on, or responding to network traffic data. Excessive network transmission of network traffic data may also exacerbate data routing or reduce response quality if the responding computing device meets or exceeds its processing capabilities, which may result in reduced bandwidth utilization. Network transmissions corresponding to ambiguous requests may create a significant amount of unnecessary network traffic between computing devices.
Disclosure of Invention
In accordance with at least one aspect of the present disclosure, a system for generating a voice-based interface in a networked system can include a data processing system. The data processing system is capable of executing a Natural Language Processor (NLP) component, an interface management component, and a direct action Application Programming Interface (API). The data processing system is capable of receiving an input audio signal detected by a sensor of the client device at an interface of the data processing system. The data processing system is capable of parsing the input audio signal to identify a plurality of candidate requests based on the input audio signal. The data processing system can determine the interface type of the client device. The data processing system can select a portion of the plurality of candidate requests based on an interface type of the client device. The data processing system can generate an action data structure for each of the portion of the plurality of candidate requests based on the interface type of the client device. The data processing system can send an action data structure for each of the portion of the plurality of candidate requests to the client device based on the interface type of the client device.
The interface type may indicate at least one of a display format of the client device, a display size of the client device, display availability of the client device, or a type of the client device. The interface may send a second action data structure for the second candidate request to the client device to be rendered after the first action data structure. The natural language processor component may parse the input audio signal to identify a request in the input audio signal, determine a confidence score for a semantic meaning of the request, and select a plurality of candidate requests based on the confidence score being below a predetermined threshold. The natural language processor component may identify a term in the input audio signal having a plurality of interpretations, and the interface management component may select a subset of the plurality of candidate requests based on the term in the input audio signal having the plurality of interpretations.
The natural language processor component may parse the input audio signal to identify a primary request in the input audio signal, and the natural language processor component may determine a plurality of candidate requests based on semantic similarity between the primary request and each of the plurality of candidate requests. The natural language processor component may parse the input audio signal to identify a primary request in the input audio signal, and the natural language processor component may select a plurality of candidate requests from a log of previously received input audio signals based at least on semantic similarity between each candidate request of the plurality of candidate requests and the primary request. The direct action application programming interface may include an indication of the plurality of candidate requests in the first action data structure.
The natural language processor component may parse the input audio signal to identify a request in the input audio signal, and the content selection component executed by the data processing system may select the digital component based on the request in the input audio signal. The natural language processor component may parse the input audio signal to identify a primary request in the input audio signal, the direct action application programming interface may determine a response to the primary request, the direct action application programming interface may determine a response to each of the plurality of candidate requests, and the interface management component may select a subset of the plurality of candidate requests based on a comparison between the response to the request and the response to each of the plurality of candidate requests. The natural language processor component may parse the input audio signal to identify a primary request in the input audio signal, the direct action application programming interface may determine a response to the primary request, the natural language processor component may determine a popularity score for the response to the primary request in the input audio signal, the natural language processor component may determine a popularity score for the response to each of the plurality of candidate requests, and the interface management component may select a subset of the plurality of candidate requests based on a comparison of the popularity score for the response to the request in the input audio signal to the popularity score for the response to each of the plurality of candidate requests.
The interface may send an audio signal request including a prompt, the natural language processor component may receive a second input audio signal from the client device, the second input audio signal being generated in response to the prompt, and the interface management component may select a subset of the plurality of candidate requests based on the second input audio signal. The interface management component may select a second client device associated with the client device, the second client device having an interface type that is different from the interface type of the client device; and the interface may send the second action data structure to the second client device. The natural language processor component may receive a second input audio signal detected by a sensor of the client device, the natural language processor component may parse the second input audio signal to identify a candidate request based on the second input audio signal, and the interface may send the candidate request to the client device.
In accordance with at least one aspect of the present disclosure, a method of generating a voice-based interface in a networked system can include: an input audio signal detected by a sensor of a client device is received by a natural language processor component executed by a data processing system via an interface. The method can include: the input audio signal is parsed by the natural language processor component to identify a plurality of candidate requests based on the input audio signal. The method can include: the interface type of the client device is determined by an interface management component executed by the data processing system. The method can include: a number of the plurality of candidate requests is selected by the interface management component based on the interface type of the client device. The method can include: an action data structure is generated for each candidate request of the number of candidate requests based on an interface type of the client device through a direct action application programming interface of the data processing system. The method can include: an action data structure for each candidate request of the number of candidate requests is sent to the client device via the interface based on the interface type of the client device.
The interface type may indicate at least one of a display format of the client device, a display size of the client device, display availability of the client device, or a type of the client device. The method may further comprise: a second action data structure for the second candidate request is sent to the client device through the interface to be rendered after the first action data structure. The method may further comprise: parsing, by the natural language processor component, the input audio signal to identify a request in the input audio signal; determining, by the natural language processor component, a confidence score for the semantic meaning of the request; and selecting, by the natural language processor component, a plurality of candidate requests based on the confidence score being below a predetermined threshold. The method may further comprise: identifying, by the natural language processor component, terms in the input audio signal having a plurality of interpretations; and selecting, by the interface management component, the portion of the plurality of candidate requests based on terms in the input audio signal having a plurality of interpretations.
The method may further comprise: parsing, by the natural language processor component, the input audio signal to identify a primary request in the input audio signal; and determining, by the natural language processor component, a plurality of candidate requests based on semantic similarity between the primary request and each of the plurality of candidate requests. The method may further comprise: parsing, by the natural language processor component, the input audio signal to identify a primary request in the input audio signal; and selecting, by the natural language processor component, a plurality of candidate requests from a log of previously received input audio signals based at least on semantic similarity between each candidate request of the plurality of candidate requests and the primary request.
The method may further comprise: an indication of a candidate request for which the first action data structure is generated is included in the first action data structure through the direct action application programming interface. The method may further comprise: parsing, by the natural language processor component, the input audio signal to identify a request in the input audio signal; and selecting, by a content selection component executed by the data processing system, a digital component based on the request in the input audio signal. The method may further comprise: parsing, by the natural language processor component, the input audio signal to identify a primary request in the input audio signal; determining, by the direct action application programming interface, a response to the primary request; and determining, by the direct action application programming interface, a response to each of the plurality of candidate requests; and selecting, by the interface management component, the portion of the plurality of candidate requests based on a comparison between the response to the primary request and the response to each of the plurality of candidate requests. The method may further comprise: parsing, by the natural language processor component, the input audio signal to identify a primary request in the input audio signal; determining, by the direct action application programming interface, a response to the primary request; determining, by the natural language processor component, a popularity score of a response to a primary request in the input audio signal; determining, by the natural language processor component, a popularity score for a response to each candidate request of the plurality of candidate requests; and selecting, by the interface management component, a subset of the plurality of candidate requests based on a comparison of the popularity score of the response to the request in the input audio signal to the popularity score of the response to each of the plurality of candidate requests.
The method may further comprise: sending an audio signal request comprising a prompt through an interface; receiving, by the natural language processor component, a second input audio signal from the client device, the second input audio signal generated in response to the prompt; and selecting, by the interface management component, a subset of the plurality of candidate requests based on the second input audio signal. The method may further comprise: selecting, by the interface management component, a second client device associated with the client device, the second client device having an interface type that is different from an interface type of the client device; and sending, via the interface, a second action data structure to the second client device. The method may further comprise: receiving, by the natural language processor component, a second input audio signal detected by a sensor of the client device; parsing, by a natural language processor component, the second input audio signal to identify candidate requests based on the second input audio signal; and sending the candidate request to the client device via the interface.
These and other aspects and embodiments are discussed in detail below. The above information and the following detailed description include illustrative examples of various aspects and embodiments and provide an overview or framework for understanding the nature and character of the claimed aspects and embodiments. The accompanying drawings provide a description and a further understanding of various aspects and embodiments and are incorporated in and constitute a part of this specification. The various aspects and embodiments of the disclosed subject matter can be combined as appropriate.
Drawings
The figures are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing. In the figure:
Fig. 1 illustrates an example system for generating an interface in a voice activated system according to examples of this disclosure.
Fig. 2 illustrates a block diagram of an example method of generating a voice-based interface in a voice activation system, according to an example of the present disclosure.
Fig. 3 and 4 show illustrations of example speech-based interfaces for presenting responses to candidate requests according to examples of the present disclosure.
Fig. 5 illustrates a block diagram of an example computer system in accordance with examples of this disclosure.
Detailed Description
Various concepts and embodiments thereof relating to methods, apparatus, and systems for generating interfaces in a voice-activated computer network environment are described in more detail below. The various concepts introduced above and discussed in more detail below are implemented in any of a number of ways.
The systems and methods of the present disclosure generally relate to a data processing system capable of identifying and generating or visualizing alternative requests when presented with requests that are ambiguous, unclear, or that other data processing systems may not be responsive. The data processing system is capable of improving network transmission efficiency by selecting alternative requests that are responsive to the intent of the original request to reduce network bandwidth occupancy and processor utilization. Selecting and responding to alternative requests can save bandwidth by not sending an error message or subsequent message to the client device requesting additional information or data about the original request. To further save bandwidth and computing resources, the data processing system can select which of these additional responses the data processing system will generate a response to based on the interface type of the client device that sent the request to the data processing system. For example, based on the screen size (or lack thereof) of the client device, the data processing system can select one of the alternative requests for which it generates a response or a subset of the alternative requests. Selecting and responding only to a portion of the possible alternative requests can save bandwidth by not sending to the client device a response generated in response to all possible interpretations of the original request.
Fig. 1 illustrates an example system 100 for generating an interface in a voice activated system. The system 100 can include a digital component selection infrastructure. System 100 can include a data processing system 102. The data processing system 102 is capable of communicating with one or more digital component provider devices 106 (e.g., content provider devices) or client computing devices 104 via a network 105. The network 105 can include a computer network such as the internet, a local area network, a wide area network, a metropolitan area network, or other area network, an intranet, a satellite network, and other communication networks such as a voice or data mobile phone network. The network 105 can be used to access an information resource, such as a web page, web site, domain name, or uniform resource locator, which can be presented, output, rendered, or displayed on at least one computing device 104, such as a laptop computer, desktop computer, tablet computer, digital assistant, personal digital assistant, smart watch, wearable device, smart phone, portable computer, or speaker. For example, via network 105, a user of client computing device 104 can access information or data provided by digital component provider device 106. The client computing device 104 may or may not include a display. For example, the client computing device 104 may include a limited type of user interface, such as a microphone and speaker (e.g., the client computing device 104 can include a voice-driven or audio-based interface). The main user interface of the computing device 104 may include a microphone and a speaker. The client computing device 104 can be a speaker-based digital assistant device.
The network 105 can include or constitute a display network, such as a subset of information resources available on the Internet that are associated with a content placement or search engine results system or that are eligible to include third-party digital components. The data processing system 102 can use the network 105 to access information resources, such as web pages, web sites, domain names, or uniform resource locators, that can be presented, output, rendered, or displayed by the client computing device 104. For example, via network 105, a user of client computing device 104 can access information or data provided by digital component provider device 106.
The network 105 may be any type or form of network and may include any of the following: point-to-point networks, broadcast networks, wide area networks, local area networks, telecommunication networks, data communication networks, computer networks, ATM (asynchronous transfer mode) networks, SONET (synchronous optical network) networks, SDH (synchronous digital hierarchy) networks, wireless networks, and wired networks. The network 105 may include wireless links such as infrared channels or satellite bands. The topology of the network 105 may include bus, star, or ring network topologies. The network may include a mobile telephone network that uses any one or more protocols for communicating between mobile devices, including advanced mobile phone protocol ("AMPS"), time division multiple access ("TDMA"), code division multiple access ("CDMA"), global system for mobile communications ("GSM"), general packet radio service ("GPRS"), or universal mobile telecommunications system ("UMTS"). Different types of data may be transmitted via different protocols, or the same type of data may be transmitted via different protocols.
The system 100 can include at least one data processing system 102. The data processing system 102 can include at least one logic device, such as a computing device, the processor of which communicates with, for example, the computing device 104 or the digital component provider device 106 via the network 105. Data processing system 102 can include at least one computing resource, server, processor, or memory. For example, data processing system 102 can include multiple computing resources or servers located at least one data center. Data processing system 102 can include multiple logically grouped servers and facilitate distributed computing techniques. The logical group of servers may be referred to as a data center, a server farm, or a machine farm. Servers can also be geographically dispersed. The data center or machine farm may be managed as a single entity or the machine farm can include multiple machine farms. The servers within each computer farm can be heterogeneous-one or more of the servers or machines can operate in accordance with one or more types of operating system platforms.
Servers in a machine farm can be housed in a high-density rack system along with associated storage systems and located at an enterprise data center. For example, in this manner, by locating servers and high performance storage systems on a localized high performance network, an integrated server may improve system manageability, data security, physical security of the system, and system performance. Centralizing all or some of the data processing system 102 components, including servers and storage systems, and coupling them with advanced system management tools allows for more efficient utilization of server resources, which saves power and processing requirements and reduces bandwidth occupation.
The client computing device 104 can include, execute, interact with, or otherwise communicate with one or more of at least one local digital assistant 134, at least one sensor 138, at least one transducer 140, at least one audio driver 142, or at least one display 144. The sensor 138 can include, for example, a camera, an ambient light sensor, a proximity sensor, a temperature sensor, an accelerometer, a gyroscope, a motion detector, a GPS sensor, a position sensor, a microphone, a video, image detection, or a touch sensor. The transducer 140 can include or be part of a speaker or microphone. The audio driver 142 can provide a software interface to the hardware transducer 140. The audio driver 142 can execute audio files or other instructions provided by the data processing system 102 to control the transducer 140 to generate corresponding sound waves or sound waves. The display 144 can include one or more hardware or software components configured to provide a visual indication or optical output, such as a light emitting diode, an organic light emitting diode, a liquid crystal display, a laser, or a display.
The local digital assistant 134 can include or be executed by one or more processors, logic arrays, or memories. The local digital assistant 134 can be a pre-processor. Local digital assistant 134 is capable of executing any of the components of data processing system 102. The local digital assistant 134 can detect the keyword and perform an action based on the keyword. Local digital assistant 134 is capable of executing instances of components executed by data processing system 102 or is capable of executing any of the functions of digital processing system 102. The local digital assistant 134 is capable of preprocessing the input audio signal received by the client computing device 104. For example, the local digital assistant 134 can filter out one or more terms or modify terms before sending the terms as data to the data processing system 102 for further processing. Local digital assistant 134 can convert the analog audio signals detected by transducer 140 to digital audio signals and send one or more data packets carrying the digital audio signals to data processing system 102 via network 105. The local digital assistant 134 is capable of transmitting data packets carrying some or all of the input audio signal in response to detecting an instruction to perform such transmission. The instructions can, for example, include triggering keywords or other keywords or approvals to send data packets including the input audio signal to the data processing system 102.
The local digital assistant 134 can perform pre-filtering or pre-processing on the input audio signal to remove certain frequencies of the audio. The prefilter can include a filter, such as a low pass filter, a high pass filter, or a band pass filter. The filter can be applied to the frequency domain. The filter can be applied by using digital signal processing techniques. The filter can be configured to maintain frequencies corresponding to human sound or human speech while eliminating frequencies beyond typical frequencies of human speech. For example, the band pass filter can be configured to remove frequencies below a first threshold (e.g., 70Hz, 75Hz, 80Hz, 85Hz, 90Hz, 95Hz, 100Hz, or 105 Hz) and above a second threshold (e.g., 200Hz, 205Hz, 210Hz, 225Hz, 235Hz, 245Hz, or 255 Hz). Applying a bandpass filter can reduce the computational resource utilization in downstream processing. The local digital assistant 134 on the computing device 104 can apply a band pass filter before sending the input audio signal to the data processing system 102, thereby reducing network bandwidth utilization. However, based on the computing resources available to computing device 104 and the available network bandwidth, it may be more efficient to provide the input audio signal to data processing system 102 to allow data processing system 102 to perform filtering.
The local digital assistant 134 can apply additional preprocessing or pre-filtering techniques, such as noise reduction techniques, to reduce the level of ambient noise that may interfere with the natural language processor. Noise reduction techniques can increase the accuracy and speed of the natural language processor, thereby increasing the performance of the data processing system 102 and managing the rendering of the graphical user interface provided via the display 144.
The client computing device 104 can be associated with an end user that inputs a voice query as audio input (via the sensor 138 or transducer 140) into the client computing device 104 and receives audio (or other) output from the data processing system 102 or digital component provider device 106 for presentation, display, or rendering to the end user of the client computing device 104. The digital components can include computer-generated speech that can be provided to the client computing device 104 from the data processing system 102 or the digital component provider device 106. The client computing device 104 is capable of rendering computer-generated speech to an end user via a transducer 140 (e.g., a speaker). The computer-generated speech can include a sound recording from a real person or a computer-generated language. The client computing device 104 is capable of providing visual output via a display device 144 communicatively coupled to the computing device 104.
An end user entering a voice query into a client computing device 104 can be associated with multiple client computing devices 104. For example, an end user can be associated with a first client computing device 104 that can be a speaker-based digital assistant device, a second client computing device 104 that can be a mobile device (e.g., a smart phone), and a third client computing device 104 that can be a desktop computer. Data processing system 102 can associate each of client computing devices 104 with a common login, location, network, or other linking data. For example, an end user can log into each of the client computing devices 104 using the same account user name and password.
The client computing device 104 is capable of receiving input audio signals detected by a sensor 138 (e.g., a microphone) of the computing device 104. The input audio signal can, for example, include a query, question, command, instruction, request, or other statement provided in a language. The input audio signal can include an identifier or name of the third party (e.g., digital component provider device 106) for which the question or request is directed. For example, the request can be content provided by a particular digital component provider device 106.
The client computing device 104 can include, execute, or be referred to as a digital assistant device. The digital assistant device can include one or more components of the computing device 104. The digital assistant device can include a graphics driver that can receive display output from the data processing system 102 and render the display output on the display 144. The graphics driver can include hardware or software components that control or enhance how graphics or visual output is displayed on the display 144. The graphics driver can, for example, include a program that controls how the graphics components cooperate with the rest of the computing device 104 (or digital assistant). The local digital assistant 134 is capable of filtering the input audio signal to create a filtered input audio signal, converting the filtered input audio signal into data packets, and transmitting the data packets to a data processing system that includes one or more processors and memory.
The digital assistant device can include an audio driver 142 and a speaker assembly (e.g., transducer 140). The preprocessor component may receive the indication of the display output and instruct the audio driver 142 to generate an output audio signal to cause the speaker component (e.g., transducer 140) to transmit an audio output corresponding to the indication of the display output.
The system 100 can include, access, or interact with at least a digital component provider device 106. The digital component provider device 106 can include one or more servers capable of providing digital components to the client computing device 104 or the data processing system 102. The digital component provider device 106 or components thereof can be integrated with the data processing system 102 or at least partially executed by the data processing system 102. The digital component provider device 106 can include at least one logic device, such as a computing device having a processor, to communicate with the computing device 104, the data processing system 102, or the digital component provider device 106 via the network 105. The digital component provider device 106 can include at least one computing resource, server, processor, or memory. For example, the digital component provider device 106 can include multiple computing resources or servers located at least one data center.
The digital component provider device 106 can provide audio, visual, or multimedia-based digital components for presentation by the client computing device 104 as audio output digital components, visual output digital components, or a mixture thereof. The digital components can be incorporated into an action data structure that is sent to the client computing device 104 and rendered by the client computing device 104. The digital component can be or include digital content. The digital component can be or include a digital object. The digital components can include subscription-based content or paid content. The digital component can include a plurality of digital components. For example, the digital component can include text that answers questions posed by the user in the request. The client computing device 104 is able to process the text into an audio output signal. The digital component can include or can be a digital movie, a website, a song, an application (e.g., a smart phone or other client device application), or other text-based, audio-based, image-based, or video-based content. The digital content provider device 106 is capable of providing digital components that are generated by the digital content provider device 106, uploaded by a user, or sourced from other digital content provider devices 106.
The digital component provider device 106 is capable of providing digital components to the client computing device 104 via the network 105 and bypassing the data processing system 102. Digital component provider device 106 is capable of providing digital components to client computing device 104 via network 105 and data processing system 102. For example, the digital component provider device 106 can provide digital components to the data processing system 102, which can store the digital components and provide the digital components to the client computing device 104 when requested by the client computing device 104.
Data processing system 102 can include at least one computing resource or server. Data processing system 102 can include, interact with, or otherwise communicate with at least one interface 110. The data processing system 102 can include, interact with, or otherwise communicate with at least one natural language processor component 114. Data processing system 102 can include, interact with, or otherwise communicate with at least one digital component selector 120. Data processing system 102 can include, interact with, or otherwise communicate with at least one interface management component 135. Data processing system 102 can include, interact with, or otherwise communicate with at least one data store 124. The at least one data store 124 can include or store in one or more data structures or databases a log 128 of past requests, templates 130, and content data 132. The data store 124 can include one or more local databases or distributed databases.
The interface 110, the natural language processor component 114, the digital component selector 120, and the interface management component 135 can each include at least one processing unit or other logic device, such as a programmable logic array engine or module, configured to communicate with the repository or database 124. The interface 110, the natural language processor component 114, the digital component selector 120, the interface management component 135, and the data store 124 can be separate components, a single component, or part of multiple data processing systems 102. System 100 and its components, such as data processing system 102, can include hardware elements, such as one or more processors, logic devices, or circuits.
Data processing system 102 can include an interface 110. The interface 110 can be configured, constructed, or operable to receive and transmit information, for example, through the use of data packets. The interface 110 is capable of receiving and transmitting information using one or more protocols, such as a network protocol. The interface 110 can include a hardware interface, a software interface, a wired interface, or a wireless interface. The interface 110 can facilitate converting or formatting data from one format to another. For example, the interface 110 can include an application programming interface that includes definitions for communication between various components, such as software components.
The data processing system 102 can include an application, script, or program installed on a client computing device 104, such as a local digital assistant 134, to communicate input audio signals to the interface 110 of the data processing system 102 and to drive components of the client computing device to render output audio signals or visual output. The data processing system 102 is capable of receiving data packets, digital files, or other signals that include or identify an input audio signal (or signals). The computing device 104 is capable of detecting audio signals via the transducer 140 and converting analog audio signals to digital files via an analog-to-digital converter. For example, the audio driver 142 can include an analog-to-digital converter component. The preprocessor component can convert the audio signal into a digital file that can be transmitted over the network 105 via data packets.
The data processing system 102 can execute or run the NLP component 114 to receive or obtain data packets that include input audio signals detected by the sensors 138 of the computing device 104. The client computing device 104 is also capable of executing instances of the client computing device 104 to process language and text at the client computing device 104. The data packets can provide a digital file. The NLP component 114 can receive or obtain digital files or data packets that include audio signals and parse the audio signals. For example, the NLP component 114 can provide interactions between a person and a computer. The NLP component 114 can be configured with techniques for understanding natural language and causing the data processing system 102 to derive meaning from human or natural language input. The NLP component 114 can include or be configured with techniques based on machine learning, such as statistical machine learning. The NLP component 114 can parse the input audio signal using a decision tree, statistical model, or probabilistic model.
The NLP component 114 can, for example, perform the following functions: named entity recognition (e.g., given a text stream, determining which items in the text map to proper names, such as people or places, and what type of each such name is, such as people, places, or organizations), natural language generation (e.g., converting information or semantic intent from a computer database into a understandable human language), natural language understanding (e.g., converting text into a more formal expression, such as a first order logical structure that a computer module can manipulate), machine translation (e.g., automatically translating text from one human language into another human language), morpheme segmentation (e.g., separating words into individual morphemes and identifying classes of morphemes, which can be challenging due to the lexical or word structure complexity of the language in question), question answering (e.g., determining answers to human language questions, which can be concrete or open), semantic processing (e.g., processing that may occur after recognizing words and encoding their meanings to correlate the recognized words with other words having similar meanings). The NLP component 114 can identify semantic representations of the identified terms. By identifying the semantic representation, the data processing system is able to match words or phrases based on their similar semantic meaning rather than specific word matches. For example, an input request search based on a semantic representation can return related requests.
The NLP component 114 can convert the input audio signal to recognized text by comparing the input signal to a stored set of representative audio waveforms (e.g., stored in the data repository 124) and choosing the closest match. The set of audio waveforms can be stored in data repository 124 or other database accessible to data processing system 102. The representative waveforms are generated across a large number of users and may then be augmented with speech samples from the users. After the audio signal is converted to recognized text, NLP component 114 matches the text with terms associated with actions that data processing system 102 is able to service, for example, via cross-user training or through manual specification. The NLP component 114 can convert image or video input into text or digital files. For example, the NLP component 114 can detect speech in a video file, convert the speech to text, and then process the text. NLP component 114 can convert, process, analyze, or interpret image or video input to perform actions, generate requests, or select or identify data structures.
In addition to or in lieu of the input audio signals, the data processing system 102 is capable of receiving image or video input signals. The data processing system 102 can process the image or video input signal, for example, by using image interpretation techniques, computer vision, machine learning engines, or other techniques, to recognize or interpret the image or video to convert the image or video into a digital file. One or more image interpretation techniques, computer vision techniques, machine learning techniques can be collectively referred to as imaging techniques. In addition to or instead of audio processing techniques, the data processing system 102 (e.g., the NLP component 114) can be configured with imaging techniques.
The NLP component 114 is able to obtain an input audio signal. From the input audio signal, the NLP component 114 can identify at least one request or at least one trigger keyword corresponding to the request. The request can indicate an intent, digital component, or theme of the input audio signal. The trigger key can indicate the type of action that may be taken. For example, the NLP component 114 can parse the input audio signal to identify at least one request for current weather for a particular location. The request may be an explicit request or an implicit request. For example, the request "is it going to rain today (not raining today)" may be an explicit request for an indication of whether raining. The request "do I need an umbrella (i need to take the umbrella)" may be a implicit request for an indication of whether or not to rain.
The NLP component 114 can parse the input audio signal to identify, determine, retrieve, or otherwise obtain a primary request from the input audio signal. For example, the NLP component 114 can apply semantic processing techniques to the input audio signal to identify a request in the input audio signal. The natural language processor component 114 can identify candidate requests based on the input audio signal. For example, the natural language processor component 114 can identify a primary request in an input audio signal. The primary request may be ambiguous or ambiguous. If the primary request has multiple possible responses, the primary request may be ambiguous or ambiguous. The primary request may be ambiguous or ambiguous if the quality of the input audio signal is poor and the natural language processor component 114 is unable to process one or more terms in the input audio signal.
The NLP component 114 can determine candidate requests based on a log of previously received input audio signals. The data processing system 102 is capable of recording previously identified requests from previously received input audio signals. The candidate requests can be recorded requests that are semantically similar to the primary request identified in the input audio signal. The NLP component 114 can rank the recorded requests based on semantic similarity between the primary request and the recorded requests. For example, each of the recorded requests can be one-hot encoded and converted into vector space. The primary request can be unithermally encoded and converted into vector space. The similarity between the primary request and the log request can be based on the distance between the primary request and the recorded request in the vector space. The similarity between the primary request and the log request can be based on a pearson correlation coefficient (Pearson Correlation) between the primary request and the recorded request.
Data processing system 102 is capable of executing or running an instance of direct action API 112. The direct action API 112 can identify, select, or generate an action data structure to fulfill a request (or candidate request) identified in the input audio signal. Based on the request or trigger key, the direct action API 112 predicts, evaluates, or otherwise determines the topic of the action data structure. The action data structures can include digital components, text, video, images, or other content that can be rendered by the client computing device 104 in response to sending the input audio signals to the data processing system 102.
The action data structure and the content item can correspond to a theme of the input audio signal. As determined by the NLP component 114, the direct action API 112 can generate specified actions to fulfill an end user's intent, primary request, or candidate request. The direct action API 112 is capable of executing code or dialog scripts that identify parameters needed to fulfill a user request, based on the actions specified in its input. Such code can look up additional information in data store 124 or send an action data structure (or a request generated thereby) to a third party device to provide data to data processing system 102 for inclusion in the action data structure. For example, the direct action API 112 can generate search phrases that are sent to a search engine. Responses from the search engine can be included in the response field of the action data structure. The search phrase or a request from the input audio signal can be included in an input field of the action data structure. The direct action API 112 can determine the necessary parameters and can package the information into an action data structure, which can then be sent to another component, such as the digital component selector component 120, or to an agent of the service provider computing device to be fulfilled. The direct action API 112 can send the primary and candidate requests to a service provider or third party, which can return the populated action data structure in response to receiving the request. For example, when the location is provided to the weather agent, the weather agent can return an action data structure indicating weather, such as { loc:94035; currentWeather: sunny; hiTemp:85; lowTemp:60}.
Data processing system 102 can execute or run instances of interface management component 135. The interface management component 135 can poll, determine, identify, or select interfaces for rendering action data structures and digital components. The interface management component 135 can identify an interface of the client computing device 104. The interface management component 135 can identify one or more interfaces associated with the client computing device 104 or the associated client computing device 104. The client computing device 104 can be associated with one or more additional client computing devices 104. The client computing device 104 and the additional client computing device 104 can be associated with each other through a common application login, end user, or other identifier. For example, an end user of the client computing device 104 can log into applications installed on the client computing device 104 and the additional client computing devices 104. The application can be associated with or provided by data processing system 102. Using the same credential sign-in application can enable data processing system 102 to link client computing devices 104 together in data store 124.
The interface management component 135 can identify the interface type of the client computing device 104. Identifying the interface type can include determining a capability of an interface of the client computing device. For example, the interface management component 135 can determine whether the client computing device 104 includes a display 144, an audio driver 142 (e.g., a speaker), or a combination thereof. Identifying or determining the interface type can include determining a device type of the client computing device 104. For example, the interface management component 135 can determine whether the client computing device 104 is a smart phone, a laptop computer, a desktop computer, a speaker-based auxiliary device, or other type of computing device. Identifying the interface type can include determining display parameters (e.g., size of the display, orientation of the display, resolution of the display); an application, user agent, content, or digital component displayed on an interface or executed by the client computing device 104; or audio parameters.
The interface management component 135 can poll the client computing devices 104 to determine the interface type of the client computing devices 104. The interface management component 135 can poll the client computing device 104 by sending a message to the client computing device 104 that determines display parameters and returns data to the interface management component 135. For example, the interface management component 135 can send a digital component to the client computing device 104 that includes a client executable script (e.g., javaScript) that can determine the resolution of the display 144 and send the resolution data to the interface management component 135. The interface management component 135 can poll the client computing devices 104 at regular intervals (e.g., in response to receiving an input audio signal from the client computing devices 104) to determine the interface type of the client computing devices 104. The interface management component 135 can poll the client computing device 104 once during the registration phase. Once registered with data processing system 102, data processing system 102 can store the interface types associated with client computing devices 104 in data store 124. The interface management component 135 can retrieve the interface type from the input audio signal. For example, the client computing device 104 can include metadata accompanying the input audio signal indicating the type of interface of the client computing device 104. The interface management component 135 can process the metadata to extract the interface type of the client computing device 104.
Based on the input audio signal, the interface management component 135 can determine which candidate request of the plurality of candidate requests is responsive. The data processing system can be responsive to a portion or subset of the identified plurality of candidate requests. The interface management component 135 can select a number of multiple candidate requests (e.g., a portion or subset thereof) in response to the input audio signal. The interface management component 135 can select the number of candidate requests based on the interface type of the client computing device 104. For example, the interface management component 135 can select a relatively larger number of candidate requests for a client computing device 104 having a relatively larger display. For example, the interface management component 135 can determine available space on the display 144 of the client computing device for displaying the response (e.g., rendered action data structure). The interface management component 135 can select a candidate number of requests for responses based on the number of responses within the available space that will fit into the display 144.
The interface management component 135 can select a number of candidate requests based on the natural language processor component 114 determining that a term or phrase in the input audio signal may have multiple interpretations or likely responses. For example, an input audio signal containing the phrase "Ok, how long it takes to work" WILL IT TAKE to get to work may have multiple interpretations. The data processing system 102 can determine a first response based on whether the user intends to drive the vehicle, a second response based on whether the user intends to ride the public transportation, and a third response based on whether the user intends to walk. When a request generates multiple responses, or when one or more fields of the action data structure of the request are left blank, the request may have multiple interpretations, thereby implementing multiple responses. For example, the action data structure for the phrase may be { start_location: "123Main St"; destination_location: "456 1 st Street"; method: "}. In this example, no traffic method is set. The data processing system is able to generate candidate requests and associated action data structures rather than generating error messages (because all parameters of the request are not defined). Each action data structure for different candidate requests may include different traffic methods in the method field.
The interface management component 135 can determine a number of the plurality of candidate requests based on a comparison between a response to a request parsed from the input audio signal and a response to each of the plurality of candidate requests generated based on the input audio signal. For example, the natural language processor component 114 can identify the request "Ok, how long WILL IT TAKE to get to work by car (please determine how long it takes to ride to work)" from the input audio signal. The natural language processor component 114 can determine candidate requests related to the identified request. For example, the natural language processor component 114 can determine candidate requests such as "how long it takes to work in public transportation" and "how long it takes to work in riding a bicycle" WILL IT TAKE to get to work by bike. The response to the request may be "35minutes by car". The responses to the candidate requests may be "15minutes by the subway (15 minutes on subway)" and "40minutes by bike". The interface management component 135 can select a response that is substantially different from a response to a request identified in the input audio signal. For example, interface management component 135 can select response "15minutes by the subway (riding for 15 minutes)" in addition to response "35minutes by car" because the responses (e.g., traffic times) are substantially different. The degree of difference or similarity between these responses can be determined by machine learning or neural networks. For example, the text of each response can be converted into a vector.
The distance between responses in the vector space can indicate the degree of similarity (or degree of variance) between the individual responses. Vectors that are closer to each other can be ranked more similarly than vectors that are farther apart. For example, the natural language processor component 114 can generate a similarity score by generating a term vector for each term within the response. The natural language processor component 114 can use a continuous bag-of-words (bag-of-words) neural network model or a skip-gram neural network model to generate a vector representation of the words in the response. The natural language processor component 114 can use Word2Vec to generate Word vectors.
The interface management component 135 can determine the number of multiple candidate requests based on the popularity score of each candidate request (or its response). For example, the natural language processor component 114 can identify requests with a popularity score above a predetermined threshold. The popularity score can be based on the number of possible responses that the data processing system 102 can return in response to the input audio signal. For example, request "WHAT IS THE top speed of a 2015Brand A car? (how fast the top speed of brand a car was in 2015. Request "WHAT IS THE top speed of a car (how fast the highest speed of the car)? "can have a relatively high popularity score because multiple responses may be returned. For example, the data processing system 102 may return different responses for different car categories, manufacturers, or car configurations. When the popularity score is higher, the interface management component 135 can select a greater number of candidate requests to which the data processing system 102 provides responses to the client computing device 104.
The interface management component 135 can determine the number of candidate requests to which the data processing system 102 provides a response based on the interface type of the second client computing device 104. The second client computing device 104 can be associated with a client computing device 104 that transmits the input audio signal to the data processing system 102. For example, the client computing device 104 can be an end user's smart phone and the second client computing device 104 can be an end user's laptop computer. The interface type of the second client computing device 104 can be different from the client computing device 104 that sends the input audio signal to the data processing system 102. For example, a user's smart phone can have a display of a first size and resolution, and a user's laptop computer can have a display of a second size and resolution. The interface management component 135 can send a portion of the response to the candidate request to the client computing device 104 and send the remaining portion of the response to the candidate request to the second client computing device 104.
The digital component selector 120 can select digital components including text, character strings, characters, video files, image files, or audio files that can be processed by the client computing device 104 and presented to a user via the display 144 or transducer 140 (e.g., a speaker). Digital component selector 120 is capable of selecting digital components that respond to a request identified in the input audio signal by NLP component 114. For a given request, the digital component selector 120 can select a supplemental digital component that can also be provided along with the primary digital component. The primary digital component can be a digital component that is directly selected in response to a request or candidate request. For example, the primary digital component can include an answer to a question posed in the request. The supplemental digital components can be additional digital components that provide additional information or are related to the primary digital component.
The digital component selector 120 can select which digital component provider device 106 should or can fulfill the request and can forward the request to the digital component provider device 106. For example, the data processing system 102 can initiate a session between the digital component provider device 106 and the client computing device 104 to enable the digital component provider device 106 to transmit the digital component to the client computing device 104. The digital component selector 120 can request a digital component from the digital component provider device 106. The digital component provider device 106 can provide digital components to the data processing system 102, which can store the digital components in the data store 124. In response to a request for a digital component, digital component selector 120 can retrieve the digital component from data store 124. In response to a request for a digital component, digital component selector 120 can select a portion or all of the digital component to provide to client computing device 104 in response to the request.
The digital component selector 120 is capable of selecting a plurality of digital components via a real-time content selection process. The digital component selector 120 can score and rank digital components and provide a plurality of digital components for inclusion in an action data structure, or more generally, for transmission to the client computing device 104. The digital component selector 120 can select one or more additional digital components to send to the client computing device 104 based on the input audio signal (or keywords and requests contained therein). In one example, the input audio signal can include a request to begin streaming an operation description video. The digital component selector 120 is capable of selecting additional digital components (e.g., advertisements). The additional digital component can notify an end user of the additional or related digital component provider device 106 that may fulfill the request from the first client computing device 104.
The digital component selector 120 can provide selected digital components selected in response to a request identified in an input audio signal to the computing device 104 or the local digital assistant 134 or an application executing on the computing device 104 for presentation. Accordingly, the digital component selector 120 is capable of receiving a content request from the client computing device 104, selecting a digital component in response to the content request, and transmitting the digital component to the client computing device 104 for presentation. The digital component selector 120 can send the selected digital component to the local digital assistant 134 for presentation by the local digital assistant 134 itself or by a third party application executed by the client computing device 104. For example, the local digital assistant 134 can play or output an audio signal corresponding to the selected digital component.
The data store 124 stores content data 132, which can include, for example, digital components provided by the digital component provider device 106 or obtained or determined by the data processing system 102 to facilitate content selection. The content data 132 can include, for example, digital components (or digital component objects) that can include, for example, digital components, online documents, audio, images, video, multimedia content, or third party content. Digital component provider device 106 is capable of providing full-length digital components to data processing system 102 for storage as content data 132. The digital component provider device 106 is capable of providing portions of digital components to the data processing system 102.
The data store 124 can store templates 130. The template 130 can be a template of an action data structure. The template 130 can include fields that the direct action API 112 can populate when fulfilling a request. The templates can include standardized fields that can be populated by the direct action API 112 or a third party when completing or responding to a request.
The data store 124 can store past requests 128. Past request 128 can be a past request received by data processing system 102 in an input audio signal or other input signal. Past requests can be parsed from the input signal by the natural language processor component 114. Past requests 128 can be logs of past requests. Past requests 128 can be databases of requests. The database can include text of past requests. The database can include vectorization of past requests. Each vector of past requests can be thermally encoded uniaxially. Vectors can be used to determine semantic similarity between past requests and current requests.
Fig. 2 illustrates a block diagram of an example method 200 of generating a voice-based interface in a voice-activated system. The method 200 can include receiving an input signal (act 202). The method 200 can include parsing the input signal (act 204). The method 200 can include determining an interface type (act 206). The method 200 can include selecting a candidate request (act 208). The method 200 can include generating an action data structure (act 210). Method 200 can include transmitting an action data structure (act 212).
As described above, the method 200 can include receiving an input signal (act 202). The method 200 can include receiving an input signal by a natural language processor component executed by a data processing system. The input signal can be an input audio signal detected by a sensor at the first client device. The sensor can be a microphone of the first client device. For example, a digital assistant component that is at least partially executed by a data processing system that includes one or more processors and memory can receive an input audio signal. The input audio signal can include a dialog facilitated by a digital assistant. A dialog can include one or more inputs and outputs. The dialog can be audio-based, text-based, or a combination of audio and text. The input audio signal can include text input or other types of input that can provide dialog information. The data processing system can receive audio input regarding a conversation corresponding to the conversation. The data processing system can receive audio input that is divided into one or more portions or uploaded in batches or batches (e.g., uploading portions of a conversation in a single transmission to reduce the number of transmissions).
The method 200 can include parsing the input signal (act 204). The NLP component of the data processing system is capable of parsing the input signal to identify a plurality of candidate requests based on the input audio signal. Each candidate request of the plurality of candidate requests can be based on a different semantic meaning or interpretation of the input signal. The data processing system is capable of identifying a primary request in the input audio signal. The candidate request can be based on the primary request. The candidate request can be based on identifying, by the NLP component, a term or phrase having multiple interpretations in the primary request. Each of these candidate requests can correspond to a primary request, the term or phrase of which is interpreted with each possible interpretation. For example, based on the primary request "Ok, what 'S THE TIME IN GREENVILLE (please determine that the green's time is a point)", the data processing system can generate candidate requests "what 'S THE TIME IN GREENVILLE, NH (green's time of new hampshire is a point)", and "what 'S THE TIME IN GREENVILLE, SC (green's time of south carolina). The data processing system can generate or select candidate requests based on semantic similarity between the primary request and each of the candidate requests. For example, the data processing system can search log files of past requests. The data processing system is capable of calculating a distance between a past request and a main request in the vector space. The data processing system can select the previous, previous three, previous five, previous ten, or more past responses that are most similar to the primary response (e.g., closest to the primary response in vector space).
The NLP component can determine a confidence score for the primary request identified in the input audio signal. The NLP can determine a confidence score based on the semantics of the primary request. The confidence score can indicate whether the primary request is ambiguous or unclear. For example, if a primary request (or some term therein) has multiple interpretations, the confidence score of the semantic meaning of the primary request may be low. A low confidence score can indicate that a primary request may have multiple interpretations, multiple possible responses, be a generalized or broad request, or that the request does not include sufficient information to provide a response. If the term in the request has multiple interpretations, the request may have multiple or different semantics. A term may have multiple interpretations because the term has multiple definitions or because the term is an homonym or an homonym (e.g., different words with the same pronunciation). For example, a user requests information about "genes," but in an audio-based interface, the data processing system might interpret "genes" as "genes" or "jeans (jeans)". Requests may have multiple or different semantics due to the lack of information, context, or other data for the request. For example, a user may request the highest speed of a bird without specifying the bird. A term may be broad when it is a generic (e.g., higher level) term rather than a term. For example, corolla (c) is one of the broader generic term automobiles. When the confidence score is below a predetermined threshold, the NLP component can determine to select a plurality of candidate requests, indicating that the primary request has a plurality of semantic meanings.
The method 200 can include determining an interface type (act 206). The data processing system can determine the type of interface that sends the input audio signal to the client computing device of the data processing system. The interface type can indicate at least one of a display format of the client device, a display size of the client device, display availability of the client device, or a type of the client device. The data processing system can poll the client computing device to determine the interface type. For example, the data processing system can send a message to the client computing device that includes processor-executable instructions that, when executed by the client computing device, determine the interface type of the client computing device. When sending the input audio signal to the data processing system, the client computing device can send an indication of the interface type to the data processing system. For example, the client computing device can include an indication of the interface type of the client computing device in metadata or as a parameter of the input audio signal.
The method 200 can include selecting a candidate request (act 208). The data processing system can select a subset of candidate requests for which the data processing system will send a response or action data structure to the client computing device. The subset of candidate requests can be one or more of the candidate requests. The data processing system is able to select all candidate requests. The number of candidate requests selected can be based on the interface type of the client computing device. The number of candidate requests selected can be based on terms or phrases in the primary request that have multiple interpretations. For example, the data processing system can select a number of candidate requests corresponding to the number of interpretations of the term or phrase. The data processing system can select the number of candidate requests based on semantic similarity between the primary request and one or more past requests stored in a past request log.
For example, referring to fig. 3 and 4, the data processing system is particularly capable of selecting the number of candidate requests (or a portion of candidate requests to which the data processing system will respond) based on the interface type of the client computing device. FIG. 3 illustrates an example client computing device 104 that displays responses to a single candidate request. FIG. 4 illustrates an example client computing device 104 that displays responses to multiple candidate requests. In the example shown in fig. 3, the client computing device 104 is a smart phone. The client computing device 104 provides an input signal. The input signal can be an audio-based signal or a text-based signal. Text 302 of the input signal can be rendered on the display 144 of the client computing device 104. The data processing system 102 is able to parse the text 302 of the input signal to determine that the input signal includes a primary request "how long does it take to get to work (how long to work)". The data processing system can determine that the primary request can include a plurality of candidate requests. For example, the candidate request can be "how long does it take to get to work by car (how long it takes to work in a bus)", "how long does it take to get to work by public transit (how long it takes to work in a public transportation vehicle)", or "how long does it take to get to work by bike (how long it takes to work in a bicycle)", as shown, in FIG. 3, the interface type can indicate that the display 144 is a relatively small display.
FIG. 4 illustrates an example of the client computing device 104 displaying multiple responses to different candidate requests. In the example shown in fig. 4, the client computing device 104 is a tablet computer and can have a relatively larger display 144 than the client computing device 104 shown in fig. 3. The client computing device 104 provides an input signal. The input signal can be an audio-based signal or a text-based signal. Text 302 of the input signal can be rendered on the display 144 of the client computing device 104. The NLP component can determine that the primary request "how fast do birds fly (how fast the bird is flying)" is broad because the request can return multiple possible responses. For example, the speed of a bird may depend on the bird for which the user intends to request information. The data processing system can determine whether the primary request is broad or generalized. In response to receiving a broad or broad primary request, the data processing system can send a message including a hint. The message can be an audio signal request. The user can respond to the prompt. The client computing device 104 is able to capture the response as a second input audio signal that is sent to the data processing system 102. The data processing system can select one or more of the candidate responses (or generate a new candidate response) based on the response identified in the second input audio signal.
As shown in FIG. 4, the data processing system can generate a plurality of candidate responses, which may include "how fast can bird A fly (how fast bird A can fly)", "how fast can bird B fly (how fast bird B can fly)", and "how fast can bird C fly (how fast bird C can fly)". The data processing system can generate other candidate requests. Based on the interface type (e.g., client computing device 104 has a relatively large screen), the data processing system can choose to generate responses to three of these candidate requests. The data processing system can generate an action data structure for each of the selected candidate requests. The action data structures can include indications of candidate requests generated in response to them. FIG. 4 illustrates that the data processing system generates three action data structures that client computing device 104 renders as cards 404 (1), 404 (2), and 404 (3), which can be collectively referred to as cards 404. Card 404 can include a separate response to each of the candidate requests. The card 404 can include text, images, video, audio, or other forms of content or digital components. Cards 404 can be arranged in a carousel such that a user can swipe or browse between cards 404 to view each of the cards 404. The user can select one or more of these cards 404 to activate the card 404. Activating card 404 can cause the user interface to display additional information associated with card 404 or begin playing or rendering media (e.g., video or audio files) associated with card 404.
The method 200 can include generating an action data structure (act 210). The data processing system can generate an action data structure for each of the selected candidate requests. The action data structure of each candidate request can include responses including digital components, video-based content, text-based content, audio-based content, or other types of content items. The action data structure can be sent to a third party server, or the data processing system can receive data from the third party server to populate one or more fields of the action data structure before the action data structure is sent to the client device. The content of the action data structure can be rendered and displayed to the user by the client computing device. Each action data structure can include a response to a respective one of the candidate requests.
Method 200 can include transmitting an action data structure (act 212). The data processing system can send the generated action data structure to the client computing device via an interface of the data processing system. The data processing system can send the action data structure to a client device associated with the client computing device. The data processing system can generate an action data structure for one or more of the unselected candidate requests, which can be referred to as additional requests. An action data structure for the additional request can be sent to the second client device. In some example, the client device can be a smart phone. The data processing system can select one candidate request (e.g., the highest ranked candidate request) for which an action data structure is generated as a response and sent to the client device. The data processing system can also generate an action data structure for the next five (or other number) of candidate requests. The action data structure can be sent to the user's laptop. A notification can be sent to the smart phone indicating that additional information and possible responses are available for viewing on the user's laptop.
The data processing system can send the action data structure to the client computing device for sequential or parallel display. For example, the data processing system can select a first candidate request and a second candidate request from a plurality of candidate requests. The data processing system can generate a first action data structure for the first candidate request and a second action data structure for the second candidate request. The data processing system can send the first action data structure and the second action data structure to cause the action data structures to be rendered together on the display in response. The data processing system can send the first action data structure and the second action data structure to the client computing device to cause the action data structures to be rendered sequentially. For example, the first action data structure can be rendered as a first possible response to the user input signal. Rendering the first action data structure can cause the client computing device to display a result associated with the first candidate request and an indication of the first candidate request (e.g., text of the first candidate request). The user can clear the rendered first action data structure. For example, the user can swipe the card including the response from the screen. The client computing device is then able to render the second action data structure. The data processing system can cause the action data structure to be rendered sequentially based on the interface type of the client computing device. For example, on a relatively small screen of a smart phone, the action data structure can be rendered sequentially. The data processing system can also send the first action data structure and the second action data structure to the client computing device, respectively. That is, the data processing system can send the first action data structure to the client computing device and send the second action data structure to the client computing device only if the client computing device indicates that the first action data structure has been cleared, e.g., indicates that the user has swiped a card including the first response from the screen. In this way, unnecessary data is not sent to the client computing device, and bandwidth and processing burden is reduced.
Fig. 5 illustrates a block diagram of an example computer system 500. Computer system or computing device 500 can include or be used to implement system 100 or components thereof, such as data processing system 102. Data processing system 102 can include an intelligent personal assistant or a voice-based digital assistant. Computing system 500 includes a bus 505 or other communication component for communicating information, and a processor 510 or processing circuit coupled to bus 505 for processing information. Computing system 500 can also include one or more processors 510 or processing circuits coupled to the bus to process information. Computing system 500 also includes a main memory 515, such as a Random Access Memory (RAM) or other dynamic storage device, coupled to bus 505 to store information and instructions to be executed by processor 510. Main memory 515 can be or include data store 124. Main memory 515 can also be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 510. Computing system 500 may further include a Read Only Memory (ROM) 520 or other static storage device coupled to bus 505 to store static information and instructions for processor 510. A storage device 525, such as a solid state device, magnetic disk, or optical disk, can be coupled to bus 505 to permanently store information and instructions. The storage device 525 can include or be part of the data store 124.
The computing system 500 may be coupled via bus 505 to a display 535, such as a liquid crystal display or an active matrix display, to display information to a user. An input device 530, such as a keyboard including alphanumeric and other keys, may be coupled to bus 505 for communicating information and command selections to processor 510. The input device 530 can include a touch screen display 535. The input device 530 can also include a cursor control, such as a mouse, a trackball, or cursor direction keys to communicate direction information and command selections to the processor 510 and to control cursor movement on the display 535. The display 535 can be part of, for example, the data processing system 102, the client computing device 104, or other components in fig. 1.
The processes, systems, and methods described herein can be implemented by computing system 500 executing an arrangement of instructions contained in main memory 515 in response to processor 510. Such instructions can be read into main memory 515 from another computer-readable medium, such as storage device 525. Execution of the arrangement of instructions contained in main memory 515 causes computing system 500 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 515. Hardwired circuitry can be used in place of or in combination with software instructions in combination with the systems and methods described herein. The systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
While an example computing system is depicted in fig. 5, the subject matter comprising the operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them.
For the case where personal information is collected about a user or available to the systems discussed herein, the user may be provided with an opportunity to control whether programs or features of personal information (e.g., information about a user's social network, social actions or activities, user preferences, or user location) may be collected or whether or how to receive content from a content server or other data processing system that is more relevant to the user. In addition, certain data may be anonymized in one or more ways prior to storage or use to remove personal identification information when parameters are generated. For example, the identity of the user may be anonymized so that no personally identifiable information can be determined for the user, or where location information is obtained, the geographic location of the user may be generalized (such as to a city, zip code, or state level) so that a particular location of the user cannot be determined. Thus, the user can control how information about him or her is collected and how it is used by the content server.
The subject matter and operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs, e.g., circuitry of one or more computer program instructions encoded on one or more computer storage media for execution by, or to control the operation of, digital processing apparatus. Alternatively or additionally, the program instructions can be encoded on a manually-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage matrix, a random or serial access memory array or device, or a combination of one or more of them. Although a computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded with an artificially generated propagated signal. A computer storage medium can also be, or be included in, one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer readable storage devices or received from other sources.
The terms "data processing system," "computing device," "component," or "data processing apparatus" encompass a variety of devices, apparatuses, and machines for processing data, including, for example, one or more programmable processors, computers, systems-on-a-chip, or combinations of the foregoing. The apparatus can comprise dedicated logic circuits, for example, an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus can also include code that creates an execution environment for the computer program described above, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an computing system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment are capable of implementing various computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures. For example, interface 110, digital component selector 120, NLP component 114, interface management component 135, and other data processing system components can include or share one or more data processing apparatuses, systems, computing devices, or processors.
A computer program (also known as a program, software application, app, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program can correspond to a file in a file system. A program can be stored in a file portion that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of data processing system 102) to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including for example: semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as built-in hard disks or removable disks; magneto-optical disk; CD-ROM discs and DVD-ROM discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
The subject matter described herein can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described herein), or a combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and point-to-point networks (e.g., ad hoc point-to-point networks).
A computing system, such as system 100 or system 500, can include clients and servers. The client and server are generally remote from each other and typically interact through a communication network (e.g., network 105). The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, the server sends data (e.g., data packets representing digital components) to the client device (e.g., for the purpose of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., results of user interactions) can be received at the server from the client device (e.g., received by the data processing system 102 from the client computing device 104).
Although operations are depicted in the drawings in a particular order, such operations are not required to be performed in the particular order shown or in sequential order, and not all illustrated operations are required to be performed. The acts described herein may be performed in a different order.
The separation of the various system components does not require separation in all embodiments, and the described program components can be included in a single hardware or software product. For example, NLP component 114 or interface management component 135 can be a single component, app, or program, or a logic device having one or more processing circuits, or a portion of one or more servers of data processing system 102.
Having now described a few illustrative embodiments, it is to be understood that the foregoing is illustrative rather than limiting and that the foregoing is illustrative. In particular, while many of the examples presented herein relate to a particular combination of method acts or system elements, those acts and those elements may be combined in other ways to achieve the same objectives. The discussed acts, elements, and features are combined with one embodiment and are not intended to exclude similar functionality in other embodiments.
The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of "including," "comprising," "having," "containing," "involving," "characterized by" and variations thereof herein is meant to encompass the items listed thereafter and equivalents thereof as well as additional items and alternative embodiments that consist exclusively of the items listed thereafter. In one embodiment, the systems and methods described herein consist of one, each combination of more than one, or all of the elements, acts or components.
Any reference to an embodiment or element or act of a system and method herein in the singular can also encompass embodiments comprising a plurality of such elements, and any reference to any embodiment or element or act herein in the plural can also encompass embodiments comprising only a single element. Singular or plural references are not intended to limit the disclosed systems or methods, their components, acts or elements to a single or multiple configurations. References to any act or element based on any information, act or element may include embodiments in which the act or element is based at least in part on any information, act or element.
Any embodiment disclosed herein may be combined with any other embodiment or embodiments, and references to "an embodiment," "some embodiments," "one embodiment," etc., are not necessarily mutually exclusive and are intended to indicate that a particular feature, structure, characteristic described in connection with the embodiment may be included in at least one embodiment or example. Thus, the terms as used herein do not necessarily all refer to the same embodiment. Any embodiment may be combined with any other embodiment, inclusive or exclusive, in any manner consistent with aspects and embodiments disclosed herein.
Reference to "or" may be construed as inclusive such that any term that "or" describes may indicate any one of the singular, more than one, and all of the stated terms. For example, a reference to "at least one of a and B" can include only "a", only "B" and both "a" and "B". Such references, used in conjunction with "comprising" or other open language, can include additional items.
Where technical features in the figures, detailed description, or any claim are followed by reference signs, those reference signs have been included to improve understanding of the figures, detailed description, and claims. Accordingly, the presence or absence of a reference numeral does not have any limiting effect on the scope of any claim element.
The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. For example, when an application is launched, the computing device 104 can generate and forward the packaged data object to a third party application. The above embodiments are illustrative and not limiting of the systems and methods described. The scope of the systems and methods described herein is, therefore, indicated by the appended claims rather than by the foregoing description, and all changes which come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.
Claims (20)
1. A system for generating a voice-based interface in a networked system, comprising:
A natural language processor component, executed by the data processing system via the interface, for receiving an input audio signal detected by a sensor of the client device;
the natural language processor component is configured to:
parsing the input audio signal to identify a primary request in the input audio signal;
determining a confidence score for the semantic meaning of the primary request; and
Determining a plurality of candidate requests based on past requests received by the data processing system in response to the confidence score being below a predetermined threshold, each candidate request of the plurality of candidate requests corresponding to a different semantic meaning of the input audio signal;
an interface management component, executed by the data processing system, for determining an interface type of the client device;
the interface management component is to select a number of the plurality of candidate requests based on semantic similarity to the primary request, the number based on an interface type of the client device, and the number of the plurality of candidate requests corresponding to a subset of the plurality of candidate requests;
A direct action application programming interface, executed by the data processing system, for generating a first action data structure for a first candidate request of the subset of the plurality of candidate requests and generating a second action data structure for a second candidate request of the subset of the plurality of candidate requests; and
The interface is used for:
Transmitting the first action data structure for the first candidate request of the subset of the plurality of candidate requests to the client device based on the interface type of the client device; and
A notification is sent to the client device based on unselected candidate requests of the plurality of candidate requests.
2. The system of claim 1, wherein the interface type indicates at least one of a display format of the client device, a display size of the client device, a display availability of the client device, or a client device type.
3. The system of claim 1, comprising:
The interface is to send the second action data structure for the second candidate request to the client device to be rendered after the first action data structure.
4. The system of claim 1, comprising:
The natural language processor component is for identifying a term in the input audio signal having a plurality of interpretations; and
The interface management component is to select the number of the plurality of candidate requests based on the term having a plurality of interpretations in the input audio signal.
5. The system of claim 1, comprising:
The natural language processor component is configured to select the plurality of candidate requests from a log of previously received input audio signals based at least on semantic similarity between each candidate request of the plurality of candidate requests and the primary request.
6. The system of claim 1, comprising:
The direct action application programming interface is to include an indication of the plurality of candidate requests in the first action data structure.
7. The system of claim 1, comprising:
A content selection component, executed by a data processing system, for selecting a digital component based on the primary request in the input audio signal.
8. The system of claim 1, comprising:
The direct action application programming interface is for determining a response to the primary request;
The direct action application programming interface is for determining a response to each candidate request of the plurality of candidate requests; and
The interface management component is to select the number of the plurality of candidate requests based on a comparison between a response to the request and a response to each of the plurality of candidate requests.
9. The system of claim 1, comprising:
The direct action application programming interface is for determining a response to the primary request;
The natural language processor component is for determining a popularity score of the response to the primary request in the input audio signal;
the natural language processor component is configured to determine a popularity score for a response to each candidate request of the plurality of candidate requests; and
The interface management component is to select the number of the plurality of candidate requests based on a comparison of a popularity score of a response to the request in the input audio signal to a popularity score of a response to each of the plurality of candidate requests.
10. The system of claim 1, comprising:
the interface is used for sending an audio signal request comprising a prompt;
the natural language processor component is for receiving a second input audio signal from the client device, the second input audio signal generated in response to the prompt; and
The interface management component is to select the number of the plurality of candidate requests based on the second input audio signal.
11. The system of claim 1, comprising:
The interface management component is configured to select a second client device associated with the client device, the second client device having an interface type that is different from an interface type of the client device; and
The interface is to send the second action data structure to the second client device.
12. The system of claim 1, comprising:
the natural language processor component is for receiving a second input audio signal detected by the sensor of the client device;
The natural language processor component is to parse the second input audio signal to identify candidate requests based on the second input audio signal; and
The interface is to send the candidate request to the client device.
13. A method for generating a voice-based interface in a networked system, comprising:
Receiving, by a natural language processor component executed by a data processing system via an interface, an input audio signal detected by a sensor of a client device;
Parsing, by the natural language processor component, the input audio signal to identify a primary request in the input audio signal;
determining, by the natural language processor component, a confidence score for the semantic meaning of the primary request;
Responsive to the confidence score being below a predetermined threshold, determining, by the natural language processor component, a plurality of candidate requests based on past requests received by the data processing system, each candidate request of the plurality of candidate requests corresponding to a different semantic meaning of the input audio signal;
Determining, by an interface management component executed by the data processing system, an interface type of the client device;
Selecting, by the interface management component, a number of the plurality of candidate requests based on semantic similarity to the primary request, the number based on the interface type of the client device;
Generating, by a direct action application programming interface of the data processing system, an action data structure for each candidate request of the number of candidate requests based on the interface type of the client device; and
Transmitting, by the interface, an action data structure for each of the number of the plurality of candidate requests to the client device based on the interface type of the client device and a notification to the client device based on unselected ones of the plurality of candidate requests.
14. The method of claim 13, wherein the interface type indicates at least one of a display format of the client device, a display size of the client device, a display availability of the client device, or a client device type.
15. The method of claim 13, comprising:
identifying, by the natural language processor component, a term in the input audio signal having a plurality of interpretations; and
The number of the plurality of candidate requests is selected by the interface management component based on the term having a plurality of interpretations in the input audio signal.
16. The method of claim 13, comprising:
The plurality of candidate requests are selected by the natural language processor component from a log of previously received input audio signals based at least on semantic similarity between each candidate request of the plurality of candidate requests and the primary request.
17. The method of claim 13, comprising:
Determining, by the direct action application programming interface, a response to the primary request;
determining, by the direct action application programming interface, a response to each candidate request of the plurality of candidate requests; and
The number of the plurality of candidate requests is selected by the interface management component based on a comparison between a response to the primary request and a response to each of the plurality of candidate requests.
18. The method of claim 13, wherein the notification includes additional information viewable on another device and an indication of a possible response.
19. The method of claim 18, further comprising generating, by the direct action application programming interface of the data processing system, an action data structure for at least one of the unselected candidate requests.
20. The method according to claim 13, wherein:
The past requests for determining the plurality of candidate requests are retrieved from a vectorized database including the past requests, and
The selecting the number of the plurality of candidate requests is further based on a distance between the primary request and each candidate request of the plurality of candidate requests in vector space.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/977,699 | 2018-05-11 | ||
US15/977,699 US11087748B2 (en) | 2018-05-11 | 2018-05-11 | Adaptive interface in a voice-activated network |
PCT/US2019/021387 WO2019216980A1 (en) | 2018-05-11 | 2019-03-08 | Adaptive interface in a voice-activated network |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110720098A CN110720098A (en) | 2020-01-21 |
CN110720098B true CN110720098B (en) | 2024-05-14 |
Family
ID=
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102137085A (en) * | 2010-01-22 | 2011-07-27 |  | Multi-dimensional disambiguation of voice commands |
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102137085A (en) * | 2010-01-22 | 2011-07-27 |  | Multi-dimensional disambiguation of voice commands |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11908462B2 (en) | Adaptive interface in a voice-activated network | |
KR102603717B1 (en) | Generation of domain-specific models in networked systems | |
US11514907B2 (en) | Activation of remote devices in a networked system | |
CN111279333B (en) | Language-based search of digital content in a network | |
CN111557002A (en) | Data transfer in a secure processing environment | |
CN110720098B (en) | Adaptive interface in voice activated networks | |
CN111213136B (en) | Generation of domain-specific models in networked systems | |
KR20230014680A (en) | Bit vector based content matching for 3rd party digital assistant actions |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant |