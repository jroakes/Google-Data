CN111815328A - Fraud prevention for payment instruments - Google Patents
Fraud prevention for payment instruments Download PDFInfo
- Publication number
- CN111815328A CN111815328A CN202010637441.8A CN202010637441A CN111815328A CN 111815328 A CN111815328 A CN 111815328A CN 202010637441 A CN202010637441 A CN 202010637441A CN 111815328 A CN111815328 A CN 111815328A
- Authority
- CN
- China
- Prior art keywords
- risk
- instrument
- machine learning
- computer
- tool
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q20/00—Payment architectures, schemes or protocols
- G06Q20/38—Payment protocols; Details thereof
- G06Q20/385—Payment protocols; Details thereof using an alias or single-use codes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
- G06N20/20—Ensemble learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q20/00—Payment architectures, schemes or protocols
- G06Q20/08—Payment architectures
- G06Q20/085—Payment architectures involving remote charge determination or related payment systems
- G06Q20/0855—Payment architectures involving remote charge determination or related payment systems involving a third party
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q20/00—Payment architectures, schemes or protocols
- G06Q20/30—Payment architectures, schemes or protocols characterised by the use of specific devices or networks
- G06Q20/34—Payment architectures, schemes or protocols characterised by the use of specific devices or networks using cards, e.g. integrated circuit [IC] cards or magnetic cards
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q20/00—Payment architectures, schemes or protocols
- G06Q20/30—Payment architectures, schemes or protocols characterised by the use of specific devices or networks
- G06Q20/36—Payment architectures, schemes or protocols characterised by the use of specific devices or networks using electronic wallets or electronic money safes
- G06Q20/367—Payment architectures, schemes or protocols characterised by the use of specific devices or networks using electronic wallets or electronic money safes involving electronic purses or money safes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q20/00—Payment architectures, schemes or protocols
- G06Q20/38—Payment protocols; Details thereof
- G06Q20/40—Authorisation, e.g. identification of payer or payee, verification of customer or shop credentials; Review and approval of payers, e.g. check credit lines or negative lists
- G06Q20/401—Transaction verification
- G06Q20/4016—Transaction verification involving fraud or risk level assessment in transaction processing
Abstract
Preventing fraud or abuse associated with payment instruments includes a processor for training a machine learning process based on historical data related to interactions with the instruments. The processor trains the machine learning process based on historical data related to interactions of the tool and tool issuer with counter-parties and users. The processor receives a request for a fraudulent risk assessment tool and inputs the accessed data into a machine learning process. The processor determines, based on a machine learning process, a first risk score based on a likelihood that a tool issuer will transfer funds to issue a ticket and a second risk score based on a likelihood that the tool issuer will initiate a rebate. The processor determines that a combination of the first and second risk scores is above a configured threshold and indicates that the requestor is not interacting with the tool.
Description
Technical Field
The present disclosure relates to preventing fraud or abuse associated with issuers of payment instruments. More specifically, the machine learning model is trained and utilized to determine whether a principal in an interaction with a payment instrument class, such as a transaction, has an elevated risk of not completing the interaction or of the interaction being revoked.
Background
In conventional systems, a processing system evaluates a particular user at the time of an interaction to determine whether the interaction is at high risk based on the user history with respect to the particular tool or other tools. Interactions that are deemed to have an elevated risk of fraud may be rejected or sent for further evaluation. When the tool is denied an interaction, the interaction may be delayed or terminated while the appropriate tool is identified. When a user application is associated with a tool, the issuer of the tool analyzes the user and the history of user interactions and determines whether the user is considered to have an increased risk of fraud.
Disclosure of Invention
The technology herein provides a computer-implemented method to prevent fraud or abuse associated with payment instruments from an instrument issuer. The method includes a processor for training a machine learning process based on historical data relating to tool interactions (e.g., transactions) with counter-parties and users. The processor receives a request for a fraudulent risk assessment tool and inputs the accessed data into a machine learning process. The processor determines, based on a machine learning process, a first risk score based on a likelihood that the instrument will transfer (transmit) funds to invoice (invoice) and a second risk score based on a likelihood that the instrument issuer will initiate a chargeback (chargeback). The processor determines that a combination of the first and second risk scores is greater than a configured threshold and indicates that the requestor is not interacting with the tool.
In certain other example aspects described herein, systems and computer program products are provided that prevent fraud or abuse associated with tools.
These and other aspects, objects, features and advantages of the exemplary embodiments will become apparent to those skilled in the art upon consideration of the following detailed description of illustrative exemplary embodiments.
Drawings
Fig. 1 is a block diagram depicting a system for preventing fraud associated with an instrument, according to some examples.
Fig. 2 is a block flow diagram depicting a method of preventing fraud associated with an instrument, according to some examples.
Fig. 3 is a block flow diagram depicting a method via a machine learning model analysis tool, according to some examples.
Fig. 4 is a block diagram depicting computing machines and modules, according to some examples.
Detailed Description
SUMMARY
In some examples, a machine learning algorithm, process, software, or other machine learning system is trained and utilized to analyze a payment instrument to determine whether the instrument has an elevated risk of fraud. If the payment instrument is determined to pose an elevated risk of fraud or is otherwise determined not to be an appropriate payment instrument based on the identified factors and characteristics, the evaluation system will reject the instrument. If the payment instrument is an appropriate payment instrument based on the history of the situation and access, the evaluation system approves the instrument for intended use. Throughout the specification, a payment instrument may alternatively be referred to as an instrument and a transaction may be referred to as an interaction. The instrument may be a credit card, debit card, pre-paid card, or any other suitable type of instrument.
The evaluation of the tool is dependent on the category or type of the tool itself and the issuer of the tool. The evaluation is event-independent, such that the evaluation can be made at any time during the interaction process and by any suitable party to the interaction process. Unlike many transaction-specific fraud ratings, the ratings are not related to the user history, user computing device history, merchant history, or any other party interacting with, other than the instrument and the issuer of the instrument, although the introduction of these factors does not change the novelty and can be used as desired by the system administrator or other interested party. The tool being evaluated may be a type of tool from a particular tool issuer. For example, the category of tools may include all tools from the issuer of the tool that provide a certain reward program or a certain credit line. The categories of tools may include all tools from a tool issuer that includes a particular plan with a particular merchant.
In one example, the instrument is a payment instrument, such as a credit card, debit card, store card, prepaid card, loyalty card, identification card, or any other suitable instrument. The tool issuer is a bank or other organization that issues tools to users for use in the interaction. The tool being evaluated may be a type of tool from a particular tool issuer. The category of tools may include all tools from the issuer of the tool that provide a certain reward program or a certain credit line. The categories of tools may include all tools from a tool issuer that includes a particular plan with a particular merchant. Alternatively, the tool is a specific instance of the tool that is issued to the user.
The interaction is performed with a digital application on the user computing device. The digital application may be a digital wallet or similar application that the user employs to manage payment instruments and other instruments. The interaction may be a payment transaction, but other types of interactions may be used, such as check-in, access authorization, ticket display, or any other suitable interaction.
In the examples described herein, the instrument will be described as a payment instrument or class of payment instruments, the digital application will be described as a digital wallet, and the interaction will be described as a transaction. These examples are for illustration.
Any party interacting may make event-independent requests to evaluate tools for increased risk of fraud or abuse. Fraud or abuse includes the risk that the transaction is not completed because funds from the transaction are not provided or because the transaction is withdrawn at some later time. Although a revoked transaction, such as a "refund," may not be fraudulent, repeated refunds may be an indication of abuse. Repeated chargeback, whether intentional fraud or mere abuse (collectively referred to herein as "fraud"), can cost parties transaction time and money to process and is undesirable. When a publisher is associated with a potentially fraudulent user, is itself fraudulent, or has policies and procedures that create environments with elevated risks of fraud and abuse, then reasonable parties will avoid interacting with the publisher.
Any suitable principal may host the machine learning processor to analyze the tool or make a request to the machine learning processor. For example, a card network may wish to analyze a tool or tool issuer and the interaction of the tool issuer with opposing parties and users. Alternatively, a digital wallet system, merchant, user, or any other suitable party may wish to analyze the tool or the tool issuer.
The machine learning processor may be a supervised machine learning processor, such as a gradient boosting Decision Tree ("GBDT"). Other machine learning processors may be used in alternative examples. GBDT is used in the examples herein to represent a machine learning processor, algorithm, or other machine learning hardware or software.
The GBDT is trained based on data specifically related to the tool issuer and the tool being issued. The data may be collected from a card network, a digital wallet application, user history, merchant data, or any other suitable data that may help quantify and characterize the tool and tool issuer (e.g., the tool issuer's interactions with the counter-party and user). In a supervised learning environment, an operator provides the GBDT with training data containing issuer-related inputs/predictors, and then provides the GBDT with a preferred conclusion based on that data. GBDTs are capable of recognizing and learning patterns from data inputs. An alternative machine learning technique or algorithm may analyze unsupervised data to search for rules, detect patterns, and summarize and group data associated with a tool. Any suitable machine learning process, algorithm, or system may be used to learn the data.
When an appropriate principal has an event that would require interaction with the tool or the tool issuer, the principal requests an evaluation of the tool to determine if the interaction is at an elevated risk of not being completed or revoked. The principal communicates data associated with the request to the evaluation system or any system hosting the GBDT.
The GBDT receives data including user history about the instrument, history of the issuer of the instrument, merchant interaction with the instrument, card network interaction with the issuer, chargebacks associated with the issuer, signals from other banks and payment processing systems associated with the issuer, and any other suitable data associated with the issuer. Data is entered into the GBDT to allow the GBDT to learn about the tool to enable a more accurate assessment of the tool's performance. For example, the GBDT determines whether the tool issuer is likely to be fraudulent, involves excessive chargebacks, is difficult to use, pays the ticket slowly, or is otherwise at risk for interaction with the tool issuer in any other way.
The GBDT may perform two analyses. The first analysis is to determine whether the issuer of the instrument is likely to complete the transaction and transfer the required funds. The GBDT may provide a model or prediction of the likelihood that the issuer will slowly or never pay the ticket or other fee that the issuer agrees to pay. The second analysis is to determine if the issuer of the instrument is likely to revoke or rebate a completed transaction. If either of these results is possible, the risk of conducting a transaction with the issuer is elevated.
The risk threshold is determined by the user, the digital wallet system, the digital wallet, the payment processing system, the card network, or any suitable party wishing to reduce fraudulent transactions. If the risk is greater than the threshold, the evaluator system recommends that the tool not be used for the current function. If the risk is less than or equal to the threshold, the evaluator system recommends that the tool be used for the current function.
Users, card networks, digital wallets, merchants, and other parties can be better protected from fraud and abuse of unsecured tools from the tool issuer by using and relying on the methods and systems, evaluator systems, described herein. The current evaluation is directed to the user history or other user interaction with the counterparty. By performing the risk analysis in a manner that is focused on the issuer of the tool, the evaluation allows other parties interacting to make informed decisions about the issuer and avoids fraud and abuse. When a publisher is associated with a potentially fraudulent user, is itself fraudulent, or has policies and procedures that create environments with elevated risks of fraud and abuse, then reasonable parties will avoid interacting with the publisher. Performing risk analysis using machine learning allows more data to be processed and more insight into the risk of the tool to be learned than would be allowed by human or typical database analysis. Machine learning will become more and more skilled in assessing tool risk as more data is acquired.
Example System architecture
Turning now to the drawings, wherein like reference numerals refer to like (but not necessarily identical) elements throughout the various views, example embodiments are described in detail.
Fig. 1 is a block diagram depicting a system 100 for preventing fraud associated with a tool issuer 130.
As shown in fig. 1, system 100 includes network computing devices/ systems 110, 120, 130, and 140 that are configured to communicate with each other via one or more networks 105 or via any suitable communication technology.
Each network 105 includes wired or wireless telecommunication means by which network devices (including devices 110, 120, 130, and 140) can exchange data. For example, each network 105 may include a local area network ("LAN"), a wide area network ("WAN"), an intranet, the internet, a mobile telephone network, a Storage Area Network (SAN), a Personal Area Network (PAN), a Metropolitan Area Network (MAN), a Wireless Local Area Network (WLAN), a Virtual Private Network (VPN), a cellular or other mobile communication network, bluetooth, NFC, or any combination of these or any other suitable architecture or system that facilitates communication of signals, data. Throughout the discussion of the example embodiments, it should be understood that the terms "data" and "information" are used interchangeably herein to refer to text, images, audio, video, or any other form of information that may be present in a computer-based environment. The communication technology utilized by devices 110, 130, and 140 may be a network similar to network 105 or an alternative communication technology.
Each network computing device/ system 110, 120, 130, and 140 includes a computing device having a communication module capable of sending and receiving data over the network 105 or similar network. For example, each network device 110, 120, 130, and 140 may include a server, a desktop computer, a laptop computer, a tablet computer, a television having one or more processors embedded therein and/or coupled thereto, a smart phone, a handheld or wearable computer, a Personal Digital Assistant (PDA), a wearable device such as a smart watch or glasses, or any other wired or wireless processor-driven device. In the example embodiment shown in fig. 1, network devices 110, 120, 130, and 140 are operated by an end user or consumer, a credit card network operator, an issuer system operator, and an evaluation system operator, respectively.
The user computing device 110 includes a user interface 114. User interface 114 may be used to display a graphical user interface and other information to user 101 to allow user 101 to interact with evaluation system 140 and others. The user interface 114 receives user input for displaying the digital wallet 112 and other applications.
The user computing device 110 also includes a data storage unit 113 that is accessible by a communication application (not shown) and one or more applications, such as a digital wallet 112. The example data storage unit 113 may include one or more tangible computer-readable storage devices. The data storage unit 113 may be stored on the user computing device 110 or may be logically coupled to the user computing device 110. For example, the data storage unit 113 may include on-board flash memory and/or one or more removable memory accounts or removable flash memory. In certain embodiments, the data storage unit 113 may reside in a cloud-based computing system.
The digital wallet application 112 may encompass any application, hardware, software, or process that the user computing device 110 may employ to assist the user 101 in completing a purchase transaction or other interaction. The digital wallet application module 112 may interact with a communication application, such as a web browser, or may be implemented as a companion application to the communication application. The digital wallet 112 may be provided by the digital wallet system to the user computing device 110 or otherwise associated with the digital wallet system. The digital wallet system may manage the operation, updates, and other functions of the digital wallet 112.
The example evaluation system 140 includes an evaluation system server 145, a data storage unit 147, and a machine learning computing system, such as a gradient boosting decision tree ("GBDT") 143.
In an example embodiment, evaluation system server 145 communicates with credit card network 120, issuer system 130, user computing device 110, or other systems over network 105 to request and receive data related to card instruments, transactions, interactions, and other appropriate data. Digital evaluation system 140 may provide data to a payment processing system (not shown) or credit card network 120 in real-time to facilitate the transaction.
In an example embodiment, data storage unit 147 can comprise any local or remote data storage structure suitable for storing information accessible to evaluation system 140. In an example embodiment, the data storage unit 147 stores encrypted information.
GBDT143 represents any type of neural network computing system or other computing system that employs any machine learning process or algorithm. The GBDT143 is capable of receiving data from many various sources and using that data to interpret patterns and characterize the user 101, the instrument, the issuer 130, and others involved in the transaction process. The GBDT143 can continuously or periodically update the received information as follows: this approach allows the data presented by the evaluation system 140 to become more useful and accurate as more data is received and stored. The GBDT143 may be a function or computing device of the evaluation system 140 that is used by the evaluation system 140 to perform some or all of the functions described herein as being performed by the evaluation system 140 or the evaluation system server 145.
Alternatively, the GBDT143 may be hosted by a third party system, the digital wallet 112, or any other suitable host. GBDT143 represents an example of a machine learning processor or algorithm. Any other suitable process may be used, such as a different supervised learning process, an unsupervised learning process, or reinforcement learning.
The issuer system 130 may be a bank or other institution that issues tools 131 such as credit cards, debit cards, prepaid cards, and other tools. In an example, the card issuer system 130 approves a credit card application, sets terms for a user, issues physical and digital cards, and funds for a transaction. The tools 131 being evaluated may be a class of tools 131 from a particular tool issuer 130. For example, the category of tools 131 may include all tools from a tool issuer that offers a certain reward program or a certain credit line. The category of tools 131 may include all tools from a tool issuer 130 that includes a particular plan with a particular merchant. In other examples, the tools are specific instances of the tools 131 issued to the user 101.
It will be appreciated that the network connections shown are examples and other means of establishing a communications link between the computer and the device may be used. Additionally, persons of ordinary skill in the art having benefit of the present disclosure will appreciate that the issuer system 130, credit card network 120, evaluation system 140, and user computing device 110 shown in FIG. 1 may have any of a number of other suitable computer system configurations. For example, the user computing device 110 may be implemented as a mobile phone or a handheld computer, and may not include all of the components described above.
In an example embodiment, the network computing device and any other computing machines associated with the techniques presented herein may be any type of computing machine, such as, but not limited to, those discussed in more detail with reference to fig. 4. Further, any functions, applications, or components associated with any of these computing machines, such as those described herein or any other associated with the techniques presented herein (e.g., scripts, web content, software, firmware, hardware, or modules), may be any of the components discussed in more detail with reference to fig. 4. The computing machines discussed herein may communicate with each other, as well as with other computing machines or communication systems over one or more networks, such as network 105. The network 105 may include any type of data or communication network, including any of the network technologies discussed with reference to fig. 4.
Example procedure
The example methods illustrated in fig. 2-3 are described below with respect to components of the example operating environment 100. The example methods of fig. 2-3 may also be performed with other systems and in other environments. The operations described with respect to any of fig. 2-3 may be implemented as executable code stored on a computer-or machine-readable non-transitory tangible storage medium (e.g., floppy disks, hard disks, ROMs, EEPROMs, non-volatile RAM, CD-ROMs, etc.), which is completed based on execution of the code by processor circuitry implemented with one or more integrated circuits; the operations described herein may also be implemented as executable logic (e.g., a programmable logic array or device, a field programmable gate array, programmable array logic, an application specific integrated circuit, etc.) encoded in one or more non-transitory tangible media for execution.
Fig. 2 is a block flow diagram depicting a method 200 of preventing fraud associated with an instrument 131, according to some example embodiments.
At block 210, the evaluation system 140 receives input from the evaluation tool 131. In this example, the evaluation system 140 is indicated as a separate entity, but the functions of the evaluation system 140 may be performed by any suitable party hosting the GBDT143, such as the credit card network 120 or a third party.
Any party interacting may make event-independent requests to evaluate the tool 131 from the issuer system 130 for an increased risk of fraud or abuse of the tool 131 associated with the issuer system 130. For example, the digital wallet 112 may communicate freely with the evaluation system 140 over an internet connection or other connection to request evaluation before accepting the tools 131 associated with the issuer system 130. The credit card network 120 may communicate the request to the evaluation system 140 before allowing the issuer system 130 to use the credit card network 120 for credit transactions. A merchant system (not shown) may communicate the request to evaluation system 140 before allowing issuer system 130 to conduct a transaction at the merchant location.
The requestor communicates the request to the evaluation system 140 via any suitable technique, such as a network connection over the internet. The request may include an identification of the issuer system 130, the particular tool 131, the purpose of the request, and any other suitable information.
At block 220, if the request is for a particular tool 131 or category of tool 131, the evaluation system 140 determines the issuer system 130 of the tool. In an example, the evaluation system 140 analyzes a tool identification number, metadata associated with the tool 131, ancillary data associated with the tool 131, or any other suitable data for identifying the issuer system 130. Any suitable manner of determining the issuer system 130 of the tool may be used.
At block 230, the evaluation system 140 analyzes the tool issuer 130 and the tool 131 via machine learning algorithms. The details of block 230 are described in more detail with reference to method 230 of fig. 3.
Fig. 3 is a block flow diagram depicting a method of analyzing tool issuers 130 and tools 131 via a machine learning algorithm, processor, model, or other machine learning process, according to some examples. Any type of machine learning algorithm, processor, model, or other machine learning process may be represented herein by the term machine learning processor or alternatively by any one of the terms algorithm, processor, model, or other machine learning process.
At block 310, the evaluation system 140 trains the machine learning processor based on a history of interactions of the plurality of existing tools 131 and the issuer 130 with the plurality of users, networks, merchants, and others. The evaluation system 140 trains the machine learning processor with data regarding credit card reliability, fraud, rebates, reputation, ease of use, and other suitable factors from any available source. In particular, the evaluation system 140 trains the processor to identify whether the issuer system 130 poses an elevated risk that the transaction is not completed, for example because funds from the transaction are not provided or because the transaction is later withdrawn at some time.
In an example, the machine learning processor is a supervised machine learning processor, such as a gradient boosting decision tree ("GBDT") 143. Other machine learning processors may be used in alternative examples. GBDT143 is used in the examples herein to represent a machine learning processor, algorithm, or other machine learning hardware or software. The GBDT143 may be hosted by a third party system, the digital wallet 112, or any other suitable host. GBDT143 represents an example of a machine learning process or algorithm. Any other suitable process may be used, such as a different supervised learning process, an unsupervised learning process, or reinforcement learning.
GBDT143 represents any type of neural network computing system or other computing system that employs any machine learning process or algorithm. The GBDT143 is capable of receiving data from many various sources and using that data to interpret patterns and characterize the user 101, the tool 131, the issuer system 130, and others involved in the transaction process. The GBDT143 can continuously or periodically update the received information as follows: this approach allows the data presented by the evaluation system 140 to become more useful as more data is received and stored. The GBDT143 may be a function or computing device of the evaluation system 140 that is used by the evaluation system 140 to perform some or all of the functions described herein as being performed by the evaluation system 140 or the evaluation system server 145.
The GBDT143 is trained based on data from the tool issuer system 130, the credit card network 120, the digital wallet application 112, merchant data, or any other suitable data that may help quantify and characterize the tool 131 and the tool issuer 130. In a supervised learning environment, the operator provides the GBDT143 with training data containing issuer-related inputs/predictors, and then provides the GBDT143 with preferred conclusions based on the data. The GBDT143 is capable of recognizing and learning patterns from data inputs. An alternative machine learning technique or algorithm may analyze unsupervised data to search for rules, detect patterns, and summarize and group data associated with a tool. Any suitable machine learning process, algorithm, or system may be used to learn the data.
In block 320, the evaluation system 140 receives input of data associated with the requesting issuer system 130. The data may be collected from any suitable source, such as a merchant, credit card network 120, financial institution, payment processing network, or other source. This data may be specific to the issuer system 130, with the results of previous interactions. The evaluation system 140 inputs the received data into the GBDT 143. This data is entered into the GBDT143 to allow the GBDT143 to learn about the issuer system 130 to enable a more accurate assessment of the performance of the issuer system 130.
In block 330, the GBDT143 determines a likelihood that funds related to the interaction will be recovered from the issuer system 130. Based on the model, algorithm, decision tree, or other system used by the GBDT143, the GBDT143 analyzes the proposed tool and determines the rate at which the issuer system 130 will transfer funds to issue the instrument. The GBDT143 can predict a percentage likelihood of receiving funds, an estimate of how the issuer system 130 will compare to other issuers, or a rating based on any suitable scale.
In block 340, the GBDT143 determines a likelihood that the interaction will result in a chargeback from the issuer system 130. Based on the model, algorithm, decision tree, or other system used by the GBDT143, the GBDT143 analyzes the proposed tool 131 and determines the rate at which the issuer system 130 will submit a chargeback, request a chargeback, or otherwise revoke an interaction. The GBDT143 can predict a percentage likelihood, an estimate of how the issuer system 130 will compare to other issuers, or a rating based on any suitable scale.
In block 350, the GBDT143 determines a risk score for interacting with the issuer system 130. The risk score separates or combines the likelihood that the tool 131 will transfer the required funds, will likely encounter a large number of rebates, or will likely pose any other risk of fraud or abuse. The risk score may be configured to any suitable scale, such as a 0-100 scale, a letter grade, a bad to excellent Lickets scale, or any other suitable risk score scale. The scores for the different possibilities may be scored separately or combined into an overall risk score.
The risk threshold is determined by the user 101, the evaluation system 140, the digital wallet 112, the payment processing system, or any suitable party wishing to reduce fraudulent interactions. If the risk score is based on, for example, a 1-100 scale, the threshold may be set at an appropriate number, such as 70.
From block 350, the method 230 returns to block 240 of fig. 2.
Returning to FIG. 2, in block 240, the evaluation system 140 determines whether the risk score is below a threshold. An overall risk score may be used, or either or both of the individual risk scores may be used. For example, if the scale is 0-100, the threshold is 70, and the overall risk score is 50, then the risk score is below the threshold. If the risk score is not below the threshold, the method 230 follows the no path to block 250. In another example, to have the decision of block 240 follow the "yes" path, both individual risk scores must be below a threshold. That is, if either the risk score for the availability of funds for which the issuer system 130 is transferring the claim or the risk score for the likelihood of the issuer system 130 submitting too many returns is not below the threshold, block 240 follows the no path in turn.
In this example, a higher risk score means that the publisher system 130 is more likely to experience fraud or abuse. In an alternative example, a lower risk score means that the publisher system 130 is more likely to experience fraud or abuse. The use of the risk score will be adjusted accordingly.
In block 250, if the risk score is not below the threshold, the evaluation system 140 recommends not to interact with the tool 131. The evaluation system 140 provides a notification to the requestor indicating that the tool 131 has an increased risk of fraud or abuse. The requestor, in response to the notification, may attempt to add at some later time, choose to replace the issuer system, or perform any other suitable action. If evaluation system 140 is a requestor, evaluation system 140 may choose not to interact further with tool 131. For example, the evaluation system 140 does not allow the tool 131 to conduct transactions with the evaluation system 140.
If the risk score, or any combination of individual risk scores, is below the threshold, the method 230 follows the YES path to block 260.
In block 260, if the risk score (or any combination of risk scores) is below a threshold, the evaluation system 140 recommends interaction with the tool 131. The evaluation system 140 provides a notification to the requestor indicating that the issuer system 130 does not have an increased risk of fraud or abuse. The requestor may then interact with the tool 131 as desired. If the evaluation system 140 is a requestor, the evaluation system 140 may in turn interact with the issuer system 130. For example, the evaluation system 140, in turn, allows the issuer system 130 to conduct transactions with the evaluation system 140.
In block 270, any suitable party provides the results of the interaction to the machine learning algorithm for further training. The GBDT143 can improve the model or algorithm for future risk scores based on continuous or periodic updates to transactions of the user 101, the instrument 131, the credit card network 120, the card issuer 130, the merchant, or any other party. The GBDT143 can more accurately predict risk due to additional training material when subsequent requestors attempt to interact with the issuer system 130.
Example System
Fig. 4 depicts a computing machine 2000 and a module 2050, according to some example embodiments. The computing machine 2000 may correspond to any of the various computers, servers, mobile devices, embedded systems, or computing systems presented herein. The module 2050 may include one or more hardware or software elements configured to facilitate the computing machine 2000 in performing the various methods and processing functions presented herein. The computing machine 2000 may include various internal or attached components, such as a processor 2010, a system bus 2020, a system memory 2030, a storage medium 2040, an input/output interface 2060, and a network interface 2070 for communicating with a network 2080.
The computing machine 2000 may be implemented as a conventional computer system, an embedded controller, a laptop, a server, a mobile device, a smartphone, a wearable computer, a set-top box, a kiosk, an in-vehicle information system, one or more processors associated with a television, a customized machine, any other hardware platform, or any combination or plurality of these. The computing machine 2000 may be a distributed system configured to operate with multiple computing machines interconnected via a data network or bus system.
The system memory 2030 may include a non-volatile memory such as a read-only memory ("ROM"), a programmable read-only memory ("PROM"), an erasable programmable read-only memory ("EPROM"), a flash memory, or any other device capable of storing program instructions or data with or without applied power. System memory 2030 may also include volatile memory such as Random Access Memory (RAM), Static Random Access Memory (SRAM), Dynamic Random Access Memory (DRAM), and Synchronous Dynamic Random Access Memory (SDRAM). Other types of RAM may also be used to implement system memory 2030. The system memory 2030 may be implemented with a single memory module or multiple memory modules. While the system memory 2030 is depicted as being part of the computing machine 2000, those skilled in the art will recognize that the system memory 2030 may be separate from the computing machine 2000 without departing from the scope of the subject technology. It should also be appreciated that the system memory 2030 may include or operate in conjunction with a non-volatile storage device, such as the storage media 2040.
The storage medium 2040 may include a hard disk, a floppy disk, a compact disk read-only memory ("CD-ROM"), a digital versatile disk ("DVD"), a blu-ray disk, a magnetic tape, a flash memory, other non-volatile memory devices, a solid state drive ("SSD"), any magnetic storage device, any optical storage device, any electrical storage device, any semiconductor storage device, any physical based storage device, any other data storage device, or any combination or plurality of these. The storage media 2040 may store one or more operating systems, application programs, and program modules, such as modules 2050, data, or any other information. The storage medium 2040 may be part of the computing machine 2000 or connected to the computing machine 2000. The storage media 2040 may also be part of one or more other computing machines in communication with the computing machine 2000, such as a server, database server, cloud storage, network attached storage, and so forth.
The module 2050 may include one or more hardware or software elements configured to facilitate the computing machine 2000 in performing the various methods and processing functions presented herein. The module 2050 may include one or more sequences of instructions stored as software or firmware in association with the system memory 2030, the storage medium 2040, or both. The storage medium 2040 may thus represent an example of a machine or computer readable medium on which instructions or code may be stored for execution by the processor 2010. A machine or computer readable medium may generally refer to any medium or media used to provide instructions to processor 2010. Such machine or computer-readable media associated with the module 2050 may include a computer software product. It should be appreciated that a computer software product including the module 2050 may also be associated with one or more processes or methods for delivering the module 2050 to the computing machine 2000 via the network 2080, any signal-bearing medium, or any other communication or delivery technique. The module 2050 may also include hardware circuitry or information for configuring hardware circuitry, such as microcode or configuration information for an FPGA or other PLD.
The input/output ("I/O") interface 2060 may be configured to couple to one or more external devices to receive data from the one or more external devices and to transmit data to the one or more external devices. Such external devices, along with various internal devices, may also be referred to as peripheral devices. The I/O interface 2060 may include both electrical and physical connections for operatively coupling various peripheral devices to the computing machine 2000 or the processor 2010. The I/O interface 2060 may be configured to communicate data, addresses, and control signals between a peripheral device, the computing machine 2000, or the processor 2010. The I/O interface 2060 may be configured to implement any standard interface, such as a small computer system interface (small SCSI), serial-attached SCSI ("SAS"), fibre channel, peripheral component interconnect ("PCI"), PCI express (PCIe), serial bus, parallel bus, advanced technology attached ("ATA"), serial ATA ("SATA"), universal serial bus ("USB"), Thunderbolt, FireWire, various video buses, and the like. The I/O interface 2060 may be configured to implement only one interface or bus technology. Alternatively, the I/O interface 2060 may be configured to implement a plurality of interface or bus technologies. The I/O interface 2060 may be configured as part of the system bus 2020, all of the system bus 2020, or operate in conjunction with the system bus 2020. The I/O interface 2060 may comprise one or more buffers for buffering transmissions between one or more external devices, internal devices, the computing machine 2000, or the processor 2010.
The I/O interface 2060 may couple the computing machine 2000 to various input devices including a mouse, touch screen, scanner, electronic digitizer, sensor, receiver, touchpad, trackball, camera, microphone, keyboard, any other pointing device, or any combination of these. The I/O interface 2060 may couple the computing machine 2000 to various output devices including video displays, speakers, printers, projectors, haptic feedback devices, automation controls, robotic components, actuators, motors, fans, solenoids, valves, pumps, transmitters, signal transmitters, lights, and so forth.
The computing machine 2000 may operate in a networked environment using logical connections through a network 2080 to one or more other systems or computing machines via a network interface 2070. The network 2080 may include a Wide Area Network (WAN), a Local Area Network (LAN), an intranet, the internet, a wireless access network, a wired network, a mobile network, a telephone network, an optical network, or a combination of these. The network 2080 may be packet-switched, circuit-switched, have any topology, and may use any communication protocol. The communication links within network 2080 may involve various digital or analog communication media such as optical cables, free-space optics, waveguides, electrical conductors, wireless links, antennas, radio frequency communications, and so forth.
The processor 2010 may be coupled to the other elements of the computing machine 2000 or various peripheral devices discussed herein by a system bus 2020. It is to be appreciated that the system bus 2020 can be internal to the processor 2010, external to the processor 2010, or both. According to some embodiments, the processor 2010, other elements of the computing machine 2000, or any of the various peripherals discussed herein may be integrated into a single device, such as a system on chip ("SOC"), a system on package ("SOP"), or an ASIC device.
In situations where the system discussed herein collects personal information about a user, or where personal information is available, the user may be provided with the opportunity to: control whether programs or features collect user information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current location), or control whether and/or how to receive content from a content server that may be more relevant to the user. In addition, certain data may be processed in one or more ways to remove personally identifiable information before it is stored or used. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized (e.g., to a city, zip code, or state level) if location information is obtained such that a particular location of the user cannot be determined. Thus, the user may have control over how information is collected about the user and used by the content server.
Embodiments may include computer programs implementing the functions described and illustrated herein, where the computer programs are implemented in a computer system including instructions stored in a machine-readable medium and a processor executing the instructions. It should be apparent, however, that there are many different ways to implement embodiments in computer programming, and embodiments should not be construed as limited to any one set of computer program instructions. In addition, a skilled programmer would be able to write such a computer program to implement the embodiments of the disclosure based on the accompanying flow charts and associated description in the application text. Therefore, disclosure of a particular set of program code instructions is not considered necessary for an adequate understanding of how to make and use the embodiments. In addition, those skilled in the art will appreciate that one or more aspects of the embodiments described herein may be performed by hardware, software, or a combination thereof, as may be implemented in one or more computing systems. Additionally, any reference to an action being performed by a computer should not be construed as being performed by a single computer, as more than one computer may perform the action.
The example embodiments described herein may be used with computer hardware and software that performs the previously described methods and processing functions. The systems, methods, and processes described herein may be implemented in a programmable computer, computer-executable software, or digital circuitry. The software may be stored on a computer readable medium. For example, the computer readable medium may include a floppy disk, a RAM, a ROM, a hard disk, a removable media, a flash memory, a memory stick, an optical media, a magneto-optical media, a CD-ROM, and so forth. Digital circuits may include integrated circuits, gate arrays, building block logic, Field Programmable Gate Arrays (FPGA), and the like.
The example systems, methods, and acts described in the previously presented embodiments are illustrative, and in alternative embodiments, certain acts may be performed in a different order, performed in parallel with each other, omitted entirely, and/or combined between different example embodiments, and/or certain additional acts may be performed without departing from the scope and spirit of the various embodiments. Accordingly, such alternative embodiments are included in the inventions described herein.
Although specific embodiments have been described in detail above, the description is for illustration only. It should be understood, therefore, that many of the aspects described above are not intended as required or essential elements unless explicitly stated otherwise. Modifications of the disclosed aspects of the example embodiments, in addition to those described above, and equivalent components or actions corresponding to the disclosed aspects of the example embodiments, may also be made by those having ordinary skill in the art having the benefit of the present disclosure without departing from the spirit and scope of the embodiments as defined by the following claims, the scope of which is to be accorded the broadest interpretation so as to encompass such modifications and equivalent structures.
Claims (12)
1. A computer-implemented method to prevent fraud or abuse associated with a category of payment instruments, comprising:
by one or more computing devices:
receiving a request to evaluate a payment instrument for a risk of fraud;
determining a first risk of interacting with the instrument, the first risk being based on a likelihood that an instrument issuer associated with the instrument will transfer funds to open a ticket;
determining a second risk of interaction with the payment instrument, the second risk being based on a likelihood that the instrument issuer associated with the instrument will initiate a chargeback;
determining that a combination of the first risk and the second risk is above a configured threshold; and is
Indicating that the requestor does not interact with the payment instrument.
2. The computer-implemented method of claim 1, further comprising:
training a machine learning process based on historical data related to interactions of the payment instrument with counterparties and users; and is
Inputting the accessed data into the machine learning process,
wherein determining a first risk of interacting with the payment instrument and determining a second risk of interacting with the payment instrument are based on the machine learning process.
3. The computer-implemented method of claim 1, further comprising:
receiving a request to evaluate a second payment instrument for risk of fraud;
determining a third risk of interaction with the second payment instrument, the third risk being based on a likelihood that an instrument issuer associated with the second instrument will transfer funds to issue a ticket;
determining a fourth risk of interaction with the payment instrument, the fourth risk being based on a likelihood that the instrument issuer associated with the instrument will initiate a chargeback;
determining that a combination of the third risk and the fourth risk is below a configured threshold; and is
Instructing the requestor to interact with the second payment instrument.
4. The computer-implemented method of claim 3, further comprising utilizing, by the requestor, the second payment instrument in a subsequent interaction.
5. The computer-implemented method of claim 1, further comprising:
determining that either of the first risk score and the second risk score is above a second threshold; and is
Indicating that the requestor does not interact with the payment instrument.
6. The computer-implemented method of claim 1, wherein the threshold is configured by one or more of a user, a payment processing system, and a card network.
7. The computer-implemented method of claim 1, wherein the higher risk is an indication of a higher likelihood associated with fraud for the payment instrument.
8. The computer-implemented method of claim 1, wherein the lower risk is an indication of a lower likelihood associated with fraud for the payment instrument.
9. The computer-implemented method of claim 2, further comprising providing, by the requestor, results of subsequent interactions of the payment instrument to the machine learning process to further train the machine learning process.
10. The computer-implemented method of claim 2, wherein the machine learning process is a supervised machine learning model.
11. The computer-implemented method of claim 2, wherein the machine learning process is a gradient boosting decision tree.
12. The computer-implemented method of claim 2, wherein the machine learning process is an unsupervised machine learning model.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/503,949 | 2019-07-05 | ||
US16/503,949 US20210004809A1 (en) | 2019-07-05 | 2019-07-05 | Fraud prevention for payment instruments |
Publications (1)
Publication Number | Publication Date |
---|---|
CN111815328A true CN111815328A (en) | 2020-10-23 |
Family
ID=72855324
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202010637441.8A Pending CN111815328A (en) | 2019-07-05 | 2020-07-03 | Fraud prevention for payment instruments |
Country Status (2)
Country | Link |
---|---|
US (1) | US20210004809A1 (en) |
CN (1) | CN111815328A (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN114930368A (en) * | 2020-11-17 | 2022-08-19 | 维萨国际服务协会 | Systems, methods, and computer program products for determining fraud |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210304206A1 (en) * | 2020-03-27 | 2021-09-30 | Visa International Service Association | System and Method for Processing a Transaction Based on a Recovery Scoring Model |
US11645711B2 (en) * | 2021-06-01 | 2023-05-09 | Capital One Services, Llc | Account risk detection and account limitation generation using machine learning |
US11831688B2 (en) * | 2021-06-18 | 2023-11-28 | Capital One Services, Llc | Systems and methods for network security |
-
2019
- 2019-07-05 US US16/503,949 patent/US20210004809A1/en not_active Abandoned
-
2020
- 2020-07-03 CN CN202010637441.8A patent/CN111815328A/en active Pending
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN114930368A (en) * | 2020-11-17 | 2022-08-19 | 维萨国际服务协会 | Systems, methods, and computer program products for determining fraud |
US11922422B2 (en) | 2020-11-17 | 2024-03-05 | Visa International Service Association | System, method, and computer program product for determining fraud |
Also Published As
Publication number | Publication date |
---|---|
US20210004809A1 (en) | 2021-01-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20200349590A1 (en) | System and method for transaction learning | |
JP6407294B2 (en) | Dynamic change of track data | |
CN111815328A (en) | Fraud prevention for payment instruments | |
US10255598B1 (en) | Credit card account data extraction | |
US20180218369A1 (en) | Detecting fraudulent data | |
US20150363785A1 (en) | Systems and methods for consumer authentication using behavioral biometrics | |
US11329832B2 (en) | System and method for dynamic knowledge-based authentication | |
US11556635B2 (en) | System for evaluation and weighting of resource usage activity | |
JP2020506473A (en) | Method for adjusting risk parameters and method and device for risk identification | |
US20220188801A1 (en) | Cryptocurrency payment and distribution platform | |
US10217178B2 (en) | Customer identity verification | |
US20230064400A1 (en) | Multi-modal routing engine and processing architecture for orchestration of variable lending repayment terms using cryptocurrency collateral | |
US8788420B1 (en) | Generating peer-to-peer transaction risk ratings | |
JP2024512076A (en) | Asset class backed tokenization platform | |
CN112183761A (en) | Digital application tool instantiation | |
US20220084035A1 (en) | System and method for facilitating direct trading of electronic transactions | |
US20200349642A1 (en) | Configuring user interface functionality based on background processing of data related to pre-qualification status | |
US20210056518A1 (en) | Authentication credential system | |
US20220399005A1 (en) | System for decisioning resource usage based on real time feedback | |
KR102502968B1 (en) | Method and system for collecting trading intent of insurance policy | |
KR102502986B1 (en) | User terminal and method for providing ui for insurance policy transaction | |
US20220335411A1 (en) | System for binding a virtual card number | |
US20230222575A1 (en) | Systems and methods for exchanging user data | |
US20240078597A1 (en) | Artificial intelligence financial restructuring | |
KR20230041634A (en) | Method, server and program for trading unlisted stocks on auction method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |