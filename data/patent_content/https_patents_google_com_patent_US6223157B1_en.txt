US6223157B1 - Method for direct recognition of encoded speech data - Google Patents
Method for direct recognition of encoded speech data Download PDFInfo
- Publication number
- US6223157B1 US6223157B1 US09/074,726 US7472698A US6223157B1 US 6223157 B1 US6223157 B1 US 6223157B1 US 7472698 A US7472698 A US 7472698A US 6223157 B1 US6223157 B1 US 6223157B1
- Authority
- US
- United States
- Prior art keywords
- speech
- data
- vector
- representation
- computer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Lifetime
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
Definitions
- the present invention relates to a method for providing robust speech recognition of encoded (or compressed) speech data.
- Speech recognition the machine translation of spoken utterances into a stream of recognized words or phrases, has received considerable attention from researchers in recent years. In the last decade, speech recognition systems have improved enough to become available to an ever larger number of consumers in the market place.
- a number of applications utilizing speech recognition technology are currently being implemented in the telephone network environment, including the digital cellular network environment. For example, a telephone user's spoken commands may now determine call routing or how a call is billed (e.g.,“collect call please” or “calling card”). Similarly, a telephone user may transact business by dialing a merchant's automated system and speaking a credit card number instead of dialing one. Further and future use of speech recognition technology in the digital cellular environment could enhance service in a limitless number of ways.
- the Internet which has also grown and become more popular in recent years, provides another environment in which subscribers may benefit extensively from further use of speech recognition technology. For example, in the future, commercially available systems may allow a user at a remote station to specify, via voice commands, instructions which are then transmitted to an Internet host and executed.
- voice data must often be compressed prior to transmission. Once the data reaches a speech recognition engine at a remote site, the limited network bandwidth is no longer a factor. Therefore, it is common practice to de-compress (or decode and reconstruct) the voice data at that point to obtain a digital representation of the original acoustic signal (i.e., a waveform). The waveform can then be processed as though it was originally generated at the remote site.
- This procedure i.e., compress-transmit-decompress
- vocoders much of the speech compression done today is performed by “vocoders.” Rather than create a compressed, digital approximation of the speech signal (i.e., an approximation of a waveform representation), vocoders instead construct digital approximations of components or characteristics of speech implied by a given speech model. For example, a model may define speech as frequency of vocal chord movement (pitch), intensity or loudness of vocal chord movement (energy) or resonance of the vocal tract (spectral). The vocoding algorithm then applies signal processing techniques to the speech signal, leaving only specific signal components including those measuring pitch, energy and spectral speech characteristics.
- a speech recognition system operates by applying signal processing techniques to extract spectral and energy information from a stream of in-coming speech data. To generate a recognition result the extracted speech components are converted into a “feature” and then used in the alignment sub-system where the in-coming feature is compared to the representative features of the models.
- one advantage of the present invention is that it saves processing time and computational resources by bypassing redundant decompression processing.
- Another advantage of the present invention is that it takes advantage of processing already performed during vocoding (i.e., speech data compression).
- Another advantage of the present invention is that it renders speech recognition applications practiced in a network environment less complex.
- the present invention overcomes the disadvantages of the above described procedure (compress-transmit-decompress). More specifically, the present invention provides a system and method for mapping a vocoded representation of parameters defining speech components, which in turn define a particular waveform, into a base feature type representation of parameters defining speech components (e.g., Linear Predictive Coding (“LPC”)), which in turn define the same digital waveform.
- LPC Linear Predictive Coding
- construction of the base feature type used in recognition does not require reconstruction of the waveform from vocoded parameters.
- FIG. 1 depicts an overall system architecture of an exemplary telecommunications network system within which the present invention may be implemented.
- FIG. 2 depicts recognition and application systems located on the Media Server shown in FIG. 1, in accordance with an exemplary embodiment of the present invention.
- FIG. 3 is a flow chart of a process performed by the Digital Signal Processor front end shown in FIG. 2, in accordance with an exemplary embodiment of the present invention.
- FIG. 4 is an operational diagram of the process of FIG. 3 whereby the Digital Signal Processing Front End steps convert vocoded data directly into LPC vectors.
- FIG. 5 is an operational diagram showing preconditioned codebooks being used to construct LPC vectors in accordance with an exemplary embodiment of the present invention.
- FIG. 1 there is illustrated an overall system architecture of an exemplary telecommunications network system within which the present invention may be implemented.
- This figure does not show a number of network system specific components (e.g. Internet specific or cellular network specific components) in order to clearly illustrate the broad context in which the present invention may be implemented.
- network system specific components e.g. Internet specific or cellular network specific components
- an end user 1 initially accesses a telecommunications network 6 with remote equipment (e.g., a client workstation 21 including microphone 2 or a cellular phone 4 ). Specifically, once the application utilizing speech recognition is initiated the speaker 1 speaks into the cellular phone 4 or a microphone 2 of the client station 21 .
- the application may be, for example, voice determined call routing or transmitting over the Internet digitized voice commands which are then executed at a host, interior gateway or exterior gateway.
- the resulting acoustic signal is converted into an electric signal and digitized with an analog-to-digital converter.
- the digitized speech data is then “vocoded” (i.e., the data is compressed) using a vocoding algorithm.
- This algorithm may be performed, for example, by a processing device in the cellular phone 4 or by a software component of an Internet browser (e.g., Microsoft “Explorer” (TM) or “Netscape Communicator” (TM)) running on the client workstation 21 .
- the compressed data is transmitted over a line 3 through the telecommunications network 6 (e.g., the Internet or a digital cellular network), through an Ethernet 7 (i.e. a local area network) or a T1 line 8 , for example, and finally to a Media Server 10 .
- the Media Server 10 might be located in, for example, a GSM cellular network at a Mobile Telephone Switching Office (“MTSO”).
- MTSO Mobile Telephone Switching Office
- the MTSO is the heart of the network and contains subscriber information databases, including authentication, billing and perhaps phone number logs (for voice identification of call destination).
- the Media Server 10 might also be located at an Internet destination host, which receives vocoded speech data and generates a recognition result.
- the vocoded speech data is processed by a recognition engine to generate a recognition result.
- the recognition engine may use resources and language models stored either locally (i.e. at the Media Server 10 ) or remotely (at what might be called, for example, an “Executive Server” 11 ).
- FIG. 2 it broadly depicts resource and application processes located on the Media Server shown in FIG. 1, which also include the method of the present invention.
- the application that is executing is a telephone number dialing service determined by voice commands (e.g., “call Bob”) from a cellular service subscriber 1 .
- the subscriber 1 initially navigates through a series of prompts or menus (e.g., “press 1 for voice dialing”) sent from the Call Manager 50 in the application domain (on the Media Server 10 ).
- the user 1 may accomplish this by, for example, pressing digits on the cellular phone 4 key pad.
- the user 1 may for example speak the name of the party to call into the cellular phone 4 (e.g., “Bob”). As noted earlier, the acoustic signal is vocoded.
- the speech signal passes through the cellular network 6 to the Digital Signal Processor Front End Process (“DSPFE”) 52 in the resource domain (on the Media Server 10 ).
- DSPFE Digital Signal Processor Front End Process
- the DSPFE 52 obtains standard acoustic vectors (e.g., LPC vectors), from incoming (digitized and compressed) speech data without decompression.
- LPC vectors are a standard in the field of speech recognition and contain parameters (values) for various components used in the Linear Predictive Coding speech model. That model defines speech as resulting from glottal pulses causing excitation of the vocal tract, which is viewed as a lossless tube. Given these assumptions, speech may be broken down into components (e.g., magnitude of the glottal pulses, and vocal tract resonance which can be measured by the envelope of the power spectra). An LPC vector contains parameter values for these components at given instances in time, with specific portions of the LPC vector assigned to represent specific speech components (e.g., energy, spectral).
- the DSPFE 52 As compressed speech data is continually passed to the DSPFE 52 , the DSPFE 52 generates LPC vectors (or other base feature type common to a plurality of recognition systems) each representing the waveform at periods in time. The speech recognition system uses the components of these vectors to ultimately generate a recognition result.
- LPC vectors or other base feature type common to a plurality of recognition systems
- the speech recognition alignment sub-system may execute in a different processor or processor context.
- the DSPFE 52 sends the generated LPC vectors to the feature buffers 61 .
- the use of such buffers, which allow two independent processes to share information is well known in the art.
- the call manager 50 accesses the resource domain (including speech task resources) via the Resource Task Manager 53 .
- the Resource Task Manager 53 manages a number of resources in the resource domain, including the speech recognition facility.
- a recognition system of the type described here could execute on a general purpose processor (e.g. Sparc) or an embedded system (e.g., the MPB).
- the Resource Task Manager 53 activates the Speech Management Task 54 , which manages the speech recognition facility.
- the Speech Task Manager 54 then causes initialization of the speech recognition facility and its application models 58 .
- the application models 58 are databases of lexicon and language models which are speaker independent and built using the techniques known to those with skill in the art. Speech Communications: Human and Machine, O'Shaughnessy, pp. 497, (Addison Wesley 1990).
- the Speech Task Management 54 also manages an Alignment Engine 57 , which draws features from the Feature Extraction Process 56 and compares them with features in the application models 58 .
- the Feature Extraction Process 56 draws LPC vectors from the Feature Buffers 61 , and converts the LPC vectors to specific features which are actually used by the Alignment Engine 57 to generate a recognition result.
- the Alignment Engine 57 compares these features to features in the Model Region 58 to generate a recognition result.
- both the DSPFE 52 and the Speech Task Management 54 (which, again, manages the speech recognition facility) continue as above as independent processes sharing LPC vectors via the feature buffers 61 . These processes terminate when the task is complete (i.e., the system has processed the digital representation of a spoken “Bob” and recognized the word “Bob” for use in voice dialing).
- the Speech Task Management 54 returns the result (the word “Bob”) to the Resource Task Management 53 , which then returns the result to the Call Manager 50 .
- the Call Manager 50 compares the recognition result to a database of telephone number records specific to user 1 (which may be located in the subscriber database 51 ) until a record with a name field containing “Bob” is found.
- the Call Manager 50 finds the telephone number contained in that record and the call is connected and routed, accordingly.
- the procedure performed by the Digital Signal Processing Front-End (“DSPFE”) of FIG. 2 is shown in further detail.
- the speech signal data passes to the DSPFE process 52 which generates LPC vectors.
- the DSPFE 52 performs the following.
- the DSPFE 52 initially determines the type of digitized speech data incoming.
- the data may be, for example, either Digital Cellular data packets 100 or standard T1/64k waveform data 101 (although other types of data is, of course possible).
- the waveform data 101 may be, for example, a set of 64K PCM samples that are decompanded to a linear form and processed.
- PCM is an acronym for “Pulse Code Modulation,” which is a coding scheme whereby the input waveform is simply sampled and non-linearly quantized. The process of converting to a linear data type at this point is trivial.
- the data are an uncompressed representation of the speech signal.
- the digital cellular data packets 100 are vocoded speech data (i.e., organized to represent isolated speech components according to a speech model, rather than to a representation of that speech signal).
- step 106 standard DSPFE processing. Because the waveform data 101 is not vocoded (compressed), only the PCM to linear conversion process is necessary. The system can in other words act just as though the original acoustic signal were generated locally.
- the standard DSPFE 52 processes the waveform data 101 , using techniques known in the art to generate LPC vectors, or any other base feature type common to many recognition systems.
- the LPC vectors are placed in feature buffers 61 , drawn by the Feature Extraction Process 108 , which generates the features used by the recognition engine (these are sometimes called Generalized Speech Features (“GSF”)).
- GSF Generalized Speech Features
- LPC vectors are constructed directly from the vocoded data.
- the incoming vocoded data is comprised of speech components (e.g., energy and spectral) which are also used in LPC vectors
- the vocoded data are represented as parameter values contained in particular data structures defined by particular bit patterns according to coder type.
- the data may have been generated using any one of a variety of coders, including, for example, VSELP, QSELP, GSM, GSM-EFR or G.728.
- Step 104 the data is processed through a transform which maps the coder representation into, for example, the LPC representation defining the same (or as similar as possible) waveform.
- a transform which maps the coder representation into, for example, the LPC representation defining the same (or as similar as possible) waveform.
- the LPC vector data is placed in feature buffers 61 , drawn by the Feature Extraction Process 108 , which generates the features used by the recognition engine (i.e., the “GSF”).
- the recognition result is generated without decoding (decompressing) the incoming digitized speech signal.
- incoming vocoded speech data may be organized in the form of packed vectors 150 (a stream of partitioned binary data) with associated “codebooks.”
- the “codebooks” can be conceptualized as an array of “template” parameters (i.e., scalar or vector) which are potentially speech component parameters (values).
- the use of such a finite number of templates to represent values in high definition spectrum is called quantization.
- Codebooks are designed to work with each specific type of vocoder such that the output of each codebook is a desired representation of one or more vocoder parameters.
- a particular parameter may be coded nonlinearly, and the codebook for that parameter is designed such that the codebook output is a linear representation of the signal.
- Another example is a codebook that is designed to produce a logarithmic output.
- codebooks that pertain to the parameters of a particular vocoder are designed using the specific parameter types, quantization methods, and bit allocations of that vocoder. These codebooks are typically provided by or derived from the vocoder design.
- codebooks are being used here only as an example. There is nothing relating to codebooks that in any way precludes the use of non-codebook based vocoders.
- the packed vector 150 will contain a set of indices, with designated indices corresponding to particular codebook entries. These indices are used in conjunction with the codebooks, which serve as look-up tables, to select particular speech component parameters contained in the codebook.
- the spectral vector has eight parameters, which again represent the spectral component of the acoustic signal at a given time.
- the parameters are mapped (as close as possible) to a number of parameters in an LPC vector(also an array of parameters) which also define the same spectral component of the same signal at the same time.
- the necessary transform (mapping) algorithm may be developed as follows. First, using a particular waveform as input, determine the differences between the vocoded and LPC speech component vector (for example, spectral and energy) defining that waveform at a given time. Next, quantify and generalize these differences. These steps are repeated for a number of different waveforms until the knowledge gained can be used to build a transform mapping the vocoded spectral and energy vector parameters directly into corresponding portions of an LPC vector.
- LPC speech component vector for example, spectral and energy
- mapping would merely consist of multiplying the spectral vector parameter values by two and constructing the spectral portion of an LPC vector with the results.
- the process depicted in FIG. 4 is also performed on the energy component of the packed vector 150 .
- the DSPFE 52 thus processes the energy and spectral components of incoming vocoded data, which are conveniently isolated, to construct LPC vectors without decompression.
- FIG. 5 a process similar to that shown in FIG. 4 is illustrated.
- the codebooks used are pre-conditioned to automatically map to corresponding LPC vectors.
- incoming vocoded data is organized in the form of packed vectors 170 .
- vectors and scalars are chosen from the coder specific spectral and energy codebook.
- the coder specific codebook for spectral related values 172 and a coder specific codebook for energy related values 174 are conditioned to automatically map to corresponding LPC vector parameters.
- There is no transform step This procedure provides more efficient transformation, but may add implementational complexity by requiring more extensive modeling to build the codebooks.
- codebooks The complexity of pre-conditioning of codebooks also depends upon each vocoder design. If the vocoder parameters map easily and distinctly onto the LPC parameters, simple changes in quantization and scaling can be applied to the codebooks. On the other hand, if each of the LPC parameters is a function of multiple codebook parameters, either much larger codebooks are required (solved mathematically), or new codebooks may be developed using similar statistical error minimization methods employed to generate the transforms.
- the DSPFE 52 may also bypass the step of constructing LPC vectors by further modeling and pre-conditioning of the codebooks so that the look-up process generates the features use by the recognition engine directly. Although this would considerably simplify front end processing, again, the complexity added to the task of pre-conditioning the look-up tables would be considerable.
Abstract
Description
Claims (21)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US09/074,726 US6223157B1 (en) | 1998-05-07 | 1998-05-07 | Method for direct recognition of encoded speech data |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US09/074,726 US6223157B1 (en) | 1998-05-07 | 1998-05-07 | Method for direct recognition of encoded speech data |
Publications (1)
Publication Number | Publication Date |
---|---|
US6223157B1 true US6223157B1 (en) | 2001-04-24 |
Family
ID=22121298
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US09/074,726 Expired - Lifetime US6223157B1 (en) | 1998-05-07 | 1998-05-07 | Method for direct recognition of encoded speech data |
Country Status (1)
Country | Link |
---|---|
US (1) | US6223157B1 (en) |
Cited By (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP1239462A1 (en) * | 2001-03-08 | 2002-09-11 | Canon Kabushiki Kaisha | Distributed speech recognition system and method |
US6487534B1 (en) * | 1999-03-26 | 2002-11-26 | U.S. Philips Corporation | Distributed client-server speech recognition system |
US20030125955A1 (en) * | 2001-12-28 | 2003-07-03 | Arnold James F. | Method and apparatus for providing a dynamic speech-driven control and remote service access system |
US20030182131A1 (en) * | 2002-03-25 | 2003-09-25 | Arnold James F. | Method and apparatus for providing speech-driven routing between spoken language applications |
US20060095261A1 (en) * | 2004-10-30 | 2006-05-04 | Ibm Corporation | Voice packet identification based on celp compression parameters |
US20070004453A1 (en) * | 2002-01-10 | 2007-01-04 | Berkana Wireless Inc. | Configurable wireless interface |
US20090094026A1 (en) * | 2007-10-03 | 2009-04-09 | Binshi Cao | Method of determining an estimated frame energy of a communication |
US20100131268A1 (en) * | 2008-11-26 | 2010-05-27 | Alcatel-Lucent Usa Inc. | Voice-estimation interface and communication system |
US8559813B2 (en) | 2011-03-31 | 2013-10-15 | Alcatel Lucent | Passband reflectometer |
US20130339038A1 (en) * | 2011-03-04 | 2013-12-19 | Telefonaktiebolaget L M Ericsson (Publ) | Post-Quantization Gain Correction in Audio Coding |
US8666738B2 (en) | 2011-05-24 | 2014-03-04 | Alcatel Lucent | Biometric-sensor assembly, such as for acoustic reflectometry of the vocal tract |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5297194A (en) * | 1990-05-15 | 1994-03-22 | Vcs Industries, Inc. | Simultaneous speaker-independent voice recognition and verification over a telephone network |
US5305421A (en) * | 1991-08-28 | 1994-04-19 | Itt Corporation | Low bit rate speech coding system and compression |
US5377301A (en) * | 1986-03-28 | 1994-12-27 | At&T Corp. | Technique for modifying reference vector quantized speech feature signals |
US5487129A (en) * | 1991-08-01 | 1996-01-23 | The Dsp Group | Speech pattern matching in non-white noise |
US5680506A (en) * | 1994-12-29 | 1997-10-21 | Lucent Technologies Inc. | Apparatus and method for speech signal analysis |
US5692104A (en) * | 1992-12-31 | 1997-11-25 | Apple Computer, Inc. | Method and apparatus for detecting end points of speech activity |
US5787390A (en) * | 1995-12-15 | 1998-07-28 | France Telecom | Method for linear predictive analysis of an audiofrequency signal, and method for coding and decoding an audiofrequency signal including application thereof |
US6003004A (en) * | 1998-01-08 | 1999-12-14 | Advanced Recognition Technologies, Inc. | Speech recognition method and system using compressed speech data |
-
1998
- 1998-05-07 US US09/074,726 patent/US6223157B1/en not_active Expired - Lifetime
Patent Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5377301A (en) * | 1986-03-28 | 1994-12-27 | At&T Corp. | Technique for modifying reference vector quantized speech feature signals |
US5297194A (en) * | 1990-05-15 | 1994-03-22 | Vcs Industries, Inc. | Simultaneous speaker-independent voice recognition and verification over a telephone network |
US5487129A (en) * | 1991-08-01 | 1996-01-23 | The Dsp Group | Speech pattern matching in non-white noise |
US5305421A (en) * | 1991-08-28 | 1994-04-19 | Itt Corporation | Low bit rate speech coding system and compression |
US5692104A (en) * | 1992-12-31 | 1997-11-25 | Apple Computer, Inc. | Method and apparatus for detecting end points of speech activity |
US5680506A (en) * | 1994-12-29 | 1997-10-21 | Lucent Technologies Inc. | Apparatus and method for speech signal analysis |
US5787390A (en) * | 1995-12-15 | 1998-07-28 | France Telecom | Method for linear predictive analysis of an audiofrequency signal, and method for coding and decoding an audiofrequency signal including application thereof |
US6003004A (en) * | 1998-01-08 | 1999-12-14 | Advanced Recognition Technologies, Inc. | Speech recognition method and system using compressed speech data |
Non-Patent Citations (1)
Title |
---|
IEEE International Conference on Multimedia Computing and Systems '97. Yapp et al, "speech recognition on MPEG/AUdio encoded files". P. 624-625. Jun. 1997. * |
Cited By (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6487534B1 (en) * | 1999-03-26 | 2002-11-26 | U.S. Philips Corporation | Distributed client-server speech recognition system |
US20020128826A1 (en) * | 2001-03-08 | 2002-09-12 | Tetsuo Kosaka | Speech recognition system and method, and information processing apparatus and method used in that system |
EP1239462A1 (en) * | 2001-03-08 | 2002-09-11 | Canon Kabushiki Kaisha | Distributed speech recognition system and method |
US20030125955A1 (en) * | 2001-12-28 | 2003-07-03 | Arnold James F. | Method and apparatus for providing a dynamic speech-driven control and remote service access system |
US7013275B2 (en) | 2001-12-28 | 2006-03-14 | Sri International | Method and apparatus for providing a dynamic speech-driven control and remote service access system |
US20070004453A1 (en) * | 2002-01-10 | 2007-01-04 | Berkana Wireless Inc. | Configurable wireless interface |
US20100041385A1 (en) * | 2002-01-10 | 2010-02-18 | Qualcomm Incorporated | Configurable wireless interface |
US20030182131A1 (en) * | 2002-03-25 | 2003-09-25 | Arnold James F. | Method and apparatus for providing speech-driven routing between spoken language applications |
US7016849B2 (en) | 2002-03-25 | 2006-03-21 | Sri International | Method and apparatus for providing speech-driven routing between spoken language applications |
US20060095261A1 (en) * | 2004-10-30 | 2006-05-04 | Ibm Corporation | Voice packet identification based on celp compression parameters |
US20090094026A1 (en) * | 2007-10-03 | 2009-04-09 | Binshi Cao | Method of determining an estimated frame energy of a communication |
US20100131268A1 (en) * | 2008-11-26 | 2010-05-27 | Alcatel-Lucent Usa Inc. | Voice-estimation interface and communication system |
US20130339038A1 (en) * | 2011-03-04 | 2013-12-19 | Telefonaktiebolaget L M Ericsson (Publ) | Post-Quantization Gain Correction in Audio Coding |
US10121481B2 (en) * | 2011-03-04 | 2018-11-06 | Telefonaktiebolaget Lm Ericsson (Publ) | Post-quantization gain correction in audio coding |
US10460739B2 (en) | 2011-03-04 | 2019-10-29 | Telefonaktiebolaget Lm Ericsson (Publ) | Post-quantization gain correction in audio coding |
US11056125B2 (en) | 2011-03-04 | 2021-07-06 | Telefonaktiebolaget Lm Ericsson (Publ) | Post-quantization gain correction in audio coding |
US8559813B2 (en) | 2011-03-31 | 2013-10-15 | Alcatel Lucent | Passband reflectometer |
US8666738B2 (en) | 2011-05-24 | 2014-03-04 | Alcatel Lucent | Biometric-sensor assembly, such as for acoustic reflectometry of the vocal tract |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Digalakis et al. | Quantization of cepstral parameters for speech recognition over the world wide web | |
US6725190B1 (en) | Method and system for speech reconstruction from speech recognition features, pitch and voicing with resampled basis functions providing reconstruction of the spectral envelope | |
EP1788555B1 (en) | Voice encoding device, voice decoding device, and methods therefor | |
KR100391287B1 (en) | Speech recognition method and system using compressed speech data, and digital cellular telephone using the system | |
CN101057275B (en) | Vector conversion device and vector conversion method | |
US5890110A (en) | Variable dimension vector quantization | |
US6223157B1 (en) | Method for direct recognition of encoded speech data | |
CN101023471A (en) | Scalable encoding apparatus, scalable decoding apparatus, scalable encoding method, scalable decoding method, communication terminal apparatus, and base station apparatus | |
JPH10307599A (en) | Waveform interpolating voice coding using spline | |
KR20000062175A (en) | Automatic speech/speaker recognition over digital wireless channels | |
US20020072899A1 (en) | Sub-band speech coding system | |
US6988067B2 (en) | LSF quantizer for wideband speech coder | |
EP1362345B1 (en) | Method and apparatus for reducing undesired packet generation | |
CN100585700C (en) | Sound encoding device and method thereof | |
EP0401452B1 (en) | Low-delay low-bit-rate speech coder | |
US7050969B2 (en) | Distributed speech recognition with codec parameters | |
US20020128826A1 (en) | Speech recognition system and method, and information processing apparatus and method used in that system | |
JP4359949B2 (en) | Signal encoding apparatus and method, and signal decoding apparatus and method | |
EP1121686A1 (en) | Speech parameter compression | |
Wang et al. | Product code vector quantization of LPC parameters | |
CN114783428A (en) | Voice translation method, voice translation device, voice translation model training method, voice translation model training device, voice translation equipment and storage medium | |
Gunawan et al. | PLP coefficients can be quantized at 400 bps | |
Dong-jian | Two stage concatenation speech synthesis for embedded devices | |
Sakka et al. | Using geometric spectral subtraction approach for feature extraction for DSR front-end Arabic system | |
JP2002073097A (en) | Celp type voice coding device and celp type voice decoding device as well as voice encoding method and voice decoding method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: DSC TELECOM L.P. (A TEXAS PARTNERSHIP), TEXASFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:FISHER, THOMAS D.;SPIESS, JEFFERY J.;MOWRY, DEARBORN R.;REEL/FRAME:009202/0749Effective date: 19980505 |
|
AS | Assignment |
Owner name: DSC TELECOM L.P., TEXASFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:FISCHER, THOMAS D.;SPIESS, JEFFREY J.;MOWRY, DEARBORN R.;REEL/FRAME:009571/0677;SIGNING DATES FROM 19980909 TO 19981006 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
AS | Assignment |
Owner name: ALCATEL USA SOURCING, L.P., TEXASFree format text: CHANGE OF NAME;ASSIGNOR:DSC TELECOM L.P.;REEL/FRAME:027346/0386Effective date: 19980908 |
|
AS | Assignment |
Owner name: ALCATEL-LUCENT USA INC., NEW JERSEYFree format text: MERGER;ASSIGNOR:ALCATEL USA SOURCING, INC.;REEL/FRAME:027350/0475Effective date: 20081101Owner name: ALCATEL USA SOURCING, INC., TEXASFree format text: CHANGE OF NAME;ASSIGNOR:ALCATEL USA SOURCING, L.P.;REEL/FRAME:027349/0236Effective date: 20061231 |
|
AS | Assignment |
Owner name: LOCUTION PITCH LLC, DELAWAREFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:ALCATEL-LUCENT USA INC.;REEL/FRAME:027437/0922Effective date: 20111221 |
|
REMI | Maintenance fee reminder mailed | ||
FPAY | Fee payment |
Year of fee payment: 12 |
|
SULP | Surcharge for late payment |
Year of fee payment: 11 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LOCUTION PITCH LLC;REEL/FRAME:037326/0396Effective date: 20151210 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044213/0313Effective date: 20170929 |