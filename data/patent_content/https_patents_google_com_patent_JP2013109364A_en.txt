JP2013109364A - Cjk name detection - Google Patents
Cjk name detection Download PDFInfo
- Publication number
- JP2013109364A JP2013109364A JP2013004333A JP2013004333A JP2013109364A JP 2013109364 A JP2013109364 A JP 2013109364A JP 2013004333 A JP2013004333 A JP 2013004333A JP 2013004333 A JP2013004333 A JP 2013004333A JP 2013109364 A JP2013109364 A JP 2013109364A
- Authority
- JP
- Japan
- Prior art keywords
- name
- names
- characters
- detection model
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Abstract
Description
本発明は、名前検出に関し、特に、中国、日本、および韓国(「CJK」)の言語のための名前検出に関する。 The present invention relates to name detection, and more particularly to name detection for Chinese, Japanese, and Korean (“CJK”) languages.
名前検出は、自然言語処理で通常使用される(例えば、自動音声認識(ASR)、機械翻訳(MT)、光学文字認識(OCR)、文の構文解析、非ローマ字インプットメソッドエディタ(IME)、およびWebサーチアプリケーション)。 Name detection is commonly used in natural language processing (e.g., automatic speech recognition (ASR), machine translation (MT), optical character recognition (OCR), sentence parsing, non-Roman input method editor (IME), and Web search application).
単純ベイズ式分類法(Naive Bayesian classification method)を使用して、文字のシーケンス「X」が名前を特定するか否かを検出することができる(そのコンテキスト(例えば、「X」の前または後に出現する文字)を付与された名前を特定する「X」の確率と、そのコンテキストを付与された名前を特定しない「X」の確率との比率によって)。言語モデルを使用して、これらの条件付き確率を計算する。代表的な統計的言語モデルは、その履歴(例えば、データのコレクションにおける、以前の単語又は文字シーケンスの出現)を付与された、単語又は文字シーケンスの確率測定である。特に、マルコフの仮定に基づく従来のn-グラム言語モデルを使用して、単語又は文字シーケンスを予測する。 The Naive Bayesian classification method can be used to detect whether the sequence of letters `` X '' identifies a name (occurs before or after its context (e.g., before or after `` X '' (The ratio of the probability of “X” identifying the name given the character) and the probability of “X” not identifying the name given the context). A language model is used to calculate these conditional probabilities. A typical statistical language model is a probability measure of a word or character sequence given its history (eg, the appearance of a previous word or character sequence in a collection of data). In particular, a conventional n-gram language model based on Markov's assumption is used to predict a word or character sequence.
n-グラムは、n個の連続したトークン(例えば、単語または文字)のシーケンスである。nグラムは、順序を有する(n-グラムの中のトークンの数である)。例えば、1グラム(または、ユニグラム)は1つのトークンを含み、2グラム(または、バイグラム)は2つのトークンを含んでいる。 An n-gram is a sequence of n consecutive tokens (eg, words or characters). n-grams have an order (the number of tokens in an n-gram). For example, 1 gram (or unigram) contains one token and 2 gram (or bigram) contains 2 tokens.
所定のn-グラムは、n-グラムの異なった部分に従って、記述することができる。n-グラムは、コンテキストおよび将来のトークン(コンテキスト、c)として記述することができ、ここで、コンテキストは長さn-1を有し、かつ、cは将来のトークンを表す。例えば、3グラム「x y z」は、n-グラムコンテキストおよび将来のトークンの観点から記述することができる。n-グラムコンテキストは、n-グラムの最後のトークンに先行するn-グラムのすべてのトークンを含んでいる。所定の例で、「x y」がコンテキストである。コンテキストの中の左端のトークンが、左のトークンと称される。将来のトークンは、n-グラムの最後のトークンであり、例では「z」である。また、n-グラムは、右のコンテキストおよびバックオフ(backed off)コンテキストに関して記述することができる。右のコンテキストは、n-グラムの第１トークンに続くn-グラムのすべてのトークンを含む((n-l)グラムとして表される)。上記した例では、「y z」が右のコンテキストである。さらに、バックオフコンテキストは、コンテキストの中の左端のトークンを差し引いた、n-グラムのコンテキストである。上記した例では、「y」がバックオフコンテキストである。 A given n-gram can be described according to different parts of the n-gram. An n-gram can be described as a context and a future token (context, c), where the context has a length n-1, and c represents a future token. For example, 3 grams “x y z” can be described in terms of n-gram context and future tokens. An n-gram context contains all n-gram tokens that precede the last token of n-grams. In the given example, “xy” is the context. The leftmost token in the context is referred to as the left token. The future token is the last token of the n-gram, which is “z” in the example. An n-gram can also be described in terms of a right context and a backed off context. The right context contains all n-gram tokens following the first n-gram token (denoted as (n-l) grams). In the above example, “y z” is the right context. In addition, the backoff context is an n-gram context minus the leftmost token in the context. In the above example, “y” is the back-off context.
各n-グラムは、トレーニングデータにおけるn-グラム相対度数の関数として計算される、関連した確率評価を有する。例えば、L個のトークンのストリングが、 Each n-gram has an associated probability estimate calculated as a function of the n-gram relative frequency in the training data. For example, a string of L tokens
として表される。ストリング Represented as: string
に以下として、 As below
確率を割り当てることができる。ここで、近似は、ストリングにおける次のトークンを予測するとき、直前の(n-1)トークンだけが関連しているというマルコフの仮定に基づき、かつ、Pに対する「＾」という記号は確率関数の近似であることを示す。 Probability can be assigned. Here, the approximation is based on Markov's assumption that only the previous (n-1) tokens are relevant when predicting the next token in the string, and the symbol "^" for P is the probability function Indicates an approximation.
CJK言語では、文には単語境界がない。その結果、文は、人々の名前の検出の前に、自動的に区分される必要がある。したがって、区分化エラーは名前検出に伝播する。 In CJK language, sentences do not have word boundaries. As a result, sentences need to be automatically segmented prior to detection of people's names. Therefore, partitioning errors are propagated to name detection.
CJK名前は、大規模な統計から得ることができる形態学的な法則を有する。例えば、300個の一般的な中国人の姓が、人口の99%以上をカバーしている。多くの場合、女性の名前は、 The CJK name has morphological laws that can be derived from extensive statistics. For example, 300 common Chinese surnames cover over 99% of the population. In many cases, the name of a woman
(na, hong, bing, li)などの文字を含んでいる。通常、一般的な名は姓から独立している。例えば、 Contains characters such as (na, hong, bing, li). Common names are usually independent of last names. For example,
という姓、および、 The surname and
という名の組合せに対する統計が利用可能である場合、名前を特定する他の If statistics for the name combination are available, other statistics that identify the name
という姓、および、 The surname and
という名の組合せを、姓を特定する Identify the last name with the combination of the names
の統計および、名を特定する Statistics and name
の統計を使用することで予測することができる。さらに、中国語の一部の単語は、人の名前又は通常の単語のどちらでもあり得る。例えば、 Can be predicted by using the statistics. Furthermore, some Chinese words can be either person names or regular words. For example,
は、中国の有名な歌手の名前か、又は、夜明けを意味する一般的な単語のどちらでもあり得る。そのような名前の検出は、コンテキストに大いに依存する。 Can be either the name of a famous Chinese singer or a common word meaning dawn. Such name detection is highly context dependent.
加えて、一般に、CJK名前は、2グラム(バイグラム)又は3グラム(トリグラム)を使用することで特定される。左から右にCJKテキストを読むことの水平の慣例を仮定すれば、コンテキストの中の左端の文字は姓である。右のコンテキストは名である。例えば、「x y z」がCJK名前である場合、次いで、「x」は姓であり、かつ、「y z」は名である。更なる例として、「x y」がCJK名前である場合、次いで、「x」は姓であり、かつ、「y」は名である。 In addition, CJK names are generally specified using 2 grams (bigrams) or 3 grams (trigrams). Given the horizontal convention of reading CJK text from left to right, the leftmost character in the context is a surname. The right context is a name. For example, if “x y z” is a CJK name, then “x” is the last name and “y z” is the first name. As a further example, if “x y” is a CJK name, then “x” is the last name and “y” is the first name.
名前検出のためのシステム、方法、およびコンピュータプログラム製品が提供され、特に、表意文字(例えば、漢字)で作られた名前を検出するのに役立つ。一般に、1態様では、方法が提供される。方法は、姓のコレクションと、n-グラムのコレクションを含む注釈付コーパスとを使用することで、未加工(raw)名前検出モデルを生成するステップを具備し、各n-グラムは、注釈付コーパスにおいて名前として出現することの対応する確率を有する。また、方法は、未加工名前検出モデルを準構造化データのコレクションに適用して、注釈付準構造化データを形成するステップを具備し、注釈付準構造化データは、名前を特定するn-グラムと名前を特定しないn-グラムとを特定する。また、方法は、未加工名前検出モデルを大規模な注釈のないコーパスに適用して、名前を特定する大規模な注釈のないコーパスのn-グラムと、名前を特定しないn-グラムとを特定する大規模な注釈付コーパスデータを形成するステップを具備している。また、方法は、名前を特定する注釈付準構造化データ、および名前を特定する大規模な注釈付コーパスデータを使用することで名前モデルを導出するステップと、名前を特定しない準構造化データを使用することで非名前モデルを導出するステップと、大規模な注釈付コーパスを使用することで言語モデルを導出するステップとを含む、名前検出モデルを生成するステップを具備している。この態様の他の具体化例は、システムおよびコンピュータプログラム製品を含んでいる。 Systems, methods, and computer program products for name detection are provided and are particularly useful for detecting names made up of ideographic characters (eg, kanji). In general, in one aspect, a method is provided. The method comprises generating a raw name detection model by using a collection of surnames and an annotated corpus that includes a collection of n-grams, each n-gram being annotated corpus Have a corresponding probability of appearing as a name. The method also includes applying a raw name detection model to the collection of semi-structured data to form annotated semi-structured data, the annotated semi-structured data identifying a name n- Identify gram and n-gram that does not specify name. The method also applies the raw name detection model to large unannotated corpora to identify large unannotated corpus n-grams that identify names and n-grams that do not identify names. Forming large-scale annotated corpus data. The method also includes deriving a name model using annotated semi-structured data that identifies names and large-scale annotated corpus data that identifies names, and semi-structured data that does not identify names. Generating a name detection model including: using to derive a non-name model; and using a large annotated corpus to derive a language model. Other embodiments of this aspect include systems and computer program products.
態様の具体化例は、以下の特徴のうちの１又は複数を含むことができる。態様は、名前検出モデルを準構造化データのコレクションに適用して、注釈付準構造化データを形成するステップと、名前検出モデルを大規模な注釈のないコーパスに適用して、名前を特定する大規模な注釈のないコーパスのn-グラムと、名前を特定しないn-グラムとを特定する大規模な注釈付コーパスデータを形成するステップと、改良された名前検出モデルを生成するステップとをさらに具備することができ、注釈付準構造化データは、名前を特定するn-グラムと名前を特定しないn-グラムとを特定する。改良された名前検出モデルを生成するステップは、名前を特定する注釈付準構造化データ、および名前を特定する大規模な注釈付コーパスデータを使用することで改良された名前モデルを導出するステップと、名前を特定しない準構造化データを使用することで改良された非名前モデルを導出するステップと、大規模な注釈付コーパスを使用することで改良された言語モデルを導出するステップとを具備することができる。 Embodiments of the aspects can include one or more of the following features. The aspect applies a name detection model to a collection of semi-structured data to form annotated semi-structured data, and applies the name detection model to a large unannotated corpus to identify names. Further comprising the steps of forming large-scale annotated corpus data identifying large unannotated corpus n-grams and unnamed n-grams, and generating an improved name detection model Annotated semi-structured data can identify n-grams that specify names and n-grams that do not specify names. The step of generating an improved name detection model includes deriving an improved name model by using annotated semi-structured data that identifies names and large-scale annotated corpus data that identifies names; Deriving an improved unnamed model using semi-structured data without specifying a name, and deriving an improved language model using a large annotated corpus be able to.
名前モデルは、名前を特定する注釈付準構造化データ、および名前を特定する大規模な注釈付コーパスからのn-グラムのコレクションを具備することができ、ここで、各n-グラムは、左の文字として姓、および右のコンテキストとして名を含み、かつ、各n-グラムは名前を特定することの対応する確率を有する。非名前モデルは、名前を特定しない注釈付準構造化データからのn-グラムのコレクションを具備することができ、ここで、各n-グラムは、左の文字として姓、および右のコンテキストとして名を含み、かつ、各n-グラムは、名前を特定しないことの対応する確率を有する。未加工名前モデルは、注釈付コーパスからのn-グラムのコレクションを含むことができ、ここで、各n-グラムは、姓のコレクションからの姓である左の文字を含み、かつ、各n-グラムは、注釈付コーパスにおける名前の相対度数に従って、名前を特定することの対応する確率を有する。未加工名前モデルは、外国人の姓のコレクションを使用することで生成することができる。 A name model can comprise annotated semi-structured data that identifies names, and a collection of n-grams from a large annotated corpus that identifies names, where each n-gram is left The last name as a character and the first name as the right context, and each n-gram has a corresponding probability of identifying the name. A non-name model can have a collection of n-grams from annotated semi-structured data that does not specify a name, where each n-gram has a surname as the left letter and a first name as the right context. And each n-gram has a corresponding probability of not identifying a name. The raw name model can include a collection of n-grams from the annotated corpus, where each n-gram includes a left letter that is the last name from the collection of surnames, and each n-gram The gram has a corresponding probability of identifying the name according to the relative frequency of the name in the annotated corpus. A raw name model can be generated using a collection of foreign surnames.
姓のコレクションは、複数の疎な姓を含むことができ、かつ、未加工名前検出モデルは、名前を特定する各n-グラムの確率(疎な姓である左の文字を含んでいる)を特定するために、複数の疎な姓のうちの特定の疎な姓の計算された確率の代わりに、すべての疎な姓の単一の確率を使用する。姓のコレクションは、複数の外国人の姓を含むことができる。 The collection of surnames can include multiple sparse surnames, and the raw name detection model determines the probability of each n-gram that identifies a name (including the left letter that is a sparse surname). To identify, use a single probability of all sparse surnames instead of the calculated probability of a particular sparse surname among multiple sparse surnames. The surname collection can include multiple foreign surnames.
一般に、１態様では、方法が提供される。方法は、文字の入力ストリングを受信するステップと、名前検出モデルを複数の文字を有する入力ストリングに適用するステップとを含んでいる。名前検出モデルを適用するステップは、１又は複数の名前を含まない、複数の文字の最も尤もらしい区分化を特定するステップと、１又は複数の名前を潜在的に特定するとして、複数の文字のうちの１又は複数の文字シーケンスを検出するステップと、１又は複数の潜在的な名前を含んでいる、複数の文字の区分化を特定するステップと、潜在的な１又は複数の名前を含む区分化の尤度が、１又は複数の名前を含まない最も尤もらしい区分化より大きいとき、１又は複数の名前を含むとして、複数の文字を区分するステップとを具備する。この態様の他の具体化例は、システムおよびコンピュータプログラム製品を含んでいる。 In general, in one aspect, a method is provided. The method includes receiving an input string of characters and applying a name detection model to the input string having a plurality of characters. Applying the name detection model includes identifying the most likely segmentation of characters that does not include one or more names, and potentially identifying one or more names, Detecting a sequence of one or more characters, identifying a segmentation of a plurality of characters containing one or more potential names, and a segment including one or more potential names Partitioning a plurality of characters as including one or more names when the likelihood of conversion is greater than the most likely segmentation not including one or more names. Other embodiments of this aspect include systems and computer program products.
態様の具体化例は、以下の特徴のうちの１又は複数を含むことができる。態様は、複数の文字が１又は複数の名前を含んでいるとして区分されるとき、１又は複数の名前を検出するステップをさらに含むことができる。態様は、複数の文字を含むストリングを受信するステップと、ストリングの特定のシーケンスが名前を特定する確率を計算するステップとをさらに具備し、名前は姓および名を含み、コーパスにおいて特定のシーケンスの頻度がしきい値より少ないとき、名を表すシーケンスの部分があらゆる姓と共に出現する相対度数と、姓を表すシーケンスの部分の相対度数との関数として、特定のシーケンスが名前を特定する確率を決定するステップを含んでいる。 Embodiments of the aspects can include one or more of the following features. The aspect can further include detecting one or more names when the plurality of characters are classified as including one or more names. The aspect further comprises receiving a string that includes a plurality of characters and calculating a probability that a particular sequence of strings identifies a name, where the name includes a first name and a last name, Determines the probability that a particular sequence will identify a name as a function of the relative frequency that the part of the sequence that represents the first name appears with every surname when the frequency is less than the threshold, and the relative frequency of the part of the sequence that represents the last name Includes steps to do.
態様は、ユーザ入力データを受信するステップと、未加工名前検出モデルをユーザ入力データに適用して、注釈付ユーザ入力データを形成するステップとをさらに具備し、注釈付ユーザ入力データは、名前を特定するn-グラム、および名前を特定しないn-グラムを特定する。名前検出モデルを生成するステップは、名前を特定する注釈付ユーザ入力データを使用することで名前モデルを導出するステップと、名前を特定しない注釈付ユーザ入力データを使用することで非名前モデルを導出するステップと、注釈付ユーザ入力データを使用することで言語モデルを導出するステップとをさらに具備することができる。 The aspect further comprises receiving user input data and applying a raw name detection model to the user input data to form annotated user input data, wherein the annotated user input data includes a name. Identify n-grams that identify and n-grams that do not identify a name. The step of generating the name detection model is to derive the name model by using the annotated user input data that specifies the name, and to derive the non-name model by using the annotated user input data that does not specify the name. And deriving a language model using the annotated user input data.
本明細書に記載された本発明の特定の具体化例は、以下の利点のうちの１又は複数を実現するように実施することができる。CJK名前検出は、入力テキストを単語にあらかじめ区分するステップの有無にかかわらず実行することができ、名前検出エラーを生じさせる単語区分化エラーを回避する。名前検出モデルをトレーニングするステップは、多量の人間による注釈付データを必要としない。一部のトレーニングデータは、準構造化データ(例えば、xmlファイルにおける、ダウンロードの記述)に適用することができる。膨大な注釈のないデータ(特に、インプットメソッドエディタ(IME)ユーザ入力、IMEユーザ辞書、ウェブページ、サーチクエリーログ、電子メール、ブログ、インスタントメッセージ(IM)、およびニュース記事)を使用して、名前検出モデルをトレーニングすることができる。このデータの使用は、名前検出において、高い精度および高い再現度の両方を保証する。また、名前検出モデルを使用して、疎な姓を有する名前および外国人の名前を検出することができる。加えて、CJK名前検出は、名前検出モデルをさらに改良するために繰り返しトレーニングをして、以前の名前検出モデルに追加された名前を検出するステップを含んでいる。 Particular embodiments of the invention described herein can be implemented to realize one or more of the following advantages. CJK name detection can be performed with or without the step of pre-segmenting the input text into words, avoiding word segmentation errors that cause name detection errors. The step of training the name detection model does not require a large amount of human annotated data. Some training data can be applied to semi-structured data (eg, a download description in an xml file). Names using huge unannotated data (especially input method editor (IME) user input, IME user dictionary, web page, search query log, email, blog, instant message (IM), and news articles) The detection model can be trained. The use of this data ensures both high accuracy and high reproducibility in name detection. The name detection model can also be used to detect names with sparse surnames and foreign names. In addition, CJK name detection includes the steps of iterative training to further improve the name detection model to detect names added to the previous name detection model.
本明細書に記載された本発明の１又は複数の具体化例の詳細は、添付図面および以下の記載で説明される。本発明の他の特徴、態様および利点は、記載、図面および特許請求の範囲から明らかになる。 The details of one or more embodiments of the invention described herein are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the invention will be apparent from the description, drawings, and claims.
様々な図面の類似の参照番号及び記号は類似の要素を示す。 Like reference numbers and symbols in the various drawings indicate like elements.
§名前を検出するための数式
中国語の一部の単語は、人の名前又は通常の単語のどちらでもあり得る。例えば、
§Formulas for detecting names Some words in Chinese can be either human names or ordinary words. For example,
は、中国の有名な歌手の名前か、又は夜明けを意味する一般的な単語のどちらでもあり得る。例えば、図1Aは、夜明けを意味する文字シーケンス Can be either the name of a famous Chinese singer or a common word meaning dawn. For example, Figure 1A shows a character sequence that means dawn
100を含む中国語テキストを示している。別の例として、図1Bは、中国の有名な歌手の名前として、文字シーケンス Chinese text including 100 is shown. As another example, Figure 1B shows the character sequence as the name of a famous Chinese singer
102を含む中国語テキストを示している。これらの文字シーケンスは、名前を特定するか、又は名前を特定しないとして、どちらにでも分類することができる。 Chinese text including 102 is shown. These character sequences can be classified either as specifying names or not specifying names.
特に、n-グラムは名前を特定するか、又は名前を特定しないとして、どちらにでも分類される。所定のn-グラムwは、ベイズ規則を使用することで、名前を特定するか(NAME)、又は、名前を特定しない(NOTNAME)として、どちらにでも分類することができる。ベイズ規則は、コンテキストを付与された名前を特定する所定のn-グラムwの確率を以下のように定義することができると定める。 In particular, n-grams are classified as either specifying a name or not specifying a name. A given n-gram w can be classified as either specifying a name (NAME) or not specifying a name (NOTNAME) by using Bayesian rules. The Bayes rule states that the probability of a given n-gram w that identifies a context-assigned name can be defined as follows:
同様に、名前を特定しない所定のn-グラムの確率を以下のように定義することができる。 Similarly, the probability of a given n-gram that does not specify a name can be defined as follows:
その上、比率を以下のように定義することができる。 In addition, the ratio can be defined as:
１実施例では、結果とした生じた比率値が１以上である場合、次いで、n-グラムは名前を特定するとして分類される。換言すれば、n-グラムwが名前を特定するコスト加重尤度(cost-weighted likelihood)は、n-グラムwが名前を特定しないコスト加重尤度より大きい。そうでなければ、n-グラムは名前を特定しないとして分類される。Lは、特定の損失関数を表す。一部の実施例では、損失関数は、以下のように方程式を単純化することができるように定数である。 In one embodiment, if the resulting ratio value is greater than or equal to 1, then the n-gram is classified as identifying a name. In other words, the cost-weighted likelihood that n-gram w identifies a name is greater than the cost-weighted likelihood that n-gram w does not identify a name. Otherwise, n-grams are classified as not specifying a name. L represents a specific loss function. In some embodiments, the loss function is a constant so that the equation can be simplified as follows:
ここで、cは定数である。結合確率(P(w=NAME, context)およびP(w = NOTNAME, context))は、名前検出モデルからの出力として、もうけることができる(図2乃至4に関して以下に詳細に説明する)。 Here, c is a constant. Coupling probabilities (P (w = NAME, context) and P (w = NOTNAME, context)) can be made as output from the name detection model (described in detail below with respect to FIGS. 2-4).
初期の概要として、名前検出モデルは、名前モデルと、非名前モデルと、言語モデルとを具備している。未加工名前モデルは、注釈付コーパスのn-グラムが名前を特定するか否かを特定するために、事前に定義された姓のコレクションと、注釈付コーパスとを使用することで生成される。未加工名前モデルは、準構造化データおよび大規模な注釈のないデータに適用されて、名前検出モデルを生成する。 As an initial overview, the name detection model comprises a name model, a non-name model, and a language model. The raw name model is generated using a predefined collection of surnames and an annotated corpus to determine whether an annotated corpus n-gram identifies a name. The raw name model is applied to semi-structured data and large unannotated data to generate a name detection model.
特に、名前検出モデルは、確率評価を、P(w = NAME, context)及びP(w = NOTNAME, context)から導出する。特に、結合確率P(w = NAME, context)を以下のように書き直すことができる。
Pname(W, context) = Pname(prefix)Pname(W | prefix)Pname(suffix | W, prefix).
Pname(W, context)は、以下のようにさらに近似することができる。
Pname(prefix)Pname(family_name, given name | prefix)Pname(suffix | family_name, given_name)Pname 式(1)
加えて、結合確率P(w = NOTNAME, context)を同様に以下のように近似することができる。
Pnotname(prefix)Pnotname (family_name, given_name | prefix)Pnotname (suffix | family_name, given_name) 式(2)
In particular, the name detection model derives probability estimates from P (w = NAME, context) and P (w = NOTNAME, context). In particular, the connection probability P (w = NAME, context) can be rewritten as follows.
Pname (W, context) = Pname (prefix) Pname (W | prefix) Pname (suffix | W, prefix).
Pname (W, context) can be further approximated as follows.
Pname (prefix) Pname (family_name, given name | prefix) Pname (suffix | family_name, given_name) Pname expression (1)
In addition, the connection probability P (w = NOTNAME, context) can be similarly approximated as follows.
Pnotname (prefix) Pnotname (family_name, given_name | prefix) Pnotname (suffix | family_name, given_name) Expression (2)
§未加工名前検出モデル
図2は、未加工名前検出モデル206の例の生成を図示したブロック図200である。便宜上、未加工名前検出モデル206の生成は、生成を実行するシステムに関して説明される。
§Raw Name Detection Model FIG. 2 is a block diagram 200 illustrating the generation of an example of the raw
CJKテキストでは、所定のn-グラムは、n-グラムにおける左の文字が姓である場合にだけ、名前を特定することができる。右のコンテキストは名である。したがって、事前に定義された姓のコレクション204を使用して、未加工名前検出モデル206を生成する。システムは、少量の注釈付トレーニングデータを使用することで、未加工名前検出モデル206を生成することができる。システムは、注釈付コーパス(例えば、少量の注釈付コーパス202)と、事前に定義された姓のコレクション204とを使用することによって、未加工名前モデル206をトレーニングする。
In CJK text, a given n-gram can only identify a name if the left character in the n-gram is a surname. The right context is a name. Accordingly, a raw
事前に定義された姓204は、１又は複数のCJK言語の姓のコレクションを含んでいる。例えば、中国の名前検出モデルのために、事前に定義された姓204は、300個の一般的な中国の姓のコレクションを含むことができ、所定の人口における99%以上の可能性のある中国の姓を統計的にカバーする。小規模な注釈付コーパス202は、テキストデータの小規模なコレクション(例えば、ウェブドキュメントまたはサーチクエリー)を含んでいる。小規模な注釈付コーパス202のテキストデータは、名前を特定するか、又は名前を特定しないとして特定された(例えば、注釈される)n-グラムを含んでいる。例えば、名前は、１又は複数の個人によって手動で特定することができる。 The predefined surname 204 includes a collection of one or more surnames in CJK languages. For example, for a Chinese name detection model, the predefined surname 204 can include a collection of 300 common Chinese surnames, with over 99% potential Chinese in a given population Statistically covers surnames of The small annotated corpus 202 includes a small collection of text data (eg, web documents or search queries). The text data of the small annotated corpus 202 includes n-grams that are identified (eg, annotated) with a name specified or not specified. For example, the name can be manually specified by one or more individuals.
生成後に、未加工名前検出モデル206は、(姓204のコレクションで見つけられる左の文字で)名前を特定する小規模な注釈付コーパス202におけるn-グラムと、名前と名前を特定しないn-グラムとの相対度数の関数として計算される確率評価を含む。したがって、未加工名前モデル206を使用して、入力n-グラムが名前を特定するか、又は名前を特定しない確率を計算することができる (例えば、上記した比率に基づく名前を検出する)。しかしながら、これは、小規模な注釈付コーパスの確率によって制限され、データの大規模なコレクションで正確でない。その結果、未加工名前検出モデル206は、トレーニングデータにさらに適用されて、名前検出モデルを生成する(図3に関して以下でさらに詳細を論じる)。
After generation, the raw
§トレーニングデータ
図3は、名前検出モデル314の例の生成を図示したブロック図300である。注釈プロセス316(例えば、未加工名前検出モデル206によって実行される)が、拡張名前検出モデルを生成するために、注釈のないデータに適用される。準構造化データ302及び大規模な注釈のないコーパス308を、注釈のないデータとして使用することができる。
§ Training Data FIG. 3 is a block diagram 300 illustrating the generation of an example name detection model 314. An annotation process 316 (eg, performed by raw name detection model 206) is applied to the unannotated data to generate an extended name detection model. Semi-structured data 302 and large
準構造化データ302は、例えばxmlファイルを含むことができる。準構造化データ302は、多くの異なったフィールドを有するデータを含むことができる。特定のフィールドを使用して、名前および非名前を特定することができる。例えば、準構造化データ302は、フィールドのうちの1つがアーティストフィールドである音楽情報を特定するXMLファイルを含むことができる。 The semi-structured data 302 can include, for example, an xml file. Semi-structured data 302 can include data having many different fields. Certain fields can be used to identify names and non-names. For example, the semi-structured data 302 can include an XML file that identifies music information where one of the fields is an artist field.
大規模な注釈のないコーパス308は、ターゲット言語(例えば、中国語、日本語、または韓国語)でテキストのコレクションを提供する。大規模な注釈のないコーパス308は、多くの異なったソースのテキスト(例えば、ウェブクエリー、ウェブページ、及びニュース記事を含む)を具備することができる。一部の実施例では、大規模な注釈のないコーパス308は、およそ数百から数千億の(又はさらに多くの)文字のテキストを含む。
The large annotation-
注釈プロセス316は、適用され、かつ、名前検出モデル314のサブモデルをトレーニングするために使用されるトレーニングデータのサブセットを形成する。特に、名前を特定するn-グラムと名前を特定しないn-グラムとの確率評価(未加工名前検出モデル206を生成するために、小規模な注釈付コーパス202から決定される)を使用して、名前を特定するトレーニングデータ、および名前を特定しないトレーニングデータにトレーニングデータを分離する。 Annotation process 316 forms a subset of training data that is applied and used to train a sub-model of name detection model 314. In particular, using a probabilistic evaluation of n-grams that specify names and n-grams that do not specify names (determined from a small annotated corpus 202 to generate a raw name detection model 206) Separate the training data into training data that specifies a name and training data that does not specify a name.
システムは、注釈付準構造化データ(例えば、304及び306)を形成するために、注釈プロセス316を準構造化データ302に適用する。特に、未加工名前検出モデル206を使用して、準構造化データ302を分離し、かつ、名前304を特定するn-グラムを含んでいる注釈付準構造化データのサブセットを形成し、かつ、名前306を特定しないn-グラムを含んでいる注釈付準構造化データのサブセットを形成する。例えば、xmlファイルが「アーティスト: c1 c2 c3」というn-グラムを含み、ここで、「c1 c2 c3」がCJK名前である場合、n-グラムは、名前304を特定するn-グラムを含んでいる注釈付準構造化データのサブセットに配置される。別の例として、xmlファイルが「タイトル: c4 c5」というn-グラムも含み、ここで、「c4 c5」が名前(例えば、歌のタイトル)を特定しない場合、n-グラムは、名前306を特定しないn-グラムを含んでいる注釈付準構造化データのサブセットに配置される。
The system applies annotation process 316 to semi-structured data 302 to form annotated semi-structured data (eg, 304 and 306). In particular, the raw
また、システムは、大規模な注釈付コーパスデータ(例えば、310及び312)を形成するために、大規模な注釈のないコーパス308に注釈プロセス316を適用する。特に、未加工名前検出モデル206を使用して、名前310を特定するn-グラムを含んでいる1セットの大規模な注釈付コーパスデータ、および名前312を特定しないn-グラムを含んでいる1セットの大規模な注釈付データに、大規模な注釈のないコーパスを分離する。例えば、ウェブページ文が文字シーケンス「c1 c2 c3 c4 c5 c6」(ここで、「c2 c3 c4」はCJK名前である)を含んでいる場合、次いで、文は、名前310を特定するn-グラムを含んでいる大規模な注釈付コーパスデータのセットの中に配置される。代わりに、文に適用されるとき、注釈プロセス316が名前を検出しない場合、文は、名前312を特定しないn-グラムを含んでいる大規模な注釈付コーパスデータのセットの中に配置される。
The system also applies an annotation process 316 to the large
トレーニングプロセス318は、名前検出モデル314を生成するために、注釈付準構造データ(例えば、304及び306)と大規模な注釈付コーパスデータ(例えば、310及び312)とを使用する(図4に関して以下で詳細に論じる)。 The training process 318 uses annotated semi-structured data (e.g., 304 and 306) and large-scale annotated corpus data (e.g., 310 and 312) to generate the name detection model 314 (with respect to FIG. 4). (Discussed in detail below).
一部の実施例では、注釈のないデータは、ユーザ入力データ(例えば、IMEのスクリプト、及び、単語又は句のユーザによって編集されたリストを含む)を具備することができる。システムは、名前を特定する注釈付ユーザ入力データと、名前を特定しない注釈付ユーザ入力データとを形成するために、注釈プロセス316をユーザ入力データに適用する。次いで、トレーニングプロセス318は、名前検出モデル314を生成するために、注釈付ユーザ入力データを使用する。 In some embodiments, unannotated data may comprise user input data (eg, including IME scripts and lists edited by the user of words or phrases). The system applies an annotation process 316 to the user input data to form annotated user input data that specifies a name and annotated user input data that does not specify a name. The training process 318 then uses the annotated user input data to generate the name detection model 314.
§名前検出モデル
図4は、例の名前検出モデル314のコンポーネントを図示したブロック図である。名前検出モデル314は、名前モデル402と、非名前モデル404と、言語モデル406とを具備している。
§Name Detection Model FIG. 4 is a block diagram illustrating the components of the example name detection model 314. The name detection model 314 includes a name model 402, a non-name model 404, and a language model 406.
§§名前モデル
名前304を特定するn-グラムを含んでいる準構造化データのサブセットと、名前310を特定するn-グラムを含んでいる大規模な注釈付コーパスデータのセットとを使用して、名前モデル402を導出する。システムは、姓および名を含むn-グラムが名前を特定する確率、又は、Pname(family_name, given_name)を決定するために、これらのデータのセットを使用する。
§§Name model Using a subset of semi-structured data containing n-
特に、名前304を特定するn-グラムを含んでいる準構造化データのサブセットと、名前を特定するn-グラムを含んでいる大規模な注釈付コーパスデータのセットとを使用して、データのセットにおいて出現する名前を特定するn-グラムの相対度数の関数として、確率評価を生成する。
In particular, using a subset of semi-structured data containing n-
一部の実施例では、注釈付ユーザ入力を使用して、確率評価を生成する。 In some embodiments, annotated user input is used to generate a probability rating.
§§非名前モデル
名前を特定しないnグラムを含んでいる準構造化データのサブセットを使用して、非名前モデル404を導出する。システムは、姓および名を含むn-グラムが名前を特定しない確率、又は、Pnotname(family_name, given_name)を決定するために、このデータのサブセットを使用する。
§§Unnamed model A non-named model 404 is derived using a subset of semi-structured data containing n-grams that do not specify a name. The system uses this subset of data to determine the probability that an n-gram that includes first and last names does not specify a name, or Pnotname (family_name, given_name).
特に、このデータのサブセットを使用して、データのサブセットにおいて名前を特定するn-グラムの相対度数の関数として確率評価を生成する。 In particular, this subset of data is used to generate a probability estimate as a function of the relative frequency of n-grams that identify names in the subset of data.
一部の実施例では、注釈付ユーザ入力を使用して、確率評価を生成する。 In some embodiments, annotated user input is used to generate a probability rating.
§§言語モデル
大規模な注釈付データ(例えば、310及び312)のセットを使用して、言語モデル406を導出する。システムは、コンテキストを使用することで確率(n-グラムが名前を特定するか、または名前を特定しない)を決定するために、これらのデータのセットを使用する。特に、システムは、名前を伴う言語サブモデルを導出するために、確率(接尾語が名前候補を付与された名前を特定し、かつ、名前候補が接頭語を付与された名前を特定する)、又は、Pname(suffix | name)およびPname(name | prefix)を決定する。
§§Language model A language model 406 is derived using a large set of annotated data (eg, 310 and 312). The system uses these sets of data to determine probabilities (n-grams specify names or do not specify names) using contexts. In particular, the system uses probabilities (names with suffixes to identify candidate names, and name candidates to identify prefixed names) to derive language submodels with names, Alternatively, Pname (suffix | name) and Pname (name | prefix) are determined.
その上、システムは、名前を伴わない言語サブモデルを導出するために、確率(接尾語が名前候補を付与された名前を特定せず、かつ、名前候補が接頭語を付与された名前を特定しない)、又は、Pnotname(suffix | name)およびPnotname(name | prefix)を決定する。 In addition, the system identifies probabilities (names whose suffixes are not given name candidates and whose name candidates are prefixed) to derive language submodels without names. Not), or Pnotname (suffix | name) and Pnotname (name | prefix) are determined.
接頭語は、nグラム名前候補に先行する文字シーケンスのうちの１又は複数の文字である。接尾語は、n-グラム候補に続く文字シーケンスのうちの１又は複数の文字である。例えば、「c1 c2 c3 c4 c5 c6 c7」という文字シーケンスに対しては、名前候補は「c3 c4 c5」であり、接頭語は「c1 c2」であり、かつ、接尾語は「c6 c7」である。 A prefix is one or more characters of a character sequence preceding an n-gram name candidate. A suffix is one or more characters in a character sequence that follows an n-gram candidate. For example, for the character sequence "c1 c2 c3 c4 c5 c6 c7", the name candidate is "c3 c4 c5", the prefix is "c1 c2", and the suffix is "c6 c7" is there.
名前310を特定するn-グラムを含んでいる大規模な注釈付データのセットを使用して、特定の接頭語又は接尾語を付与されたデータのセットにおいて名前であるn-グラムの相対度数の関数として確率評価を生成する。また、名前312を特定しないn-グラムを含んでいる大規模な注釈付データのセットを使用して、特定の接頭語又は接尾語を付与されたデータのセットにおいて名前でないn-グラムの相対度数の関数として確率評価を生成する。
Using a large set of annotated data containing n-grams identifying the
一部の実施例では、注釈付ユーザ入力を使用して、確率評価を生成する。 In some embodiments, annotated user input is used to generate a probability rating.
要約すれば、未加工名前検出モデル206を注釈プロセス316で使用して、準構造化データ302と、大規模な注釈のないコーパス308とを分離し、かつ、注釈付準構造化データ(304及び306)と、大規模な注釈付コーパス(310及び312)とを形成する。システムは、名前モデル402、非名前モデル404、および言語モデル406を含む名前検出モデル314をトレーニングするために、この注釈付データとトレーニングプロセス318とを使用する。
In summary, the raw
§名前を検出するための改良された数式
名前モデルおよび言語モデルからの確率評価を使用して、P(NAME | context)を決定する。例えば、文字シーケンスが「c1 c2 c3 c4 c5 c6 c7」であり、かつ、「c3 c4 c5」が名前である場合、次いで、「c3 c4 c5」がコンテキストを付与された名前である(即ち、接頭語が「c1 c2」であり、かつ、接尾語が「c6 c7」である)確率、又は、P(NAME | context)は、上記した式(1)から導出することができる。P(NAME | context)は、以下として表現することができる。
Pname(c3 | prefix)Pname(c4 c5 | c3)Pname(suffix | c3, c4 c5)
§Improved formula for detecting names Using probabilistic evaluations from name and language models, determine P (NAME | context). For example, if the character sequence is `` c1 c2 c3 c4 c5 c6 c7 '' and `` c3 c4 c5 '' is a name, then `` c3 c4 c5 '' is the context-assigned name (i.e. prefix The probability that the word is “c1 c2” and the suffix is “c6 c7”, or P (NAME | context) can be derived from the above equation (1). P (NAME | context) can be expressed as:
Pname (c3 | prefix) Pname (c4 c5 | c3) Pname (suffix | c3, c4 c5)
この式は、一般的に以下のように書き直すことができる。
Pname(family_name | prefix)Pname(given_name | family_name)Pname(suffix | family_name, given_name)
ここで、
This equation can generally be rewritten as:
Pname (family_name | prefix) Pname (given_name | family_name) Pname (suffix | family_name, given_name)
here,
である。 It is.
上記したように、名前モデルをトレーニングして、Pname(family_name, given_name)を決定することができる。その上、言語モデルをトレーニングして、Pname(family_name | prefix)およびPname(suffix | family, given_name)を決定することができる。 As described above, the name model can be trained to determine Pname (family_name, given_name). Moreover, the language model can be trained to determine Pname (family_name | prefix) and Pname (suffix | family, given_name).
同じように、名前モデルおよび言語モデルからの確率評価を使用して、P(NOTNAME | context)を決定する。例えば、文字シーケンスが「c1 c2 c3 c4 c5 c6 c7」であり、かつ、「c3 c4 c5」が名前でない場合、次いで、「c3 c4 c5」がコンテキストを付与された名前でない(すなわち、接頭語が「c1 c2」であり、かつ、接尾語が「c6 c7」である)確率、又は、P(NOTNAME | context)は、上記した式(2)から導出することができる。P(NOTNAME | context)は、以下として表現することができる。
Pnotname(c3 | prefix)Pnotname(c4 c5 | c3)Pnotname(suffix | c4 c5)
Similarly, P (NOTNAME | context) is determined using probability estimates from the name model and the language model. For example, if the character sequence is “c1 c2 c3 c4 c5 c6 c7” and “c3 c4 c5” is not a name, then “c3 c4 c5” is not a context-assigned name (i.e., the prefix is The probability of “c1 c2” and the suffix “c6 c7”) or P (NOTNAME | context) can be derived from the above equation (2). P (NOTNAME | context) can be expressed as:
Pnotname (c3 | prefix) Pnotname (c4 c5 | c3) Pnotname (suffix | c4 c5)
この式は、一般に以下のように書き直すことができる。
Pnotname(family_name | prefix)Pnotname(given_name |family_name)Pnotname(suffix | family_name, given_name)
This equation can generally be rewritten as:
Pnotname (family_name | prefix) Pnotname (given_name | family_name) Pnotname (suffix | family_name, given_name)
上記したように、非名前モデルをトレーニングして、Pnotname(family_name, given_name)を決定することができる。その上、言語モデルをトレーニングして、Pnotname(family_name | prefix)および Pnotname(suffix | family_name, given_name)を決定することができる。 As described above, the non-name model can be trained to determine Pnotname (family_name, given_name). Furthermore, the language model can be trained to determine Pnotname (family_name | prefix) and Pnotname (suffix | family_name, given_name).
§トレーニング繰り返し
一部の実施例では、名前検出モデル314をさらに使用して、準構造化データ302と大規模な注釈のないコーパス308とを、注釈付準構造化データ(304及び306)と大規模な注釈付コーパス(310及び312)とに分離する。例えば、図3では、名前検出モデル314は、注釈プロセス316で使用されて、準構造化データ302と大規模な注釈のないコーパス308とを分離する。一部の実施例では、これらの新しいトレーニングデータのセットを使用して、より改良された名前検出モデルを生成する。より改良された名前検出モデルは、名前を特定するか、または名前を特定しないn-グラムの確率評価を導出するために、より大規模なトレーニングデータの使用によって、未加工名前検出モデルより広い適用範囲を有する。
§Training repetition In some embodiments, the name detection model 314 is further used to convert the semi-structured data 302 and the large
一部の実施例では、注釈付ユーザ入力を使用して、より改良された名前検出モデルを生成する。 In some embodiments, an annotated user input is used to generate a more improved name detection model.
名前検出モデルの更なる改良は、2回以上の繰り返しで、名前検出モデルをトレーニングすることによって達成することができる。各繰り返しは、名前モデルの適用範囲を向上させる。一部の実施例では、繰り返しの回数(例えば、3回の繰り返し)を指定することができる。代わりに、繰り返しの回数は条件に基づくことができる(例えば、名前検出モデルによる出力として提供される確率評価が、繰り返し間にしきい値以上に変化しないという条件)。 Further improvement of the name detection model can be achieved by training the name detection model in two or more iterations. Each iteration improves the scope of the name model. In some embodiments, the number of iterations (eg, 3 iterations) can be specified. Alternatively, the number of iterations can be based on a condition (eg, the condition that the probability assessment provided as output by the name detection model does not change more than a threshold between iterations).
§名前検出モデルへの更なる改良
相対度数は、特定の名前(例えば、トレーニングデータにおいて低い出現頻度を有する疎な名前、疎な姓、または外国人の名前)に対して低い場合がある。その結果、対応する確率評価は不正確であり得る。これは、追加的な疎データ問題をもたらす。したがって、平滑化技術を使用して、低頻度、または疎な名前を計上することができる。トレーニングデータに出現する文字シーケンスの頻度がしきい値より低い場合、平滑化技術を使用することができる。
§ Further improvements to the name detection model The relative frequency may be low for certain names (eg, sparse names, sparse surnames, or foreign names with low frequency of appearance in training data). As a result, the corresponding probability evaluation can be inaccurate. This introduces additional sparse data problems. Thus, smoothing techniques can be used to account for infrequent or sparse names. If the frequency of the character sequence appearing in the training data is lower than the threshold, a smoothing technique can be used.
§§疎な名前
一部の実施例では、出現する名前の確率は、出現する姓および出現する名の確率から独立している。例えば、「y」が姓「x」に対する名である場合、次いで、名前は「xy」である。その上、「z」は疎な姓である。名前「zy」は疎な姓「z」及び名「y」を表し、ここで、疎な姓「z」は、サンプリングされなかったか、または低頻度でサンプリングされる(例えば、指定されたしきい値頻度より低い)。１実施例では、システムは、「zy」の確率を近似するために、「xy」の確率を使用する。特に、「x」が姓であるというイベントと、「y」が名であるというイベントとの確率は、独立して扱われる。
§§ Sparse Names In some embodiments, the probability of an appearing name is independent of the probable surname and the probable name probabilities. For example, if “y” is the first name for the last name “x”, then the name is “xy”. In addition, “z” is a sparse surname. The name “zy” represents the sparse surname “z” and the surname “y”, where the sparse surname “z” was not sampled or was sampled infrequently (e.g., the specified threshold Lower than the value frequency). In one embodiment, the system uses the probability of “xy” to approximate the probability of “zy”. In particular, the probabilities of the event that “x” is a surname and the event that “y” is a first name are handled independently.
その結果、疎な姓「z」を付与された出現する名「y」の確率、又は、P(y| z)は、「xy」の統計の形で近似することができる。ここで、 As a result, the probability of the appearing first name “y” given the sparse surname “z”, or P (y | z), can be approximated in the form of “xy” statistics. here,
である。 It is.
例えば、トレーニングデータにおいて「zy」の頻度がしきい値より低い場合、「z y」が名前である確率は、「y」があらゆる名前に対して名である確率と、姓「z」出現の確率との関数である。 For example, if the frequency of “zy” is lower than the threshold in the training data, the probability that “zy” is a name is the probability that “y” is a first name for every name and the probability that a surname “z” will appear. And a function.
例えば、名前を検出するために改良された数式に戻れば、Pnotname(suffix | family _name, given_name)は、正確に推定できないかもしれない。一部の実施例では、Pnotname(suffix | family _name, given_name)が以下として表すことができるように、バックオフストラテジーを実施することができる。
BackoffWeight(family_name, given_name)Pnotname(suffix | all_family_names, given_name).
For example, Pnotname (suffix | family_name, given_name) may not be able to be estimated accurately if we return to an improved formula to detect the name. In some embodiments, a backoff strategy can be implemented such that Pnotname (suffix | family_name, given_name) can be expressed as:
BackoffWeight (family_name, given_name) Pnotname (suffix | all_family_names, given_name).
§§疎な姓
一部の実施例では、すべての疎な姓の確率は、単一の疎な姓の確率のための置換として使用される。例えば、「a」は名であり、かつ、「b」は姓である。コンテキストを付与された出現する名前の確率を、P(a | b)P(b | context)によって表すことができる。「b」が疎な姓である場合、確率P(a | b)は不正確である場合がある。この実施例では、所定のコンテキストで出現する名前の確率は、確率(すべての疎な姓がコンテキストを付与されたトレーニングデータにおいて出現する)によって多重化されたすべての疎な姓を付与されたトレーニングデータにおいて「a」が出現する確率、又は、
P(a | all_sparse_family_name)P(b | all_sparse_family)P(all_sparse_family_name | context)
を使用することによってより正確に表される。
§§ Sparse Surnames In some embodiments, all sparse surname probabilities are used as replacements for a single sparse surname probability. For example, “a” is a first name and “b” is a last name. The probability of an appearing name given a context can be represented by P (a | b) P (b | context). If “b” is a sparse surname, the probability P (a | b) may be inaccurate. In this example, the probability of a name appearing in a given context is the training given all sparse surnames multiplexed by the probability (all sparse surnames appear in the contextual training data). The probability of occurrence of "a" in the data, or
P (a | all_sparse_family_name) P (b | all_sparse_family) P (all_sparse_family_name | context)
It is expressed more accurately by using.
§§外国人の名前検出モデル
外国人の名前(例えば、翻訳された名前)の相対度数も低く、不正確な確率評価をもたらす。したがって、外国人の名前検出モデルを、名前検出モデル314を生成するステップに関して上記した同じステップに従って、生成することができる。特に、未加工外国人の名前検出モデルは、未加工名前検出モデル206を生成するステップと同じように、外国人の姓の事前に定義されたコレクションから生成される。未加工外国人の名前検出モデルを他のデータ(例えば、大規模な注釈のないデータおよび準構造化データ)に適用して、名前検出モデル314を生成するステップと同じように、外国人の名前検出モデルを生成することができる。
§§ Foreign name detection model The relative frequency of foreign names (eg, translated names) is also low, resulting in an inaccurate probability assessment. Thus, a foreign name detection model can be generated according to the same steps described above with respect to generating the name detection model 314. In particular, the raw foreign name detection model is generated from a predefined collection of foreign family names, similar to the step of generating the raw
§区分化
n-グラムの所定の入力シーケンスに対して名前を検出するために、名前検出モデルを使用するとき、名前を特定するか、または名前を特定しないn-グラムの確率評価を使用して、文字シーケンスを単語に区分し、かつ、同時に、名前を検出する。
§Segmentation
When using a name detection model to detect names against a given input sequence of n-grams, character sequences using name-specific or non-name-specific n-gram probability evaluation Are divided into words, and at the same time, names are detected.
一部の実施例では、CJK文字シーケンスは隠れマルコフモデルで配列される。隠れマルコフモデルは、隠されたパラメタと観測可能なパラメタとを含んでいる統計モデルである。例えば、観測可能なパラメタはCJK文字シーケンスであり、かつ、隠されたパラメタはCJK単語の可能性のあるシーケンスである。特に、CJK文字又はCJK文字の組み合わせが異なった意味を持つことができるので、CJKの文字の特定のシーケンスは、単語の１又は複数のシーケンスをもたらすことができる。例えば、文字シーケンス「c1 c2 c3」はCJK単語の可能性のあるシーケンスである。さらに、「c1 c2」も、別のCJK単語の可能性のあるシーケンスであり得る。 In some embodiments, CJK character sequences are arranged in a hidden Markov model. A hidden Markov model is a statistical model that includes hidden parameters and observable parameters. For example, the observable parameter is a CJK character sequence, and the hidden parameter is a possible sequence of CJK words. In particular, because a CJK character or combination of CJK characters can have different meanings, a particular sequence of CJK characters can result in one or more sequences of words. For example, the character sequence “c1 c2 c3” is a possible sequence of CJK words. Further, “c1 c2” may also be a possible sequence of another CJK word.
一部の実施例では、ビタビアルゴリズムを使用して、隠れマルコフモデルを区分する。ビタビアルゴリズムは、観測されたイベントのシーケンスをもたらす隠された状態の最も尤もらしいシーケンス(例えば、区分化経路)を見つけ出すための動的プログラミングアルゴリズムである。例えば、ビタビアルゴリズムを使用して、CJK文字のシーケンスをもたらすCJK単語の最も尤もらしいシーケンスを見つけ出す。 In some embodiments, a Viterbi algorithm is used to partition the hidden Markov model. The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states (eg, a segmented path) that results in a sequence of observed events. For example, the Viterbi algorithm is used to find the most likely sequence of CJK words that results in a sequence of CJK characters.
CJK単語の最も尤もらしいシーケンスを以下のように書くことができる。 The most likely sequence of CJK words can be written as
これは、CJK単語のシーケンスである、W(CJK単語のすべての可能性のあるシーケンスの中から)を記述し、P(W | C)のための最高値を提供し、ここで、W= w1, w2, ...wMであり、かつ、Cは、C=c1,c2, ...cLによって表されるCJK単語のシーケンスである。加えて、ベイズ規則は、 This describes the sequence of CJK words, W (among all possible sequences of CJK words), and provides the highest value for P (W | C), where W = w 1, w 2, a ... w M, and, C is, C = c 1, c 2 , a CJK sequence of words represented by ... c L. In addition, Bayesian rules
を提供する。 I will provide a.
言語モデルはP(W)を提供する。ベイズ規則を使用することで、CJK文字のシーケンスを付与された、CJK単語の最も尤もらしいシーケンスを以下として書き直すことができる。 The language model provides P (W). Using Bayesian rules, the most likely sequence of CJK words given a sequence of CJK characters can be rewritten as:
その結果、最も尤もらしいW(すなわち、CJK単語の最も尤もらしいシーケンス)は、Wが出現する確率と、WがCからなる確率(すなわち、CJK単語の所定のシーケンスが、CJK文字のシーケンスを写像する確率)との積を最大化するものである。 As a result, the most likely W (i.e., the most likely sequence of CJK words) is the probability that W will appear and the probability that W will consist of C (i.e., a given sequence of CJK words maps a sequence of CJK characters. The product with the probability of
CJK名前検出は、それが文字シーケンスを単語に区分しているとしてCJK名前を検出する。 CJK name detection detects a CJK name as it divides the character sequence into words.
図5を参照すれば、例えば、観測されたCJK文字502の入力ストリングは、
Referring to FIG. 5, for example, the observed input string of
を含み、ここで、 Where, where
はシーケンスの開始を指定する識別子<S>によって先行され、かつ、 Is preceded by an identifier <S> that specifies the start of the sequence, and
はシーケンスの終了を指定する識別子<E>が後に続いている。 Followed by an identifier <E> specifying the end of the sequence.
文字シーケンス Character sequence
および and
がトレーニングにおいて以前に検出された単語であると仮定する。さらに、 Is the word previously detected in the training. further,
および and
が潜在的名前であると仮定する(すなわち、トレーニングデータにおいて名前を特定するとして、検出された)。単純モデルでは、 Is a potential name (ie, detected as identifying a name in the training data). For simple models,
および and
が単語として検出されなかった場合、 Is not detected as a word,
および and
が単語であるという確率は低く、かつ、文字シーケンスは、単一の文字におそらく区分される。この区分化スキームの後に名前を検出するステップはエラーを生じさせる。 Is unlikely to be a word, and the character sequence is probably partitioned into single characters. The step of finding the name after this partitioning scheme causes an error.
単純モデルでは、隠れマルコフモデル(例えば、隠れマルコフモデル500)における単語の一部の例の区分化は、 For simple models, the segmentation of some examples of words in a hidden Markov model (e.g., Hidden Markov Model 500) is
である。 It is.
しかし、名前検出モデルを組み込めば、文字シーケンス However, if you incorporate a name detection model, the character sequence
は、潜在的に名前を特定する文字として検出することができ、かつ、文字シーケンス Can be detected as a potentially name-identifying character and a character sequence
も潜在的に名前を特定する文字として検出することができる。これらの文字シーケンスは、単語であることの関連付けられた確率を有する(文字シーケンスが、潜在的に名前を特定することの関連付けられた確率を有するという意味で)。 Can also be detected as potentially identifying characters. These character sequences have an associated probability of being a word (in the sense that a character sequence has an associated probability of potentially identifying a name).
したがって、単語の他の例の区分化がモデルに追加される。この改良された隠れマルコフモデルでは、単語の区分の追加例は以下の通りである。 Thus, another example segmentation of words is added to the model. In this improved hidden Markov model, additional examples of word segmentation are as follows:
このモデルを使用して、文字シーケンスを区分するステップは、潜在的な名前を含む区分の尤度によって、文字シーケンスを単語に区分するステップを含んでいる。潜在的な名前を含んでいる他の尤もらしいシーケンスの導入は、前述の区分化エラーが名前検出に伝播するのを回避させる。名前を含む区分経路が、名前を含んでいない分割経路より出現しそうである場合、次いで、名前を含む区分経路は使用され、かつ、名前は検出される。検出された名前を特定する文字シーケンスと、名前を特定することのその対応する確率とは、名前検出モデル314に追加される。 Using this model, segmenting the character sequence includes segmenting the character sequence into words according to the likelihood of the segment containing the potential name. The introduction of other plausible sequences that contain potential names prevents the aforementioned segmentation error from propagating to name detection. If a segment path that includes a name is more likely to appear than a segment path that does not include a name, then the segment path that includes the name is used and the name is detected. The character sequence that identifies the detected name and its corresponding probability of identifying the name are added to the name detection model 314.
一部の実施例では、名前検出モデル314を使用して、入力テキストで名前を検出する。例えば、検出器は、同時に、CJK入力テキストを単語に区分し、かつ、CJK入力テキストから名前を検出するために、CJK入力テキストを受信し、かつ、名前検出モデル314を使用する。 In some embodiments, the name detection model 314 is used to detect names in the input text. For example, the detector simultaneously receives the CJK input text and uses the name detection model 314 to segment the CJK input text into words and to detect names from the CJK input text.
図6は、名前を検出するための例のプロセス600を示したフローチャートである。便宜上、名前を検出するためのプロセス600は、検出を実行するシステムに関して説明される。名前を検出するためのプロセス600の間、システムは、名前のためのシーケンスの開始からシーケンスの終了まで、受信された文字シーケンスをスキャンする。 FIG. 6 is a flowchart illustrating an example process 600 for detecting names. For convenience, the process 600 for detecting names is described with respect to a system that performs detection. During the process 600 for finding a name, the system scans the received character sequence from the start of the sequence for the name to the end of the sequence.
システムは、文字シーケンス(例えば、漢字シーケンス)を受信する602。特に、システムは、シーケンスの第1文字を特定する604。システムは、特定された文字が姓の候補であるか否かを決定する606。文字が姓の候補である場合(例えば、姓のコレクション204の中の文字)、システムは、上記したように、名前を検出するための改良された数式(例えば、改良を伴う比率)を使用することで名前を検出する614。 The system receives 602 a character sequence (eg, a Chinese character sequence). In particular, the system identifies 604 the first character of the sequence. The system determines 606 whether the identified character is a surname candidate. If the character is a surname candidate (eg, a character in the surname collection 204), the system uses an improved formula (eg, a ratio with refinement) to detect the name, as described above. 614 to detect the name.
文字が姓の候補でない場合、次いで、システムは、文字が姓の候補の接頭語であるか否かを決定する608。文字が姓の候補の接頭語である場合、次いで、システムは、上記したように、名前を検出するための改良された数式(例えば、改良を伴う比率)を使用することで名前を検出する614。
If the character is not a surname candidate, the system then determines 608 whether the character is a surname candidate prefix. If the letter is a candidate surname prefix, then the system detects the
文字が姓の候補の接頭語でない場合、次いで、システムは、システムが文字シーケンスの終端に達した否かを決定する610。また、同様に、名前を検出するための改良された数式を使用することで名前を検出した614後に、システムは、システムが文字シーケンスの終端に達したか否かを決定する610。システムがシーケンスの終端に達した場合、プロセスは終了する616。システムが文字シーケンスの終端に達していない場合、次いで、システムは、シーケンスの次の文字を特定し612、かつ、シーケンスの終端に達するまで、シーケンスの他の文字に対して、ステップ606、608、610、および選択的に614を繰り返す。
If the character is not a surname candidate prefix, then the system determines 610 whether the system has reached the end of the character sequence. Similarly, after 614 detecting a name using an improved formula for detecting the name, the system determines 610 whether the system has reached the end of the character sequence. If the system reaches the end of the sequence, the process ends 616. If the system has not reached the end of the character sequence, then the system identifies the next character in the
§例のシステム
図7は、CJK名前検出の例のシステム700である。データ処理装置710は、検出プログラム720を含む、ハードウェア/ファームウェア、オペレーティングシステム、および１又は複数のプログラムを含むことができる。検出プログラム720は、本明細書に記載されたオペレーションを実行するために、データ処理装置710に関連して、動作する。その結果、検出プログラム720は、１又は複数のプロセッサおよびコンピュータ可読媒体(例えば、メモリ)を組み合わせて、システム700の１又は複数の構造上のコンポーネントを表す。
Example System FIG. 7 is an example system 700 for CJK name detection. The data processing device 710 can include hardware / firmware, an operating system, and one or more programs, including a
検出プログラム720は、検出処理アプリケーション、または部分であってもよい。本明細書で使用されるように、アプリケーションは、ユーザが定義された目的に使用される異なったコンピュータツールとして知覚するコンピュータプログラムである。アプリケーションは、データ処理装置710のオペレーティングシステム(OS)に完全に組み込むことができ、または、アプリケーションは、異なったロケーションに配置された異なったコンポーネントを有することができ(例えば、OS又はカーネルモードにおける1部分、ユーザモードにおける1部分、およびリモートサーバにおける1部分)、かつ、アプリケーションは、装置710のソフトウェアプラットホームとして機能するランタイムライブラリ上に組み込むことができる。そのうえ、アプリケーション処理は、１又は複数のプロセッサ790を使用することでネットワーク780の上に分散することができる。例えば、検出プログラム720の言語モデルは、１又は複数のプロセッサ790上で分配的にトレーニングすることができる。
The
データ処理装置710は、１又は複数のプロセッサ730と、少なくとも1つのコンピュータ可読媒体740 (例えば、ランダムアクセスメモリ、ストレージデバイスなど)とを備えている。また、データ処理装置710は、通信インターフェース750、１又は複数のユーザインターフェースデバイス760、および１又は複数の追加デバイス770を備えることができる。ユーザインターフェースデバイス760は、ディスプレイスクリーン、キーボード、マウス、スタイラス、またはこれらの組み合わせを備えることができる。
The data processing device 710 includes one or more processors 730 and at least one computer readable medium 740 (eg, random access memory, storage device, etc.). The data processing device 710 may also include a
いったんプログラムされると、データ処理装置710は、言語モデル、名前モデル、および外国人の名前モデルを生成するよう動作する。 Once programmed, the data processing device 710 operates to generate a language model, a name model, and a foreign name model.
本発明の実施例及び本明細書に開示された機能操作は、本明細書で開示された構造又はそれらの構造的等価物、又は、それらの１又は複数の組合せを含む、デジタル電子回路又はコンピュータソフトウェア、ファームウェア又はハードウェアで実行することができる。本明細書に開示された本発明の実施例は、１又は複数のコンピュータプログラム製品として実行することができる(即ち、データ処理装置によって実行するためか、又は、その動作を制御するために、有形のプログラムキャリア上にコード化されたコンピュータプログラム命令の１又は複数のモジュール)。有形のプログラムキャリアは、伝播信号又はコンピュータ可読媒体であり得る。伝播信号は、例えば、機械により発生された電気的、光学的又は電磁的信号等の人工的に発生された信号であり、好適な受信装置への転送用に情報を符号化するために発生される。コンピュータ可読媒体は、マシーン可読記憶装置、マシーン可読記憶基板、メモリ装置、又は、マシーン可読伝播信号を生じるものの構成、又は、それらの１又は複数の組み合わせであり得る。用語「データ処理装置」は、例として、プログラム可能プロセッサ、コンピュータ、又は、多重プロセッサ又は多重コンピュータを含む、データを処理するための全ての装置、デバイス及びマシーンを包含する。装置は、ハードウェアに加えて、問題になっているコンピュータプログラムのための実行環境を創り出すコード(例えば、プロセッサファームウェア、プロトコルスタック、データベース管理システム、オペレーティングシステム、又は、それらの１又は複数の組合せを構成するコード)を含むことができる。 Embodiments of the invention and the functional operations disclosed herein include digital electronic circuits or computers comprising the structures disclosed herein or their structural equivalents, or one or more combinations thereof. It can be implemented in software, firmware or hardware. The embodiments of the invention disclosed herein may be implemented as one or more computer program products (ie, tangible for execution by a data processing device or to control its operation). One or more modules of computer program instructions encoded on the program carrier). The tangible program carrier can be a propagated signal or a computer readable medium. Propagation signals are artificially generated signals, such as electrical, optical or electromagnetic signals generated by a machine, for example, to encode information for transfer to a suitable receiving device. The The computer readable medium may be a machine readable storage device, a machine readable storage substrate, a memory device, or a configuration of one that produces a machine readable propagation signal, or one or more combinations thereof. The term “data processing device” encompasses all devices, devices, and machines for processing data, including, by way of example, programmable processors, computers, or multiple processors or multiple computers. In addition to hardware, the device contains code that creates an execution environment for the computer program in question (e.g., processor firmware, protocol stack, database management system, operating system, or one or more combinations thereof). Can be configured code).
コンピュータプログラム(プログラム、ソフトウェア、ソフトウェアアプリケーション、スクリプト、又はコードとしても知られる)は、コンパイル又はインタープリタ言語を含むどのような形式のプログラミング言語ででも書くことができ、スタンドアローンプログラムとして、又は、モジュール、コンポーネント、サブルーチンとして、又は、コンピュータ環境での使用に好適な他のユニットを含む形式で配備することができる。コンピュータプログラムは必ずしもファイルシステム中のファイルに対応しなくてもよい。プログラムは、他のプログラム又はデータを保持するファイルの一部に(例えば、マークアップ言語文書中に蓄積された１又は複数のスクリプト)、問題になっているプログラムに専用の単一のファイルに、又は、複数の組織的なファイル(例えば、１又は複数のモジュール、サブプログラム、又はコードの一部を格納するファイル)に、蓄積することができる。コンピュータプログラムは、１つのコンピュータ、又は、複数のコンピュータ(１つのサイトに配置されたか、又は、複数サイトにわたって分散され、かつ、通信ネットワークによって相互接続された)で実行するために配備することができる。 A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including a compiled or interpreted language, as a stand-alone program or as a module, It can be deployed as a component, subroutine, or in a form that includes other units suitable for use in a computer environment. A computer program does not necessarily correspond to a file in a file system. The program may be part of a file that holds other programs or data (eg, one or more scripts stored in a markup language document), a single file dedicated to the program in question, Or it can accumulate in multiple organized files (eg, files that store one or more modules, subprograms, or portions of code). A computer program can be deployed to run on one computer or on multiple computers (located at one site or distributed across multiple sites and interconnected by a communications network) .
本明細書で説明したプロセスおよび論理の流れは、入力データに対して動作して出力を生成することにより機能を実行する１又は複数のコンピュータプログラムを実行する、１又は複数のプログラム可能なプロセッサによって実行することができる。例えば、ＦＰＧＡ(Field Programmable Gate Array、フィールドプログラマブルゲートアレイ)又はＡＳＩＣ(Application-Specific Integrated Circuit、特定用途向け集積回路)等の特定用途論理回路により、そのプロセスおよび論理の流れを実行することもでき、装置を実装することもできる。 The process and logic flows described herein are performed by one or more programmable processors that execute one or more computer programs that perform functions by operating on input data and generating output. Can be executed. For example, the process and the logic flow can be executed by a special purpose logic circuit such as FPGA (Field Programmable Gate Array) or ASIC (Application-Specific Integrated Circuit). A device can also be implemented.
コンピュータプログラムの実行に好適なプロセッサには、例として、汎用および特定目的用の両方のマイクロプロセッサ、及び、どのような種類のデジタルコンピュータでも１又は複数のプロセッサが含まれる。一般に、プロセッサは読み出し専用メモリ又はランダムアクセスメモリ又はその両方から、命令及びデータを受け取る。コンピュータの本質的要素は、命令を実行するプロセッサ、及び、命令及びデータを格納する１又は複数のメモリである。コンピュータはまた、一般に、データを格納するための１又は複数の大容量記憶装置(例えば、磁気、磁気光学ディスク、又は光学ディスク)を含むか、又は、それらからデータを受信し又はそれらへデータを送信し、又は、その両方を実行できるようそれらに接続される。しかし、コンピュータはそのような装置を持たなくてもよい。さらに、コンピュータは、他の装置(例えば、例を挙げると、ＰＤＡ(Personal Digital Assistance、携帯型情報端末)、ＧＰＳ(Global Positioning System、全地球測位システム))に組み込むことができる。 Processors suitable for executing computer programs include, by way of example, both general and special purpose microprocessors and any type of digital computer with one or more processors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor that executes instructions and one or more memories that store instructions and data. Computers also typically include or receive data from or receive data from one or more mass storage devices (eg, magnetic, magneto-optical disks, or optical disks) for storing data. Connected to them so that they can transmit or both. However, a computer need not have such a device. Furthermore, the computer can be incorporated into other devices (for example, PDA (Personal Digital Assistance, portable information terminal), GPS (Global Positioning System)).
コンピュータプログラム命令及びデータを格納するのに好適なコンピュータ可読媒体には、全ての形態の不揮発性メモリ、媒体及びメモリ装置(例としては、ＥＰＲＯＭ、ＥＥＰＲＯＭ、及びフラッシュメモリ装置等の半導体メモリ装置、内蔵ハードディスク又は取り外し可能ディスク等の磁気ディスク装置、磁気光学ディスク、及び、ＣＤ−ＲＯＭ及びＤＶＤ−ＲＯＭディスクを含む)が含まれる。プロセッサ及びメモリは、特定用途論理回路で補足することができ、又は、それに組み込むことができる。 Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices (eg, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices, internal Magnetic disk devices such as hard disks or removable disks, magneto-optical disks, and CD-ROM and DVD-ROM disks). The processor and memory can be supplemented by, or incorporated in, special purpose logic circuitry.
ユーザとの相互作用に備えて、開示された実施例は、コンピュータ(情報をユーザに提示するための表示装置、例えば、ＣＲＴ(cathode ray tube、陰極線管)又はＬＣＤ(liquid crystal display、液晶ディスプレイ)モニタ、及び、ユーザがコンピュータに入力を与えることができるキーボードおよびポインティングデバイス、例えば、マウスまたはトラックボールを備えた)上で実行することができる。その他の種類の装置もまたユーザとの相互作用に備えて使用することができ、例えば、ユーザに提供されるフィードバックは、例えば、視覚的フィードバック、聴覚的フィードバック、又は触覚的フィードバック等のどのような形式の感覚フィードバックであってもよく、かつ、ユーザからの入力は、音響、音声又は触覚入力を含むどのような形式で受信されてもよい。 In preparation for user interaction, the disclosed embodiments include computers (display devices for presenting information to the user, such as CRT (cathode ray tube) or LCD (liquid crystal display)). It can run on a monitor and a keyboard and pointing device (eg, equipped with a mouse or trackball) that allows the user to provide input to the computer. Other types of devices can also be used in preparation for interaction with the user, for example, the feedback provided to the user can be any visual feedback, audio feedback, or tactile feedback, for example There may be some form of sensory feedback and the input from the user may be received in any form including acoustic, voice or haptic input.
本明細書に開示された本発明の実施例は、コンピュータシステム(バックエンド構成要素、例えば、データサーバ等、を含む、又は、ミドルウェア構成要素、例えば、アプリケーションサーバ、を含む、又は、フロントエンド構成要素、例えば、ユーザが本明細書に開示された本発明の実施例と相互作用できるようにするためのグラフィカルユーザインタフェース又はウェブブラウザを有するクライアントコンピュータ、を含む)において実行することができ、又は、そのようなバックエンド、ミドルウェア、又はフロントエンド構成要素の１又は複数のどのような組合せでも実行することができる。システムの構成要素は、通信ネットワーク等のデジタルデータ通信のどのような形態又は媒体によってでも相互接続することができる。通信ネットワークの例には、ローカルエリアネットワーク(「ＬＡＮ」）及びインターネット等の広域ネットワーク（「ＷＡＮ」）が含まれる。 Embodiments of the invention disclosed herein include computer systems (including back-end components, such as data servers, etc.) or middleware components, such as application servers, or front-end configurations. Element, including a client computer having a graphical user interface or web browser to allow a user to interact with the embodiments of the invention disclosed herein, or Any combination of one or more of such backend, middleware, or frontend components can be implemented. The components of the system can be interconnected by any form or medium of digital data communication such as a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”) such as the Internet.
コンピュータシステムはクライアント及びサーバを含むことができる。クライアント及びサーバは、一般には相互に遠く離れており、通常は通信ネットワークを通して相互作用する。クライアント及びサーバの関係は、それぞれのコンピュータ上で実行され、かつ、相互にクライアント−サーバ関係にあるコンピュータプログラムによって、発生する。 The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The client and server relationship is generated by computer programs that are executed on each computer and in a client-server relationship with each other.
本明細書は多くの詳細を含むが、これらは、特許請求の範囲を限定するものとして解釈されてはならず、むしろ、特定の実施例に固有の特徴の説明として解釈されなければならない。別々の実施例との関連で本明細書に説明されたある特徴は、単一の実施例に組み合わせて実行することもできる。逆に、単一の実施例との関連で説明された種々の特徴は、多様な実施例に別々で、又は、適切なサブコンビネーションで実行することもできる。さらに、特徴がある組み合わせで動作するように上記され、かつ、さらにそのように初期に請求されるが、請求された組み合わせから１又は複数の特徴が、一部の場合は、組み合わせから切り取られることが可能であり、かつ、請求された組み合わせは、サブコンビネーション又はサブコンビネーションの変形例に導かれる。 This specification contains many details, which should not be construed as limiting the claims, but rather as a description of features specific to a particular embodiment. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, the various features described in the context of a single embodiment can also be implemented separately in various embodiments or in appropriate subcombinations. Further, a feature is described above to operate in a certain combination, and even so initially claimed, but one or more features from the claimed combination may be cut from the combination in some cases. And the claimed combination leads to a sub-combination or sub-combination variant.
同様に、操作が特定の順序で図面に示されているが、これは、所望の結果を得るために、そのような操作が図示された特定の順序又は起こった順番で実行されるか、又は、図示された全操作が実行される必要があると理解されてはならない。ある環境では、マルチタスク及びパラレル処理が好ましい。さらに、上記した実施例の種々のシステムコンポーネントの分離は、全ての実施例にそのような分離が必要であると理解されてはならず、かつ、上記したプログラムコンポーネント及びシステムは、一般に、単一のソフトウェア製品に一緒に組み込まれるか、又は、複数のソフトウェア製品にパッケージ化されることが可能である。 Similarly, operations are shown in the drawings in a particular order, which may be performed in the particular order shown or in the order in which they occur to achieve the desired result, or It should not be understood that all the operations illustrated need to be performed. In some environments, multitasking and parallel processing are preferred. Furthermore, the separation of the various system components of the embodiments described above should not be understood as requiring such separation in all embodiments, and the program components and systems described above are generally Can be embedded together in a software product or packaged into multiple software products.
このように、特定の実施例について説明がなされた。他の実施例も添付の特許請求の範囲の範囲に含まれている。例えば、請求項に記載されたアクションは、異なった順序で実行することができ、所望の結果をまだ達成させる。１例として、添付図面に示されたプロセスは、示された特定の順序、シーケンシャルの順序を必ずしも必要とすることなく、所望の結果を達成させる。ある実施例では、マルチタスクおよび並列処理が好都合であってもよい。 Thus, specific embodiments have been described. Other embodiments are within the scope of the appended claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. By way of example, the process illustrated in the accompanying drawings achieves the desired result without necessarily requiring the particular order shown, sequential order. In certain embodiments, multitasking and parallel processing may be advantageous.
202 小規模な注釈付コーパス
204 姓のコレクション
206 名のコレクション
402 名前モデル
404 非名前モデル
406 言語モデル
202 Small Annotated Corpus
204 Last Name Collection
206 collections
402 Name Model
404 unnamed model
406 Language Model
Claims (7)
名前検出モデルを複数の文字を有する前記入力ストリングに適用するステップと
を具備し、
前記名前検出モデルを適用するステップは、
１又は複数の名前を含まない、前記複数の文字の最も尤もらしい区分化を特定するステップと、
１又は複数の名前を潜在的に特定するとして、前記複数の文字のうちの１又は複数の文字シーケンスを検出するステップと、
前記１又は複数の潜在的な名前を含んでいる、前記複数の文字の区分化を特定するステップと、
前記潜在的な１又は複数の名前を含む前記区分化の尤度が、１又は複数の名前を含まない最も尤もらしい区分化より大きいとき、前記１又は複数の名前を含むとして、前記複数の文字を区分するステップと
を具備することを特徴とする方法。 Receiving an input string of characters;
Applying a name detection model to the input string having a plurality of characters; and
Applying the name detection model comprises:
Identifying the most likely segmentation of the plurality of characters that does not include one or more names;
Detecting one or more character sequences of the plurality of characters as potentially identifying one or more names;
Identifying a segmentation of the plurality of characters that includes the one or more potential names;
The plurality of characters as including the one or more names when the likelihood of the partitioning including the potential name or names is greater than the most likely partitioning not including the name or names And a step of partitioning.
名前検出モデルを複数の文字を有する前記入力ストリングに適用する手段と
を備え、
前記名前検出モデルを適用する手段は、
１又は複数の名前を含まない、前記複数の文字の最も尤もらしい区分化を特定する手段と、
１又は複数の名前を潜在的に特定するとして、前記複数の文字のうちの１又は複数の文字シーケンスを検出する手段と、
前記１又は複数の潜在的な名前を含んでいる、前記複数の文字の区分化を特定する手段と、
前記潜在的な１又は複数の名前を含む前記区分化の尤度が、１又は複数の名前を含まない最も尤もらしい区分化より大きいとき、前記１又は複数の名前を含むとして、前記複数の文字を区分する手段と
を備えることを特徴とするシステム。 Means for receiving an input string of characters;
Applying a name detection model to the input string having a plurality of characters;
The means for applying the name detection model is:
Means for identifying the most likely segmentation of the plurality of characters that does not include one or more names;
Means for detecting one or more character sequences of the plurality of characters as potentially identifying one or more names;
Means for identifying a segmentation of the plurality of characters including the one or more potential names;
The plurality of characters as including the one or more names when the likelihood of the partitioning including the potential name or names is greater than the most likely partitioning not including the name or names And a means for classifying the system.
有形のプログラムキャリア上にコード化され、
文字の入力ストリングを受信するステップと、
名前検出モデルを複数の文字を有する前記入力ストリングに適用するステップと
を具備するオペレーションをデータ処理装置に実行させ、
前記名前検出モデルを適用するステップは、
１又は複数の名前を含まない、前記複数の文字の最も尤もらしい区分化を特定するステップと、
１又は複数の名前を潜在的に特定するとして、前記複数の文字のうちの１又は複数の文字シーケンスを検出するステップと、
前記１又は複数の潜在的な名前を含んでいる、前記複数の文字の区分化を特定するステップと、
前記潜在的な１又は複数の名前を含む前記区分化の尤度が、１又は複数の名前を含まない最も尤もらしい区分化より大きいとき、前記１又は複数の名前を含むとして、前記複数の文字を区分するステップと
を具備することを特徴とするコンピュータプログラム製品。 A computer program product,
Coded on a tangible program carrier,
Receiving an input string of characters;
Applying a name detection model to the input string having a plurality of characters, causing the data processing apparatus to perform an operation comprising:
Applying the name detection model comprises:
Identifying the most likely segmentation of the plurality of characters that does not include one or more names;
Detecting one or more character sequences of the plurality of characters as potentially identifying one or more names;
Identifying a segmentation of the plurality of characters that includes the one or more potential names;
The plurality of characters as including the one or more names when the likelihood of the partitioning including the potential name or names is greater than the most likely partitioning not including the name or names A computer program product comprising the steps of:
文字の入力ストリングを受信するステップと、
名前検出モデルを複数の文字を有する前記入力ストリングに適用するステップと
を具備するオペレーションを実行するように構成された１又は複数のコンピュータを備え、
前記名前検出モデルを適用するステップは、
１又は複数の名前を含まない、前記複数の文字の最も尤もらしい区分化を特定するステップと、
１又は複数の名前を潜在的に特定するとして、前記複数の文字のうちの１又は複数の文字シーケンスを検出するステップと、
前記１又は複数の潜在的な名前を含んでいる、前記複数の文字の区分化を特定するステップと、
前記潜在的な１又は複数の名前を含む前記区分化の尤度が、１又は複数の名前を含まない最も尤もらしい区分化より大きいとき、前記１又は複数の名前を含むとして、前記複数の文字を区分するステップと
を具備することを特徴とするシステム。 A system,
Receiving an input string of characters;
Applying one or more name detection models to the input string having a plurality of characters, and comprising one or more computers configured to perform operations comprising:
Applying the name detection model comprises:
Identifying the most likely segmentation of the plurality of characters that does not include one or more names;
Detecting one or more character sequences of the plurality of characters as potentially identifying one or more names;
Identifying a segmentation of the plurality of characters that includes the one or more potential names;
The plurality of characters as including the one or more names when the likelihood of the partitioning including the potential name or names is greater than the most likely partitioning not including the name or names And a step of classifying the system.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2013004333A JP5770753B2 (en) | 2013-01-15 | 2013-01-15 | CJK name detection |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2013004333A JP5770753B2 (en) | 2013-01-15 | 2013-01-15 | CJK name detection |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2010536305A Division JP5379155B2 (en) | 2007-12-06 | 2007-12-06 | CJK name detection |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2013109364A true JP2013109364A (en) | 2013-06-06 |
JP5770753B2 JP5770753B2 (en) | 2015-08-26 |
Family
ID=48706101
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2013004333A Active JP5770753B2 (en) | 2013-01-15 | 2013-01-15 | CJK name detection |
Country Status (1)
Country | Link |
---|---|
JP (1) | JP5770753B2 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11238409B2 (en) | 2017-09-29 | 2022-02-01 | Oracle International Corporation | Techniques for extraction and valuation of proficiencies for gap detection and remediation |
US11367034B2 (en) | 2018-09-27 | 2022-06-21 | Oracle International Corporation | Techniques for data-driven correlation of metrics |
US11467803B2 (en) | 2019-09-13 | 2022-10-11 | Oracle International Corporation | Identifying regulator and driver signals in data systems |
US11487729B2 (en) | 2017-12-18 | 2022-11-01 | Yahoo Japan Corporation | Data management device, data management method, and non-transitory computer readable storage medium |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH0721196A (en) * | 1993-06-15 | 1995-01-24 | N T T Data Tsushin Kk | Proper noun specifying method |
JP2004046775A (en) * | 2002-05-15 | 2004-02-12 | Nippon Telegr & Teleph Corp <Ntt> | Device, method and program for extracting intrinsic expression |
JP2004102856A (en) * | 2002-09-12 | 2004-04-02 | Fuji Xerox Co Ltd | Device and method for morpheme string processing |
JP2005092883A (en) * | 2003-09-15 | 2005-04-07 | Microsoft Corp | Chinese word segmentation |
JP2005539283A (en) * | 2001-12-21 | 2005-12-22 | ウエスト パブリッシング カンパニー，ディー．ビー．エー．ウエスト グループ | System, method, and software for hyperlinking names |
JP2006031295A (en) * | 2004-07-14 | 2006-02-02 | Internatl Business Mach Corp <Ibm> | Apparatus and method for estimating word boundary probability, apparatus and method for constructing probabilistic language model, apparatus and method for kana-kanji conversion, and method for constructing unknown word model |
CN101271449A (en) * | 2007-03-19 | 2008-09-24 | 株式会社东芝 | Method and device for reducing vocabulary and Chinese character string phonetic notation |
-
2013
- 2013-01-15 JP JP2013004333A patent/JP5770753B2/en active Active
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH0721196A (en) * | 1993-06-15 | 1995-01-24 | N T T Data Tsushin Kk | Proper noun specifying method |
JP2005539283A (en) * | 2001-12-21 | 2005-12-22 | ウエスト パブリッシング カンパニー，ディー．ビー．エー．ウエスト グループ | System, method, and software for hyperlinking names |
JP2004046775A (en) * | 2002-05-15 | 2004-02-12 | Nippon Telegr & Teleph Corp <Ntt> | Device, method and program for extracting intrinsic expression |
JP2004102856A (en) * | 2002-09-12 | 2004-04-02 | Fuji Xerox Co Ltd | Device and method for morpheme string processing |
JP2005092883A (en) * | 2003-09-15 | 2005-04-07 | Microsoft Corp | Chinese word segmentation |
JP2006031295A (en) * | 2004-07-14 | 2006-02-02 | Internatl Business Mach Corp <Ibm> | Apparatus and method for estimating word boundary probability, apparatus and method for constructing probabilistic language model, apparatus and method for kana-kanji conversion, and method for constructing unknown word model |
CN101271449A (en) * | 2007-03-19 | 2008-09-24 | 株式会社东芝 | Method and device for reducing vocabulary and Chinese character string phonetic notation |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11238409B2 (en) | 2017-09-29 | 2022-02-01 | Oracle International Corporation | Techniques for extraction and valuation of proficiencies for gap detection and remediation |
US11487729B2 (en) | 2017-12-18 | 2022-11-01 | Yahoo Japan Corporation | Data management device, data management method, and non-transitory computer readable storage medium |
US11367034B2 (en) | 2018-09-27 | 2022-06-21 | Oracle International Corporation | Techniques for data-driven correlation of metrics |
US11467803B2 (en) | 2019-09-13 | 2022-10-11 | Oracle International Corporation | Identifying regulator and driver signals in data systems |
Also Published As
Publication number | Publication date |
---|---|
JP5770753B2 (en) | 2015-08-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP5379155B2 (en) | CJK name detection | |
JP5901001B1 (en) | Method and device for acoustic language model training | |
Han et al. | Lexical normalisation of short text messages: Makn sens a# twitter | |
CN107480143B (en) | Method and system for segmenting conversation topics based on context correlation | |
US9836453B2 (en) | Document-specific gazetteers for named entity recognition | |
US8731901B2 (en) | Context aware back-transliteration and translation of names and common phrases using web resources | |
US9454962B2 (en) | Sentence simplification for spoken language understanding | |
US20120047172A1 (en) | Parallel document mining | |
KR20100135819A (en) | Segmenting words using scaled probabilities | |
EP2643770A2 (en) | Text segmentation with multiple granularity levels | |
KR20110083623A (en) | Machine learning for transliteration | |
WO2018057427A1 (en) | Syntactic re-ranking of potential transcriptions during automatic speech recognition | |
EP2707808A2 (en) | Exploiting query click logs for domain detection in spoken language understanding | |
Jayan et al. | A hybrid statistical approach for named entity recognition for malayalam language | |
JP5770753B2 (en) | CJK name detection | |
JP2022510818A (en) | Transliteration of data records for improved data matching | |
Wong et al. | isentenizer-: Multilingual sentence boundary detection model | |
CN110705285B (en) | Government affair text subject word library construction method, device, server and readable storage medium | |
KR20120095914A (en) | Generating input suggestions | |
CN114595696A (en) | Entity disambiguation method, entity disambiguation apparatus, storage medium, and electronic device | |
Kang et al. | A language independent n-gram model for word segmentation | |
Simunec et al. | N-gram Based Croatian Language Network | |
CN103136190B (en) | CJK name detects | |
CN115905297B (en) | Method, apparatus and medium for retrieving data | |
KR20140049148A (en) | Method for part-of-speech tagging based on morpheme segmentation and apparatus thereof |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20130625 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20130913 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20140331 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20140602 |
|
A02 | Decision of refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A02Effective date: 20141222 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20150213 |
|
A911 | Transfer to examiner for re-examination before appeal (zenchi) |
Free format text: JAPANESE INTERMEDIATE CODE: A911Effective date: 20150325 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20150601 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20150625 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 5770753Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
S533 | Written request for registration of change of name |
Free format text: JAPANESE INTERMEDIATE CODE: R313533 |
|
R350 | Written notification of registration of transfer |
Free format text: JAPANESE INTERMEDIATE CODE: R350 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |