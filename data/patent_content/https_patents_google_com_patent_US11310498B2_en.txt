US11310498B2 - Receptive-field-conforming convolutional models for video coding - Google Patents
Receptive-field-conforming convolutional models for video coding Download PDFInfo
- Publication number
- US11310498B2 US11310498B2 US17/086,591 US202017086591A US11310498B2 US 11310498 B2 US11310498 B2 US 11310498B2 US 202017086591 A US202017086591 A US 202017086591A US 11310498 B2 US11310498 B2 US 11310498B2
- Authority
- US
- United States
- Prior art keywords
- block
- partition
- size
- classifier
- feature
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/119—Adaptive subdivision aspects, e.g. subdivision of a picture into rectangular or non-rectangular coding blocks
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/146—Data rate or code amount at the encoder output
- H04N19/147—Data rate or code amount at the encoder output according to rate distortion criteria
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/189—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding
- H04N19/19—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding using optimisation based on Lagrange multipliers
Definitions
- Digital video streams may represent video using a sequence of frames or still images.
- Digital video can be used for various applications, including, for example, video conferencing, high-definition video entertainment, video advertisements, or sharing of user-generated videos.
- a digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data.
- Various approaches have been proposed to reduce the amount of data in video streams, including compression and other encoding techniques.
- Coding efficiency can mean encoding a video at the lowest possible bit rate while minimizing distortion (i.e., while maintaining a certain level of video quality).
- the improved coding efficiency has resulted in increased computational complexity. That is, more computation time is required by an encoder to achieve the improved coding efficiency. As such, it is desirable to obtain improved coding efficiencies with less computation time (i.e., reduced computational complexity).
- a first aspect is an apparatus for encoding a block of a picture.
- the apparatus includes a convolutional neural network (CNN) for determining a block partitioning of the block, the block having an N ⁇ N size and a smallest partition determined by the CNN being of size S ⁇ S.
- the CNN includes feature extraction layers; a concatenation layer that receives, from the feature extraction layers, first feature maps of the block, where each first feature map of the first feature maps is of the smallest possible partition size S ⁇ S of the block; and at least one classifier that is configured to infer partition decisions for sub-blocks of size ( ⁇ S) ⁇ ( ⁇ S) of the block, where ⁇ is a power of 2.
- a second aspect is a method of encoding a block.
- the method includes obtaining, using a convolutional neural network (CNN), partition decisions for sub-blocks of size ( ⁇ S) ⁇ ( ⁇ S) of the block, by steps including obtaining first feature maps from feature extraction layers of the CNN; inputting the first feature maps to a concatenation layer of the CNN, where each first feature map of the first feature maps is of a smallest possible partition size S ⁇ S of the block; and obtaining the partition decisions from classifiers of the CNN, where each classifier of the classifiers infers partition decisions for sub-blocks of size ( ⁇ S) ⁇ ( ⁇ S) of the block for a respective value of ⁇ , where ⁇ is a power of 2.
- the method further includes encoding the block in a compressed bitstream according to the partition decisions.
- a third aspect is an apparatus for decoding a block.
- the apparatus includes a processor that is configured to decode, from a compressed bitstream, partition decisions for the block and decode the block according to the partition decisions.
- the partition decisions are such that an encoder encoded the partition decisions in the compressed bitstream using a convolutional neural network (CNN) that inferred the partition decisions.
- CNN convolutional neural network
- the encoder performs the steps of obtaining, using the CNN, the partition decisions for sub-blocks of size ( ⁇ S) ⁇ ( ⁇ S) of the block, by steps including obtaining feature maps from feature extraction layers of the CNN; inputting the feature maps to a concatenation layer of the CNN, where each feature map of the feature maps is of a smallest possible partition size S ⁇ S of the block; and obtaining the partition decisions from classifiers of the CNN, where each classifier of the classifiers infers partition decisions for sub-blocks of size ( ⁇ S) ⁇ ( ⁇ S) of the block for a respective value of ⁇ , where ⁇ is a power of 2.
- FIG. 1 is a schematic of a video encoding and decoding system.
- FIG. 2 is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station.
- FIG. 3 is a diagram of a video stream to be encoded and subsequently decoded.
- FIG. 4 is a block diagram of an encoder according to implementations of this disclosure.
- FIG. 5 is a block diagram of a decoder according to implementations of this disclosure.
- FIG. 6 is a block diagram of a representation of a portion of a frame according to implementations of this disclosure.
- FIG. 7 is a block diagram of an example of a quad-tree representation of a block according to implementations of this disclosure.
- FIG. 8 is a flowchart of a process for searching for a best mode to code a block.
- FIG. 9 is a block diagram of an example of estimating the rate and distortion costs of coding an image block by using a prediction mode.
- FIG. 10 is a block diagram of an example of a convolutional neural network (CNN) for mode decision using a non-linear function of a quantization parameter according to implementations of this disclosure.
- CNN convolutional neural network
- FIG. 11 is a flowchart of a process for encoding, by an encoder, an image block using a first quantization parameter according to implementations of this disclosure.
- FIG. 12 is an example of approximating a non-linear function of a quantization parameter using linear segments according to implementations of this disclosure.
- FIG. 13 is an example of a rate-distortion performance comparison of a first machine-learning model that uses as input a non-linear QP function and a second machine-learning model that uses a linear QP function.
- FIG. 14 is an example of a convolution filter according to implementations of this disclosure.
- FIG. 15 is an example of receptive fields according to implementations of this disclosure.
- FIG. 16 is an example of non-square partitions of a block.
- Modern video codecs (e.g., H.264, which is also known as MPEG-4 AVC; VP9; H.265, which is also known as HEVC; AVS2; and AV1) define and use a large number of tools and configurations that are used to improve coding efficiency. Coding efficiency is typically measured in terms of both rate and distortion. Rate refers to the number of bits required for encoding (such as encoding a block, a frame, etc.). Distortion measures the quality loss between, for example, a source video block and a reconstructed version of source video block. By performing a rate-distortion optimization (RDO) process, a video codec optimizes the amount of distortion against the rate required to encode the video.
- RDO rate-distortion optimization
- a video encoder can use a mode decision process.
- the mode decision process can examine (e.g., test, evaluate, etc.) at least some of the valid combinations of tools. In an example, all possible combinations are examined.
- a procedure e.g., a technique, etc.
- the metric can be computed for each of the examined combinations and the respective metrics compared.
- the metric can combine the rate and distortion to produce one single scalar value, as described below.
- the rate-distortion cost is used as such as scalar value.
- An example of a mode decision process is an intra-prediction mode decision process, which determines the best intra-prediction mode for coding a coding block.
- 35 intra-prediction modes are possible for blocks that are larger than 4 ⁇ 4.
- Each of the intra-prediction modes dictates how a respective prediction block is determined.
- the mode decision process in this context, may determine a respective prediction block for each of the intra-prediction modes and select the intra-prediction mode corresponding to the smallest rate-distortion cost. Said another way, the mode decision process selects the intra-prediction mode that provides the best rate-distortion performance.
- a mode decision process is a partition decision process, which determines an optimal sub-partitioning of a superblock (also known as a coding tree unit or CTU).
- a partition decision process is described below with respect to FIG. 7 .
- Quantization parameters in video codecs can be used to control the tradeoff between rate and distortion.
- a larger quantization parameter means higher quantization (such as of transform coefficients) resulting in a lower rate but higher distortion; and a smaller quantization parameter means lower quantization resulting in a higher rate but a lower distortion.
- the variables QP, q, and Q may be used interchangeably in this disclosure to refer to a quantization parameter.
- the value of the quantization parameter can be fixed.
- an encoder can use one quantization parameter value to encode all frames and/or all blocks of a video.
- the quantization parameter can change, for example, from frame to frame.
- the encoder can change the quantization parameter value based on fluctuations in network bandwidth.
- the quantization parameter can be used to control the tradeoff between rate and distortion
- the quantization parameter can be used to calculate the metrics associated with each combination of parameters.
- the metric can combine the rate and the distortion values of a combination of encoding parameters.
- the metric can be the rate-distortion (RD) cost.
- the combination resulting in the lowest cost can be used for encoding, for example, a block or a frame in a compressed bitstream.
- the RD costs are computed using a quantization parameter. More generally, whenever an encoder decision (e.g., a mode decision) is based on the RD cost, the QP value may be used by the encoder to determine the RD cost.
- an encoder decision e.g., a mode decision
- the rate and distortion cost of coding an image block X by using a prediction mode m i is described with respect to FIGS. 8-9 .
- the QP can be used to derive a multiplier that is used to combine the rate and distortion values into one metric.
- Some codecs may refer to the multiplier as the Lagrange multiplier (denoted ⁇ mode ); other codecs may use a similar multiplier that is referred as rdmult. Each codec may have a different method of calculating the multiplier. Unless the context makes clear, the multiplier is referred to herein, regardless of the codec, as the Lagrange multiplier or Lagrange parameter.
- the Lagrange multiplier can be used to evaluate the RD costs of competing modes (i.e., competing combinations of parameters). Specifically, let r m denote the rate (in bits) resulting from using a mode m and let d m denote the resulting distortion. The rate distortion cost of selecting the mode m can be computed as a scalar value: d m + ⁇ mode r m . By using the Lagrange parameter ⁇ mode , it is then possible to compare the cost of two modes and select one with the lower combined RD cost. This technique of evaluating rate distortion cost is a basis of mode decision processes in at least some video codecs.
- Different video codecs may use different techniques to compute the Lagrange multipliers from the quantization parameters. This is due in part to the fact that the different codecs may have different meanings (e.g., definitions, semantics, etc.) for, and method of use of, quantization parameters.
- Codecs (referred to herein as HEVC codecs) that implement the HEVC standard may use a formula that is similar to the formula (1).
- the multiplier has a non-linear relationship to the quantization parameter.
- the multiplier In the cases of HEVC and H.264, the multiplier has an exponential relationship to the QP; and in the cases of H.263, VP9, and AV1, the multiplier has a quadratic relationship to the QP.
- the multipliers may undergo further changes before being used in the respective codecs to account for additional side information included in a compressed bitstream by the encoder. Examples of side information include picture type (e.g., intra vs. inter predicted frame), color components (e.g., luminance or chrominance), and/or region of interest. In an example, such additional changes can be linear changes to the multipliers.
- the best mode can be selected from many possible combinations. As the number of possible tools and parameters increases, the number of combinations also increases, which, in turn, increases the time required to determine the best mode.
- the AV1 codec includes roughly 160 additional tools over the AV1 codec, thereby resulting in a significant increase in search time for the best mode.
- Machine learning can be well suited to address the computational complexity problem in video coding.
- a vast amount of training data can be generated, for example, by using the brute-force approaches to mode decision. That is, the training data can be obtained by an encoder performing standard encoding techniques, such as those described with respect to FIGS. 4 and 6-9 .
- the brute-force, on-the-fly mode decision process may be replaced with the trained machine-learning model, which can infer a mode decision for use for a large class of video data input.
- a well-trained machine-learning model can be expected to closely match the brute-force approach in coding efficiency but at a significantly lower computational cost or with a regular or dataflow-oriented computational cost.
- the training data can be used, during the learning phase of machine learning, to derive (e.g., learn, infer, etc.) a machine-learning model that is (e.g., defines, constitutes) a mapping from the input data to an output that constitutes a mode decision.
- a machine-learning model that is (e.g., defines, constitutes) a mapping from the input data to an output that constitutes a mode decision.
- the machine-learning model can be used to replace the brute-force, computation heavy encoding processes (such as those described with respect to FIGS. 4 and 6-9 ), thereby reducing the computation complexity in mode decision.
- the predictive capabilities (i.e., accuracy) of a machine-learning model are as good as the inputs used to train the machine-learning model and the inputs presented to the machine-learning model to predict a result (e.g., the best mode).
- a result e.g., the best mode.
- the machine-learning model can be a neural-network model, such as a convolutional neural-network model.
- presenting the correct inputs and optimal forms of such inputs, as described in this disclosure, is applicable to any machine-learning technique.
- a feed-forward neural network can be used to approximate any continuous function on a compact subset of the n-dimensional real coordinate space R n .
- the intrinsic linear nature of existing neural networks implies that a smaller network or shorter learning time may be achieved if a neural network is tasked (i.e., trained) to approximate (e.g., map, solve, infer) a linear function (e.g., mapping) than a non-linear function.
- a linear function e.g., mapping
- the mapping of video blocks to mode decisions can be characterized as a continuous function.
- the universal approximation theorem does not characterize feasibility or time and space complexity of the learning phase. That is, while a neural network may be theoretically capable of approximating the non-linear function, an unreasonably large (e.g., in terms of the number of nodes and/or layers) network and/or an unreasonably long training time may be required for the neural network to learn to approximate, using linear functions, the non-linear function. For practical purposes, the unreasonable size and time required may render the learning infeasible.
- the quantization parameter i.e., the value of the QP
- a disconnect may result between how the QP is used in evaluating the RD cost and how the QP is used in training machine-learning models.
- mappings from quantization parameters to Lagrange multipliers in many modern video codecs are nonlinear. Namely, the mapping is quadratic in H.263, VP9, and AV1; and exponential in H.264 and HEVC.
- better performance can be achieved by using non-linear (e.g., exponential, quadratic, etc.) forms of the QPs as input to machine-learning models as compared to using linear (e.g., scalar) forms of the QPs.
- Better performance can mean smaller network size and/or better inference performance.
- implementations according to this disclosure can significantly reduce the computational complexity of the mode decision processes of video encoders while maintaining the coding efficiencies of brute-force techniques. Additionally, implementations according to this disclosure can improve the inference performance of machine-learning models as compared to machine-learning models that use QP (i.e., a linear value of QP) as input to the training and inferencing phases of machine learning.
- QP i.e., a linear value of QP
- the architecture of the machine-learning model can also be critical to the performance and/or predictable capability of the machine-learning model.
- a typical machine-learning model such as a classification deep-learning model, includes two main portions: a feature-extraction portion and a classification portion.
- the feature-extraction portion detects features of the model.
- the classification portion attempts to classify the detected features into a desired response.
- Each of the portions can include one or more layers and/or one or more operations.
- a CNN is an example of a machine-learning model.
- the feature extraction portion typically includes a set of convolutional operations, which is typically a series of filters that are used to filter an input image based on a filter (typically a square of size k, without loss of generality).
- these filters can be used to find features in an input image.
- the features can include, for example, edges, corners, endpoints, and so on. As the number of stacked convolutional operations increases, later convolutional operations can find higher-level features.
- the classification portion is typically a set of fully connected layers.
- the fully connected layers can be thought of as looking at all the input features of an image in order to generate a high-level classifier.
- stages e.g., a series
- high-level classifiers eventually generate the desired classification output.
- a typical CNN network is composed of a number of convolutional operations (e.g., the feature-extraction portion) followed by a number of fully connected layers.
- the number of operations of each type and their respective sizes is typically determined during the training phase of the machine learning.
- additional layers and/or operations can be included in each portion. For example, combinations of Pooling, MaxPooling, Dropout, Activation, Normalization, BatchNormalization, and other operations can be grouped with convolution operations (i.e., in the features-extraction portion) and/or the fully connected operation (i.e., in the classification portion).
- the fully connected layers may be referred to as Dense operations.
- a convolution operation can use a SeparableConvolution2D or Convolution2D operation.
- a convolution layer can be a group of operations starting with a Convolution2D or SeparableConvolution2D operation followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof), until another convolutional layer, a Dense operation, or the output of the CNN is reached.
- operations e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof
- a Dense layer can be a group of operations or layers starting with a Dense operation (i.e., a fully connected layer) followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof) until another convolution layer, another Dense layer, or the output of the network is reached.
- a Dense operation i.e., a fully connected layer
- zero or more operations e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof
- the boundary between feature extraction based on convolutional networks and a feature classification using Dense operations can be marked by a Flatten operation, which flattens the multidimensional matrix from the feature extraction into a vector.
- each of the convolution layers may consist of a set of filters. While a filter is applied to a subset of the input data at a time, the filter is applied across the full input, such as by sweeping over the input.
- the operations performed by this layer are typically linear/matrix multiplications.
- An example of a convolution filter is described with respect to FIG. 14 .
- the output of the convolution filter may be further filtered using an activation function.
- the activation function may be a linear function or non-linear function (e.g., a sigmoid function, an arcTan function, a tanH function, a ReLu function, or the like).
- Each of the fully connected operations is a linear operation in which every input is connected to every output by a weight.
- a fully connected layer with N number of inputs and M outputs can have a total of NxM weights.
- a Dense operation may be generally followed by a non-linear activation function to generate an output of that layer.
- Some CNN network architectures used to perform analysis of frames and superblocks may include several feature extraction portions that extract features at different granularities (e.g., at different sub-block sizes of a superblock) and a flattening layer (which may be referred to as a concatenation layer) that receives the output(s) of the last convolution layer of each of the extraction portions.
- the flattening layer aggregates all the features extracted by the different feature extractions portions into one input set.
- the output of the flattening layer may be fed into (i.e., used as input to) the fully connected layers of the classification portion.
- the number of parameters of the entire network may be dominated (e.g., defined, set) by the number of parameters at the interface between the feature extraction portion (i.e., the convolution layers) and the classification portion (i.e., the fully connected layers). That is, the number of parameters of the network is dominated by the parameters of the flattening layer.
- CNN architectures that include a flattening layer whose output is fed into fully connected layers can have several disadvantages.
- the machine-learning model of such architectures tend to have a large number of parameters and operations.
- the machine-learning model may include over 1 million parameters.
- Such large models may not be effectively or efficiently used, if at all, to infer classifications on devices (e.g., mobile devices) that may be constrained (e.g., computationally constrained, energy constrained, and/or memory constrained). That is, some devices may not have sufficient computational capabilities (for example, in terms of speed) or memory storage (e.g., RAM) to handle (e.g., execute) such large models.
- devices e.g., mobile devices
- constrained e.g., computationally constrained, energy constrained, and/or memory constrained
- memory storage e.g., RAM
- the fully connected layers of such network architectures are said to have a global view of all the features that are extracted by the feature extraction portions.
- the fully connected layers may, for example, lose a correlation between a feature and the location of the feature in the input image.
- the receptive fields of the convolution operations can become mixed by the fully connected layers.
- a receptive field can be defined as the region in the input space that a particular feature is looking at and/or is affected by. An example of a receptive field is described with respect to FIG. 15 .
- a CNN as described above may be used to determine a partition of a block 702 of FIG. 7 .
- the CNN may extract features corresponding to different regions and/or sub-block sizes of the block 702 .
- features extracted from blocks 702 - 1 , 702 - 2 , 702 - 3 , and 702 - 4 of the block 702 are flattened into one input vector to the fully connected layers.
- features of at least one of the blocks 702 - 1 , 702 - 3 , 702 - 4 may be used by the fully connected layers.
- features of sub-blocks e.g., the blocks 702 - 1 , 702 - 3 , 702 - 4
- the sub-block e.g., the block 702 - 2
- receptive-field-conforming convolutional models for video coding As such, also described herein is receptive-field-conforming convolutional models for video coding. That is, when analyzing an image region, such as for determining a quadtree partitioning, the receptive fields of any features extracted (e.g., calculated, inferred, etc.) for the image region are confined to the image region itself. Implementations according to this disclosure can ensure that machine-learning models (generated during training and used during inference) for determining block partitioning are not erroneously based on irrelevant or extraneous features, such as pixels from outside the image region.
- Implementations according to this disclosure result in CNN machine-learning models with reduced numbers of parameters and/or that respect the receptive field of an image block (e.g., a superblock) when analyzing the image block for extracting quadtree-based features of the image block. As such, the inference accuracy for mode decision in video encoding can be significantly improved.
- an image block e.g., a superblock
- FIG. 1 is a schematic of a video encoding and decoding system 100 .
- a transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2 .
- the processing of the transmitting station 102 can be distributed among multiple devices.
- a network 104 can connect the transmitting station 102 and a receiving station 106 for encoding and decoding of the video stream.
- the video stream can be encoded in the transmitting station 102
- the encoded video stream can be decoded in the receiving station 106 .
- the network 104 can be, for example, the Internet.
- the network 104 can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmitting station 102 to, in this example, the receiving station 106 .
- LAN local area network
- WAN wide area network
- VPN virtual private network
- the receiving station 106 can be a computer having an internal configuration of hardware, such as that described with respect to FIG. 2 .
- the processing of the receiving station 106 can be distributed among multiple devices.
- an implementation can omit the network 104 .
- a video stream can be encoded and then stored for transmission at a later time to the receiving station 106 or any other device having memory.
- the receiving station 106 receives (e.g., via the network 104 , a computer bus, and/or some communication pathway) the encoded video stream and stores the video stream for later decoding.
- a real-time transport protocol RTP
- a transport protocol other than RTP e.g., an HTTP-based video streaming protocol
- the transmitting station 102 and/or the receiving station 106 may include the ability to both encode and decode a video stream as described below.
- the receiving station 106 could be a video conference participant who receives an encoded video bitstream from a video conference server (e.g., the transmitting station 102 ) to decode and view and further encodes and transmits its own video bitstream to the video conference server for decoding and viewing by other participants.
- FIG. 2 is a block diagram of an example of a computing device 200 that can implement a transmitting station or a receiving station.
- the computing device 200 can implement one or both of the transmitting station 102 and the receiving station 106 of FIG. 1 .
- the computing device 200 can be in the form of a computing system including multiple computing devices, or in the form of a single computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like.
- a CPU 202 in the computing device 200 can be a central processing unit.
- the CPU 202 can be any other type of device, or multiple devices, now-existing or hereafter developed, capable of manipulating or processing information.
- the disclosed implementations can be practiced with a single processor as shown (e.g., the CPU 202 ), advantages in speed and efficiency can be achieved by using more than one processor.
- a memory 204 in the computing device 200 can be a read-only memory (ROM) device or a random-access memory (RAM) device. Any other suitable type of storage device can be used as the memory 204 .
- the memory 204 can include code and data 206 that is accessed by the CPU 202 using a bus 212 .
- the memory 204 can further include an operating system 208 and application programs 210 , the application programs 210 including at least one program that permits the CPU 202 to perform the methods described herein.
- the application programs 210 can include applications 1 through N, which further include a video coding application that performs the methods described herein.
- the computing device 200 can also include a secondary storage 214 , which can, for example, be a memory card used with a computing device 200 that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- a secondary storage 214 can, for example, be a memory card used with a computing device 200 that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- the computing device 200 can also include one or more output devices, such as a display 218 .
- the display 218 may be, in one example, a touch-sensitive display that combines a display with a touch-sensitive element that is operable to sense touch inputs.
- the display 218 can be coupled to the CPU 202 via the bus 212 .
- Other output devices that permit a user to program or otherwise use the computing device 200 can be provided in addition to or as an alternative to the display 218 .
- the output device is or includes a display
- the display can be implemented in various ways, including as a liquid crystal display (LCD); a cathode-ray tube (CRT) display; or a light-emitting diode (LED) display, such as an organic LED (OLED) display.
- LCD liquid crystal display
- CRT cathode-ray tube
- LED light-emitting diode
- OLED organic LED
- the computing device 200 can also include or be in communication with an image-sensing device 220 , for example, a camera, or any other image-sensing device, now existing or hereafter developed, that can sense an image, such as the image of a user operating the computing device 200 .
- the image-sensing device 220 can be positioned such that it is directed toward the user operating the computing device 200 .
- the position and optical axis of the image-sensing device 220 can be configured such that the field of vision includes an area that is directly adjacent to the display 218 and from which the display 218 is visible.
- the computing device 200 can also include or be in communication with a sound-sensing device 222 , for example, a microphone, or any other sound-sensing device, now existing or hereafter developed, that can sense sounds near the computing device 200 .
- the sound-sensing device 222 can be positioned such that it is directed toward the user operating the computing device 200 and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates the computing device 200 .
- FIG. 2 depicts the CPU 202 and the memory 204 of the computing device 200 as being integrated into a single unit, other configurations can be utilized.
- the operations of the CPU 202 can be distributed across multiple machines (each machine having one or more processors) that can be coupled directly or across a local area or other network.
- the memory 204 can be distributed across multiple machines, such as a network-based memory or memory in multiple machines performing the operations of the computing device 200 .
- the bus 212 of the computing device 200 can be composed of multiple buses.
- the secondary storage 214 can be directly coupled to the other components of the computing device 200 or can be accessed via a network and can comprise a single integrated unit, such as a memory card, or multiple units, such as multiple memory cards.
- the computing device 200 can thus be implemented in a wide variety of configurations.
- FIG. 3 is a diagram of an example of a video stream 300 to be encoded and subsequently decoded.
- the video stream 300 includes a video sequence 302 .
- the video sequence 302 includes a number of adjacent frames 304 . While three frames are depicted as the adjacent frames 304 , the video sequence 302 can include any number of adjacent frames 304 .
- the adjacent frames 304 can then be further subdivided into individual frames, for example, a frame 306 .
- the frame 306 can be divided into a series of segments 308 or planes.
- the segments 308 can be subsets of frames that permit parallel processing, for example.
- the segments 308 can also be subsets of frames that can separate the video data into separate colors.
- the frame 306 of color video data can include a luminance plane and two chrominance planes.
- the segments 308 may be sampled at different resolutions.
- the frame 306 may be further subdivided into blocks 310 , which can contain data corresponding to, for example, 16 ⁇ 16 pixels in the frame 306 .
- the blocks 310 can also be arranged to include data from one or more segments 308 of pixel data.
- the blocks 310 can also be of any other suitable size, such as 4 ⁇ 4 pixels, 8 ⁇ 8 pixels, 16 ⁇ 8 pixels, 8 ⁇ 16 pixels, 16 ⁇ 16 pixels, or larger.
- FIG. 4 is a block diagram of an encoder 400 in accordance with implementations of this disclosure.
- the encoder 400 can be implemented, as described above, in the transmitting station 102 , such as by providing a computer software program stored in memory, for example, the memory 204 .
- the computer software program can include machine instructions that, when executed by a processor, such as the CPU 202 , cause the transmitting station 102 to encode video data in manners described herein.
- the encoder 400 can also be implemented as specialized hardware included in, for example, the transmitting station 102 .
- the encoder 400 has the following stages to perform the various functions in a forward path (shown by the solid connection lines) to produce an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter-prediction stage 402 , a transform stage 404 , a quantization stage 406 , and an entropy encoding stage 408 .
- the encoder 400 may also include a reconstruction path (shown by the dotted connection lines) to reconstruct a frame for encoding of future blocks.
- the encoder 400 has the following stages to perform the various functions in the reconstruction path: a dequantization stage 410 , an inverse transform stage 412 , a reconstruction stage 414 , and a loop filtering stage 416 .
- Other structural variations of the encoder 400 can be used to encode the video stream 300 .
- a block can be encoded using intra-frame prediction (also called intra-prediction) or inter-frame prediction (also called inter-prediction), or a combination of both.
- intra-frame prediction also called intra-prediction
- inter-frame prediction also called inter-prediction
- a prediction block can be formed.
- intra-prediction all or part of a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed.
- inter-prediction all or part of a prediction block may be formed from samples in one or more previously constructed reference frames determined using motion vectors.
- the prediction block can be subtracted from the current block at the intra/inter-prediction stage 402 to produce a residual block (also called a residual).
- the transform stage 404 transforms the residual into transform coefficients in, for example, the frequency domain using block-based transforms.
- block-based transforms i.e., transform types
- DCT Discrete Cosine Transform
- ADST Asymmetric Discrete Sine Transform
- Other block-based transforms are possible.
- combinations of different transforms may be applied to a single residual.
- the DCT transforms the residual block into the frequency domain where the transform coefficient values are based on spatial frequency.
- the lowest frequency (DC) coefficient is at the top-left of the matrix, and the highest frequency coefficient is at the bottom-right of the matrix. It is worth noting that the size of a prediction block, and hence the resulting residual block, may be different from the size of the transform block. For example, the prediction block may be split into smaller blocks to which separate transforms are applied.
- the quantization stage 406 converts the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients, using a quantizer value or a quantization level. For example, the transform coefficients may be divided by the quantizer value and truncated.
- the quantized transform coefficients are then entropy encoded by the entropy encoding stage 408 . Entropy coding may be performed using any number of techniques, including token and binary trees.
- the entropy-encoded coefficients, together with other information used to decode the block (which may include, for example, the type of prediction used, transform type, motion vectors, and quantizer value), are then output to the compressed bitstream 420 .
- the information to decode the block may be entropy coded into block, frame, slice, and/or section headers within the compressed bitstream 420 .
- the compressed bitstream 420 can also be referred to as an encoded video stream or encoded video bitstream; these terms will be used interchangeably herein.
- the reconstruction path in FIG. 4 can be used to ensure that both the encoder 400 and a decoder 500 (described below) use the same reference frames and blocks to decode the compressed bitstream 420 .
- the reconstruction path performs functions that are similar to functions that take place during the decoding process and that are discussed in more detail below, including dequantizing the quantized transform coefficients at the dequantization stage 410 and inverse transforming the dequantized transform coefficients at the inverse transform stage 412 to produce a derivative residual block (also called a derivative residual).
- the prediction block that was predicted at the intra/inter-prediction stage 402 can be added to the derivative residual to create a reconstructed block.
- the loop filtering stage 416 can be applied to the reconstructed block to reduce distortion, such as blocking artifacts.
- encoder 400 can be used to encode the compressed bitstream 420 .
- a non-transform based encoder 400 can quantize the residual signal directly without the transform stage 404 for certain blocks or frames.
- an encoder 400 can have the quantization stage 406 and the dequantization stage 410 combined into a single stage.
- FIG. 5 is a block diagram of a decoder 500 in accordance with implementations of this disclosure.
- the decoder 500 can be implemented in the receiving station 106 , for example, by providing a computer software program stored in the memory 204 .
- the computer software program can include machine instructions that, when executed by a processor, such as the CPU 202 , cause the receiving station 106 to decode video data in the manners described below.
- the decoder 500 can also be implemented in hardware included in, for example, the transmitting station 102 or the receiving station 106 .
- the decoder 500 similar to the reconstruction path of the encoder 400 discussed above, includes in one example the following stages to perform various functions to produce an output video stream 516 from the compressed bitstream 420 : an entropy decoding stage 502 , a dequantization stage 504 , an inverse transform stage 506 , an intra/inter-prediction stage 508 , a reconstruction stage 510 , a loop filtering stage 512 , and a post filtering stage 514 .
- Other structural variations of the decoder 500 can be used to decode the compressed bitstream 420 .
- the data elements within the compressed bitstream 420 can be decoded by the entropy decoding stage 502 to produce a set of quantized transform coefficients.
- the dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage 506 inverse transforms the dequantized transform coefficients using the selected transform type to produce a derivative residual that can be identical to that created by the inverse transform stage 412 in the encoder 400 .
- the decoder 500 can use the intra/inter-prediction stage 508 to create the same prediction block as was created in the encoder 400 , for example, at the intra/inter-prediction stage 402 .
- the prediction block can be added to the derivative residual to create a reconstructed block.
- the loop filtering stage 512 can be applied to the reconstructed block to reduce blocking artifacts. Other filtering can be applied to the reconstructed block.
- the post filtering stage 514 is applied to the reconstructed block to reduce blocking distortion, and the result is output as an output video stream 516 .
- the output video stream 516 can also be referred to as a decoded video stream; these terms will be used interchangeably herein.
- the decoder 500 can be used to decode the compressed bitstream 420 .
- the decoder 500 can produce the output video stream 516 without the post filtering stage 514 .
- the post filtering stage 514 is applied after the loop filtering stage 512 .
- the loop filtering stage 512 can include an optional deblocking filtering stage.
- the encoder 400 includes an optional deblocking filtering stage in the loop filtering stage 416 .
- a codec can use multiple transform types.
- a transform type can be the transform type used by the transform stage 404 of FIG. 4 to generate the transform block.
- the transform type i.e., an inverse transform type
- Available transform types can include a one-dimensional Discrete Cosine Transform (1D DCT) or its approximation, a one-dimensional Discrete Sine Transform (1D DST) or its approximation, a two-dimensional DCT (2D DCT) or its approximation, a two-dimensional DST (2D DST) or its approximation, and an identity transform.
- Other transform types can be available.
- a one-dimensional transform (1D DCT or 1D DST) can be applied in one dimension (e.g., row or column), and the identity transform can be applied in the other dimension.
- the quantized coefficients can be coded by using a row-by-row (i.e., raster) scanning order or a column-by-column scanning order.
- a different scanning order may be used to code the quantized coefficients.
- different templates can be used to derive contexts for coding the non-zero flags of the non-zero map based on the types of transforms used.
- the template can be selected based on the transform type used to generate the transform block.
- a transform type include: 1D DCT applied to rows (or columns) and an identity transform applied to columns (or rows); 1D DST applied to rows (or columns) and an identity transform applied to columns (or rows); 1D DCT applied to rows (or columns) and 1D DST applied to columns (or rows); a 2D DCT; and a 2D DST.
- Other combinations of transforms can comprise a transform type.
- FIG. 6 is a block diagram of a representation of a portion 600 of a frame, such as the frame 306 of FIG. 3 , according to implementations of this disclosure.
- the portion 600 of the frame includes four 64 ⁇ 64 blocks 610 , which may be referred to as superblocks, in two rows and two columns in a matrix or Cartesian plane.
- a superblock can have a larger or a smaller size. While FIG. 6 is explained with respect to a superblock of size 64 ⁇ 64, the description is easily extendable to larger (e.g., 128 ⁇ 128) or smaller superblock sizes.
- a superblock can be a basic or maximum coding unit (CU).
- Each superblock can include four 32 ⁇ 32 blocks 620 .
- Each 32 ⁇ 32 block 620 can include four 16 ⁇ 16 blocks 630 .
- Each 16 ⁇ 16 block 630 can include four 8 ⁇ 8 blocks 640 .
- Each 8 ⁇ 8 block 640 can include four 4 ⁇ 4 blocks 650 .
- Each 4 ⁇ 4 block 650 can include 16 pixels, which can be represented in four rows and four columns in each respective block in the Cartesian plane or matrix. The pixels can include information representing an image captured in the frame, such as luminance information, color information, and location information.
- a block such as a 16 ⁇ 16-pixel block as shown, can include a luminance block 660 , which can include luminance pixels 662 ; and two chrominance blocks 670 / 680 , such as a U or Cb chrominance block 670 , and a V or Cr chrominance block 680 .
- the chrominance blocks 670 / 680 can include chrominance pixels 690 .
- the luminance block 660 can include 16 ⁇ 16 luminance pixels 662
- each chrominance block 670 / 680 can include 8 ⁇ 8 chrominance pixels 690 , as shown. Although one arrangement of blocks is shown, any arrangement can be used.
- N ⁇ N blocks in some implementations, N ⁇ M, where M ⁇ M, blocks can be used.
- M ⁇ M 32 ⁇ 64 blocks, 64 ⁇ 32 blocks, 16 ⁇ 32 blocks, 32 ⁇ 16 blocks, or any other size blocks can be used.
- N ⁇ 2N blocks, 2N ⁇ N blocks, or a combination thereof can be used.
- video coding can include ordered block-level coding.
- Ordered block-level coding can include coding blocks of a frame in an order, such as raster-scan order, wherein blocks can be identified and processed starting with a block in the upper left corner of the frame, or a portion of the frame, and proceeding along rows from left to right and from the top row to the bottom row, identifying each block in turn for processing.
- the superblock in the top row and left column of a frame can be the first block coded
- the superblock immediately to the right of the first block can be the second block coded.
- the second row from the top can be the second row coded, such that the superblock in the left column of the second row can be coded after the superblock in the rightmost column of the first row.
- coding a block can include using quad-tree coding, which can include coding smaller block units with a block in raster-scan order.
- quad-tree coding can include coding smaller block units with a block in raster-scan order.
- the 64 ⁇ 64 superblock shown in the bottom-left corner of the portion of the frame shown in FIG. 6 can be coded using quad-tree coding in which the top-left 32 ⁇ 32 block can be coded, then the top-right 32 ⁇ 32 block can be coded, then the bottom-left 32 ⁇ 32 block can be coded, and then the bottom-right 32 ⁇ 32 block can be coded.
- Each 32 ⁇ 32 block can be coded using quad-tree coding in which the top-left 16 ⁇ 16 block can be coded, then the top-right 16 ⁇ 16 block can be coded, then the bottom-left 16 ⁇ 16 block can be coded, and then the bottom-right 16 ⁇ 16 block can be coded.
- Each 16 ⁇ 16 block can be coded using quad-tree coding in which the top-left 8 ⁇ 8 block can be coded, then the top-right 8 ⁇ 8 block can be coded, then the bottom-left 8 ⁇ 8 block can be coded, and then the bottom-right 8 ⁇ 8 block can be coded.
- Each 8 ⁇ 8 block can be coded using quad-tree coding in which the top-left 4 ⁇ 4 block can be coded, then the top-right 4 ⁇ 4 block can be coded, then the bottom-left 4 ⁇ 4 block can be coded, and then the bottom-right 4 ⁇ 4 block can be coded.
- 8 ⁇ 8 blocks can be omitted for a 16 ⁇ 16 block, and the 16 ⁇ 16 block can be coded using quad-tree coding in which the top-left 4 ⁇ 4 block can be coded, and then the other 4 ⁇ 4 blocks in the 16 ⁇ 16 block can be coded in raster-scan order.
- video coding can include compressing the information included in an original, or input, frame by omitting some of the information in the original frame from a corresponding encoded frame.
- coding can include reducing spectral redundancy, reducing spatial redundancy, reducing temporal redundancy, or a combination thereof.
- reducing spectral redundancy can include using a color model based on a luminance component (Y) and two chrominance components (U and V or Cb and Cr), which can be referred to as the YUV or YCbCr color model or color space.
- YUV color model can include using a relatively large amount of information to represent the luminance component of a portion of a frame and using a relatively small amount of information to represent each corresponding chrominance component for the portion of the frame.
- a portion of a frame can be represented by a high-resolution luminance component, which can include a 16 ⁇ 16 block of pixels, and by two lower resolution chrominance components, each of which representing the portion of the frame as an 8 ⁇ 8 block of pixels.
- a pixel can indicate a value (e.g., a value in the range from 0 to 255) and can be stored or transmitted using, for example, eight bits.
- Reducing spatial redundancy can include transforming a block into the frequency domain as described above.
- a unit of an encoder such as the entropy encoding stage 408 of FIG. 4 , can perform a DCT using transform coefficient values based on spatial frequency.
- Reducing temporal redundancy can include using similarities between frames to encode a frame using a relatively small amount of data based on one or more reference frames, which can be previously encoded, decoded, and reconstructed frames of the video stream.
- a block or a pixel of a current frame can be similar to a spatially corresponding block or pixel of a reference frame.
- a block or a pixel of a current frame can be similar to a block or a pixel of a reference frame at a different spatial location.
- reducing temporal redundancy can include generating motion information indicating the spatial difference (e.g., a translation between the location of the block or the pixel in the current frame and the corresponding location of the block or the pixel in the reference frame).
- Reducing temporal redundancy can include identifying a block or a pixel in a reference frame, or a portion of the reference frame, that corresponds with a current block or pixel of a current frame.
- a reference frame, or a portion of a reference frame, which can be stored in memory can be searched for the best block or pixel to use for encoding a current block or pixel of the current frame.
- the search may identify the block of the reference frame for which the difference in pixel values between the reference block and the current block is minimized, and can be referred to as motion searching.
- the portion of the reference frame searched can be limited.
- the portion of the reference frame searched which can be referred to as the search area, can include a limited number of rows of the reference frame.
- identifying the reference block can include calculating a cost function, such as a sum of absolute differences (SAD), between the pixels of the blocks in the search area and the pixels of the current block.
- SAD sum of absolute differences
- the spatial difference between the location of the reference block in the reference frame and the current block in the current frame can be represented as a motion vector.
- the difference in pixel values between the reference block and the current block can be referred to as differential data, residual data, or as a residual block.
- generating motion vectors can be referred to as motion estimation, and a pixel of a current block can be indicated based on location using Cartesian coordinates such as f x,y .
- a pixel of the search area of the reference frame can be indicated based on a location using Cartesian coordinates such as r x,y .
- a motion vector (MV) for the current block can be determined based on, for example, a SAD between the pixels of the current frame and the corresponding pixels of the reference frame.
- FIG. 7 is a block diagram of an example 700 of a quad-tree representation of a block according to implementations of this disclosure.
- the example 700 includes the block 702 .
- the block 702 can be referred to as a superblock or a CTB.
- the example 700 illustrates a partition of the block 702 .
- the block 702 can be partitioned differently, such as by an encoder (e.g., the encoder 400 of FIG. 4 ) or a machine-learning model (such as described with respect to FIGS. 10-11 ). Partitioning a block by an encoder, such as the encoder 400 of FIG. 4 , is referred to herein as brute-force approach to encoding.
- the example 700 illustrates that the block 702 is partitioned into four blocks, namely, the blocks 702 - 1 , 702 - 2 , 702 - 3 , and 702 - 4 .
- the block 702 - 2 is further partitioned into the blocks 702 - 5 , 702 - 6 , 702 - 7 , and 702 - 8 .
- the blocks 702 - 1 , 702 - 2 , 702 - 3 , and 702 - 4 are each of size N/2 ⁇ N/2 (e.g., 64 ⁇ 64), and the blocks 702 - 5 , 702 - 6 , 702 - 7 , and 702 - 8 are each of size N/4 ⁇ N/4 (e.g., 32 ⁇ 32). If a block is partitioned, it is partitioned into four equally sized, non-overlapping square sub-blocks.
- a quad-tree data representation is used to describe how the block 702 is partitioned into sub-blocks, such as blocks 702 - 1 , 702 - 2 , 702 - 3 , 702 - 4 , 702 - 5 , 702 - 6 , 702 - 7 , and 702 - 8 .
- a quad-tree 703 of the partition of the block 702 is shown. Each node of the quad-tree 703 is assigned a flag of “1” if the node is further split into four sub-nodes and assigned a flag of “0” if the node is not split.
- the flag can be referred to as a split bit (e.g., 1) or a stop bit (e.g., 0) and is coded in a compressed bitstream.
- a split bit e.g., 1
- a stop bit e.g., 0
- a node either has four child nodes or has no child nodes.
- a node that has no child nodes corresponds to a block that is not split further.
- Each of the child nodes of a split block corresponds to a sub-block.
- each node corresponds to a sub-block of the block 702 .
- the corresponding sub-block is shown between parentheses.
- a node 704 - 1 which has a value of 0, corresponds to the block 702 - 1 .
- a root node 704 - 0 corresponds to the block 702 .
- the value of the root node 704 - 0 is the split bit (e.g., 1).
- the flags indicate whether a sub-block of the block 702 is further split into four sub-sub-blocks.
- a node 704 - 2 includes a flag of “1” because the block 702 - 2 is split into the blocks 702 - 5 , 702 - 6 , 702 - 7 , and 702 - 8 .
- Each of nodes 704 - 1 , 704 - 3 , and 704 - 4 includes a flag of “0” because the corresponding blocks are not split.
- nodes 704 - 5 , 704 - 6 , 704 - 7 , and 704 - 8 are at a bottom level of the quad-tree, no flag of “0” or “1” is necessary for these nodes. That the blocks 702 - 5 , 702 - 6 , 702 - 7 , and 702 - 8 are not split further can be inferred from the absence of additional flags corresponding to these blocks.
- the quad-tree data representation for the quad-tree 703 can be represented by the binary data of “10100,” where each bit represents a node 704 of the quad-tree 703 .
- the binary data indicates the partitioning of the block 702 to the encoder and decoder.
- the encoder can encode the binary data in a compressed bitstream, such as the compressed bitstream 420 of FIG. 4 , in a case where the encoder needs to communicate the binary data to a decoder, such as the decoder 500 of FIG. 5 .
- the blocks corresponding to the leaf nodes of the quad-tree 703 can be used as the bases for prediction. That is, prediction can be performed for each of the blocks 702 - 1 , 702 - 5 , 702 - 6 , 702 - 7 , 702 - 8 , 702 - 3 , and 702 - 4 , referred to herein as coding blocks.
- the coding block can be a luminance block or a chrominance block. It is noted that, in an example, the superblock partitioning can be determined with respect to luminance blocks. The same partition can be used with the chrominance blocks.
- a prediction type (e.g., intra- or inter-prediction) is determined at the coding block (e.g., a block 702 - 1 , 702 - 5 , 702 - 6 , 702 - 7 , 702 - 8 , 702 - 3 , or 702 - 4 ) level. That is, a coding block is the decision point for prediction.
- a mode decision process determines the quad-tree partition of a coding block, such as the block 702 .
- the partition decision process calculates the RD costs of different combinations of coding parameters. That is, for example, different combinations of prediction blocks and predictions (e.g., intra-prediction, inter-prediction, etc.) are examined to determine an optimal partitioning.
- FIG. 8 is a flowchart of a process 800 process for searching for a best mode to code a block.
- the process 800 is an illustrative, high level process of a mode decision process that determines a best mode. For ease of description, the process 800 is described with respect to selecting an intra-prediction mode for encoding a prediction block. Other examples of best modes that can be determined by processes similar to the process 800 include determining a transform type and determining a transform size.
- the process 800 can be implemented by an encoder, such as the encoder 400 of FIG. 4 , using a brute-force approach to mode decision.
- the process 800 receives an image block.
- the image block can be a prediction unit.
- each of the leaf node coding blocks e.g., a block 702 - 1 , 702 - 5 , 702 - 6 , 702 - 7 , 702 - 8 , 702 - 3 , or 702 - 4
- the image block can be one such prediction unit.
- the process 800 determines (e.g., selects, calculates, choses, etc.) a list of modes.
- the list of modes can include K modes, where K is an integer number.
- the list of modes can be denoted ⁇ m 1 , m 2 , . . . , m k ⁇ .
- the encoder can have available a list of intra-prediction modes.
- the list of available intra-prediction modes can be ⁇ DC_PRED, V_PRED, H_PRED, D45_PRED, D135_PRED, D117_PRED, D153_PRED, D207_PRED, D63_PRED, SMOOTH_PRED, SMOOTH_V_PRED, and SMOOTH_H_PRED, PAETH_PRED ⁇ .
- a description of these intra-prediction modes is omitted as the description in impertinent to the understanding of this disclosure.
- the list of modes determined at 804 can be any subset of the list of available intra-prediction modes.
- the process 800 initializes a BEST_COST variable to a high value (e.g., INT_MAX, which may be equal to 2,147,483,647) and initializes a loop variable i to 1, which corresponds to the first mode to be examined.
- a high value e.g., INT_MAX, which may be equal to 2,147,483,647
- the process 800 computes (e.g., calculates) an RD_COST i for the mode i .
- the process 800 proceeds back to 808 ; otherwise the process 800 proceeds to 816 .
- the process 800 outputs the index of the best mode, BEST_MODE. Outputting the best mode can mean returning the best mode to a caller of the process 800 . Outputting the best mode can mean encoding the image using the best mode. Outputting the best mode can have other semantics.
- the process 800 terminates at 820 .
- FIG. 9 is a block diagram of an example 900 of estimating the rate and distortion costs of coding an image block X by using a prediction mode m i .
- the process 900 can be performed by an encoder, such as the encoder 400 of FIG. 4 .
- the process 900 includes performing a hypothetical encoding of the image block X using the prediction mode m i to determine the RD cost of encoding the block.
- the process 900 can be used by the process 800 at 808 .
- a hypothetical encoding process is a process that carries out the coding steps but does not output bits into a compressed bitstream, such as the compressed bitstream 420 of FIG. 4 . Since the purpose is to estimate a rate (also referred as bit rate), a hypothetical encoding process may be regarded or called a rate estimation process.
- the hypothetical encoding process computes the number of bits (RATE) required to encode the image block X.
- the example 900 also calculates a distortion (DISTORTION) based on a difference between the image block X and a reconstructed version of the image block X.
- a prediction using the mode m i is determined.
- the prediction can be determined as described with respect to intra/inter-prediction stage 402 of FIG. 4 .
- a residual is determined as a difference between the image block 902 and the prediction.
- the residual is transformed and quantized, such as described, respectively, with respect to the transform stage 404 and the quantization stage 406 of FIG. 4 .
- the rate (RATE) is calculated by a rate estimator 912 , which performs the hypothetical encoding.
- the rate estimator 912 can perform entropy encoding, such as described with respect to the entropy encoding stage 408 of FIG. 4 .
- the quantized residual is dequantized at 914 (such as described, for example, with respect to the dequantization stage 410 of FIG. 4 ), inverse transformed at 916 (such as described, for example, with respect to the inverse transform stage 412 of FIG. 4 ), and reconstructed at 918 (such as described, for example, with respect to the reconstruction stage 414 of FIG. 4 ) to generate a reconstructed block.
- a distortion estimator 920 calculates the distortion (i.e., the loss in video quality) between the image block X and the reconstructed block.
- the distortion can be a mean square error between pixel values of the image block X and the reconstructed block.
- the distortion can be a sum of absolute differences error between pixel values of the image block X and the reconstructed block. Any other suitable distortion measure can be used.
- the rate, RATE, and distortion, DISTORTION are then combined into a scalar value (i.e., the RD cost) by using the Lagrange multiplier as shown in formula (5) DISTORTION+ ⁇ mode ⁇ RATE, (5)
- the Lagrange multiplier ⁇ mode of the formula 5 can be calculated as described above, depending on the encoder performing the operations of the example 900 .
- FIGS. 8 and 9 illustrate that the traditional (i.e., brute-force) approach to mode decision is largely a serial process that essentially codes an image block X by using candidate modes to determine the mode with the best cost.
- Techniques have been used to reduce the complexity in mode decision. For example, early termination techniques have been used to terminate the loop of the process 800 of FIG. 8 as soon as certain conditions are met, such as, for example, that the rate distortion cost is lower than a threshold.
- Other techniques include selecting, for example based on heuristics, a subset of the available candidate modes or using multi-passes over the candidate modes. However, such techniques may not sufficiently reduce the complexity in mode decision.
- Machine learning can be used to reduce the computational complexity in mode decision.
- an encoder may perform several mode decision processes. If a mode decision requires the use of the QP (for example to determine RD costs), then it is critical that the correct form (i.e., function) of the QP is used during the learning and the inferencing phases of the machine learning.
- FIG. 10 is an illustrative example of using a non-linear function of the QP with a machine-learning model.
- FIG. 10 is a block diagram of an example of a convolutional neural network (CNN) 1000 for mode decision using a non-linear function of a quantization parameter (QP) according to implementations of this disclosure.
- the CNN 1000 can be used for determining a block partition of an image block.
- the block can be a superblock.
- the CNN can be used to determine the block size used in the intra/inter-prediction stage 402 of FIG. 4 .
- the partition can be a quad-tree partition, such as described with respect to FIG. 7 .
- the CNN 1000 can be used to determine a partition for an intra-coded block.
- the block can be a block of intra-coded frame, such as the frame 304 of FIG. 3 .
- the CNN 1000 can be used by an encoder where the smallest possible block partition is an 8 ⁇ 8 partition. As such, determinations of whether to split a block need be made for blocks (i.e., sub-blocks of the superblock) that are 16 ⁇ 16 or larger.
- the architecture of the CNN 1000 is described with respect to FIG. 10 .
- the training and using (i.e., for inferencing) of the CNN 1000 are described with respect to FIG. 11 .
- the number of parallel branches of the feature extraction portion of the CNN 1000 can be parameterizable (e.g., configurable). For example, in a configuration, only 1 branch (e.g., a linear branch) can be used. This is possible as long as the receptive field conformance property, as further described below, is maintained. Except for the top and left rows of the block, the receptive field conformance property means that the receptive field boundary of the block does not cross the boundaries of the block.
- a block 1002 (i.e., an image block) to be encoded is presented to the CNN 1000 .
- the block 1002 can be a one color-plane block.
- the block 1002 can be a luminance block. That the block is a one color-plane block is illustrated by the “ ⁇ 1” in “64 ⁇ 64 ⁇ 1” in FIG. 10 .
- the block 1002 can be a superblock. While a superblock of size 64 ⁇ 64 is shown and used to describe the CNN 1000 , the block 1002 can be of any size.
- the block 1002 can be 128 ⁇ 128, 32 ⁇ 32, or any size block for which a quad tree partition is typically determined by the encoder.
- the encoder can be an H.264, a VP9, an HEVC, an H.263, an AV1, or any other encoder that uses QP in mode decision determination.
- the block 1002 i.e., the block that is used as input to the CNN 1000
- the block 1002 can include pixels that are outside of the block for which a partitioning is to be determined. For example, if a partitioning of a 64 ⁇ 64 is to be determined, then a block of size 65 ⁇ 65 ⁇ 1 can be used as input to the CNN 1000 .
- a first filter e.g., a first filter in each branch of the feature extraction layers
- the stride will be 2 k .
- a feature extraction layers 1003 includes three branches; namely a branch 1003 -A, a branch 1003 -B, and a branch 1003 -C.
- the number of branches in the feature extraction layer can be configurable to include more or fewer branches.
- Each of the branches can include one or more layers.
- respective feature maps are extracted.
- features maps such as a feature maps 1004 , having a dimension of A ⁇ B ⁇ C are referred to.
- the feature maps 1004 is of size 8 ⁇ 8 ⁇ 256. This is to be interpreted as follows: the feature maps 1004 includes 256 feature maps and each of the feature maps is of size 8 ⁇ 8 pixels (or features).
- the feature maps 1004 can be thought of as a set of 256 matrices where each matrix is of size 8 ⁇ 8.
- the feature extraction of each partition type can be separated, instead of sharing the feature extraction as in FIG. 10 .
- the number of features at a feature map can be configurable.
- a feature compression rate can be applied to a machine-learning model to expand or reduce the number of features in the model.
- the feature compression rate can be multiplied by all feature maps for feature expansion (or reduction).
- the branch 1003 -A extracts, in a first layer of the branch 1003 -A, features corresponding to 8 ⁇ 8 blocks of the block 1002 .
- the branch 1003 -A convolves, with the block 1002 , 256 filters (also referred to as kernels). Each of the filters is of size 8 ⁇ 8.
- a stride that is equal to the size of the filters i.e., a stride that is equal to 8) is used.
- 256 feature maps i.e., the feature maps 1004
- each of size 8 ⁇ 8 are extracted.
- a filter of size 8 ⁇ 8 is defined by a kernel of the same size where each entry in the kernel can be a real number. In an example, the entries can be non-negative integers that are greater than 1. Filtering an 8 ⁇ 8 block may thus be achieved by computing the inner product between the block and a filter kernel of the same size.
- filter kernels i.e., the real numbers which constitute the values of the kernels
- the branch 1003 -B extracts 256 feature maps (i.e., feature maps 1008 ), each of size 8 ⁇ 8.
- the branch 1003 -B first extracts, at a first layer of the branch 1003 -B, feature maps 1006 by convolving the block 1002 with 128 filters, each of size 4 ⁇ 4, and using a stride of 4 (i.e., a stride that is equal to the filter size).
- a stride of 4 i.e., a stride that is equal to the filter size.
- each of the 128 of the feature maps 1006 is convolved with two 2 ⁇ 2 filters, using a stride of 2, thereby resulting in the feature maps 1008 .
- the branch 1003 -C extracts 256 feature maps (i.e., feature maps 1014 ), each of size 8 ⁇ 8.
- the branch 1003 -C first extracts, at a first layer of the branch 1003 -C, feature maps 1010 by convolving the block 1002 with 64 filters, each of size 2 ⁇ 2, and using a stride of 2.
- each of the 64 of the feature maps 1010 is convolved with two 2 ⁇ 2 filters, using a stride of 2, thereby resulting in 128 feature maps (i.e., feature maps 1012 ).
- each of the 128 of the feature maps 1012 is convolved with two 2 ⁇ 2 filters, using a stride of 2, thereby resulting in the feature maps 1014 .
- the unit is downsized (i.e., down-sampled), in each dimension, by the size of the filter.
- the feature maps 1010 are feature maps of the 32 ⁇ 32 blocks of the block 1002 .
- the feature maps 1006 are feature maps of the 16 ⁇ 16 blocks of the block 1002 .
- the feature maps 1004 are feature maps of the 8 ⁇ 8 blocks of the block 1002 .
- the feature maps 1008 normalizes the feature maps 1006 to be, like the feature maps 1004 , of size 8 ⁇ 8.
- the feature maps 1012 followed by the feature maps 1014 normalize the feature maps 1010 to be, similarly, of size 8 ⁇ 8.
- the feature maps can be normalized, via successive convolutions, to be feature maps of the smallest possible partition that can be used by the encoder.
- the feature extraction layers 1003 can normalize the feature maps to be of size 4 ⁇ 4.
- the feature extraction layers 1003 can include an additional branch and each of the branches would generate, via successive convolutions, feature maps that are each of size 4 ⁇ 4.
- the feature maps can be normalized to a size that does not necessarily correspond to the smallest partition size.
- the features maps can be normalized to any size that is larger than or equal 8 ⁇ 8.
- a concatenation layer 1016 receives the feature maps 1004 , 1008 , and 1014 . Additionally, since the CNN 1000 is used to determine (e.g., infer, provide, etc.) a partition for the block 1002 that is to be intra-predicted, and as intra-prediction uses at least some samples (i.e., pixels) of neighboring blocks, at least some samples of the neighboring blocks can also be used as input to the concatenation layer 1016 . While samples from the top neighboring block (indicated with TOP in FIG. 10 ) and samples from the left neighboring block (indicated with LEFT in FIG. 10 ) are shown for illustrative purposes, other neighboring blocks may be used, depending on the scan order used to process blocks of a video frame.
- LEFT and TOP are used in the case of a raster scan order.
- all the samples of the top and left neighboring blocks are used as inputs to the concatenation layer 1016 .
- samples of the top and left neighboring blocks can be included in the input block (e.g., the block 1002 of FIG. 10 ).
- samples from neighboring blocks may or may not be used as inputs to the CNN.
- TOP can be a row of previously reconstructed pixels that are peripheral to the top edge of the block 1002 ; and LEFT can be a column of previously reconstructed pixels that are peripheral to the left edge of the block 1002 .
- TOP and LEFT can be added, instead or in addition, to the input block that is presented to the CNN.
- a non-linear function of QP is also used as an input to the concatenation layer 1016 .
- a quadratic function (i.e., QP 2 ) is illustrated in FIG. 10 .
- the function used depends on the codec; more specifically, the function used depends on the standard implemented by the codec. For example, a quadratic function is used in the case of a codec that implements H.263, VP9, or AV1; and an exponential function is used in the case of a codec that implements H.264 or HEVC.
- a total of 897 inputs can be received by the concatenation layer 1016 .
- a sample i.e., a pixel
- the concatenation layer 1016 receives 898 inputs.
- the CNN 1000 includes three classifiers; namely, classifiers 1018 , 1020 , and 1022 .
- Each of the classifiers 1018 , 1020 , 1022 includes a set of classification layers and uses convolutions as further described below.
- the classifier 1018 infers (i.e., outputs) partition decisions for sub-blocks of size 16 ⁇ 16 of the block 1002 . It is noted that the block 1002 can be partitioned into 4 ⁇ 4 blocks, each of size 16 ⁇ 16. As such, the classifier 1018 reduces, to a size of 4 ⁇ 4, the feature maps (which are each of size 8 ⁇ 8) received from the concatenation layer 1016 .
- feature maps 1019 are obtained from the feature maps received from the concatenation layer 1016 by applying non-overlapping convolutions using 2 ⁇ 2 separable convolution filters to combine some of the feature maps into one, thereby resulting in 256 feature maps, each of size 4 ⁇ 4.
- a series of 1 ⁇ 1 convolutions are applied, successively, to gradually reduce the feature dimension size to 1.
- 1 ⁇ 1 ⁇ 128 convolutions (where the 128 being the number of filters) are applied, to the feature maps 1019 , resulting in 4 ⁇ 4 ⁇ 128 feature maps, to which 1 ⁇ 1 ⁇ 64 convolutions (where the 64 being the number of filters) are applied resulting in 4 ⁇ 4 ⁇ 64 feature maps, to which 1 ⁇ 1 ⁇ 32 convolutions are applied resulting in 4 ⁇ 4 ⁇ 32 feature maps, to which a 1 ⁇ 1 ⁇ 1 convolution is applied resulting in a 4 ⁇ 4 ⁇ 1 feature map, namely the feature map 1025 .
- the classifier 1018 For each 16 ⁇ 16 sub-block of the block 1002 , the classifier 1018 infers whether to split or not split the sub-block. As such, the classifier 1018 outputs 16 decisions corresponding, respectively, to each of the 16 ⁇ 16 sub-blocks of the block 1002 .
- the 16 decisions can be binary decisions. That is, the feature map 1025 can be thought of as a matrix of binary decisions. For example, a zero (0) can correspond to a decision not to split a sub-block and a one (1) can correspond to a decision to split the sub-block.
- the order of the output of the classifier 1018 can correspond to a raster scan order of the 16 ⁇ 16 sub-blocks of the block 1002 .
- the decisions can correspond to probabilities (i.e., values that range from 0 to 1), or some other values, such as values that range from 0 to 100.
- a decision is greater than a threshold that is appropriate for the range of the decision values (e.g., 0.9, 0.75%, 90, etc.), it can be considered to correspond to a binary decision of 1.
- the classifier 1020 infers (i.e., outputs) partition decisions for sub-blocks of size 32 ⁇ 32 of the block 1002 .
- the classifier 1020 receives the feature maps 1019 and convolves each of the feature maps with a 2 ⁇ 2 separable convolution filters to combine feature maps of the feature maps 1019 into one, thereby resulting in feature maps 1021 .
- the block 1002 can be partitioned into 2 ⁇ 2 blocks, each of size 32 ⁇ 32.
- the classifier 1020 reduces, to the size of 2 ⁇ 2, the feature maps 1019 (which are each of size 4 ⁇ 4) through a series of non-overlapping convolutions using 1 ⁇ 1 filters to gradually reduce the feature dimension size to 1, as described above with respect to the feature maps 1019 , thereby resulting in a feature map 1027 .
- the classifier 1020 infers whether to split or not split the sub-block. As such, the classifier 1020 outputs 4 decisions corresponding, respectively, to each of the 32 ⁇ 32 sub-blocks of the block 1002 .
- the classifier 1022 infers (i.e., outputs) partition decisions for the block 1002 itself.
- the classifier 1022 receives the feature maps 1021 and convolves each of the feature maps with a 2 ⁇ 2 separable convolution filter resulting in feature maps 1023 , which some of the feature maps of the features maps 1021 into 1.
- the block 1002 can be partitioned into only one 1 ⁇ 1 block of size 64 ⁇ 64.
- the classifier 1022 reduces, to the size of 1 ⁇ 1, the feature maps 1023 (which are each of size 1 ⁇ 1) through a series of non-overlapping convolutions using 1 ⁇ 1 filters to gradually reduce the feature dimension size to 1, as described above with respect to the feature maps 1019 , thereby resulting in a feature map 1029 .
- the classifier 1022 infers whether to split or not split the block 1002 .
- the classifier 1022 outputs 1 decision corresponding to whether to split or not split the block 1002 into four 32 ⁇ 32 sub-blocks.
- Separable convolution filters of size 2 ⁇ 2 are described to obtain the feature maps 1019 , 1021 and 1023 (of the classifiers 1018 , 1020 , and 1022 , respectively) in order to ultimately determine, for a block of size 64 ⁇ 64, 4 ⁇ 4 16 ⁇ 16 partitions (i.e., the feature map 1025 ), 2 ⁇ 2 32 ⁇ 32 partitions (i.e., the feature map 1027 ), and 1 ⁇ 1 64 ⁇ 64 partition (i.e., the feature map 1029 ), respectively.
- any convolutional filters of size 2 k can be used as long the classifiers 1018 , 1020 , and 1022 determine, as described, 4 ⁇ 4 16 ⁇ 16 partitions (i.e., the feature map 1025 ), 2 ⁇ 2 32 ⁇ 32 partitions (i.e., the feature map 1027 ), and 1 ⁇ 1 64 ⁇ 64 partition (i.e., the feature map 1029 ).
- the feature map 1025 which has a dimension of 4 ⁇ 4 ⁇ 1, is shown as being directly derived (i.e., there are no additional intervening convolution operations) from a feature maps 1034 , which is of size 4 ⁇ 4 ⁇ 32.
- a dot-dashed line 1036 This is illustrated by a dot-dashed line 1036 .
- the same can be applicable to the classifiers 1020 and 1022 with respect to feature map 1027 and the feature map 1029 , respectively.
- a parameter can be used as a configuration parameter (i.e., a threshold parameter) of the CNN. If the number of remaining features is less than or equal to the threshold parameter, then the number of features of the next layer can be set to 1.
- the threshold parameter is set to 32.
- the next layer corresponds to the layer that produces the feature map 1025 , which has a feature dimension of 1.
- each of the classifiers can be configured with a different respective threshold parameter. In another example, all the classifiers can be configured to use the same threshold parameter.
- the feature map dimensionality (i.e., the last dimension of a feature maps) within a classifier can be reduced using a feature reduction parameter F.
- a classifier can reduce the number of channels according to the progression IncomingFeature, IncomingFeature/F, IncomingFeature/F 2 , . . . , 1, where IncomingFeature is the number of features that are initially received by the layer.
- each of the classifiers can be configured with a different respective feature reduction parameter.
- all the classifiers can be configured to use the same feature reduction parameter.
- the classifier 1018 is now used to illustrate the threshold parameter and the feature reduction parameter.
- IncomingFeature is 256 (as illustrated by the features maps 1019 , which is of size 4 ⁇ 4 ⁇ 256), the feature reduction parameter F is 2, and the threshold parameter is 32.
- the classifier 1018 reduces the number of channels according to the progression 256, 256/2, 256/2 2 , 256/2 3 , and 1. That is, the classifier 1018 reduces the number of channels according to the progression 256, 128, 64, 32, and 1.
- the classifier 1018 does not include a layer where the number of channels is 256/2 4 (i.e., 16) since, at the progression 256/2 3 (i.e., 32), the threshold parameter 32 for the number of channels is reached.
- the CNN 1000 can be extended to infer partition decisions for other block sizes.
- an encoder may allow the smallest partition to be of size 4 ⁇ 4.
- a branch can be added to the feature extraction layers 1003 such that each branch of the feature extraction layers 1003 can generate feature maps, each of size 4 ⁇ 4, as inputs to the concatenation layer 1016 .
- a classifier can be added between the concatenation layer 1016 and the classifier 1018 .
- the added classifier infers (i.e., outputs) partition decisions for sub-blocks of size 8 ⁇ 8 of the block 1002 .
- the block 1002 can be partitioned into 8 ⁇ 8 sub-blocks, each of size 8 ⁇ 8.
- the added classifier reduces, to a size of 8 ⁇ 8 ⁇ 1, the feature maps received from the concatenation layer 1016 through a series of non-overlapping convolutions using 2 ⁇ 2 filters.
- the CNN 1000 can be configured to infer partition decisions of a 128 ⁇ 128 block.
- a CNN can be configured to include classifiers that determine, respectively, 1 (i.e., a 1 ⁇ 1 output matrix) 128 ⁇ 128 decision (i.e., one decision corresponding to whether the block is or is not to be split), 4 (i.e., a 2 ⁇ 2 output matrix) 64 ⁇ 64 decisions, 16 (i.e., a 4 ⁇ 4 output matrix) 32 ⁇ 32 decisions, and 64 (i.e., a 8 ⁇ 8 output matrix) 16 ⁇ 16 decisions.
- 1 i.e., a 1 ⁇ 1 output matrix
- 128 ⁇ 128 decision i.e., one decision corresponding to whether the block is or is not to be split
- 4 i.e., a 2 ⁇ 2 output matrix
- 64 ⁇ 64 decisions i.e., 16 (i.e., a 4 ⁇ 4 output matrix) 32 ⁇ 32 decisions
- 64 i.e., a 8 ⁇ 8 output matrix
- the CNN 1000 can include early termination features. For example, if the classifier 1022 infers that the block 1002 is not to be split, then processing through the classifiers 1020 and 1018 need not be continued. Similarly, if the classifier 1020 infers that none of the 32 ⁇ 32 sub-blocks of the block 1002 is to be split, then processing through the classifier 1020 need not be continued.
- FIG. 11 is a flowchart of a process 1100 for encoding, by an encoder (i.e., a first encoder), an image block using a first quantization parameter according to implementations of this disclosure.
- the process 1100 trains, using input data, a machine-learning model to infer a mode decision.
- the process 1100 uses the trained machine-learning model to infer a mode decision for an image block, which is to be encoded using a quantization parameter (i.e., a first quantization parameter).
- the mode decision can be a quad-tree partition decision of the image block.
- the image block can be a block of an image (e.g., a video frame) that is encoded using intra-prediction.
- the mode decision can be a partition that includes partitions described with respect to FIG. 16 .
- some of the partitions of FIG. 16 include square and non-square sub-partition; and each of the square sub-partitions can be further partitioned according to one of the partitions of FIG. 16 .
- the process 1100 trains the machine-learning (ML) model.
- the ML model can be trained using a training data 1112 .
- Each training datum of the training data 1112 can include a video block that was encoded by traditional encoding methods (e.g., by a second encoder), such as described with respect to FIGS. 4 and 6-9 ; a value (i.e., a second value) corresponding to a quantization parameter (i.e., a second quantization parameter) used by the second encoder; zero or more additional inputs corresponding to inputs used by the second encoder in determining the mode decision for encoding the video block; and the resulting mode decision determined by the second encoder.
- a quantization parameter i.e., a second quantization parameter
- parameters of the ML model are generated such that, for at least some of the training data, the ML model can infer, for a training datum, the resulting mode decision of the training datum for a set of inputs that includes the video block, the value corresponding to a quantization parameter, and zero or more additional inputs of the training datum.
- the second value corresponding to the second quantization parameter has a non-linear relation to the second the quantization parameter. That is, the second value is derived from the second quantization parameter based on a non-linear function of the second quantization parameter.
- the non-linear function can be an exponential function of the second quantization parameter.
- the exponential function can be used when the second encoder is an H.264 or an HEVC encoder.
- c 1 ⁇ 3.
- the non-linear function can be a quadratic function of the second quantization parameter. The quadratic function can be used when the second encoder is an H.263, an AV1, or a VP9 encoder.
- the non-linear function is of a same type as a function used by the second encoder for determining a multiplier used in a rate-distortion calculation, as described above.
- the resulting mode decision determined by the second encoder can be indicative of the quad-tree partition of the training block of the training datum.
- Many indications (e.g., representations) of the quad-tree partition are possible.
- a vector (e.g., sequence) of binary flags, as described with respect to the quad-tree 703 can be used.
- the zero or more additional inputs corresponding to inputs used by the second encoder in determining the mode decision for encoding the video block can include at least some of the samples (i.e., first samples) of the top neighboring block, at least some of the samples (i.e., second samples) of the left neighboring block of the input block, at least some of the samples of the top-left neighboring block, or a combination thereof.
- the top-left neighboring block can be considered to be part of either the top neighboring block or the left neighboring block.
- the first samples or the second samples can be considered to include samples from the top-left neighboring block.
- the ML model learns (e.g., trains, builds, derives, etc.) a mapping (i.e., a function) that accepts, as input, a block (such as the block 1002 of FIG. 10 ) and a non-linear value of a quantization parameter (e.g., QP 2 as shown in FIG. 10 ) and output a partitioning of the block.
- a mapping i.e., a function
- the ML model be trained using a large range of input blocks and a large range of possible QP values, such as QP values that are used in representative of real-world applications.
- the ML model may well learn how to determine a mode decision for dark blocks but provide unreliable output when presented with non-dark blocks during the inference phase.
- the second encoder uses a discrete set of the QP values, then it is preferable that each of the QP values is well represented in the training data set.
- the QP value can vary from 0 to 1, then it is preferable that the training data include varying QP values in the range 0 to 1.
- the ML model may misbehave (e.g., provide erroneous output) when the missed QP value is presented to the ML model during the inference phase.
- a missed QP value i.e., a QP value that is not used during the training phase
- the missed QP can be interpolated from QP values that are used during the training phase and the interpolated QP value can then be used during the inference phase.
- the ML model can then be used by the process 1100 during an inference phase.
- the inference phase includes the operations 1104 and 1106 .
- a separation 1110 indicates that the training phase and the inference phase can be separated in time.
- the inferencing phase can be performed by a first encoder and the training data 1112 can be generated by a second encoder.
- the first encoder and the second encoder are the same encoder. That is, the training data 1112 can be generated by the same encoder that performs the inference phase.
- the inference phase uses a machine-learning model that is trained as described with respect to 1102 .
- inputs are presented to ML module. That is, the inputs are presented to a module that incorporates, includes, executes, implements, and the like the ML model.
- the inputs include the image block (e.g., as described with respect to the block 1002 of FIG. 10 ) and a non-linear function of a value (i.e., a first value) corresponding to the first quantization parameter.
- the first value is derived (i.e., results) from the non-linear function using the first quantization parameter as input to the non-linear function.
- the first value can be as described with respect to QP 2 of FIG. 10 .
- the inputs can also include additional inputs, as described above with respect to the zero or more additional inputs.
- the process 1100 obtains first mode decision parameters from the machine-learning model.
- the process 1100 obtains the first mode decision parameters as described with respect to FIG. 10 . That is, for example, for the block 1002 , the CNN 1000 of FIG. 10 provides an output that is indicative of a quad-tree partition of the block 1002 .
- the process 1100 encodes the image block using the first mode decision parameters. That is, and continuing with the example of inferring a block partitioning, for each of the sub-blocks (i.e., according to the output that is indicative of a quad-tree partition), the process 1100 can intra-predict the block as described with respect to the intra/inter-prediction stage 402 of FIG. 4 , and consistent with the description of FIG. 4 , ultimately entropy encode, as described with respect to the entropy encoding stage 408 , the image block in a compressed bitstream, such as the bitstream 420 of FIG. 4 .
- the non-linear function can be approximated by linear segments. Approximating the non-linear function by piecewise linear segments is illustrated with respect to FIG. 12 .
- FIG. 12 is an example 1200 of approximating a non-linear function of a quantization parameter using linear segments according to implementations of this disclosure.
- a quadratic function is used to describe the non-linear function.
- the example 1200 shows, as a dashed curve, a non-linear function 1202 of the quantization parameter.
- the non-linear function 1202 is QP 2 .
- the QP values range from 0 to 1.
- the example 1200 illustrates splitting the range 0 to 1 into several segments; namely, segments 1204 , 1206 , and 1208 . While three segments are illustrated, more or fewer, but more than 1, segments can be used.
- the range 0 to 1 can be split into a first range that includes the QP values 0 to 0.25, a second range that includes the QP values 0.25 to 0.75, and a third range that includes the QP values 0.75 to 1.
- FIG. 13 is an example 1300 of a rate-distortion performance comparison of a first machine-learning model 1316 that uses as input a non-linear QP function and a second machine-learning model 1326 that uses a linear QP function.
- the peak signal-to-noise ratio (PSNR) is used as the distortion metric.
- the results of graphs 1310 and 1320 are obtained by experimentation.
- the first machine-learning model 1316 is a model of a CNN that has an architecture as described (above and below) with respect to FIG. 10 .
- the fully connected layers of the second machine-learning model 1326 has 1.2 million parameters
- the first machine-learning model 1316 (which is an all-convolutional model and does not include fully connected classification layers) is much smaller with only 300,000 parameters (using a feature compression rate of 0.5).
- a power- and/or capacity-constrained platform e.g., a mobile device
- the reduced model size is due in part to each of, or the combination of, using the non-linear value of the QP (in this example, QP 2 ) and the CNN architecture, which has the receptive field conforming properties described herein.
- the first machine-learning model 1316 and the second machine-learning model 1326 are depicted as generic machine-learning models with an input layer, internal layers, and an output layer.
- the first machine-learning model 1316 and the second machine-learning model 1326 are depicted only to illustrate that the first machine-learning model 1316 uses a non-linear function of QP, namely QP 2 , whereas the second machine-learning model 1326 uses a linear function of QP, namely the QP value itself.
- a curve 1312 of graphs 1310 and 1320 depicts the rate-distortion performance of a VP9 encoder, as described with respect to FIG. 4 . That is the curve 1312 is generated based on brute-force encoding (i.e., encoding that is not based on a machine-learning model).
- a curve 1314 of the graph 1310 depicts the rate-distortion performance resulting from using the first machine-learning model 1316 to infer block partitions to be used in a VP9 software encoder.
- a curve 1324 of the graph 1320 depicts the rate-distortion performance resulting from using the second machine-learning model 1326 to infer block partitions to be used in a VP9 software encoder.
- the graphs show that, on average, higher rate-distortion performance can be achieved when using QP 2 .
- the performance, in BD-rate, is approximately 1.78% worse than brute-force encoding when using QP 2 ; whereas using QP, the performance is approximately 3.6% worse than brute-force encoding.
- an encoder that uses a machine-learning model, such as the ML model described with respect to FIG. 10 , to infer mode decision parameters for image block, can encode the mode decision parameters, in a compressed bitstream, such as the bitstream 420 of FIG. 4 .
- the image block can be a superblock and the mode decision parameters can be indicative of a quad-tree partition of the superblock.
- a decoder such as the decoder 500 of FIG. 5 , can decode the image block using the mode decisions parameters received in the compressed bitstream.
- a process of decoding an image block can include receiving, in a compressed bitstream, such as the compressed bitstream 420 of FIG. 5 , an indication of a quad-tree partitioning of the image block into sub-blocks; and decoding the image block using the indication of the quad-tree partitioning of the image block.
- a compressed bitstream such as the compressed bitstream 420 of FIG. 5
- the quad-tree partitioning can be determined by an encoder using a machine-learning model that is trained by using training data as inputs to train the machine-learning model.
- Each training datum can include a training block that is encoded by the encoder, mode decision parameters used by the encoder for encoding the training block, and a value corresponding to a quantization parameter.
- the mode decision parameters can be indicative of a quad-tree partitioning of the training block.
- the encoder used the quantization parameter for encoding the training block the value is the result of a non-linear function using the quantization parameter as input.
- the non-linear function can be of a same type as a function used by the encoder for determining a multiplier used in rate-distortion calculation.
- FIG. 14 is an example 1400 of a convolution filter according to implementations of this disclosure.
- the example 1400 includes a region 1402 of an image.
- the region 1402 is shown as a 6 ⁇ 6 region for the purposes of this example.
- convolution filters can be applied to any size block, superblock, region of image, or an image.
- convolution operations can be used to generate any of the feature maps 1004 , 1006 , 1008 , 1010 , and so on.
- a filter 1404 of size 3 ⁇ 3 is used in this example.
- filters can have different sizes.
- the example 1400 uses a non-overlapping convolution operation with a stride that is equal to the filter size. As such, the stride size, in each of the horizontal and vertical directions is 3.
- the filter 1404 is shown as including binary (i.e., zero and one) values. However, the values of a filter can by any value (e.g., positive and/or negative real values). As mentioned above, the values of a filter can be determined, by the machine-learning model, during the training phase of the machine-learning model, such as at 1102 of FIG. 11 .
- Feature map 1414 is the output of convolving the filter 1404 with the region 1402 .
- the filter 1404 is first convolved (e.g., using a matrix multiplication operation) with a sub-region 1406 .
- the filter 1404 is then convolved with a sub-region 1408 .
- the filter 1404 is then convolved with a sub-region 1410 .
- the filter 1404 is then convolved with a sub-region 1412 .
- FIG. 15 is an example 1500 of receptive fields according to implementations of this disclosure.
- the example 1500 includes an input 1502 .
- the example 1500 and the explanation herein are adapted from Dang Ha The Hien,. ‘A guide to receptive field arithmetic for Convolutional Neural Networks,’ April, 2017, [retrieved on Aug. 6, 2018]. Retrieved from the Internet ⁇ URL: https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807>.
- the input 1502 can be a portion of an image for which it is desirable to extract features (e.g., a feature map).
- the input 1502 can be, for example, the block 702 , one of the blocks 702 - 1 , 702 - 2 , 702 - 3 , and 702 - 4 , or one of the 702 - 5 , 702 - 6 , 702 - 7 , and 702 - 8 .
- the input 1502 is shown as having a size of 5 ⁇ 5 pixels. However, the size of the input 1502 is not pertinent to the description of the concept of a receptive field.
- Description of the concepts of padding, stride, and kernel (i.e., filter) size are omitted herein as such concepts are well known to a person skilled in the art.
- the example 1500 illustrates a first feature map 1504 that is the result of convolving the input 1502 with a first filter and a second feature map 1506 that is the result of convolving the first feature map with a second filter.
- the first filter and the second filter can have different values.
- the values of the filters can be determined (e.g., learned) during the training phase.
- a pixel 1508 (which may also be referred to as a feature) of the first feature map 1504 results from the convolution of pixels of the input 1502 .
- Such pixels are the receptive field of the pixel 1508 .
- the receptive field of the pixel 1508 is defined by a square whose corners are marked by black squares, such as a black square 1513 .
- Short-dash lines, such as a short-dash line 1512 and emanating from the corners of the pixel 1508 also illustrate the receptive field of the pixel 1508 . The end points of the short-dash lines are the back squares.
- a pixel 1510 (which may also be referred to as a feature) of the second feature map 1506 results from the convolution of pixels of the first feature map 1504 .
- Such pixels are the receptive field of the pixel 1510 in the first feature map 1504 and can be further projected onto the input 1502 to determine receptive field in the input 1502 .
- the convolution uses padding, some of the pixels used for generating the pixel 1510 are outside of the first feature map 1504 .
- the padding pixels of the first feature map 1504 are not shown so as to not further clutter FIG. 15 .
- the receptive field of the pixel 1510 in the input 1502 is defined by a square whose corners are marked by black circles, such as a black circle 1515 .
- Two-dot-dash lines, such as a two-dot-dash line 1514 , emanating from the corners of the pixel 1510 also illustrate the receptive field in the input 1502 of the pixel 1510 .
- the end points of the two-dot-dash lines are the back circles.
- the receptive field can play an important role in image analysis during video encoding.
- the receptive field of a series of convolution layers can be interpreted as the “region” of the image (e.g., a block, a superblock, a frame, or any other portion of an image) that each pixel (e.g., feature) “sees” (e.g., influenced by, summarizes, etc.) when computing the pixel (e.g., feature).
- Pixels at the initial input layer (e.g., the input 1502 ) become features (via a series of convolutions) for later layers (e.g., the second layer, which includes the second feature map 1506 ) of a CNN that will aid the CNN analyze the initial input layer.
- each analysis region becomes confined to the boundaries of its quadtree representation. That is, for example, it can be critical that features describing a region of an image, and which are used for inferring a partitioning of the region of the image, do not mix pixels from other regions of the image. That is, for example and referring to FIG.
- features describing the blocks 702 - 2 and/or the blocks 702 - 5 , 702 - 6 , 702 - 7 , and 702 - 8 not include, in their respective fields, pixels from any of the blocks 702 - 1 , 702 - 3 , or 702 - 4 .
- n out is the number of output features in a layer.
- a first layer corresponds to (e.g., includes) the first feature map 1504 and a second layer corresponds to (e.g., includes) the second feature map 1506 .
- n in is the number of input features to the layer.
- the number of input features to the second layer is the number of features in the first feature map 1504 , namely 9.
- k, p, and s are, respectively, the convolution kernel size, the convolution padding size, and the convolution stride size.
- Equation (1) calculates the number of output features of a layer based on the number of input features and the convolution properties.
- Equation (2) calculates a distance (i.e., a jump j out ) between two adjacent features in the output feature map.
- Equation (3) calculates the receptive field size (i.e., r out ) of the output feature map, which is define as the area that is covered by k input features and the extra area that is covered by the receptive field of the input feature that on the border.
- Equation (4) calculates the center position (i.e., star t out ) of the receptive field of the first output feature (e.g., the pixel 1508 and the pixel 1510 correspond, respectively, to the first output feature in the first feature map 1504 and the second feature map 1506 ).
- FIG. 10 is referred to again to describe additional features of the CNN 1000 of FIG. 10 .
- the CNN 1000 is an all-convolutional network. That is, the feature extraction and the classification layers use convolution operations. Whereas, as described above, a typical CNN includes fully connected layers for classification, the CNN 1000 uses convolution layers for classification.
- non-overlapping convolution operations are performed on the input at each layer by setting the stride value the same as the kernel size.
- N can be 2 k .
- the classification layers i.e., the layers of each of the classifiers 1018 , 1020 , and 1022
- convolution reduction with 1 ⁇ 1 kernels are performed until the number of desired outputs is reached.
- Convolutional layers are used in the classification layers. As such, the receptive fields are respected (e.g., preserved).
- non-overlapping convolution operations i.e., between the concatenation layer 1016 and the first layer of the classifiers 1018
- a kernel size 2 are performed to reduce the number of channels from 8 ⁇ 8 (i.e., the size of each of the feature maps of the concatenation layer 1016 as described above) to 4 ⁇ 4 (i.e., the size of each of the feature maps 1019 ), and from then on apply kernel size 1 ⁇ 1 and gradually reduce the feature dimension size to 1 (i.e., the feature map 1025 , which is of size 4 ⁇ 4 ⁇ 1).
- the output of the last classification layer is 4 ⁇ 4 ⁇ 1, which is the partition determination of the 16 sub-blocks of the input 1002 .
- Each of the 16 sub-blocks is of size 16 ⁇ 16 pixels
- the partition decision for each of the 32 ⁇ 32 sub-blocks can be inferred by the classifiers 1020 ; and the partition of the 64 ⁇ 64 block can be inferred by the classifiers 1022 .
- a kernel of size 1 ⁇ 1 can be used to reduce the dimensionality of feature maps.
- an input e.g., the feature maps 1034 , which is of size 4 ⁇ 4 ⁇ 32
- a feature map e.g., the feature maps 1025
- a kernel of size 1 ⁇ 1 can be used to pool (e.g., combine) information from multiple feature maps.
- a kernel of size 1 ⁇ 1 does not mix values from different locations of the input. That is, for example, when determining the value at location (x, y) of the feature map 1025 , only the 32 values at the location (x, y) of the each of the 32 maps of the feature maps 1034 are used. As such, by using 1 ⁇ 1 convolutions, the receptive fields can be preserved (e.g., respected).
- the advance in the state of the art is provided by the combination of using non-overlapping kernel sizes with an all-convolutional network (for feature extraction and for classification) that respects receptive fields.
- the kernel sizes can be even number (i.e., multiples of 2).
- the CNN 1000 is described for determining a partitioning of a 64 ⁇ 64 block (i.e., the block 1002 ) from a 64 ⁇ 64 partition (using the classifier 1022 ) down to whether (using the classifier 1018 ) each 16 ⁇ 16 sub-block should be further partitioned into 8 ⁇ 8 blocks.
- the disclosure herein is not so limited.
- a CNN architecture according to implementations of the disclosure can be generalized as follows.
- a convolutional neural network (CNN) for determining a block partitioning in video coding where the block is of size N ⁇ N (e.g., 64 ⁇ 64, 128 ⁇ 128) and where a smallest partition determined by the CNN is of size S ⁇ S (e.g., 4 ⁇ 4, 8 ⁇ 8), can include feature extraction layers (e.g., feature extraction layers 1003 ), a concatenation layer (e.g., the concatenation layer 1016 ), and classifiers (e.g., the classifiers 1018 , 1020 , 1022 ).
- the classifiers include all-convolutional layers.
- Other values of N and S can be possible. In some examples, N can be 32, 64, or 128, and S can be 4, 8, or 16.
- the concatenation layer receives, from the feature extraction layers, first feature maps of the block.
- Each first feature map is of size S ⁇ S (e.g., 8 ⁇ 8).
- the first feature maps can be as described with respect to the feature maps 1004 , 1008 , and 1014 of FIG. 10 .
- Each of the classifiers includes one or more classification layers.
- Each classification layer receives second feature maps having a respective feature dimension.
- the classifier 1018 includes 5 classification layers (illustrated by the 5 squares representing the feature maps of each layer) the classifier 1020 includes 4 classification layers, and the classifier 1022 includes 3 classification layers.
- the first classifier (e.g., the classifiers 1018 of FIG. 10 ), can receive the first feature maps from the concatenation layer (e.g., the concatenation layer 1016 ) and apply a first non-overlapping convolution operation using a first 2 ⁇ 2 kernel to reduce the first feature maps to a size of (S/2) ⁇ (S/2).
- the first layer of the classifier 1018 receives the 8 ⁇ 8 feature maps from the concatenation layer 1016 and reduces them to the size of 4 ⁇ 4 (i.e., the feature maps 1019 ).
- the feature maps 1019 is shown as having a dimension of 256. However, that need not be the case so long as the dimension of the last layer of each of the classifiers is N/( ⁇ S) ⁇ N/( ⁇ S) ⁇ 1.
- the feature maps 1019 is shown, for illustration purposes, as including a feature dimensionality of 256. However, that need not be the case.
- a kernel of size (2 k +N, 2 k +N) and a stride size of (2 k , 2 k ) can be used to propagate the N left/top information and observe (e.g., preserve) the perception field.
- the CNN includes a second classifier that infers partition decisions for sub-blocks of size ( ⁇ S) ⁇ ( ⁇ S).
- the second classifier can be the classifier 1022 of FIG. 10 .
- ⁇ 8.
- the second classifier can receive third feature maps, each of size M ⁇ M, from a third classifier.
- the third classifier can be the classifier 1020 .
- the second classifier can apply a second non-overlapping convolution operation using a second 2 ⁇ 2 kernel to reduce the third feature maps to a size of (M/2) ⁇ (M/2).
- the classifier 1022 receives the features maps 1021 from the classifier 1020 and applies a second non-overlapping convolution operation using a second 2 ⁇ 2 kernel to generate the feature maps 1023 .
- Each of the output values corresponds a block location and can be a value indicating whether a sub-block at that location should be partitioned or not. For example, a value of 0 can indicate that the sub-block is not to be partitioned and a value of 1 can indicate that the sub-block is to be partitioned. Other values are, of course, possible.
- the feature extraction layers can include a first feature extraction layer that applies an (N/S) ⁇ (N/S) non-overlapping convolutional filter to the block to generate a first subset of the first feature maps of the block.
- the feature extraction layers can further include a second feature extraction layer that is configured to apply an M ⁇ M non-overlapping convolutional filter to the block to generate maps each of size (N/M) ⁇ (N/M), where M is less than S, is greater than 1, and is a power of 2; and successively apply non-overlapping 2 ⁇ 2 convolutional filters to the maps to generate a second subset of the first feature maps of the block.
- the feature extraction layers 1003 -B and 1003 -C can be examples of the second feature extraction layer.
- a non-linear value of a quantization parameter can be used as an input to the CNN.
- the non-linear value of QP is shown as an input to the concatenation layer 1016 .
- the non-linear value of the QP can be used as an input to other layers of the CNN.
- the non-linear value of the QP can be used as the input to at least one of the classification layers.
- a CNN that is configured as described above can be used by an encoder, such as the encoder 400 of FIG. 4 , to infer a block partitioning.
- the block partitioning is not derived by brute force methods as are known in the art.
- the CNN can be used by the intra/inter-prediction stage 402 .
- an encoder can predict the blocks of the partitions using known prediction techniques, such as inter-prediction, intra-prediction, other techniques, or a combination thereof.
- a quad-tree such as described with respect to FIG. 7
- a quad-tree can be output in a compressed bitstream, such as the bitstream 420 of FIG. 4 .
- a decoder such as the decoder 500 of FIG. 5
- the quad-tree can be determined (e.g., inferred) in the encoder using a CNN that is configured as described above and output in the compressed bitstream.
- a decoder decodes, from the compressed bitstream, the quad-tree, which was inferred by the CNN that is configured as described with respect to FIG. 10 .
- a CNN can be used to infer non-square partitions that may or may not be represented by a quad-tree. That is, for example, a non-square partition can correspond to an internal node of the quad-tree having a number of children that is greater than or equal to two children.
- FIG. 16 is an example 1600 of non-square partitions of a block.
- Some encoders may partition a superblock, such as a super-block of size 64 ⁇ 64, 128 ⁇ 128, or any other size, of a square sub-block of the superblock, into one of the partitions of the example 1600 .
- a partition type 1602 (which may be referred to as the PARTITION_VERT_A) splits an N ⁇ N coding block into two horizontally adjacent square blocks, each of size N/2 ⁇ N/2, and a rectangular prediction unit of size N ⁇ N/2.
- a partition type 1608 (which may be referred to as the PARTITION_VERT_B) splits an N ⁇ N coding block into a rectangular prediction unit of size N ⁇ N/2 and two horizontally adjacent square blocks, each of size N/2 ⁇ N/2.
- a partition type 1604 (which may be referred to as the PARTITION_HORZ_A) splits an N ⁇ N coding block into two vertically adjacent square blocks, each of size N/2 ⁇ N/2, and a rectangular prediction unit of size N/2 ⁇ N.
- a partition type 1610 (which may be referred to as the PARTITION_HORZ_B) splits an N ⁇ N coding block into a rectangular prediction unit of size N/2 ⁇ N and two vertically adjacent square blocks, each of size N/2 ⁇ N/2.
- a partition type 1606 (which may be referred to as the PARTITION_VERT_4) splits an N ⁇ N coding block into four vertically adjacent rectangular blocks, each of size N ⁇ N/4.
- a partition type 1612 (which may be referred to as the PARTITION_HORZ_4) splits an N ⁇ N coding block into four horizontally adjacent rectangular blocks, each of size N/4 ⁇ N.
- a partition type 1614 (also referred to herein as the PARTITION_SPLIT partition type and partition-split partition type) splits an N ⁇ N coding block into four equally sized square sub-blocks. For example, if the coding block 1614 is of size N ⁇ N, then each of the four sub-blocks of the PARTITION_SPLIT partition type, such as a sub-block 1616 A, is of size N/4 ⁇ N/4.
- a partition type 1616 (also referred to herein as the PARTITION_VERT partition type) splits the coding block into two adjacent rectangular prediction units, each of size N ⁇ N/2.
- a partition type 1618 (also referred to herein as the PARTITION_HORZ partition type) splits the coding block into two adjacent rectangular prediction units, each of size N/2 ⁇ N.
- a partition type 1620 (also referred to herein as the PARTITION_NONE partition type and partition-none partition type) uses one prediction unit for the coding block such that the prediction unit has the same size (i.e., N ⁇ N) as the coding block.
- partition types 1614 - 1620 are referred to herein as basic partition types and the partitions 1602 - 1612 are referred to herein as extended partition types.
- a partition can be represented by a tree.
- a tree can be represented by a vector.
- P denote the set of all valid partitions (or, equivalently, the respective representations of the partitions). Accordingly, a CNN can be trained to infer a mapping into the set P. Configuring a CNN to infer the partitions described with respect to FIG. 16 includes defining an appropriate set P and using appropriate training data.
- the processes 800 and 1100 are each depicted and described as a series of blocks, steps, or operations. However, the blocks, steps, or operations in accordance with this disclosure can occur in various orders and/or concurrently. Additionally, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a technique in accordance with the disclosed subject matter.
- example or “implementation” are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” or “implementation” is not necessarily to be construed as being preferred or advantageous over other aspects or designs. Rather, use of the words “example” or “implementation” is intended to present concepts in a concrete fashion.
- the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise or clearly indicated otherwise by the context, “X includes A or B” is intended to mean any of the natural inclusive permutations thereof.
- Implementations of the transmitting station 102 and/or the receiving station 106 can be realized in hardware, software, or any combination thereof.
- the hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit.
- IP intellectual property
- ASICs application-specific integrated circuits
- programmable logic arrays optical processors
- programmable logic controllers programmable logic controllers
- microcode microcontrollers
- servers microprocessors, digital signal processors, or any other suitable circuit.
- signal processors should be understood as encompassing any of the foregoing hardware, either singly or in combination.
- signals and “data” are used interchangeably. Further, portions of the transmitting station 102 and the receiving station 106 do not necessarily have to be implemented in the same manner.
- the transmitting station 102 or the receiving station 106 can be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein.
- a special-purpose computer/processor which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein, can be utilized.
- the transmitting station 102 and the receiving station 106 can, for example, be implemented on computers in a video conferencing system.
- the transmitting station 102 can be implemented on a server, and the receiving station 106 can be implemented on a device separate from the server, such as a handheld communications device.
- the transmitting station 102 using an encoder 400 , can encode content into an encoded video signal and transmit the encoded video signal to the communications device.
- the communications device can then decode the encoded video signal using a decoder 500 .
- the communications device can decode content stored locally on the communications device, for example, content that was not transmitted by the transmitting station 102 .
- Other transmitting station 102 and receiving station 106 implementation schemes are available.
- the receiving station 106 can be a generally stationary personal computer rather than a portable communications device, and/or a device including an encoder 400 may also include a decoder 500 .
- implementations of the present disclosure can take the form of a computer program product accessible from, for example, a tangible computer-usable or computer-readable medium.
- a computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor.
- the medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.
Abstract
Description
λmode=0.85×2(QP−12)/3 (1)
λmode=0.85·Q H263 2 (2)
rdmult=88·q 2/24 (3)
λmode=0.12·Q AV1 2/256 (4)
DISTORTION+λmode×RATE, (5)
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/086,591 US11310498B2 (en) | 2018-09-18 | 2020-11-02 | Receptive-field-conforming convolutional models for video coding |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/134,165 US10869036B2 (en) | 2018-09-18 | 2018-09-18 | Receptive-field-conforming convolutional models for video coding |
US17/086,591 US11310498B2 (en) | 2018-09-18 | 2020-11-02 | Receptive-field-conforming convolutional models for video coding |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/134,165 Continuation US10869036B2 (en) | 2018-09-18 | 2018-09-18 | Receptive-field-conforming convolutional models for video coding |
Publications (2)
Publication Number | Publication Date |
---|---|
US20210051322A1 US20210051322A1 (en) | 2021-02-18 |
US11310498B2 true US11310498B2 (en) | 2022-04-19 |
Family
ID=69773628
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/134,165 Active 2038-10-31 US10869036B2 (en) | 2018-09-18 | 2018-09-18 | Receptive-field-conforming convolutional models for video coding |
US17/086,591 Active US11310498B2 (en) | 2018-09-18 | 2020-11-02 | Receptive-field-conforming convolutional models for video coding |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/134,165 Active 2038-10-31 US10869036B2 (en) | 2018-09-18 | 2018-09-18 | Receptive-field-conforming convolutional models for video coding |
Country Status (1)
Country | Link |
---|---|
US (2) | US10869036B2 (en) |
Families Citing this family (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11159789B2 (en) * | 2018-10-24 | 2021-10-26 | City University Of Hong Kong | Generative adversarial network based intra prediction for video coding |
CN110209859B (en) * | 2019-05-10 | 2022-12-27 | 腾讯科技（深圳）有限公司 | Method and device for recognizing places and training models of places and electronic equipment |
WO2020236446A1 (en) * | 2019-05-17 | 2020-11-26 | Corning Incorporated | Predicting optical fiber manufacturing performance using neural network |
JP2021047711A (en) * | 2019-09-19 | 2021-03-25 | キオクシア株式会社 | Arithmetic unit, arithmetic method, and learning method |
US11023791B2 (en) * | 2019-10-30 | 2021-06-01 | Kyocera Document Solutions Inc. | Color conversion using neural networks |
CN111126589B (en) * | 2019-12-31 | 2022-05-20 | 昆仑芯(北京)科技有限公司 | Neural network data processing device and method and electronic equipment |
US11394980B2 (en) * | 2020-01-05 | 2022-07-19 | Isize Limited | Preprocessing image data |
US11948090B2 (en) * | 2020-03-06 | 2024-04-02 | Tencent America LLC | Method and apparatus for video coding |
US11379951B2 (en) | 2020-03-25 | 2022-07-05 | Nintendo Co., Ltd. | Systems and methods for machine learned image conversion |
EP4128043A4 (en) * | 2020-03-25 | 2023-08-23 | Nintendo Co., Ltd. | Systems and methods for machine learned image conversion |
US11494875B2 (en) | 2020-03-25 | 2022-11-08 | Nintendo Co., Ltd. | Systems and methods for machine learned image conversion |
US11496151B1 (en) * | 2020-04-24 | 2022-11-08 | Tencent America LLC | Neural network model compression with block partitioning |
US11645587B2 (en) * | 2020-07-08 | 2023-05-09 | Vmware, Inc. | Quantizing training data sets using ML model metadata |
TWI806199B (en) * | 2020-10-20 | 2023-06-21 | 大陸商華為技術有限公司 | Method for signaling of feature map information, device and computer program |
US11582442B1 (en) * | 2020-12-03 | 2023-02-14 | Amazon Technologies, Inc. | Video encoding mode selection by a hierarchy of machine learning models |
CN112200169B (en) * | 2020-12-07 | 2021-04-30 | 北京沃东天骏信息技术有限公司 | Method, apparatus, device and storage medium for training a model |
WO2022139616A1 (en) | 2020-12-24 | 2022-06-30 | Huawei Technologies Co., Ltd. | Decoding with signaling of feature map data |
CN113158573B (en) * | 2021-04-29 | 2022-03-29 | 广西大学 | Small hydropower station regional distribution network optimal power flow obtaining method based on deep learning |
US20220408098A1 (en) * | 2021-06-18 | 2022-12-22 | Tencent America LLC | Block-wise entropy coding method in neural image compression |
WO2023081091A2 (en) * | 2021-11-04 | 2023-05-11 | Op Solutions, Llc | Systems and methods for motion information transfer from visual to feature domain and feature-based decoder-side motion vector refinement control |
CN115278360B (en) * | 2022-07-18 | 2023-11-07 | 天翼云科技有限公司 | Video data processing method and electronic equipment |
CN116246075B (en) * | 2023-05-12 | 2023-07-21 | 武汉纺织大学 | Video semantic segmentation method combining dynamic information and static information |
Citations (29)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5568414A (en) | 1993-03-22 | 1996-10-22 | Fuji Xerox Co., Ltd. | Nonlinear operation unit and data processing apparatus using the nonlinear operation unit |
US20060140401A1 (en) | 2000-12-08 | 2006-06-29 | Johnson Harold J | System and method for protecting computer software from a white box attack |
US20090010557A1 (en) | 2006-03-16 | 2009-01-08 | Huawei Technologies Co., Ltd. | Method and apparatus for realizing adaptive quantization in process of image coding |
US20100002764A1 (en) | 2008-07-03 | 2010-01-07 | National Cheng Kung University | Method For Encoding An Extended-Channel Video Data Subset Of A Stereoscopic Video Data Set, And A Stereo Video Encoding Apparatus For Implementing The Same |
US7788196B2 (en) | 2003-09-09 | 2010-08-31 | Semeion | Artificial neural network |
US20120173527A1 (en) | 2010-12-31 | 2012-07-05 | Microsoft Corporation | Variational Mode Seeking |
US20130128963A1 (en) | 2010-01-06 | 2013-05-23 | Dolby Laboratories Licensing Corporation | Multiple-Pass Rate Control for Video Coding Applications |
US20160065959A1 (en) | 2014-08-26 | 2016-03-03 | Lyrical Labs Video Compression Technology, LLC | Learning-based partitioning for video encoding |
CN105791826A (en) | 2016-05-11 | 2016-07-20 | 南京大学 | Data mining-based HEVC inter-frame fast mode selection method |
US20170085915A1 (en) | 2015-09-21 | 2017-03-23 | Google Inc. | Low-latency two-pass video coding |
US9615401B2 (en) | 2012-12-11 | 2017-04-04 | Qualcomm Incorporated | Methods and apparatus for updating a device configuration |
US20170140253A1 (en) | 2015-11-12 | 2017-05-18 | Xerox Corporation | Multi-layer fusion in a convolutional neural network for image classification |
KR20170059040A (en) | 2015-11-19 | 2017-05-30 | 전자부품연구원 | Optimal mode decision unit of video encoder and video encoding method using the optimal mode decision |
US20170231550A1 (en) | 2014-08-25 | 2017-08-17 | Singapore University Of Technology And Design | Method and device for analysing an image |
US20170278289A1 (en) | 2016-03-22 | 2017-09-28 | Uru, Inc. | Apparatus, systems, and methods for integrating digital media content into other digital media content |
US20180098062A1 (en) | 2016-09-30 | 2018-04-05 | Qualcomm Incorporated | Frame rate up-conversion coding mode |
US20180139450A1 (en) | 2016-11-15 | 2018-05-17 | City University Of Hong Kong | Systems and methods for rate control in video coding using joint machine learning and game theory |
US20180227585A1 (en) | 2017-02-06 | 2018-08-09 | Google Inc. | Multi-level Machine Learning-based Early Termination in Partition Search for Video Encoding |
US20180253622A1 (en) | 2017-03-06 | 2018-09-06 | Honda Motor Co., Ltd. | Systems for performing semantic segmentation and methods thereof |
US20180322606A1 (en) | 2017-05-05 | 2018-11-08 | Intel Corporation | Data parallelism and halo exchange for distributed machine learning |
US20180350110A1 (en) * | 2017-05-31 | 2018-12-06 | Samsung Electronics Co., Ltd. | Method and device for processing multi-channel feature map images |
US20190035431A1 (en) | 2017-07-28 | 2019-01-31 | Adobe Systems Incorporated | Apparatus, systems, and methods for integrating digital media content |
US20190147332A1 (en) | 2017-11-14 | 2019-05-16 | Advanced Micro Devices, Inc. | Memory bandwidth reduction techniques for low power convolutional neural network inference applications |
US20190244394A1 (en) | 2018-02-08 | 2019-08-08 | Uber Technologies, Inc. | Leveraging jpeg discrete cosine transform coefficients in neural networks |
US10489703B2 (en) | 2015-05-20 | 2019-11-26 | Nec Corporation | Memory efficiency for convolutional neural networks operating on graphics processing units |
US20190361456A1 (en) * | 2018-05-24 | 2019-11-28 | GM Global Technology Operations LLC | Control systems, control methods and controllers for an autonomous vehicle |
US20200042860A1 (en) | 2018-08-01 | 2020-02-06 | Nanjing Iluvatar CoreX Technology Co., Ltd. (DBA “Iluvatar CoreX Inc. Nanjing”) | Data reuse method based on convolutional neural network accelerator |
US20200213587A1 (en) | 2017-08-28 | 2020-07-02 | Interdigital Vc Holdings, Inc. | Method and apparatus for filtering with mode-aware deep learning |
US20200244997A1 (en) | 2017-08-28 | 2020-07-30 | Interdigital Vc Holdings, Inc. | Method and apparatus for filtering with multi-branch deep learning |
-
2018
- 2018-09-18 US US16/134,165 patent/US10869036B2/en active Active
-
2020
- 2020-11-02 US US17/086,591 patent/US11310498B2/en active Active
Patent Citations (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5568414A (en) | 1993-03-22 | 1996-10-22 | Fuji Xerox Co., Ltd. | Nonlinear operation unit and data processing apparatus using the nonlinear operation unit |
US20060140401A1 (en) | 2000-12-08 | 2006-06-29 | Johnson Harold J | System and method for protecting computer software from a white box attack |
US7788196B2 (en) | 2003-09-09 | 2010-08-31 | Semeion | Artificial neural network |
US20090010557A1 (en) | 2006-03-16 | 2009-01-08 | Huawei Technologies Co., Ltd. | Method and apparatus for realizing adaptive quantization in process of image coding |
US20100002764A1 (en) | 2008-07-03 | 2010-01-07 | National Cheng Kung University | Method For Encoding An Extended-Channel Video Data Subset Of A Stereoscopic Video Data Set, And A Stereo Video Encoding Apparatus For Implementing The Same |
US20130128963A1 (en) | 2010-01-06 | 2013-05-23 | Dolby Laboratories Licensing Corporation | Multiple-Pass Rate Control for Video Coding Applications |
US20120173527A1 (en) | 2010-12-31 | 2012-07-05 | Microsoft Corporation | Variational Mode Seeking |
US9615401B2 (en) | 2012-12-11 | 2017-04-04 | Qualcomm Incorporated | Methods and apparatus for updating a device configuration |
US20170231550A1 (en) | 2014-08-25 | 2017-08-17 | Singapore University Of Technology And Design | Method and device for analysing an image |
US20160065959A1 (en) | 2014-08-26 | 2016-03-03 | Lyrical Labs Video Compression Technology, LLC | Learning-based partitioning for video encoding |
US10489703B2 (en) | 2015-05-20 | 2019-11-26 | Nec Corporation | Memory efficiency for convolutional neural networks operating on graphics processing units |
US9807416B2 (en) | 2015-09-21 | 2017-10-31 | Google Inc. | Low-latency two-pass video coding |
US20170085915A1 (en) | 2015-09-21 | 2017-03-23 | Google Inc. | Low-latency two-pass video coding |
US20170140253A1 (en) | 2015-11-12 | 2017-05-18 | Xerox Corporation | Multi-layer fusion in a convolutional neural network for image classification |
KR20170059040A (en) | 2015-11-19 | 2017-05-30 | 전자부품연구원 | Optimal mode decision unit of video encoder and video encoding method using the optimal mode decision |
US20170278289A1 (en) | 2016-03-22 | 2017-09-28 | Uru, Inc. | Apparatus, systems, and methods for integrating digital media content into other digital media content |
CN105791826A (en) | 2016-05-11 | 2016-07-20 | 南京大学 | Data mining-based HEVC inter-frame fast mode selection method |
US20180098062A1 (en) | 2016-09-30 | 2018-04-05 | Qualcomm Incorporated | Frame rate up-conversion coding mode |
US20180139450A1 (en) | 2016-11-15 | 2018-05-17 | City University Of Hong Kong | Systems and methods for rate control in video coding using joint machine learning and game theory |
US20180227585A1 (en) | 2017-02-06 | 2018-08-09 | Google Inc. | Multi-level Machine Learning-based Early Termination in Partition Search for Video Encoding |
US20180253622A1 (en) | 2017-03-06 | 2018-09-06 | Honda Motor Co., Ltd. | Systems for performing semantic segmentation and methods thereof |
US20180322606A1 (en) | 2017-05-05 | 2018-11-08 | Intel Corporation | Data parallelism and halo exchange for distributed machine learning |
US20180350110A1 (en) * | 2017-05-31 | 2018-12-06 | Samsung Electronics Co., Ltd. | Method and device for processing multi-channel feature map images |
US20190035431A1 (en) | 2017-07-28 | 2019-01-31 | Adobe Systems Incorporated | Apparatus, systems, and methods for integrating digital media content |
US20200213587A1 (en) | 2017-08-28 | 2020-07-02 | Interdigital Vc Holdings, Inc. | Method and apparatus for filtering with mode-aware deep learning |
US20200244997A1 (en) | 2017-08-28 | 2020-07-30 | Interdigital Vc Holdings, Inc. | Method and apparatus for filtering with multi-branch deep learning |
US20190147332A1 (en) | 2017-11-14 | 2019-05-16 | Advanced Micro Devices, Inc. | Memory bandwidth reduction techniques for low power convolutional neural network inference applications |
US20190244394A1 (en) | 2018-02-08 | 2019-08-08 | Uber Technologies, Inc. | Leveraging jpeg discrete cosine transform coefficients in neural networks |
US20190361456A1 (en) * | 2018-05-24 | 2019-11-28 | GM Global Technology Operations LLC | Control systems, control methods and controllers for an autonomous vehicle |
US20200042860A1 (en) | 2018-08-01 | 2020-02-06 | Nanjing Iluvatar CoreX Technology Co., Ltd. (DBA “Iluvatar CoreX Inc. Nanjing”) | Data reuse method based on convolutional neural network accelerator |
Non-Patent Citations (45)
Title |
---|
"Introduction to Video Coding Part 1: Transform Coding", Mozilla, Mar. 2012, 171 pp. |
"Overview VP7 Data Format and Decoder", Version 1.5, On2 Technologies, Inc., Mar. 28, 2005, 65 pp. |
"VP6 Bitstream and Decoder Specification", Version 1.02, On2 Technologies, Inc., Aug. 17, 2006, 88 pp. |
"VP6 Bitstream and Decoder Specification", Version 1.03, On2 Technologies, Inc., Oct. 29, 2007, 95 pp. |
"VP8 Data Format and Decoding Guide, WebM Project", Google On2, Dec. 1, 2010, 103 pp. |
Bankoski et al., "VP8 Data Format and Decoding Guide draft-bankoski-vp8-bitstream-02", Network Working Group, Internet-Draft, May 18, 2011, 288 pp. |
Bankoski et al., "VP8 Data Format and Decoding Guide", Independent Submission RFC 6389, Nov. 2011, 305 pp. |
Bankoski, et al., "Technical Overview of VP8, An Open Source Video Codec for the Web", Jul. 11, 2011, 6 pp. |
Dang Ha The Hien, "A guide to receptive field arithmetic for Convolutional Neural Networks", 2017. <https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807>. |
Dertat; "Applied Deep Learning—Part 4: Convolution Neural Networks", https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks584bc134c1e2, Nov. 8, 2017. |
Duanmu et al; "Fast CU Partition Decision Using Machine Learning for Screen Content Compression"; IEEE International Conference on Image Processing; 2015; pp. 1-5. |
Duanmu, Fanyi et al., "Fast CU Partition Decision Using Machine Learning for Screen Content Compression", New York University; 2015; 5 Pages. |
Dumoulin et al., "A guide to convolution arithmetic for deep learning", 2016.<https://arxiv.org/abs/1603.07285>. |
Feng Zeqi et al: Based CTU Depth Range Prediction, 2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC), IEEE, Jun. 27, 2018, pp. 551-555. |
Galpin et al; "AHG9: CNN-Based Driving of Block Partitioning for Intra Slices Encoding"; JVEI Meeting; Apr. 3, 2018 http://phenix.int-evry.fr/jvet; pp. 1, 3. |
He et al.; "Fast HEVC Coding Unit Decision Based on BP-Neural Network"; International Journal of Grid Distribution Computing vol. 8, No. 4, (2015), pp. 289-300. |
He, Jing et al.; "Fast HEVC Coding Unit Decision Based on BP-Neutral Network" International Journal of Grid Distribution Computing vol. 8, No. 4, (2015), pp. 289-300. |
Howard, Andrew G., et al.; "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications" https://arxiv.org/abs/1704.04861; Apr. 17, 2017. |
International Search Report and Written Opinion of International Application No. PCT/US2019/051453; dated Dec. 4, 2019; 15 pages. |
International Search Report and Written Opinion of International Application No. PCT/US2019/051458; dated Dec. 13, 2019; 15 pages. |
Jin et al., "Fast QTBT Partition Algorithm for Intra Frame Coding through Convolutional Neural Network", IEEE Access vol. 6, 2018, pp. 54660-54673 (Year: 2018). * |
Li, T, et al, "A deep convolutional neural network approach for complexity reduction on intra-mode HEVC," Proceedings of the IEEE International Conference on Multimedia and Expo (ICME) 2017, (<http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8019316>). |
Liu Zhenyu et al; "CU Partition Mode Decision for HEVC Hardwired Intra Encoder Using Convolution Neural Netword" IEEE Transactions on Image Processing IEEE Service Center, vol. 25, No. 11; Nov. 1, 2016; pp. 5088-5103. |
MAI XU, TIANYI LI, ZULIN WANG, XIN DENG, REN YANG, ZHENYU GUAN: "Reducing Complexity of HEVC: A Deep Learning Approach", IEEE TRANSACTIONS ON IMAGE PROCESSING, IEEE, USA, vol. 27, no. 10, 22 March 2018 (2018-03-22), USA, pages 5044 - 5059, XP055647622, ISSN: 1057-7149, DOI: 10.1109/TIP.2018.2847035 * |
Maixu et al: "Reducing Complexity of HEVC: A Deep Learning Approach", IEEE Transactions on Image Processing., vol. 27, No. 10, Mar. 22, 2018 (Mar. 22, 2018), pp. 5044-5059, XP055647622, US ISSN:1057-7149, DOI: 10.1109 /TIP.2018.2847035 (Year: 2018). * |
Meng Xiandong et al: Filter Based on Multi-channel Long-Short-Term Dependency Residual Networks 2018 Data Compression Conference, IEEE, Mar. 27, 2018, pp. 187-196. |
Perez et al; "Genetic Selection of Non-Linear Product Terms in the Inputs to a Linear Classifier for Handwritten Digit Recognition"; IEEE International Conference on Systems, Man and Cybernetics; vol. 4; Oct. 7, 2001; p. 2339. |
Redmon et al., "You Only Look Once: Unified, Real-Time Object Detection" 2016. <https://arxiv.org/pdf/1506.02640.pdf>. |
Series H: Audiovisual and Multimedia Systems, Coding of moving video: Implementors Guide for H.264: Advanced video coding for generic audiovisual services, International Telecommunication Union, Jul. 30, 2010, 15 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video. Advanced video coding for generic audiovisual services, Amendment 1: Support of additional colour spaces and removal of the High 4:4:4 Profile, International Telecommunication Union, Jun. 2006, 16 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video. Advanced video coding for generic audiovisual services, International Telecommunication Union, Version 11, Mar. 2009. 670 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video. Advanced video coding for generic audiovisual services, International Telecommunication Union, Version 12, Mar. 2010, 676 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video. Advanced video coding for generic audiovisual services, Version 1, International Telecommunication Union, May 2003, 282 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video. Advanced video coding for generic audiovisual services, Version 3, International Telecommunication Union, Mar. 2005, 343 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video. Advanced video coding for generic audiovisual services, Version 8, International Telecommunication Union, Nov. 1, 2007, 564 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video. Amendment 2: New profiles for professional applications, International Telecommunication Union, Apr. 2007, 75 pp. |
Springenberg et al., "Striving for simplicity: the all convolutional net", ICLR, 2015. |
Springenberg, J. T., et al, "Striving for Simplicity: the all Convolutional Net," (<https://arxiv.org/pdf/1412.6806.pdf>). |
T. Wiegand, H. Schwarz, A. Joch, F. Kossentini, G. J. Sullivan, "Rate-constrained coder control and comparison of video coding standards", IEEE Trans. Circuits Syst. Video Technol., v. 13, Jul. 2003. |
Wiegand, et al, "Rate-constrained coder control and comparison of video coding standards", IEEE Trans. Circuits Syst. Video Technol., v. 13, Jul. 2003. |
Xu, M, et al, "Reducing Complexity of HEVC: A Deep Learning Approach", (<https://arxiv.org/abs/1710.01218>); Mar. 2018. |
Xu, M, et al, "Reducing Complexity of HEVC: A Deep Learning Approach", 2017;<https://arxiv.org/abs/1710.01218>. |
Zhang Yun et al; Machine Learning-Based Coding Unit Depth Decisions for Flexible Complexity Allocation in High Efficiency Video Coding: IEEE Transactions on Image Processing, IEEE Service Center, vol. 24, No. 7; Jul. 1, 2015; pp. 2230-2231. |
Zhang, Y, et al, "Machine Learning-Based Coding Unit Depth Decisions for Flexible Complexity Allocation in High Efficiency Video Coding", IEEE Transactions on Image Processing, vol. 24, No. 7, Jul. 201(<https://drive.google.com/open?id=1-csHxd6yrJ9K4GIFqXelKMFn3Dzyzeyw>). |
Zhenyu, L, et al, "CU Partition Mode Decision for HEVC Hardwired Intra Encoder Using Convolution Neural Network", IEEE Transactions on Image Processing, vol. 25, No. 11, Nov. 2016 (<https://drive.google.com/open?id=1QIJG7xJiNkz81Dcw4a6qQTmLV6MRIEAH>). |
Also Published As
Publication number | Publication date |
---|---|
US20200092552A1 (en) | 2020-03-19 |
US20210051322A1 (en) | 2021-02-18 |
US10869036B2 (en) | 2020-12-15 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11310501B2 (en) | Efficient use of quantization parameters in machine-learning models for video coding | |
US11310498B2 (en) | Receptive-field-conforming convolutional models for video coding | |
US11025907B2 (en) | Receptive-field-conforming convolution models for video coding | |
US11689726B2 (en) | Hybrid motion-compensated neural network with side-information based video coding | |
US10848765B2 (en) | Rate/distortion/RDcost modeling with machine learning | |
US10812813B2 (en) | Multi-level machine learning-based early termination in partition search for video coding | |
US11956447B2 (en) | Using rate distortion cost as a loss function for deep learning | |
US10009625B2 (en) | Low-latency two-pass video coding | |
EP3743855A1 (en) | Receptive-field-conforming convolution models for video coding | |
US20230123355A1 (en) | Adaptation of scan order for entropy coding | |
EP3198555B1 (en) | Frequency-domain denoising | |
US10652552B1 (en) | Efficient noise reduction coding | |
US20230007284A1 (en) | Ultra Light Models and Decision Fusion for Fast Video Coding | |
EP4338416A1 (en) | End-to-end learning-based, eg neural network, pre-processing and post-processing optimization for image and video coding | |
US20230131228A1 (en) | Debanding Using A Novel Banding Metric |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:COELHO, CLAUDIONOR;KUUSELA, AKI;LI, SHAN;AND OTHERS;SIGNING DATES FROM 20180917 TO 20180918;REEL/FRAME:054239/0450 |
|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: APPLICATION DISPATCHED FROM PREEXAM, NOT YET DOCKETED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |