US9177549B2 - Method and system for cross-lingual voice conversion - Google Patents
Method and system for cross-lingual voice conversion Download PDFInfo
- Publication number
- US9177549B2 US9177549B2 US14/069,492 US201314069492A US9177549B2 US 9177549 B2 US9177549 B2 US 9177549B2 US 201314069492 A US201314069492 A US 201314069492A US 9177549 B2 US9177549 B2 US 9177549B2
- Authority
- US
- United States
- Prior art keywords
- hmm
- output
- generator
- speech
- auxiliary
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/14—Speech classification or search using statistical models, e.g. Hidden Markov Models [HMMs]
- G10L15/142—Hidden Markov Models [HMMs]
-
- G06F17/289—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/033—Voice editing, e.g. manipulating the voice of the synthesiser
- G10L13/0335—Pitch control
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/065—Adaptation
- G10L15/07—Adaptation to the speaker
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/14—Speech classification or search using statistical models, e.g. Hidden Markov Models [HMMs]
- G10L15/142—Hidden Markov Models [HMMs]
- G10L15/144—Training of HMMs
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
Definitions
- ASR automatic speech recognition
- a goal of speech synthesis technology is to convert written language into speech that can be output in an audio format, for example directly or stored as an audio file suitable for audio output.
- the written language could take the form of text, or symbolic linguistic representations.
- the speech may be generated as a waveform by a speech synthesizer, which produces artificial human speech. Natural sounding human speech may also be a goal of a speech synthesis system.
- ASR automatic speech recognition
- Communication networks may in turn provide communication paths and links between some or all of such devices, supporting speech synthesis system capabilities and services that may utilize ASR and/or speech synthesis system capabilities.
- an example embodiment presented herein provides a method comprising: training an output hidden Markov model (HMM) based speech features generator implemented by one or more processors of a system using speech signals of an output speaker speaking an output language, wherein the output HMM based speech features generator comprises a first configuration of output HMM state models, each of the output HMM state models having a set of generator-model functions; training an auxiliary HMM based speech features generator implemented by one or more processors of the system using speech signals of an auxiliary speaker speaking an input language, wherein the auxiliary HMM based speech features generator comprises a second configuration of auxiliary HMM state models, each of the auxiliary HMM state models having a set of generator-model functions; for each given output HMM state model of the first configuration, determining a particular set of generator-model functions from among the auxiliary HMM state models of the second configuration that most closely matches the set of generator-model functions of the given output HMM; determining a fundamental frequency (F0) transform that speech-adapts F0 statistics of the output HMM
- an example embodiment presented herein provides a method comprising: implementing an output hidden Markov model (HMM) based speech features generator by one or more processors of a system, wherein the output HMM based speech features generator comprises a first configuration of output HMM state models, each of the output HMM state models having a set of generator-model functions, and wherein the implemented output HMM based speech features generator is trained using speech signals of an output speaker speaking an output language; implementing an auxiliary HMM based speech features generator by one or more processors of the system, wherein the auxiliary HMM based speech features generator comprises a second configuration of auxiliary HMM state models, each of the auxiliary HMM state models having a set of generator-model functions, and wherein the implemented auxiliary HMM based speech features generator is trained using speech signals of an auxiliary speaker speaking an input language; implementing a chimaera HMM based speech features generator that is the same as the output HMM based speech features generator, but wherein (i) the set of generator-model functions of each given output
- an example embodiment presented herein provides a system comprising: one or more processors; memory; and machine-readable instructions stored in the memory, that upon execution by the one or more processors cause the system to carry out functions including: implementing an output hidden Markov model (HMM) based speech features generator, wherein the output HMM based speech features generator comprises a first configuration of output HMM state models, each of the output HMM state models having a set of generator-model functions, and wherein the implemented output HMM based speech features generator is trained using speech signals of an output speaker speaking an output language; implementing an auxiliary HMM based speech features generator, wherein the auxiliary HMM based speech features generator comprises a second configuration of auxiliary HMM state models, each of the auxiliary HMM state models having a set of generator-model functions, and wherein the implemented auxiliary HMM based speech features generator is trained using speech signals of an auxiliary speaker speaking an input language; implementing a chimaera HMM based speech features generator that is the same as the output HMM based speech features
- an example embodiment presented herein provides an article of manufacture including a computer-readable storage medium having stored thereon program instructions that, upon execution by one or more processors of a system, cause the system to perform operations comprising: implementing an output hidden Markov model (HMM) based speech features generator, wherein the output HMM based speech features generator comprises a first configuration of output HMM state models, each of the output HMM state models having a set of generator-model functions, and wherein the implemented output HMM based speech features generator is trained using speech signals of an output speaker speaking an output language; implementing an auxiliary HMM based speech features generator, wherein the auxiliary HMM based speech features generator comprises a second configuration of auxiliary HMM state models, each of the auxiliary HMM state models having a set of generator-model functions, and wherein the implemented auxiliary HMM based speech features generator is trained using speech signals of an auxiliary speaker speaking an input language; implementing a chimaera HMM based speech features generator that is the same as the output HMM based speech features generator
- an example embodiment presented herein provides an article of manufacture including a computer-readable storage medium, having stored thereon program instructions that, upon execution by one or more processors of a system, cause the system to perform operations comprising: training an output hidden Markov model (HMM) based speech features generator using speech signals of an output speaker speaking an output language, wherein the output HMM based speech features generator comprises a first configuration of output HMM state models, each of the output HMM state models having a set of generator-model functions; training an auxiliary HMM based speech features generator using speech signals of an auxiliary speaker speaking an input language, wherein the auxiliary HMM based speech features generator comprises a second configuration of auxiliary HMM state models, each of the auxiliary HMM state models having a set of generator-model functions; for each given output HMM state model of the first configuration, determining a particular set of generator-model functions from among the auxiliary HMM state models of the second configuration that most closely matches the set of generator-model functions of the given output HMM; determining a fundamental frequency
- FIG. 1 is a flowchart illustrating an example method in accordance with an example embodiment.
- FIG. 2 is a flowchart illustrating an example method in accordance with another example embodiment.
- FIG. 3 is a block diagram of an example network and computing architecture, in accordance with an example embodiment.
- FIG. 4A is a block diagram of a server device, in accordance with an example embodiment.
- FIG. 4B depicts a cloud-based server system, in accordance with an example embodiment.
- FIG. 5 depicts a block diagram of a client device, in accordance with an example embodiment.
- FIG. 6 depicts a block diagram of an example cross-lingual speech-to-speech system, in accordance with an example embodiment.
- FIG. 7 depicts a block diagram of an example automatic speech recognition system, in accordance with an example embodiment.
- FIG. 8 depicts a block diagram of an example text-to-speech speech system, in accordance with an example embodiment.
- FIG. 9A is a schematic illustration of a first aspect of configuring of a voice conversion system, in accordance with an example embodiment.
- FIG. 9B is a schematic illustration of a second aspect of configuring of a voice conversion system, in accordance with an example embodiment.
- FIG. 9C is a schematic illustration of a third aspect using a voice conversion system, in accordance with an example embodiment.
- FIG. 10 depicts a block diagram of an example cross-lingual voice conversion system, in accordance with an example embodiment.
- FIG. 11 is a conceptual illustration of parametric and non-parametric mapping between vector spaces, in accordance with an example embodiment.
- a Speech-to-Speech (S2S) system can be a processor-based system configured to recognize speech of an input speaker in an input language, translate the recognized speech into a different, output language, and output the translated speech in the output language, in a synthesized voice for example.
- S2S translation system can be configured to translate in both directions between pairs of languages, enabling verbal communication between people who speak different languages of a pair but otherwise have little or no knowledge of, or ability to speak, each other's language. Even in just one translation direction, a S2S translation can provide useful uni-directional, cross-lingual verbal communications. Other applications are possible as well.
- a S2S language translation system may operate by recognizing input speech in an input language, converting the input speech into a form suitable for language translation (e.g., text), translating the converted input into a textual form in an output language, converting the translated text into a form suitable for speech synthesis in the output language, and generating audio output of the translated speech as synthesized speech in the output language.
- language translation e.g., text
- Functional elements of a S2S translation system may include an automatic speech recognition (ASR) subsystem, a language translation subsystem, and a text-to-speech (TTS) subsystem.
- An ASR subsystem can be used for receiving input speech in an input language, analytically decomposing the input speech into speech components that characterize elemental acoustic content and properties of the input language, and generating a textual version of the input speech from the particular speech components that, as determined by the ASR function, make up the input speech.
- a language translation subsystem may utilize statistical-machine-translation functions to translate the textual version of the input speech into a textual version of the speech translated into an output language.
- a TTS subsystem trained in the output language may then generate synthesized speech of the translated textual version in the output language.
- a S2S translation system can be seen as performing linguistic functions of language translation between an input and an output language, and acoustic functions of voice conversion between an input speaker and a synthesized output voice.
- language translation and voice conversion can be separated.
- voice conversion may be an aspect deserving of focused attention in a S2S translation system because, for among other reasons, it can be desirable in some applications for a synthesized output voice to sound like the voice of the input speaker.
- the ability to synthesize an output voice with characteristics of an input speaker can help facilitate a more natural flow of the conversation.
- the conversion of an input voice to an output voice is referred to as “cross-lingual voice conversion,” and the quality with which it can be achieved may be among the criteria used in rating the overall quality and effectiveness of a S2S translation system.
- cross-lingual voice conversion mainly theoretical in nature
- quality the quality of cross-lingual voice conversion may be viewed as having two basic dimensions: voice conversion quality and speech quality.
- voice conversion quality describes how convincing the transformation is, while the latter describes the quality of the synthesized speech.
- Speech quality can itself be broken down to two characteristic components: “naturalness,” which describes how natural synthesized speech sounds; and “intelligibility,” which evaluates the ability of synthesized speech to convey the linguistic content.
- naturalness has generally proven to be a more elusive goal, and achieving it is therefore afforded attention herein, together with achieving conversion quality.
- Scalability can be viewed in terms of at least two fundamental factors: maintenance cost and run-time cost.
- Maintenance cost is the cost to develop and maintain a system that can scale up to many 1000s of users. For example, a system that supports 10 million users each making 50 queries per month for translations between two languages per user can have storage costs for holding millions of transforms and run-time costs of handling millions of translation queries per month.
- Example embodiments of a method and system described herein are aimed at achieving natural sounding cross-lingual voice conversion with high conversion quality in a manner that also addresses issues of scalability. More specifically, an approach is devised that uses a hidden Markov model (HMM)-based ASR subsystem for receiving speech in an input language, a HMM-based TTS subsystem for outputting translated speech in an output language, and an optimized form of voice-conversion mapping between the HMM of the ASR subsystem and the HMM of the TTS subsystem that causes the synthesized output voice, in the output language, to sound like the input voice.
- HMM hidden Markov model
- TTS subsystem for outputting translated speech in an output language
- voice-conversion mapping between the HMM of the ASR subsystem and the HMM of the TTS subsystem that causes the synthesized output voice, in the output language, to sound like the input voice.
- the particular considerations of naturalness and conversion quality are addressed by an application of new techniques for the voice-conversion mapping.
- an HMM-based ASR subsystem for an input language may be trained using extensive input-language standard-voice recordings in an input language. This can amount to application of high-quality, proven training techniques, for example.
- this training process may be said to train the auxiliary HMM in the voice of the auxiliary speaker.
- the auxiliary HMM which is structured to model the input language, acquires a set of Gaussian statistical generator functions that have been iteratively and cumulatively built based on voice characteristics the auxiliary speaker.
- the Gaussian generator functions of the auxiliary HMM correspond to probability density functions (PDFs) for jointly modeling spectral envelope parameters and excitation parameters of fundamental speech units of the input language.
- the fundamental speech units could be phonemes or triphones, for example.
- an HMM-based TTS subsystem for an output language may be trained using extensive output-language standard-voice recordings in an output language. This too can amount to application of high-quality, proven training techniques, for example.
- this training process may be said to train the output HMM in the voice of the output speaker.
- the output HMM which is structured to model the output language, acquires a set of Gaussian statistical generator functions that have been iteratively and cumulatively built based on voice characteristics the output speaker.
- the Gaussian generator functions of the output HMM correspond to PDFs for jointly modeling spectral envelope parameters and excitation parameters of fundamental speech units of the output language. Again, the fundamental speech units could be phonemes or triphones, for example.
- an analytical matching procedure may be carried out to identify for each Gaussian statistical generator function of the output HMM a closest match from among the Gaussian statistical generator functions of the auxiliary HMM.
- This process is enabled by a novel and effective “matching under transform” technique, and results in a set of Gaussian statistical generator functions fashioned from characteristics of the auxiliary voice that can be applied by the output HMM, which, as noted, is structured for modeling the output language.
- the matching under transform (“MUT”) technique entails a matching procedure that can compensate for inter-speaker speech differences (e.g., differences between the output speaker and the auxiliary speaker).
- the matching procedure can be specified in terms of a MUT algorithm suitable for implementation as executable instructions on one or more processors of a system, such as a S2S system.
- the effect can be to construct a speech synthesizer in the output language with voice characteristics of the auxiliary speaker.
- a transform that adapts statistics of fundamental frequency (F0) of the output HMM to the F0 statistics of the auxiliary HMM is computed.
- F0 relates to the pitch of the voice.
- duration statistics of the output HMM to the duration statistics of the auxiliary HMM is also computed.
- duration relates to the tempo of the speech and time durations of the fundamental speech units (e.g., phonemes and/or triphones) and/or time durations of HMM states used to model the fundamental speech units.
- a cross-lingual HMM referred to herein as a “chimaera” HMM
- a cross-lingual HMM is constructed by first creating a copy of the output HMM, such that the chimaera HMM initially has the Gaussian statistical generator functions of the output HMM.
- the Gaussian statistical generator functions of the chimaera HMM which are initially the same as those of the output HMM, are replaced with the Gaussian statistical generator functions of the auxiliary HMM identified using the matching under transform algorithm.
- the F0 and duration transformations are applied to the chimaera HMM.
- the chimaera HMM can now be considered as being configured to generate acoustic features of speech units of the output language, but characterized by the sound of the auxiliary voice.
- the chimaera HMM may be used at run-time in a speech synthesizer to synthesize speech in the output language, but with voice characteristics of a run-time input speaker speaking in the input language. More specifically, prior to run-time, a HMM-adaption technique may be used compute an input-speaker transform that adapts the auxiliary HMM to the recordings of the input speaker. Such input-speaker adaptation transforms may be derived from a relatively small number and/or variety of the input speaker recordings (small compared with the extent of recordings typically used in the sort of robust training applied to the auxiliary HMM or the output HMM). The input-speaker adaptation transforms can provide a basis for adapting the voice of the auxiliary speaker to that of the input speaker.
- the input-speaker adaptation transform may be applied to the Gaussian statistical generator functions of the chimaera HMM at run-time.
- the result is to adapt the chimaera HMM, which is structured to generate acoustic features in the output language, to the voice of the input speaker.
- the input speaker speaks the input speech is converted to text, translated to the output language, and transformed into synthesized speech in the output language.
- the chimaera HMM performs voice conversion that causes the output voice in the output language to sound like the voice of the input speaker.
- auxiliary HMM and the output HMM of example embodiments can both be constructed and trained independently in their respective languages, proven techniques together with extensive single-language, standard-voice recordings can be used to configure both HMMs.
- This convenience of HMM configuration applies for any pair of languages for which a S2S system may be constructed, and is one of the aspects of the example embodiments that makes them highly scalable.
- the incorporation of robust, independently-constructed, single-language HMMs in the example embodiments is enabled by the introduction of the chimaera HMM for voice conversion, and the matching under transform technique that makes high-quality voice conversion mapping practical to implement.
- a chimaera HMM can be automatically constructed offline for each pair of input-output languages, yielding a further efficiency gain.
- a relatively small number of initial voice recordings may be used to compute an input-speaker adaptation transform that can be reused any time the input speaker subsequently invokes the S2S system.
- a S2S system may include one or more processors, one or more forms of memory, one or more input devices/interfaces, one or more output devices/interfaces, and machine-readable instructions that when executed by the one or more processors cause the S2S system to carry out the various functions and tasks described herein.
- the S2S system may also include implementations based on one or more hidden Markov models.
- the S2S system may employ methods that incorporate HMM-based speech recognition, HMM-based speech synthesis, and HMM-based voice conversion, as well as other possible components. Two examples of such a method are described in the current section.
- FIG. 1 is a flowchart illustrating an example method in accordance with example embodiments.
- an output HMM based speech features generator implemented by one or more processors of the system, is trained using speech signals of an output speaker speaking an output language.
- Speech features generated by the output HMM-based speech features generator may be used for synthesis of speech in the output language. More particularly, speech features generally include quantitative measures of acoustic properties of speech that may be processed, for example by a speech synthesizer apparatus, to produce synthesized speech.
- the output HMM based speech features generator may include a first configuration of output HMM state models, each of which has a set of generator-model functions.
- the output HMM state models are used to model states, and transitions between states, of phonetic units of the output language.
- the set of generator-model functions for each output HMM state model specifies how speech features corresponding to the modeled phonetic unit of the output language are generated.
- the first configuration of output HMM state models is structured for modeling speech in the output language.
- an auxiliary hidden HMM based speech features generator implemented by one or more processors of the system, is trained using speech signals of an auxiliary speaker speaking an input language.
- Speech features generated by the auxiliary HMM-based speech features generator may be used for synthesis of speech in the input language.
- the auxiliary HMM based speech features generator may include a second configuration of auxiliary HMM state models, each of which has a set of generator-model functions.
- the auxiliary HMM state models are used to model states, and transitions between states, of phonetic units of the input language.
- the set of generator-model functions for each auxiliary HMM state model specifies how speech features corresponding to the modeled phonetic unit of the input language are generated.
- the second configuration of auxiliary HMM state models is structured for modeling speech in the input language.
- a procedure for matching generator-model functions of the output HMM state models and generator-model functions of the auxiliary HMM state models is carried out. Specifically, for each given output HMM state model of the first configuration, a particular set of generator-model functions from among the auxiliary HMM state models of the second configuration that most closely matches the set of generator-model functions of the given output HMM is determined.
- the matching is determined using a procedure that simultaneously applies a parametric, transformation-based mapping from an analytic space of the output HMM state models to an analytic space of the auxiliary HMM state models, and a nonparametric, probabilistic-associative mapping from the analytic space of the auxiliary HMM state models to the analytic space of the output HMM state models.
- MUT matching under transform
- the matching procedure can compensate for differences between speech (e.g., voice) of the output speaker and speech (e.g., voice) of the auxiliary speaker.
- a fundamental frequency (F0) transform that speech-adapts F0 statistics of the output HMM based speech features generator to match F0 statistics of the auxiliary HMM based speech features generator is determined.
- a duration transform that speech-adapts duration statistics of the output HMM based speech features generator to match duration statistics of the auxiliary HMM based speech features generator is determined.
- a “chimaera” HMM based speech features generator is constructed for mapping between the auxiliary HMM based speech features generator and the output HMM based speech features generator. More particularly, the chimaera HMM based speech features generator, also implemented by one or more processors of the system, is constructed to initially be the same as the output HMM based speech features generator. Thus, the chimaera HMM base speech features generator initially has the output HMM state models and generator-model functions of the output HMM based speech features generator. Then, the set of generator-model functions of each output HMM state model of the chimaera HMM based speech features generator is replaced with the particular most closely matching set of generator-model functions, as determined at step 106 .
- the F0 statistics of the chimaera HMM based speech features generator are speech-adapted using the F0 transform determined at step 108
- the duration statistics of the chimaera HMM based speech features generator are speech-adapted using the duration transform duration transform determined at step 110 .
- the output HMM based speech features generator, the auxiliary HMM based speech features generator, and the chimaera HMM based speech features generator could all be implemented by at least one common processor from among the one or more processors of the system.
- all three HMM based speech features generators could be implemented by a single, common processor.
- one or more of the three could be implemented in a distributed fashion, such that all three share at least one common processor.
- one or more of the three could be implemented such that it shares no processor with the others.
- Other implementation of the three HMM based speech features generators among configurations of the one or more processors of the system are possible as well.
- the S2S system may be configured for run-time voice conversion of an input speaker by further transformation and adaptation of the chimaera HMM based speech features generator.
- speech signals of an input speaker speaking the input language may be used to determine speech-adaptation transforms that speech-adapt the auxiliary HMM based speech features generator to the input speaker.
- one or more HMM-adaptation techniques may be used to compute a transform that adapts the auxiliary HMM based speech features generator to the voice recordings of the input speaker.
- the techniques could include maximum likelihood linear regression, constrained maximum likelihood linear regression, and corresponding techniques for HMM-based speech synthesis.
- the S2S system may be used to carry out run-time voice conversion of the input speaker. More particularly, operating in a run-time mode, the S2S system could receive run-time input speech signals of the input speaker speaking the input language. This could be live, run-time speech of the input speaker, or recorded speech of the input speaker received at run-time. In a manner described below, the S2S system could generate an enriched transcription in the output language of the received run-time input speech signals, in preparation for synthesis of a translation of the input speech into the output language. As used herein, an enriched transcription is a symbolic representation of the phonetic and linguistic content of written text or other symbolic form of speech.
- each label identifying a phonetic speech unit, such as a phoneme, and further identifying or encoding linguistic and/or syntactic context, temporal parameters, and other information for specifying how to render the symbolically-represented sounds as meaningful speech in a given language.
- the speech-adaptation transforms determined for the auxiliary HMM based speech features generator could then be applied to speech-adapt the output HMM state models of the chimaera HMM based speech features generator, thereby producing a speech-adapted chimaera HMM based speech features generator adapted to the input speaker's voice.
- the speech-adapted chimaera HMM based speech features generator could be used to convert the enriched transcription into corresponding output speech features in the output language.
- a spoken utterance of the enriched transcription in the output language could be synthesized using the output speech features.
- generating the enriched transcription in the output language of the received run-time input speech signals could entail elements of speech-to-text conversion (e.g., ASR functionality), language translation, and text-to-speech conversion.
- a speech recognition subsystem (or component) of the S2S system can be used to convert the received run-time input speech signals into text in the input language.
- a language translation subsystem (or component) could next translate the text from the input language to the output language.
- the translated text could then be converted into an enriched transcription in the output language as an initial operation of a TTS subsystem (or component) of the S2S system.
- the enriched transcription in the output language could include a concatenation of phonetic units of the output language, as well as indicia of linguistic context, and time durations, for example.
- the chimaera HMM based speech features generator can be used to model speech features corresponding to the translated text.
- the modeled speech features can serve as input to a speech synthesizer, such a vocoder, in order to generate synthesized speech of the translated text.
- a speech synthesizer such as a vocoder
- the synthesized output voice is made to sound like that of the input speaker. The effect is to translate the input speaker's speech in the input language to speech in the output language spoken as if with the input speaker's voice.
- the input and output languages may be different. However, this need not be the case for all embodiments, and intra-lingual voice conversion, where the input and output languages are the same, can have useful applications as well.
- the set of generator-model functions for each given output HMM state model could include a multivariate spectral probability density function (PDF) for jointly modeling spectral envelope parameters of a phonetic unit of the output language modeled by a given output HMM state model, and a multivariate excitation PDF for jointly modeling excitation parameters of the phonetic unit of the output language.
- the set of generator-model functions for each given auxiliary HMM state model could include a multivariate spectral PDF for jointly modeling spectral envelope parameters of a phonetic unit of the input language modeled by the given auxiliary HMM state model, and a multivariate excitation PDF for jointly modeling excitation parameters of the phonetic unit of the input language.
- phonetic speech units could phonemes and/or triphones.
- the matching procedure of step 106 may be described in terms of finding pairs of multivariate PDFs which, in some sense, differ minimally. More specifically, making a determination of the set of generator-model functions of the auxiliary HMM state models of the second configuration that most closely matches the set of generator-model functions of a given output HMM could entail determining a multivariate spectral PDF from among the auxiliary HMM state models that is computationally nearest to the multivariate spectral PDF of the given output HMM state model in terms of a distance criterion that could be based on mean-squared-error (mse) or the Kullback-Leibler distance. The distance criterion that could be based on other metrics as well.
- mse mean-squared-error
- Kullback-Leibler distance The distance criterion that could be based on other metrics as well.
- Making the determination could additionally entail determining a multivariate excitation PDF from among the auxiliary HMM state models that is computationally nearest to the multivariate excitation PDF of the given output HMM state model in terms of a distance criterion that could be based on mse or the Kullback-Leibler distance.
- determining the set of generator-model functions of the auxiliary HMM state models of the second configuration that most closely matches the set of generator-model functions of a given output HMM could entail making a determination an optimal correspondence between a multivariate PDF of the given output HMM and a particular multivariate PDF from among the auxiliary HMM state models.
- the determination could made under a transform that compensates for differences between speech of the output speaker and speech of the auxiliary speaker. That is, a matching under transform technique could be used to make an optimal matching determination.
- the multivariate spectral PDF of each output HMM state model could have the mathematical form of a multivariate Gaussian function.
- the multivariate spectral PDF of each auxiliary HMM state model could also have the mathematical form of a multivariate Gaussian function. While generator-model functions of HMMs can take the form of Gaussian PDFs, this is not necessarily a requirement.
- the spectral envelope parameters of the phonetic units of the output language could be Mel Cepstral coefficients, Line Spectral Pairs, Linear Predictive coefficients, Mel-Generalized Cepstral Coefficients, or other acoustic-related quantities.
- the spectral envelope parameters of the phonetic units of the output language could also include first and second time derivatives of the acoustic-related quantities of the output language.
- the spectral envelope parameters of the phonetic units of the input language could also be Mel Cepstral coefficients, Line Spectral Pairs, Linear Predictive coefficients, Mel-Generalized Cepstral Coefficients, or other acoustic-related quantities.
- the spectral envelope parameters of the phonetic units of the input language could also include first and second time derivatives of the acoustic-related quantities of the input language.
- construction of the chimaera HMM based speech features generator at step 112 could entail transforming the output HMM based speech features generator into the chimaera HMM based speech features generator. More particularly, each output HMM state model of the output HMM based speech features generator could be replaced with the particular most closely matching set of generator-model functions, as determined at step 106 . The F0 statistics and the duration statistics of the transformed output HMM based speech features generator could then be speech-adapted using the F0 transform and duration transform, as determined as steps 108 and 110 , respectively. In this approach, the chimaera HMM based speech features generator can be viewed as being constructed “in place” from the output HMM based speech features generator.
- FIG. 2 is a flowchart illustrating an example an alternative method in accordance with example embodiments.
- an output HMM based speech features generator is implemented by one or more processors of a system.
- the implemented output HMM based speech features generator can include a first configuration of output HMM state models, and each of the output HMM state models may have a set of generator-model functions. Further, the implemented output HMM based speech features generator is trained using speech signals of an output speaker speaking an output language.
- an auxiliary HMM based speech features generator is implemented by one or more processors of the system.
- the implemented auxiliary HMM based speech features generator can include a second configuration of auxiliary HMM state models, and each of the auxiliary HMM state models may have a set of generator-model functions. Further, the implemented auxiliary HMM based speech features generator is trained using speech signals of an auxiliary speaker speaking an input language.
- a chimaera HMM based speech features generator is implemented that is the same as the output HMM based speech features generator, but with some specific differences.
- the set of generator-model functions of each given output HMM state model of the chimaera HMM based speech features generator is replaced with a particular set of generator-model functions from among the auxiliary HMM state models of the second configuration that most closely matches the set of generator-model functions of the given output HMM.
- fundamental frequency (F0) statistics of the chimaera HMM based speech features generator are speech-adapted using an F0 transform that speech-adapts F0 statistics of the output HMM based speech features generator to match F0 statistics of the auxiliary HMM based speech features generator.
- duration statistics of the chimaera HMM based speech features generator are speech-adapted using a duration transform that speech-adapts duration statistics of the output HMM based speech features generator to match duration statistics of the auxiliary HMM based speech features generator.
- speech signals of an input speaker speaking the input language may be used to determine speech-adaptation transforms that speech-adapt the auxiliary HMM based speech features generator to the input speaker.
- the S2S system could receive run-time input speech signals of the input speaker speaking the input language. This could be live, run-time speech of the input speaker, or recorded speech of the input speaker received at run-time. The input speech could be received at an input device of the system.
- the S2S could convert the received run-time input speech signals into an enriched transcription in the output language, in preparation for synthesis of a translation of the input speech into the output language.
- the speech-adaptation transforms determined at step 208 could be applied to speech-adapt the output HMM state models of the chimaera HMM based speech features generator, thereby producing a speech-adapted chimaera HMM based speech features generator adapted to the input speaker's voice.
- the speech-adapted chimaera HMM based speech features generator could be used to generate voice-converted speech features in the output language from the enriched transcription produced at step 212 .
- a spoken utterance of the enriched transcription in the output language could be synthesized using the voice-converted speech features.
- FIGS. 1 and 2 are meant to illustrate a methods in accordance with example embodiments. As such, various steps could be altered or modified, the ordering of certain steps could be changed, and additional steps could be added, while still achieving the overall desired operation.
- client devices such as mobile phones and tablet computers
- client services are able to communicate, via a network such as the Internet, with the server devices.
- applications that operate on the client devices may also have a persistent, server-based component. Nonetheless, it should be noted that at least some of the methods, processes, and techniques disclosed herein may be able to operate entirely on a client device or a server device.
- This section describes general system and device architectures for such client devices and server devices.
- the methods, devices, and systems presented in the subsequent sections may operate under different paradigms as well.
- the embodiments of this section are merely examples of how these methods, devices, and systems can be enabled.
- FIG. 3 is a simplified block diagram of a communication system 300 , in which various embodiments described herein can be employed.
- Communication system 300 includes client devices 302 , 304 , and 306 , which represent a desktop personal computer (PC), a tablet computer, and a mobile phone, respectively.
- Client devices could also include wearable computing devices, such as head-mounted displays and/or augmented reality displays, for example.
- Each of these client devices may be able to communicate with other devices (including with each other) via a network 308 through the use of wireline connections (designated by solid lines) and/or wireless connections (designated by dashed lines).
- Network 308 may be, for example, the Internet, or some other form of public or private Internet Protocol (IP) network.
- IP Internet Protocol
- client devices 302 , 304 , and 306 may communicate using packet-switching technologies. Nonetheless, network 308 may also incorporate at least some circuit-switching technologies, and client devices 302 , 304 , and 306 may communicate via circuit switching alternatively or in addition to packet switching.
- a server device 310 may also communicate via network 308 .
- server device 310 may communicate with client devices 302 , 304 , and 306 according to one or more network protocols and/or application-level protocols to facilitate the use of network-based or cloud-based computing on these client devices.
- Server device 310 may include integrated data storage (e.g., memory, disk drives, etc.) and may also be able to access a separate server data storage 312 .
- Communication between server device 310 and server data storage 312 may be direct, via network 308 , or both direct and via network 308 as illustrated in FIG. 3 .
- Server data storage 312 may store application data that is used to facilitate the operations of applications performed by client devices 302 , 304 , and 306 and server device 310 .
- communication system 300 may include any number of each of these components.
- communication system 300 may comprise millions of client devices, thousands of server devices and/or thousands of server data storages.
- client devices may take on forms other than those in FIG. 3 .
- FIG. 4A is a block diagram of a server device in accordance with an example embodiment.
- server device 400 shown in FIG. 4A can be configured to perform one or more functions of server device 310 and/or server data storage 312 .
- Server device 400 may include a user interface 402 , a communication interface 404 , processor 406 , and data storage 408 , all of which may be linked together via a system bus, network, or other connection mechanism 414 .
- User interface 402 may comprise user input devices such as a keyboard, a keypad, a touch screen, a computer mouse, a track ball, a joystick, and/or other similar devices, now known or later developed.
- User interface 402 may also comprise user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCD), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, now known or later developed.
- user interface 402 may be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices, now known or later developed.
- user interface 402 may include software, circuitry, or another form of logic that can transmit data to and/or receive data from external user input/output devices.
- Communication interface 404 may include one or more wireless interfaces and/or wireline interfaces that are configurable to communicate via a network, such as network 308 shown in FIG. 3 .
- the wireless interfaces may include one or more wireless transceivers, such as a BLUETOOTH® transceiver, a Wifi transceiver perhaps operating in accordance with an IEEE 802.11 standard (e.g., 802.11b, 802.11g, 802.11n), a WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard, a Long-Term Evolution (LTE) transceiver perhaps operating in accordance with a 3rd Generation Partnership Project (3GPP) standard, and/or other types of wireless transceivers configurable to communicate via local-area or wide-area wireless networks.
- a BLUETOOTH® transceiver e.g., 802.11b, 802.11g, 802.11n
- WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard
- the wireline interfaces may include one or more wireline transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link or other physical connection to a wireline device or network.
- wireline transceivers such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link or other physical connection to a wireline device or network.
- USB Universal Serial Bus
- communication interface 404 may be configured to provide reliable, secured, and/or authenticated communications.
- information for ensuring reliable communications e.g., guaranteed message delivery
- a message header and/or footer e.g., packet/message sequencing information, encapsulation header(s) and/or footer(s), size/time information, and transmission verification information such as cyclic redundancy check (CRC) and/or parity check values.
- CRC cyclic redundancy check
- Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, the data encryption standard (DES), the advanced encryption standard (AES), the Rivest, Shamir, and Adleman (RSA) algorithm, the Diffie-Hellman algorithm, and/or the Digital Signature Algorithm (DSA).
- DES data encryption standard
- AES advanced encryption standard
- RSA Rivest, Shamir, and Adleman
- Diffie-Hellman algorithm Diffie-Hellman algorithm
- DSA Digital Signature Algorithm
- Other cryptographic protocols and/or algorithms may be used instead of or in addition to those listed herein to secure (and then decrypt/decode) communications.
- Processor 406 may include one or more general purpose processors (e.g., microprocessors) and/or one or more special purpose processors (e.g., digital signal processors (DSPs), graphical processing units (GPUs), floating point processing units (FPUs), network processors, or application specific integrated circuits (ASICs)).
- DSPs digital signal processors
- GPUs graphical processing units
- FPUs floating point processing units
- ASICs application specific integrated circuits
- Processor 406 may be configured to execute computer-readable program instructions 410 that are contained in data storage 408 , and/or other instructions, to carry out various functions described herein.
- Data storage 408 may include one or more non-transitory computer-readable storage media that can be read or accessed by processor 406 .
- the one or more computer-readable storage media may include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with processor 406 .
- data storage 408 may be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments, data storage 408 may be implemented using two or more physical devices.
- Data storage 408 may also include program data 412 that can be used by processor 406 to carry out functions described herein.
- data storage 408 may include, or have access to, additional data storage components or devices (e.g., cluster data storages described below).
- server device 310 and server data storage device 312 may store applications and application data at one or more locales accessible via network 308 . These locales may be data centers containing numerous servers and storage devices. The exact physical location, connectivity, and configuration of server device 310 and server data storage device 312 may be unknown and/or unimportant to client devices. Accordingly, server device 310 and server data storage device 312 may be referred to as “cloud-based” devices that are housed at various remote locations.
- cloud-based One possible advantage of such “cloud-based” computing is to offload processing and data storage from client devices, thereby simplifying the design and requirements of these client devices.
- server device 310 and server data storage device 312 may be a single computing device residing in a single data center.
- server device 310 and server data storage device 312 may include multiple computing devices in a data center, or even multiple computing devices in multiple data centers, where the data centers are located in diverse geographic locations.
- FIG. 3 depicts each of server device 310 and server data storage device 312 potentially residing in a different physical location.
- FIG. 4B depicts an example of a cloud-based server cluster.
- functions of server device 310 and server data storage device 312 may be distributed among three server clusters 420 A, 420 B, and 420 C.
- Server cluster 420 A may include one or more server devices 400 A, cluster data storage 422 A, and cluster routers 424 A connected by a local cluster network 426 A.
- server cluster 420 B may include one or more server devices 400 B, cluster data storage 422 B, and cluster routers 424 B connected by a local cluster network 426 B.
- server cluster 420 C may include one or more server devices 300 C, cluster data storage 422 C, and cluster routers 424 C connected by a local cluster network 426 C.
- Server clusters 420 A, 420 B, and 420 C may communicate with network 308 via communication links 428 A, 428 B, and 428 C, respectively.
- each of the server clusters 420 A, 420 B, and 420 C may have an equal number of server devices, an equal number of cluster data storages, and an equal number of cluster routers. In other embodiments, however, some or all of the server clusters 420 A, 420 B, and 420 C may have different numbers of server devices, different numbers of cluster data storages, and/or different numbers of cluster routers. The number of server devices, cluster data storages, and cluster routers in each server cluster may depend on the computing task(s) and/or applications assigned to each server cluster.
- server devices 400 A can be configured to perform various computing tasks of a server, such as server device 310 . In one embodiment, these computing tasks can be distributed among one or more of server devices 400 A.
- Server devices 400 B and 400 C in server clusters 420 B and 420 C may be configured the same or similarly to server devices 400 A in server cluster 420 A.
- server devices 400 A, 400 B, and 400 C each may be configured to perform different functions.
- server devices 400 A may be configured to perform one or more functions of server device 310
- server devices 400 B and server device 400 C may be configured to perform functions of one or more other server devices.
- the functions of server data storage device 312 can be dedicated to a single server cluster, or spread across multiple server clusters.
- Cluster data storages 422 A, 422 B, and 422 C of the server clusters 420 A, 420 B, and 320 C, respectively, may be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives.
- the disk array controllers alone or in conjunction with their respective server devices, may also be configured to manage backup or redundant copies of the data stored in cluster data storages to protect against disk drive failures or other types of failures that prevent one or more server devices from accessing one or more cluster data storages.
- server device 310 and server data storage device 312 can be distributed across server clusters 420 A, 420 B, and 420 C
- various active portions and/or backup/redundant portions of these components can be distributed across cluster data storages 422 A, 422 B, and 422 C.
- some cluster data storages 422 A, 422 B, and 422 C may be configured to store backup versions of data stored in other cluster data storages 422 A, 422 B, and 422 C.
- Cluster routers 424 A, 424 B, and 424 C in server clusters 420 A, 420 B, and 420 C, respectively, may include networking equipment configured to provide internal and external communications for the server clusters.
- cluster routers 424 A in server cluster 420 A may include one or more packet-switching and/or routing devices configured to provide (i) network communications between server devices 400 A and cluster data storage 422 A via cluster network 426 A, and/or (ii) network communications between the server cluster 420 A and other devices via communication link 428 A to network 408 .
- Cluster routers 424 B and 424 C may include network equipment similar to cluster routers 424 A, and cluster routers 424 B and 424 C may perform networking functions for server clusters 420 B and 420 C that cluster routers 424 A perform for server cluster 420 A.
- the configuration of cluster routers 424 A, 424 B, and 424 C can be based at least in part on the data communication requirements of the server devices and cluster storage arrays, the data communications capabilities of the network equipment in the cluster routers 424 A, 424 B, and 424 C, the latency and throughput of the local cluster networks 426 A, 426 B, 426 C, the latency, throughput, and cost of the wide area network connections 428 A, 428 B, and 428 C, and/or other factors that may contribute to the cost, speed, fault-tolerance, resiliency, efficiency and/or other design goals of the system architecture.
- FIG. 5 is a simplified block diagram showing some of the components of an example client device 500 .
- client device 500 may be or include a “plain old telephone system” (POTS) telephone, a cellular mobile telephone, a still camera, a video camera, a fax machine, an answering machine, a computer (such as a desktop, notebook, or tablet computer), a personal digital assistant, a wearable computing device, a home automation component, a digital video recorder (DVR), a digital TV, a remote control, or some other type of device equipped with one or more wireless or wired communication interfaces.
- POTS plain old telephone system
- client device 500 may include a communication interface 502 , a user interface 504 , a processor 506 , and data storage 508 , all of which may be communicatively linked together by a system bus, network, or other connection mechanism 510 .
- Communication interface 502 functions to allow client device 500 to communicate, using analog or digital modulation, with other devices, access networks, and/or transport networks.
- communication interface 502 may facilitate circuit-switched and/or packet-switched communication, such as POTS communication and/or IP or other packetized communication.
- communication interface 502 may include a chipset and antenna arranged for wireless communication with a radio access network or an access point.
- communication interface 502 may take the form of a wireline interface, such as an Ethernet, Token Ring, or USB port.
- Communication interface 502 may also take the form of a wireless interface, such as a Wifi, BLUETOOTH®, global positioning system (GPS), or wide-area wireless interface (e.g., WiMAX or LTE).
- communication interface 502 may comprise multiple physical communication interfaces (e.g., a Wifi interface, a BLUETOOTH® interface, and a wide-area wireless interface).
- User interface 504 may function to allow client device 500 to interact with a human or non-human user, such as to receive input from a user and to provide output to the user.
- user interface 504 may include input components such as a keypad, keyboard, touch-sensitive or presence-sensitive panel, computer mouse, trackball, joystick, microphone, still camera and/or video camera.
- User interface 504 may also include one or more output components such as a display screen (which, for example, may be combined with a touch-sensitive panel), CRT, LCD, LED, a display using DLP technology, printer, light bulb, and/or other similar devices, now known or later developed.
- User interface 504 may also be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices, now known or later developed.
- user interface 504 may include software, circuitry, or another form of logic that can transmit data to and/or receive data from external user input/output devices.
- client device 500 may support remote access from another device, via communication interface 502 or via another physical interface (not shown).
- Processor 506 may comprise one or more general purpose processors (e.g., microprocessors) and/or one or more special purpose processors (e.g., DSPs, GPUs, FPUs, network processors, or ASICs).
- Data storage 508 may include one or more volatile and/or non-volatile storage components, such as magnetic, optical, flash, or organic storage, and may be integrated in whole or in part with processor 506 .
- Data storage 508 may include removable and/or non-removable components.
- processor 506 may be capable of executing program instructions 518 (e.g., compiled or non-compiled program logic and/or machine code) stored in data storage 508 to carry out the various functions described herein.
- Data storage 508 may include a non-transitory computer-readable medium, having stored thereon program instructions that, upon execution by client device 500 , cause client device 500 to carry out any of the methods, processes, or functions disclosed in this specification and/or the accompanying drawings.
- the execution of program instructions 518 by processor 506 may result in processor 506 using data 512 .
- program instructions 518 may include an operating system 522 (e.g., an operating system kernel, device driver(s), and/or other modules) and one or more application programs 520 (e.g., address book, email, web browsing, social networking, and/or gaming applications) installed on client device 500 .
- data 512 may include operating system data 516 and application data 514 .
- Operating system data 516 may be accessible primarily to operating system 522
- application data 514 may be accessible primarily to one or more of application programs 520 .
- Application data 514 may be arranged in a file system that is visible to or hidden from a user of client device 500 .
- Application programs 520 may communicate with operating system 512 through one or more application programming interfaces (APIs). These APIs may facilitate, for instance, application programs 520 reading and/or writing application data 514 , transmitting or receiving information via communication interface 502 , receiving or displaying information on user interface 504 , and so on.
- APIs application programming interfaces
- application programs 520 may be referred to as “apps” for short. Additionally, application programs 520 may be downloadable to client device 500 through one or more online application stores or application markets. However, application programs can also be installed on client device 500 in other ways, such as via a web browser or through a physical interface (e.g., a USB port) on client device 500 .
- FIG. 6 an example cross-lingual speech-to-speech (S2S) system 600 , in accordance with an example embodiment.
- S2S speech-to-speech
- FIG. 6 also shows selected example inputs, outputs, and intermediate products of example operation.
- the functional components of the S2S system 600 include an ASR subsystem 602 for recognition of speech in an input language, a language translation subsystem 604 for translating the input speech into an output language, and a TTS subsystem 606 for outputting speech in the output language.
- These functional components could be implemented as machine-language instructions in a centralized and/or distributed fashion on one or more computing platforms or systems, such as those described above.
- the machine-language instructions could be stored in one or another form of a tangible, non-transitory computer-readable medium (or other article of manufacture), such as magnetic or optical disk, or the like, and made available to processing elements of the system as part of a manufacturing procedure, configuration procedure, and/or execution start-up procedure, for example.
- a tangible, non-transitory computer-readable medium such as magnetic or optical disk, or the like
- the ASR subsystem 602 may be employ HMM-based speech recognition in the input language. This is illustrated in FIG. 6 by a symbolic depiction of an input HMM in the ASR subsystem 602 .
- the input HMM is represented by a configuration of speech-unit HMMs, each corresponding to a phonetic speech unit of the input language.
- the phonetic units could be phonemes or triphones, for example.
- Each speech-unit HMM is drawn as a set of circles, each circle representing a state of the speech unit, and arrows connecting the circles, each arrow representing a state transition.
- a circular arrow at each state represents a self-transition.
- Above each circle is a symbolic representation of a PDF.
- the PDF specifies the probability that a given state will “emit” or generate speech features corresponding to a phase of the speech unit modeled by the state.
- the depiction in the figure of three states per speech-unit HMM is consistent with some HMM techniques that model three states for each speech unit.
- HMM techniques using different numbers of states per speech units may be employed as well, and the illustrative use of three states in FIG. 6 (as well as in other figures herein) is not intended to be limiting with respect to example embodiments described herein. Further details of an example ASR are described below.
- an input utterance 601 in the input language and spoken by an input speaker may be input to the ASR subsystem 602 .
- the input utterance 601 could be a spoken word, phrase, sentence, or other construct of speech spoken in the input language. It could be spoken in real-time by the input speaker, or input from a recording of the input speaker.
- the ASR subsystem 602 may process the input utterance 602 in order to produce output text 603 in the input language. That is, the output text 603 can be a conversion of the input utterance 601 into a textual form, still in the input language.
- the output text 603 may then serve as input to the language translation subsystem 604 , which may use one or another technique to translate the text in the input language to corresponding translated text 605 in the output language.
- the language translation subsystem 604 could use statistical translation, for example.
- the translated text 605 may then be input into the TTS subsystem 606 , which then processes the input to generate a translated utterance 607 as synthesized speech in the output language.
- the TTS subsystem 606 is also based on HMM technology, which is represented pictorially by an output HMM in a manner similar to that described above for the input HMM of the ASR subsystem 602 .
- the S2S system 600 receives the input utterance 601 in the input language and converts it into the output utterance 607 translated into the output language.
- the input utterance 601 is spoken in the voice of the input speaker, and the output utterance 607 is spoken in the voice of an output speaker.
- the output speaker's voice may depend on how the TTS subsystem 606 is configured (e.g., trained). In practice, the input and output speakers may be different.
- the spoken input utterance 601 may be converted into a translated output utterance 607 , and the output utterance 607 may be spoken in a different voice that of the input speaker.
- voice of the input speaker may be considered as having been technically “converted” to the voice of the output speaker, the practical result may seem more like “replacement” of the input speaker's voice with the output speaker's voice.
- This possibility is represented by the dashed arrow labeled “voice ‘replacement’” from the input utterance 601 to the TTS subsystem 606 .
- Voice conversion techniques described herein may be used to make the output voice sound like the input voice, even when the input utterance is translated to the output language prior to output speech synthesis.
- An ASR apparatus may operate by receiving an input audio signal containing a spoken utterance in an input language, processing the audio input signal (e.g., using a digital signal processor) to generate a quantified representation of the signal in the form of “features,” and then using the features as observations to model a probable sequence of phonetic speech units of the input language that make up the input utterance. Identifying labels of the phonetic speech unit of the sequence can then be used to derive written text corresponding to the input utterance, thereby rendering the input spoken utterance as a text string.
- features can be considered metrics of an input speech signal that characterize its acoustic contents.
- Generation of the features is referred to as “feature extraction,” and typically involves sampling and quantizing an input speech utterance within sequential temporal frames, and performing spectral analysis of the data in the frames to derive a vector of features associated with each frame. Each feature vector thus provides a snapshot of the temporal evolution of the speech utterance.
- the features may include Mel Filter Cepstral (MFC) coefficients.
- MFC coefficients may represent the short-term power spectrum of a portion of an input utterance, and may be based on, for example, a linear cosine transform of a log power spectrum on a nonlinear Mel scale of frequency. (A Mel scale may be a scale of pitches subjectively perceived by listeners to be about equally distant from one another, even though the actual frequencies of these pitches are not equally distant from one another.)
- one or more frames of an input utterance may be represented by a feature vector of MFC coefficients, first-order cepstral coefficient derivatives, and second-order cepstral coefficient derivatives.
- the feature vector may contain 13 coefficients, 13 first-order derivatives (“delta”), and 13 second-order derivatives (“delta-delta”), therefore having a length of 39.
- feature vectors may use different combinations of features in other possible embodiments.
- feature vectors could include Perceptual Linear Predictive (PLP) coefficients, Relative Spectral (RASTA) coefficients, Filterbank log-energy coefficients, or some combination thereof.
- PLP Perceptual Linear Predictive
- RASTA Relative Spectral
- Filterbank log-energy coefficients or some combination thereof.
- Each feature vector may be thought of as including a quantified characterization of the acoustic content of a corresponding temporal frame of the utterance (or more generally of an audio input signal).
- the phonetic content of a spoken utterance can be represented as a sequence of phonetic speech units.
- the phonetic speech units could be phonemes.
- a phoneme may be considered to be the smallest segment of speech of given language that encompasses a meaningful contrast with other speech segments of the given language.
- a word typically includes one or more phonemes.
- phonemes may be thought of as utterances of letters, although this is not a perfect analogy, as some phonemes may present multiple letters.
- the phonemic spelling for the American English pronunciation of the word “cat” is /k/ /ae/ /t/, and consists of the phonemes /k/, /ae/, and /t/.
- phonemic spelling for the word “dog” is /d/ /aw/ /g/, consisting of the phonemes /d/, /aw/, and /g/.
- Different phonemic alphabets exist, and other phonemic representations are possible. Common phonemic alphabets for American English contain about 40 distinct phonemes. Other languages may be described by different phonemic alphabets containing different phonemes.
- the phonetic properties of a phoneme in an utterance can depend on, or be influenced by, the context in which it spoken.
- a “triphone” is a triplet of phonemes in which the spoken rendering of a given phoneme is shaped by a temporally-preceding phoneme, referred to as the “left context,” and a temporally-subsequent phoneme, referred to as the “right context.”
- the ordering of the phonemes of English-language triphones corresponds to the direction in which English is read.
- Other phoneme contexts, such as quinphones may be considered as well.
- HMM-based modeling can be used to recognize an observed sequence of feature vectors as corresponding in a probabilistic sense to a sequence of speech units (e.g., phonemes, triphones, words, etc.).
- each phonetic speech unit may be represented by a respective HMM that models temporal evolution as transitions between HMM states, and that models acoustic properties as probabilities of “emitting” or generating particular features while in a temporal phase of the speech unit modeled by the state.
- the sequence of phonetic speech units corresponding the computed sequence of HMMs and HMM states may be used to determine a phonetic transcription of the input speech utterance, from which the speech content recognized and converted to text.
- phonemes and/or triphones are represented by HMMs as having three states corresponding to three temporal phases, namely beginning, middle, and end.
- Other HMMs with a different number of states per phoneme could be used as well.
- emission probabilities associated with HMM states may be modeled by multivariate probability distribution functions (PDFs) for jointly modeling the different features that make up the feature vectors.
- PDFs multivariate probability distribution functions
- multivariate Gaussian PDFs can be used to compute probabilities of observing multiple dimensions of features from a given state of the model. Each dimension of a given multivariate Gaussian PDF could thus correspond to different feature. It is also possible to model a feature along a given dimension with more than one Gaussian PDF in that dimension. In such an approach, the feature is said to be modeled by a mixture of Gaussians, referred to a “Gaussian mixture model” or “GMM.”
- values of parameters of the multivariate PDFs could be determined by way of a training procedure. More particularly, the parameters could be “tuned” for a given language by inputting sample speech utterances for which corresponding text in the given language is known, and adjusting the parameters to achieve the known results.
- the sample speech utterances and corresponding text may be part of “corpus” or body of speech samples contained in a speech database.
- FIG. 7 depicts a block diagram of an example automatic speech recognition (ASR) system 700 , in accordance with an example embodiment.
- ASR automatic speech recognition
- FIG. 7 also shows selected example inputs, outputs, and intermediate products of example operation.
- the functional components of the speech synthesis system 700 include a feature extraction module 702 , a text prediction module 704 that includes a HMM module 706 , which in turn includes HMM parameters 708 , a speech database 710 , and a HMM training module 712 .
- These functional components could be implemented as machine-language instructions in a centralized and/or distributed fashion on one or more computing platforms or systems, such as those described above.
- the machine-language instructions could be stored in one or another form of a tangible, non-transitory computer-readable medium (or other article of manufacture), such as magnetic or optical disk, or the like, and made available to processing elements of the system as part of a manufacturing procedure, configuration procedure, and/or execution start-up procedure, for example.
- a tangible, non-transitory computer-readable medium such as magnetic or optical disk, or the like
- the figure is depicted in a way that represents two operational modes: training-time and run-time.
- a thick, horizontal line marks a conceptual boundary between these two modes, with “Training-Time” labeling a portion of FIG. 7 above the line, and “Run-Time” labeling a portion below the line.
- various arrows in the figure signifying information and/or processing flow and/or transmission are shown as dashed lines in the “Training-Time” portion of the figure, and as solid lines in the “Run-Time” portion.
- a training-time speech signal 701 from the speech database 708 may be input to the feature extraction module 702 , which then generates training-time feature vectors 705 .
- the training-time feature vectors 705 are then input to the text prediction module 704 , which generates a training-time predicted text string 707 according to the technique described generally above.
- the HMM module is used to compute a most likely sequence of phonemes or triphones (or other phonetic speech units) corresponding to the observed training-time feature vectors 705 .
- the sequence of phonetic units may then be used to construct a corresponding the training-time predicted text string 707 .
- the corpus of speech data may also include phonetic speech units and associated labels accessed by the text prediction module 704 in the process of predicting training-time text strings (including the training-time predicted text string 707 ).
- the training-time predicted text string 707 is then input to the HMM training module 712 .
- a training-time target text string 703 from the speech database 710 is also provided as input to the HMM training module 712 .
- the training-time target text string 703 is predetermined to correspond to the training-time speech signal 701 ; this is signified by a wavy, dashed double arrow between the training-time target text string 703 and the training-time speech signal 701 .
- the training-time speech signal 701 could be a speech recording of a speaker reading the training-time target text string 703 .
- the corpus of training data in the speech database 710 could include numerous recordings of one or more speakers reading numerous text strings.
- the HMM training module 712 can determine how to adjust the HMM parameters 708 so as to achieve closest or optimal agreement between the predicted result and the known result. While this conceptual illustration of HMM training may appear suggestive of a feedback loop for error reduction, the procedure could entail a maximum likelihood (ML) adjustment of the HMM parameters. This is indicated by the return of ML-adjusted HMM parameters 709 from the HMM training module 712 to the HMM parameters 708 . In practice, the training procedure may involve many iterations over many different speech samples and corresponding text strings in order to cover all (or most) of the phonetic speech units of the language of the ASR subsystem 700 with sufficient data to determine accurate parameter values.
- ML maximum likelihood
- a run-time speech signal 711 is input to the feature extraction module 702 , which then generates run-time feature vectors 713 .
- the run-time feature vectors 713 are then input to the text prediction module 704 , which generates a run-time predicted text string 715 , again according to the HMM-based technique.
- the run-time predicted text string 715 may have a high likelihood of being an accurate textual transcription of the run-time speech signal 711 .
- a TTS apparatus may operate by receiving an input text string, processing the text string into a symbolic representation of the phonetic and linguistic content of the text string, generating a sequence of speech features corresponding to the symbolic representation, and providing the speech features as input to a speech synthesizer in order to produce a spoken rendering of the input text string.
- the symbolic representation of the phonetic and linguistic content of the text string may take the form of a sequence of labels, each label identifying a phonetic speech unit, such as a phoneme, and further identifying or encoding linguistic and/or syntactic context, temporal parameters, and other information for specifying how to render the symbolically-represented sounds as meaningful speech in a given language.
- phonetic transcription is sometimes used to refer to such a symbolic representation of text
- enriched transcription will instead be used herein, in order to signify inclusion of extra-phonetic content, such as linguistic and/or syntactic context and temporal parameters, represented in the sequence of labels.
- TTS operation can be viewed conceptually as a reverse of ASR operation, though the analogy is not precise.
- speech features may be taken as a signature of acoustic properties of speech, although different specific type of features may be used for driving generation of a synthesized waveform corresponding to an output speech signal.
- features for speech synthesis account for three major components of speech signals, namely spectral envelopes that resemble the effect of the vocal tract, excitation that simulates the glottal source, and prosody that describes pitch contour (“melody”) and tempo (rhythm).
- the sequence of labels corresponding to enriched transcription of the input text may be treated as observed data, and a sequence of HMMs and HMM states is computed so as to maximize a joint probability of generating the observed enriched transcription.
- the labels of the enriched transcription sequence may identify phonemes, triphones, and/or other phonetic speech units.
- the enriched transcription may also include additional information about the input text string, such as time or duration models for the phonetic speech units, linguistic and/or syntactic context, and other indicators that may characterize how the output text string should sound when rendered as speech, for example.
- speech features corresponding to HMMs and HMM states may be represented by multivariate PDFs for jointly modeling the different features that make up the feature vectors.
- multivariate Gaussian PDFs can be used to compute probabilities of a given state emitting or generating multiple dimensions of features from a given state of the model. Each dimension of a given multivariate Gaussian PDF could thus correspond to different feature.
- the sequence of features generated by the most probable sequence of HMMs and HMM states can be converted to speech by a speech synthesizer, for example.
- Gaussian mixture models may be used for one more dimensions of the multivariate PDFs of a TTS system.
- the HMMs of a HMM-based TTS or speech synthesis subsystem may be trained by tuning the PDF parameters using a database of text recorded speech and corresponding known text strings.
- the training techniques may be similar to those used for an HMM-based ASR subsystem, although the specific speech parameters may differ, at least in part, from those used in ASR.
- FIG. 8 depicts a block diagram of an example text-to-speech speech system 800 , in accordance with an example embodiment.
- FIG. 8 also shows selected example inputs, outputs, and intermediate products of example operation.
- the functional components of the speech synthesis system 800 include a text analysis module 802 , a HMM module 804 that includes HMM parameters 806 , a speech synthesizer module 808 , a speech database 810 , a feature extraction module 812 , and a HMM training module 814 .
- These functional components could be implemented as machine-language instructions in a centralized and/or distributed fashion on one or more computing platforms or systems, such as those described above.
- the machine-language instructions could be stored in one or another form of a tangible, non-transitory computer-readable medium (or other article of manufacture), such as magnetic or optical disk, or the like, and made available to processing elements of the system as part of a manufacturing procedure, configuration procedure, and/or execution start-up procedure, for example.
- a tangible, non-transitory computer-readable medium such as magnetic or optical disk, or the like
- FIG. 8 is depicted in a way that represents two operational modes: training-time and run-time.
- a thick, horizontal line marks a conceptual boundary between these two modes, with “Training-Time” labeling a portion of FIG. 8 above the line, and “Run-Time” labeling a portion below the line.
- various arrows in the figure signifying information and/or processing flow and/or transmission are shown as dashed lines in the “Training-Time” portion of the figure, and as solid lines in the “Run-Time” portion.
- a training-time text string 801 from the speech database 810 may be input to the text analysis module 802 , which then generates training-time labels 805 (an enriched transcription of training-time text string 801 ).
- Each training-time label could be made up of a phonetic label identifying a phonetic speech unit (e.g., a phoneme), context information (e.g., one or more left-context and right-context phoneme labels, physical speech production characteristics, linguistic context, etc.), and timing information, such as a duration, relative timing position, and/or phonetic state model.
- the training-time labels 805 are then input to the HMM module 804 , which models training-time predicted spectral parameters 811 and training-time predicted excitation parameters 813 . These may be considered speech features that are generated by the HMM module according to state transition probabilities and state emission probabilities that make up (at least in part) the HMM parameters.
- the training-time predicted spectral parameters 811 and training-time predicted excitation parameters 813 are then input to the HMM training module 814 , as shown.
- a training-time speech signal 803 from the speech database 810 is input to the feature extraction module 812 , which processes the input signal to generate target spectral parameters 807 and target excitation parameters 809 .
- the training-time speech signal 803 is predetermined to correspond to the training-time text string 801 ; this is signified by a wavy, dashed double arrow between the training-time speech signal 803 and the training-time text string 801 .
- the training-time speech signal 801 could be a speech recording of a speaker reading the training-time text string 803 .
- the corpus of training data in the speech database 810 could include numerous recordings of one or more speakers reading numerous text strings.
- the target spectral parameters 807 and target excitation parameters 809 may be considered known parameters, since they are derived from a known speech signal.
- the target spectral parameters 807 and target excitation parameters 809 are provided as input to the HMM training module 814 .
- the HMM training module 814 can determine how to adjust the HMM parameters 806 so as to achieve closest or optimal agreement between the predicted results and the known results. While this conceptual illustration of HMM training may appear suggestive of a feedback loop for error reduction, the procedure could entail a maximum likelihood (ML) adjustment of the HMM parameters. This is indicated by the return of ML-adjusted HMM parameters 815 from the HMM training module 814 to the HMM parameters 806 .
- the training procedure may involve many iterations over many different speech samples and corresponding text strings in order to cover all (or most) of the phonetic speech units of the language of the TTS subsystem 800 with sufficient data to determine accurate parameter values.
- a run-time text string 817 is input to the text analysis module 802 , which then generates run-time labels 819 (an enriched transcription of run-time text string 817 ).
- the form of the run-time labels 819 may be the same as that for the training-time labels 805 .
- the run-time labels 819 are then input to the HMM module 804 , which generates run-time predicted spectral parameters 821 and run-time predicted excitation parameters 823 , again according to the HMM-based technique.
- run-time predicted spectral parameters 821 and run-time predicted excitation parameters 823 can generated in pairs, each pair corresponding to a predicted pair of feature vectors for generating a temporal frame of waveform data.
- the run-time predicted spectral parameters 821 and run-time predicted excitation parameters 823 may next be input to the speech synthesizer module 808 , which may then synthesize a run-time speech signal 825 .
- speech synthesize could include a vocoder that can translate the acoustic features of the input into an output waveform suitable for playout on an audio output device, and/or for analysis by a signal measuring device or element. Such a device or element could be based on signal measuring hardware and/or machine language instructions that implement an analysis algorithm.
- the run-time speech signal 825 may have a high likelihood of being an accurate speech rendering of the run-time text string 817 .
- the input HMM in the ASR subsystem 602 may be considered to be a configuration of HMM state models for modeling the input language.
- Multivariate Gaussian PDFs for modeling features of each input state may be determined by way of training. As such, they are constructed according to phonetics of the input language.
- the output HMM in the TTS subsystem 606 may be considered to be a configuration of HMM state models for modeling in the output language.
- Multivariate Gaussian PDFs for modeling features of each output state may also be determined by way of training. As such, they are constructed according to phonetics of the output language.
- the S2S system 600 may convert an input utterance 601 at run-time into a translated output utterance 607 spoken in a voice modeled by the output HMM.
- the output voice may be different than that of a run-time speaker, or even different than that of in input HMM training voice (or voices).
- the HMM used by the TTS subsystem (or, more generally, by an output speech modeling subsystem of a S2S system) may be configured so as to sound like the voice a run-time input speaker.
- the HMM of an output speech modeling subsystem of a S2S system may be configured using parameters and characteristics of an input HMM, and further adapted with characteristics of a run-time input speaker, while still retaining structure for modeling speech of the output language.
- FIG. 9A is a schematic illustration of a first aspect of configuring of a voice conversion system, in accordance with example embodiments.
- an auxiliary HMM 902 may be trained in an input language using recordings (or other speech waveforms) of an auxiliary speaker.
- the auxiliary HMM 902 can include a configuration of Q HMM state models 902 - 1 , 902 - 2 , 902 - 3 , . . . , 902 -Q, as shown.
- the auxiliary HMM 902 could be used, for example, as the input HMM of a S2S system, such as the illustrated in FIG. 6 .
- an output HMM 904 can be trained in an output language using recordings (or other speech waveforms) of an output speaker.
- the output HMM 904 can include a configuration of N HMM state models 904 - 1 , 904 - 2 , 904 - 3 , 904 - 4 , . . . , 904 -N, as shown.
- the output HMM 904 could be used, for example, as the output HMM of a S2S system, such as the illustrated in FIG. 6 .
- each HMM state model in FIG. 9A is represented pictorially as a sequence of three states (shown as circles) connected by arrows representing state transitions; each state has a self-transition represented by a circular arrow.
- a symbolic PDF is shown above each state. The particular forms of the PDFs, as well as the representation of three states per HMM state model is intended to be illustrative, and not necessarily limiting with respect to embodiments herein.
- the PDFs of the HMM state models of the auxiliary HMM 902 and the output HMM 904 may include multivariate Gaussian PDFs for jointly modeling spectral envelope parameters, and multivariate Gaussian PDFs for jointly modeling excitation parameters.
- this detail of the PDFs is not necessarily shown explicitly in the pictorial representation of the HMM states in FIG. 9A , it may be assumed in references to PDFs in the discussions below. It should also be noted, however, that other forms of PDFs may be used in statistical modeling of speech, including in other HMM-based techniques.
- the number of HMM state models in the auxiliary HMM 902 may be different from the number of HMM state models in the output HMM 904 . That is Q may not, in general, necessarily be equal to N. Further, the specific forms of the HMM state models of the auxiliary HMM 902 may differ from the specific forms of the HMM state models of the output HMM 904 .
- the input and output languages may have different phonemes, triphones, and/or other phonetic speech units. The number of states used to model phonetic speech units could also differ between the input and output languages.
- a matching procedure is carried out to determine a best match to each state model of the output HMM 904 from among the state models of the auxiliary HMM 902 .
- the matching procedure is indicated in descriptive text, enumerated as step 1 , in FIG. 9A , and illustrated conceptually in FIG. 9A by a respective forked arrow pointing from each of a respective example HMM state of the output HMM 904 to the HMM states of the auxiliary HMM 902 .
- each of three PDFs of the HMM state model 904 - 1 and one of the PDFs of the HMM state model 904 -N has a forked arrow pointing to the HMM state models of the auxiliary HMM 902 .
- Each of the four forked arrows shown symbolizes a form of search for a most closely matching PDF from among the HMM state models of the auxiliary HMM 902 .
- a similar search may be performed for all of the other HMM state models of the output HMM 904 .
- only four are depicted for the sake of brevity in the figure.
- the matching procedure is involves a matching under transform technique that compensates for differences between the auxiliary speaker and the output speaker.
- a cross-lingual HMM referred to herein as a “chimaera” HMM 906
- the chimaera HMM is initially just a copy of the output HMM.
- the chimaera HMM 906 thus includes a configuration of N HMM state models 906 - 1 , 906 - 2 , 906 - 3 , 906 - 4 , . . . , 906 -N, as shown. These are initially identical to the N HMM state models of the output HMM 904 .
- each of its state models is replaced by a particular state model from the auxiliary HMM 902 determined to be the closest match to the corresponding state model of the output HMM 904 .
- the replacement operation is indicated in descriptive text, enumerated as step 3, in FIG. 9A , and displayed pictorially by curved, dashed arrows from representative HMM states of the auxiliary HMM 902 to representative HMM states of the chimaera HMM 906 .
- a PDF of each of the HMM state models 902 - 1 , 902 - 2 , 902 - 3 , and 902 -Q is shown as being a replacement for PDFs of HMM state models 906 - 1 and 906 -N of the chimaera HMM 906 .
- a similar replacement may be performed for all of the other HMM state models of the chimaera HMM 906 .
- only four are explicitly shown for the sake of brevity in the figure.
- the chimaera HMM 906 is next speech adapted to other characteristics of the auxiliary speaker not necessarily represented in the PDFs of the HMM state models of the auxiliary HMM 902 . More specifically, an F0 transform may be computed that adapts statistics of F0 of the output HMM 904 to the F0 statistics of the auxiliary HMM 902 .
- the adaptation can be made to match up to first order statistics (means), second order statistics (means and variances), or higher (e.g., matching PDFs).
- the adaptation can be made directly on values of F0, or on the means of Gaussian states of the HMMs.
- a duration transform may be computed that adapts statistics of durations of the output HMM 904 to the duration statistics of the auxiliary HMM 902 .
- the adaptation can be made to match up to first order statistics (means), second order statistics (means and variances), or higher (e.g., matching PDFs).
- the adaptation can be made directly on values of duration, or on the means of Gaussian states of the HMMs, or on the parameters of other statistical models that may be used to model durations.
- the computed F0 transform can be applied to the chimaera HMM 906 to adapt the F0 statistics of the chimaera HMM 906 to the F0 statistics of the auxiliary HMM 902 .
- the means of the Gaussian states of the chimaera HMM 906 may be transformed in this way.
- computed duration transform can be applied to the chimaera HMM 906 to adapt the duration statistics of the chimaera HMM 906 to the duration statistics of the auxiliary HMM 902 .
- the durations of the chimaera HMM 906 may be transformed in this way. This adaptation operation is indicated by as step 4 in descriptive text in FIG. 9A .
- the operations illustrated by way of example in FIG. 9A could be carried out offline or otherwise prior to run-time application of a S2S voice conversion system, and for multiple pairs of input and output languages.
- a S2S voice conversion system could be prepared for use by multiple run-time users (or speakers), and for multiple configurations of language translation with voice conversion.
- robust, well-trained ASR systems and TTS system that have already been developed, configured, and/or deployed their respective tasks could be serve as ASR and TTS subsystems of cross-lingual voice conversion system.
- the respective HMMs of these subsystems could provide the initial auxiliary and output HMMs, for example.
- FIG. 9B is a schematic illustration of a second aspect of configuring of a voice conversion system, in accordance with an example embodiment.
- the chimaera HMM 906 constructed as described above may be prepared for voice conversion of a particular input speaker by computing an applying speech adaptation transforms.
- speech adaptation transforms labeled “T(X)” in FIG. 9B , that adapt the auxiliary HMM 902 to input speaker speech recordings 908 maybe be computed.
- adaptation techniques used in automatic speech recognition such as maximum likelihood linear regression (MLLR) and constrained MLLR (CMLLR) may be used to compute one or more appropriate speech adaptation transforms.
- MLLR maximum likelihood linear regression
- CMLLR constrained MLLR
- the input speaker speech recordings 908 could be created by the input speaker at any time prior to run-time, for example.
- the computed speech adaptation transforms T(X) could then be stored for future use in one or more invocations of voice conversion specific to the input speaker, and applicable to any one or more chimaera HMMs corresponding to any one or more input-output language pairs.
- FIG. 9C is a schematic illustration of a third aspect using a voice conversion system, in accordance with an example embodiment. More particularly, FIG. 9C illustrates run-time operation of voice conversion using the chimaera HMM 906 .
- the speech adaptation transforms T(X) may be applied to the chimaera HMM 906 in order to adapt the HMM states of the chimaera HMM 906 to the voice of the input speaker.
- the speech-adapted chimaera HMM 906 is shown in square brackets in FIG. 9C to signify that the chimaera HMM 906 has been configured for run-time conversion of the input speaker's voice.
- a run-time speech signal 907 of the input speaker in the input language may processed by the S2S system, ultimately arriving at the speech adapted chimaera HMM 906 as text in the output language.
- the speech adapted chimaera HMM 906 can then generate speech features for speech synthesis of the translate text, and the translated text can then be output as a run-time speech signal 909 in the output language and with the voice of the input speaker.
- FIG. 10 depicts a block diagram of an example cross-lingual S2S system 1000 that implements voice conversion, in accordance with an example embodiment.
- the S2S system 1000 is similar to the S2S system shown in FIG. 6 , except that a chimaera HMM configured as described above is used for generating speech features for speech synthesis.
- the functional components of the S2S system 1000 include an ASR subsystem 1002 for recognition of speech in an input language, a language translation subsystem 1004 for translating the input speech into an output language, and a TTS subsystem 1006 for outputting speech in the output language. In selected example inputs, outputs, and intermediate products of example operation are also depicted.
- Operation of the S2S system 1000 is largely the same that described for the S2S system 600 , except that the S2S system 1000 performs voice conversion that causes the output voice of the TTS subsystem 1006 to sound like the voice of the input speaker. More specifically, an input utterance 1001 in an input language and in the voice of an input speaker may recognized by the ASR subsystem 1002 , converted to text 1003 in the input language, translated by the language translation system 1004 into translated text 1005 in the output language, the converted by the TTS subsystem 1006 to a translated utterance 1007 in the output language and in the voice of the input speaker.
- the TTS subsystem 1006 implements a chimaera HMM that may be prepared as described above to generate speech features in the output language but with the voice of the input speaker.
- the ASR subsystem 1002 implements the auxiliary HMM for recognition. It should be noted that this is not necessarily required for voice conversion. However, the auxiliary HMM is shown as a visual cue to role it plays in preparation of the chimaera HMM. A different input HMM could be used in the ASR subsystem 1002 , but an HMM trained in the input language can still be part of the configuration of the chimaera HMM.
- the HMM states of the chimaera HMM are initially the same as those of the output HMM are replaced by HMM states of the auxiliary HMM that are selected for being closest matches to HMM states of the output HMM.
- the matching operation is carried out under a transform that compensates for differences between the auxiliary speaker used to train the auxiliary HMM and the output speaker use to train the output HMM. Further details of the matching under transform technique are described below.
- voice conversion is concerned with converting the voice of a “source speaker” to the voice of a “target speaker.”
- the source speaker is designated X
- the target speaker is designated Y.
- These designations are intended for convenience of discussion, and other designations could be used.
- feature analysis of speech samples of speaker X could generate a vector space of speech features, designated X-space.
- feature analysis of speech samples of speaker Y could generate a vector space of speech features, designated Y-space.
- feature vectors could correspond to parameterizations of spectral envelopes and/or excitation, as discussed above.
- X-space and Y-space may be different. For example, they could have a different number of vectors and/or different parameters. Further, they could correspond to different languages, be generated using different feature extraction techniques, and so on.
- Matching under transform may be considered a technique for matching the X-space and Y-space vectors under a transform that compensates for differences between speakers X and Y. It may be described in algorithmic terms as a computational method, and can be implemented as machine-readable instructions executable by the one or more processors of a computing system, such as a S2S system.
- the machine-language instructions could be stored in one or another form of a tangible, non-transitory computer-readable medium (or other article of manufacture), such as magnetic or optical disk, or the like, and made available to processing elements of the system as part of a manufacturing procedure, configuration procedure, and/or execution start-up procedure, for example.
- N and Q may not necessarily be equal, although the possibility that they are is not precluded.
- N and Q could correspond to a number of samples from speakers X and Y, respectively.
- the transformation function defines a parametric mapping from X-space to Y-space.
- a non-parametric, association mapping from Y-space to X-space may be defined in terms of conditional probabilities.
- ⁇ right arrow over (y) ⁇ q ) may be used to specify a probability that ⁇ right arrow over (y) ⁇ q maps to ⁇ right arrow over (x) ⁇ n .
- MUT involves bi-directional mapping between X-space and Y-space: parametric in a “forward direction” (X ⁇ Y) via F(•), and non-parametric in the “backward direction” (Y ⁇ X) via p( ⁇ right arrow over (x) ⁇ n
- a goal of MUT is to determine which X-space vectors ⁇ right arrow over (x) ⁇ n correspond to a Y-space ⁇ right arrow over (y) ⁇ q vector in the sense that F ( ⁇ right arrow over (x) ⁇ ) is close ⁇ right arrow over (y) ⁇ q in L2-norm, and under the circumstance that F( ⁇ right arrow over (x) ⁇ ) and the probabilities p ( ⁇ right arrow over (x) ⁇ n
- FIG. 11 is a conceptual illustration of parametric and non-parametric mapping between vector spaces, in accordance with example embodiments.
- the figure includes an X-space 1102 , represented as an oval containing several dots, each dot symbolically representing an X-space vector (e.g., ⁇ right arrow over (x) ⁇ n ).
- a Y-space 1104 is represented as an oval containing several dots, each dot symbolically representing an Y-space vector (e.g., ⁇ right arrow over (y) ⁇ q ).
- the two spaces are shown to contain a different number of vectors (dots).
- an arrow 1105 from Y-space to X-space symbolically represents non-parametric mapping via p( ⁇ right arrow over (x) ⁇ n
- D′ D ⁇ H ( X
- association probabilities may be expressed in the form of a Gibbs distribution and determined in what is referred to algorithmically herein as an “association step.”
- association step When ⁇ approaches zero, the mapping between Y-space and X-space becomes many to one (many Y-space vectors may be matched to one X-space vector). It can be shown in this case ( ⁇ 0) that the association probabilities may be determined from a search for the nearest X-space vector in terms of the distortion metric d( ⁇ right arrow over (y) ⁇ q
- the transform function Given the associations determined either by an association step or a matching step, the transform function can be defined and its optimal parameters determined by solving a minimization of D′ with respect to the defined form of F(•). This determination of F ( ⁇ right arrow over (x) ⁇ ) is referred to algorithmically herein as a “minimization step.”
- the purpose of the transform is to compensate for speaker differences between, in this example, speakers X and Y. More specifically, cross-speaker variability can be captured by a linear transform of the form ⁇ right arrow over ( ⁇ ) ⁇ k + ⁇ k ⁇ right arrow over (x) ⁇ n , where ⁇ right arrow over ( ⁇ ) ⁇ k is a bias vector, and ⁇ k is linear transformation matrix of the k-th class.
- the linear transform matrix can compensate for differences in the vocal tract that are related to vocal tract shape and size.
- F( ⁇ right arrow over (x) ⁇ ) may be expressed as:
- I is the identity matrix (appropriately dimensioned)
- ⁇ k ′ ⁇ vec ⁇ k ′ ⁇ contains only the free parameters of the structured matrix ⁇ k
- ⁇ k ⁇ right arrow over (x) ⁇ n X n ⁇ right arrow over ( ⁇ ) ⁇ k ′.
- the optimal ⁇ right arrow over ( ⁇ ) ⁇ can then be obtained by partial differentiation, setting
- association-minimization two algorithms may be used to obtain matching under transform.
- matching-minimization two algorithms may be used to obtain matching under transform.
- association-minimization may be implemented with the following steps:
- Initialization sets a starting point for MUT optimization, and may differ depending on the speech features used.
- MCEP mel-cepstral coefficient
- a search for a good vocal-tract length normalization transform with a single linear frequency warping factor may suffice.
- Empirical evidence suggests that an adequate initialization transform is one that minimizes the distortion in an interval [0.7, 1.3] of frequency warping factor.
- the association step uses the Gibbs distribution function for the association probabilities, as described above.
- the minimization step then incorporates the transformation function. Steps 5 and 6 iterate for convergence and cooling.
- matching-minimization may be implemented with the following steps:
- Initialization is the same as that for association-minimization, starting with a transform that minimizes the distortion in an interval of values of [0.7, 1.3] in linear frequency warping factor.
- the matching step uses association probabilities determined from a search for the nearest X-space vector, as described above.
- matching-minimization may yield comparable results to association-minimization, but at lower computation cost. Accordingly, only matching-minimization is considered below for MUT. It will be appreciated that the techniques discussed below could be generalized for application to association-minimization.
- matching under transform may be used for determining the closest matching HMM state from the auxiliary HMM to each HMM state of the output HMM.
- this matching may be accomplished by implementing the matching-minimization algorithm described above.
- each Gaussian state jointly models either the spectral envelope parameters and their delta and delta-delta values, or the excitation parameters and their delta and delta-delta values.
- the matching-minimization algorithm may be applied separately for both spectral envelope parameters and for excitation parameters in order to match Gaussian states of the auxiliary and output HMMs. Since the procedure is largely the same for both spectral envelope parameters and for excitation parameters, no explicit distinction is made between parameter types in relation to the Gaussian states referenced in the discussion below.
- a corresponding Gaussian state of the auxiliary HMM that is nearest in terms of a distance criterion based on mean-squared-error (mse) may be determined as follows. Matching is first initialized by scanning a range of values from 0.7 to 1.3 of linear warping factor that minimizes the overall distortion. This may be accomplished by resampling the relevant parameter (spectral envelope or excitation) at the linearly warped frequency factor. The matching-minimization algorithm may then be run with a single class transform to obtain the matching between the means of the HMM-state Gaussians of the output HMM and the means of the HMM-state Gaussians of the auxiliary HMM. The single class transform serves to compensate speaker differences such that the only source of distortion arises from language mismatch.
- the accuracy of the results of the matching-minimization procedure helps ensure high-quality voice conversion using the chimaera HMM with the HMM states replaced by those determined from the matching.
- the matching-minimization procedure may also be implemented efficiently and cost-effectively, thereby contributing to overall scalability of a S2S system that incorporates voice conversion.
- HMM state matching can be applied to robust, independently-constructed and configured auxiliary and output HMMs, thereby enabling cross-lingual voice conversion to be implemented in a scalable fashion for any arbitrary pair of languages.
Abstract
Description
d({right arrow over (y)} q ,{right arrow over (x)} n)=({right arrow over (y)} q −F({right arrow over (x)} n))T W q({right arrow over (y)} q −F({right arrow over (x)} n)), [1]
where Wq is a weighting matrix depending on Y-space vector {right arrow over (y)}q. Then taking p({right arrow over (x)}n|{right arrow over (y)}q) to be the joint probability of matching vectors {right arrow over (y)}q and {right arrow over (x)}n, an average distortion over all possible vector combinations may be expressed as:
D=Σ n,q p({right arrow over (y)} q {right arrow over (x)} n)d({right arrow over (y)} q {right arrow over (x)} n)=Σq p({right arrow over (y)} q)Σn p({right arrow over (x)} n |{right arrow over (y)} q)d({right arrow over (y)} q {right arrow over (x)} n). [2]
In the MUT approach, the bi-directional mapping provides a balance between forward and backward mapping, ensuring convergence to a meaningful solution.
so as to ensure that all Y-space vectors are accounted for equally, it follows that H(Y) is constant. A composite minimization criterion D′ may then be defined as:
D′=D−λH(X|Y), [3]
where the entropy Lagrangian λ corresponds to an annealing temperature.
F({right arrow over (x)} n)=Σk=1 K p((k|{right arrow over (x)} n)[{right arrow over (μ)}k+Σk {right arrow over (x)} n], [4]
where p (k|{right arrow over (x)}n) is the probability that {right arrow over (x)}n belongs to the k-th class.
In the above expressions, I is the identity matrix (appropriately dimensioned), σk′≡vec{Σk′} contains only the free parameters of the structured matrix Σk, and Σk{right arrow over (x)}n=Xn{right arrow over (σ)}k′. The optimal {right arrow over (γ)} can then be obtained by partial differentiation, setting
Doing so yields the following unique solution:
{right arrow over (γ)}=−(Σq p({right arrow over (y)} q)Σn p({right arrow over (x)} n |{right arrow over (y)} q)Γn T W qΓn)−1(Σq p({right arrow over (y)} q)Σn p({right arrow over (x)} n |{right arrow over (y)} q)Γn T W q {right arrow over (y)} q). [10]
-
- 1. Initialization.
- 2. Set λ to high value (e.g., λ=1).
- 3. Association step.
- 4. Minimization step.
- 5. Repeat from
step 3 until convergence. - 6. Lower λ according to a cooling schedule and repeat from
step 3, until λ approaches zero or other target value.
-
- 1. Initialization.
- 2. Matching step.
- 3. Minimization step.
- 4. Repeat from
step 2 until convergence.
Claims (36)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/069,492 US9177549B2 (en) | 2013-11-01 | 2013-11-01 | Method and system for cross-lingual voice conversion |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/069,492 US9177549B2 (en) | 2013-11-01 | 2013-11-01 | Method and system for cross-lingual voice conversion |
Publications (2)
Publication Number | Publication Date |
---|---|
US20150127349A1 US20150127349A1 (en) | 2015-05-07 |
US9177549B2 true US9177549B2 (en) | 2015-11-03 |
Family
ID=53007671
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/069,492 Active 2034-04-17 US9177549B2 (en) | 2013-11-01 | 2013-11-01 | Method and system for cross-lingual voice conversion |
Country Status (1)
Country | Link |
---|---|
US (1) | US9177549B2 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10423727B1 (en) | 2018-01-11 | 2019-09-24 | Wells Fargo Bank, N.A. | Systems and methods for processing nuances in natural language |
CN110364186A (en) * | 2019-08-08 | 2019-10-22 | 清华大学深圳研究生院 | A kind of emotion identification method across language voice end to end based on confrontation study |
US10994201B2 (en) * | 2019-03-21 | 2021-05-04 | Wormhole Labs, Inc. | Methods of applying virtual world elements into augmented reality |
US11848005B2 (en) | 2022-04-28 | 2023-12-19 | Meaning.Team, Inc | Voice attribute conversion using speech to speech |
Families Citing this family (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9613620B2 (en) * | 2014-07-03 | 2017-04-04 | Google Inc. | Methods and systems for voice conversion |
JP6118838B2 (en) * | 2014-08-21 | 2017-04-19 | 本田技研工業株式会社 | Information processing apparatus, information processing system, information processing method, and information processing program |
JP6293912B2 (en) * | 2014-09-19 | 2018-03-14 | 株式会社東芝 | Speech synthesis apparatus, speech synthesis method and program |
US20160110349A1 (en) * | 2014-10-20 | 2016-04-21 | Kimberly Norman-Rosedam | Language Translating Device |
US10366689B2 (en) * | 2014-10-29 | 2019-07-30 | Kyocera Corporation | Communication robot |
US10775996B2 (en) * | 2014-11-26 | 2020-09-15 | Snap Inc. | Hybridization of voice notes and calling |
US9773426B2 (en) * | 2015-02-01 | 2017-09-26 | Board Of Regents, The University Of Texas System | Apparatus and method to facilitate singing intended notes |
US11120816B2 (en) | 2015-02-01 | 2021-09-14 | Board Of Regents, The University Of Texas System | Natural ear |
US9683862B2 (en) * | 2015-08-24 | 2017-06-20 | International Business Machines Corporation | Internationalization during navigation |
JP6495850B2 (en) * | 2016-03-14 | 2019-04-03 | 株式会社東芝 | Information processing apparatus, information processing method, program, and recognition system |
US9990916B2 (en) * | 2016-04-26 | 2018-06-05 | Adobe Systems Incorporated | Method to synthesize personalized phonetic transcription |
US10249314B1 (en) * | 2016-07-21 | 2019-04-02 | Oben, Inc. | Voice conversion system and method with variance and spectrum compensation |
CN107103900B (en) * | 2017-06-06 | 2020-03-31 | 西北师范大学 | Cross-language emotion voice synthesis method and system |
CN107705802B (en) * | 2017-09-11 | 2021-01-29 | 厦门美图之家科技有限公司 | Voice conversion method and device, electronic equipment and readable storage medium |
CN107958269B (en) * | 2017-11-28 | 2020-01-24 | 江苏大学 | Driving risk degree prediction method based on hidden Markov model |
US20190221208A1 (en) * | 2018-01-12 | 2019-07-18 | Kika Tech (Cayman) Holdings Co., Limited | Method, user interface, and device for audio-based emoji input |
US11538455B2 (en) | 2018-02-16 | 2022-12-27 | Dolby Laboratories Licensing Corporation | Speech style transfer |
GB201804073D0 (en) * | 2018-03-14 | 2018-04-25 | Papercup Tech Limited | A speech processing system and a method of processing a speech signal |
JP7040258B2 (en) * | 2018-04-25 | 2022-03-23 | 日本電信電話株式会社 | Pronunciation converter, its method, and program |
US10997970B1 (en) * | 2019-07-30 | 2021-05-04 | Abbas Rafii | Methods and systems implementing language-trainable computer-assisted hearing aids |
HUE064070T2 (en) * | 2019-12-30 | 2024-02-28 | Tmrw Found Ip Sarl | Cross-lingual voice conversion system and method |
CN111754027B (en) * | 2020-06-03 | 2023-07-11 | 南京信息工程大学 | PM2.5 emission right allocation method based on probabilistic language term set |
US11626112B1 (en) | 2021-02-05 | 2023-04-11 | Wells Fargo Bank, N.A. | Bias detection in speech recognition models |
CN113160796B (en) * | 2021-04-28 | 2023-08-08 | 北京中科模识科技有限公司 | Language identification method, device and equipment for broadcast audio and storage medium |
US11783813B1 (en) | 2021-05-02 | 2023-10-10 | Abbas Rafii | Methods and systems for improving word discrimination with phonologically-trained machine learning models |
CN116844523B (en) * | 2023-08-31 | 2023-11-10 | 深圳市声扬科技有限公司 | Voice data generation method and device, electronic equipment and readable storage medium |
Citations (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5129002A (en) * | 1987-12-16 | 1992-07-07 | Matsushita Electric Industrial Co., Ltd. | Pattern recognition apparatus |
US5307444A (en) * | 1989-12-12 | 1994-04-26 | Matsushita Electric Industrial Co., Ltd. | Voice analyzing system using hidden Markov model and having plural neural network predictors |
US5440662A (en) * | 1992-12-11 | 1995-08-08 | At&T Corp. | Keyword/non-keyword classification in isolated word speech recognition |
US5617509A (en) * | 1995-03-29 | 1997-04-01 | Motorola, Inc. | Method, apparatus, and radio optimizing Hidden Markov Model speech recognition |
US5679001A (en) * | 1992-11-04 | 1997-10-21 | The Secretary Of State For Defence In Her Britannic Majesty's Government Of The United Kingdom Of Great Britain And Northern Ireland | Children's speech training aid |
US6125345A (en) * | 1997-09-19 | 2000-09-26 | At&T Corporation | Method and apparatus for discriminative utterance verification using multiple confidence measures |
US6212500B1 (en) * | 1996-09-10 | 2001-04-03 | Siemens Aktiengesellschaft | Process for the multilingual use of a hidden markov sound model in a speech recognition system |
US6460017B1 (en) * | 1996-09-10 | 2002-10-01 | Siemens Aktiengesellschaft | Adapting a hidden Markov sound model in a speech recognition lexicon |
US6629073B1 (en) * | 2000-04-27 | 2003-09-30 | Microsoft Corporation | Speech recognition method and apparatus utilizing multi-unit models |
US20050131694A1 (en) * | 2003-12-12 | 2005-06-16 | Seiko Epson Corporation | Acoustic model creating method, acoustic model creating apparatus, acoustic model creating program, and speech recognition apparatus |
US20050203737A1 (en) * | 2002-05-10 | 2005-09-15 | Toshiyuki Miyazaki | Speech recognition device |
US7003460B1 (en) * | 1998-05-11 | 2006-02-21 | Siemens Aktiengesellschaft | Method and apparatus for an adaptive speech recognition system utilizing HMM models |
US20060100874A1 (en) * | 2004-10-22 | 2006-05-11 | Oblinger Daniel A | Method for inducing a Hidden Markov Model with a similarity metric |
US20060136209A1 (en) * | 2004-12-16 | 2006-06-22 | Sony Corporation | Methodology for generating enhanced demiphone acoustic models for speech recognition |
US20060230140A1 (en) * | 2005-04-05 | 2006-10-12 | Kazumi Aoyama | Information processing apparatus, information processing method, and program |
US7216077B1 (en) * | 2000-09-26 | 2007-05-08 | International Business Machines Corporation | Lattice-based unsupervised maximum likelihood linear regression for speaker adaptation |
US20080091424A1 (en) * | 2006-10-16 | 2008-04-17 | Microsoft Corporation | Minimum classification error training with growth transformation optimization |
US20080319743A1 (en) * | 2007-06-25 | 2008-12-25 | Alexander Faisman | ASR-Aided Transcription with Segmented Feedback Training |
US7565282B2 (en) * | 2005-04-14 | 2009-07-21 | Dictaphone Corporation | System and method for adaptive automatic error correction |
US7603276B2 (en) * | 2002-11-21 | 2009-10-13 | Panasonic Corporation | Standard-model generation for speech recognition using a reference model |
US20100198577A1 (en) * | 2009-02-03 | 2010-08-05 | Microsoft Corporation | State mapping for cross-language speaker adaptation |
US8136154B2 (en) * | 2007-05-15 | 2012-03-13 | The Penn State Foundation | Hidden markov model (“HMM”)-based user authentication using keystroke dynamics |
US8620655B2 (en) * | 2010-08-16 | 2013-12-31 | Kabushiki Kaisha Toshiba | Speech processing system and method |
-
2013
- 2013-11-01 US US14/069,492 patent/US9177549B2/en active Active
Patent Citations (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5129002A (en) * | 1987-12-16 | 1992-07-07 | Matsushita Electric Industrial Co., Ltd. | Pattern recognition apparatus |
US5307444A (en) * | 1989-12-12 | 1994-04-26 | Matsushita Electric Industrial Co., Ltd. | Voice analyzing system using hidden Markov model and having plural neural network predictors |
US5679001A (en) * | 1992-11-04 | 1997-10-21 | The Secretary Of State For Defence In Her Britannic Majesty's Government Of The United Kingdom Of Great Britain And Northern Ireland | Children's speech training aid |
US5440662A (en) * | 1992-12-11 | 1995-08-08 | At&T Corp. | Keyword/non-keyword classification in isolated word speech recognition |
US5617509A (en) * | 1995-03-29 | 1997-04-01 | Motorola, Inc. | Method, apparatus, and radio optimizing Hidden Markov Model speech recognition |
US6212500B1 (en) * | 1996-09-10 | 2001-04-03 | Siemens Aktiengesellschaft | Process for the multilingual use of a hidden markov sound model in a speech recognition system |
US6460017B1 (en) * | 1996-09-10 | 2002-10-01 | Siemens Aktiengesellschaft | Adapting a hidden Markov sound model in a speech recognition lexicon |
US6125345A (en) * | 1997-09-19 | 2000-09-26 | At&T Corporation | Method and apparatus for discriminative utterance verification using multiple confidence measures |
US7003460B1 (en) * | 1998-05-11 | 2006-02-21 | Siemens Aktiengesellschaft | Method and apparatus for an adaptive speech recognition system utilizing HMM models |
US6629073B1 (en) * | 2000-04-27 | 2003-09-30 | Microsoft Corporation | Speech recognition method and apparatus utilizing multi-unit models |
US7216077B1 (en) * | 2000-09-26 | 2007-05-08 | International Business Machines Corporation | Lattice-based unsupervised maximum likelihood linear regression for speaker adaptation |
US7487091B2 (en) * | 2002-05-10 | 2009-02-03 | Asahi Kasei Kabushiki Kaisha | Speech recognition device for recognizing a word sequence using a switching speech model network |
US20050203737A1 (en) * | 2002-05-10 | 2005-09-15 | Toshiyuki Miyazaki | Speech recognition device |
US7603276B2 (en) * | 2002-11-21 | 2009-10-13 | Panasonic Corporation | Standard-model generation for speech recognition using a reference model |
US20050131694A1 (en) * | 2003-12-12 | 2005-06-16 | Seiko Epson Corporation | Acoustic model creating method, acoustic model creating apparatus, acoustic model creating program, and speech recognition apparatus |
US20060100874A1 (en) * | 2004-10-22 | 2006-05-11 | Oblinger Daniel A | Method for inducing a Hidden Markov Model with a similarity metric |
US20060136209A1 (en) * | 2004-12-16 | 2006-06-22 | Sony Corporation | Methodology for generating enhanced demiphone acoustic models for speech recognition |
US20060230140A1 (en) * | 2005-04-05 | 2006-10-12 | Kazumi Aoyama | Information processing apparatus, information processing method, and program |
US7565282B2 (en) * | 2005-04-14 | 2009-07-21 | Dictaphone Corporation | System and method for adaptive automatic error correction |
US20080091424A1 (en) * | 2006-10-16 | 2008-04-17 | Microsoft Corporation | Minimum classification error training with growth transformation optimization |
US8301449B2 (en) * | 2006-10-16 | 2012-10-30 | Microsoft Corporation | Minimum classification error training with growth transformation optimization |
US8136154B2 (en) * | 2007-05-15 | 2012-03-13 | The Penn State Foundation | Hidden markov model (“HMM”)-based user authentication using keystroke dynamics |
US20080319743A1 (en) * | 2007-06-25 | 2008-12-25 | Alexander Faisman | ASR-Aided Transcription with Segmented Feedback Training |
US7881930B2 (en) * | 2007-06-25 | 2011-02-01 | Nuance Communications, Inc. | ASR-aided transcription with segmented feedback training |
US20100198577A1 (en) * | 2009-02-03 | 2010-08-05 | Microsoft Corporation | State mapping for cross-language speaker adaptation |
US8620655B2 (en) * | 2010-08-16 | 2013-12-31 | Kabushiki Kaisha Toshiba | Speech processing system and method |
Non-Patent Citations (43)
Title |
---|
Alan W Black, Heiga Zen, and Keichi Tokuda, "Statistical Parametric Speech Synthesis," ICASSP 2007, pp. IV-1229-IV-1232. |
Alexander Kain and Michael W Macon, "Spectral voice conversion for text-to-speech synthesis," in Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on. IEEE, 1998, vol. 1, pp. 285-288. |
Arun Kumar and Ashish Verma, "Using phone and diphone based acoustic models for voice conversion: a step towards creating voice fonts," in Multimedia and Expo, 2003. ICME'03. Proceedings. 2003 International Conference on. IEEE, 2003, vol. 1, pp. I-393. |
Athanasios Mouchtaris, Jan Van der Spiegel, and Paul Mueller, "Non-parallel training for voice conversion by maximum likelihood constrained adaptation," in Acoustics, Speech, and Signal Processing, 2004. Proceedings.(ICASSP'04). IEEE International Conference on. IEEE, 2004, vol. 1, pp. I-1. |
CJ Leggetter and PC Woodland, "Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models," Computer speech and language, vol. 9, No. 2, pp. 171, 1995. |
Daisuke Saito, ShinjiWatanabe, Atsushi Nakamura, and Nobuaki Minematsu, "Statistical voice conversion based on noisy channel model," Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, No. 6, pp. 1784-1794, 2012. |
Daniel Erro and Asunci'on Moreno, "Frame alignment method for cross-lingual voice conversion," in Interspeech, 2007. |
Daniel Erro Eslava, "Intra-lingual and cross-lingual voice conversion using harmonic plus stochastic models," Barcelona, Spain: PhD Thesis, Universitat Politechnica de Catalunya, 2008. |
Daniel Erro, Asunci'on Moreno, and Antonio Bonafonte, "Inca algorithm for training voice conversion systems from nonparallel corpora," Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, No. 5, pp. 944-953, 2010. |
Daniel Erro, Inaki Sainz, Eva Navas, and Inma Hern'aez, "Improved hnm-based vocoder for statistical synthesizers," in Proc. Interspeech, 2011, pp. 1809-1812. |
David Sundermann, Antonio Bonafonte, Hermann Ney, and Harald Hoge, "A first step towards text-independent voice conversion," in Proc. of the ICSLP'04, 2004. |
H Valbret, E Moulines, and Jean-Pierre Tubach, "Voice transformation using psola technique," Speech Communication, vol. 11, No. 2, pp. 175-187, 1992. |
Heiga Zen, Keiichi Tokuda, and Alan W Black, "Statistical parametric speech synthesis," Speech Communication, vol. 51, No. 11, pp. 1039-1064, 2009. |
Hideki Kawahara, "Straight, exploitation of the other aspect of vocoder: Perceptually isomorphic decomposition of speech sounds," Acoustical science and technology, vol. 27, No. 6, pp. 349-353, 2006. |
Hui Ye and Steve Young, "Perceptually weighted linear transformations for voice conversion," in Proc. of the Eurospeech'03, 2003. |
Junichi Yamagishi, "Average-Voice-Based Speech Synthesis" PhD Thesis, 2006. |
Junichi Yamagishi, "Average-voice-based speech synthesis," Tokyo Institute of Technology, 2006. |
Junichi Yamagishi, Bela Usabaev, Simon King, Oliver Watts, John Dines, Jilei Tian, Rile Hu, Keiichiro Oura, Keiichi Tokuda, Reima Karhila, Mikko Kurimo, "Thousands of Voices for HMM-based Speech Synthesis," Interspeech 2009, 10th Annual Conference of the International Speech Communication Association, Brighton, United Kingdom, Sep. 6-10, 2009. |
Junichi Yamagishi, Oliver Watts, Simon King, Bela Usabaev, "Roles of the Average Voice in Speaker-adaptive HMM-based Speech Synthesis," INTERSPEECH 2010, Sep. 26-30, 2010, Makuhari, Chiba, Japan, pp. 418-421. |
Junichi Yamagishi, Takashi Nose, Heiga Zen, Zhen-Hua Ling, Tomoki Toda, Keiichi Tokuda, Simon King, and Steve Renals, "Robust speaker-adaptive hmm-based text-to-speech synthesis," Audio, Speech, and Language Processing, IEEE Transactions on, vol. 17, no. 6, pp. 1208-1230, 2009. |
Keiichi Tokuda, Heiga Zen, and Alan W Black, "An hmm-based speech synthesis system applied to english," in Speech Synthesis, 2002. Proceedings of 2002 IEEE Workshop on. IEEE, 2002, pp. 227-230. |
Kenneth Rose, "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems," Proceedings of the IEEE, vol. 86, No. 11, pp. 2210-2239, 1998. |
Kuldip K Paliwal and Bishnu S Atal, "Efficient vector quantization of Ipc parameters at 24 bits/frame," Speech and Audio Processing, IEEE Transactions on, vol. 1, No. 1, pp. 3-14, 1993. |
Mark JF Gales and PC Woodland, "Mean and variance adaptation within the mllr framework," Computer Speech and Language, vol. 10, No. 4, pp. 249-264, 1996. |
Masatsune Tamura, Takashi Masuko, Keiichi Tokuda, and Takao Kobayashi, "Adaptation of pitch and spectrum for hmm-based speech synthesis using mllr," in Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP'01). 2001 IEEE International Conference on. IEEE, 2001, vol. 2, pp. 805-808. |
Michael Pitz, Sirko Molau, Ralf Schlüter, and Hermann Ney, "Vocal tract normalization equals linear transformation in cepstral space," in Proc. EuroSpeech2001, 2001. |
Mikiko Mashimo, Tomoki Toda, Kiyohiro Shikano, and Nick Campbell, "Evaluation of crosslanguage voice conversion based on gmm and straight," 2001. |
Mouchtaris et al., "Non-Parallel Training for Voice Conversion by Maximum Likelihood Constrained Adaptation," Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing 2004 (ICASSP 2004), vol. 1, pp. I-1 to I-4. |
M-W Feng, Richard Schwartz, Francis Kubala, and John Makhoul, "Iterative normalization for speaker-adaptive training in continuous speech recognition," in Acoustics, Speech, and Signal Processing, 1989. ICASSP-89., 1989 International Conference on. IEEE, 1989, pp. 612-615. |
R Faltlhauser, T Pfau, and G Ruske, "On-line speaking rate estimation using gaussian mixture models," in Acoustics, Speech, and Signal Processing, 2000. ICASSP'00. Proceedings. 2000 IEEE International Conference on. IEEE, 2000, vol. 3, pp. 1355-1358. |
Robert J McAulay and Thomas F Quatieri, "Computationally efficient sine-wave synthesis and its application to sinusoidal transform coding," in Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on. IEEE, 1988, pp. 370-373. |
Sankaran Panchapagesan and Abeer Alwan, "Frequency warping for vtln and speaker adaptation by linear transformation of standard mfcc," Computer speech & language, vol. 23, No. 1, pp. 42-64, 2009. |
Sankaran Panchapagesan and Abeer Alwan, "Frequency warping for vtln and speaker adaptation by linear transformation of standard mfcc," Computer speech & language, vol. 23, No. 1, pp. 42-64, 2009. |
Shrikanth Narayanan and Dagen Wang, "Speech rate estimation via temporal correlation and selected sub-band correlation," in Proc. of the IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP). Citeseer, 2005. |
Vassilios V Digalakis, Dimitry Rtischev, and Leonardo G Neumeyer, "Speaker adaptation using constrained estimation of gaussian mixtures," Speech and Audio Processing, IEEE Transactions on, vol. 3, No. 5, pp. 357-366, 1995. |
Vassilis D Diakoloukas and Vassilios V Digalakis, "Maximum-likelihood stochastic-transformation adaptation of hidden markov models," Speech and Audio Processing, IEEE Transactions on, vol. 7, No. 2, pp. 177-187, 1999. |
Vincent Wan, Javier Latorre, Kayoko Yanagisawa, Norbert Braunschweller, Langzhou Chen, Mark J. F. Gales, and Masami Akamine, "Building HMM-TTS Voices on Diverse Data," IEEE Journal of Selected Topics in Signal Processing, vol. 8, No. 2, Apr. 2014, pp. 296-306. |
W Bastiaan Kleijn and Kuldip K Paliwal, Speech coding and synthesis, Elsevier Science Inc., 1995. |
Xianglin Peng, Keiichiro Oura, Yoshihiko Nankaku, and Keiichi Tokuda, "Cross-lingual speaker adaptation for hmm-based speech synthesis considering differences between language-dependent average voices," in Signal Processing (ICSP), 2010 IEEE 10th International Conference on. IEEE, 2010, pp. 605-608. |
Yamato Ohtani, Tomoki Toda, Hiroshi Saruwatari, and Kiyohiro Shikano, "Maximum likelihood voice conversion based on gmm with straight mixed excitation," in Proc. ICSLP, 2006, pp. 2266-2269. |
Yannis Stylianou and Eric Moulines, "Continuous probabilistic transform for voice conversion," IEEE Transactions on Speech and Audio Processing, vol. 6, pp. 131-142, 1998. |
Yannis Stylianou, "Applying the harmonic plus noise model in concatenative speech synthesis," Speech and Audio Processing, IEEE Transactions on, vol. 9, No. 1, pp. 21-29, 2001. |
Yi-Jian Wu, Yoshihiko Nankaku, and Keiichi Tokuda, "State mapping based method for cross-lingual speaker adaptation in hmm-based speech synthesis," in Proc. of Interspeech, 2009, pp. 528-531. |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10423727B1 (en) | 2018-01-11 | 2019-09-24 | Wells Fargo Bank, N.A. | Systems and methods for processing nuances in natural language |
US11244120B1 (en) | 2018-01-11 | 2022-02-08 | Wells Fargo Bank, N.A. | Systems and methods for processing nuances in natural language |
US10994201B2 (en) * | 2019-03-21 | 2021-05-04 | Wormhole Labs, Inc. | Methods of applying virtual world elements into augmented reality |
CN110364186A (en) * | 2019-08-08 | 2019-10-22 | 清华大学深圳研究生院 | A kind of emotion identification method across language voice end to end based on confrontation study |
CN110364186B (en) * | 2019-08-08 | 2021-06-25 | 清华大学深圳研究生院 | End-to-end cross-language speech emotion recognition method based on counterstudy |
US11848005B2 (en) | 2022-04-28 | 2023-12-19 | Meaning.Team, Inc | Voice attribute conversion using speech to speech |
Also Published As
Publication number | Publication date |
---|---|
US20150127349A1 (en) | 2015-05-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9177549B2 (en) | Method and system for cross-lingual voice conversion | |
US9183830B2 (en) | Method and system for non-parametric voice conversion | |
US9542927B2 (en) | Method and system for building text-to-speech voice from diverse recordings | |
US8527276B1 (en) | Speech synthesis using deep neural networks | |
JP7106680B2 (en) | Text-to-Speech Synthesis in Target Speaker's Voice Using Neural Networks | |
US8442821B1 (en) | Multi-frame prediction for hybrid neural network/hidden Markov models | |
US20200211529A1 (en) | Systems and methods for multi-style speech synthesis | |
US11605368B2 (en) | Speech recognition using unspoken text and speech synthesis | |
US8484022B1 (en) | Adaptive auto-encoders | |
US9240184B1 (en) | Frame-level combination of deep neural network and gaussian mixture models | |
US8571871B1 (en) | Methods and systems for adaptation of synthetic speech in an environment | |
US11450313B2 (en) | Determining phonetic relationships | |
US8374866B2 (en) | Generating acoustic models | |
US8594993B2 (en) | Frame mapping approach for cross-lingual voice transformation | |
KR20230156121A (en) | Unsupervised parallel tacotron non-autoregressive and controllable text-to-speech | |
KR20160058470A (en) | Speech synthesis apparatus and control method thereof | |
US20230122824A1 (en) | Method and system for user-interface adaptation of text-to-speech synthesis | |
US11322133B2 (en) | Expressive text-to-speech utilizing contextual word-level style tokens | |
US20230298567A1 (en) | Speech synthesis and speech recognition | |
US20230335111A1 (en) | Method and system for text-to-speech synthesis of streaming text | |
US20230017892A1 (en) | Injecting Text in Self-Supervised Speech Pre-training |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:AGIOMYRGIANNAKIS, IOANNIS;REEL/FRAME:031526/0909Effective date: 20131031 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044334/0466Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |