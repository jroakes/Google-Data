CN103703465A - Sentimental information associated with object within media - Google Patents
Sentimental information associated with object within media Download PDFInfo
- Publication number
- CN103703465A CN103703465A CN201280036367.8A CN201280036367A CN103703465A CN 103703465 A CN103703465 A CN 103703465A CN 201280036367 A CN201280036367 A CN 201280036367A CN 103703465 A CN103703465 A CN 103703465A
- Authority
- CN
- China
- Prior art keywords
- emotion
- user
- media
- input data
- user input
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 30
- 230000008451 emotion Effects 0.000 claims description 216
- 238000006116 polymerization reaction Methods 0.000 claims description 5
- 239000012634 fragment Substances 0.000 claims description 4
- 238000004590 computer program Methods 0.000 abstract description 11
- 230000002996 emotional effect Effects 0.000 description 19
- 238000003860 storage Methods 0.000 description 16
- 238000010586 diagram Methods 0.000 description 12
- 230000015654 memory Effects 0.000 description 12
- 230000008569 process Effects 0.000 description 10
- 230000005055 memory storage Effects 0.000 description 8
- 238000012545 processing Methods 0.000 description 7
- 230000009471 action Effects 0.000 description 6
- 230000008859 change Effects 0.000 description 5
- 238000004364 calculation method Methods 0.000 description 4
- 230000002123 temporal effect Effects 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 230000000007 visual effect Effects 0.000 description 3
- 238000004891 communication Methods 0.000 description 2
- 230000002349 favourable effect Effects 0.000 description 2
- 238000001914 filtration Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000012163 sequencing technique Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 210000003813 thumb Anatomy 0.000 description 2
- 230000000712 assembly Effects 0.000 description 1
- 238000000429 assembly Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 230000005611 electricity Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000000556 factor analysis Methods 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/435—Filtering based on additional data, e.g. user or group profiles
- G06F16/436—Filtering based on additional data, e.g. user or group profiles using biological or physiological data of a human being, e.g. blood pressure, facial expression, gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/25—Management operations performed by the server for facilitating the content distribution or administrating data related to end-users or client devices, e.g. end-user or client device authentication, learning user preferences for recommending movies
- H04N21/251—Learning process for intelligent management, e.g. learning user preferences for recommending movies
- H04N21/252—Processing of multiple end-users' preferences to derive collaborative data
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/25—Management operations performed by the server for facilitating the content distribution or administrating data related to end-users or client devices, e.g. end-user or client device authentication, learning user preferences for recommending movies
- H04N21/266—Channel or content management, e.g. generation and management of keys and entitlement messages in a conditional access system, merging a VOD unicast channel into a multicast channel
- H04N21/2668—Creating a channel for a dedicated end-user group, e.g. insertion of targeted commercials based on end-user profiles
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/475—End-user interface for inputting end-user data, e.g. personal identification number [PIN], preference data
- H04N21/4756—End-user interface for inputting end-user data, e.g. personal identification number [PIN], preference data for rating content, e.g. scoring a recommended movie
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/475—End-user interface for inputting end-user data, e.g. personal identification number [PIN], preference data
- H04N21/4758—End-user interface for inputting end-user data, e.g. personal identification number [PIN], preference data for providing answers, e.g. voting
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/60—Network structure or processes for video distribution between server and client or between remote clients; Control signalling between clients, server and network components; Transmission of management data between server and client, e.g. sending from server to client commands for recording incoming content stream; Communication details between server and client
- H04N21/65—Transmission of management data between client and server
- H04N21/658—Transmission by the client directed to the server
- H04N21/6582—Data stored in the client, e.g. viewing habits, hardware capabilities, credit card number
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/845—Structuring of content, e.g. decomposing content into time segments
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/845—Structuring of content, e.g. decomposing content into time segments
- H04N21/8456—Structuring of content, e.g. decomposing content into time segments by decomposing the content in the time domain, e.g. in time segments
Abstract
Methods, systems, and apparatuses, including computer programs encoded on computer readable media, for receiving user input data that includes a sentimental identifier and an indication of at least one of an amount of pressure and an amount of time used to select a sentimental input are disclosed. The user input data is associated with a media, and the associated media includes a plurality of objects. For each user input data, the user input data is associated with a portion of the associated media. The user input data is aggregated based upon the portion of the associated media. An average sentiment value is determined based upon the sentimental identifier and the indication of at least one of the amount of pressure and the amount of time used to select a sentimental input for at least one of the displayed objects.
Description
Background technology
Internet provides the access to various content.For example, by internet, can access the media about a large amount of different themes, such as image, audio frequency, video and webpage.User can visit this media by various devices.For example, user can use hand-held device (for example cell phone) to visit and play media.The spectators of media can also create the data associated with these media.Watching of media sorted, expressed hobby or will watching of media be published to the way of example that social networks website is the user creatable data associated with these media.Yet these data are conventionally relevant to whole media, rather than to the specific part of these media or in these media, occur role (character)/object is relevant.
Summary of the invention
Generally speaking, an aspect of the theme of describing in this instructions may be embodied in for receiving the method for user input data, and this user input data comprises emotion identifier and for selecting at least one indication of the amount of pressure of emotion input and time quantum.User input data is associated with media, and associated media comprise a plurality of objects.For each user input data, this user input data is associated with a part for associated media.The part of the media based on associated is carried out syndication users input data.Based on emotion identifier and for selecting at least one indication of the amount of pressure of emotion input and time quantum to determine average emotion value at least one in shown object.Other implementations of this aspect comprise corresponding system, equipment and the computer-readable medium of the action that is configured to carry out the method.
Accompanying drawing explanation
The details of one or more implementations of the theme of in the the accompanying drawings and the following description this instructions of elaboration being described.According to specification, drawings and the claims, other features, aspect and the advantage of this theme will be more obvious.
Fig. 1 is according to the block diagram of the example context of an illustrative implementation, wherein can create, store and the use data associated with media;
Fig. 2 A is according to the play medium of an illustrative implementation and the diagram that records the device of emotion information;
Fig. 2 B is according to the first device of the play medium of an illustrative implementation and the diagram that can record the second device of emotion information;
Fig. 3 be according to an illustrative implementation for recording the process flow diagram of the process of emotion information;
Fig. 4 is according to the process flow diagram of the process that emotion information is associated with the object of showing associated with media of an illustrative implementation;
Fig. 5 is a plurality of users' the front associated with media and the example report of negative emotion illustrating according to an illustrative implementation;
Fig. 6 is according to the block diagram of the computer system of an illustrative implementation.
Ref. No. and title same in each accompanying drawing are indicated same element.
Embodiment
The spectators of media can have and how not like/to like the particular portion of these media to assign to provide input based on them.The example of media includes but not limited to streaming audio, stream-type video, audio file and video file.Input can be associated with special object or role in this parts of media.Once collected emotion information, user dynamically create they like best, the most interesting, fear most, least like and other the list of media fragment (clip).In addition, can use the emotion information from a plurality of users to generate report, to determine that how welcome object, role or media have.
As running through the term that used all the time herein, emotion input is the assembly that allows user record emotion.Emotion input is associated with one or more emotions.Emotion input can allow user to specify the amount of emotion.For example, the amount of emotion can allow user indicate they think media a part just not bad, they like (like) it, or they like (love) it.The rank of the amount indication emotion of emotion.Emotion sign is for distinguishing various possible emotions, for example, positive, negative, interesting, fear etc.Emotion information is the set of the data that are associated with the specific record of user feeling.It can comprise various data, the position such as but not limited to start time, the end time of user's input of the amount of emotion, emotion sign, media identification, user's input, in inputting with user the media that are associated, consensus data etc.
Fig. 1 is according to the block diagram of the example context of an illustrative implementation, wherein can create, store and the use data associated with media.Example context 100 comprises network 102, for example Local Area Network, wide area network (WAN), internet or their combination.Network 102 connects media server 104, media player 108, emotion load module 110 and Emotional Service device 106.Example context 100 can comprise thousands of media server 104, media player 108, Emotional Service device 106 and emotion load module 110.
Media server 104 provides the access to media to media player 108.Media can include but not limited to music file, video, film etc.Media can be present on media player.Media can be stored in media server 104 or can be in the media file of another device on (not shown) of media server accesses.Can use variety of protocol well known by persons skilled in the art to visit media and media file by network 102, such as HTTP(Hypertext Transport Protocol), equity (P2P) agreement etc.
Media player 108 can present media for watching/listening to.User can show that they have with emotion load module 110 and like more or how do not like the part of specific media, media or the object in media.The part of media can refer to the object of showing in special time in whole media, media or period, video, or the object of listening in audio frequency media.The object of showing in video for example can comprise, such as role, stage property (prop), the part (narrator) that can listen, or in particular video frequency the object of showing in video of visual or any other project of audibly presenting and so on.Object in audio file can comprise the object that can listen, such as musical instrument, singer, narrator etc.In an implementation, emotion load module 110 is integrated in media player 108.In another implementation, emotion load module 110 is integrated in the second device different from media player 108.For example, emotion load module 110 can be integrated into cell phone, notebook, desk-top computer, PDA(Personal Digital Assistant) etc.Input from user can be transferred to one or more Emotional Service devices 106.In an implementation, the function of the function of media server 104 and Emotional Service device 106 can be integrated in individual server.Emotional Service device 106 can store received input in memory storage 112 into.Memory storage 112 can be any memory storage well known by persons skilled in the art, such as database, hard-drive, remote data storage etc.As described in detail hereinafter, user's input of storing can be used for various objects.
Fig. 2 A is according to the play medium of an illustrative implementation and the diagram that records the device of emotion information.Can be from one or more media server 104 retrieval media or stream transmission media.In an implementation, media player has the screen of display video media.Device 200 can screen watch part 206 in displaying video.Can in the message part 202 of screen, show the information being associated with media.Information can be included in media self, separate storage with these media, or can for example, from remote-control device (media server 104), retrieve.The emotion part 204 of screen can be used for collecting the input from user, for example emotion value.
In another implementation, the emotion part 204 of screen is divided into two or more parts, and each part is corresponding with a kind of emotion.In an implementation, emotion part 204 is divided into front portion and negative part.When user presses front portion, can there is change color in the background color of front portion.Similarly, the amount of pressure based on putting on negative part, also can there is change color in the background color of negative part.The color of using in background can be associated with specific emotion.For example, green, blueness and purple can be associated with positive emotion, and yellow, orange and red can being associated with negative emotion.
Emotion input can also be used to catch the information relevant with emotion except positive or negative.For example, the part that can catch and record specific medium has how fearful or how vulgar.Any amount of emotion input capable of being combined is caught the emotion of expectation.In an implementation, when not pressing any one possible emotion input, can suppose a kind of emotion of neutrality.
In another example, single emotion part 204 is provided and makes to use gesture and determine emotion sign, for example positive or negative.In an implementation, can be by press and be moved to the left to record positive emotion in emotion part 204, and can be by press and move right to record negative emotion in emotion part 204.Certainly, other gestures are also possible and gesture can be mapped to other emotions.
In other implementations, some factors that the amount of emotion can be based on except putting on the amount of pressure of emotion input.For example, with the time quantum of pressing specific emotion input, come indicating user to have and like more/do not like the part of media.In another implementation, can indicate with index dial or slide block the amount of emotion.
Fig. 2 B is according to the first device of the play medium of an illustrative implementation and the diagram that can record the second device of emotion information.In this implementation, media player 220 is the devices that separate with the device 230 that comprises emotion load module.For example, media player 220 can be the TV that can play media signal, notebook, desk-top computer, panel computer etc.What media player 220 comprised screen watches part 206, and can comprise message part 202.In some implementations, message part 202 is not shown.Device 230 is for catching emotion input to compare similar mode with above-described device 200.In an implementation, media player 220 can be with device 230 for example, by network (network 102) or communicate by PAN (Personal Area Network).In an implementation, device 230 can be sent to emotion information media player 220, and it can then be transferred to Emotional Service device 106 by this emotion information.In another implementation, device 230 can be sent to emotion information Emotional Service device 106.
Fig. 3 be according to an illustrative implementation for recording the process flow diagram of the process of emotion information.Can implementation procedure 300 on media player 108 or in the part of emotion load module 110.In an implementation, process 300 is coded on computer-readable medium, and this computer-readable medium comprises the instruction that makes the operation of this calculation element implementation 300 when being carried out by calculation element.
When detecting, when inputting, user starts to record emotion input (302).In an implementation, user's input is pressed emotion with user and is inputted corresponding.When user being detected and input, can record with this and input associated information.For example, with the associated information of input can comprise detect user's input time, input position, emotion sign on associated screen, put on the amount of pressure of emotion input etc. with user.When user continues to provide input, record is inputted associated information (304) with this.Finally, user's end of input (306) detected.For example, user's end of input can or lack with the input of release emotion the pressure correlation connection that puts on emotion input.Once user's end of input be detected, can part input associated information with user and determine various emotion informations based on recorded.The amount of pressure that for example puts on emotion input can be used to calculate the amount (308) of emotion.Can calculate in every way the amount of emotion, for example, by calculating put on the mean pressure of emotion input or put on emotion input pressure maximum and it is associated with the amount of emotion.Can also determine emotion sign, for example positive or negative (310).For example, can user the position of input, the indication of the emotion input of pressing, or the action of user's input is determined with user and is inputted associated emotion value.After calculating these values, emotion information can be sent to one or more Emotional Service devices (312).In an implementation, amount, media identification information and the user totem information of emotion sign, emotion can be sent to Emotional Service device.Subscriber identity data can comprise unique or part unique identifier, and it doesn't matter can to conceal this unique or part unique identifier and this unique or part unique identifier and user's name.In another implementation, emotion information can comprise the part of recorded data, for example action and the position of user's input.In this implementation, Emotional Service device can calculate or recalculate amount and the emotion value of emotion.
Fig. 4 is according to the process flow diagram of the process that emotion input is associated with the shown object associated with media of an illustrative implementation.Server (for example Emotional Service device 106) receives emotion information (402).Server can receive the emotion information about various media from many users.Emotion information can comprise the anonymous identifier for this emotion information is associated with user.The medium identifier that can be included in emotion information can be used for this emotion information to be associated with specific medium.The amount of emotion can conventionally be associated with media or can be associated with the specific part of media.Data in emotion information can be used for the time correlation connection (404) in this emotion information and media.These data can be to be used to indicate time in media or any data of position, such as but not limited to scope of the time in media or period, frame number, frame number etc.Once determine the part of media, can guide the object associated with specific part.For video file, object can be role, event, shown object, speaker (speaker) etc.For audio file, object can be musical instrument, the lyrics, singer, speaker etc.Definite one or more objects (406) that will be associated with emotion information.
In an implementation, based on written record (transcript) or closed caption information, emotion information is associated with object.In this implementation, temporal information is for the object in the part of definite media.This object can be the role who has said loquacity, the role who has said maximum duration within the period being associated, and sends the role of admiration or the music of playing within this period.In another implementation, can determine the role who speaks in specific fragment by Speaker Identification.Can on media player or another device, complete Speaker Identification, and the speaker who identifies can be included in emotion information.In another implementation, for example, at server (Emotional Service device 106), handle affairs and first completed Speaker Identification, and temporal information can be used for determining within this period, who says at most, the longest or and/or the most loud.So, this role can be the affiliated partner of emotion input.Can also determine the role who occurs in the part of video with recognition of face.Which role of specific part that the action of role's mouth can be indicated at video speaks.Can adopt and compare similar mode with Speaker Identification and determine recognition of face information.In an implementation, media player or another device and the speaker who identifies can be included in emotion information.In another implementation, for example, at server (Emotional Service device 106), handle affairs and first completed recognition of face, and temporal information can be used for determining within this period, who says at most, the longest and/or the most loud.Can also emotion input be associated with object with the perceived position of object in the part of media.In an implementation, can identify the object being associated with emotion input with the role in video foreground or other visual objects.
The polymerization of emotion information that can also be based on from one or more users is inferred with emotion and is inputted associated object.In an implementation, analyze the various emotion informations of collecting on whole specific medium or in the part of these media and determine the object associated with some or all emotion information.For example, can analyze the part that role has wherein said specific lines or done the media of introducing.If recorded the user's of emotion information quantity during this part of media, surpass predetermined threshold, this role can be the object associated with this emotion information.In addition a plurality of parts that, can analyzing medium.For example, can analyze the part of the media that comprise specific role or object.If user has consistent emotion information for two or more in these parts, specific role or object can be associated with this emotion information.For example, if user is for the great majority input negative emotion comprising in the appearance of specific role or the part of special object, this role or object can be associated with this user's negative emotion.In another implementation, from one or more users' the emotion information that comprises phase feeling of sympathy identifier for determining the object being associated with one or more emotion informations.For each emotion information, determine the part of the media associated with this emotion information.Next, determine the object with the partial association of these media, and increase progressively the operation counter with each object association.For each emotion information, repeat this process.There is the object of high counting and be confirmed as affiliated partner, and each emotion information that the media portion of definite object and the object definite with comprising this is associated is associated.
Can also use the information receiving from another source to determine the object associated with user feeling.For example, suppose the on-the-spot broadcasting of an awards ceremony.During ceremony, can input and receive a plurality of emotion informations by Emotional Service device by user.This emotion input can comprise about when recorded the temporal information of emotion information in program.Live associated time monitoring Search Results and/or social media site by with awards ceremony, can be used as the object associated with emotion information to the increase of the use of Personal name.Want restricting data amount, can use filtration.In one example, the identifier based on awards ceremony or other media comes filter search results or social media data.Such filtration has reduced analyzed data volume, and has increased emotion input is carried out to associated accuracy with object.
In another implementation, the position of pressing on media player or other devices can be used for emotion information to be associated with object.In an implementation, use media player or other devices with touch-screen.User is by directly press to input affection data on role or object, rather than uses special-purpose emotion input.In this implementation, role or to as if emotion input.User can indicate emotion identifier by gesture, for example for positive emotion by pressing and moving up, or for negative emotion by pressing and moving down.The position that user presses screen can be used for determining which role or object are associated with emotion information.In an implementation, on screen, the relative position of user's input is sent to Emotional Service device as the part of emotion information.Emotional Service device can the data based on describing the position of role in media and/or object determines that distance users inputs nearest object.
Once Emotional Service device is collected affection data, can make to use in various manners this affection data.Generating report is an example.Fig. 5 is a plurality of users' the front associated with media and the example report of negative emotion illustrating according to an illustrative implementation.Report 500 comprises average positive emotion 504 and the negative emotion 506 of calculating for the one or more users that play media.Can calculate average emotion 504 and 506 for single, a plurality of or all media play.In addition, report can or can comprise a plurality of users based on unique user.In an implementation, user shares common feature, for example age, geographic position, reproduction time, broadcasting date etc.Report 500 can comprise the indication of the one or more objects that are associated with the part of emotion 504 and 506.Icon 510,512,514 and 516 can indicate this emotion relevant to specific role.Select icon (for example hovering by clickable icon or on icon) that the information being associated with role in media and specified point can be provided.In an implementation, select icon to show link, or be directly linked to the video segment associated with affection data.For example, clickable icon 512 can cause being directed to a part in the media that were close to before low emotion information record.Icon 518 can indicating user emotion lifting join with some factor analysis except role.For example, solution that icon 518 can be indicated emotion information and diverting final result, conflict etc. is associated.The average emotion of two or more emotions based on above-mentioned provided can also be provided in report 500.
In another implementation, can be the specific role in media or object generation report.For example, report can indicate in media, when to occur specific role or object.The average emotion that can show each media portion that comprises this role or object.Report can comprise all emotion informations that are associated with the part of media, the emotion information being associated with specific role or object and with specific role or the irrelevant emotion information of object in one or more.The peak value of the amount of emotion also can appear in report.Can be the one or more objects that are included in media and generate report.Can by these reports each other relatively and these reports can provide to have about spectators and like more/how not like special object and/or role's indication.
Emotion information can also be for creating or revise laugh track.For example, can be specific media and create initial laugh track.After having received the emotion information of scheduled volume, can revise laugh track based on this emotion information.Laugh track can continue to change by the emotion information based on received.For example, can upgrade laugh track according to variation of the amount of period planning, emotion information based on received, emotion information based on received etc.In one example, can determine the media portion of extremely not liked.Can in laugh track, insert other negative sound clip of hiss, opposition sound or some comes corresponding with emotion information.Can also be at associated time modification laugh track to increase the volume of hiss, opposition sound etc.In addition the part of the video corresponding with the peak value of positive emotion, that for example liked or interesting part, the volume that can be used for inserting laugh, cheer etc. or increase laugh, cheer etc..The volume and/or the quantity that can also the emotion input based on received reduce laugh, hiss etc.Media can have one or more laugh tracks.For example, single laugh track can be used for media.In another implementation, can the emotion based on collected input for specific user or user organize and generate laugh track.In one example, like user's group of specific medium to compare with not liking the group of this specific medium, will there is the laugh track that comprises more and/or more loud cheer/laugh.The second laugh track with less and/or lighter cheer/laugh can be offered to user's group of not liking this specific medium.
Emotion information can also be used to one or more users to create media fragment list.For example, emotion information video segment list favorite and that least like for this user creates that can be based on unique user.In another implementation, can generate list based on one or more users, for example input of the emotion based on user friend.By using emotion information, can recommend other video segments to user.As an example, determined the video segment that user likes and determined other users that like equally one or more these video segments.The video segment that welcome by other users can be recommended to this user.In another implementation, can determine with user and feel role and/or the object that the most interesting video segment is associated.Other video segments that comprise this role and/or object can be recommended to this user.
Emotion information from one or more users can also be offered to the current user who is playing media.Shown in the feedback fraction 234 of device 230 as shown in Figure 2 B, can provide the figure of the emotion information corresponding with current just played media.Such information can impel the spectators of these media that themselves emotion information is provided.
Fig. 6 is according to the block diagram of the computer system of an illustrative implementation.This computer system or calculation element 600 can be used for realizing media player 108, emotion load module 110, media server 104 and/or Emotional Service device 106.Computing system 600 comprises bus 605 or for transmitting other communications components of information, and is coupled to bus 605 for the treatment of the processor 610 of information.Computing system 600 also comprises and is for example coupled to bus 605, for the primary memory 615 of storage information and the instruction that will be carried out by processor 610, random-access memory (ram) or other dynamic storage device.Primary memory 615 can also be for being carried out stored position information, temporary variable between order periods or other intermediate informations by processor 610.Computing system 600 can also comprise that being coupled to bus 605 is used to processor 610 to store ROM (read-only memory) (ROM) 610 or other static memories of static informations and instruction.Memory storage 625(is solid-state device, disk or CD for example) be coupled to bus 605 for storing enduringly information and instruction.
According to each implementation, in response to processor 610, carry out the order structure comprising in primary memory 615, can have been realized by computing system 600 method of illustrative implementation described herein.Such instruction for example, can be read primary memory 615 from another computer-readable medium (memory storage 625).The order structure that execution comprises in primary memory 615, makes computing system 600 carry out illustrative method described herein.Can adopt the one or more processors in multiprocessing configuration to carry out the instruction comprising in primary memory 615.In optional implementation, can replace software instruction or combine with software instruction by hard-wired circuit, thereby realize illustrative implementation.Therefore, implementation is not limited to any particular combination of hardware circuit and software.
Although described an example of disposal system in Fig. 6, but can be at the Fundamental Digital Circuit of other types or at computer software, firmware or hardware (comprising disclosed in this manual structure and their structural equivalents), or the theme of describing implement this instructions in their one or more combination in and the realization of feature operation.
Can be in Fundamental Digital Circuit or in computer software, firmware or hardware (comprising disclosed in this manual structure and their structural equivalents), or the theme of describing realize this instructions in their one or more combination in and the implementation of operation.The implementation of the theme of describing in this instructions can be embodied as on one or more computer-readable storage mediums, encode, for carry out or control one or more computer programs of the operation of data processing equipment by data processing equipment, i.e. one or more modules of computer program instructions.Alternatively or additionally, can for example,, in the upper coded program instruction of the artificial transmitting signal generating (machine produces electricity, light or electromagnetic signal), wherein generate the information that this transmitting signal is carried out by data processing equipment for being transferred to suitable receiving equipment with coding.Computer-readable storage medium can be or be contained in computer readable storage means, computer-readable storage substrate, random or sequential access memory array or device, or their one or more combination.In addition,, although computer-readable storage medium is not transmitting signal, computer-readable storage medium can be source or the destination of the computer program instructions of encoding in the artificial transmitting signal generating.Computer-readable storage medium can also be one or more independent assemblies or medium (for example a plurality of CD, dish or other memory storages), or is contained in wherein.Therefore, computer-readable storage medium be tangible be also permanent.
The operation of describing in this instructions can be embodied as to the operation of the data of storing or receive from other sources being carried out by data processing equipment in one or more computer readable storage means.
Unit and the machine for the treatment of any type of data contained in term " data processing equipment ", for example comprise programmable processor, computing machine, SOC (system on a chip) or above-mentioned in a plurality of or combination.Equipment can comprise dedicated logic circuit, for example FPGA(field programmable gate array) or ASIC(special IC).Except hardware, equipment can also be included as the code that above-mentioned computer program creates execution environment, for example, form the code of processor firmware, protocol stack, data base management system (DBMS), operating system, cross-platform runtime environment, virtual machine or their one or more combination.Equipment and execution environment can be realized various different computation model framework, as network service, Distributed Calculation and grid computing framework.
Can adopt any type of programming language to write computer program (being called again program, software, software application, script or code), comprise compiling or interpretative code, statement or procedural language, and can adopt any form to carry out deploying computer programs, comprise as stand-alone program or as module, assembly, subroutine, object or other unit of being suitable for using in computing environment.Computer program can but be not must be corresponding with the file in file system.Program can be stored in to (one or more scripts of for example storing) in the part of the file that keeps other programs or data in marking language document, be exclusively used in the Single document of said procedure, or for example, in a plurality of file that matches (storing a plurality of files of one or more modules, subroutine or partial code).Computer program can be deployed as on a computing machine and carry out, or carry out being positioned at the three unities or being distributed on a plurality of computing machines interconnected on a plurality of places and by communication network.
The processor that is suitable for computer program for example comprises: general and special microprocessor, and any one or more processors of the digital machine of any type.Conventionally, processor will receive from ROM (read-only memory) or random access memory or the two instruction and data.For the processor performing an action according to instruction, and for storing the necessary element that one or more memory storages of instruction and data are computing machines.Conventionally, computing machine also can comprise that one or more mass storage devices are for storing data, or be operatively coupled so as to receive from the data of one or more mass storage devices or to its transmission data or carry out above-mentioned both, this mass storage device is for example disk, magneto-optic disk or CD.Yet computing machine must not have such device.In addition, computing machine can be embedded to another device, only lift several examples, for example mobile phone, PDA(Personal Digital Assistant), Mobile audio frequency or video player, game console, GPS (GPS) receiver or flash memory device (for example, USB (universal serial bus) (USB) flash drive).The device that is suitable for storing computer program instructions and data comprises and for example comprises nonvolatile memory, medium and the memory storage of form of ownership: semiconductor storage, for example EPROM, EEPROM and flash memory device; Disk, for example built-in hard disk or removable dish; Magneto-optic disk; And CD-ROM and DVD-ROM dish.Can supplement processor or storer by dedicated logic circuit, or processor or storer are incorporated to wherein.
For mutual with user is provided, can for example, in the display device (CRT(cathode-ray tube (CRT)) from information to user or the LCD(liquid crystal display that have for show) monitor), and the implementation that for example, realizes the theme of describing in this instructions on the computing machine of keyboard and indicating device (mouse or trace ball), wherein user can provide input to computing machine by keyboard and indicating device.Also can use the device of other types that mutual with user is provided, for example, the feedback that offers user can be any type of sensory feedback, for example visual feedback, audio feedback or tactile feedback; And can receive in any form from user's input, comprise acoustics, voice or sense of touch input.
Although this instructions comprises a plurality of concrete implementation details, these should be interpreted as to the restriction to any invention or tenable scope, but be construed as, be the description for the feature of the specific implementation mode of specific invention.Can also in single implementation, combine some feature of describing in the context of realizing different implementations in this manual.Conversely, also can be in a plurality of implementations respectively or in any suitable sub-portfolio, realize each feature of describing in the context in single implementation.In addition, although may describe feature as hereinbefore in some combination, work, and be to advocate so even at first, but one or more features of the combination from advocated can be removed from this combination in some cases, and advocated combination can be directed to the distortion of sub-portfolio or sub-portfolio.
Similarly, although described operation with particular order in the accompanying drawings, this should be interpreted as require according to shown in particular order or sequencing carry out this operation, or require all operations illustrating of execution, to reach the result of expectation.In some cases, multitasking and parallel processing may be favourable.In addition, independently being interpreted as of various system components in above-described implementation should do not required to this independence in all implementations, and be construed as conventionally can be in single software product by described program assembly and the system integration to together with, or be bundled to a plurality of software products.
Therefore, the specific implementation mode of theme has been described.Other implementations within the scope of the appended claims.In some cases, can adopt and carry out in differing order the action of describing in the claims, and still reach the result of expectation.In addition the method being described in the drawings, and do not require shown in particular order or the sequencing result that reaches expectation.In some implementation, multitasking and parallel processing may be favourable.
Claims (20)
1. a method, comprising:
Receive user input data, this user input data comprises emotion identifier and for selecting at least one indication of the amount of pressure of emotion input and time quantum, wherein this user input data is associated with media, and wherein associated media comprise a plurality of objects;
For each user input data, this user input data is associated with the part of associated media;
The partially polymerized user input data of the media based on associated; And
With treatment circuit based on described emotion identifier and described for selecting at least one indication of the amount of pressure of emotion input and time quantum to determine average emotion value at least one in shown object.
2. method according to claim 1, wherein said average emotion value only comprises the user data being associated with positive emotion identifier.
3. method according to claim 1, wherein said average emotion value only comprises the user data being associated with negative emotion identifier.
4. method according to claim 1, also comprises:
Described media are divided into continuous time portion; And
Based on described emotion identifier with for selecting each time portion that the amount of pressure of emotion input or the indication of time quantum are described media to determine average emotion value.
5. method according to claim 1, is also included as each user input data and determines the object being associated with this user input data.
6. method according to claim 5, wherein determine that inputting with this user the object being associated comprises:
Determine the role who speaks in the part of associated media; And
Described user's input is associated with the role who speaks.
7. method according to claim 5, wherein determine that inputting with this user the object being associated comprises:
Collection has the set of user's input of phase feeling of sympathy identifier, and wherein the set of this user's input is associated with described media;
For each the user's input in described set, determine all objects that occur in the part of the media that are associated with this user's input;
For each object, increase progressively the counter being associated with this object; And
Each user's input in described set and the object being associated with maximum counter are associated.
8. method according to claim 5, wherein determine that inputting with this user the object being associated comprises:
From remote resource, receive the information from more than second user, wherein this remote resource is different from described media;
Determine referent in this information; And
Described user's input is associated with determined object.
9. method according to claim 5, wherein determine that inputting with this user the object being associated comprises:
Receive the indication of the position of wherein having selected emotion input; And
The object that identification is associated with this position.
10. method according to claim 1, also comprises that the user based on institute's polymerization inputs generation laugh track.
11. methods according to claim 1, also comprise that the user based on institute's polymerization inputs the list that generates media fragment.
12. methods according to claim 1, also comprise that the user based on institute's polymerization inputs the timeline that generates the associated media that comprise positive emotion line and negative emotion line.
13. 1 kinds of systems, comprising:
One or more processors, it is configured to:
Receive user input data, this user input data comprises emotion indication and for selecting at least one indication of the amount of pressure of emotion input and time quantum, wherein this user input data is associated with media, and wherein associated media comprise a plurality of objects;
For each user input data, this user input data is associated with the part of associated media;
The partially polymerized user input data of the media based on associated; And
Based on described emotion identifier and described for selecting at least one indication of the amount of pressure of emotion input and time quantum to determine average emotion value at least one in described object.
14. systems according to claim 13, wherein said one or more processors are also configured to: for each user input data is determined the object being associated with this user input data.
15. systems according to claim 13, wherein said one or more processors are also configured to:
Determine the role who speaks in the part of associated media; And
Described user's input is associated with the role who speaks.
16. systems according to claim 13, wherein said one or more processors are also configured to:
From remote resource, receive the information from more than second user, wherein this remote resource is different from described media;
Determine referent in this information; And
Described user's input is associated with determined object.
17. systems according to claim 13, wherein said one or more processors are also configured to:
Receive the indication of the position of wherein having selected emotion input; And
The object that identification is associated with this position.
18. 1 kinds of nonvolatile computer-readable mediums that store instruction on it, described instruction comprises:
For receiving the instruction of user input data, this user input data comprises emotion indication and for selecting at least one indication of the amount of pressure of emotion input and time quantum, wherein this user input data is associated with media, and wherein associated media comprise a plurality of objects;
For for each user input data, the instruction that this user input data is associated with the part of associated media;
Be used for the instruction of the partially polymerized user input data of the media based on associated; And
For based on described emotion identifier and describedly determine the instruction of average emotion value at least one that select that at least one indication of the amount of pressure of emotion input and time quantum is described object.
19. nonvolatile computer-readable mediums according to claim 18, wherein said instruction also comprises: be used to each user input data to determine the instruction of the object being associated with this user input data.
20. nonvolatile computer-readable mediums according to claim 18, wherein said instruction also comprises for carrying out the instruction of following operation:
Determine the role who speaks in the part of associated media; And
Described user's input is associated with the role who speaks.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/205,240 US8719277B2 (en) | 2011-08-08 | 2011-08-08 | Sentimental information associated with an object within a media |
US13/205,240 | 2011-08-08 | ||
PCT/US2012/049838 WO2013022879A1 (en) | 2011-08-08 | 2012-08-07 | Sentimental information associated with an object within media |
Publications (1)
Publication Number | Publication Date |
---|---|
CN103703465A true CN103703465A (en) | 2014-04-02 |
Family
ID=47668879
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201280036367.8A Pending CN103703465A (en) | 2011-08-08 | 2012-08-07 | Sentimental information associated with object within media |
Country Status (5)
Country | Link |
---|---|
US (4) | US8719277B2 (en) |
EP (1) | EP2742490A4 (en) |
KR (1) | KR101949308B1 (en) |
CN (1) | CN103703465A (en) |
WO (1) | WO2013022879A1 (en) |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104583924A (en) * | 2014-08-26 | 2015-04-29 | 华为技术有限公司 | Method and terminal for processing media file |
CN106056398A (en) * | 2015-04-07 | 2016-10-26 | 国际商业机器公司 | Method and system for collecting and presenting user feedback information for composite offering |
CN107209769A (en) * | 2014-12-31 | 2017-09-26 | 开放电视公司 | The metadata management transmitted for content |
CN108293150A (en) * | 2015-12-22 | 2018-07-17 | 英特尔公司 | Mood timed media plays back |
CN109660853A (en) * | 2017-10-10 | 2019-04-19 | 腾讯科技（北京）有限公司 | Interactive approach, apparatus and system in net cast |
CN111414506A (en) * | 2020-03-13 | 2020-07-14 | 腾讯科技（深圳）有限公司 | Emotion processing method and device based on artificial intelligence, electronic equipment and storage medium |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2013033375A (en) * | 2011-08-02 | 2013-02-14 | Sony Corp | Information processing apparatus, information processing method, and program |
US8789120B2 (en) * | 2012-03-21 | 2014-07-22 | Sony Corporation | Temporal video tagging and distribution |
US9483118B2 (en) * | 2013-12-27 | 2016-11-01 | Rovi Guides, Inc. | Methods and systems for selecting media guidance functions based on tactile attributes of a user input |
JP6484079B2 (en) * | 2014-03-24 | 2019-03-13 | 株式会社 ハイディープＨｉＤｅｅｐ Ｉｎｃ． | Kansei transmission method and terminal for the same |
US9438480B2 (en) * | 2014-03-24 | 2016-09-06 | Ca, Inc. | Generating a representation of the status of a data processing system based on empirical operations metrics and derived sentiment metrics |
US11010726B2 (en) * | 2014-11-07 | 2021-05-18 | Sony Corporation | Information processing apparatus, control method, and storage medium |
US9786299B2 (en) * | 2014-12-04 | 2017-10-10 | Microsoft Technology Licensing, Llc | Emotion type classification for interactive dialog system |
US20160182954A1 (en) | 2014-12-18 | 2016-06-23 | Rovi Guides, Inc. | Methods and systems for generating a notification |
WO2016160941A1 (en) | 2015-03-30 | 2016-10-06 | Twiin, Inc. | Systems and methods of generating consciousness affects |
CN104853257A (en) * | 2015-04-30 | 2015-08-19 | 北京奇艺世纪科技有限公司 | Subtitle display method and device |
US9571891B2 (en) * | 2015-06-30 | 2017-02-14 | Brooke Curtis PALMER | System and apparatus enabling conversation between audience and broadcast or live-streamed media |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1362682A (en) * | 2000-12-28 | 2002-08-07 | 卡西欧计算机株式会社 | Electronic book data transmitting apparatus, electronic book apparatus and recording medium |
CN1460185A (en) * | 2001-03-30 | 2003-12-03 | 皇家菲利浦电子有限公司 | Method and apparatus for audio-image speaker detection and location |
CN1500353A (en) * | 2001-04-03 | 2004-05-26 | ��̫�õ��ɷ�����˾ | Interactive media response processing system |
US20050289582A1 (en) * | 2004-06-24 | 2005-12-29 | Hitachi, Ltd. | System and method for capturing and using biometrics to review a product, service, creative work or thing |
WO2006126116A2 (en) * | 2005-05-23 | 2006-11-30 | Koninklijke Philips Electronics N.V. | Processing a user input signal for determining a content selection criterion |
US20070157239A1 (en) * | 2005-12-29 | 2007-07-05 | Mavs Lab. Inc. | Sports video retrieval method |
US20070214471A1 (en) * | 2005-03-23 | 2007-09-13 | Outland Research, L.L.C. | System, method and computer program product for providing collective interactive television experiences |
CN101454771A (en) * | 2006-03-31 | 2009-06-10 | 依玛奇灵控股有限公司 | System and method of segmenting and tagging entities based on profile matching using a multi-media survey |
Family Cites Families (43)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4646145A (en) * | 1980-04-07 | 1987-02-24 | R. D. Percy & Company | Television viewer reaction determining systems |
US5778135A (en) * | 1994-12-30 | 1998-07-07 | International Business Machines Corporation | Real-time edit control for video program material |
KR100347710B1 (en) * | 1998-12-05 | 2002-10-25 | 엘지전자주식회사 | Method and data structure for video browsing based on relation graph of characters |
US8302127B2 (en) * | 2000-09-25 | 2012-10-30 | Thomson Licensing | System and method for personalized TV |
US20060129458A1 (en) | 2000-10-12 | 2006-06-15 | Maggio Frank S | Method and system for interacting with on-demand video content |
GB2379016A (en) | 2001-07-27 | 2003-02-26 | Hewlett Packard Co | Portable apparatus monitoring reaction of user to music |
US6585521B1 (en) * | 2001-12-21 | 2003-07-01 | Hewlett-Packard Development Company, L.P. | Video indexing based on viewers' behavior and emotion feedback |
RU24303U1 (en) | 2002-02-12 | 2002-07-27 | Открытое акционерное общество "Телекомпания НТВ" | VOTING SYSTEM |
US8896575B2 (en) * | 2002-11-04 | 2014-11-25 | Neonode Inc. | Pressure-sensitive touch screen |
US8229888B1 (en) * | 2003-10-15 | 2012-07-24 | Radix Holdings, Llc | Cross-device playback with synchronization of consumption state |
JP3953024B2 (en) * | 2003-11-20 | 2007-08-01 | ソニー株式会社 | Emotion calculation device, emotion calculation method, and portable communication device |
US7672864B2 (en) | 2004-01-09 | 2010-03-02 | Ricoh Company Ltd. | Generating and displaying level-of-interest values |
US7844482B1 (en) * | 2006-02-28 | 2010-11-30 | Intuit Inc. | Mechanism for collecting feedback from users |
WO2007115224A2 (en) * | 2006-03-30 | 2007-10-11 | Sri International | Method and apparatus for annotating media streams |
US8572169B2 (en) * | 2006-08-28 | 2013-10-29 | Myspace, Llc | System, apparatus and method for discovery of music within a social network |
US8099084B2 (en) * | 2006-12-31 | 2012-01-17 | Ektimisi Semiotics Holdings, Llc | Method, system, and computer program product for creating smart services |
JP4539712B2 (en) * | 2007-12-03 | 2010-09-08 | ソニー株式会社 | Information processing terminal, information processing method, and program |
US8091103B2 (en) | 2007-07-22 | 2012-01-03 | Overlay.Tv Inc. | Server providing content directories of video signals and linkage to content information sources |
US8275764B2 (en) | 2007-08-24 | 2012-09-25 | Google Inc. | Recommending media programs based on media program popularity |
US8347326B2 (en) | 2007-12-18 | 2013-01-01 | The Nielsen Company (US) | Identifying key media events and modeling causal relationships between key events and reported feelings |
US8621502B2 (en) | 2007-12-21 | 2013-12-31 | Microsoft Corporation | Obtaining user reactions to video |
US7889073B2 (en) | 2008-01-31 | 2011-02-15 | Sony Computer Entertainment America Llc | Laugh detector and system and method for tracking an emotional response to a media presentation |
US9152258B2 (en) * | 2008-06-19 | 2015-10-06 | Neonode Inc. | User interface for a touch screen |
US8225348B2 (en) * | 2008-09-12 | 2012-07-17 | At&T Intellectual Property I, L.P. | Moderated interactive media sessions |
US8925001B2 (en) * | 2008-09-12 | 2014-12-30 | At&T Intellectual Property I, L.P. | Media stream generation based on a category of user expression |
US20110179385A1 (en) | 2008-09-24 | 2011-07-21 | Wencheng Li | Content classification utilizing a reduced description palette to simplify content analysis |
US9129008B1 (en) * | 2008-11-10 | 2015-09-08 | Google Inc. | Sentiment-based classification of media content |
KR101163010B1 (en) * | 2008-12-15 | 2012-07-09 | 한국전자통신연구원 | Apparatus for online advertisement selecting based on content affective and intend analysis and method thereof |
US8301444B2 (en) * | 2008-12-29 | 2012-10-30 | At&T Intellectual Property I, L.P. | Automated demographic analysis by analyzing voice activity |
US8386935B2 (en) | 2009-05-06 | 2013-02-26 | Yahoo! Inc. | Content summary and segment creation |
KR101604693B1 (en) * | 2009-07-01 | 2016-03-18 | 엘지전자 주식회사 | Mobile terminal and method for controlling multimedia contents thereof |
KR20110028834A (en) * | 2009-09-14 | 2011-03-22 | 삼성전자주식회사 | Method and apparatus for providing user interface using touch pressure on touch screen of mobile station |
US8438592B2 (en) * | 2009-12-22 | 2013-05-07 | Qualcomm Incorporated | Dynamic live content promoter for digital broadcast TV |
KR101084782B1 (en) * | 2010-05-06 | 2011-11-21 | 삼성전기주식회사 | Touch screen device |
US20110295848A1 (en) * | 2010-05-28 | 2011-12-01 | Peters Michael D | Computer-implemented system and method for determining a response to a stimulus |
US8650635B2 (en) * | 2010-12-16 | 2014-02-11 | Blackberry Limited | Pressure sensitive multi-layer passwords |
US8543454B2 (en) * | 2011-02-18 | 2013-09-24 | Bluefin Labs, Inc. | Generating audience response metrics and ratings from social interest in time-based media |
WO2012135048A2 (en) * | 2011-04-01 | 2012-10-04 | Votini Llc | Systems and methods for capturing event feedback |
US8508494B2 (en) * | 2011-06-01 | 2013-08-13 | Motorola Mobility Llc | Using pressure differences with a touch-sensitive display screen |
US9137573B2 (en) * | 2011-06-06 | 2015-09-15 | Netgear, Inc. | Systems and methods for managing media content based on segment-based assignment of content ratings |
US20150106155A1 (en) * | 2011-06-08 | 2015-04-16 | Maria Guadalupe Castellanos | Determining and Visualizing Social Media Expressed Sentiment |
US8976128B2 (en) * | 2011-09-12 | 2015-03-10 | Google Technology Holdings LLC | Using pressure differences with a touch-sensitive display screen |
KR101943436B1 (en) * | 2012-04-18 | 2019-01-31 | 삼성전자주식회사 | Pressure-type touch panel and portable terminal including the same |
-
2011
- 2011-08-08 US US13/205,240 patent/US8719277B2/en active Active
-
2012
- 2012-08-07 EP EP20120822175 patent/EP2742490A4/en not_active Withdrawn
- 2012-08-07 CN CN201280036367.8A patent/CN103703465A/en active Pending
- 2012-08-07 WO PCT/US2012/049838 patent/WO2013022879A1/en active Application Filing
- 2012-08-07 KR KR1020147003314A patent/KR101949308B1/en active IP Right Grant
-
2014
- 2014-03-17 US US14/216,591 patent/US20140207797A1/en not_active Abandoned
-
2019
- 2019-04-04 US US16/375,192 patent/US11080320B2/en active Active
-
2021
- 2021-08-02 US US17/391,551 patent/US11947587B2/en active Active
Patent Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1362682A (en) * | 2000-12-28 | 2002-08-07 | 卡西欧计算机株式会社 | Electronic book data transmitting apparatus, electronic book apparatus and recording medium |
CN1460185A (en) * | 2001-03-30 | 2003-12-03 | 皇家菲利浦电子有限公司 | Method and apparatus for audio-image speaker detection and location |
CN1500353A (en) * | 2001-04-03 | 2004-05-26 | ��̫�õ��ɷ�����˾ | Interactive media response processing system |
US20050289582A1 (en) * | 2004-06-24 | 2005-12-29 | Hitachi, Ltd. | System and method for capturing and using biometrics to review a product, service, creative work or thing |
US20070214471A1 (en) * | 2005-03-23 | 2007-09-13 | Outland Research, L.L.C. | System, method and computer program product for providing collective interactive television experiences |
WO2006126116A2 (en) * | 2005-05-23 | 2006-11-30 | Koninklijke Philips Electronics N.V. | Processing a user input signal for determining a content selection criterion |
US20070157239A1 (en) * | 2005-12-29 | 2007-07-05 | Mavs Lab. Inc. | Sports video retrieval method |
CN101454771A (en) * | 2006-03-31 | 2009-06-10 | 依玛奇灵控股有限公司 | System and method of segmenting and tagging entities based on profile matching using a multi-media survey |
Cited By (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2016029351A1 (en) * | 2014-08-26 | 2016-03-03 | 华为技术有限公司 | Method and terminal for processing media file |
CN104583924B (en) * | 2014-08-26 | 2018-02-02 | 华为技术有限公司 | A kind of method and terminal for handling media file |
CN104583924A (en) * | 2014-08-26 | 2015-04-29 | 华为技术有限公司 | Method and terminal for processing media file |
US10678427B2 (en) | 2014-08-26 | 2020-06-09 | Huawei Technologies Co., Ltd. | Media file processing method and terminal |
CN107209769A (en) * | 2014-12-31 | 2017-09-26 | 开放电视公司 | The metadata management transmitted for content |
CN107209769B (en) * | 2014-12-31 | 2021-08-31 | 开放电视公司 | Intermittent management for content delivery |
US10846710B2 (en) | 2015-04-07 | 2020-11-24 | International Business Machines Corporation | Rating aggregation and propagation mechanism for hierarchical services and products |
CN106056398A (en) * | 2015-04-07 | 2016-10-26 | 国际商业机器公司 | Method and system for collecting and presenting user feedback information for composite offering |
US10460328B2 (en) | 2015-04-07 | 2019-10-29 | International Business Machines Corporation | Rating aggregation and propagation mechanism for hierarchical services and products |
US10796319B2 (en) | 2015-04-07 | 2020-10-06 | International Business Machines Corporation | Rating aggregation and propagation mechanism for hierarchical services and products |
CN108293150A (en) * | 2015-12-22 | 2018-07-17 | 英特尔公司 | Mood timed media plays back |
CN109660853A (en) * | 2017-10-10 | 2019-04-19 | 腾讯科技（北京）有限公司 | Interactive approach, apparatus and system in net cast |
CN109660853B (en) * | 2017-10-10 | 2022-12-30 | 腾讯科技（北京）有限公司 | Interaction method, device and system in live video |
CN111414506A (en) * | 2020-03-13 | 2020-07-14 | 腾讯科技（深圳）有限公司 | Emotion processing method and device based on artificial intelligence, electronic equipment and storage medium |
CN111414506B (en) * | 2020-03-13 | 2023-09-19 | 腾讯科技（深圳）有限公司 | Emotion processing method and device based on artificial intelligence, electronic equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
EP2742490A1 (en) | 2014-06-18 |
KR20140063590A (en) | 2014-05-27 |
US20140207797A1 (en) | 2014-07-24 |
US20190228029A1 (en) | 2019-07-25 |
WO2013022879A1 (en) | 2013-02-14 |
KR101949308B1 (en) | 2019-02-18 |
US8719277B2 (en) | 2014-05-06 |
US20130041905A1 (en) | 2013-02-14 |
US11080320B2 (en) | 2021-08-03 |
EP2742490A4 (en) | 2015-04-08 |
US11947587B2 (en) | 2024-04-02 |
US20210357446A1 (en) | 2021-11-18 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN103703465A (en) | Sentimental information associated with object within media | |
Neuendorf | The content analysis guidebook | |
Egliston et al. | ‘The metaverse and how we’ll build it’: The political economy of Meta’s Reality Labs | |
Glynn et al. | CACTI: Free, open-source software for the sequential coding of behavioral interactions | |
CN109154935A (en) | The intelligence for the information completed for task is captured, stored and fetched | |
CN106255965A (en) | Automatic opinion for electrical form | |
CN109710799B (en) | Voice interaction method, medium, device and computing equipment | |
CN103488669A (en) | Information processing apparatus, information processing method and program | |
CN107810638A (en) | By the transmission for skipping redundancy fragment optimization order content | |
Moon et al. | Digital and mobile health technology in collaborative behavioral health care: scoping review | |
WO2017083205A1 (en) | Provide interactive content generation for document | |
CN111711865A (en) | Method, apparatus and storage medium for outputting data | |
WO2023056850A1 (en) | Page display method and apparatus, and device and storage medium | |
US20140136469A1 (en) | Logic model for media customization with item association | |
Porcaro et al. | Assessing the Impact of Music Recommendation Diversity on Listeners: A Longitudinal Study | |
Mateo Navarro et al. | Run-time model based framework for automatic evaluation of multimodal interfaces | |
US20140108329A1 (en) | Logic model for media customization | |
CN111858920A (en) | User group matching method and device, terminal equipment and storage medium | |
Papadakis | A digital elearning educational tool library for synchronization composition & orchestration of learning session data. | |
Sehgal et al. | The Benefits of Crowdsourcing to Seed and Align an Algorithm in an mHealth Intervention for African American and Hispanic Adults: Survey Study | |
CN116440382B (en) | Autism intervention system and method based on multilayer reinforcement strategy | |
Soares et al. | Design, User Experience, and Usability: UX Research, Design, and Assessment: 11th International Conference, DUXU 2022, Held as Part of the 24th HCI International Conference, HCII 2022, Virtual Event, June 26–July 1, 2022, Proceedings, Part I | |
WO2024069741A1 (en) | Software technological field extraction device and software technological field extraction method | |
Karavellas et al. | Local Crowdsourcing for Annotating Audio: the Elevator Annotator platform | |
de Andrade et al. | Edge emotion recognition: applying fast Fourier transform on speech Mel spectrograms to classify emotion on a Raspberry Pi for near real-time analytics |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
WD01 | Invention patent application deemed withdrawn after publication | ||
WD01 | Invention patent application deemed withdrawn after publication |
Application publication date: 20140402 |