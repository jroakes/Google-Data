US5977978A - Interactive authoring of 3D scenes and movies - Google Patents
Interactive authoring of 3D scenes and movies Download PDFInfo
- Publication number
- US5977978A US5977978A US08/748,613 US74861396A US5977978A US 5977978 A US5977978 A US 5977978A US 74861396 A US74861396 A US 74861396A US 5977978 A US5977978 A US 5977978A
- Authority
- US
- United States
- Prior art keywords
- camera
- user
- scene
- stage
- camera path
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Lifetime
Links
- 230000002452 interceptive effect Effects 0.000 title claims description 13
- 230000008676 import Effects 0.000 claims abstract description 12
- 238000000034 method Methods 0.000 claims description 59
- 239000000463 material Substances 0.000 claims description 18
- 238000004590 computer program Methods 0.000 claims description 14
- 238000009877 rendering Methods 0.000 claims description 11
- 241000238876 Acari Species 0.000 claims description 6
- 239000000203 mixture Substances 0.000 claims description 5
- 230000000007 visual effect Effects 0.000 claims description 5
- 230000000295 complement effect Effects 0.000 claims description 3
- 239000000779 smoke Substances 0.000 claims description 3
- 230000004048 modification Effects 0.000 claims 1
- 238000012986 modification Methods 0.000 claims 1
- NJPPVKZQTLUDBO-UHFFFAOYSA-N novaluron Chemical compound C1=C(Cl)C(OC(F)(F)C(OC(F)(F)F)F)=CC=C1NC(=O)NC(=O)C1=C(F)C=CC=C1F NJPPVKZQTLUDBO-UHFFFAOYSA-N 0.000 abstract description 9
- 230000008859 change Effects 0.000 description 8
- 239000003086 colorant Substances 0.000 description 6
- 230000008901 benefit Effects 0.000 description 5
- 230000000694 effects Effects 0.000 description 5
- XUIMIQQOPSSXEZ-UHFFFAOYSA-N Silicon Chemical compound [Si] XUIMIQQOPSSXEZ-UHFFFAOYSA-N 0.000 description 3
- 230000008569 process Effects 0.000 description 3
- 229910052710 silicon Inorganic materials 0.000 description 3
- 239000010703 silicon Substances 0.000 description 3
- 238000007796 conventional method Methods 0.000 description 2
- 238000005286 illumination Methods 0.000 description 2
- 238000012545 processing Methods 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 238000012549 training Methods 0.000 description 2
- 101001022148 Homo sapiens Furin Proteins 0.000 description 1
- 101000701936 Homo sapiens Signal peptidase complex subunit 1 Proteins 0.000 description 1
- 102100030313 Signal peptidase complex subunit 1 Human genes 0.000 description 1
- 230000001133 acceleration Effects 0.000 description 1
- 238000000429 assembly Methods 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 239000010438 granite Substances 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 229910052754 neon Inorganic materials 0.000 description 1
- GKAOGPIIYCISHV-UHFFFAOYSA-N neon atom Chemical compound [Ne] GKAOGPIIYCISHV-UHFFFAOYSA-N 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002829 reductive effect Effects 0.000 description 1
- 230000000284 resting effect Effects 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
- G06T15/20—Perspective computation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2200/00—Indexing scheme for image data processing or generation, in general
- G06T2200/24—Indexing scheme for image data processing or generation, in general involving graphical user interfaces [GUIs]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2210/00—Indexing scheme for image generation or computer graphics
- G06T2210/61—Scene description
Definitions
- This invention relates to the interactive authoring of 3D scenes and movies using a computer system.
- a 3D scene typically comprises one or more 3D graphic objects placed in a background setting to create an informative or aesthetically pleasing image.
- Such 3D scenes may be used in generating image data files (e.g., GIF or JPEG files) or web pages (e.g., HTML or VRML files), or may be strung together in a sequence to create an animated 3D movie (e.g., MPEG-1 or Quicktime files).
- FIG. 2 shows an example of a 3D scene in which a teapot 41 rests on a horizontal surface 43 with a vertical surface 45 appearing in the background.
- a 3D scene also may embody characteristics, other than its graphic elements, that affect how the scene appears to a human viewer.
- the scene of FIG. 2 for example, is illuminated by three separate spot light sources positioned at locations respectively behind the viewer over the left and right shoulders, and in front of and above the viewer. Different types, locations and numbers of light sources may be used as desired to create different effects.
- scene characteristics include the orientation and position of the graphic objects in the scene relative to the viewer's eye point, which may be thought of as governed by what would be seen through a camera.
- the camera may be moved through virtual 3D space (forward, back, side-to-side, rotating around an object, etc.) to change how the scene appears to the viewer.
- virtual refers to the fact that the 3D scene, the space that it occupies and its graphic elements and characteristics, are conceptual rather than actual, but possess the functional (e.g., optical) properties of the physical entities that they represent.
- the camera in the 3D scene of FIG. 2 is oriented in virtual 3D space such that the viewer is presented with a side view of the teapot 41, with the viewer's eye positioned at a point slightly above the teapot and looking down at it.
- the graphic objects in a 3D scene each may have a color, texture and/or material composition which will affect how the 3D scene appears to the viewer.
- the colors, textures and material compositions of the objects in the scene of FIG. 2 are such that the three light sources create bright reflection spots 47 and shadows 49 on the teapot 41 and on the horizontal surface 43.
- FIG. 2 shows how the scene of FIG. 2 appears when it is rendered without the benefit of ray tracing.
- Examples of programming tools that may be used to create 3D scenes include Silicon Graphics' "Open Inventor Toolkit” (described in Josie Wernecke, The Inventor Mentor, Addison-Wesley (1994); Josie Wernecke, The Inventor Toolmaker, Addison-Wesley (1994), and Open Inventor C++ Reference Manual, Addison-Wesley (1994)); "OpenGL” (described in Jackie Neider et al., OpenGL Programming Guide, Addison-Wesley (1993); and Pixar's "Renderman” (described in Steve Upstill, The Renderman Companion, Addison-Wesley (1990)).
- An author is provided with a set of predefined environments, or "stages,” into which models can be imported and later showcased for review and presentations. These stages may include preset colors, textures, lighting and camera paths in 3D space to create photorealistic ray-traced results with just a few items of input from the author, for example, by means of a graphical user interface ("GUI"). These stages also may include a "drop zone" at which 3D objects appear when imported into the scene, and which is optimally positioned within the stage to take advantage of its predefined lighting and other characteristics.
- GUI graphical user interface
- Authors are provided with an intuitive GUI through which they can interactively specify a path in 3D space for a virtual camera to travel as it renders successive views of a 3D scene.
- the GUI includes visual tools that allow the author to modify the camera path as desired, to specify the direction in which the camera looks, and to add pauses and timing markers to control the rate at which the camera moves along its specified path.
- three-dimensional scenes or movies may be generated by a user of a computer system by interactively selecting a stage from among several available 3D stages, each of which has at least one predetermined feature such as a pedestal, lighting characteristics, a camera path or the like.
- the user may import predefined 3D objects into the selected stage and move, rotate or resize the objects as desired to compose a scene.
- the 3D objects may be static or self-animated. Once composed, the scene is rendered to generate an image or a series of successive scenes is rendered to generate an animated movie.
- the selecting of a stage may be performed by a user of the computer system.
- the user may be presented with a visual interface having graphical abstractions to allow the user to communicate with the computer system.
- this enables the user of the computer system to modify the predetermined feature of the selected 3D stage.
- the predetermined feature of the selected 3D stage may be a functional element or an aesthetic element, a combination of both, or some other type of element.
- the predetermined feature of the selected 3D stage may include lighting characteristics for the 3D scene, or a headlight that moves in synchronization with a viewpoint from which a user of the computer system views the stage.
- the predetermined feature of the selected 3D stage may comprise a platform on which the 3D object may be placed.
- the predetermined feature of the selected 3D stage may be a texture, a color or a material composition, for example, on one or more of the stage's graphical elements.
- the predetermined feature also could be an environment (e.g., fog, haze, pollution or smoke) in which the 3D object is displayed, or a home view position with respect to which the 3D object is rendered.
- the predetermined feature of the selected stage may be a camera path that defines a series of positions in 3D virtual space along which a virtual camera travels while rendering views of the scene.
- a stage may include two or more predetermined features which are a combination of any of the above.
- each 3D stage Prior to being selected by the user, each 3D stage is defined such that its features are tailored to complement each other. This allows each stage to represent a different atmosphere in which 3D objects may be displayed.
- a 3D object Upon being imported into a stage, a 3D object is placed at a predetermined drop zone, for example, a virtual 3D region, within the selected 3D stage.
- the drop zone may be a bounding box that encompasses the imported 3D object, which is automatically scaled to fit within the bounding box upon importation.
- the 3D object may be selectively imported at a location other than at the drop zone, or imported without being automatically scaled, or both.
- the virtual camera looks at the drop zone at all points along the camera path, although the user is able to modify the virtual camera's orientation.
- the user may manipulate (e.g., translate, rotate, scale) the 3D objects that have been imported into a scene or the 3D objects that make up a stage or both.
- a user of the computer system may interactively define (i) a camera path in virtual 3D space along which a virtual camera travels; and (ii) a variable rate at which the camera moves along the camera path. These parameters may be specified independently of each other by the user. Virtual camera views of the 3D scene are rendered based on the camera path and the variable rate defined by the user.
- the user-defined variable rate at which the camera moves along the camera path may include pause information that causes the virtual camera to pause for a specified duration when the virtual camera reaches a specified location on the camera path, time tick information that causes the virtual camera to arrive at a specified location on the camera path at a specified time, or both. This information may cause the virtual camera to accelerate or decelerate as it moves along the camera path.
- Advantages of this invention may include one or more of the following.
- the visually based tools provided by this application program provide users with an intuitive environment in which 3D graphic content may be developed.
- the tools are simple to use yet sophisticated in their functionality.
- Creators of 3D scenes need not learn and use complicated programming techniques to achieve professional looking results. Rather, authors need only learn a few basic skills, such as standard cursor manipulation techniques, to create complex 3D scenes on their own. As a result, personnel training time is reduced while the quality of output is enhanced.
- predefined stages provide a number of predefined stages from which users may select enables authors, even inexperienced ones, to quickly and easily generate sophisticated and aesthetically pleasing 3D scenes.
- Each predefined stage presents a different environment designed for various purposes (e.g., marketing presentations, engineering presentations, artistic renderings) and in different artistic styles (modern, gothic, art-deco, etc.).
- the availability of predefined stages allows users to create visually rich scenes simply by adding and/or arranging a few 3D objects and associated characteristics such as lighting, texture, color and the like. This ability to generate sophisticated images in an expedited manner applies to both experienced and inexperienced authors.
- Each predefined stage includes a default camera path that is tailored to the stage's features. If the default stage suits the user's needs, making a movie may be accomplished with just a few cursor manipulations. If the user wants to modify the default camera path, or create an entirely new camera path, the Camera Path Editor presents the user with an intuitive environment in which to do so. The user need only visually manipulate (i.e., drag in the desired direction) in 3D virtual space the camera points that define the camera path.
- FIG. 1 shows a computer system displaying a 3D image.
- FIG. 2 shows a 3D scene rendered using ray tracing.
- FIG. 3 shows the 3D scene of FIG. 2 rendered without ray tracing.
- FIG. 4 shows the basic structure of a program for 3D scene generation.
- FIG. 5 shows a screen display of the main window in the LightBoxTM application.
- FIG. 6 shows how components may be combined to form a typical 3D scene.
- FIGS. 7(a), 7(b) and 7(c) show example screen displays of a predefined stage and a 3D object within the drop zone of the predefined stage.
- FIGS. 8(a) through 8(l) show screen displays of the 12 predefined stages provided by the LightBoxTM application.
- FIGS. 9(a) and 9(b) show respective screen displays of the Light Editor window and the Environment Editor window in the LightBoxTM application.
- FIGS. 10(a) and 10(b) show example 3D scenes respectively rendered without fog and with fog.
- FIG. 11 shows a screen display of the Color Editor window in the LightBoxTM application.
- FIG. 12(a) and 12(b) show respective screen displays of the Material Palette window and the Material Editor window in the LightBoxTM application.
- FIG. 13 shows a screen display of the Texture Editor window in the LightBoxTM application.
- FIG. 14 shows a screen display of the Render Options dialog box in the LightBoxTM application.
- FIG. 15 shows a screen display of the Camera Path Editor window in the LightBoxTM application.
- FIGS. 16(a) and 16(b) show screen displays of the Camera Path Editor showing a default camera path from top and right side views, respectively.
- FIGS. 17(a) and 17(b) show screen displays of the Camera Path Editor showing top and right side views, respectively, of a camera path that has been modified by a user.
- the techniques described here were implemented on Silicon Graphics machines using the Open Inventor Toolkit, Motif, OpenGL, and the C++ programming language and apply to any computer system that has a display device such as CRT 33.
- the 3D graphic objects may be self-animated objects or static objects (i.e., non-self-animated).
- Self-animated objects are objects that change their state (e.g., appearance) based on the value of a global parameter (e.g., time or position of a virtual camera).
- An example of a self-animated object is an image of a windmill whose blades spin at a constant rate in correspondence with the passage of global time.
- the Open Inventor file format facilitates the use of self-animated objects as described in The Inventor Mentor, supra.
- the user can manipulate the Dolly thumbwheel 57 to move the camera (i.e., the user's viewpoint) closer to or farther from the scene as desired.
- the user can cause the camera to translate (move side-to-side, up or down) relative to the scene by manipulating the Roty and Rotx thumbwheels 61 and 63, respectively.
- Full details on the use and operation of widgets 55-73 is provided in appendix A.
- a mouse is used to manipulate widgets 55-73 although any other input device could be used for this purpose.
- the 3D scene is redrawn in real time using the input received from the user as appropriate.
- the 3D object 41 typically is a model (a data structure that describes how the object appears in 3D virtual space) that was previously generated in a separate program.
- the user imports one or more of these models into a predefined stage selected by the user.
- the stage 77 is a collection of one or more graphic objects and associated characteristics that provide a backdrop for the 3D object (or objects) being displayed.
- the user may pick, from among several different predefined stages provided by the LightBoxTM application, a stage that accentuates or otherwise complements the 3D object being showcased.
- a user can create and use a new stage, either by modifying one of the existing predefined stages, or by building one from scratch. In either case, the stage serves as a template into which the user imports 3D models to create a new scene.
- stage 77 in FIG. 6 includes platform 75 resting on a horizontal surface 43, vertical surface 45, and several inherent characteristics such as lighting, texture, color, home viewpoint (the default camera position relative to the scene), all of which affect how the scene appears to the viewer.
- a model upon being imported into a stage may be automatically scaled to be as large as possible while still fitting within the drop zone and while still retaining its original proportions.
- the drop zone typically acts as a focal point for the scene.
- stage is designed so that the drop zone is positioned at a strategic location that is optimized for the scene's lighting and other characteristics.
- a user may selectively disable the auto-scaling of 3D objects upon importation, and may further import objects at locations (e.g., a location specified by a model) other than the drop zone.
- locations e.g., a location specified by a model
- These capabilities are useful when the user is importing sub-assemblies that were modeled relative to one another. Upon importing the parts into the scene, they may be properly re-assembled, which may not otherwise be possible if the parts were re-scaled or re-oriented relative to each other.
- model manipulation operations are accomplished by interactive cursor manipulation techniques with the assistance of visual clues (e.g., manipulator box, directional arrows, knobs) provided by the LightBoxTM application. Complete details are provided in appendix A.
- the user may selectively turn on/off the display of the drop zone 81 so that the wire-frame box 83 and the reference pattern are visible/invisible.
- the default setting is that the drop zone is not displayed. This feature may be desirable when the user prefers that the scene have a less cluttered appearance while it is being constructed. Although the drop zone may be invisible, it remains present logically and functionally within the stage.
- the drop zone feature provides several advantages. For example, because models are automatically scaled when they are imported into a drop zone, the user is free to import virtually any model into the scene without regard to its size. Once imported, the model will always fit nicely within the scene and will have a size that is similar to any other model that is imported into the scene. Moreover, a stage's drop zone typically is positioned at a location that is optimized for the scene characteristics--for example, the scene's lighting and home viewpoint. As a result, any model imported into a stage, if left within the drop zone, also will be positioned at a point within the stage that is optimal for the scene's characteristics. This feature is particularly important for lighting which typically is one of the most difficult characteristics to define in generating a scene.
- the LightBoxTM application provides the user with 12 different predefined stages (FIGS. 8(a) through 8(l)) that may be used in creating 3D scenes.
- Each stage includes different graphic objects so that the user may select the one that best suits his or her needs.
- a user who wishes to create a 3D scene for purposes of marketing a consumer product may choose a stage that has aesthetic elements--for example, features relating to a certain artistic style (e.g., modern, arts-and-crafts, impressionistic), bold colors or aesthetically appealing features to attract the attention of consumers, as in FIG. 8(l).
- an engineer or scientist who wishes to create a 3D scene to convey detailed technical information in a precise manner may choose a stage that includes functional elements such as a ruler or grid lines (e.g., FIG. 8(k)).
- a stage that includes functional elements such as a ruler or grid lines (e.g., FIG. 8(k)).
- Each of the predefined stages provided by LightBoxTM in addition to its drop zone and any graphic objects (e.g., a platform or a ruler) it may contain, has one or more associated characteristics, or default features, that affect how the scene appears to the user. These characteristics include lighting, environment, background color, texture, material, home viewpoint and a camera path, each of which is described below.
- the LightBoxTM application provides the user with a Light Editor window 87, as shown in FIG. 9(a), which allows the user to specify, through interactive cursor manipulation techniques, the type, number, color, and position or direction of the light sources that are used to illuminate the 3D scene.
- Three types of light sources are available to the user to add to a scene: a spot light 93, a directional light 95, and a point light 91.
- a fourth type of light source, a headlight 89 is present in every scene, although the Light Editor allows it to be turned on and off by the user.
- the headlight is a white light that illuminates a scene from the point at which the camera views the scene and which moves automatically with the camera as the user changes the viewpoint.
- a spot light provides a cone of light that diverges from the light's location and illuminates from a point along a primary direction.
- Spot lights have two characteristics that the user may vary by means of sliders 101 and 103: drop-off rate (the rate at which light intensity drops off with increased distance from the spot light's primary direction) and cut-off angle (the threshold angle beyond which the light intensity is zero).
- a directional light is infinitely far away from the scene and illuminates uniformly along a particular direction.
- the user may specify the direction along which a directional light illuminates by positioning arrow 107 relative to sphere 105 in the Direction box 109.
- a point light has a specific location in space and radiates equally in all directions from its location. Point lights and spot lights may be moved by the user to new locations relative to the scene by appropriate manipulation of the icons that represent the active light sources in the scene.
- the color of any light may be changed by the user as desired. To do so, the user moves the cursor to the desired location on color wheel 97 within Color box 96. The user may adjust the intensity of the chosen color with slider 99. The illumination from the selected light changes color in the scene in real time as the user moves the cursor within color wheel 97.
- any light other than the headlight may be removed from the scene by the user by clicking on the Remove button 111.
- any light also may be turned on or off without removing it permanently from the scene by alternately clicking on the "Light On" box 112.
- the user may simulate fog, haze, pollution or smoke in the scene by using the Environment Editor window 113 (FIG. 9(b)). If fog or the like is desired, the user manipulates the widgets in the Environment Editor window 113 to select values of visibility and fog color and intensity to achieve the desired effect.
- FIG. 10(a) A scene without fog is shown in FIG. 10(a) and the same scene with fog is shown in FIG. 10(b).
- the color of an object in a scene may be changed by using the Color Editor window 115 shown in FIG. 11. To do so, the user first selects the object under consideration by clicking on it with the cursor, clicks the cursor at a location within color wheel 117 that corresponds to the desired color, and adjusts the intensity (V) of the selected color with slider 119.
- the material from which an object in a scene appears to be made may be selected from among 442 predefined materials (glass, metal, neon, etc.) provided by LightBoxTM and shown in the Material Palette window 121 (FIG. 12(a)), which shows one of the 13 palettes provided by LightBoxTM.
- Each palette defines 34 materials, plus two default materials, from which the user may select.
- a user may define a custom material with the Material Editor window 123 (FIG. 12(b)) by specifying values that define how light is reflected by the material.
- the Material Editor window 123 allows the user to specify values relating to ambient light, diffuse light, specular light, emissive light, shininess, transparency, and the refractive index of the material.
- a user may apply a texture to an object within a scene by using the Texture Editor window 125 shown in FIG. 13.
- LightBoxTM provides the user with various other options that affect how the 3D scene appears.
- the user may set the home viewpoint (i.e., the default view that will be displayed upon opening a saved scene file) by manipulating the Dolly, Rotx, Roty thumbwheels 57, 61, 63 until the desired view is obtained and then selecting the "set home" button from among the Examiner Viewer Tools (FIG. 5).
- the user also may select different lens types (wide-angle, normal, telephoto), zoom levels, resolutions, and transparency settings to change the appearance of the scene. Appendix A describes these features in detail.
- LightBoxTM provides several "draw commands" in the menu bar 73 which allow the user to specify how the scene's imported 3D objects or its stage, or both, are displayed on the computer screen while the user is building the scene. For example, rather the displaying a scene with its full complement of visual features, the user may selectively display the imported objects or the stage in a scene in wire-frame form, as a series of dots, with no texture, or not at all (i.e., invisible) to minimize screen clutter and the time it takes for the computer to refresh the screen.
- FIGS. 8(a) through 8(l) show the 12 predefined stages provided by LightBoxTM.
- the stage in FIG. 8(a), named "Simple,” includes a cubical drop zone 800 that has an "X and O" reference pattern 802 at its base and which is positioned on a floor 806 with a wall 808 in the background.
- the Simple stage includes a spot light whose parameters are set to create a soft glow on an object 810 (not part of Simple stage) that has been imported into the stage and which remains positioned within the drop zone 800.
- the light fades off relatively quickly as the distance from the center of the drop zone increases, thereby drawing a viewer's attention to the center of the drop zone.
- the home viewpoint (i.e., the default camera view) of the Simple stage is set such that the viewer has a top side view of an object positioned within the drop zone.
- the "New Simple” stage shown in FIG. 8(b) is similar to the Simple stage except that it has modified lighting characteristics to create a slightly different effect.
- each of the predefined stages may use different colors for lights and objects to create the desired atmosphere. For example, the Simple stage uses a green light to illuminate the drop zone whereas the New Simple stage uses a white light for that purpose.
- the "Simple Pedestal” stage (FIG. 8(c)) is substantially similar to the Simple stage except that it adds a red, cylindrical pedestal underneath the drop zone 800 on which imported object 810 rests.
- the "New Simple Pedestal” stage in FIG. 8(d) differs from the Simple Pedestal stage in that the "new" version includes a tall, metallic green pedestal 814 having beveled edges 816 along its top surface.
- the New Simple Pedestal stage resents the drop zone in a side angle view by default, and directional lighting is used to illuminate the entire scene uniformly. In addition, a green granite texture is applied to the floor 806. .
- FIGS. 8(e) and 8(f) show the "Bowl” and “New Bowl” stages respectively, in which the imported object 810 is placed at the bottom of a large bowl 819 as shown in FIG. 8(f).
- the Bowl stage in FIG. 8(e) uses a point light directly above and focused on the drop zone 800 to illuminate the imported object 810 whereas the New Bowl stage (FIG. 8(f)) illuminates object 810 indirectly with four spot lights of different colors aimed at the sides of the bowl.
- object 810 is surrounded by a manipulator box 820 that has knobs at its apexes so the user may readily manipulate (translate, rotate, scale) the imported object.
- the manipulator box 820 which is available to the user regardless of the particular stage or scene being viewed, is not part of the stage but rather appears when the user "selects" an object by clicking on it with the mouse. Any object in the scene, including the graphic elements that make up the stage, may be selected and manipulated in this manner.
- the "Dome” stage shown in FIG. 8(g) is similar to the Bowl stage in FIG. 8(e) in that it presents the drop zone in a side angle view and uses a spot light directly above the drop zone for illumination.
- the Dome stage is dome shaped rather than bowl shaped.
- the dividing line 822 between the dome and the floor appears slightly curved thus giving a viewer the sense that the object 810 (an X-29 aircraft) is sitting on the floor of a large hangar that has a domed roof.
- the "New Plainfloor" stage in FIG. 8(h) places the drop zone 800 within a large open room or box 824.
- the viewpoint shown in FIG. 8(h) is at a position above and a relatively far distance from the drop zone 800.
- the default viewpoint (not shown) is at a point inside of the box closer to the drop zone.
- Directional lighting illuminates the interior of box 824 uniformly.
- FIGS. 8(i) and 8(j) show two different stages having tiled, checkerboard floors.
- the "Tile Floor” stage (FIG. 8(i)) uses alternating green tiles 826 and white tiles 828 to give a bolder effect while the "New Tile Floor” stage in FIG. 8(j) uses alternating tiles 830, 832 in slightly different shades of gray to give a more subdued effect.
- the lighting characteristics of the stages also vary somewhat to alter the atmosphere between the two.
- the "New Furniture” stage (FIG. 8(k) and the “Modern Table” stage (FIG. 8(l)) are the most elaborate of the 12 predefined stages.
- the New Furniture stage includes functional features such as a ruler 834 to allow measurement of the object (shown within manipulator box 836), for example, as it is being resized.
- the New Furniture stage also includes several objects, such as table 838 and chair 840, that add to the aesthetics of the stage while at the same time providing a frame of reference for qualitatively gauging the size of an imported object.
- the home viewpoint (not shown) of the New Furniture stage is at a distance from the drop zone that allows the user to see the table 838 and chair 840 in their entirety.
- the Modern Table stage presents an artistic and somewhat whimsical stage in which objects may be placed to build a scene.
- the varied shapes and colors of the objects used in this stage suggest that it is designed as a template for generating aesthetically pleasing, as opposed to informative or functional, scenes.
- each predefined stage provided by LightBoxTM comes with its own set of default characteristics and graphic objects, these may be changed or set by the user as desired by editing an existing stage or by creating a new stage.
- the user can add or remove objects from the stage and can change any of the above-described characteristics as desired.
- LightBoxTM redraws the scene in real time to reflect the changes.
- the user starts with a blank screen and adds objects and/or characteristics as desired.
- the user may save the customized stage in a new stage file which can be later invoked to build 3D scenes. The user also may delete unwanted stage files.
- Render Options dialog box 127 (FIG. 14) which allows the user to specify various parameters relating to file format (e.g., JPEG), image resolution, render quality (ray traced or draft) and antialiasing.
- file format e.g., JPEG
- render quality ray traced or draft
- antialiasing e.g., ray traced or draft
- the Render Options dialog box 127 also allows the user to specify parameters relating to animation length--a feature that has meaning when LightBoxTM is being used to generate animated movies as described below.
- An animated movie is a series of rendered images of a scene displayed in succession. Typically, each successive scene in a movie represents a relatively minor change from its predecessor to give the viewer the impression of motion, change of state, passage of time or the like.
- LightBoxTM provides several tools that allow a user to create movies easily and quickly with interactive GUI techniques.
- a camera path is a course in 3D virtual space that defines a series of logically connected viewpoints of a scene.
- An animated movie is the display of the images corresponding to the respective viewpoints one after another in their predefined logical order.
- Each of the predefined stages provided by LightBoxTM has an associated default camera path.
- the Camera Path Editor window 129 presents a user with an intuitive, graphical representation of a stage's camera path.
- the user is able to change the Camera Path Editor's vantage point to view either top or side views of a camera path.
- various widgets Examiner Viewer Tools 55 and Camera Path Editor Tools 150
- items from menu bar 74 provided by the Camera Path Editor the user is able to quickly and easily modify a camera path in an interactive manner.
- It is a continuous circular path 131, defined by camera points 139, that sits at a uniform height 133 above the floor 135 of the stage.
- the virtual camera represented by camera icon 137, travels along the camera path at variable, user-specified speeds and continuously renders new views of the stage (and any objects that may have been imported into the stage).
- the camera is oriented so that it looks at the center of drop zone 141 at all points along the camera path.
- a stage's lighting and other characteristics are predefined to accentuate the drop zone at which the camera points.
- a user need only import a 3D object into the drop zone to generate a high quality movie.
- Viewpoints 143 each represent a single viewpoint that the user has selected in creating a single image of a scene. Because these single viewpoints 143 are not part of the camera path 131, the views represented by points 143 will not appear in any movie made using the camera path 131 of FIGS. 16(a) and 16(b). Rather, viewpoints 143 typically will be rendered as single still images for display by the user apart from any movie associated with the same scene.
- the user manipulates controls 145 as desired. If the user presses the "play" key, the camera begins to travel along its specified path--a circle in the case of FIGS. 16(a) and (b). As it does so, an arrow 147 moves along a scrub bar 149 in a manner representative of the camera's movement along the camera path. More particularly, the arrow 147 moves in proportion with the time elapsed as the camera moves along its path. For example, in FIGS. 16(a) and (b), the camera path 131 contains 17 unique camera points 139.
- Each of these camera points is represented by a corresponding one of the 18 blue tick marks above the scrub bar 149, with the leftmost and rightmost tick marks, 151 and 153, respectively, representing the same camera point--namely, the camera point at which the camera icon appears in FIGS. 16(a) and (b).
- the arrow 147 moves from its location at tick mark 151 to tick mark 153 with the same timing that camera icon 137 moves to an adjacent camera point 139.
- new views of the scene are rendered in real time in the display region of the main window 51 shown in FIG. 5.
- the number of views that are rendered along the camera path typically is considerably greater than the number of camera points in the camera path.
- the camera points serve as "control points” that define the shape of the camera path. View are rendered all along the path depending on the speed of the camera motion and the desired number of frames per second as specified by the user.
- the "Increment (frames)" field 162 of the Camera Path Editor window (FIG. 15) allows the user to specify how many frames to skip when stepping along the camera path using the "Step Forward” and "Step Reverse” buttons.
- FIGS. 17(a) (top view) and 17(b) (right side view) for example, the user has altered (using standard point-and-click cursor manipulation techniques, for example) the camera path 131 by moving camera points 139 in toward the center (FIG. 17(a)) and down towards the floor 135 (FIG. 17(b)) along the right side of the path.
- a new camera path 155 which is no longer circular nor at a constant height above floor 135, is generated.
- the movie corresponding to camera path 155 in FIGS. 17(a) and (b) will appear the same as the movie that corresponds to camera path 131 in FIGS. 16(a) and 16(b) until the camera icon 137 enters region 157--the region in which the user has modified the camera path from its original form.
- region 157 the user will be presented with movie images in which the camera, while it rotates around the drop zone 141, appears to move closer to the drop zone.
- the camera will continue to move toward the drop zone until the camera icon 137 reaches point 159.
- the successive movie images displayed will be such that the camera appears to move gradually away from drop zone until a distance corresponding to the original camera path 131 is reached.
- the user has several other options for modifying the camera path. Additional camera points may be added or unwanted camera points may be deleted as desired by the user.
- An entirely new camera path may be created by starting with a null camera path (i.e., one that contains no camera points), manually generating successive views of the model in the manner describe above and adding each of the viewpoints as they are created to the camera path.
- the user may change the "look at" point for any of the camera points to cause the camera to point toward a location in the scene other than at the drop zone.
- the user also may insert "time ticks" which appear as red tick marks 161 below the scrub bar 149 at the specified locations in the camera path (FIG. 15). Time ticks allow the user to set a time at which the camera will arrive at the specified point in the camera path.
- Pause ticks 163 also are available to the user to place pauses in the camera path. A pause tick 163 appears as a red horizontal red line below the scrub bar 149, with the length of the line corresponding to the length of the pause.
- the speed at which the camera moves along its path at a given time may be varied by the user.
- the camera does not move at constant velocity but instead obeys the directives placed by the time and pause ticks. If a successive time ticks instruct the camera to be at certain locations along the path at certain times, then the camera may have to slow down and/or speed up in order to arrive at each tick precisely at the user-specified time.
- the algorithms that govern the camera motion use mathematical techniques to insure that the acceleration and deceleration of the camera proceed smoothly. These algorithms are defined in Foley, van Dam, Feiner and Hughes, Computer Graphics: Principles and Practice, Addison-Wesley (1990), pp. 483-88, which is incorporated by reference.
- a movie file may be rendered by using the Render Options dialog box 127 (FIG. 14) in the manner described above.
- the user may specify the start and end times ("Animation Length Options") for the sequence to be rendered.
- the start and end times correspond to the specified time tick marks and may be used to generate a movie file that includes fewer than all of the camera views potentially available in the camera path.
- the techniques described here may be implemented in hardware or software, or a combination of the two.
- the techniques are implemented in computer programs executing on programmable computers that each include a processor, a storage medium readable by the processor (including volatile and non-volatile memory and/or storage elements), at least one input device, and two or more output devices.
- Program code is applied to data entered using the input device to perform the functions described and to generate output information.
- the output information is applied to one or more output devices.
- Each program is preferably implemented in a high level procedural or object oriented programming language to communicate with a computer system.
- the programs can be implemented in assembly or machine language, if desired.
- the language may be a compiled or interpreted language.
- Each such computer program is preferably stored on a storage medium or device (e.g., CD-ROM, hard disk or magnetic diskette) that is readable by a general or special purpose programmable computer for configuring and operating the computer when the storage medium or device is read by the computer to perform the procedures described in this document.
- a storage medium or device e.g., CD-ROM, hard disk or magnetic diskette
- the system may also be considered to be implemented as a computer-readable storage medium, configured with a computer program, where the storage medium so configured causes a computer to operate in a specific and predefined manner.
Abstract
Description
Claims (53)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US08/748,613 US5977978A (en) | 1996-11-13 | 1996-11-13 | Interactive authoring of 3D scenes and movies |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US08/748,613 US5977978A (en) | 1996-11-13 | 1996-11-13 | Interactive authoring of 3D scenes and movies |
Publications (1)
Publication Number | Publication Date |
---|---|
US5977978A true US5977978A (en) | 1999-11-02 |
Family
ID=25010186
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US08/748,613 Expired - Lifetime US5977978A (en) | 1996-11-13 | 1996-11-13 | Interactive authoring of 3D scenes and movies |
Country Status (1)
Country | Link |
---|---|
US (1) | US5977978A (en) |
Cited By (55)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2000028478A1 (en) * | 1998-11-05 | 2000-05-18 | Computer Associates Think, Inc. | Method and apparatus for interfacing with intelligent three-dimensional components |
US6100895A (en) * | 1994-12-01 | 2000-08-08 | Namco Ltd. | Apparatus and method of image synthesization |
US6124857A (en) * | 1998-08-12 | 2000-09-26 | International Business Machines Corporation | Meshing method and apparatus |
US6141013A (en) * | 1998-02-03 | 2000-10-31 | Sun Microsystems, Inc. | Rapid computation of local eye vectors in a fixed point lighting unit |
US6222546B1 (en) * | 1996-07-25 | 2001-04-24 | Kabushiki Kaisha Sega Enterprises | Image processing device, image processing method, game device, and craft simulator |
US6262739B1 (en) * | 1996-10-16 | 2001-07-17 | Real-Time Geometry Corporation | System and method for computer modeling of 3D objects or surfaces by mesh constructions having optimal quality characteristics and dynamic resolution capabilities |
US20010050778A1 (en) * | 2000-05-08 | 2001-12-13 | Hiroaki Fukuda | Method and system for see-through image correction in image duplication |
US6331858B2 (en) * | 1997-04-16 | 2001-12-18 | British Telecommunications Public Limited Company | Display terminal user interface with ability to select remotely stored surface finish for mapping onto displayed 3-D surface |
US6377255B1 (en) * | 1997-01-24 | 2002-04-23 | Sony Corporation | Pattern data generator, pattern data generating method and its medium |
US6452905B1 (en) | 1995-03-08 | 2002-09-17 | British Telecommunications Public Limited Company | Broadband switching system |
US6466231B1 (en) * | 1998-08-07 | 2002-10-15 | Hewlett-Packard Company | Appliance and method of using same for capturing images |
US20030011604A1 (en) * | 2001-06-20 | 2003-01-16 | Capers Walter E. | Method and apparatus for capturing and viewing a sequence of 3-D images |
WO2003007272A1 (en) * | 2001-07-11 | 2003-01-23 | Simsurgery As | Systems and methods for interactive training of procedures |
US6667741B1 (en) * | 1997-12-24 | 2003-12-23 | Kabushiki Kaisha Sega Enterprises | Image generating device and image generating method |
US6727925B1 (en) * | 1999-12-20 | 2004-04-27 | Michelle Lyn Bourdelais | Browser-based room designer |
US6791572B1 (en) * | 1999-06-18 | 2004-09-14 | Phoenix Technologies Ltd. | Generating media output during BIOS boot-up |
US20040239763A1 (en) * | 2001-06-28 | 2004-12-02 | Amir Notea | Method and apparatus for control and processing video images |
US20050131857A1 (en) * | 2003-10-17 | 2005-06-16 | Canon Kabushiki Kaisha | Information processing method and image processing method |
US20050171964A1 (en) * | 1999-05-21 | 2005-08-04 | Kulas Charles J. | Creation and playback of computer-generated productions using script-controlled rendering engines |
US20060119618A1 (en) * | 2001-11-09 | 2006-06-08 | Knighton Mark S | Graphical interface for manipulation of 3D models |
US7139970B2 (en) * | 1998-04-10 | 2006-11-21 | Adobe Systems Incorporated | Assigning a hot spot in an electronic artwork |
US20070030283A1 (en) * | 2005-08-02 | 2007-02-08 | Seiko Epson Corporation | Image display method and device, image display system, server, program, and recording medium |
US20070211269A1 (en) * | 2006-03-07 | 2007-09-13 | Canon Information Systems Research Australia Pty Ltd | Print presentation |
US20070262986A1 (en) * | 1998-12-19 | 2007-11-15 | Kabushiki Kaisha Sega Enterprises | Image generating device and image generating method |
US20080181486A1 (en) * | 2007-01-26 | 2008-07-31 | Conversion Works, Inc. | Methodology for 3d scene reconstruction from 2d image sequences |
US20080246836A1 (en) * | 2004-09-23 | 2008-10-09 | Conversion Works, Inc. | System and method for processing video images for camera recreation |
US20080259073A1 (en) * | 2004-09-23 | 2008-10-23 | Conversion Works, Inc. | System and method for processing video images |
AU2004201263B2 (en) * | 2003-03-28 | 2009-06-18 | Saab Ab | Presentation surface and method for indicating a sequence of events on the presentation surface |
US20090153648A1 (en) * | 2007-12-13 | 2009-06-18 | Apple Inc. | Three-dimensional movie browser or editor |
US20090231284A1 (en) * | 2008-03-14 | 2009-09-17 | Darfon Electronics Corp. | Portable Electronic Device and Program Image Selecting Method |
US20090256903A1 (en) * | 2004-09-23 | 2009-10-15 | Conversion Works, Inc. | System and method for processing video images |
US20090290773A1 (en) * | 2008-05-21 | 2009-11-26 | Varian Medical Systems, Inc. | Apparatus and Method to Facilitate User-Modified Rendering of an Object Image |
US20100142801A1 (en) * | 2008-12-09 | 2010-06-10 | Microsoft Corporation | Stereo Movie Editing |
US20110052043A1 (en) * | 2009-08-25 | 2011-03-03 | Samsung Electronics Co., Ltd. | Method of mobile platform detecting and tracking dynamic objects and computer-readable medium thereof |
US20110096083A1 (en) * | 2009-10-26 | 2011-04-28 | Stephen Schultz | Method for the automatic material classification and texture simulation for 3d models |
US20120194518A1 (en) * | 2009-08-18 | 2012-08-02 | Sony Computer Entertainment Inc. | Content Creation Supporting Apparatus, Image Processing Device, Content Creation Supporting Method, Image Processing Method, And Data Structure of Image Display Content. |
US20120249746A1 (en) * | 2011-03-28 | 2012-10-04 | Cornog Katherine H | Methods for detecting, visualizing, and correcting the perceived depth of a multicamera image sequence |
US20130073619A1 (en) * | 2011-09-15 | 2013-03-21 | Ramakrishna J. Tumuluri | System and method for collaborative 3D visualization and real-time interaction on a computer network. |
US20140043340A1 (en) * | 2012-08-10 | 2014-02-13 | Microsoft Corporation | Animation Transitions and Effects in a Spreadsheet Application |
US20140135986A1 (en) * | 2012-11-14 | 2014-05-15 | Fanuc America Corporation | Teaching point program selection method for robot simulator |
US8791941B2 (en) | 2007-03-12 | 2014-07-29 | Intellectual Discovery Co., Ltd. | Systems and methods for 2-D to 3-D image conversion using mask to model, or model to mask, conversion |
US8930844B2 (en) * | 2000-08-22 | 2015-01-06 | Bruce Carlin | Network repository of digitalized 3D object models, and networked generation of photorealistic images based upon these models |
US9001128B2 (en) | 2011-05-06 | 2015-04-07 | Danglesnort, Llc | Efficient method of producing an animated sequence of images |
CN104802513A (en) * | 2014-01-23 | 2015-07-29 | 海德堡印刷机械股份公司 | Quality control method for printed product using 3d digital proofing |
US9519986B1 (en) * | 2013-06-20 | 2016-12-13 | Pixar | Using stand-in camera to determine grid for rendering an image from a virtual camera |
US20170206680A1 (en) * | 2014-07-16 | 2017-07-20 | Koninklijke Philips N.V. | Irecon: intelligent image reconstruction system with anticipatory execution |
US20180182168A1 (en) * | 2015-09-02 | 2018-06-28 | Thomson Licensing | Method, apparatus and system for facilitating navigation in an extended scene |
US10311630B2 (en) * | 2017-05-31 | 2019-06-04 | Verizon Patent And Licensing Inc. | Methods and systems for rendering frames of a virtual scene from different vantage points based on a virtual entity description frame of the virtual scene |
US10347037B2 (en) | 2017-05-31 | 2019-07-09 | Verizon Patent And Licensing Inc. | Methods and systems for generating and providing virtual reality data that accounts for level of detail |
US20190244435A1 (en) * | 2018-02-06 | 2019-08-08 | Adobe Inc. | Digital Stages for Presenting Digital Three-Dimensional Models |
US20190287302A1 (en) * | 2018-03-13 | 2019-09-19 | Canon Kabushiki Kaisha | System and method of controlling a virtual camera |
US10586377B2 (en) | 2017-05-31 | 2020-03-10 | Verizon Patent And Licensing Inc. | Methods and systems for generating virtual reality data that accounts for level of detail |
US10602200B2 (en) | 2014-05-28 | 2020-03-24 | Lucasfilm Entertainment Company Ltd. | Switching modes of a media content item |
US10685679B1 (en) | 2018-11-27 | 2020-06-16 | Canon Kabushiki Kaisha | System and method of determining a virtual camera path |
US20200380771A1 (en) * | 2019-05-30 | 2020-12-03 | Samsung Electronics Co., Ltd. | Method and apparatus for acquiring virtual object data in augmented reality |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5307452A (en) * | 1990-09-21 | 1994-04-26 | Pixar | Method and apparatus for creating, manipulating and displaying images |
-
1996
- 1996-11-13 US US08/748,613 patent/US5977978A/en not_active Expired - Lifetime
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5307452A (en) * | 1990-09-21 | 1994-04-26 | Pixar | Method and apparatus for creating, manipulating and displaying images |
Non-Patent Citations (25)
Title |
---|
Alias Wavefront, Advanced Visualizer User s Guide, Version 4.2 , Silicon Graphics, Inc., pp. i xxiv, 1 1 1 11, 13 26, 14 1 14 23,. Oct. 1995. * |
Alias Wavefront, Design Product Line (http://www.alias.com/General/Overview/product line.html). * |
Alias Wavefront, Explore Anim, Display and Mapper, Version 4.2, Silicon Graphics, Inc., Nov. 1995. * |
Alias Wavefront, Power Animator V7 (http://www.alias.com/Film Video/PowerAnimator/PowerAnimator1.html) through http://www.alias.com/Film Video/PowerAnimator/PowerAnimator4.html). * |
Alias|Wavefront, "Design Product Line" (http://www.alias.com/General/Overview/product-- line.html). |
Alias|Wavefront, "Power Animator V7" (http://www.alias.com/Film-- Video/PowerAnimator/PowerAnimator1.html) through http://www.alias.com/Film-- Video/PowerAnimator/PowerAnimator4.html). |
Alias|Wavefront, Advanced Visualizer User's Guide, Version 4.2, Silicon Graphics, Inc., pp. i-xxiv, 1-1-1-11, 13-26, 14-1-14-23,. Oct. 1995. |
Alias|Wavefront, Explore Anim, Display and Mapper, Version 4.2, Silicon Graphics, Inc., Nov. 1995. |
Biedny, David, "The Third Dimension", MacUser, vol. 8, No. 9, p. 114(12), Sep. 1992 (Abstract No. 01528969 with full text). |
Biedny, David, The Third Dimension , MacUser , vol. 8, No. 9, p. 114(12), Sep. 1992 (Abstract No. 01528969 with full text). * |
CGW, "Product Spotlight: Pixar Introduces RenderMan, Intros Three New Products", Nov. 1995 (http://www.cgw.com/cgw/Archives/1998/11/11prod1-- 01.html). |
CGW, Product Spotlight: Pixar Introduces RenderMan, Intros Three New Products , Nov. 1995 (http://www.cgw.com/cgw/Archives/1998/11/11prod1 01.html). * |
Corel Dream 3D (Abstract and screenshot). * |
Jakman, Mike and RoseAnn Alspektor, "Pixar Announces Showplace", Business Wire Report (No. 0157260 with full text). |
Jakman, Mike and RoseAnn Alspektor, Pixar Announces Showplace , Business Wire Report (No. 0157260 with full text). * |
Murie, Michael, "ShowPlace Brings 3-D into View", MacWEEK, vol. 5, No. 34, p. 54(1), Oct. 8, 1991 (Abstract No. 05466716 with full text). |
Murie, Michael, ShowPlace Brings 3 D into View , MacWEEK , vol. 5, No. 34, p. 54(1), Oct. 8, 1991 (Abstract No. 05466716 with full text). * |
Pixar, "Pixar Typestry 2.1" (http://www.macpi.org/typestry.html). |
Pixar, "Pixar Typestry 2.1" (http://www.macworld.com/pages/february95/Reviews.174html). |
Pixar, "Pixar's RenderMan", (http://www.pixar.com/renderman/rm-- info.html). |
Pixar, "The RenderMan Interface Specification, Version 3.1", Sep. 1989, (http://giga.cps.unizar.es/prman/RISpec/Index.html to http://giga.cps.unizar.es/prman/Toolkit/errata2.html). |
Pixar, Pixar s RenderMan , (http://www.pixar.com/renderman/rm info.html). * |
Pixar, Pixar Typestry 2.1 (http://www.macpi.org/typestry.html). * |
Pixar, Pixar Typestry 2.1 (http://www.macworld.com/pages/february95/Reviews.174html). * |
Pixar, The RenderMan Interface Specification, Version 3.1 , Sep. 1989, (http://giga.cps.unizar.es/prman/RISpec/Index.html to http://giga.cps.unizar.es/prman/Toolkit/errata2.html). * |
Cited By (117)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6100895A (en) * | 1994-12-01 | 2000-08-08 | Namco Ltd. | Apparatus and method of image synthesization |
US6452905B1 (en) | 1995-03-08 | 2002-09-17 | British Telecommunications Public Limited Company | Broadband switching system |
US6222546B1 (en) * | 1996-07-25 | 2001-04-24 | Kabushiki Kaisha Sega Enterprises | Image processing device, image processing method, game device, and craft simulator |
US6731278B2 (en) | 1996-07-25 | 2004-05-04 | Kabushiki Kaisha Sega Enterprises | Image processing device, image processing method, game device, and craft simulator |
US6392647B1 (en) | 1996-10-16 | 2002-05-21 | Viewpoint Corporation | System and method for computer modeling of 3D objects or surfaces by mesh constructions having optimal quality characteristics and dynamic resolution capabilities |
US6262739B1 (en) * | 1996-10-16 | 2001-07-17 | Real-Time Geometry Corporation | System and method for computer modeling of 3D objects or surfaces by mesh constructions having optimal quality characteristics and dynamic resolution capabilities |
US6611267B2 (en) | 1996-10-16 | 2003-08-26 | Viewpoint Corporation | System and method for computer modeling of 3D objects or surfaces by mesh constructions having optimal quality characteristics and dynamic resolution capabilities |
US6377255B1 (en) * | 1997-01-24 | 2002-04-23 | Sony Corporation | Pattern data generator, pattern data generating method and its medium |
US6331858B2 (en) * | 1997-04-16 | 2001-12-18 | British Telecommunications Public Limited Company | Display terminal user interface with ability to select remotely stored surface finish for mapping onto displayed 3-D surface |
US6667741B1 (en) * | 1997-12-24 | 2003-12-23 | Kabushiki Kaisha Sega Enterprises | Image generating device and image generating method |
US6141013A (en) * | 1998-02-03 | 2000-10-31 | Sun Microsystems, Inc. | Rapid computation of local eye vectors in a fixed point lighting unit |
US7949943B2 (en) | 1998-04-10 | 2011-05-24 | Adobe Systems Incorporated | Assigning a hot spot in an electronic artwork |
US20070130501A1 (en) * | 1998-04-10 | 2007-06-07 | Adobe Systems Incorporated | Assigning a hot spot in an electronic artwork |
US7139970B2 (en) * | 1998-04-10 | 2006-11-21 | Adobe Systems Incorporated | Assigning a hot spot in an electronic artwork |
US8819544B2 (en) | 1998-04-10 | 2014-08-26 | Adobe Systems Incorporated | Assigning a hot spot in an electronic artwork |
US6466231B1 (en) * | 1998-08-07 | 2002-10-15 | Hewlett-Packard Company | Appliance and method of using same for capturing images |
US6124857A (en) * | 1998-08-12 | 2000-09-26 | International Business Machines Corporation | Meshing method and apparatus |
AU770038B2 (en) * | 1998-11-05 | 2004-02-12 | Computer Associates Think, Inc. | Method and apparatus for interfacing with intelligent three-dimensional components |
WO2000028478A1 (en) * | 1998-11-05 | 2000-05-18 | Computer Associates Think, Inc. | Method and apparatus for interfacing with intelligent three-dimensional components |
US6947037B1 (en) * | 1998-11-05 | 2005-09-20 | Computer Associates Think, Inc. | Method and apparatus for interfacing with intelligent three-dimensional components |
US20070262986A1 (en) * | 1998-12-19 | 2007-11-15 | Kabushiki Kaisha Sega Enterprises | Image generating device and image generating method |
US7479958B1 (en) * | 1998-12-19 | 2009-01-20 | Kabushiki Kaisha Sega Enterprises | Image generating device and image generating method |
US8717359B2 (en) * | 1999-05-21 | 2014-05-06 | Quonsil Pl. 3, Llc | Script control for camera positioning in a scene generated by a computer rendering engine |
US8674996B2 (en) | 1999-05-21 | 2014-03-18 | Quonsil Pl. 3, Llc | Script control for lip animation in a scene generated by a computer rendering engine |
US20090189989A1 (en) * | 1999-05-21 | 2009-07-30 | Kulas Charles J | Script control for camera positioning in a scene generated by a computer rendering engine |
US8199150B2 (en) | 1999-05-21 | 2012-06-12 | Quonsil Pl. 3, Llc | Multi-level control language for computer animation |
US20050171964A1 (en) * | 1999-05-21 | 2005-08-04 | Kulas Charles J. | Creation and playback of computer-generated productions using script-controlled rendering engines |
US20090189906A1 (en) * | 1999-05-21 | 2009-07-30 | Kulas Charles J | Script control for gait animation in a scene generated by a computer rendering engine |
US7830385B2 (en) | 1999-05-21 | 2010-11-09 | Kulas Charles J | Script control for gait animation in a scene generated by a computer rendering engine |
US20090184967A1 (en) * | 1999-05-21 | 2009-07-23 | Kulas Charles J | Script control for lip animation in a scene generated by a computer rendering engine |
US6791572B1 (en) * | 1999-06-18 | 2004-09-14 | Phoenix Technologies Ltd. | Generating media output during BIOS boot-up |
US6727925B1 (en) * | 1999-12-20 | 2004-04-27 | Michelle Lyn Bourdelais | Browser-based room designer |
US7064863B2 (en) * | 2000-05-08 | 2006-06-20 | Ricoh Company, Ltd. | Method and system for see-through image correction in image duplication |
US20010050778A1 (en) * | 2000-05-08 | 2001-12-13 | Hiroaki Fukuda | Method and system for see-through image correction in image duplication |
US8930844B2 (en) * | 2000-08-22 | 2015-01-06 | Bruce Carlin | Network repository of digitalized 3D object models, and networked generation of photorealistic images based upon these models |
US20030011604A1 (en) * | 2001-06-20 | 2003-01-16 | Capers Walter E. | Method and apparatus for capturing and viewing a sequence of 3-D images |
US6894690B2 (en) | 2001-06-20 | 2005-05-17 | Engineering Technology Associates, Inc. | Method and apparatus for capturing and viewing a sequence of 3-D images |
US20080109729A1 (en) * | 2001-06-28 | 2008-05-08 | Amir Notea | Method and apparatus for control and processing of video images |
US20040239763A1 (en) * | 2001-06-28 | 2004-12-02 | Amir Notea | Method and apparatus for control and processing video images |
WO2003007272A1 (en) * | 2001-07-11 | 2003-01-23 | Simsurgery As | Systems and methods for interactive training of procedures |
US20060119618A1 (en) * | 2001-11-09 | 2006-06-08 | Knighton Mark S | Graphical interface for manipulation of 3D models |
AU2004201263B2 (en) * | 2003-03-28 | 2009-06-18 | Saab Ab | Presentation surface and method for indicating a sequence of events on the presentation surface |
US7834890B2 (en) * | 2003-10-17 | 2010-11-16 | Canon Kabushiki Kaisha | Information processing method and image processing method |
US20050131857A1 (en) * | 2003-10-17 | 2005-06-16 | Canon Kabushiki Kaisha | Information processing method and image processing method |
US20090256903A1 (en) * | 2004-09-23 | 2009-10-15 | Conversion Works, Inc. | System and method for processing video images |
US20110169827A1 (en) * | 2004-09-23 | 2011-07-14 | Conversion Works, Inc. | System and method for processing video images |
US20080246836A1 (en) * | 2004-09-23 | 2008-10-09 | Conversion Works, Inc. | System and method for processing video images for camera recreation |
US8860712B2 (en) | 2004-09-23 | 2014-10-14 | Intellectual Discovery Co., Ltd. | System and method for processing video images |
US20080259073A1 (en) * | 2004-09-23 | 2008-10-23 | Conversion Works, Inc. | System and method for processing video images |
US8217931B2 (en) | 2004-09-23 | 2012-07-10 | Conversion Works, Inc. | System and method for processing video images |
US7583265B2 (en) * | 2005-08-02 | 2009-09-01 | Seiko Epson Corporation | Image display method and device, image display system, server, program, and recording medium |
US20070030283A1 (en) * | 2005-08-02 | 2007-02-08 | Seiko Epson Corporation | Image display method and device, image display system, server, program, and recording medium |
US20070211269A1 (en) * | 2006-03-07 | 2007-09-13 | Canon Information Systems Research Australia Pty Ltd | Print presentation |
US8107141B2 (en) * | 2006-03-07 | 2012-01-31 | Canon Information Systems Research Australia Pty. Ltd. | Print presentation |
US20080181486A1 (en) * | 2007-01-26 | 2008-07-31 | Conversion Works, Inc. | Methodology for 3d scene reconstruction from 2d image sequences |
US8655052B2 (en) | 2007-01-26 | 2014-02-18 | Intellectual Discovery Co., Ltd. | Methodology for 3D scene reconstruction from 2D image sequences |
US8878835B2 (en) | 2007-03-12 | 2014-11-04 | Intellectual Discovery Co., Ltd. | System and method for using feature tracking techniques for the generation of masks in the conversion of two-dimensional images to three-dimensional images |
US9082224B2 (en) | 2007-03-12 | 2015-07-14 | Intellectual Discovery Co., Ltd. | Systems and methods 2-D to 3-D conversion using depth access segiments to define an object |
US8791941B2 (en) | 2007-03-12 | 2014-07-29 | Intellectual Discovery Co., Ltd. | Systems and methods for 2-D to 3-D image conversion using mask to model, or model to mask, conversion |
US20090153648A1 (en) * | 2007-12-13 | 2009-06-18 | Apple Inc. | Three-dimensional movie browser or editor |
US8395660B2 (en) * | 2007-12-13 | 2013-03-12 | Apple Inc. | Three-dimensional movie browser or editor |
US20090231284A1 (en) * | 2008-03-14 | 2009-09-17 | Darfon Electronics Corp. | Portable Electronic Device and Program Image Selecting Method |
WO2009143346A3 (en) * | 2008-05-21 | 2010-03-04 | Varian Medical Systems Inc. | Apparatus and method to facilitate user-modified rendering of an object image |
WO2009143346A2 (en) * | 2008-05-21 | 2009-11-26 | Varian Medical Systems Inc. | Apparatus and method to facilitate user-modified rendering of an object image |
US20090290773A1 (en) * | 2008-05-21 | 2009-11-26 | Varian Medical Systems, Inc. | Apparatus and Method to Facilitate User-Modified Rendering of an Object Image |
US20100142801A1 (en) * | 2008-12-09 | 2010-06-10 | Microsoft Corporation | Stereo Movie Editing |
US8330802B2 (en) * | 2008-12-09 | 2012-12-11 | Microsoft Corp. | Stereo movie editing |
US20120194518A1 (en) * | 2009-08-18 | 2012-08-02 | Sony Computer Entertainment Inc. | Content Creation Supporting Apparatus, Image Processing Device, Content Creation Supporting Method, Image Processing Method, And Data Structure of Image Display Content. |
US8649557B2 (en) * | 2009-08-25 | 2014-02-11 | Samsung Electronics Co., Ltd. | Method of mobile platform detecting and tracking dynamic objects and computer-readable medium thereof |
US20110052043A1 (en) * | 2009-08-25 | 2011-03-03 | Samsung Electronics Co., Ltd. | Method of mobile platform detecting and tracking dynamic objects and computer-readable medium thereof |
US9330494B2 (en) * | 2009-10-26 | 2016-05-03 | Pictometry International Corp. | Method for the automatic material classification and texture simulation for 3D models |
US20110096083A1 (en) * | 2009-10-26 | 2011-04-28 | Stephen Schultz | Method for the automatic material classification and texture simulation for 3d models |
US8654181B2 (en) * | 2011-03-28 | 2014-02-18 | Avid Technology, Inc. | Methods for detecting, visualizing, and correcting the perceived depth of a multicamera image sequence |
US20120249746A1 (en) * | 2011-03-28 | 2012-10-04 | Cornog Katherine H | Methods for detecting, visualizing, and correcting the perceived depth of a multicamera image sequence |
US9001128B2 (en) | 2011-05-06 | 2015-04-07 | Danglesnort, Llc | Efficient method of producing an animated sequence of images |
US9424677B2 (en) | 2011-05-06 | 2016-08-23 | Danglesnort, Llc | Efficient method of producing an animated sequence of images |
US20130073619A1 (en) * | 2011-09-15 | 2013-03-21 | Ramakrishna J. Tumuluri | System and method for collaborative 3D visualization and real-time interaction on a computer network. |
US8935328B2 (en) * | 2011-09-15 | 2015-01-13 | Ramakrishna J Tumuluri | System and method for collaborative 3D visualization and real-time interaction on a computer network |
CN104520854A (en) * | 2012-08-10 | 2015-04-15 | 微软公司 | Animation transitions and effects in a spreadsheet application |
US20140043340A1 (en) * | 2012-08-10 | 2014-02-13 | Microsoft Corporation | Animation Transitions and Effects in a Spreadsheet Application |
US10008015B2 (en) | 2012-08-10 | 2018-06-26 | Microsoft Technology Licensing, Llc | Generating scenes and tours in a spreadsheet application |
US9996953B2 (en) | 2012-08-10 | 2018-06-12 | Microsoft Technology Licensing, Llc | Three-dimensional annotation facing |
US9317963B2 (en) | 2012-08-10 | 2016-04-19 | Microsoft Technology Licensing, Llc | Generating scenes and tours in a spreadsheet application |
US9881396B2 (en) | 2012-08-10 | 2018-01-30 | Microsoft Technology Licensing, Llc | Displaying temporal information in a spreadsheet application |
US9339932B2 (en) * | 2012-11-14 | 2016-05-17 | Fanuc America Corporation | Teaching point program selection method for robot simulator |
CN103809463A (en) * | 2012-11-14 | 2014-05-21 | 发纳科机器人美国公司 | Teaching point program selection method for robot simulator |
DE102013112516B4 (en) | 2012-11-14 | 2022-07-07 | Fanuc Robotics America Corporation | Teaching point program selection process for robot simulator |
JP2014097569A (en) * | 2012-11-14 | 2014-05-29 | Fanuc Robotics America Inc | Teaching point program selection method for robot simulator |
US20140135986A1 (en) * | 2012-11-14 | 2014-05-15 | Fanuc America Corporation | Teaching point program selection method for robot simulator |
US9519986B1 (en) * | 2013-06-20 | 2016-12-13 | Pixar | Using stand-in camera to determine grid for rendering an image from a virtual camera |
US10192342B1 (en) * | 2013-06-20 | 2019-01-29 | Pixar | Using stand-in camera to determine grid for rendering an image from a virtual camera |
CN104802513B (en) * | 2014-01-23 | 2017-11-21 | 海德堡印刷机械股份公司 | 3D numeral specimen pages |
CN104802513A (en) * | 2014-01-23 | 2015-07-29 | 海德堡印刷机械股份公司 | Quality control method for printed product using 3d digital proofing |
US10602200B2 (en) | 2014-05-28 | 2020-03-24 | Lucasfilm Entertainment Company Ltd. | Switching modes of a media content item |
US11508125B1 (en) * | 2014-05-28 | 2022-11-22 | Lucasfilm Entertainment Company Ltd. | Navigating a virtual environment of a media content item |
US10600245B1 (en) * | 2014-05-28 | 2020-03-24 | Lucasfilm Entertainment Company Ltd. | Navigating a virtual environment of a media content item |
US10275906B2 (en) * | 2014-07-16 | 2019-04-30 | Koninklijke Philips N.V. | iRecon: intelligent image reconstruction system with anticipatory execution |
US20190139271A1 (en) * | 2014-07-16 | 2019-05-09 | Koninklijke Philips N.V. | Irecon: intelligent image reconstruction system with anticipatory execution |
US20170206680A1 (en) * | 2014-07-16 | 2017-07-20 | Koninklijke Philips N.V. | Irecon: intelligent image reconstruction system with anticipatory execution |
US11017895B2 (en) * | 2014-07-16 | 2021-05-25 | Koninklijke Philips N.V. | Irecon: intelligent image reconstruction system with anticipatory execution |
US20180182168A1 (en) * | 2015-09-02 | 2018-06-28 | Thomson Licensing | Method, apparatus and system for facilitating navigation in an extended scene |
US11699266B2 (en) * | 2015-09-02 | 2023-07-11 | Interdigital Ce Patent Holdings, Sas | Method, apparatus and system for facilitating navigation in an extended scene |
US10891781B2 (en) | 2017-05-31 | 2021-01-12 | Verizon Patent And Licensing Inc. | Methods and systems for rendering frames based on virtual entity description frames |
US10586377B2 (en) | 2017-05-31 | 2020-03-10 | Verizon Patent And Licensing Inc. | Methods and systems for generating virtual reality data that accounts for level of detail |
US10311630B2 (en) * | 2017-05-31 | 2019-06-04 | Verizon Patent And Licensing Inc. | Methods and systems for rendering frames of a virtual scene from different vantage points based on a virtual entity description frame of the virtual scene |
US10699471B2 (en) | 2017-05-31 | 2020-06-30 | Verizon Patent And Licensing Inc. | Methods and systems for rendering frames based on a virtual entity description frame of a virtual scene |
US10347037B2 (en) | 2017-05-31 | 2019-07-09 | Verizon Patent And Licensing Inc. | Methods and systems for generating and providing virtual reality data that accounts for level of detail |
US10803653B2 (en) | 2017-05-31 | 2020-10-13 | Verizon Patent And Licensing Inc. | Methods and systems for generating a surface data projection that accounts for level of detail |
US20190251737A1 (en) * | 2017-05-31 | 2019-08-15 | Verizon Patent And Licensing Inc. | Methods and Systems for Rendering Frames Based on a Virtual Entity Description Frame of a Virtual Scene |
US11244518B2 (en) | 2018-02-06 | 2022-02-08 | Adobe Inc. | Digital stages for presenting digital three-dimensional models |
US20190244435A1 (en) * | 2018-02-06 | 2019-08-08 | Adobe Inc. | Digital Stages for Presenting Digital Three-Dimensional Models |
US10740981B2 (en) * | 2018-02-06 | 2020-08-11 | Adobe Inc. | Digital stages for presenting digital three-dimensional models |
US10902676B2 (en) * | 2018-03-13 | 2021-01-26 | Canon Kabushiki Kaisha | System and method of controlling a virtual camera |
US20190287302A1 (en) * | 2018-03-13 | 2019-09-19 | Canon Kabushiki Kaisha | System and method of controlling a virtual camera |
US10685679B1 (en) | 2018-11-27 | 2020-06-16 | Canon Kabushiki Kaisha | System and method of determining a virtual camera path |
US20200380771A1 (en) * | 2019-05-30 | 2020-12-03 | Samsung Electronics Co., Ltd. | Method and apparatus for acquiring virtual object data in augmented reality |
US11682171B2 (en) * | 2019-05-30 | 2023-06-20 | Samsung Electronics Co.. Ltd. | Method and apparatus for acquiring virtual object data in augmented reality |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US5977978A (en) | Interactive authoring of 3D scenes and movies | |
Mullen | Mastering blender | |
Deering | HoloSketch: a virtual reality sketching/animation tool | |
EP0870284B1 (en) | Interactive image editing | |
Deering | The HoloSketch VR sketching system | |
EP2828831B1 (en) | Point and click lighting for image based lighting surfaces | |
CN106415667A (en) | Computer graphics with enhanced depth effect | |
Harper | Mastering Autodesk 3ds Max 2013 | |
Guevarra | Modeling and animation using blender: blender 2.80: the rise of Eevee | |
Bikmullina et al. | The development of 3D object modeling techniques for use in the unity environmen | |
JPH10222701A (en) | Computer graphic device and generating method for image data | |
Guevarra | Modeling and Animation Using Blender | |
Guevarra et al. | Blending with Blender: Getting Started | |
Abbasov | A Fascinating Journey into the World of 3D Graphics with 3ds Max | |
Gee | 3D in Photoshop: the ultimate guide for creative professionals | |
Gerhard et al. | Mastering Autodesk 3ds Max Design 2010 | |
Cox | Simulation studio | |
Eräniitty | Creating a game scene in Unity | |
US20030043145A1 (en) | Three dimensional depth cue for selected data | |
CN107292943B (en) | Ceramic product design method based on three-dimensional animation technology | |
Kol et al. | Expressive single scattering for light shaft stylization | |
CN117788689A (en) | Interactive virtual cloud exhibition hall construction method and system based on three-dimensional modeling | |
Glanville | Anim8or | |
Havemann et al. | 3D-Powerpoint-Towards a Design Tool for Digital Exhibitions of Cultural Artifacts. | |
McFarland | Mastering Autodesk VIZ 2008 |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: SILICON GRAPHICS, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CAREY, RICHARD JOSEPH;IMMEL, DAVID STANLEY;STRAUSS, PAUL S.;AND OTHERS;REEL/FRAME:008403/0467;SIGNING DATES FROM 19961219 TO 19961230 |
|
AS | Assignment |
Owner name: PLATINUM TECHNOLOGY, INC., ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SILICON GRAPHICS, INC.;REEL/FRAME:009678/0261Effective date: 19981223Owner name: PLATINUM TECHNOLOGY IP, INC., ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:PLATINUM TECHNOLOGY, INC.;REEL/FRAME:009670/0906Effective date: 19981223 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: COMPUTER ASSOCIATES THINK, INC., NEW YORKFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:PLATINUM TECHNOLOGY IP, INC.;REEL/FRAME:010351/0936Effective date: 19991028 |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FEPP | Fee payment procedure |
Free format text: PAYOR NUMBER ASSIGNED (ORIGINAL EVENT CODE: ASPN); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
FPAY | Fee payment |
Year of fee payment: 12 |
|
FEPP | Fee payment procedure |
Free format text: PAYER NUMBER DE-ASSIGNED (ORIGINAL EVENT CODE: RMPN); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYFree format text: PAYOR NUMBER ASSIGNED (ORIGINAL EVENT CODE: ASPN); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:COMPUTER ASSOCIATES THINK, INC.;REEL/FRAME:028801/0049Effective date: 20120330 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044213/0313Effective date: 20170929 |