JP2023539020A - Entering computing device interaction mode using off-screen gesture detection - Google Patents
Entering computing device interaction mode using off-screen gesture detection Download PDFInfo
- Publication number
- JP2023539020A JP2023539020A JP2023505446A JP2023505446A JP2023539020A JP 2023539020 A JP2023539020 A JP 2023539020A JP 2023505446 A JP2023505446 A JP 2023505446A JP 2023505446 A JP2023505446 A JP 2023505446A JP 2023539020 A JP2023539020 A JP 2023539020A
- Authority
- JP
- Japan
- Prior art keywords
- computing device
- gesture
- mobile computing
- identifying
- housing
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/1694—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being a single or a set of motion sensors for pointer control or gesture input obtained by sensing movements of the portable computer
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1626—Constructional details or arrangements for portable computers with a single-body enclosure integrating a flat display, e.g. Personal Digital Assistants [PDAs]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/169—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being an integrated pointing device, e.g. trackball in the palm rest area, mini-joystick integrated between keyboard keys, touch pads or touch stripes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/033—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor
- G06F3/0346—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor with detection of the device orientation or free movement in a 3D space, e.g. 3D mice, 6-DOF [six degrees of freedom] pointers using gyroscopes, accelerometers or tilt-sensors
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/033—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor
- G06F3/0354—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor with detection of 2D relative movements between the device, or an operating part thereof, and a plane or surface, e.g. 2D mice, trackballs, pens or pucks
- G06F3/03547—Touch pads, in which fingers can move on a surface
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04847—Interaction techniques to control parameter settings, e.g. interaction with sliders or dials
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04883—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures for inputting data by handwriting, e.g. gesture or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04886—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures by partitioning the display area of the touch-screen or the surface of the digitising tablet into independently controllable areas, e.g. virtual keyboards or menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2200/00—Indexing scheme relating to G06F1/04 - G06F1/32
- G06F2200/16—Indexing scheme relating to G06F1/16 - G06F1/18
- G06F2200/163—Indexing scheme relating to constructional details of the computer
- G06F2200/1636—Sensing arrangement for detection of a tap gesture on the housing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2203/00—Indexing scheme relating to G06F3/00 - G06F3/048
- G06F2203/033—Indexing scheme relating to G06F3/033
- G06F2203/0339—Touch strips, e.g. orthogonal touch strips to control cursor movement or scrolling; single touch strip to adjust parameter or to implement a row of soft keys
Abstract
方法の例は、ハウジングと存在感知ディスプレイとを含むモバイルコンピューティングデバイスが、１つ以上のセンサに含まれる少なくとも慣性測定ユニットによって提供されるセンサ信号の第１のグループに基づいて、ハウジングの部分において実行される少なくとも１つの第１のジェスチャを特定することを備え、１つ以上の部分はディスプレイとは別個であり、方法はさらに、対話モードを開始することと、モバイルコンピューティングデバイスの特定の機能に関連付けられた対話モードのための少なくとも１つの視覚または音声インジケータを出力することと、１つ以上のセンサによって提供されるセンサ信号の第３のグループに基づいて、特定の機能のユーザ選択を確認するために、ハウジングの１つ以上の部分において実行される少なくとも１つの第２のジェスチャを特定することと、少なくとも１つの第２のジェスチャを特定することに応答して、特定の機能を実行することとを備える。An example method includes a mobile computing device that includes a housing and a presence sensing display that detects at least one sensor signal in a portion of the housing based on a first group of sensor signals provided by at least an inertial measurement unit included in one or more sensors. identifying at least one first gesture to be performed, the one or more portions being separate from the display, the method further comprising: initiating an interactive mode; and determining the particular functionality of the mobile computing device. and confirming user selection of a particular function based on a third group of sensor signals provided by the one or more sensors. identifying at least one second gesture to be performed on the one or more portions of the housing to perform the specified function; and in response to identifying the at least one second gesture, performing the specified function. Be prepared for things.
Description
背景
モバイルコンピューティングデバイスを含む既存のコンピューティングデバイスは、さまざまな異なるアプリケーションを実行するように構成され得る。多くの場合、これらのコンピューティングデバイスは、ユーザがこれらのアプリケーションによって出力されるグラフィカルユーザインターフェイスと対話することを可能にするタッチスクリーンを提供する。たとえば、ユーザは、１本以上の指を使用して、タッチスクリーンにおいてタッチベースのジェスチャを実行し得る。タッチスクリーンにおいてこれらのタッチベースのジェスチャを検出すると、アプリケーションは、タッチスクリーンに表示されるオブジェクトの選択、移動、または操作などの１つ以上の対応する機能を実行し得る。他の場合には、特定のコンピューティングデバイスは、ユーザがそのようなデバイスの背面ハウジングにおいて複数のタップジェスチャを実行することに応答して、定義された機能（たとえば、スクリーンショットのキャプチャ、デバイスロック機能の実行）を実行するように構成され得る。
Background Existing computing devices, including mobile computing devices, may be configured to run a variety of different applications. These computing devices often provide touch screens that allow users to interact with the graphical user interfaces output by these applications. For example, a user may use one or more fingers to perform touch-based gestures on a touch screen. Upon detecting these touch-based gestures on the touch screen, the application may perform one or more corresponding functions, such as selecting, moving, or manipulating objects displayed on the touch screen. In other cases, certain computing devices may perform a defined function (e.g., capture a screenshot, lock the device) in response to a user performing multiple tap gestures on the back housing of such device. performance of functions).
概要
本開示は、オフスクリーンジェスチャ検出を使用して、たとえば、デバイスハウジングの１つ以上の部分においてユーザによって行われる１つ以上のオフスクリーンジェスチャ（たとえば、タップジェスチャ）を検出するために、コンピューティングデバイス対話モードを開始するための技術に関する。そのような対話モードを開始すると、コンピューティングデバイスは、１つ以上の追加的に検出されたオンスクリーンジェスチャおよび／またはオフスクリーンジェスチャに基づいて、１つ以上の機能を実行するように構成され得る。オフスクリーン対話は、場合によっては、片手および／またはアイズフリーモバイル対話を可能にし得る。さまざまな例では、モバイルコンピューティングデバイスは、１つ以上のセンサ（たとえば、加速度計、ジャイロスコープ、および／または磁力計のうちの１つ以上を含む慣性測定ユニット）を利用して、デバイスハウジングの１つ以上の部分においてユーザによって実行されるオフスクリーンジェスチャを検出可能である。
SUMMARY The present disclosure uses off-screen gesture detection to detect one or more off-screen gestures (e.g., tap gestures) performed by a user on one or more portions of a device housing, for example. Relating to techniques for initiating device interaction mode. Upon initiation of such interaction mode, the computing device may be configured to perform one or more functions based on the one or more additionally detected on-screen gestures and/or off-screen gestures. . Off-screen interaction may enable one-handed and/or eyes-free mobile interaction in some cases. In various examples, the mobile computing device utilizes one or more sensors (e.g., an inertial measurement unit including one or more of an accelerometer, a gyroscope, and/or a magnetometer) to Off-screen gestures performed by the user in one or more portions can be detected.
一例では、方法は、ハウジングと存在感知表示デバイスとを含むモバイルコンピューティングデバイスが、少なくとも慣性測定ユニットによって提供されるセンサ信号の第１のグループに基づいて、ハウジングの１つ以上の部分において実行される少なくとも１つの第１のジェスチャを特定することを備え、ハウジングの１つ以上の部分は、存在感知表示デバイスとは別個であり、慣性測定ユニットは、モバイルコンピューティングデバイスの１つ以上のセンサに含まれ、方法はさらに、少なくとも１つの第１のジェスチャを特定することに応答して、モバイルコンピューティングデバイスが、対話モードを開始することと、モバイルコンピューティングデバイスの特定の機能に関連付けられた対話モードのための少なくとも１つの視覚または音声インジケータを出力することと、モバイルコンピューティングデバイスが、対話モードのための少なくとも１つの視覚または音声インジケータに関連付けられた特定の機能のユーザ選択を確認するために、１つ以上のセンサによって提供されるセンサ信号の第３のグループに基づいて、ハウジングの１つ以上の部分において実行される少なくとも１つの第２のジェスチャを特定することと、少なくとも１つの第２のジェスチャを特定することに応答して、モバイルコンピューティングデバイスが、特定の機能を実行することとを備える。 In one example, a method is configured to perform a mobile computing device including a housing and a presence sensing display device in one or more portions of the housing based on at least a first group of sensor signals provided by an inertial measurement unit. the one or more portions of the housing are separate from the presence sensing display device, the inertial measurement unit is configured to identify at least one first gesture of the mobile computing device; The method further includes, in response to identifying the at least one first gesture, the mobile computing device initiates an interaction mode and an interaction associated with a particular feature of the mobile computing device. outputting at least one visual or audio indicator for the interactive mode; and for the mobile computing device to confirm user selection of a particular feature associated with the at least one visual or audio indicator for the interactive mode. , identifying at least one second gesture to be performed on the one or more portions of the housing based on a third group of sensor signals provided by the one or more sensors; and performing a particular function in response to identifying the gesture of the mobile computing device.
別の例では、モバイルコンピューティングデバイスは、存在感知表示デバイスと、存在感知ディスプレイに結合されたハウジングと、慣性測定ユニットを含む１つ以上のセンサと、少なくとも１つのプロセッサと、コンピュータ読取可能ストレージデバイスとを備える。コンピュータ読取可能ストレージデバイスは、少なくとも１つのプロセッサによって実行可能な命令を記憶するように構成され、命令は、少なくとも慣性測定ユニットによって提供されるセンサ信号の第１のグループに基づいて、ハウジングの１つ以上の部分において実行される少なくとも１つの第１のジェスチャを特定させ、ハウジングの１つ以上の部分は、存在感知表示デバイスとは別個であり、命令はさらに、少なくとも１つの第１のジェスチャを特定することに応答して、対話モードを開始させ、モバイルコンピューティングデバイスの特定の機能に関連付けられた対話モードのための少なくとも１つの視覚または音声インジケータを出力させ、対話モードのための少なくとも１つの視覚または音声インジケータに関連付けられた特定の機能のユーザ選択を確認するために、１つ以上のセンサによって提供されるセンサ信号の第３のグループに基づいて、ハウジングの１つ以上の部分において実行される少なくとも１つの第２のジェスチャを特定させ、少なくとも１つの第２のジェスチャを特定することに応答して、特定の機能を実行させる。 In another example, a mobile computing device includes a presence sensing display device, a housing coupled to the presence sensing display, one or more sensors including an inertial measurement unit, at least one processor, and a computer readable storage device. Equipped with. The computer readable storage device is configured to store instructions executable by the at least one processor, the instructions being configured to detect one of the housings based on the first group of sensor signals provided by at least the inertial measurement unit. the one or more portions of the housing are separate from the presence sensing display device, the instructions further specify at least one first gesture to be performed in the portion; in response to initiating an interaction mode and outputting at least one visual or audio indicator for the interaction mode associated with a particular feature of the mobile computing device; or in one or more portions of the housing based on a third group of sensor signals provided by the one or more sensors to confirm user selection of a particular function associated with the audio indicator. causing at least one second gesture to be identified and causing a particular function to be performed in response to identifying the at least one second gesture.
別の例では、コンピュータ読取可能ストレージデバイスは、実行されると、モバイルコンピューティングデバイスの少なくとも１つのプロセッサに動作を実行させる命令を記憶する。これらの動作の例は、少なくとも慣性測定ユニットによって提供されるセンサ信号の第１のグループに基づいて、モバイルコンピューティングデバイスのハウジングの１つ以上の部分において実行される少なくとも１つの第１のジェスチャを特定することを含み、ハウジングの１つ以上の部分は、存在感知表示デバイスとは別個であり、慣性測定ユニットは、モバイルコンピューティングデバイスの１つ以上のセンサに含まれ、動作はさらに、少なくとも１つの第１のジェスチャを特定することに応答して、対話モードを開始することと、モバイルコンピューティングデバイスの特定の機能に関連付けられた対話モードのための少なくとも１つの視覚または音声インジケータを出力することと、対話モードのための少なくとも１つの視覚または音声インジケータに関連付けられた特定の機能のユーザ選択を確認するために、１つ以上のセンサによって提供されるセンサ信号の第３のグループに基づいて、ハウジングの１つ以上の部分において実行される少なくとも１つの第２のジェスチャを特定することと、少なくとも１つの第２のジェスチャを特定することに応答して、特定の機能を実行することとを含む。 In another example, a computer readable storage device stores instructions that, when executed, cause at least one processor of the mobile computing device to perform operations. Examples of these operations include at least one first gesture performed at one or more portions of the housing of the mobile computing device based on at least the first group of sensor signals provided by the inertial measurement unit. the one or more portions of the housing are separate from the presence-sensing display device, the inertial measurement unit is included in the one or more sensors of the mobile computing device, and the operation further includes determining that the one or more portions of the housing are separate from the presence-sensing display device; in response to identifying a first gesture, initiating an interaction mode and outputting at least one visual or audio indicator for the interaction mode associated with a particular feature of the mobile computing device; and based on a third group of sensor signals provided by the one or more sensors to confirm user selection of a particular function associated with at least one visual or audio indicator for the interaction mode; identifying at least one second gesture to be performed on one or more portions of the housing; and performing a particular function in response to identifying the at least one second gesture. .
１つ以上の例について、添付の図面および以下の説明に詳細に記載されている。本開示の他の特徴、目的、および利点は、説明および図面、ならびに特許請求の範囲から明らかになるであろう。 One or more examples are set forth in detail in the accompanying drawings and the description below. Other features, objects, and advantages of the disclosure will be apparent from the description and drawings, and from the claims.
詳細な説明
図１Ａ～図１Ｃは、本開示の１つ以上の態様に係る、オフスクリーンジェスチャ検出を使用して、モバイルコンピューティングデバイス１００のための対話モードを開始するように構成されたモバイルコンピューティングデバイス１００の例を示す概念図である。モバイルコンピューティングデバイス１００の例は、限定されないが、携帯電話、タブレットコンピュータ、携帯情報端末（ＰＤＡ）、ポータブルゲームデバイス、ポータブルメディアプレーヤ、ウェアラブルコンピューティングデバイス（たとえば、腕時計、手首装着型コンピューティングデバイス、頭部装着型コンピューティングデバイス）、または他のタイプのコンピューティングデバイスを含み得る。以下でさらに詳細に説明するように、モバイルコンピューティングデバイス１００は、１つ以上のプロセッサでもよい、または１つ以上のプロセッサを含んでもよい。モバイルコンピューティングデバイス１００は、表示デバイス１０２（たとえば、存在感知表示デバイス）および１つ以上のセンサ１０４を備える。センサ１０４は、タッチセンサまたは存在感知センサ、マイクロフォン、内部測定ユニット（たとえば、１つ以上のジャイロスコープ、加速度計、磁力計）、気圧計、カメラセンサ、光センサ、および温度センサ等の任意の数の１つ以上のセンサを含んでもよい。場合によっては、センサ１０４のうちの１つ以上は、表示デバイス１０２（たとえば、表示デバイス１０２が存在感知ディスプレイを備える場合）に含まれてもよい、または他の態様ではそれと関連付けられてもよい。
DETAILED DESCRIPTION FIGS. 1A-1C illustrate a mobile computing device configured to initiate an interaction mode for a
くわえて、モバイルコンピューティングデバイス１００は、ハウジング１０３も備える。ハウジング１０３は、１つ以上の前面部分、１つ以上の背面部分、および１つ以上の側面部分（たとえば、左側面部分、右側面部分、上側面部分、下側面部分）などの１つ以上の部分を含み得る。さまざまな例では、モバイルコンピューティングデバイス１００の前面側は、表示デバイス１０２と、ハウジング１０３の１つ以上の前面部分とを含み得る。モバイルコンピューティングデバイス１００の前面側と反対側に位置するモバイルコンピューティングデバイス１００の裏面側または背面側は、ハウジング１０３の１つ以上の背面部分を含み得る。モバイルコンピューティングデバイス１００の正面側および背面側に隣接するモバイルコンピューティングデバイス１００の残りの側面またはエッジは、ハウジング１０３の１つ以上の側面部分を含んでもよい。ハウジング１０３は、表示デバイス１０２を含んでも含まなくてもよい。ハウジング１０３の１つ以上の部分（たとえば、ハウジング１０３の１つ以上の側面部分および／または背面部分）は、表示デバイス１０２とは別個であり、かつ区別される。
In addition,
表示デバイス１０２は、コンピューティングデバイス１００のための入力デバイスおよび／または出力デバイスとして機能し得る。表示デバイス１０２は、さまざまな技術を使用して実装され得る。たとえば、表示デバイス１０２は、抵抗膜方式タッチスクリーン、表面弾性波タッチスクリーン、容量性タッチスクリーン、投影静電容量式タッチスクリーン、存在感知スクリーン、音響パルス認識タッチスクリーン、レーダ技術を介して動きを検出する存在感知スクリーン、または別の存在感知技術など、存在感知入力デバイスを使用する入力デバイスとして機能し得る。上述のように、特定の場合には、センサ１０４のうちの１つ以上は、表示デバイス１０２内に含まれてもよい、または他の態様ではそれと関連付けられてもよい。表示デバイス１０２は、液晶ディスプレイ（ＬＣＤ）、ドットマトリックスディスプレイ、発光ダイオード（ＬＥＤ）ディスプレイ、有機発光ダイオード（ＯＬＥＤ）ディスプレイ、電子インク、またはモバイルコンピューティングデバイス１００のユーザに可視情報を出力可能な同様のモノクロもしくはカラーディスプレイなど、１つ以上の表示デバイスのいずれかを使用する出力デバイスとして機能し得る。たとえば、表示デバイス１０２は、モバイルコンピューティングデバイス１００で実行するアプリケーション１３２のさまざまなユーザインターフェイスに関連付けられた出力を提示し得る。ユーザは、アプリケーション１３２の各々のそれぞれのユーザインターフェイスと対話して、コンピューティングモバイルデバイス１００に、対応するアプリケーション機能に関する動作を実行させることができる。
Display device 102 may function as an input device and/or output device for
いくつかの例では、モバイルコンピューティングデバイス１００は、（図２に示すような）１つ以上の通信ユニットを含み得る。これらの通信ユニットは、１つ以上の他のコンピューティングデバイスに対するデータの送信および／または受信が可能である。いくつかの例では、通信ユニットは、無線および／または有線通信をサポートし、それらは、任意のさまざまな通信プロトコルを使用してデータを送信および／または受信し得る。
In some examples,
モバイルコンピューティングデバイス１００は、ＵＩモジュール１３０、ジェスチャ検出モジュール１３４、対話モードセレクタ１３６、および１つ以上の対話モードモジュール１３８を実行するように構成されている。ＵＩモジュール１３０、アプリケーション１３２、ジェスチャ検出モジュール１３４、対話モードセレクタ１３６、および対話モードモジュール１３８は、モバイルコンピューティングデバイス１００に常駐するおよび／またはそこで実行するソフトウェア、ハードウェア、および／またはファームウェアの任意の組み合わせを使用して、本明細書で説明する動作を実行し得る。モバイルコンピューティングデバイス１００は、１つ以上のプロセッサを使用して、モジュール１３０，１３４，１３６，１３８およびアプリケーション１３２を実行し得る。モバイルコンピューティングデバイス１００は、場合によっては、基礎をなすハードウェア上で実行する１つ以上の仮想マシンとして、モジュール１３０，１３４，１３６，１３８およびアプリケーション１３２を実行し得る。モジュール１３０，１３４，１３６，１３８およびアプリケーション１３２は、さまざまな方法で実装され得る。たとえば、モジュール１３０，１３４，１３６，１３８および／またはアプリケーション１３２のいずれかは、ダウンロード可能またはプリインストールされたアプリケーション、すなわち「アプリ」として実装されてもよい。いくつかの例では、これらのうちの１つ以上は、オペレーティングシステムまたはコンピューティングプラットフォームのサービスとして実行することができる。
モバイルコンピューティングデバイス１００のアプリケーション１３２は、モバイルコンピューティングデバイス１００のためのさまざまな機能を実行し得る、または１つ以上のサービスにアクセスし得る。電子メールアプリケーション、カメラアプリケーション、カレンダーアプリケーション、メッセージングアプリケーション、ソーシャルメディアアプリケーション、旅行アプリケーション、ゲームアプリケーション、株式アプリケーション、および天気アプリケーションはすべて、アプリケーション１３２の例である。
ＵＩモジュール１３０は、表示デバイス１０２に、グラフィカルユーザインターフェイスをユーザに対して提示させ得る。たとえば、グラフィカルユーザインターフェイスは、表示デバイス１０２の種々の場所において表示されるグラフィカル要素（たとえば、表示）を含んでもよい。ＵＩモジュール１３０は、場合によっては、モバイルコンピューティングデバイス１００のさまざまなコンポーネント、アプリケーション、および／またはモジュールの間の仲介として機能して、表示デバイス１０２によって検出された入力に基づいて判定を行い、表示デバイス１０２によって提示される出力を生成し得る。たとえば、ＵＩモジュール１３０は、表示デバイス１０２によって検出された入力に関連する情報を表示デバイス１０２から受信し、モジュール１３０，１３４，１３６，１３８および／またはアプリケーション１３２のうちの１つ以上に入力情報を送信し得る。ＵＩモジュール１３０はまた、モジュール１３０，１３４，１３６，１３８および／またはアプリケーション１３２から出力情報を受信し、表示デバイス１０２において表示するための出力情報を提供し得る。
UI module 130 may cause display device 102 to present a graphical user interface to a user. For example, a graphical user interface may include graphical elements (eg, displays) that are displayed at various locations on display device 102. UI module 130 may act as an intermediary between various components, applications, and/or modules of
ユーザは、経時的にモバイルコンピューティングデバイス１００と能動的に対話し得る。モバイルコンピューティングデバイス１００は、実行中にユーザがアプリケーション１３２のうちの１つ以上などのモバイルコンピューティングデバイス１００と対話可能な１つ以上の対話モードを提供し得る。多くの場合、ユーザは、モバイルコンピューティングデバイス１００と対話するために、１つ以上のジェスチャを実行可能である。
A user may actively interact with
モバイルデバイス入力（たとえば、表示デバイス１０２がタッチスクリーンなどの存在感知ディスプレイを備える場合）の唯一の方法としての表示デバイス１０２の使用は、特定の制限によってますます困難になる可能性がある。たとえば、片手での使用が困難であること、および操作する指によって視覚情報が遮断されることが、これらの制約のうちの２つの場合がある。結果として、本開示は、表示デバイス１０２とのオンスクリーン対話だけでなく、モバイルコンピューティングデバイス１００上の既存のセンサ１０４を使用するオフスクリーン対話も利用するための技術について説明する。
The use of display device 102 as the only method for mobile device input (eg, if display device 102 includes a presence-sensitive display such as a touch screen) can become increasingly difficult due to certain limitations. For example, difficulty in one-handed use and visual information being blocked by the operating finger may be two of these limitations. As a result, this disclosure describes techniques for utilizing not only on-screen interaction with display device 102, but also off-screen interaction using existing
たとえば、センサ１０４は、表示デバイス１０２に関連付けられた他のセンサ（たとえば、存在感知センサ）に加えて、１つ以上のジャイロスコープ、加速度計、および／または磁力計を含む慣性測定ユニット（ＩＭＵ）（たとえば、図２の慣性測定ユニット２３３）を含み得る。センサ１０４によって提供される入力に基づいて、ジェスチャ検出モジュール１３４は、さまざまな例において、以下でさらに詳細に説明するように、オフスクリーンジェスチャ（たとえば、タップ）認識のために多入力多出力畳み込みニューラルネットワークを使用し得る。この検出方法によって、各々が１つの認識タスクであり得る複数のジェスチャ属性の予測が可能になる。これらのジェスチャ属性は、以下でさらに説明するように、ジェスチャ位置、ジェスチャ方向、ジェスチャ条件、および／またはジェスチャ力を含み得る。
For example,
一例として、ユーザは、モバイルコンピューティングデバイス１００においてジェスチャを実行するために片手または両手を使用することができ、ジェスチャは、表示デバイス１０２において実行されるジェスチャ、および／または表示デバイス１０２において実行されないオフスクリーンジェスチャを含み得る。オフスクリーンジェスチャは、モバイルコンピューティングデバイス１００のハウジング１０３の１つ以上の部分（たとえば、１つ以上の背面部分および／または側面部分）で実行されるジェスチャを含み得る。
As an example, a user may use one or both hands to perform gestures at
たとえば、図１Ａの例では、ユーザは、右手１１３を使用して、モバイルコンピューティング１００において１つ以上のジェスチャを実行し得る。右手１１３は、指１１５，１１６，１１７，１１８および１１９を含む。図１Ａに示すように、ユーザの右手１１３は、モバイルコンピューティングデバイス１００を保持している。親指１１５は、ハウジング１０３の１つ以上の右側面部分と接触しており、指１１７，１１８，１１９は、ハウジング１０３の１つ以上の左側面部分と接触している。くわえて、人差し指１１６は、図１Ａに示されるように、ハウジング１０３の１つ以上の背面部分１０１と接触している。背面部分１０１は、ハウジング１０３の背面または裏面に含まれる１つ以上の特定の部分または領域を備え得る。
For example, in the example of FIG. 1A, the user may use
以下でより詳細に説明するように、モバイルコンピューティングデバイス１００は、ジェスチャ検出モジュール１３４を使用して、少なくともセンサ１０４に含まれる慣性測定ユニット（ＩＭＵ）によって提供されるセンサ信号の第１のグループに基づいて、表示デバイス１０２とは別個のハウジング１０３の１つ以上の部分において実行される少なくとも１つのジェスチャを特定し得る。たとえば、ジェスチャ検出モジュール１３４は、センサ１０４に含まれるＩＭＵによって提供されるセンサ信号のグループに基づいて、ハウジング１０３の１つ以上の背面部分１０１においてユーザの指１１６によって実行される１つ以上のバックタップジェスチャ（たとえば、ダブルバックタップジェスチャ）を特定し得る。場合によっては、ＩＭＵ（たとえば、図２に示すＩＭＵ２３３）は、ジャイロスコープ、加速度計、および／または磁力計のうちの１つ以上の任意の組み合わせを含み得る。
As described in more detail below,
ジェスチャ検出モジュール１３４が少なくとも１つのジェスチャを特定することに応答して、モバイルコンピューティングデバイス１００は、対話モードセレクタ１３６および／または対話モードモジュール１３８を使用して、対話モードを開始し得る。場合によっては、対話モードモジュール１３８の各々は、アプリケーション１３２の実行中にユーザと対話するためのそれぞれの対話モードを提供することができる。たとえば、以下でさらに詳細に説明するように、また図２にも示すように、対話モードモジュール１３８は、アクセシビリティモードモジュール２４０、アシストモードモジュール、慣性モードモジュール、および／または背景モードモジュールを含み得る。
In response to
対話モードモジュール１３８のうちの１つ以上は、センサ１０４によって提供されるセンサ信号の第２のグループに基づいて、モバイルコンピューティングデバイス１００の特定の機能に関連付けられた対話モードのための少なくとも１つの視覚または音声インジケータを出力し得る。たとえば、ジェスチャ検出モジュール１３４がダブルバックタップジェスチャを特定すると、対話モジュールセレクタ１３６は、対話モードモジュール１３８によって提供されるアクセシビリティモードを選択し得る。アクセシビリティモードは、ユーザがモバイルコンピューティングデバイス１００と、たとえば表示デバイス１０２と対話している間に、音声および／または視覚アクセシビリティ機能を提供し得る。
One or more of the
たとえば、図１Ａに示されるように、そのようなアクセシビリティモードの間、ユーザは、１本以上の指を使用して、表示デバイス１０２において出力されるコンテンツを探索してもよい。図１Ａは、モバイルコンピューティングデバイス１００がＵＩモジュール１３０を使用して、日付（たとえば、「５月５日火曜日」）および温度（たとえば、「６０°Ｆ」）情報を表示デバイス１０２において出力し得ることを示す。ＵＩモジュール１３０はまた、アプリケーション１３２などの異なるコンポーネントまたはアプリケーションに関連付けられたさまざまなグラフィカルアイコンを出力してもよい。図１Ａでは、モバイルコンピューティングデバイス１００によって実行され得る４つの異なるアプリケーション（たとえば、「アプリ１」、「アプリ２」、「アプリ３」、および「アプリ４」）に対応する４つのそのようなグラフィカルアイコンが、表示デバイス１０２に表示されている。
For example, as shown in FIG. 1A, while in such an accessibility mode, a user may use one or more fingers to explore content output on display device 102. FIG. 1A shows that
これらの４つのグラフィカルアイコンの各々は、表示デバイス１０２において出力されるグラフィカルユーザインターフェイス（ＧＵＩ）の特定のそれぞれの領域に表示され得る。たとえば、第１のアプリケーション（「アプリ１」）に関連付けられた第１のアイコンは、ＧＵＩの領域１０８内に表示可能であり、第２のアプリケーション（「アプリ２」）に関連付けられた第２のアイコンは、ＧＵＩの領域１１０内に表示可能であり、第３のアプリケーション（「アプリ３」）に関連付けられた第３のアイコンは、ＧＵＩの領域１１２内に表示可能であり、第４のアプリケーション（「アプリ４」）に関連付けられた第４のアイコンは、ＧＵＩの領域１１４内に表示可能である。
Each of these four graphical icons may be displayed in a particular respective area of a graphical user interface (GUI) output on display device 102. For example, a first icon associated with a first application ("App 1") may be displayable within
上述のように、アクセシビリティモード中、ユーザは、１本以上の指を使用して、表示デバイス１０２において出力されるＧＵＩのコンテンツを探索可能である。場合によっては、対話モジュールモジュール１３８は、モバイルコンピューティングデバイス１００が現在アクセシビリティモードで動作していることを示す別のグラフィカルアイコン１０６を出力してもよい。表示デバイス１０２が存在感知ディスプレイを備える場合、ユーザ（たとえば、視覚障害者ユーザ）は、１本以上の指（たとえば、右手１１３またはユーザの左手の１本以上の指）を使用して、表示デバイス１０２において１つ以上のタッチジェスチャ（たとえば、スライドジェスチャまたは動きジェスチャ）を実行し得る。センサ１０４のうちの１つ以上によって提供されるセンサ信号（たとえば、表示デバイス１０２によって出力されるＧＵＩの１つ以上の領域における任意の指のタッチまたは存在を特定する１つ以上のセンサ）に基づいて、対話モジュール１３８は、モバイルコンピューティングデバイス１００の特定の機能に関連付けられた対話モードのための少なくとも１つの視覚または音声インジケータを出力し得る。
As mentioned above, while in accessibility mode, the user can use one or more fingers to explore the content of the GUI output on the display device 102. In some cases,
たとえば、ユーザが指を使用して、表示デバイス１０２において出力されるＧＵＩの領域１０８内の任意の位置に触れる、またはそれに近接する場合、対話モードモジュール１３８は、第１のアプリケーション（「アプリ１」）に関連付けられた音声インジケータを出力し得る。一例として、対話モードモジュール１３８は、この第１のアプリケーション（たとえば、電子メールアプリケーション）の名前を聴覚的に特定するために、フレーズ「アプリ１」を指定する（たとえば、モバイルコンピューティングデバイス１００のスピーカデバイスにおける）音声スピーチ出力を出力してもよい。出力される音声スピーチは、ジェスチャが実行されるグラフィカルユーザインターフェイスの領域１０８において表示されるコンテンツを示す。その結果、ユーザは、ユーザの指が領域１０８に触れると、この第１のアプリケーションの名前を聞くことができる。
For example, if the user uses a finger to touch or be in close proximity to any location within the
図１Ｂに示すように、ユーザがユーザの指を領域１１４内の位置に移動し続ける場合、対話モードモジュール１３８は、この第２のアプリケーション（たとえば、計算機アプリケーション）の名前を聴覚的に特定するために、「アプリ４」というフレーズを指定する音声スピーチ出力を提供し得る。図１Ｂの例に示すように、ユーザは、右手１１３の指１１５を領域１１４に移動させてもよいが、他の例では、ユーザは、右手１１３の任意の他の指またはユーザの左手の任意の指を領域１１４に移動してもよい。このように、ユーザは、表示デバイスにおいて出力されるＧＵＩを横切って１本以上の指をナビゲートして、たとえば、ユーザの指（複数可）が現在位置するＧＵＩの領域と関連付けられたアプリケーションの識別子の対応する名前を聞くことができる。
If the user continues to move the user's finger to a position within
特定の例では、上述のように領域１０８，１１０，１１２，１１４に関連付けられた情報の１つ以上の音声インジケータを提供することに加えて、またはその代わりに、対話モードモジュール１３８は、対話モード（たとえば、アクセシビリティモード）のための１つ以上の視覚的インジケータを表示デバイス１０２として出力し得る。たとえば、ユーザが第４のアプリケーション（「アプリ４」）に関連付けられた領域１１４内に指１１５を移動させた上記の例を続けると、フレーズ「アプリ４」を指定する音声出力を提供することに加えて、対話モードモジュール１３８はまた、たとえば、図１Ｃに示すように、領域１１４に含まれる「アプリ４」用のグラフィカルアイコンの視覚的な拡大図を（たとえば、視覚障害者ユーザのために）提供することによって、領域１１４用の「アプリ４」に関連付けられた視覚的インジケータを提供してもよい。図１Ｃは、現在のタッチジェスチャが指１１５によって実行される領域１１４に表示されるコンテンツであり得る、拡大鏡アイコン１２１内に含まれるこのコンテンツのそのような視覚的な拡大図を示す。
In certain examples, in addition to or in lieu of providing one or more audio indicators of information associated with
ユーザが表示デバイス１０２において出力された情報（たとえば、領域１０８，１１０および／または１１２に含まれる情報）をさらに探索することを望む場合、ユーザは、指１１５を表示デバイス１０２で移動させて、指１１５が領域から領域に移動すると、これらの領域のいずれかに含まれるコンテンツの視覚および／または音声インジケータを受信し得る。この場合、図１Ｃに示す拡大鏡アイコン１２１はまた、ＧＵＩ内で視覚的に移動して、指１１５が移動されると、これらの領域内のコンテンツを選択的に拡大してもよい。特定の場合、対話モードモジュール１３８は、指１１５が領域から領域へ移動すると、必ずしも音声インジケータ（たとえば、音声語または音声フレーズ）を出力することなく、図１Ｃに示すような１つ以上の視覚的インジケータのみを出力し得る。そのような態様で、対話モードモジュール１３８は、表示デバイス１０２において出力されるコンテンツを横切る１本以上の指のユーザナビゲーションに応答して、視覚および／または音声インジケータを提供し得る。
If the user desires to further explore the information output on display device 102 (e.g., information contained in
そのようなナビゲーションに応答して、ユーザはまた、ＧＵＩのそれぞれの領域内に含まれるグラフィカルアイコンのいずれかを選択することが可能であってもよい。たとえば、ユーザの指が領域１１４に位置する場合、ユーザは、たとえば、指１１６を使用して、ハウジング１０３の部分１０１においてシングルバックタップジェスチャを行うこと等によって、「アプリ４」用のグラフィカルアイコンを選択するための１つ以上のジェスチャを行うことが可能であり得る。
In response to such navigation, the user may also be able to select any of the graphical icons included within each region of the GUI. For example, if the user's finger is located in
ジェスチャ検出モジュール１３４は、センサ１０４によって提供されるセンサ信号のさらに別のグループに基づいて、領域１１４に関連付けられた第４のアプリケーションの実行などの特定の機能のユーザ選択を確認するために、ハウジング１０３の１つ以上の部分において実行されるこの少なくとも１つの第２のジェスチャを特定し得る。少なくとも１つの第２のジェスチャを特定することに応答して、モバイルコンピューティングデバイス１００は、対応する機能を実行する（たとえば、アプリケーション１３２から第４のアプリケーションを実行する）ことができる。したがって、そのような対話モードを開始すると、モバイルコンピューティングデバイス１００は、１つ以上の検出されたオンスクリーンおよび／またはオフスクリーンジェスチャに基づいて、１つ以上の機能を実行するように構成され得る。オフスクリーン対話は、さまざまな場合において、片手および／またはアイズフリーモバイル対話を可能にし得る。ジェスチャ検出モジュール１３４が、現在の対話モードを終了するユーザの意図を示す、センサ１０４によって提供される１つ以上のさらに別のセンサ信号の検出に基づいて、ハウジング１０３の１つ以上の部分において実行されるさらに別のジェスチャ（たとえば、別のバックタップジェスチャ）を特定すると、対話モードモジュール１３８は、この対話モードを終了することができる。
図２は、本開示の１つ以上の態様に係る、モバイルコンピューティングデバイス２００の例を示すブロック図である。モバイルコンピューティングデバイス２００は、図１Ａ～図１Ｃに示すモバイルコンピューティングデバイス１００の一例を備えてもよく、同様の番号が付けられたコンポーネントは、図１Ａ～図１Ｃを参照して説明したものと同様の機能を提供し得る。図２は、モバイルコンピューティングデバイス２００の１つの特定の例のみを示し、モバイルコンピューティングデバイス２００の多くの他の例は、他の事例において使用可能であり、モバイルコンピューティングデバイス２００の例に含まれるコンポーネントのサブセットを含み得る、または図２に示されない追加のコンポーネントを含み得る。
FIG. 2 is a block diagram illustrating an example
図２の例では、モバイルコンピューティングデバイス２００は、ハウジング２０３を有し、存在感知表示デバイス２０２と、１つ以上のプロセッサ２２０と、１つ以上の入力コンポーネント２２２と、１つ以上の通信ユニット２２４と、１つ以上の出力コンポーネント２２６と、１つ以上のセンサ２０４と、電源２２８と、１つ以上のストレージデバイス２５０とを備える。通信チャネル２２７は、（物理的に、通信可能に、および／または動作可能に）コンポーネント間通信のためにコンポーネント２２０，２２２，２０２，２２４，２０４，２２８，２２６および／または２５０の各々を相互接続し得る。いくつかの例では、通信チャネル２２７は、システムバス、ネットワーク接続、プロセス間通信データ構造、またはハードウェアおよび／もしくはソフトウェア間でデータを通信するための任意の他の方法を含み得る。
In the example of FIG. 2,
モバイルコンピューティングデバイス２００の１つ以上の入力コンポーネント２２２は、ユーザからの入力などの入力を受信し得る。入力の例は、タッチ／触覚、存在感知、および音声入力である。入力コンポーネント２２２の例は、存在感知スクリーン、タッチ感知スクリーン、タッチスクリーン、マウス、キーボード、トラックパッド、音声応答システム、ビデオカメラ、マイクロフォン、または人間もしくは機械からの入力を検出するための任意の他のタイプのデバイスを含む。
One or
モバイルコンピューティングデバイス２００の１つ以上の出力コンポーネント２２６は、出力を生成し得る。出力の例は、触覚出力、音声出力、および視覚出力である。出力コンポーネント２２６の例は、存在感知スクリーン、タッチ感知スクリーン、タッチスクリーン、サウンドカード、ビデオグラフィックアダプタカード、スピーカ、液晶ディスプレイ（ＬＣＤ）、有機発光ダイオード（ＯＬＥＤ）ディスプレイ、マイクロ発光ダイオード（マイクロＬＥＤ）ディスプレイ、アクティブマトリクス有機発光ダイオード（ＡＭＯＬＥＤ）ディスプレイ、触覚デバイス、または人間もしくは機械への出力を生成するための任意の他のタイプのデバイスを含む。
One or more output components 226 of
モバイルコンピューティングデバイス２００の１つ以上の通信ユニット２２４は、１つ以上のネットワーク（たとえば、１つ以上の有線および／または無線ネットワーク）上でネットワーク信号を送信および／または受信することによって、１つ以上のネットワークを介して外部デバイスと通信し得る。たとえば、モバイルコンピューティングデバイス２００は、通信ユニット２２４を使用して、セルラー無線ネットワークなどの無線ネットワーク上で無線信号を送信および／または受信し得る。同様に、通信ユニット２２４は、全地球測位システム（ＧＰＳ）ネットワークなどの衛星ネットワーク上で衛星信号を送信および／または受信し得る。通信ユニット２２４の例は、ネットワークインターフェイスカード（たとえば、イーサネット（登録商標）カードなど）、光トランシーバ、無線周波数トランシーバ、ＧＰＳ受信機、または情報を送信および／または受信可能な任意の他のタイプのデバイスを含む。通信ユニット２２４の他の例は、短波無線機、セルラーデータ無線機、無線イーサネットネットワーク無線機、およびユニバーサルシリアルバス（ＵＳＢ）コントローラを含み得る。
One or
モバイルコンピューティングデバイス２００の存在感知表示デバイス２０２は、表示コンポーネント２２３および存在感知入力コンポーネント２２５を含む。いくつかの例では、存在感知表示デバイス２０２は、出力コンポーネント２２６を参照して上述したように、触覚刺激、音声刺激、または視覚刺激を使用して、ユーザに出力を提供してもよい。たとえば、表示コンポーネント２２３は、出力コンポーネント２２６を参照して説明したように、表示またはビデオ出力を提供してもよい。存在感知表示デバイス２０２はまた、入力コンポーネント２２２を参照して上述したような入力機能を提供し得る。たとえば、存在感知入力コンポーネント２２５は、入力コンポーネント２２２を参照して説明したような入力機能を提供し得る。
Presence
表示コンポーネント２２３は、情報が存在感知表示デバイス２０２によって表示されるスクリーンでもよく、存在感知入力コンポーネント２２５は、表示コンポーネント２２３における、および／またはその近くのオブジェクトを検出し得る。１つの範囲の例として、存在感知入力コンポーネント２２５は、表示コンポーネント２２３の２インチ以内にある指またはスタイラスなどのオブジェクトを検出し得る。存在感知入力コンポーネント２２５は、オブジェクトが検出された表示コンポーネント２２３の位置（たとえば、（ｘ，ｙ）座標）を決定し得る。範囲の別の例では、存在感知入力コンポーネント２２５は、表示コンポーネント２２３から６インチ以内のオブジェクトを検出可能であり、他の範囲も可能である。存在感知入力コンポーネント２２５は、容量性、誘導性、レーダーベース、および／または光学認識技術を使用して、ユーザの指によって選択された表示コンポーネント２２３の位置を決定し得る。いくつかの例では、存在感知入力コンポーネント２２５はまた、表示コンポーネント２２３に関して説明したように、タッチ刺激、存在感知刺激、オーディオ刺激、または映像刺激を使用して、ユーザに出力を提供する。表示コンポーネント２２３は、出力コンポーネント２２６に関して説明したような、視覚出力を提供する任意のタイプの出力デバイスでもよい。
モバイルコンピューティングデバイス２００の内部コンポーネントとして示されているが、存在感知表示デバイス２０２はまた、入力および出力を送信ならびに／または受信するためにモバイルコンピューティングデバイス２００とデータ経路を共有する外部コンポーネントを表してもよい。たとえば、一例では、存在感知表示デバイス２０２は、モバイルコンピューティングデバイス２００の外部パッケージング内に配置され、それに物理的に接続されたモバイルコンピューティングデバイス２００の組込みコンポーネント（たとえば、携帯電話上の画面）を表す。別の例では、存在感知表示デバイス２０２は、モバイルコンピューティングデバイス２００のパッケージの外側に配置され、それとは物理的に別個であるモバイルコンピューティングデバイス２００の外部コンポーネント（たとえば、有線および／もしくは無線データ経路をタブレットコンピュータと共有するモニタならびに／またはプロジェクタ）を表す。
Although shown as an internal component of
モバイルコンピューティングデバイス２００の存在感知表示デバイス２０２は、モバイルコンピューティングデバイス２００のユーザからの入力として、２次元および／または３次元ジェスチャを検出し得る。たとえば、存在感知表示デバイス２０２のセンサ（たとえば、存在感知入力コンポーネント２２５のセンサ）は、存在感知表示デバイス２０２のセンサの閾値距離内のユーザの動き（たとえば、手、腕、ペン、スタイラスの移動）を検出し得る。存在感知表示デバイス２０２は、動きの２次元または３次元ベクトル表現を決定し、ベクトル表現を、複数の次元を有するジェスチャ入力（たとえば、ハンドウェーブ、ピンチ、拍手、ペンストローク）に相関させることができる。言い換えれば、存在感知表示デバイス２０２は、存在感知表示デバイス２０２が表示用の情報を出力するスクリーンまたは表面（たとえば、表示コンポーネント２２３）において、またはその近くでユーザにジェスチャを要求することなく、多次元ジェスチャを検出し得る。代わりに、存在感知表示デバイス２０２は、存在感知表示デバイス２０２が表示用の情報を出力するスクリーンまたは表面の近くに位置してもしなくてもよいセンサにおいて、またはその近くで実行される多次元ジェスチャを検出することができる。
Presence
モバイルコンピューティングデバイス２００内の１つ以上のストレージデバイス２５０は、モバイルコンピューティングデバイス２００の動作中に（たとえば、ＵＩモジュール２３０、アプリケーション２３２、オペレーティングシステム２５４、またはジェスチャ検出モジュール２３４のうちの１つ以上の実行中に）処理するための情報を記憶し得る。いくつかの例では、ストレージデバイス２５０は一時記憶装置を含み、これは、ストレージデバイス２５０の主目的が長期記憶ではないことを意味する。モバイルコンピューティングデバイス２００上のストレージデバイス２５０は、揮発性メモリとして情報の短期記憶のために構成されてもよく、したがって、電源がオフにされた場合、記憶されたコンテンツを保持しない。揮発性メモリの例は、ランダムアクセスメモリ（ＲＡＭ）、ダイナミックランダムアクセスメモリ（ＤＲＡＭ）、スタティックランダムアクセスメモリ（ＳＲＡＭ）、および当該技術分野で知られている他の形態の揮発性メモリを含む。
One or
ストレージデバイス２５０は、いくつかの例では、１つ以上のコンピュータ読取可能記憶媒体を含む。ストレージデバイス２５０は、揮発性メモリよりも大量の情報を記憶するように構成され得る。ストレージデバイス２５０はさらに、不揮発性メモリ空間として情報の長期記憶のために構成され、電源オン／オフサイクル後に情報を保持し得る。不揮発性メモリの例は、磁気ハードディスク、光ディスク、フロッピー（登録商標）ディスク、フラッシュメモリ、または電気的プログラマブルメモリ（ＥＰＲＯＭ）もしくは電気的消去可能プログラマブル（ＥＥＰＲＯＭ）メモリの形態を含む。ストレージデバイス２５０は、１つ以上のアプリケーション２３２、ＵＩモジュール２３０、オペレーティングシステム２３１、ジェスチャ検出モジュール２３４、対話モードセレクタ２３６、および対話モードモジュール２３８に関連するプログラム命令ならびに／またはデータを記憶すし得る。ＵＩモジュール２３０、アプリケーション２３２、ジェスチャ検出モジュール２３４、対話モードセレクタ２３６、および対話モードモジュール２３８は、図１に示される対応するＵＩモジュール１３０、アプリケーション１３２、ジェスチャ検出モジュール１３４、対話モードセレクタ１３６、および対話モードモジュール１３８の例を含み得る。
特定の例では、ストレージデバイス２５０、またはストレージデバイス２５０に含まれるコンポーネントのうちの１つ以上は、モバイルコンピューティングデバイス２００の外部の１つ以上のリモートコンピューティングデバイス上に（たとえば、１つ以上の外部サーバ上に）記憶され得る。いくつかの例では、１つ以上のリモートコンピューティングデバイスは、ＵＩモジュール２３０、アプリケーション２３２、および／またはオペレーティングシステム２３１を記憶および／または実行してもよい。これらの例では、１つ以上のリモートコンピューティングデバイスは、プロセッサ２２０に関して本明細書で説明するものと同様の機能を実行し得る。
In certain examples,
図２に示されるように、モバイルコンピューティングデバイス２００は、電源２２８を備え得る。いくつかの例では、電源２２８は電池でもよい。電源２２８は、コンピューティングデバイス２の１つ以上のコンポーネントに電力を供給し得る。電源２２８の非限定的な例としては、亜鉛－炭素、鉛－酸、ニッケルカドミウム（ＮｉＣｄ）、ニッケル金属水素化物（ＮｉＭＨ）、リチウムイオン（Ｌｉ－イオン）、および／またはリチウムイオンポリマー（Ｌｉ－イオンポリマー）の化学的性質を有する電池を挙げることができるが、必ずしもこれらに限定されない。いくつかの例では、電源２２８は、限られた容量（たとえば、１０００～３０００ｍＡｈ）を有し得る。
As shown in FIG. 2,
モバイルコンピューティングデバイス２００は、１つ以上のセンサ２０４も備える。いくつかの例では、センサ２０４のうちの１つ以上は、入力コンポーネント２２２および／または存在感知入力コンポーネント２２５のうちの１つ以上の例でもよい。センサ２０４は、慣性測定ユニット（ＩＭＵ）２３３を含む。たとえば、ＩＭＵ２３３は、１つ以上の高周波ジャイロスコープ（たとえば、２００Ｈｚジャイロスコープ）、加速度計、および／または磁力計などの１つ以上のジャイロスコープを含み得る。本明細書で説明するように、モバイルコンピューティングデバイス２００のセンサ２０４は、３次元空間におけるモバイルコンピューティングデバイス２００のリアルタイムの向き、回転、または他の動きを判定するように構成され得る。
１つ以上のプロセッサ２２０は、モバイルコンピューティングデバイス２００内で機能を実装し、および／または命令を実行し得る。たとえば、モバイルコンピューティングデバイス２００上のプロセッサ２２０は、アプリケーション２３２、オペレーティングシステム２３１、ＵＩモジュール２３０、対話モードセレクタ２３６、ジェスチャ検出モジュール２３４、および／または対話モードモジュール２３８の機能を実行する、ストレージデバイス２５０によって記憶された命令を受信し、実行し得る。プロセッサ２２０によって実行されるこれらの命令は、プログラム実行中にモバイルコンピューティングデバイス２００にストレージデバイス２５０内に情報を記憶させ得る。プロセッサ２２０は、オペレーティングシステム２３１およびアプリケーション２３２の命令を実行して、１つ以上の動作を実行し得る。すなわち、オペレーティングシステム２３１およびアプリケーション２３２は、本明細書で説明するさまざまな機能を実行するようにプロセッサ２２０によって動作可能であり得る。
One or
いくつかの代替例では、モバイルコンピューティングデバイス２００は、プロセッサ２２０のみから構成され得る、または他の態様ではプロセッサ２２０を備え得る。これらの例では、入力コンポーネント２２２、存在感知表示デバイス２０２、通信ユニット２２４、出力コンポーネント２２６、センサ２０４、電源２２８、およびストレージデバイス２５０は、モバイルコンピューティングデバイス２００の外部にありながら、それと（たとえば、通信チャネル２２７を介して）通信可能に結合され得る。
In some alternatives,
アプリケーション２３２は、１つ以上の異なるさまざまなアプリケーションを含み得る。電子メールアプリケーション、カメラアプリケーション、地図またはナビゲーションアプリケーション、カレンダーアプリケーション、メッセージングアプリケーション、ソーシャルメディアアプリケーション、旅行アプリケーション、ゲームアプリケーション、株式アプリケーション、および天気アプリケーションはすべて、アプリケーション２３２の例である。
図２に示すように、ジェスチャ検出モジュール２３４は、機械学習モジュール２２９を含む。センサ２０４によって提供される入力に基づいて、ジェスチャ検出モジュール２３４は、さまざまな例において、以下でさらに詳細に説明するように、オフスクリーンジェスチャ（たとえば、タップ）認識のために、図３に示すような多入力多出力畳み込みニューラルネットワークモデルを使用し得る。この検出方法によって、複数のジェスチャ属性の予測が可能になる。機械学習モジュール２２９は、そのような予測を行うように構成され得る。
As shown in FIG. 2, gesture detection module 234 includes a
たとえば、機械学習モジュール２２９は、畳み込みニューラルネットワークを利用して、慣性測定ユニット２３３などのセンサ２０４によって提供される信号を認識し得る。たとえば、特定の例では、ユーザがモバイルコンピューティングデバイス２００のハウジング２０３をタップするたびに、慣性測定ユニット２３３からのタップ誘発運動信号が取り込まれ、機械学習モジュール２２９によって使用されて、タップ位置、方向（たとえば、正面、背面、および４つのエッジ）、条件（たとえば、指パッドに対して指を用いたタッピング）、および／または印加された力が認識され得る。同様に、機械学習モジュール２２９は、ユーザがモバイルコンピューティングデバイス２００を傾けるたびに慣性測定ユニット信号を分類し得る。
For example,
このニューラルネットワークモデルは、もしあればタッチスクリーンからのタッチ位置情報（たとえば、存在感知入力コンポーネント２２５）、ならびにタップ位置（ｘ，ｙ）、方向（前／後／４つの側面）、条件（指パッド対爪）、および／または力を共同で推定するための気圧計およびＩＭＵ信号を取得する。推定されるタップ力が十分に強い場合、それは潜在的な意図的なタップと見なされる。なお、強力なタップが電話ハウジングを変形させ、内部の空気圧を圧縮し、したがって、気圧計読み取り値に影響を及ぼすと思われるため、気圧計値は有用であり得る。 This neural network model uses touch location information from the touch screen (e.g., presence sensing input component 225), if any, as well as tap location (x, y), direction (front/back/four sides), condition (finger pad anti-claw) and/or obtain barometer and IMU signals for joint estimation of force. If the estimated tap force is strong enough, it is considered a potential intentional tap. Note that the barometer reading may be useful because a strong tap would deform the phone housing and compress the air pressure inside, thus affecting the barometer reading.
図３は、本開示の１つ以上の態様に係る、ジェスチャ属性を特定するためのニューラルネットワークモデル３５２のそのような１つの使用例を示すブロック図である。この例では、図２に示す機械学習モジュール２２９は、ニューラルネットワークモデル３５２を利用して、センサ２０４によって提供されるセンサ信号の分析に基づいて、タップ属性などのジェスチャ属性を分類および／または判定することができる。場合によっては、機械学習モジュール２２９は、プロセッサ２２０に含まれる埋め込みデジタルプロセッサユニットによって実行され得る。
FIG. 3 is a block diagram illustrating one such example use of a
場合によっては、機械学習モジュール２２９は、軽量畳み込みニューラルネットワークモデルを利用してもよく、たとえば、低電力モードで埋め込みデジタルプロセッサユニット（たとえば、プロセッサ２２０のうちの１つ）を使用して実行され得る。さまざまな例では、ジェスチャ検出モジュール２３４がハウジング２０３において特定のジェスチャ（たとえば、ダブルタップ）を検出するときのみ、より計算集約的なネットワークが起動され、たとえば、プロセッサ２２０の主中央処理装置によって実行されて、位置および方向などの異なるタップ属性をもたらす。そうすることによって、ジェスチャ検出モジュール２３４は、潜在的に誤ったトリガケース（たとえば、表示デバイス２０２において表示するための視覚的な壁紙出力との意図しない対話）を制限し、モバイルコンピューティングデバイス２００の消費電力を最大限に低減することができる。
In some cases,
図３に示されるように、ニューラルネットワークモデル３５２は、センサ２０４から、１つ以上のタッチスクリーン信号３５３（たとえば、存在感知入力コンポーネント２２５に関連付けられた信号）、慣性測定ユニット２３３によって提供される１つ以上の慣性測定ユニット信号３５４、および１つ以上の他の任意のセンサ信号３５５（たとえば、センサ２０４に含まれる場合、気圧計からの信号）等の種々の異なる信号入力を受信し得る。これらの入力信号３５３、３５４、３５５を入力として処理することによって、機械学習モジュール２２９のニューラルネットワークモデル３５２は、ジェスチャに関連するタップ位置３５６、ジェスチャに関連するタップ方向３５７、ジェスチャに関連するタップ条件３５８、および／またはジェスチャに関連するタップ力３５９などのさまざまなジェスチャ属性を、予測または特定し得る。
As shown in FIG. 3, the
タップ位置３５６は、（ｘ，ｙ）位置またはエリアなど、モバイルコンピューティングデバイス２００のハウジング２０３の部分上のタップイベントの予測される位置を指示または特定し得る。タップ位置３５６は、ハウジング２０３の任意の部分に、または存在感知表示デバイス２０２に配置され得る。タップ方向３５７は、予測方向（たとえば、ハウジング２０３の前面部分、ハウジング２０３の背面部分、ハウジング２０３の右エッジ部分／側面部分、ハウジング２０３の左エッジ部分／側面部分、ハウジング２０３の上部エッジ部分／側面部分、ハウジング２０３の底部エッジ部分／側面部分）を表示または特定し得る。タップ条件３５８は、タップが爪でまたは指のパッドで行われるかといった、タップの１つ以上の条件を特定または表示してもよく、タップ力３５９は、タップによって加えられる力の量を表示または特定し得る。場合によっては、推定タップ力が十分に強い場合、機械学習モジュール２２９は、タップを偶発的なタップではなく意図的なタップと見なし得る。
Tap location 356 may indicate or identify the expected location of a tap event on a portion of
図２の対話モードモジュール２３８は、ジェスチャ検出モジュール２３４によって特定されたジェスチャに基づいて、対話モードセレクタ２３６によって選択され得るさまざまな異なるモジュールを含む。特定の例では、対話モードモジュール２３８のうちの１つ以上は、オペレーティングシステム２３１に含まれ得る、またはそれによって実行され得る。たとえば、対話モードモジュール２３８は、アクセシビリティモードモジュール２４０と、アシストモードモジュール２４１と、慣性モードモジュール２４２と、背景モードモジュール２４３とを含む。さまざまな例では、ジェスチャ検出モジュール２３４は、対話モードセレクタ２３６に特定の対話モードを選択させ得る、ダブルバックタップジェスチャなどの（たとえば、あらかじめ定義された）特定のジェスチャを特定し得る。この特定のジェスチャは、ダブルバックタップジェスチャ、シングルバックタップジェスチャ、ハウジング２０３の側面部分（たとえば、右側面部分）および／または表示デバイス２０２の部分上で実行されるエッジジェスチャなどの任意の形態のジェスチャであり得る。場合によっては、モバイルコンピューティングデバイス２００は、対話モードセレクタ２３６に対話モードを選択させるために使用され得る特定のジェスチャのユーザカスタマイズを可能にし得る。
ジェスチャ検出モジュール２３４が特定のジェスチャを特定すると、対話モードセレクタ２３６は、対話モードモジュール２３８のうちの１つに対応する対話を選択し得る。対話モードセレクタ２３６は、１つ以上の因子（たとえば、モバイルコンピューティングデバイス２００の現在の動作状態、現在実行されているアプリケーション２３２の現在のアプリケーション（複数可）、ジェスチャ検出モジュール２３４による１つ以上前のおよび／または後のジェスチャの特定、センサ２０４によって提供される他のセンサ信号の検出、ＵＩモジュール２３０によって検出される他の入力および／または出力）に基づいて、そのような選択を行い得る。次に、対話モードセレクタ２３６および／またはオペレーティングシステム２３１は、選択された対話モードに基づいて、アクセシビリティモードモジュール２４０、アシストモードモジュール２４１、慣性モードモジュール２４２、または背景モードモジュール２４３のうちの１つを実行し得る。
Once gesture detection module 234 identifies a particular gesture, interaction mode selector 236 may select the interaction that corresponds to one of
図１Ａ～図１Ｃを参照して上述したように、アクセシビリティモードモジュール２４０によって提供されるアクセシビリティモードは、ユーザが表示デバイス１０２などのモバイルコンピューティングデバイス２００と対話している間に、音声および／または視覚アクセシビリティ機能を提供し得る。たとえば、そのようなアクセシビリティモード中に、ユーザは、１本以上の指を使用して、表示デバイス２０２において出力されるコンテンツを探索してもよい。オンスクリーンタッチイベントのみに依存するのではなく、アクセシビリティモードモジュール２４０はまた、ジェスチャ検出モジュール２３４によって特定されるオフスクリーンジェスチャを処理する。
As discussed above with reference to FIGS. 1A-1C, the accessibility mode provided by the accessibility mode module 240 allows the user to use voice and/or May provide visual accessibility features. For example, during such an accessibility mode, a user may use one or more fingers to explore content output on
アクセシビリティ機能のためのオンスクリーンタッチイベントの唯一の使用は、特定の制限につながる場合がある。たとえば、オンスクリーンタッチイベントのみを使用する音声アクセシビリティモードによるオブジェクト選択は、オンスクリーンタップジェスチャおよび／またはオンスクリーンドラッグ／スワイプジェスチャで実行されてもよい。しかしながら、オンスクリーンアクセシビリティジェスチャのこのようなパラダイムは、タッチ探索継続（たとえば、指－表面接触）の中断につながり得る。探索継続は、触読、または視覚障害者が感覚および触覚を用いて世界を認識し得る方法に関する、一般的な挙動である。 The sole use of on-screen touch events for accessibility features may lead to certain limitations. For example, object selection with a voice accessibility mode using only on-screen touch events may be performed with on-screen tap gestures and/or on-screen drag/swipe gestures. However, such a paradigm of on-screen accessibility gestures can lead to disruption of touch exploration continuity (eg, finger-surface contact). Continuous exploration is a common behavior related to tactile reading, or the way visually impaired people can perceive the world using their senses and touch.
くわえて、ユーザが、オンスクリーンタッチイベントのみを使用する音声アクセシビリティモードを有効にすると、これらのユーザは、もはや、直接使用のために設計される従来のジェスチャナビゲーションシステムを使用することができない場合がある。代わりに、音声アクセシビリティユーザは、マルチタッチジェスチャまたはスワイプアップおよびスワイプダウンなどのジェスチャの組み合わせを伴い得るシステムナビゲーションジェスチャのより複雑なセットを開発しなければならず、これによって、使用が非常に困難になる場合がある。これらの制限は長期ユーザにとって学習のハードルになる場合があるが、音声アクセシビリティおよびサポート機能への頻繁かつ一時的なアクセスから利益を得ることが可能な新規ユーザに、そのようなアクセシビリティモードを全く使用させない場合もある。 Additionally, when users enable a voice accessibility mode that uses only on-screen touch events, these users may no longer be able to use traditional gesture navigation systems designed for direct use. be. Instead, voice accessibility users must develop a more complex set of system navigation gestures that may involve multi-touch gestures or combinations of gestures such as swipe up and swipe down, making them extremely difficult to use. It may happen. While these limitations may present a learning hurdle for long-term users, such accessibility modes may not be used at all for new users who may benefit from frequent and temporary access to voice accessibility and support features. In some cases, it may not be allowed.
したがって、本開示の技術によれば、アクセシビリティモードモジュール２４０は、ジェスチャ検出モジュール２３４によって特定されるオフスクリーンジェスチャをさらに利用し、ユーザが、アクセシビリティモードで動作している間、より典型的なオンスクリーンシステムナビゲーションジェスチャを使用し続けることを可能にし得る。ナビゲーションジェスチャの新しいセットの開発とは対照的に、アクセシビリティモードモジュール２４０の（たとえば、バックタップジェスチャなどのオフスクリーンジェスチャを使用した）実行および使用は、アクセシビリティユーザが、表示されたＧＵＩ内の項目のナビゲーションおよび探索中に従来のシステムナビゲーションジェスチャを使用し続けることを可能にする。その結果、探索継続を維持することができる。 Thus, in accordance with the techniques of this disclosure, accessibility mode module 240 further utilizes off-screen gestures identified by gesture detection module 234 to allow users to perform more typical on-screen gestures while operating in accessibility mode. It may be possible to continue using system navigation gestures. In contrast to developing a new set of navigation gestures, execution and use of the accessibility mode module 240 (e.g., using off-screen gestures such as the back-tap gesture) allows the accessibility user to Allows you to continue using traditional system navigation gestures during navigation and exploration. As a result, search continuation can be maintained.
本開示の技術によれば、アクセシビリティモードモジュール２４０は、ユーザが音声および／または視覚アクセシビリティモードを迅速に呼び出し利用するためにオフスクリーンジェスチャを実行することを可能にする探索ジェスチャ（たとえば、探索タップ）技術を提供し得る。たとえば、さまざまな場合において、アクセシビリティモードモジュール２４０は、アクセシビリティモードの音声および／または視覚アクセシビリティ特徴への迅速なアクセスを開始するために、ジェスチャ検出モジュール２３４によって特定されたデバイス背面タップジェスチャ（たとえば、モバイルコンピューティングデバイス２００の１つ以上の背面部分上のダブルバックタップジェスチャ）を処理し得る。迅速なアクセスおよび探索継続の目標を達成するために、アクセシビリティモードモジュール２４０は、さまざまな例において、デバイス背面ジェスチャ（たとえば、タップ）検出を利用して、対話中にオンザフライでアクセシビリティモードの呼び出しおよび終了を行うようにユーザに促し、特定の場合、両手対話パラダイムにおける触覚読み取り挙動を模倣し得る。
In accordance with the techniques of this disclosure, the accessibility mode module 240 provides a search gesture (e.g., a search tap) that allows a user to perform off-screen gestures to quickly invoke and utilize audio and/or visual accessibility modes. technology can be provided. For example, in various cases, accessibility mode module 240 detects a device back tap gesture (e.g., a mobile A double back tap gesture on one or more back portions of
このモードでは、ユーザは、タッチ探索指（たとえば、図１Ｂに示される指１１５）を使用して、タッチ探索のために画面上を探索および滑走して、アクセシビリティモードモジュール２４０に、たとえば、ナビゲートされたオンスクリーンコンテンツに対応する可聴音声を出力させることができる。このタッチ探索期間中、ユーザは、たとえば、ハウジング２０３の背面部分上でシングルバックタップジェスチャを実行して、話されたオブジェクトの選択を確認することができる。ユーザは、タッチ探索指を持ち上げてアクセシビリティモードを終了し、次に、さらに別のシステムナビゲーションのために典型的なオンスクリーンの指ベースのジェスチャを使用することができる。したがって、このアプローチによって、新しい音声アクセシビリティユーザが、複雑なセットの新しいシステムナビゲーションジェスチャを学習する必要がなく、代わりに、ユーザが、アクセシビリティモード中に特定のタスクを実行するために別個のオフスクリーンジェスチャを使用することが可能になる。
In this mode, the user uses a touch-searching finger (e.g.,
たとえば、ユーザは、アクセシビリティモードモジュール２４０によって提供されるアクセシビリティモードを呼び出すために、ハウジング２０３の背面部分上でダブルタップジェスチャを実行することができる。次に、ユーザは、アクセシビリティモードモジュール２４０が、たとえば、センサ２０４および／もしくは入力コンポーネント２２５からの１つ以上のグループまたは受信したセンサ信号に基づくディスプレイデバイス２０２におけるタッチジェスチャを識別することに基づいて、ディスプレイデバイス２０２において出力されるＧＵＩ上でタッチ探索指（たとえば、図１Ｂの指１１５）をドラッグして、音声サポートを聞き、かつ／またはオンスクリーンの表示オブジェクトの視覚サポート（たとえば、拡大）を見ることができる。このタイプの対話は、ユーザが、アクセシビリティモードのフェーズ全体にわたってタッチ探索指を画面上に維持して、探索継続を提供し、触覚読み取り挙動の効果を模倣できることを意味する。タッチ探索中にオンスクリーンオブジェクトを選択するために、ユーザは、タッチ探索指をオブジェクトの上にホバリングさせ続け、たとえば、図１Ａに示す手１１３等のモバイルコンピューティングデバイス２００を保持している手を使用して、オブジェクトを選択するために、たとえば、ハウジング２０３の背面部分上でシングルバックタップジェスチャを行うことができる。この継続はまた、オブジェクト選択のためにシングルバックタップジェスチャを使用することによって、対話の労力を低減させることができ、これは、仮想オンスクリーンキーボードを使用するタイピングなど、頻繁なオブジェクト選択が使用される場合に潜在的に大きな利益であり得る。対話のあるフェーズの後で、ユーザがアクセシビリティモードを終了することを望む場合、ユーザは、タッチ探索指を表示デバイス２０２から持ち上げてアクセシビリティモードを終了することができ、次に、従来のオンスクリーンジェスチャナビゲーションシステムを使用して、表示デバイス２０２においてさらに別の動作を実行することができる。
For example, a user may perform a double tap gesture on the back portion of
一例として、ＵＩモジュール２３０は、アプリケーション２３２などの異なるコンポーネントまたはアプリケーションに関連付けられたさまざまなグラフィカルアイコン、リンク、および／または他の情報を出力し得る。図１Ａでは、実行され得る４つの異なるアプリケーション（たとえば、「アプリ１」、「アプリ２」、「アプリ３」、および「アプリ４」）に対応する４つのそのようなグラフィカルアイコンが表示される。他の例では、表示コンポーネント２２３は、ハイパーテキストまたはウェブリンク、テキストまたはシンボル、グラフィカルキーボードのコンポーネント（たとえば、キー）などの任意の形態の他のグラフィカル情報またはアイコンを表示することができ、ウェブリンクは、モバイルコンピューティングデバイス２００によってアクセスされ得る１つ以上のウェブサイトに関連付けられ得る、および／または、グラフィカルキーボードのキーが、対話モードモジュール２３８による１つ以上の機能の実行中にモバイルコンピューティングデバイス２００によって選択され得る。
As one example, UI module 230 may output various graphical icons, links, and/or other information associated with different components or applications, such as
表示コンポーネント２２３を使用して表示される情報は、存在感知表示デバイス２０２における表示の関連する位置または領域を有し得る。たとえば、図１Ａの例では、これらの４つのグラフィカルアイコンの各々は、ＧＵＩの特定のそれぞれの領域に表示され得る。すなわち、第１のアプリケーション（「アプリ１」）に関連付けられた第１のアイコンは、ＧＵＩの領域１０８内に表示されてもよく、第２のアプリケーション（「アプリ２」）に関連付けられた第２のアイコンは、ＧＵＩの領域１１０内に表示されてもよく、第３のアプリケーション（「アプリ３」）に関連付けられた第３のアイコンは、ＧＵＩの領域１１２内に表示されてもよく、第４のアプリケーション（「アプリ４」）に関連付けられた第４のアイコンは、ＧＵＩの領域１１４内に表示されてもよい。
Information displayed using
アクセシビリティモードモジュール２４０の実行中、ユーザは、１本以上の指を使用して、表示デバイス２０２において出力されるＧＵＩのコンテンツを探索してもよい。たとえば、ユーザ（たとえば、視覚障害者ユーザ）は、１本以上の指（たとえば、右手１１３またはユーザの左手の１本以上の指）を使用して、表示デバイス２０２において１つ以上のタッチジェスチャ（たとえば、スライドジェスチャまたは動きジェスチャ）を実行することができる。センサ２０４のうちの１つ以上によって提供されるセンサ信号（たとえば、表示デバイス２０２によって出力されるＧＵＩの１つ以上の領域における任意の指のタッチまたは存在を特定する１つ以上のセンサ）に基づいて、対話モジュール１３８は、モバイルコンピューティングデバイス２００の特定の機能に関連付けられた対話モードのための少なくとも１つの視覚または音声インジケータを出力し得る。
While the accessibility mode module 240 is running, the user may use one or more fingers to explore the content of the GUI output on the
たとえば、ユーザが指を使用してＧＵＩの領域１０８内の任意の位置に触れる、またはそれに近接する場合、アクセシビリティモードモジュール２４０は、第１のアプリケーション（「アプリ１」）に関連付けられた音声インジケータを出力し得る。一例として、アクセシビリティモードモジュール２４０は、この第１のアプリケーション（たとえば、電子メールアプリケーション）の名前を聴覚的に特定するために「アプリ１」というフレーズを指定する音声出力を提供し得る。その結果、ユーザは、ユーザの指が領域１０８に触れると、この第１のアプリケーションの名前を聞くことが可能になる。ユーザがユーザの指を別の領域（たとえば、領域１１４）内の位置に移動し続ける場合、アクセシビリティモードモジュール２４０は、音声出力を提供して、この第２のアプリケーション（たとえば、計算機アプリケーション）の名前を聴覚的に特定し得る。ユーザは、表示デバイス２０２上に表示された情報をナビゲートする際に、ユーザの右手または左手のいずれかの指、たとえば、ユーザの指（複数可）が現在位置するＧＵＩの領域と関連付けられるアプリケーションの識別子の対応する名前を聞くことができる。
For example, if the user uses a finger to touch or be in close proximity to any location within
また、図１Ｃを参照して上述したように、特定の例では、表示される情報の１つ以上の音声インジケータを提供することに加えて、またはその代わりに、アクセシビリティモードモジュール２４０は、対話モード（たとえば、アクセシビリティモード）のための１つ以上の視覚的インジケータを表示デバイス２０２として出力し得る。たとえば、ユーザが第４のアプリケーション（「アプリ４」）に関連付けられた領域１１４内に指１１５を移動させ得る上記の例を続けると、「アプリ４」というフレーズを指定する音声出力を提供することに加えて、アクセシビリティモードモジュール２４０はまた、たとえば、図１Ｃに示すように、領域１１４に含まれる「アプリ４」用のグラフィカルアイコンの視覚的な拡大図を（たとえば、視覚障害者ユーザのために）提供することによって、領域１１４用の「アプリ４」に関連付けられた視覚的インジケータを提供し得る。この場合、図１Ｃに示す拡大鏡アイコン１２１はまた、ＧＵＩ内で視覚的に移動して、指１１５が移動されるとこれらの領域内のコンテンツを選択的に拡大し得る。そのような態様で、アクセシビリティモードモジュール２４０は、表示デバイス２０２において出力されたコンテンツを横切る１本以上の指のユーザナビゲーションに応答して、視覚および／または音声インジケータを提供し得る。
Also, as described above with reference to FIG. 1C, in certain examples, in addition to or in lieu of providing one or more audio indicators of the displayed information, accessibility mode module 240 provides interactive mode One or more visual indicators for (eg, accessibility mode) may be output as
そのようなナビゲーションに応答して、ユーザはまた、ＧＵＩのそれぞれの領域内に含まれるグラフィカルアイコンのいずれかを選択することが可能であってもよい。たとえば、ユーザの指が領域１１４に位置する場合、ユーザは、たとえばハウジング２０３の背面部分でシングルバックタップジェスチャを実行することなどによって、「アプリ４」用のグラフィカルアイコンを選択するために１つ以上のジェスチャを実行することができる。ジェスチャ検出モジュール２３４は、センサ２０４によって提供されるセンサ信号の別のグループに基づいて、領域１１４に関連付けられた第４のアプリケーションの実行などの特定の機能のユーザ選択を確認するために、ハウジング２０３の１つ以上の部分において実行されるこの少なくとも１つの第２のジェスチャを特定し得る。
In response to such navigation, the user may also be able to select any of the graphical icons included within each region of the GUI. For example, if the user's finger is located in
少なくとも１つの第２のジェスチャを特定することに応答して、モバイルコンピューティングデバイス２００は、対応する機能を実行（たとえば、アプリケーション２３２から第４のアプリケーションを実行）し得る。そのような態様で、アクセシビリティモードモジュール２４０は、たとえば、デバイス背面タップ検出を使用して、音声および／または視覚アクセシビリティモードに関して直感的なユーザ体験を提供し得る。オフスクリーンタップ対話は、音声および／または視覚アクセシビリティサポートの無数の新しい機会を開く。これは、アクセシビリティモードへの迅速なアクセスの可能性およびタッチ探索における継続性をユーザに与え、したがって、アクセシビリティモードにおけるより良好なユーザ体験につながり得る。
In response to identifying the at least one second gesture,
図２に示すように、対話モードモジュール２３８は、アシストモードモジュール２４１も含む。さまざまな例において、アシストモードモジュール２４１は、ユーザがモバイルコンピューティングデバイス２００のハウジング２０３上でバックタップジェスチャを使用することによって、特定の機能（たとえば、スクロールアップ、スクロールダウン、アプリケーションの切り替え）を実行することを可能にする傾斜・バックタップアシストインターフェイスを提供し得る。
As shown in FIG. 2,
モバイルコンピューティングデバイス２００のその場でのモバイル使用のために片手対話が好ましい状況が多々ある可能性がある（たとえば、ユーザが地下鉄で手すりを持っているとき、またはベッドに横たわっているとき）。くわえて、片手対話のサポートの改善も、障害のあるユーザにとって有益な場合がある。 There may be many situations in which one-handed interaction is preferred for on-the-spot mobile use of mobile computing device 200 (eg, when a user is holding a handrail on the subway or lying in bed). Additionally, improved support for one-handed interaction may also be beneficial for users with disabilities.
現在のタッチベースの対話は、典型的には、ユーザが、モバイルコンピューティングデバイス２００をあまり自然ではない把持ジェスチャで保持しながら親指タッチを行うことを伴う。このジェスチャの使用は、ユーザがモバイルコンピューティングデバイス２００をしっかりと把持することをより困難にすることがあり、ユーザが（たとえば、横たわっている間に）モバイルコンピューティングデバイス２００を空中に持ち上げている場合、このジェスチャを使用することが特に困難になる可能性がある。この問題に対処するために、アシストモードモジュール２４１は、傾斜・バックタップインターフェイスを含むアシスト実行モードを提供して、図４Ａに示すように、ユーザがより自然な電話把持ジェスチャでモバイルデバイス対話を実行することを可能にする。
Current touch-based interactions typically involve a user making a thumb touch while holding the
図４Ａ～図４Ｂは、本開示の１つ以上の態様に係る、アシストモードモジュール２４１の実行中などにアシスト対話モードを開始するように構成されたモバイルコンピューティングデバイス４００の例を示す概念図である。図４Ａ～図４Ｂでは、モバイルコンピューティングデバイス４００は、モバイルコンピューティングデバイス２００（図２）および／またはモバイルコンピューティングデバイス１００（図１Ａ～図１Ｃ）の一例でもよい。ハウジング４０３は、ハウジング２０３および／またはハウジング１０３の一例でもよい。表示デバイス４０２は、表示デバイス２０２および／または表示デバイス１０２の一例でもよい。
4A-4B are conceptual diagrams illustrating an example of a
アクセシビリティモードを開始するのと同様に、ユーザは、たとえばダブルバックタップジェスチャを実行することによって、アシストモードモジュール２４１（図２）によって提供されるアシストモードを開始することができる。たとえば、ユーザは、手４１３の人差し指４１６を使用して、ハウジング４０３の背面部分４０１でダブルバックタップジェスチャを実行することができる。ジェスチャ検出モジュール２３４は、センサ２０４によって提供されるセンサ信号（たとえば、ＩＭＵ２３３によって提供されるＩＭＵセンサ信号）の１つ以上のグループに基づいて、ジェスチャを特定し得る。他の指４１５，４１７，４１８および４１９は、モバイルコンピューティングデバイス４００を保持し得る。
Similar to initiating accessibility mode, a user can initiate an assist mode provided by assist mode module 241 (FIG. 2), for example, by performing a double back tap gesture. For example, a user may use
図４Ａに示すように、指４１６がダブルバックタップジェスチャを実行した後で、対話モードセレクタ２３６は、アシストモードモジュール２４１を開始し得る。アシストモードモジュール２４１は、表示デバイス４０２において視覚メニュー４６０を出力し得る。メニュー４６０は、異なるメニュー項目に関連付けられたアシストモード中にアシストモードモジュール２４１によって実行され得るさまざまな異なる機能の視覚的インジケータを提供する。たとえば、メニュー４６０は、メニュー項目４６１，４６２，４６４，４６５を含み得る。メニュー項目４６１は、スクロールアップ機能に関連付けられたグラフィカルな上向き矢印を含んでもよく、メニュー項目４６４は、スクロールダウン機能に関連付けられたグラフィカルな下向き矢印を含んでもよい。メニュー項目４６２は、ページバック／バックワード機能に関連付けられたグラフィカルな左矢印を備えてもよく、メニュー項目４６５は、実行され得る異なるアプリケーション１３２間でのトグルまたはスイッチのための機能に関連付けられたグラフィカルなアイコンを備えてもよい。その結果、メニュー項目４６１，４６２，４６４，４６５の各々は、アシストモードモジュール２４１によって実行され得るそれぞれの機能の視覚的なインジケータを含む。
As shown in FIG. 4A, after
アシストモードモジュール２４１および／またはジェスチャ検出モジュール２３４は、センサ２０４によって提供される動き信号から傾斜ジェスチャおよび逆タップジェスチャを認識するように構成され得る。ユーザが、アシストモードモジュール２４１によって提供されるアシスト対話モードを開始するために、ハウジング４０３の１つ以上の背面部分４０１において指４１６を使用して最初の背面タップジェスチャを実行した後で、ユーザは続いて、メニュー４６０のメニュー項目を選択するために、１つ以上の傾斜ジェスチャを使用してモバイルコンピューティングデバイス４００を傾けることができ、１つ以上の傾斜ジェスチャは、（たとえば、センサ２０４によって提供されるセンサ信号に基づいて）モバイルコンピューティングデバイス４００の検出された動きおよび／または回転に関連付けられている。次に、ユーザは、ジェスチャ検出モジュール２３４によってこれらのジェスチャを特定するために使用されるセンサ２０４によって提供されるセンサ信号（たとえば、ＩＭＵ２３３によって提供されるＩＭＵセンサ信号）の１つ以上のグループに基づいて、選択されたメニュー項目に関連付けられた機能を実行するために、１つ以上の背面部分４０１において別の（たとえば、１つの）バックタップジェスチャを実行することができる。
Assist mode module 241 and/or gesture detection module 234 may be configured to recognize tilt and reverse tap gestures from the motion signals provided by
アシストモードモジュール２４１は、図４Ａ～図４Ｂに示されるようなインターフェイスを提供し、このインターフェイスは、ユーザが、モバイルコンピューティングデバイス４００を自然な把持ジェスチャで保持し、片手対話を提供することを可能にする傾斜ジェスチャおよびバックタップジェスチャによって、通常使用されるシステムナビゲーション機能（たとえば、スクロール機能、ページバック機能、スイッチアプリケーション機能）を実行することを可能にする。
Assist mode module 241 provides an interface, as shown in FIGS. 4A-4B, that allows a user to hold
図４Ａに示すように、ユーザが傾斜ジェスチャを実行する前、およびアシストモードモジュール２４１が最初にアシスト対話モードに入った後、メニュー項目はまだ選択されていない。その結果、メニュー中心アイコン４６３が、ハイライト、シェーディング、または他の態様では目立つように表示されて、図４Ｂに示すように、メニュー項目４６１，４６２，４６４または４６５のいずれもまだ選択されていないことを示す。
As shown in FIG. 4A, before the user performs the tilt gesture and after the assist mode module 241 first enters the assist interaction mode, the menu item has not yet been selected. As a result, the
しかしながら、アシストモードが開始された後、ユーザは、手４１３を使用して１つ以上の傾斜ジェスチャを実行して、モバイルコンピューティングデバイス４００を１つ以上の方向または向きに移動および／または回転させてもよい。センサ２０４のうちの１つ以上（たとえば、ＩＭＵ２３３）は、移動および／または回転を示すセンサ信号を提供してもよく、ジェスチャ検出モジュール２３４は、これらの信号を処理して、１つ以上の傾斜ジェスチャを特定し得る。たとえば、ユーザは、モバイルコンピューティングデバイス４００を上に、下に、右に、左に、時計回りに、反時計回りに、またはそれらの任意の組合せで傾けることができる。ジェスチャ検出モジュール２３４は、特定された傾斜ジェスチャの指示、および傾斜ジェスチャの対応する属性を、アシストモードモジュール２４１に提供してもよく、これは、モバイルコンピューティングデバイス４００の検出された動きもしくは回転の方向または向きのうちの少なくとも１つを示し得る。
However, after the assist mode is initiated, the user performs one or more tilt
アシストモードモジュール２４１は、次に、モバイルコンピューティングデバイス４００の検出された動きもしくは回転の方向または向きのうちの少なくとも１つに関連付けられた傾斜ジェスチャの属性に基づいて、メニュー４６０のどのメニュー項目を選択するかを特定し得る。たとえば、アシストモードモジュール２４１は、選択されたメニュー項目へのジェスチャ属性のマッピングを定義し得る。一例として、アシストモードモジュール２４１は、その現在の位置および／または向きを考慮して、モバイルコンピューティングデバイス４００の上部に対してモバイルコンピューティングデバイス４００を上に傾ける傾斜ジェスチャに応答して、メニュー項目４６１を選択し得る。同様に、アシストモードモジュール２４１は、モバイルコンピューティングデバイス４００を下に傾ける傾斜ジェスチャに応答して、メニュー項目４６４を選択し得る。アシストモードモジュール２４１は、モバイルコンピューティングデバイス４００を左に傾ける傾斜ジェスチャに応答して、メニュー項目４６２を選択することができ、モバイルコンピューティングデバイスを右に傾ける傾斜ジェスチャに応答して、メニュー項目４６５を選択し得る。
Assist mode module 241 then selects which menu items of
または、図４Ｂの例に示されるように、アシストモードモジュール２４１は、モバイルコンピューティングデバイス４００を（モバイルコンピューティングデバイス４００の現在の位置および／または向きに対して）反時計回りに傾ける傾斜ジェスチャに応答して、メニュー項目４６２を選択し、モバイルコンピューティングデバイス４００を時計回りに傾ける傾斜ジェスチャに応答して、メニュー項目４６５を選択し得る。そのような態様では、アシストモードモジュール２４１は、アシスト対話モード用のメニュー４６０のそれぞれのメニュー項目を介して１つ以上の視覚的インジケータを出力することができ、各メニュー項目は、以下でさらに説明するように、対応する機能に関連付けられている。
Alternatively, as shown in the example of FIG. 4B, assist mode module 241 performs a tilt gesture that tilts
図４Ｂでは、ユーザは、手４１３を使用して、図４Ａに示されるモバイルコンピューティングデバイス４００の位置および／または向きに対して時計回りにモバイルコンピューティングデバイスを傾けている。ジェスチャ検出モジュール２３４がこの傾斜ジェスチャの属性を検出し、この情報をアシストモードモジュール２４１に提供した結果、アシストモードモジュール２４１は、メニュー４６０のメニュー項目４６５を選択する。図４Ｂに示すように、アシストモードモジュール２４１は、メニュー項目４６５が選択されているという視覚的インジケータをユーザに提供するために、他のメニュー項目４６１，４６２，４６４に対してメニュー項目４６５をシェーディング、ハイライト、または他の態様では目立つように表示し、メニュー項目４６５は、特定の機能（たとえば、アプリケーション機能の切り替え）に関連付けられている。
In FIG. 4B, the user uses
メニュー項目４６５の表示された選択を確認するために、ユーザは、指４１６を使用して、ハウジング４０３の１つ以上の背面部分４０１においてシングルバックタップジェスチャを実行してもよい。センサ２０４から受信されたセンサ信号の１つ以上のグループに基づいてこのジェスチャを特定するバックタップジェスチャの指示をジェスチャ検出モジュール２３４から受信すると、アシストモードモジュール２４１は、メニュー項目４６５の選択を確認し、表示デバイス４０２の前景で実行および／または表示されるアプリケーション１３２間で切り替えるアプリケーション切り替え機能を実行するなど、対応する機能を実行し得る。ユーザは、ハイライトされたメニュー項目（たとえば、メニュー項目４６５）のさらに別の選択を実行するために、１つ以上の背面部分４０１においてバックタップジェスチャを実行し続けることができる。すなわち、ユーザは、図４Ｂに示すように、メニュー項目４６５がハイライトされている間に反復バックタップジェスチャを実行して、アシストモードモジュール２４１に、メニュー項目４６５に関連付けられた反復機能（たとえば、反復アプリケーションスイッチ機能）を実行させることができる。
To confirm the displayed selection of
さらに、ユーザは、メニュー４６０から異なるメニュー項目を選択するために、追加の傾斜ジェスチャを実行することができる。したがって、一例として、ユーザは、メニュー項目４６１を選択するために、モバイルコンピューティングデバイス４００を上方に傾けることができ、次に、メニュー項目４６１は、（メニュー項目４６５ではなく）表示デバイス４０２において目立つように表示することができ、ユーザは次に、メニュー項目４６１の選択を確認するために、１つ以上のバックタップジェスチャを実行することができ、メニュー項目４６１によって、アシストモードモジュールに、（たとえば、ウェブコンテンツをブラウジングしている間に）１つ以上の対応するスクロールアップ機能を実行させる。ユーザがアシスト対話モードを終了する準備ができている場合、ユーザは、たとえば、ダブルバックタップジェスチャを実行して、アシストモードモジュール２４１にこのモードを終了させることができる。この場合、ユーザは、ダブルバックタップジェスチャを実行して、アシスト対話モードの開始および終了の両方を行うことができ、これらの間に傾斜ジェスチャを実行して、アシストモードモジュール２４１に、メニュー４６０から選択されたメニュー項目に対応する機能（たとえば、システムナビゲーションまたは他の機能）を実行させることができる。
Additionally, the user may perform additional tilt gestures to select different menu items from
さまざまな例では、アシストモードモジュール２４１およびジェスチャ検出モジュール２３４の機械学習モジュール２２９の実装のために、機械学習モジュール２２９は、軽量畳み込みニューラルネットワークモデル（たとえば、ニューラルネットワークモデル３５２）を利用することができ、たとえば、低電力モードで埋め込みデジタルプロセッサユニット（たとえば、プロセッサ２２０のうちの１つ）を利用して実行され得る。このモデルは、センサ２０４によって示される任意のタップの様な信号から、タップ方向および非タップイベントを分類し得る。アシストモードモジュール２４１のインターフェイスが既に呼び出されている場合、全ての認識されたバックタップジェスチャおよび傾斜ジェスチャは、ナビゲーション機能を制御し、メニュー４６０の選択されたメニュー項目を特定するために、ジェスチャ検出モジュール２３４からアシストモードモジュール２４１および／またはオペレーティングシステム２３１に渡され得る。アシストモードモジュール２４１のインターフェイスがまだ呼び出されていない場合、デジタルプロセッサユニットおよび／またはジェスチャ検出モジュール２３４内の信号処理サービスは、意図的なダブルバックタップジェスチャを認識してアシストモードを開始するために、バックタップタイミングを分析し、ダブルタップパターンに適合するジェスチャのみを探し得る。
In various examples, for the implementation of
さまざまな例では、傾斜イベントは、たとえば、センサ２０４によって提供されるジャイロスコープデータからのヒューリスティックまたは機械学習方法のいずれかを使用して、ジェスチャ検出モジュール２３４によって検出することができる。特定の場合には、ユーザはまた、トリプルバックタップジェスチャを使用して、オンスクリーンカーソルを呼び出し、傾斜を使用して、表示デバイス２０２におけるナビゲーションのためにカーソルを操作することができる。
In various examples, tilt events can be detected by gesture detection module 234 using, for example, either heuristics or machine learning methods from gyroscope data provided by
傾斜・バックタップ対話は、無数の新しい機会を開く。そのような対話の使用は、容易かつ便利な場合があり、図４Ａ～図４Ｂに示すモバイルコンピューティングデバイス４００の自然な保持ジェスチャで片手対話を実行する可能性をユーザに付与することができ、ユーザが、たとえばニュースフィードまたはソーシャルネットワークを閲覧するときの対話挙動を可能にする。
Tilt-back-tap dialogue opens up countless new opportunities. The use of such interactions may be easy and convenient, and may provide the user with the possibility to perform one-handed interactions with natural holding gestures of the
図２の対話モードモジュール２３８はまた、慣性モードモジュール２４２を含む。慣性モードモジュール２４２は、本明細書では慣性対話モードと呼ばれるものを提供し得る。さまざまな例では、この対話モード中に、慣性モードモジュール２４２および／またはジェスチャ検出モジュール２３４は、ＩＭＵ２３３によって提供されるＩＭＵ信号からタップ力を推定することができ、また、たとえば、タップジェスチャに関連付けられた力および／または角度の変化に基づいて、機械学習モジュール２２９による出力からタップ位置も推定し得る。その結果、特定の場合、慣性モードモジュール２４２および／またはジェスチャ検出モジュール２３４は、指が手袋を装着しているまたは長い指爪を有する場合であっても、または、ハウジング２０３もしくは表示デバイス２０２上に水滴が存在する場合であっても、センサ２０４および／または入力コンポーネント２２５からのセンサ信号の１つ以上のグループに基づいて、ハウジング２０３の１つ以上の部分および／または表示デバイス２０２におけるタッチジェスチャを特定して、感知の信頼性に影響を及ぼす水滴がタッチスクリーン上に存在する場合に誤ったタッチイベントを潜在的に拒絶するように構成され得る。
The
意図しないタッチ拒絶のためにタッチ圧力を使用することと同様に、慣性モードモジュール２４２および／またはジェスチャ検出モジュール２３４は、タップジェスチャによって生成される力およびタップによって誘発される向きの変化を含む、意図的なタップによって生成される慣性を利用することができ、動作感知は、ユーザのタップ意図を識別するために使用され得る。これらの種々の属性は、センサ２０４に含まれるＩＭＵ２３３からの信号によって捕捉、またはそれらに基づいて判定することができる。さまざまな場合において、機械学習モジュール２２９は、ニューラルネットワークモデル（たとえば、図３のニューラルネットワークモデル３５２）を利用して、タップジェスチャの１つ以上の属性を判定し得る。開示される技術は、容量感知に依存しないため、デバイスのエッジを覆わないウォーターフォール画面または通常画面上で動作可能ある。したがって、表示デバイス２０２は、ウォーターフォールスクリーンまたは非ウォーターフォールスクリーンを含み得る。
As well as using touch pressure for unintentional touch rejection, inertial mode module 242 and/or gesture detection module 234 can detect intent, including the force generated by a tap gesture and the change in orientation induced by the tap. The inertia generated by a physical tap can be exploited, and motion sensing can be used to identify the user's tap intent. These various attributes can be captured by or determined based on signals from the
特定の場合、慣性モードモジュール２４２は、エッジボタンの使用のための効果的かつ人間工学的な置換を提供し得るサイドタップおよびスワイプジェスチャに関連する特定の機能を提供し得る。ボタンレスのモバイルコンピューティングデバイス（たとえば、電話）は長年のデザイン目標であり、携帯電話のデザインでは、端末のサイドエッジにカーブした画面であるウォーターフォールスクリーンがトレンドになっている。物理ボタン、特に音量制御ボタンは、さまざまな代替案が与えられているが、それらは通常、使い勝手が悪い。くわえて、視覚エッジボタン上でタッチイベントのみを使用することは、多くの誤った事例につながる可能性がある。ダブルタッチジェスチャは、（たとえば、表示デバイス２０２における）画面上の偶発的なタッチを拒絶するのに効果的であり得る。しかしながら、このジェスチャは複数のタッチイベントを伴うため、対話がより困難になり、ユーザ体験がより直感的でなくなる。 In certain cases, inertial mode module 242 may provide certain functionality related to side tap and swipe gestures that may provide an effective and ergonomic replacement for edge button use. Buttonless mobile computing devices (e.g., phones) have been a long-standing design goal, and waterfall screens, which are curved screens on the side edges of a terminal, have become a trend in mobile phone design. Physical buttons, especially volume control buttons, have been offered various alternatives, but they are usually not user-friendly. Additionally, using only touch events on visual edge buttons can lead to many false cases. A double touch gesture may be effective in rejecting accidental touches on a screen (eg, on display device 202). However, this gesture involves multiple touch events, making interaction more difficult and the user experience less intuitive.
そのような困難および課題に対処するために、慣性モードモジュール２４２は、たとえば、タイムアウト期間後にタイムアウトする音量または他のユーザインターフェイスコントロール状態にシステムをプライムするためにＩＭＵ応答を生成するサイドインパクトアクションに関連付けられた機能を提供する。タイムアウト期間中およびその満了前に、慣性モードモジュール２４２は、表示デバイス２０２におけるスワイプジェスチャを音量または他の可変ユーザインターフェイスコントロールにマッピングし得る。
To address such difficulties and challenges, the inertial mode module 242 may, for example, be associated with a side impact action that generates an IMU response to prime the system to a volume or other user interface control state that times out after a timeout period. Provide the functionality provided. During the timeout period and before its expiration, inertial mode module 242 may map swipe gestures on
その結果、慣性モードモジュール２４２は、タップ・スワイプ対話パラダイムを提供する。サイドタップに続くスワイプジェスチャは、ユーザの意図の確認でもある。人間工学的観点から、ユーザは、図５Ａ～図５Ｂに示すように、人差し指を使用してエッジをタップして、エッジインターフェイス（たとえば、音量制御インターフェイス）を呼び出すことができる。 As a result, inertial mode module 242 provides a tap-swipe interaction paradigm. The swipe gesture that follows the side tap also confirms the user's intent. From an ergonomic perspective, the user can use the index finger to tap the edge to invoke the edge interface (eg, volume control interface), as shown in FIGS. 5A-5B.
図５Ａ～図５Ｃは、本開示の１つ以上の態様に係る、タップ・スワイプ対話のための慣性対話モードを開始するように構成されたモバイルコンピューティングデバイス５００の例を示す概念図である。図５Ａ～図５Ｃの例では、ユーザは、手５１３を使用して、片手対話によってモバイルコンピューティングデバイス５００を操作することができる。モバイルコンピューティングデバイス５００のエッジにおいてまたはその近くで行われるタップ・スワイプ対話では、ユーザは、図５Ａに示すように、まず指５１６を使用してハウジング５０３の１つ以上の側面部分５７０をタップし得る。モバイルコンピューティングデバイス５００のエッジは、特定の例では、ハウジング５０３の側面部分（たとえば、左側面部分、右側面部分、上側面部分、下側面部分）においてもしくはその付近、および／または表示デバイス５０２のエッジ付近の領域を備え得る。ジェスチャ検出モジュール２３４は、このジェスチャを、指５１６の印加力だけでなく、モバイルコンピューティングデバイス５００のハウジング５０３上の対応する動きに基づき、センサ２０４から受信されたセンサ信号の１つ以上のグループ（たとえば、ＩＭＵ２３３によって提供されるＩＭＵセンサ信号）に基づいて特定し得る。
5A-5C are conceptual diagrams illustrating an example of a
ジェスチャ検出モジュール２３４がこのタップジェスチャを特定した後、慣性モードモジュール２４２は、グラフィカル音量コントロール５７４を出力し、タイムアウト期間などの定義されたまたはカスタマイズ可能な期間内の表示デバイス５０２における後続のスワイプジェスチャの（たとえば、ジェスチャ検出モジュール２３４による）特定を監視し得る。ジェスチャ検出モジュール２３４は、センサ２０４および／または存在感知入力コンポーネント２２５によって提供される画面容量感知信号の１つ以上のグループに基づいて、スワイプジェスチャを特定し得る。スワイプジェスチャの使用によって、慣性モードのエッジインターフェイスを呼び出すことができ、図５Ｂに示すように、線形制御のために表示デバイス５０２のスクリーンエッジに沿って移動し得る、または図５Ｃに示すように、マルチターゲット選択のためにスクリーンエリア内に移動し得る。グラフィカル音量コントロール５７４が図５Ｂに示されているが、任意の他の形態のグラフィカルユーザインターフェイスコントロール（たとえば、表示コントロール、別のタイプの設定コントロール）が、さまざまな他の例において使用可能である。
After gesture detection module 234 identifies this tap gesture, inertial mode module 242 outputs a
たとえば、図５Ｂに示されるように、ユーザは、指５１６を使用して、表示デバイス５０２の領域５７２で実行されるスライドジェスチャを介して、グラフィカル音量コントロール５７４を操作することができる。ユーザは、指５１６を使用して、領域５７２内で上方にスライドジェスチャを実行して、それに対応して音量コントロール５７４を上方に移動させ、音量を増加させてもよく、ユーザはまた、領域５７２内で下方にスライドジェスチャを実行して、それに対応して音量コントロール５７４を下方に移動させ、音量を下げてもよい。これに応答して、慣性モードモジュール２４２は、対応する視覚的調整をグラフィカル音量コントロール５７４（たとえば、体積の増加に対する上方向の視覚的調整、または体積の減少に対する下方向の視覚的調整）に出力し得る。音量コントロール５７４は、慣性モードモジュール２４２によって提供される慣性モード中にユーザによって操作され得るユーザインターフェイスコントロールの一例に過ぎず、音量制御要素の増減などの１つ以上の機能に関連付けられた慣性モードモジュール２４２の視覚的インジケータである。音量コントロール５７４はまた、表示デバイス５０２がウォーターフォールスクリーンを含む場合、領域５７２において、表示デバイス５０２のスクリーンエッジの近くに表示され得るユーザインターフェイスコントロールの例である。表示デバイス５０２がエッジ上にウォーターフォールスクリーンまたは容量性感知を有する場合、音量コントロール５７４などのエッジユーザインターフェイスコントロールは、サイドタップ位置（たとえば、サイド部分５７０）の付近または周囲に表示され得る。
For example, as shown in FIG. 5B, a user may use
しかしながら、表示デバイス５０２がエッジ容量感知および／またはウォーターフォールスクリーンを有さない場合、エッジユーザインターフェイスコントロールは、図５Ｃに示される領域５７６などの、手５１３の親指５１５が位置し得るエリアに近接して位置するエリアまたは領域などの、表示デバイス５０２の他のエリアに表示され得る。特定の場合には、慣性モードモジュール２４３は、ニューラルネットワークモデル３５２によって示されるように、指５１６がハウジング５０３に接触する側面部分５７０の位置を使用して、図５Ｃに示される音量コントロール５７４を表示する領域５７６を推定し得る。図５Ｃに示されるように、ユーザは、親指５１５を使用して、表示デバイス５０２の領域５７６に表示される音量コントロール５７４と対話してもよい。たとえば、ユーザは、音量コントロール５７４に含まれるターゲットオブジェクトと対話もしくは選択するために（たとえば、マルチターゲット選択のために）１つ以上のジェスチャ（たとえば、スワイプジェスチャ）を実行してよい、または慣性モードのためのグラフィカルユーザインターフェイスにおいて視覚的に示される任意の他の形態の制御機能を実行してもよい。慣性対話モードを終了または休止するために、さまざまな例では、ユーザはダブルバックタップジェスチャを実行し得る。
However, if
その結果、タップ・スワイプジェスチャは、モバイルコンピューティングデバイス５００の意図しない把持またはタッチと区別可能なジェスチャとすることができ、たとえば、片手対話の一部として使用して、エッジベースのユーザインターフェイスコントロールを操作し得る。このジェスチャは、ウォーターフォールスクリーン設計の有無にかかわらず、デバイスにわたって十分に一般化することができる。
As a result, the tap-swipe gesture can be a gesture that is distinguishable from an unintentional grasp or touch of the
図２の対話モードモジュール２３８はまた、背景モードモジュール２４３を含む。さまざまな例では、少なくともジェスチャ検出モジュール２３４が１つ以上のジェスチャを検出することに基づいて、対話モードセレクタ２３６は、実行のために背景モードモジュール２４３を選択し得る。さまざまな例では、背景モードモジュール２４３は、オフスクリーンタップ認識を使用して背景ユーザインターフェイス対話を可能にし得る。
さまざまな場合では、ユーザインターフェイスは、前景レイヤ（たとえば、ホーム画面上のアプリケーション２３２のうちの１つ以上のためのアプリケーションショートカットを有する層）と、１つ以上の背景レイヤ（たとえば、背景壁紙を有する１つ以上の層）とを含み得る。壁紙はしばしば、ロックスクリーンおよびホームスクリーンの両方に現れ得るので、ユーザに対して頻繁に露出される。したがって、プログラムユーザインターフェイスを含む壁紙サービスは、モバイルデバイスとのユーザ対話に大きな影響を及ぼす可能性がある。
In various cases, the user interface includes a foreground layer (e.g., a layer with application shortcuts for one or more of the
しかしながら、既存の技術は、前景サービスと背景サービスとの両方と対話するためにオンスクリーンタッチイベントのみに依存する。それらは同じ対話メカニズムを共有するので、特に視覚的に乱雑なインターフェイスにおいて、システムがユーザの意図を認識し、前景と背景との対話を明確に分離することは困難である。言い換えれば、ユーザがアプリケーションアイコンのエッジまたは境界をタップする場合、モバイルコンピューティングデバイス２００が（たとえば、ＵＩモジュール２３０を使用して）、ユーザがアイコンに関連付けられたアプリケーション２３２のうちの１つを開くことを意図するか、または壁紙と対話することを意図するかを決定することは困難なことがある。この曖昧さによって、壁紙サービスとの効果的な対話が妨げられ、それゆえ対話型壁紙の機能が制限される。
However, existing techniques rely only on on-screen touch events to interact with both foreground and background services. Because they share the same interaction mechanism, it is difficult for the system to recognize the user's intent and clearly separate foreground and background interactions, especially in visually cluttered interfaces. In other words, if the user taps an edge or border of an application icon, the mobile computing device 200 (e.g., using the UI module 230) allows the user to open one of the
この問題に対処するために、背景モードモジュール２４３は、ジェスチャ検出モジュール２３４によって検出されたオフスクリーンタップを使用して、壁紙サービス、ゲーム、またはフラッシュカードアプリケーションなどの背景ユーザインターフェイスと対話する。そのようなオフスクリーンジェスチャの検出によって、前景と背景との対話のユーザ意図の識別が明確になり、したがって、ユーザが、たとえば、対話型壁紙とのよりリッチな対話を実現することが可能になる。 To address this issue, background mode module 243 uses off-screen taps detected by gesture detection module 234 to interact with background user interfaces, such as wallpaper services, games, or flashcard applications. Detection of such off-screen gestures makes the identification of the user's intention of interacting with the foreground and background clear, thus enabling the user to achieve richer interaction with, for example, an interactive wallpaper. .
背景モードモジュール２４３は、ジェスチャ検出モジュール２３４および機械学習モジュール２２９によって提供される情報を使用する。機械学習モジュール２２９は、ニューラルネットワークモデル（たとえば、入力としてＩＭＵ信号を処理するマルチタスク畳み込みニューラルネットワークを備え得る、図３に示されるニューラルネットワークモデル３５２）を使用して、ハウジング２０３の背面および／または側面部分上のジェスチャ（たとえば、タップ）位置を推定するように構成されてもよい。この推定によって、背景ユーザインターフェイス対話のためのオフスクリーンタップ検出の前例のない粒度がもたらされる。ユーザがモバイルコンピューティングデバイス２００のハウジング２０３をタップするたびに、タップ誘発運動信号が捕捉され、図３に示す属性３５６，３５７，３５８，３５９などのジェスチャの属性を認識するために使用され得る。
Background mode module 243 uses information provided by gesture detection module 234 and
さまざまな例では、背景モードモジュール２４３およびジェスチャ検出モジュール２３４の機械学習モジュール２２９の実装のために、機械学習モジュール２２９は、軽量畳み込みニューラルネットワークモデルを利用することができ、たとえば、低電力モードで埋め込みデジタルプロセッサユニット（たとえば、プロセッサ２２０のうちの１つ）を使用して実行され得る。さまざまな例では、ジェスチャ検出モジュール２３４がハウジング２０３においてダブルタップイベントを検出するときのみ、より計算集約的なネットワークが起動され、メイン中央処理装置によって実行されて、たとえば、位置および方向などの異なるタップ属性をもたらす。そうすることによって、ジェスチャ検出モジュール２３４は、誤ったトリガ事例（たとえば、壁紙との意図しない対話）を潜在的に制限し、消費電力を最大限に低減することができる。
In various examples, for the implementation of
背景モードモジュール２４３は、オフスクリーンタップジェスチャ（たとえば、バックタップおよび／またはサイドタップジェスチャ）を使用して、対話モードの少なくとも１つの視覚的インジケータおよびさまざまな関連機能を提供する対話型壁紙などの背景ユーザインターフェイスとのユーザ対話を可能にして、ユーザが、前景対話と背景対話との間に明確な分離を有することを可能にし得る。すなわち、ユーザは、表示デバイス２０２におけるオンスクリーンジェスチャを使用して、前景ユーザインターフェイスまたはアプリケーションと対話することができ、代替的に、ハウジング２０３の１つ以上の部分において実行されるオフスクリーンジェスチャを使用して、背景ユーザインターフェイスまたはアプリケーションと対話することができる。ユーザは、バックタップまたはエッジタップジェスチャを使用して、対話型壁紙などの背景ユーザインターフェイスと対話することができるが、これは、さまざまな場合において、オンスクリーンタッチイベントに反応しないことがある。この技術は、ユーザ対話のために新しい場を開くことができる。そのような対話は、ロックスクリーンモードであっても行うことができ、したがって、ユーザは、フラッシュカードサービス、ニュースフィードサービス、または軽量ゲームなどのさまざまなものに対して、それをロック解除する前であっても、またはロック解除する必要なく、最速のアクセスを有することができる。一般に、エッジタップジェスチャなどのエッジジェスチャは、ハウジング２０３の１つ以上の側面部分および／または表示デバイス２０２の１つ以上の部分（たとえば、表示デバイス２０２のエッジ部分）で実行され得る。
Background mode module 243 uses off-screen tap gestures (e.g., back tap and/or side tap gestures) to create a background, such as an interactive wallpaper, that provides at least one visual indicator of interactive mode and various related functions. User interaction with the user interface may be enabled to allow the user to have a clear separation between foreground and background interaction. That is, a user can interact with a foreground user interface or application using on-screen gestures at
ニュースフィードを示す壁紙サービスの場合、ユーザが最初にハウジング２０３の背面部分でダブルバックタップジェスチャを実行して背景モードモジュール２４３に背景対話モードを開始させた後で、ユーザは、別のバックタップジェスチャを実行して次のニュースフィードに循環する、または、ハウジング２０３の側面部分および／または表示デバイス２０２の部分においてエッジタップジェスチャを実行して、別のニュースカテゴリに切り替えてもよい。次のニュースフィードに循環すること、または別のニュースカテゴリに切り替えることは、現在の背景ユーザインターフェイス要素（たとえば、現在のニュースフィード要素）を表示から削除することと、表示のために１つ以上の新しい背景ユーザインターフェイス要素（たとえば、同じまたは異なるニュースカテゴリのニュースフィード要素）を出力することとを含み得る。ジェスチャ検出モジュール２３４は、センサ２０４からのセンサ信号（たとえば、ＩＭＵ２３３によって提供されるＩＭＵセンサ信号）の１つ以上のグループの受信に基づいて、任意のジェスチャを検出し得る。場合によっては、壁紙サービスはまた、落書きゲームなどの軽いゲームを含んでもよい、または提供してもよい。これらの場合、ユーザが背景対話モードを開始するためにダブルバックタップジェスチャを実行して、背景モードモジュール２４３に背景ユーザインターフェイスアプリケーションとしてゲームを実行させると、ユーザは、ゲームと対話するために、またはゲームをプレイするために、ハウジング２０３の背面部分の異なる位置で１つ以上の追加のバックタップジェスチャを実行して、離散イベントベースの信号を超える制御の柔軟性をユーザに提供することが可能になる。たとえば、ユーザは、ハウジング２０３の背面部分の異なる位置でバックタップジェスチャを実行して、ゲームに異なるアクションを実行させることができる。背景モードモジュール２４３によって提供される視覚的出力は、以下でさらに詳細に説明されるように、１つ以上のそれぞれの機能と関連付けられる、背景モードのための１つ以上の視覚的インジケータを備え得る。
In the case of a wallpaper service showing a news feed, after the user first performs a double back tap gesture on the back portion of the
図６Ａ～図６Ｃは、本開示の１つ以上の態様に係る、背景ユーザインターフェイス対話のために背景モードモジュール２４３を開始するように構成されたモバイルコンピューティングデバイスの例を示す概念図である。これらの図は、背景ユーザインターフェイス（たとえば、壁紙）対話のためのオフスクリーンタップの例を示す。ユーザは、たとえば、バックダブルタップジェスチャによって背景インターフェイス対話を呼び出して一時停止することができ、後続のバックタップジェスチャまたはエッジタップジェスチャを実行して、特定の制御を実行することができる。 6A-6C are conceptual diagrams illustrating an example of a mobile computing device configured to initiate background mode module 243 for background user interface interactions in accordance with one or more aspects of the present disclosure. These figures show examples of off-screen taps for background user interface (eg, wallpaper) interaction. A user can, for example, invoke and pause background interface interaction with a back double tap gesture and perform a subsequent back tap gesture or edge tap gesture to perform specific controls.
具体的には、ユーザは、単純なオフスクリーンジェスチャ（たとえば、バックダブルタップ）を使用して、背景インターフェイスとの対話を開始することができる。次に、ユーザは、モバイルコンピューティングデバイス６００のハウジング６０３上の異なる位置をタップしてもよい。たとえば、図６Ａに示されるように、ユーザは、手６１３の指６１６を使用して、ハウジング６０３の１つ以上の背面部分６０１においてバックタップ（たとえば、ダブルバックタップ）ジェスチャを行い得る。ジェスチャ検出モジュール２３４は、このジェスチャを特定することができ、対話モードセレクタ２３６は、実行のために背景モードモジュール２４３を選択し得る。実行時に、表示デバイス６０２は、背景ユーザインターフェイス要素６７０およびインジケータ６７１，６７２，６７３を出力可能であり、インジケータは、現在選択されているインジケータであることを示すために、何らかのフォーマットで目立つように表示され得る。たとえば、背景ユーザインターフェイス要素６７０は、背景モードモジュール２４３の壁紙サービスによって提供されるニュースフィード項目を備えてもよく、インジケータ６７１，６７２，６７３は、異なるニュースカテゴリ（たとえば、スポーツ、天気、地方のニュース）に対応してもよい。現在選択されているインジケータ６７１がスポーツカテゴリに関連付けられている場合、背景ユーザインターフェイス要素６７０は、表示デバイス６０２で表示するために出力されるスポーツ関連ニュース情報を含んでもよい。別の例では、背景ユーザインターフェイス要素６７０は、ユーザの言語学習を助けるために壁紙サービスによって提供される言語ベースのフラッシュカードを含んでもよく、インジケータ６７１，６７２，６７３は、フラッシュカードの異なるカテゴリ（たとえば、動物、玩具、衣服）に対応してもよい。背景ユーザインターフェイス要素は、特定の例では、１つ以上のビジュアルフラッシュカード要素、ビジュアルニュース要素、または表示のために出力される他のタイプの要素を備えてもよい。
Specifically, the user can initiate interaction with the background interface using simple off-screen gestures (eg, double-tap back). The user may then tap different locations on the
後の時点で、ユーザは、後続のバックタップジェスチャ（たとえば、シングルバックタップジェスチャ）を実行して、インジケータ６７１によって指定される同じカテゴリで表示するために出力される背景ユーザインターフェイス要素を変更し得る。したがって、現在選択されているインジケータ６７１がスポーツニュースカテゴリに関連付けられている場合、ユーザは、図６Ｂに示すように、ジェスチャ検出モジュール２３４によって検出されるように、指６１６を使用してハウジング６０３の背面部分６０１でバックタップジェスチャを実行して、背景モードモジュール２４３に、背景ユーザインターフェイス要素６７０を表示から削除し、代わりに、表示デバイス６０２で異なる背景ユーザインターフェイス要素６７５を出力させてもよい。この要素６７５は、インジケータ６７１に関連付けられたスポーツニュースカテゴリの新しいスポーツ関連ニュースアイテムを含み得る。別の例では、現在選択されているインジケータ６７１が動物言語カテゴリに関連付けられている場合、ユーザは、指６１６を使用して、ハウジング６０３の背面部分６０１でバックタップジェスチャを実行して、背景モードモジュール２４３に、インジケータ６７１に関連付けられている動物言語カテゴリ内の新しい言語ベースのフラッシュカード要素を含み得る背景ユーザインターフェイス要素６７５を出力させてもよい。
At a later point, the user may perform a subsequent backtap gesture (e.g., a single backtap gesture) to change the background user interface elements that are output for display in the same category specified by
図６Ｃにも示すように、ユーザは、ハウジング６０３の１つ以上のエッジもしくは側面部分６８０において、および／または表示デバイス６０２の１つ以上の部分（たとえば、表示デバイス６０２の１つ以上のエッジ部分）においてサイドタップまたはエッジタップ（たとえば、シングルエッジタップ）ジェスチャを実行して、表示デバイス６０２での表示のために出力される背景ユーザインターフェイス要素のカテゴリを変更し得る。図６Ｃは、ユーザが指６８２を使用してこのジェスチャを実行することを示す。指６８２は、手６１３またはユーザの別の手の指のような、任意のユーザの指でもよい。
As also shown in FIG. 6C, the user may be placed at one or more edges or
ジェスチャ検出モジュール２３４がこのジェスチャを特定すると、背景モードモジュール２４３は、背景ユーザインターフェイス対話において出力される情報のカテゴリを変更し得る。たとえば、背景モードモジュール２４３は、インジケータ６７２に関連付けられた第２のカテゴリに切り替えてもよく、背景ユーザインターフェイス要素６７５を表示から削除し、代わりに、このカテゴリの背景ユーザインターフェイス要素６７７を出力してもよい。図６Ｃに示すように、背景モードモジュール２４３は、現在選択されているカテゴリとしてインジケータ６７２を視覚的に強調し得る、または他の態様では目立つように表示し得る。
Once gesture detection module 234 identifies this gesture, background mode module 243 may change the category of information output in the background user interface interaction. For example, background mode module 243 may switch to a second category associated with
したがって、現在選択されているインジケータ６７２が天気ニュースカテゴリに関連付けられている場合、ユーザは、指６８２を使用して、ジェスチャ検出モジュール２３４によって検出されるように、ハウジング６０３の１つ以上の側面部分６８０において、および／または表示デバイス６０２の１つ以上の部分においてエッジタップジェスチャを実行して、背景モードモジュール２４３に、天気ニュースカテゴリについて異なる背景ユーザインターフェイス要素６７７を表示デバイス６０２において出力させ得る。この要素６７７は、インジケータ６７２に関連付けられた天気ニュースカテゴリ内の新しい天気関連ニュース項目を含んでもよい。別の例では、現在選択されているインジケータ６７２がゲーム言語カテゴリに関連付けられている場合、ユーザは、指６８２を使用してエッジタップジェスチャを実行して、背景モードモジュール２４３に、インジケータ６７２に関連付けられているゲーム言語カテゴリ内の新しい言語ベースのフラッシュカード要素を含み得る背景ユーザインターフェイス要素６７７を出力させてもよい。背景対話モードを終了または一時停止するために、さまざまな例では、ユーザはダブルバックタップジェスチャを実行し得る。
Accordingly, if the currently selected
その結果、背景ユーザインターフェイス対話のためのオフスクリーンタップ認識の使用は、無数の新しい機会を開く。さまざまな場合において、このタイプの対話は、必ずしもスクリーンをロック解除する必要なく、および／または前景で実行されるアプリケーションに関するジェスチャ対話を妨げる必要なく、たとえば、ユーザのお気に入りの背景アプリケーションまたはサービスへの迅速なアクセスを可能にし得る。 As a result, the use of off-screen tap recognition for background user interface interactions opens up countless new opportunities. In various cases, this type of interaction can be used, for example, to quickly navigate to a user's favorite background application or service, without necessarily unlocking the screen and/or interfering with gestural interaction with applications running in the foreground. access.
図７は、本開示の１つ以上の態様に係る、モバイルコンピューティングデバイス１００（図１）および／またはモバイルコンピューティングデバイス２００（図２）などのモバイルコンピューティングデバイスによって実行されるプロセス７８０の動作例を示すフロー図である。例示のみを目的として、図７の動作は、図２に示されるモバイルコンピューティングデバイス２００を参照して説明される。
FIG. 7 illustrates operations of a
プロセス７８０は、ハウジング（たとえば、ハウジング２０３）および存在感知表示デバイス（たとえば、表示デバイス２０２）を含むモバイルコンピューティングデバイスが（たとえば、モバイルコンピューティングデバイス２００のジェスチャ検出モジュール２３４が）、少なくとも慣性測定ユニット（たとえば、慣性測定ユニット２３３）によって提供されるセンサ信号の第１のグループに基づいて、ハウジングの１つ以上の部分において実行される少なくとも１つの第１のジェスチャを特定すること（７８２）を備える。ハウジングの１つ以上の部分は、存在感知表示デバイスとは別個であり、慣性測定ユニットは、モバイルコンピューティングデバイスの１つ以上のセンサ（たとえば、センサ２０４）に含まれる。
プロセス７８０はまた、少なくとも１つの第１のジェスチャを特定することに応答して、モバイルコンピューティングデバイスが（たとえば、対話モードセレクタ２３６および／または対話モードモジュール２３８を使用して）対話モードを開始すること（７８４）と、モバイルコンピューティングデバイスの特定の機能に関連付けられた対話モードのための少なくとも１つの視覚または音声インジケータを出力すること（７８６）とを備える。プロセス７８０はまた、モバイルコンピューティングデバイスが（たとえば、ジェスチャ検出モジュール２３４を使用して）、対話モードのための少なくとも１つの視覚または音声インジケータに関連付けられた特定の機能のユーザ選択を確認するために、１つ以上のセンサによって提供されるセンサ信号の第３のグループに基づいて、ハウジングの１つ以上の部分において実行される少なくとも１つの第２のジェスチャを特定すること（７８８）を備える。少なくとも１つの第２のジェスチャを特定することに応答して、プロセス７８０はさらに、モバイルコンピューティングデバイスが（たとえば、対話モードモジュール２３８を使用して）、特定の機能を実行すること（７９０）を備える。
１つ以上の例では、説明した機能は、ハードウェア、ソフトウェア、ファームウェア、またはそれらの任意の組合せで実装され得る。ソフトウェアで実装される場合、機能は、１つ以上の命令もしくはコードとしてコンピュータ読取可能媒体上に記憶され得る、またはコンピュータ読取可能媒体を介して送信され、ハードウェアベースの処理ユニットによって実行され得る。コンピュータ読取可能媒体は、データ記憶媒体などの有形媒体に対応するコンピュータ読取可能記憶媒体、または、たとえば、通信プロトコルに従って、ある場所から別の場所へのコンピュータプログラムの転送を容易にする任意の媒体を含む通信媒体を含み得る。このように、コンピュータ読取可能媒体は、概して、（１）非一時的である有形コンピュータ読取可能記憶媒体、または（２）信号もしくは搬送波などの通信媒体に対応し得る。データ記憶媒体は、本開示で説明する技術の実行のための命令、コードおよび／またはデータ構造を取り出すために、１つ以上のコンピュータまたは１つ以上のプロセッサによってアクセス可能な任意の利用可能な媒体であり得る。コンピュータプログラム製品は、コンピュータ読取可能媒体を含み得る。 In one or more examples, the described functionality may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted as one or more instructions or code on a computer-readable medium and executed by a hardware-based processing unit. Computer-readable media refers to a computer-readable storage medium that corresponds to a tangible medium such as a data storage medium or any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol. may include a communication medium that includes. Thus, computer-readable media generally may correspond to (1) tangible computer-readable storage media that is non-transitory, or (2) a communication medium such as a signal or carrier wave. A data storage medium is any available medium that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for performance of the techniques described in this disclosure. It can be. A computer program product may include a computer readable medium.
限定ではなく一例として、そのようなコンピュータ読取可能記憶媒体は、ＲＡＭ、ＲＯＭ、ＥＥＰＲＯＭ、ＣＤ－ＲＯＭまたは他の光ディスクストレージ、磁気ディスクストレージもしくは他の磁気ストレージデバイス、フラッシュメモリ、または命令もしくはデータ構造の形態の所望のプログラムコードを記憶するために使用可能であり、かつ、コンピュータによってアクセス可能である、任意の他の記憶媒体を備え得る。また、いかなる接続もコンピュータ読取可能媒体と適切に呼ばれる。たとえば、命令が、同軸ケーブル、光ファイバーケーブル、ツイストペア、デジタル加入者回線（ＤＳＬ）、または赤外線、無線、およびマイクロ波などのワイヤレス技術を使用して、ウェブサイト、サーバ、または他のリモートソースから送信される場合、同軸ケーブル、光ファイバーケーブル、ツイストペア、ＤＳＬ、または赤外線、無線、およびマイクロ波などのワイヤレス技術は、媒体の定義に含まれる。ただし、コンピュータ読取可能記憶媒体およびデータ記憶媒体は、接続、搬送波、信号、または他の一時的媒体を含まないが、代わりに非一時的有形記憶媒体を対象とする。本明細書で使用するディスク（ｄｉｓｋおよびディスクｄｉｓｃ）は、コンパクトディスク（ｄｉｓｃ：ＣＤ）、レーザーディスク（登録商標）（ｄｉｓｃ）、光ディスク（ｄｉｓｃ）、デジタル多用途ディスク（ｄｉｓｃ：ＤＶＤ）、フロッピー（登録商標）ディスク（ｄｉｓｋ）およびＢｌｕ－ｒａｙ（登録商標）ディスク（ｄｉｓｃ）を含み、ディスク（ｄｉｓｋ）は、通常、データを磁気的に再生し、ディスク（ｄｉｓｃ）は、データをレーザーで光学的に再生する。上記の組合せもコンピュータ読取可能媒体の範囲内に含まれるべきである。 By way of example and not limitation, such computer-readable storage media may include RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, flash memory, or a memory for storing instructions or data structures. Any other storage medium usable for storing desired form of program code and accessible by the computer may be included. Also, any connection is properly termed a computer-readable medium. For example, instructions may be transmitted from a website, server, or other remote source using coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave. If so, coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. However, computer-readable storage media and data storage media do not include connections, carrier waves, signals or other transitory media, but instead refer to non-transitory tangible storage media. As used herein, the term "disk" refers to a compact disc (CD), a laser disc (registered trademark), an optical disc (DISC), a digital versatile disc (DISC), a floppy disk ( (registered trademark) disc (disk) and Blu-ray (registered trademark) disc (disc), where the disc (disc) typically reproduces data magnetically, and the disc (disc) reproduces data optically with a laser. to play. Combinations of the above should also be included within the scope of computer-readable media.
命令は、１つ以上のデジタル信号プロセッサ（ＤＳＰ）などの１つ以上のプロセッサ、汎用マイクロプロセッサ、特定用途向け集積回路（ＡＳＩＣ）、フィールドプログラマブル論理アレイ（ＦＰＧＡ）、または他の等価集積回路またはディスクリート論理回路によって実行され得る。したがって、本明細書で使用する「プロセッサ」という用語は、前述の構造、または本明細書で説明する技術の実行に好適な任意の他の構造のいずれかを指すことがある。さらに、いくつかの態様では、本明細書で説明した機能は、専用ハードウェアおよび／またはソフトウェアモジュール内で提供され得る。また、本技術は、１つ以上の回路または論理要素において完全に実装され得る。 The instructions may be implemented on one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated circuits or discrete It can be implemented by logic circuits. Accordingly, the term "processor" as used herein may refer to any of the aforementioned structures or any other structure suitable for carrying out the techniques described herein. Additionally, in some aspects, the functionality described herein may be provided within dedicated hardware and/or software modules. Also, the techniques may be implemented entirely in one or more circuits or logic elements.
本開示の技術は、ワイヤレスハンドセット、集積回路（ＩＣ）、またはＩＣのセット（たとえば、チップセット）を含む、多種多様なデバイスまたは装置において実装され得る。本開示では、開示する技術を実行するように構成されたデバイスの機能的態様を強調するためにさまざまなコンポーネント、モジュール、またはユニットについて説明したが、これらを、必ずしも異なるハードウェアユニットによって実現する必要があるとは限らない。むしろ、上述したように、さまざまなユニットは、適切なソフトウェアおよび／またはファームウェアとともに、上述したような１つ以上のプロセッサを含むハードウェアユニットにおいて組み合わされ得る、または動作中のハードウェアユニットの集合によって提供され得る。 The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including wireless handsets, integrated circuits (ICs), or sets of ICs (eg, chipsets). Although this disclosure describes various components, modules, or units to highlight functional aspects of devices configured to perform the disclosed techniques, these may not necessarily be implemented by different hardware units. There is no guarantee that there will be. Rather, as described above, the various units may be combined in a hardware unit including one or more processors as described above, together with appropriate software and/or firmware, or by a collection of hardware units in operation. may be provided.
なお、実施形態に応じて、本明細書で説明される方法のいずれかの特定の行為またはイベントは、異なるシーケンスで実行され得、追加、結合、または完全に除外され得る（たとえば、記載された行為またはイベントの全てが方法の実施に必要であるとは限らない）。さらに、特定の実施形態では、行為またはイベントは、順次ではなく、たとえば、マルチスレッド処理、割込み処理、または複数のプロセッサを通じて同時に実行され得る。 Note that, depending on the embodiment, certain acts or events of any of the methods described herein may be performed in a different sequence, and may be added, combined, or excluded entirely (e.g., the (Not all acts or events are necessary for carrying out the method). Furthermore, in certain embodiments, acts or events may be performed concurrently rather than sequentially, eg, through multi-threaded processing, interrupt processing, or multiple processors.
いくつかの例では、コンピュータ読取可能記憶媒体は、非一時的媒体を備える。「非一時的」という用語は、記憶媒体が搬送波または伝搬信号で具現化されないことを示す。特定の例では、非一時的記憶媒体は、経時的に変化し得るデータを（たとえば、ＲＡＭまたはキャッシュに）記憶し得る。 In some examples, computer-readable storage media comprises non-transitory media. The term "non-transitory" indicates that the storage medium is not embodied in a carrier wave or propagated signal. In certain examples, non-transitory storage media may store data that can change over time (eg, in RAM or cache).
さまざまな例について説明した。これらおよび他の例は、以下の特許請求の範囲内にある。 Explained various examples. These and other examples are within the scope of the following claims.
上述のように、アクセシビリティモード中、ユーザは、１本以上の指を使用して、表示デバイス１０２において出力されるＧＵＩのコンテンツを探索可能である。場合によっては、対話モードモジュール１３８は、モバイルコンピューティングデバイス１００が現在アクセシビリティモードで動作していることを示す別のグラフィカルアイコン１０６を出力してもよい。表示デバイス１０２が存在感知ディスプレイを備える場合、ユーザ（たとえば、視覚障害者ユーザ）は、１本以上の指（たとえば、右手１１３またはユーザの左手の１本以上の指）を使用して、表示デバイス１０２において１つ以上のタッチジェスチャ（たとえば、スライドジェスチャまたは動きジェスチャ）を実行し得る。センサ１０４のうちの１つ以上によって提供されるセンサ信号（たとえば、表示デバイス１０２によって出力されるＧＵＩの１つ以上の領域における任意の指のタッチまたは存在を特定する１つ以上のセンサ）に基づいて、対話モジュール１３８は、モバイルコンピューティングデバイス１００の特定の機能に関連付けられた対話モードのための少なくとも１つの視覚または音声インジケータを出力し得る。
As mentioned above, while in accessibility mode, the user can use one or more fingers to explore the content of the GUI output on the display device 102. In some cases,
図２に示されるように、モバイルコンピューティングデバイス２００は、電源２２８を備え得る。いくつかの例では、電源２２８は電池でもよい。電源２２８は、モバイルコンピューティングデバイス２００の１つ以上のコンポーネントに電力を供給し得る。電源２２８の非限定的な例としては、亜鉛－炭素、鉛－酸、ニッケルカドミウム（ＮｉＣｄ）、ニッケル金属水素化物（ＮｉＭＨ）、リチウムイオン（Ｌｉ－イオン）、および／またはリチウムイオンポリマー（Ｌｉ－イオンポリマー）の化学的性質を有する電池を挙げることができるが、必ずしもこれらに限定されない。いくつかの例では、電源２２８は、限られた容量（たとえば、１０００～３０００ｍＡｈ）を有し得る。
As shown in FIG. 2,
Claims (22)
ハウジングと存在感知表示デバイスとを含むモバイルコンピューティングデバイスが、少なくとも慣性測定ユニットによって提供されるセンサ信号の第１のグループに基づいて、前記ハウジングの１つ以上の部分において実行される少なくとも１つの第１のジェスチャを特定することを備え、前記ハウジングの前記１つ以上の部分は、前記存在感知表示デバイスとは別個であり、前記慣性測定ユニットは、前記モバイルコンピューティングデバイスの１つ以上のセンサに含まれ、前記方法はさらに、
前記少なくとも１つの第１のジェスチャを特定することに応答して、前記モバイルコンピューティングデバイスが、対話モードを開始することと、
前記モバイルコンピューティングデバイスが、前記モバイルコンピューティングデバイスの特定の機能に関連付けられた前記対話モードのための少なくとも１つの視覚または音声インジケータを出力することと、
前記モバイルコンピューティングデバイスが、前記対話モードのための前記少なくとも１つの視覚または音声インジケータに関連付けられた前記特定の機能のユーザ選択を確認するために、前記１つ以上のセンサによって提供されるセンサ信号の第２のグループに基づいて、前記ハウジングの前記１つ以上の部分において実行される少なくとも１つの第２のジェスチャを特定することと、
前記少なくとも１つの第２のジェスチャを特定することに応答して、前記モバイルコンピューティングデバイスが、前記特定の機能を実行することとを備える、方法。 A method,
A mobile computing device including a housing and a presence sensing display device is configured to perform at least one first group of sensor signals in one or more portions of the housing based on a first group of sensor signals provided by at least an inertial measurement unit. 1 gesture, the one or more portions of the housing are separate from the presence sensing display device, and the inertial measurement unit is configured to identify a gesture of one or more of the mobile computing device. and the method further comprises:
In response to identifying the at least one first gesture, the mobile computing device initiates an interaction mode;
the mobile computing device outputting at least one visual or audio indicator for the interaction mode associated with a particular feature of the mobile computing device;
a sensor signal provided by the one or more sensors for the mobile computing device to confirm user selection of the particular function associated with the at least one visual or audio indicator for the interaction mode; identifying at least one second gesture to be performed on the one or more portions of the housing based on a second group of;
and in response to identifying the at least one second gesture, the mobile computing device performs the particular function.
前記存在感知表示デバイスは、前記モバイルコンピューティングデバイスの前面側に位置し、
前記ハウジングの前記１つ以上の部分は、（ｉ）前記モバイルコンピューティングデバイスの前記前面側に隣接する前記モバイルコンピューティングデバイスの側に位置する前記ハウジングの少なくとも１つの側面部分、または（ｉｉ）前記モバイルコンピューティングデバイスの背面側に位置する前記ハウジングの少なくとも１つの背面部分のうちの１つ以上を含み、前記背面側は、前記モバイルコンピューティングデバイスの前記前面側の反対側にある、請求項１に記載の方法。 Identifying the at least one first gesture may include: the mobile computing device detecting the one or more gestures of the housing based on the first group of sensor signals provided by at least the inertial measurement unit; identifying at least one first tap gesture to be performed on the portion;
the presence sensing display device is located on the front side of the mobile computing device;
The one or more portions of the housing may be (i) at least one side portion of the housing located on a side of the mobile computing device adjacent to the front side of the mobile computing device, or (ii) the 2. The at least one rear portion of the housing located on a rear side of a mobile computing device, the rear side being opposite the front side of the mobile computing device. The method described in.
前記モバイルコンピューティングデバイスが、前記存在感知表示デバイスによって提供されるセンサ信号の第３のグループに基づいて、前記存在感知表示デバイスにおいて出力されるグラフィカルユーザインターフェイスの領域において実行される少なくとも１つの第３のジェスチャを特定することを備え、
前記アクセシビリティ対話モードのための前記少なくとも１つの視覚または音声インジケータを出力することは、前記モバイルコンピューティングデバイスが、前記少なくとも１つの第３のジェスチャが実行される前記グラフィカルユーザインターフェイスの前記領域に基づいて、前記アクセシビリティ対話モードのための前記少なくとも１つの視覚または音声インジケータを出力することを含む、請求項２に記載の方法。 The interaction mode includes an accessibility interaction mode, the one or more sensors further include the presence sensing display device, and the method further comprises:
The mobile computing device executes at least one third method in a region of a graphical user interface output at the presence sensing display device based on a third group of sensor signals provided by the presence sensing display device. comprises identifying the gesture of
Outputting the at least one visual or audio indicator for the accessibility interaction mode is configured such that the mobile computing device is configured to output the at least one visual or audio indicator for the accessibility interaction mode based on the area of the graphical user interface in which the at least one third gesture is performed. 3. The method of claim 2, comprising: outputting the at least one visual or audio indicator for the accessibility interaction mode.
前記少なくとも１つの第２のジェスチャを特定することは、前記モバイルコンピューティングデバイスが、少なくとも前記慣性測定ユニットによって提供されるセンサ信号の前記第２のグループに基づいて、前記グラフィカルユーザインターフェイスの前記領域に表示される前記コンテンツに関連付けられた前記特定の機能の前記ユーザ選択を確認するために、前記ハウジングの前記少なくとも１つの背面部分において実行されるバックタップジェスチャを特定することを含む、請求項３に記載の方法。 Outputting the at least one visual or audio indicator for the accessibility interaction mode causes the mobile computing device to display content displayed in the area of the graphical user interface in which the third gesture is performed. outputting audio speech indicating;
Identifying the at least one second gesture may include: causing the mobile computing device to respond to the area of the graphical user interface based on the second group of sensor signals provided by at least the inertial measurement unit. 4. The method of claim 3, comprising identifying a back tap gesture performed on the at least one rear portion of the housing to confirm the user selection of the particular function associated with the displayed content. Method described.
前記特定の機能を実行することは、
前記モバイルコンピューティングデバイスが、前記アプリケーションアイコンに関連付けられたアプリケーションを実行すること、
前記モバイルコンピューティングデバイスが、前記ウェブリンクに関連付けられたウェブサイトにアクセスすること、または、
前記モバイルコンピューティングデバイスが、前記グラフィカルキーボードの前記キーを選択することのうちの１つを含む、請求項４に記載の方法。 The content displayed in the area of the graphical user interface in which the third gesture is performed may include (i) an application icon, (ii) a web link, or (iii) a key on a graphical keyboard. including,
Executing the specific function includes:
the mobile computing device executes an application associated with the application icon;
the mobile computing device accesses a website associated with the web link; or
5. The method of claim 4, wherein the mobile computing device includes one of selecting the key of the graphical keyboard.
前記少なくとも１つの第２のジェスチャを特定することは、前記モバイルコンピューティングデバイスが、少なくとも前記慣性測定ユニットによって提供されるセンサ信号の前記第２のグループに基づいて、前記グラフィカルユーザインターフェイスの前記領域に表示される前記コンテンツに関連付けられる前記特定の機能の前記ユーザ選択を確認するために、前記ハウジングの前記少なくとも１つの背面部分において実行されるバックタップジェスチャを特定することを含む、請求項３に記載の方法。 Outputting the at least one visual or audible indicator for the accessibility interaction mode may cause the mobile computing device to control the content displayed in the area of the graphical user interface in which the third gesture is performed. including outputting a visual enlargement;
Identifying the at least one second gesture may include: causing the mobile computing device to respond to the area of the graphical user interface based on the second group of sensor signals provided by at least the inertial measurement unit. 4. The method of claim 3, comprising identifying a back tap gesture performed on the at least one rear portion of the housing to confirm the user selection of the particular function associated with the displayed content. the method of.
前記アシスト対話モードを開始することに応答して、前記モバイルコンピューティングデバイスが、前記存在感知表示デバイスにおける表示のために、複数のメニュー項目を含む視覚メニューを出力することと、
前記モバイルコンピューティングデバイスが、少なくとも前記慣性測定ユニットによって提供されるセンサ信号の第３のグループに基づいて、前記モバイルコンピューティングデバイスの検出された動きまたは回転に関連付けられた傾斜ジェスチャを特定することとを備え、
前記アシスト対話モードのための前記少なくとも１つの視覚または音声インジケータを出力することは、前記モバイルコンピューティングデバイスが、前記存在感知表示デバイスにおける表示のために、前記モバイルコンピューティングデバイスの前記検出された動きもしくは回転の方向または向きのうちの少なくとも１つに基づいて、前記複数のメニュー項目からメニュー項目を出力することを含み、前記メニュー項目は前記特定の機能に関連付けられ、
前記少なくとも１つの第２のジェスチャを特定することは、前記モバイルコンピューティングデバイスが、少なくとも前記慣性測定ユニットによって提供されるセンサ信号の前記第２のグループに基づいて、前記メニュー項目に関連付けられた前記特定の機能の前記ユーザ選択を確認するために、前記ハウジングの前記少なくとも１つの背面部分において実行されるバックタップジェスチャを特定することを含む、請求項２に記載の方法。 The interaction mode includes an assisted interaction mode, and the method further includes:
In response to initiating the assisted interaction mode, the mobile computing device outputs a visual menu including a plurality of menu items for display on the presence sensing display device;
the mobile computing device identifying a tilt gesture associated with a detected movement or rotation of the mobile computing device based on at least a third group of sensor signals provided by the inertial measurement unit; Equipped with
Outputting the at least one visual or audio indicator for the assisted interaction mode allows the mobile computing device to detect the detected movement of the mobile computing device for display on the presence sensing display device. or outputting a menu item from the plurality of menu items based on at least one of a direction or an orientation of rotation, the menu item being associated with the specific function;
Identifying the at least one second gesture may include determining the at least one second gesture associated with the menu item based on the second group of sensor signals provided by at least the inertial measurement unit. 3. The method of claim 2, comprising identifying a back tap gesture performed on the at least one rear portion of the housing to confirm the user selection of a particular function.
前記背景対話モードのための前記少なくとも１つの視覚または音声インジケータを出力することは、前記モバイルコンピューティングデバイスが、前記存在感知表示デバイスにおける表示のために、前記特定の機能に関連付けられた１つ以上の背景ユーザインターフェイス要素を含む対話型壁紙を出力することを含み、
前記少なくとも１つの第２のジェスチャを特定することは、前記モバイルコンピューティングデバイスが、前記１つ以上のセンサによって提供されるセンサ信号の前記第２のグループに基づいて、前記１つ以上の背景ユーザインターフェイス要素に関連付けられた前記特定の機能の前記ユーザ選択を確認するために、前記少なくとも１つの第２のジェスチャを特定することを含み、
前記少なくとも１つの第２のジェスチャは、（ｉ）前記ハウジングの前記少なくとも１つの側面部分または前記存在感知表示デバイスの少なくとも１つの部分のうちの１つ以上において実行されるエッジタップジェスチャ、または（ｉｉ）前記ハウジングの前記少なくとも１つの背面部分において実行されるバックタップジェスチャのうちの１つを含む、請求項２に記載の方法。 The interaction mode includes a background interaction mode,
Outputting the at least one visual or audio indicator for the background interaction mode may cause the mobile computing device to output one or more visual or audio indicators associated with the particular functionality for display on the presence sensing display device. outputting an interactive wallpaper containing background user interface elements;
Identifying the at least one second gesture may include: the mobile computing device detecting the one or more background user gestures based on the second group of sensor signals provided by the one or more sensors; identifying the at least one second gesture to confirm the user selection of the particular function associated with an interface element;
The at least one second gesture is (i) an edge tap gesture performed on one or more of the at least one side portion of the housing or at least one portion of the presence sensing display device; or (ii) 3. The method of claim 2, comprising one of:) a back tap gesture performed on the at least one rear portion of the housing.
前記特定の機能を実行することはさらに、
前記モバイルコンピューティングデバイスが、前記存在感知表示デバイスにおける表示から、前記背景ユーザインターフェイス要素の第１のグループを削除することと、
前記モバイルコンピューティングデバイスが、前記存在感知ディスプレイにおける表示のために、背景ユーザインターフェイス要素の第２のグループを出力することとを含む、請求項１０に記載の方法。 the one or more background user interface elements includes a first group of background user interface elements;
Performing the specific function further comprises:
the mobile computing device removing the first group of background user interface elements from display on the presence sensitive display device;
and outputting a second group of background user interface elements for display on the presence sensing display.
前記１つ以上のセンサはさらに、前記存在感知表示デバイスを含み、
前記少なくとも１つの第１のジェスチャを特定することは、前記モバイルコンピューティングデバイスが、少なくとも前記慣性測定ユニットによって提供されるセンサ信号の前記第１のグループに基づいて、前記ハウジングの前記少なくとも１つの側面部分または前記存在感知表示デバイスの少なくとも１つの部分のうちの１つ以上において実行されるエッジタップジェスチャを特定することを含み、
前記慣性対話モードのための前記少なくとも１つの視覚または音声インジケータを出力することは、前記モバイルコンピューティングデバイスが、グラフィカルユーザインターフェイスの領域内での前記存在感知表示デバイスにおける表示のために、１つ以上のユーザインターフェイスコントロールを出力することを含み、
前記少なくとも１つの第２のジェスチャを特定することは、前記モバイルコンピューティングデバイスが、前記存在感知表示デバイスによって提供されるセンサ信号の前記第２のグループに基づいて、前記１つ以上のユーザインターフェイスコントロールに関連付けられた前記特定の機能の前記ユーザ選択を確認するために、前記グラフィカルユーザインターフェイスの前記領域において実行される前記少なくとも１つの第２のジェスチャを特定することを含む、請求項２に記載の方法。 The interaction mode includes an inertial interaction mode,
The one or more sensors further include the presence sensing display device;
Identifying the at least one first gesture is configured such that the mobile computing device detects the at least one side surface of the housing based on the first group of sensor signals provided by at least the inertial measurement unit. identifying an edge tap gesture performed on one or more of the portion or at least one portion of the presence sensing display device;
Outputting the at least one visual or audio indicator for the inertial interaction mode may cause the mobile computing device to output one or more visual or audio indicators for display on the presence-sensing display device within an area of a graphical user interface. including outputting a user interface control of
Identifying the at least one second gesture causes the mobile computing device to detect the one or more user interface controls based on the second group of sensor signals provided by the presence sensing display device. 3. The method of claim 2, comprising identifying the at least one second gesture performed in the area of the graphical user interface to confirm the user selection of the particular function associated with the Method.
前記特定の機能を実行することは、前記モバイルコンピューティングデバイスが、前記存在感知表示デバイスにおける表示のために、前記１つ以上のユーザインターフェイスコントロールに対する視覚調整を出力することを含む、請求項１３に記載の方法。 the at least one gesture includes a sliding gesture performed in the area of the graphical user interface;
14. Performing the particular function includes the mobile computing device outputting visual adjustments to the one or more user interface controls for display on the presence sensing display device. Method described.
前記少なくとも１つのさらに別のジェスチャを特定することに応答して、前記モバイルコンピューティングデバイスが、前記対話モードを終了することとをさらに備える、請求項１～１６のいずれか１項に記載の方法。 the mobile computing device executes in one or more portions of the housing to terminate the interaction mode based on a further group of sensor signals provided by the one or more sensors; identifying one further gesture;
The method of any one of claims 1-16, further comprising: in response to identifying the at least one further gesture, the mobile computing device exits the interaction mode. .
存在感知表示デバイスと、
前記存在感知表示デバイスに結合されるハウジングと、
慣性測定ユニットを含む１つ以上のセンサと、
少なくとも１つのプロセッサと、
前記少なくとも１つのプロセッサによって実行可能な命令を記憶するように構成されたコンピュータ読取可能ストレージデバイスとを備え、前記命令は、
少なくとも前記慣性測定ユニットによって提供されるセンサ信号の第１のグループに基づいて、前記ハウジングの１つ以上の部分において実行される少なくとも１つの第１のジェスチャを特定させ、前記ハウジングの前記１つ以上の部分は、前記存在感知表示デバイスとは別個であり、前記命令はさらに、
前記少なくとも１つの第１のジェスチャを特定することに応答して、対話モードを開始させ、
前記モバイルコンピューティングデバイスの特定の機能に関連付けられた前記対話モードのための少なくとも１つの視覚または音声インジケータを出力させ、
前記対話モードのための前記少なくとも１つの視覚または音声インジケータに関連付けられた前記特定の機能のユーザ選択を確認するために、前記１つ以上のセンサによって提供されるセンサ信号の第３のグループに基づいて、前記ハウジングの前記１つ以上の部分において実行される少なくとも１つの第２のジェスチャを特定させ、
前記少なくとも１つの第２のジェスチャを特定することに応答して、前記特定の機能を実行させる、モバイルコンピューティングデバイス。 A mobile computing device,
a presence sensing display device;
a housing coupled to the presence sensing display device;
one or more sensors including an inertial measurement unit;
at least one processor;
a computer-readable storage device configured to store instructions executable by the at least one processor, the instructions comprising:
determining at least one first gesture performed at one or more portions of the housing based on a first group of sensor signals provided by at least the inertial measurement unit; is separate from the presence sensing display device, and the instructions further include:
in response to identifying the at least one first gesture, initiating an interaction mode;
outputting at least one visual or audio indicator for the interaction mode associated with a particular feature of the mobile computing device;
based on a third group of sensor signals provided by the one or more sensors to confirm user selection of the particular function associated with the at least one visual or audio indicator for the interaction mode; at least one second gesture to be performed on the one or more portions of the housing;
The mobile computing device causes the particular function to be performed in response to identifying the at least one second gesture.
少なくとも慣性測定ユニットによって提供されるセンサ信号の第１のグループに基づいて、前記モバイルコンピューティングデバイスのハウジングの１つ以上の部分において実行される少なくとも１つの第１のジェスチャを特定することを含み、前記ハウジングの前記１つ以上の部分は、存在感知ディスプレイデバイスとは別個であり、前記慣性測定ユニットは、前記モバイルコンピューティングデバイスの１つ以上のセンサに含まれ、前記動作はさらに、
前記少なくとも１つの第１のジェスチャを特定することに応答して、対話モードを開始することと、
前記モバイルコンピューティングデバイスの特定の機能に関連付けられた前記対話モードのための少なくとも１つの視覚または音声インジケータを出力することと、
前記対話モードのための前記少なくとも１つの視覚または音声インジケータに関連付けられた前記特定の機能のユーザ選択を確認するために、前記１つ以上のセンサによって提供されるセンサ信号の第３のグループに基づいて、前記ハウジングの前記１つ以上の部分において実行される少なくとも１つの第２のジェスチャを特定することと、
前記少なくとも１つの第２のジェスチャを特定することに応答して、前記特定の機能を実行することとを含む、コンピュータ読取可能ストレージデバイス。 A computer-readable storage device storing instructions that, when executed, cause at least one processor of a mobile computing device to perform operations, the operations comprising:
determining at least one first gesture performed at one or more portions of a housing of the mobile computing device based on a first group of sensor signals provided by at least an inertial measurement unit; the one or more portions of the housing are separate from a presence sensing display device, the inertial measurement unit is included in one or more sensors of the mobile computing device, and the operation further comprises:
Initiating an interaction mode in response to identifying the at least one first gesture;
outputting at least one visual or audio indicator for the interaction mode associated with a particular feature of the mobile computing device;
based on a third group of sensor signals provided by the one or more sensors to confirm user selection of the particular function associated with the at least one visual or audio indicator for the interaction mode; identifying at least one second gesture to be performed on the one or more portions of the housing;
and performing the specified function in response to identifying the at least one second gesture.
22. The computer-readable storage device of claim 21, wherein the instructions further cause the at least one processor of the computing device to perform the method of any one of claims 2-18.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/070460 WO2022046151A1 (en) | 2020-08-25 | 2020-08-25 | Initiating a computing device interaction mode using off-screen gesture detection |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2023539020A true JP2023539020A (en) | 2023-09-13 |
Family
ID=72428391
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2023505446A Pending JP2023539020A (en) | 2020-08-25 | 2020-08-25 | Entering computing device interaction mode using off-screen gesture detection |
Country Status (6)
Country | Link |
---|---|
EP (1) | EP4204926A1 (en) |
JP (1) | JP2023539020A (en) |
KR (1) | KR20230056657A (en) |
CN (1) | CN116507990A (en) |
DE (1) | DE112020007543T5 (en) |
WO (1) | WO2022046151A1 (en) |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090262074A1 (en) * | 2007-01-05 | 2009-10-22 | Invensense Inc. | Controlling and accessing content using motion processing on mobile devices |
CN105474157A (en) * | 2013-05-09 | 2016-04-06 | 亚马逊技术股份有限公司 | Mobile device interfaces |
DK201870347A1 (en) * | 2018-01-24 | 2019-10-08 | Apple Inc. | Devices, Methods, and Graphical User Interfaces for System-Wide Behavior for 3D Models |
-
2020
- 2020-08-25 CN CN202080104578.5A patent/CN116507990A/en active Pending
- 2020-08-25 EP EP20768486.1A patent/EP4204926A1/en active Pending
- 2020-08-25 WO PCT/US2020/070460 patent/WO2022046151A1/en active Application Filing
- 2020-08-25 KR KR1020237002827A patent/KR20230056657A/en unknown
- 2020-08-25 DE DE112020007543.5T patent/DE112020007543T5/en active Pending
- 2020-08-25 JP JP2023505446A patent/JP2023539020A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20230056657A (en) | 2023-04-27 |
DE112020007543T5 (en) | 2023-08-03 |
CN116507990A (en) | 2023-07-28 |
EP4204926A1 (en) | 2023-07-05 |
WO2022046151A1 (en) | 2022-03-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11692840B2 (en) | Device, method, and graphical user interface for synchronizing two or more displays | |
US11609681B2 (en) | Reduced size configuration interface | |
US11474626B2 (en) | Button functionality | |
US11386266B2 (en) | Text correction | |
US11354015B2 (en) | Adaptive user interfaces | |
US11941243B2 (en) | Handwriting keyboard for screens | |
US11816325B2 (en) | Application shortcuts for carplay | |
EP3326081B1 (en) | Intelligent device identification | |
US11181988B1 (en) | Incorporating user feedback into text prediction models via joint reward planning | |
US10254948B2 (en) | Reduced-size user interfaces for dynamically updated application overviews | |
JP6602372B2 (en) | Inactive area of touch surface based on contextual information | |
US20180089166A1 (en) | User interface for providing text prediction | |
US9965035B2 (en) | Device, method, and graphical user interface for synchronizing two or more displays | |
US11249579B2 (en) | Devices, methods, and graphical user interfaces for manipulating embedded interactive content | |
JP2017531246A (en) | Handedness detection from touch input | |
KR20160016526A (en) | Method for Providing Information and Device thereof | |
KR20160016545A (en) | Method for Providing Information and Device thereof | |
JP2023539020A (en) | Entering computing device interaction mode using off-screen gesture detection |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230824 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20230824 |