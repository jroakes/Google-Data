CN109478142B - Methods, systems, and media for presenting a user interface customized for predicted user activity - Google Patents
Methods, systems, and media for presenting a user interface customized for predicted user activity Download PDFInfo
- Publication number
- CN109478142B CN109478142B CN201780043785.2A CN201780043785A CN109478142B CN 109478142 B CN109478142 B CN 109478142B CN 201780043785 A CN201780043785 A CN 201780043785A CN 109478142 B CN109478142 B CN 109478142B
- Authority
- CN
- China
- Prior art keywords
- user
- content item
- video content
- user interface
- intent
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/438—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/951—Indexing; Web crawling techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/01—Probabilistic graphical models, e.g. probabilistic networks
Abstract
Methods, systems, and media for presenting a user interface customized for predicted user activity are provided. In some embodiments, the method comprises: selecting a user of the content distribution service, causing the user device to prompt the associated user to provide activity data related to a user's intent when requesting the media content item, training a predictive model based on objective data received from a user device associated with the user and the activity data received from the user device to identify the user's intent in requesting the media content item, wherein the predictive model is trained to identify whether to present a first user interface related to the first user intent or a second user interface associated with the second user intent, causing the first user interface or the second user interface to be presented. And (5) entertainment.
Description
Technical Field
The present disclosure relates to methods, systems, and media presenting a user interface customized for predicted user activity.
Background
Many users choose to access media content from a service that collects a large number of different items of media content. Often, users may access these different media content items in different contexts. For example, in some cases a user may access instructional (instructional) video to entertain, and in other cases obtain information about how to perform a task. However, most services provide only a single user experience of consuming content, or require the user to manually select how the content will be presented.
Accordingly, it would be desirable to provide new methods, systems, and media that present a user interface customized for predicted user activity.
Disclosure of Invention
According to some embodiments of the present disclosure, mechanisms are provided for presenting a user interface customized for predicted user activity.
According to some embodiments of the present disclosure, there is provided a method of presenting a customized user interface, the method comprising: selecting at least a plurality of users of a content delivery service from among users of the content delivery service; for a plurality of user devices associated with the plurality of users: receiving a request for a media content item; receiving objective data related to a context in which the request for a media content item was made; causing each of the plurality of user devices to prompt the associated user for data relating to the user's intent when requesting the media content item; and receiving activity data generated based on user input in response to the prompt; receiving, from a first user device, an input mapping each of a plurality of user intents to at least one of a plurality of different user interfaces presenting media content items; training a predictive model using at least a portion of the objective data received from the plurality of user devices and at least a portion of the activity data received from the plurality of user devices to identify a user intent in requesting a media content item based on objective data received from user devices associated with the user, wherein the predictive model is trained to identify whether to present a first user interface associated with the first user intent or a second user interface associated with the second user intent to the user; receiving a request for a first media content item from a second user device; receiving, from the second user device, objective data related to a context in which the request for the first media content item was made; providing at least a portion of the objective data received from the second user device to the predictive model; receiving a first output from the predictive model indicating that the second user device should present the first media content item using the first user interface; in response to receiving the first output from the predictive model, cause the second user device to present the first media content item using the first user interface; receiving a request for the first media content item from a third user device; receiving, from the third user device, objective data related to a context in which the request for the first media content item was made; providing at least a portion of the objective data received from the third user device to the predictive model; receiving a second output from the predictive model indicating that the third user device should present the first media content item using the second user interface; and in response to receiving the second output from the predictive model, cause the third user device to present the first media content item using the second user interface.
In some embodiments, a first user intent of the plurality of user intents is an intent to consume the media content item to obtain information included in the media content item.
In some embodiments, a second user intent of the plurality of user intents is an intent to consume the media content item for entertainment.
In some embodiments, causing each of the plurality of user devices to prompt the associated user comprises: causing each of the plurality of user devices to query the user to determine whether the user intends to consume the requested media content primarily for entertainment or primarily for obtaining information included in the media content item.
In some embodiments, the objective data includes information indicating whether a request originated from search results provided by the content delivery service.
In some embodiments, the objective data comprises a search request for initiating the search.
According to some embodiments of the present disclosure, there is provided a method for presenting a customized user interface, the method comprising: identifying, from a plurality of user devices associated with a plurality of users, contextual information relating to a context in which a request for a media content item was made; providing a prompt to each of the plurality of user devices to provide intent information related to the user's intent when requesting the media content item; receiving the intent information in response to the prompt; generating a trained predictive model to identify a user intent when requesting a media content item using the identified contextual information and the received intent information, wherein the trained predictive model determines which version of the user interface is to be presented based on a predicted user intent determined based on information related to a context in which the request for media content was made; receiving a request for a media content item from a second plurality of user devices; for each request for a media content item received from the second plurality of user devices, identifying contextual information relevant to a context in which the request for the media content item was made; receiving an output from the predictive model for each request for a media content item received from the second plurality of user devices, the output indicating which version of the user interface to present based on at least a portion of the identified contextual information; and causing each of the second plurality of user devices to present a user interface for presenting one version of the media content based on the output from the predictive model, wherein two of the second plurality of user devices are caused to present two different versions of user interfaces to present the same item of media content based on the output of the predictive model. According to another embodiment of the present disclosure, there is provided a system for presenting a user interface, the system including: a memory storing computer-executable instructions; and a hardware processor configured to perform the method of the present embodiment when executing computer-executable instructions stored in the memory. According to another embodiment of the disclosure, a computer-readable medium (which may be a non-transitory computer-readable medium, although the present embodiments are not limited to non-transitory computer-readable media) is provided that contains computer-executable instructions that, when executed by a processor, cause the processor to perform the method of the present embodiments.
According to some embodiments of the present disclosure, there is provided a system for presenting a customized user interface, the system comprising: a memory storing computer-executable instructions; and a hardware processor, which when executing the computer-executable instructions stored in the memory, is configured to: selecting at least a plurality of users of a content delivery service from among users of the content delivery service; for a plurality of user devices associated with the plurality of users: receiving a request for a media content item; receiving objective data related to a context in which the request for a media content item was made; causing each of the plurality of user devices to prompt the associated user for data relating to the user's intent when requesting the media content item; and receiving activity data generated based on user input in response to the prompt; receiving, from a first user device, an input mapping each of a plurality of user intents to at least one of a plurality of different user interfaces presenting media content items; training a predictive model using at least a portion of the objective data received from the plurality of user devices and at least a portion of the activity data received from the plurality of user devices to identify a user intent in requesting a media content item based on objective data received from user devices associated with the user, wherein the predictive model is trained to identify whether to present a first user interface associated with the first user intent or a second user interface associated with the second user intent to the user; receiving a request for a first media content item from a second user device; receiving, from the second user device, objective data related to a context in which the request for the first media content item was made; providing at least a portion of the objective data received from the second user device to the predictive model; receiving a first output from the predictive model indicating that the second user device should present the first media content item using the first user interface; in response to receiving the first output from the predictive model, cause the second user device to present the first media content item using the first user interface; receiving a request for the first media content item from a third user device; receiving, from the third user device, objective data related to a context in which the request for the first media content item was made; providing at least a portion of the objective data received from the third user device to the predictive model; receiving a second output from the predictive model indicating that the third user device should present the first media content item using the second user interface; and in response to receiving the second output from the predictive model, cause the third user device to present the first media content item using the second user interface.
According to some embodiments of the present disclosure, a computer-readable medium (which may be a non-transitory computer-readable medium, although the present embodiments are not limited to non-transitory computer-readable media) is provided that contains computer-executable instructions that, when executed by a processor, cause the processor to perform a method of presenting a customized user interface. The method comprises the following steps: selecting at least a plurality of users of a content delivery service from among users of the content delivery service; for a plurality of user devices associated with the plurality of users: receiving a request for a media content item; receiving objective data related to a context in which the request for a media content item was made; causing each of the plurality of user devices to prompt the associated user for data relating to the user's intent when requesting the media content item; and receiving activity data generated based on user input in response to the prompt; receiving, from a first user device, an input mapping each of a plurality of user intents to at least one of a plurality of different user interfaces presenting media content items; training a predictive model using at least a portion of the objective data received from the plurality of user devices and at least a portion of the activity data received from the plurality of user devices to identify a user intent in requesting a media content item based on objective data received from user devices associated with the user, wherein the predictive model is trained to identify whether to present a first user interface associated with the first user intent or a second user interface associated with the second user intent to the user; receiving a request for a first media content item from a second user device; receiving objective data from the second user device relating to making a request for the first media content item; providing at least a portion of the objective data received from the second user device to the predictive model; receiving a first output from the predictive model indicating that the second user device should present the first media content item using the first user interface; in response to receiving the first output from the predictive model, cause the second user device to present the first media content item using the first user interface; receiving a request for the first media content item from a third user device; receiving, from the third user device, objective data related to a context in which the request for the first media content item was made; providing at least a portion of the objective data received from the third user device to the predictive model; receiving a second output from the predictive model indicating that the third user device should present the first media content item using the second user interface; and in response to receiving the second output from the predictive model, cause the third user device to present the first media content item using the second user interface.
According to some embodiments of the present disclosure, there is provided a system for presenting a customized user interface, the system comprising: means for selecting at least a plurality of users of a content delivery service from among users of the content delivery service; for a plurality of user devices associated with the plurality of users: means for receiving a request for a media content item; means for receiving objective data related to a context in which the request for a media content item was made; means for causing each of the plurality of user devices to prompt the associated user for data relating to the user's intent when requesting the media content item; and means for receiving activity data generated based on user input in response to the prompt; means for receiving, from a first user device, an input mapping each of a plurality of user intents to at least one of a plurality of different user interfaces presenting media content items; means for training a predictive model using at least a portion of the objective data received from the plurality of user devices and at least a portion of the activity data received from the plurality of user devices to identify a user intent in requesting a media content item based on objective data received from user devices associated with the user, wherein the predictive model is trained to identify whether to present a first user interface associated with a first user intent or a second user interface associated with a second user intent to the user; means for receiving a request for a first media content item from a second user equipment; means for receiving objective data from the second user device relating to making a request for the first media content item; means for providing at least a portion of the objective data received from the second user device to the predictive model; means for receiving a first output from the predictive model indicating that the second user device should present the first media content item using the first user interface; means for, in response to receiving the first output from the predictive model, causing the second user device to present the first media content item using the first user interface; means for receiving a request for the first media content item from a third user device; means for receiving objective data from the third user device related to the context in which the request for the first media content item was made; means for providing at least a portion of said objective data received from said third user device to said predictive model; means for receiving a second output from the predictive model indicating that the third user device should present the first media content item using the second user interface; and means, responsive to receiving the second output from the predictive model, for causing the third user device to present the first media content item using the second user interface.
Drawings
The disclosed matter, features and advantages may be more fully understood by reference to the following detailed description of the present disclosure when considered in connection with the following drawings in which like reference numerals identify like elements. Any feature described herein in one aspect or implementation may be incorporated in any other aspect or implementation described herein.
FIG. 1 illustrates an example of a process of presenting a user interface customized for predicted user activity, in accordance with some embodiments of the present disclosure;
FIG. 2 illustrates an example of a process of receiving information related to a user's intended activity with respect to a video item, in accordance with some embodiments of the present disclosure;
FIG. 3 illustrates an example of a process of training a model to predict intended user activity, in accordance with some embodiments of the present disclosure;
FIG. 4 illustrates an example of a process of presenting a user interface customized based on predicted user activity, in accordance with some embodiments of the present disclosure;
FIG. 5 illustrates an example of a process of presenting a user interface for a predicted instructional activity, according to some embodiments of the present disclosure;
FIG. 6A illustrates an example of a user interface customized for teaching user activities according to some embodiments of the present disclosure;
FIG. 6B illustrates an example of a user interface customized for an entertainment activity, according to some embodiments of the present disclosure;
FIG. 7 illustrates a schematic diagram of a system suitable for implementing the mechanisms described herein for presenting a user interface customized for predicted user activity, in accordance with some embodiments of the present disclosure;
FIG. 8 illustrates an example of hardware that may be used in the server and/or user device of FIG. 7, according to some embodiments of the present disclosure;
FIG. 9 illustrates a more detailed example of a system suitable for implementing the mechanisms described herein for presenting a user interface customized for predicted user activity in accordance with some embodiments of the present disclosure.
Detailed Description
According to various embodiments of the present disclosure, mechanisms (which may include methods, systems, and media) are provided that present a user interface customized for predicted user activity.
In some embodiments, the mechanisms described herein may use survey data regarding the intended activity when a respondent accesses a media content item on a media platform to generate a model that can be used to predict an intended activity of an individual associated with a request for the media content item, and cause the individual to be presented with a user interface corresponding to the predicted intended activity without asking for the individual's intent. For example, the mechanism may investigate a group of users of the media platform (and/or others) about their intended activities when requesting media content items by questions of their intended activities, and obtain information indicating that some users are intended to browse video items as entertainment, for example, while others are intended to browse videos to learn how to perform tasks. In some embodiments, based on the information and information about the context in which the user may request the media content item for these activities, the mechanism may train a model to predict, for example, when the user intends to browse the video item for entertainment and/or when the user intends to browse the video item for learning how to perform a task. In some embodiments, the mechanism may use the prediction to cause a user interface customized for the predicted intended activity to be presented to the user. For example, if the model predicts that the user intends to browse videos in a group setting, the mechanism may present the user with a user interface that presents the video item in full-screen mode without presenting other content items, such as user comments, menu options, and/or other user interface features (hereinafter "secondary content items"). As another example, if the model predicts that the user intends to browse videos to shop, the mechanism may present the user with a user interface that includes more content items (e.g., details of other products, prices of products, product reviews, and/or user reviews). Thus, these embodiments only present additional content items when they are predicted to be useful to the user, so reducing the amount of data, bandwidth requirements, etc. transmitted to the user device by not transmitting additional content items otherwise. This provides for more resource efficient content distribution and is particularly advantageous when the user's device is a mobile device.
It should be noted that, as used herein, the term "media content item" may apply to video content, audio content, textual content, image content, any other suitable media content, or any suitable combination thereof.
FIG. 1 illustrates an example of a process 100 for presenting a user interface customized for predicted user activity according to some embodiments of the present disclosure.
At 102, process 100 may receive information from a user test group related to their intended activity on a media platform.
In some embodiments, process 100 may select a user test set using any suitable technique or combination of techniques. For example, process 100 may select a test set as described below in conjunction with 202 of FIG. 2.
In some embodiments, process 100 may receive any suitable information related to a user's intended activity on a media platform. For example, the process 100 may receive activity information related to user activity (e.g., information received in response to a query asking the user to input a response regarding the user's intended activity when accessing the media platform, as described below in connection with 206 of FIG. 2-such "activity information" may be considered "subjective data" or "subjective information" to distinguish from "contextual data" or "objective data" mentioned below). In another example, process 100 may receive contextual information (e.g., as described below in connection with 106) from a user device for accessing a media platform, such as information regarding a request for a video item (e.g., as described below in connection with 210 of fig. 2).
In some embodiments, process 100 may receive information using any suitable technique or combination of techniques. For example, process 100 may receive activity information by causing a user device (e.g., as described below in connection with 206 and/or 210 of fig. 2) being used to access the media platform to query the user for activity information. In another embodiment, process 100 may receive information by querying a database that collects information related to user devices and/or user accounts accessing the media platform (e.g., an intent activity database and/or a contextual information database, as described below in connection with fig. 9).
In some embodiments, where the mechanisms described herein collect personal information about a user, or where personal information may be used, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., behavioral data and/or contextual information as described above), or whether and/or how such information is used. In addition, certain data may be processed in one or more ways to remove personal information therefrom prior to storing or using the data. For example, the identity of the user may be processed such that no personal information can be determined for the user, or the geographic location of the user (e.g., city, zip code, or provincial) may be summarized at the time of location information acquisition such that no particular location of the user can be determined. Thus, the user may control how information is collected about the user and used by the mechanisms described herein.
At 104, the process 100 may train a model to predict an intended activity for a user of the media platform based on information received from the test set.
In some embodiments, process 100 may train the model using any suitable technique or combination of techniques. For example, the process 100 may use linear regression, logistic regression, other non-linear regression, stepwise regression, decision tree models, machine learning, pattern recognition, gradient enhancement, analysis of variance, cluster analysis, any other suitable modeling technique, or other suitable combination.
In some embodiments, the process 100 may train the model to produce any suitable indicator of one or more predicted intended activities. For example, the process 100 may train the model to output scores associated with one or more predicted intended activities, probabilities associated with one or more predicted intended activities, confidences associated with one or more predicted intended activities, any other suitable indicators, or any suitable combination thereof. In some embodiments, the process 100 may train the model to produce an indicator for each of the two or more predicted intended activities.
In some embodiments, process 100 may train the model using any suitable information. For example, process 100 may train a model based on information about the requested media content item (e.g., the media content item requested in conjunction with the received information from the test set). In a more specific example, the process 100 may train the model based on metadata associated with the requested media content item, such as metadata indicating a media category, a length of time, a popularity, a term describing the media content item, any other metadata associated with the requested media content item, or any suitable combination thereof.
At 106, process 100 may receive contextual information from a user device requesting a media content item.
In some real examples, the contextual information may be any suitable objective information. For example, the contextual information may be objective information related to the user device requesting the media content item, such as the type of device (e.g., mobile device, desktop computer, television device, or any other suitable type of device), the type of network to which the device is connected (e.g., mobile network, WiFi network, local area network, or any other suitable type of network), the type of application used on the user device requesting the media content item (e.g., web browser, media presentation application, media streaming application, social media application, or any other suitable type of application), the operating system used by the user device, any other suitable information related to the type of device, or any suitable combination thereof. In another example, the contextual information can be objective information related to a location of the user device requesting the media content item, such as an area associated with the user device (e.g., a time zone, a city, a province, any other suitable area, or any suitable combination thereof), a contextual location associated with the user (e.g., a home location, a work location, any other suitable contextual location, and/or any suitable combination thereof), or any other suitable information related to the user device location. In yet another example, the contextual information may be objective information related to the request for the media content item, such as a search query sent by the user device (e.g., a search query directed to the media content item), other media content item requests requested by the user device, one or more URLs recently requested by the user device, one or more URLs currently being visited in a web browser of the user device, a top-level domain or URL of a website that references the user device to a URL associated with the media content item, a time at which the user device sent the request for the media content item, any other suitable information related to the request, or any suitable combination thereof. In another example, the contextual information may be information related to the accessed media content item, such as metadata information associated with the media content item, a popularity of the media content item, any other suitable information related to the accessed media content, or any suitable combination thereof.
In some embodiments, the process 100 may receive the context information using any suitable technique or combination of techniques. For example, process 100 may request context information from a user device. In another example, process 100 may request contextual information from a database storing information (e.g., a contextual information database as described below in connection with fig. 9). In a more specific example, where the user device is logged into a known user account, the process 100 can request contextual information from a database that stores user account preferences (e.g., user account information related to language preferences, time zone preferences, media presentation preferences, any other suitable contextual information associated with the user account, or any suitable combination).
In some embodiments, where personal information about a user is collected, or may be used, as described herein, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., behavioral data and/or contextual information as described above), or whether and/or how such information is used. In addition, certain data may be processed in one or more ways to remove personal information therefrom prior to storing or using the data. For example, the user's identity information may be processed such that no personal information can be determined for the user, or the user's geographic location (e.g., city, zip code, or provincial) may be summarized at the time of location information acquisition such that no particular location of the user can be determined. Thus, a user may control how information about the user is collected and used by the mechanisms described herein.
At 108, process 100 may predict an intended activity with respect to the requested media content item based on the received contextual information and the trained model.
In some embodiments, process 100 may input the received contextual information into a trained model to predict any suitable intended user activity with respect to the media content item. For example, the trained model may predict, based on the received contextual information, that the user intends to consume the media content item as part of a business presentation, as personal entertainment, shopping, educational instructions (e.g., when the media content item is a recording of a lecture), leisure browsing, comedy entertainment, any other suitable activity, or any suitable combination thereof.
In another example, the trained model may predict, based on the received contextual information, that the user intends to consume the media content item as a group entertainment activity. In a more specific example, the trained model may predict that the user intends to watch the video item at home with one or more other people based on received contextual information indicating, for example, that the user device requests content via a WiFi connection on friday nights and that the video content is to be presented using a television. Additionally or alternatively, depending on the activity information received in 102, the trained model may predict any other suitable activity or any suitable combination of activities based on the same contextual information.
In yet another example, the trained model may predict that the user intends to consume the media content item as an instructional activity (e.g., as described below in connection with fig. 6A). In a more specific example, the trained model may predict that the user intends to view the video item as an instructional activity based on received contextual information indicating, for example, that the user device requested the video item after sending a search query containing the term "how". Additionally or alternatively, depending on the activity information received in 102, the trained model may predict any other suitable activity or any suitable combination of activities based on the same contextual information. In another more specific example, when process 100 receives a request for the same video item, but also receives contextual information indicating that the user device is a television device and that the term "interesting" is included in the search query in addition to or instead of "how," the trained model may predict that the user intends to view the video as an entertainment activity. Additionally or alternatively, depending on the activity information received in 102, the trained model may predict any other suitable activity or any suitable combination of activities based on the same contextual information.
In some embodiments, process 100 may predict the intent activity based on any suitable metric produced by the intent activity model, such as any suitable metric as described above in connection with 104. For example, where the predictive activity model produces scores and/or probabilities for two or more predicted activities, the process 100 may predict the activity with the highest score or probability. In another example, the process 100 may predict the intended activity by determining whether the metric exceeds a predetermined threshold. In such an example, if none of the indicators of intended activity exceed the predetermined threshold, the process 100 may avoid predicting intended activity.
At 110, the process 100 may present the media content item by the user device using a user interface corresponding to the predicted intent activity.
In some embodiments, process 100 may present a user interface that includes features customized for the predicted activity. For example, where process 100 predicts that the user intends to view a video as an instructional activity (e.g., as described above in connection with 106 and below in connection with fig. 6A), process 100 may cause a user interface to be presented that includes a list of videomarks (e.g., videomarks 612, 614, and 616 as described below in connection with fig. 6A) noting where particular steps of the instructional video are located and written instructions (e.g., instructions 606) corresponding to the video items. In another example, where the process 100 predicts that the user intends to present a slide as part of a commercial presentation, the process 100 may present a user interface that hides selectable user interface elements. In yet another example, where process 100 predicts that the user intends to present the video item as part of a commercial presentation, process 100 may present a user interface that includes a larger selectable user interface element (e.g., a larger pause button, a larger full screen button, any other selectable user interface element, or any suitable combination thereof) than the selectable user interface elements included in the default user interface.
In some embodiments, process 100 may present the user interface using any suitable technique or combination of techniques. For example, process 100 may respond to the request by providing instructions to the requested media content item that cause an application of the user device to present a user interface corresponding to the predicted activity. In a more specific example, where the application is a web browser and the request is sent via the web browser, the process 100 may respond to the request by providing HTML instructions that cause the web browser to present a user interface corresponding to the predicted activity. Additionally or alternatively, the process 100 may respond to a request sent via a web browser by redirecting to a web page accessible to the requested media content item, the web page including a user interface corresponding to the predicted activity.
In some embodiments, process 100 may present a default user interface including user selectable features that are pre-activated corresponding to the predicted activity in addition to or in lieu of presenting a user interface including customized features. This further reduces network traffic and required network resources as it avoids the need for the user to request process 100 to activate these features. For example, process 100 may present a default user interface that includes a pre-activated mute feature, a pre-activated full screen feature, a pre-activated cast feature (e.g., a feature that causes a media content item to be presented by another device), any other suitable pre-activated feature, or any suitable combination thereof. In another example, process 100 may present a default user interface modified to include more or fewer secondary content items, more or fewer comments, larger or smaller media presentation areas, any other suitable modification, or any suitable combination thereof.
FIG. 2 illustrates an example 200 of a process of receiving information related to a user's intended activity on a video item, in accordance with some embodiments of the present disclosure.
At 202, process 200 may select a user test set from a total group of users of the media platform.
In some embodiments, process 200 may select a user test group using any suitable information. For example, process 200 may select a test group based on information associated with the user's geographic location, age, language preference, frequency of use, user device type, any other suitable information, or any suitable combination thereof. Additionally or alternatively, the process 200 may randomly select a user test set.
In some embodiments, process 200 may select a user test set from a user population of any suitable media platform. For example, process 200 may select a user of a media platform, a third party media platform, other suitable media platform, or any suitable combination thereof using the mechanisms described herein that present a user interface customized for predicted user activity. Additionally or alternatively, the process 200 may select a test group that includes people who may not have used any media platform.
In some embodiments, process 200 may select a user test set based on any suitable information that can be associated with the user. For example, process 200 may select a user account associated with the user, an email address associated with the user, an IP address that can be associated with the user, any other suitable information associated with the user, or any suitable combination thereof.
At 204, process 200 may receive a request for a video item from a user device associated with a user that is part of the selected test group using any suitable technique or combination of techniques. For example, process 200 may receive a request for a video item from a user device logged into a user account that is part of the user test set selected at 202. In another example, process 200 may receive a request for a video item from a user device having an IP address that is part of the user test set selected at 202.
At 206, the process 200 may cause the user device to present a query related to an intended activity of the user device requesting the video item at 204.
In some embodiments, process 200 may present the query to the user using any suitable technique or combination of techniques. For example, the process 200 may send instructions to a user device requesting a video item that may cause the user device to present one or more queries related to, for example, the user's intended activity, and prompt the user to enter user input. In a more specific example, where the process 200 receives a request for a video item from a user device via a web browser, the process 200 can send HTML instructions that cause the web browser to present one or more questions to the user regarding the user's intended activity. In some embodiments, process 200 may send instructions that may cause one or more questions to be presented to the user before, during, and/or after presenting the requested video, or at any other suitable time.
In some embodiments, the query may include a user interface that allows the user to respond to the query by any suitable user input. For example, the query may include a user interface having a text window in which the user may enter a text response (e.g., via a keyboard, touch screen, voice input, or any other suitable text input device). In another example, the query may include a user interface having selectable user interface elements, where each user interface element corresponds to a different potential answer to the query.
In some embodiments, the process 200 may present the user with a query by generating and sending an email or other message that provides the user with an opportunity to answer questions regarding the user's intended activity with respect to the requested video. For example, where a user device logged into a user account requests a video item and the user account is associated with an email address, process 200 may generate and send an email to the associated email address that includes questions regarding the user's intended activity. In the above examples, the email may include any suitable prompt for answering the question, such as a prompt instructing the user to respond via email, a prompt providing the user with a hyperlink to a website where the user may answer the question, any other suitable prompt, or any suitable combination thereof
In some embodiments, the query may relate to any suitable information related to the intended activity of the user. For example, the query may relate to an environment in which the user plans to watch the video, such as a work environment, a social environment, a relaxation environment, or any other suitable environment. In another example, the query may be related to a purpose of the user viewing the video, such as a teaching purpose, an entertainment purpose, a humor purpose, an educational purpose, any other suitable purpose, or any suitable combination thereof. In yet another example, the query may be related to social aspects of the user's intended activity, such as whether the user intends to watch the video with others, whether the user is recommended to watch the video by another, whether the user intends to share the video with others, any other social aspects of the user's intended activity, or any suitable combination thereof. In another example, the query may relate to the user's attitudes and/or preferences with respect to the user interface, such as with respect to whether the user is satisfied with the user interface, whether the user prefers other user interface features, whether the user prefers to use the user interface in a different setting, and/or any other suitable relationship with the user's attitudes and/or preferences with respect to the user interface.
At 208, the process 200 may receive intent activity information based on the query.
In some embodiments, process 200 may receive the intended activity information using any suitable technique or combination of techniques. For example, where process 200 presents a query to a user using a user interface presented by an application requesting a media content item, process 200 may receive intent activity information from a user device. In another example, where process 200 causes the query to be presented to the user by email, process 200 may receive the intended activity information by email. In yet another example, where process 200 presents a query to a user through a hyperlink included in an email to a website (e.g., as described above in connection with 206) where the user may enter a response to the question, process 200 may receive the intended activity information through the website.
At 210, process 200 may receive contextual information related to the request for the video item using any suitable technique or combination of techniques. For example, process 200 may receive the context information by requesting context information from a user device requesting the video item. As another example, process 200 may request information from a database storing information (e.g., a contextual information database as described below in connection with fig. 9).
In some embodiments, the contextual information may include any suitable objective information related to the request for the video item. For example, the contextual information may include the objective information described above in connection with 106 of fig. 1.
At 212, the process 200 may associate the intent activity information received at 208 with the contextual information received at 210.
In some embodiments, process 200 may use any suitable technique or combination of techniques to associate the intent activity information and the contextual information. For example, process 200 may statistically analyze the intent activity information and the contextual information using any suitable statistical analysis technique (e.g., the statistical analysis technique described above in connection with 104 in fig. 1) to determine a correlation between the intent activity information and the contextual information. In the above example, process 200 may associate particular parameters of the contextual information with particular types of the intended activity information in response to determining a relatively high correlation. In a more specific example, the process 200 may determine that there is a relatively high correlation between a particular combination of context information parameters and intended activity information indicating that the user intends to view the requested video for entertainment.
In some embodiments, process 200 may refine the intent activity information and associate the refined information with contextual information using any suitable technique or combination of techniques. For example, process 200 may refine the data by classifying the data, encoding or re-encoding the data, removing errors, refining the data using any other suitable technique, or any suitable combination thereof.
In some embodiments, associating the intent activity information with the contextual information may be performed manually or refined manually. For example, the association of intent activity information with contextual information may be performed and/or refined based on input from an administrative user and/or a developer of the mechanisms described above.
Although process 200 is described herein as generally directed to video items, in addition or alternatively, in some embodiments, process 200 may be adapted to receive information relating to a user's intended use of any suitable type of media content item.
FIG. 3 illustrates an example 300 of a process of training a model to predict intended user activity according to some embodiments of the present disclosure.
At 302, process 300 may receive intent activity information and contextual information associated with a request for media content from a test group (e.g., the test group described above in connection with 202 of fig. 2).
In some embodiments, the process 300 may receive any suitable intended activity information. For example, the process 300 may receive the intent activity information as described above in connection with 206 of fig. 2.
In some embodiments, process 300 may receive any suitable contextual information. For example, process 300 may receive contextual information as described above in connection with 106 of fig. 1.
At 304, the process 300 may train a model to predict the user's intended activity based on the intended activity information and contextual information received at 302.
In some embodiments, the process 300 may train the model using any suitable technique or combination of techniques. For example, process 300 may use techniques as described above in connection with 104 of fig. 1.
In some embodiments, process 300 may train the model based on contextual information that is not relevant to the media content request from the test group in addition to the contextual information received at 302. For example, process 300 may merge contextual information associated with the request for other media content (e.g., pre-existing contextual information) with the contextual information received at 302 and train the model based on the merged contextual information.
In some embodiments, the process 300 may train a plurality of models, where each model of the plurality of models corresponds to a different situation and/or different user information. For example, the process 300 may train the model to predict user intent activities for users associated with a particular geographic area, users associated with known user accounts, users who frequently share content, any other suitable user information, or any suitable combination thereof. In another example, the process 300 may train a model to predict user intent activity with respect to a particular type of media content request. In a more specific example, for a video item, the process 300 may train a separate model to predict user intent activity with respect to a request for a music video, a television program, a streaming video, or any other suitable type of video item.
At 306, the process 300 may obtain behavioral data related to the use of the user interface presented based on the trained model.
In some embodiments, the process 300 may obtain any suitable behavioral data. For example, the process 300 may obtain behavioral data related to a search query, a click-through rate, a rate at which a user projects media content from a first user device to a second device, a rate at which a user shares media content items, a number of requests received for media content, a number of user account logins, comments posted by a user, any other suitable behavioral data, or any suitable combination thereof.
In some embodiments, the process 300 may obtain behavioral data related to presentation of a user interface corresponding to the predicted intent activity. For example, the process 300 may obtain behavioral data related to a user request for a different user interface after being provided with a user interface corresponding to the predicted intended activity. In a more specific example, where a user is presented with a user interface (e.g., as described below in connection with fig. 6A) corresponding to presenting a video for instructional purposes, process 300 may obtain data indicating that the user requests a different user interface to play the video.
In another example, process 300 may obtain behavior data related to a user manipulating certain features of the user interface, such as activating a full screen feature, increasing or decreasing volume, expanding or collapsing user comments, and/or any other behavior that manipulates a feature of the user interface.
In some embodiments, process 300 may use any suitable technique or combination of techniques to obtain behavioral data. For example, the process 300 may query a database storing behavioral data. In another example, the process 300 may obtain the behavior data by storing data related to a request for a media content item in response to receiving the request. In yet another example, process 300 may query a user device for behavior data stored by an application program for requesting and/or presenting media content items. In a more specific example, the process 300 may query a user device for data indicating when a user has activated certain features of an application that includes a user interface for presenting media content items and stores such data.
In some embodiments, where the mechanisms described herein collect personal information about or use personal information, a user may be provided with an opportunity to control whether programs or features collect user information (e.g., behavioral data and/or contextual information as described above), or whether and/or how such information is used. In addition, certain data may be processed in one or more ways to remove personal information therefrom prior to storing or using the data. For example, the user's identity information may be processed such that no personal information can be determined for the user, or the user's geographic location (e.g., city, zip code, or provincial) may be summarized at the time of location information acquisition such that no particular location of the user can be determined. Thus, the user may control how information is collected about the user and used by the mechanisms described herein.
In some embodiments, process 300 may obtain the behavior data by presenting queries to one or more users of the media platform related to their behavior with respect to the media platform. For example, process 300 may present a query to one or more users of the media platform as described above in connection with 206 of fig. 2. In some embodiments, the query may be related to any suitable information about the user's behavior. For example, the query may relate to a reason for the user activating a user interface feature, requesting a different user interface, requesting a different media content item, any other suitable user behavior with respect to the media platform, or any suitable combination thereof.
At 308, the process 300 may refine the intent activity model based on the obtained behavioral data.
In some embodiments, process 300 may refine the intent activity model using any suitable technique or combination of techniques based on the obtained behavioral data. For example, the process 300 may utilize machine learning algorithms to refine parameters, coefficients, and/or variables in the model based on the obtained behavioral data. In a more particular example, where the model predicts that the user intends to view the requested video for entertainment based on a set of contextual information corresponding to a set of parameters and/or variables of the model, and the user is presented with a user interface corresponding to entertainment, but the behavior data indicates that such user is dissatisfied with the user interface corresponding to entertainment, the process 300 may refine the parameters, coefficients, and/or variables of the model such that the model may predict that the intended activity is entertainment less frequently based on similar contextual information.
In some embodiments, the process 300 may refine the intent activity model by testing the model on the obtained behavioral data. For example, if the intent activity model predicts that the user associated with the request intends to view a video item as an instructional activity for a particular set of requests for the video item recorded in the obtained behavior data, but the behavior data indicates that the video item is most often viewed for entertainment (e.g., by representing that the user rarely pauses the video, frequently views the video in full screen mode, any other suitable indication that the video item is viewed for entertainment, or any suitable combination thereof), the process 300 may refine the intent activity model so that it can predict the intended activity as an instructional activity for a particular set of requests for the video item and/or similar requests less frequently.
FIG. 4 illustrates an example 400 of a process of presenting a user interface customized for predicted user activity, according to some embodiments of the present disclosure.
At 402, process 400 may receive a user request to access a video item.
In some embodiments, the user request to access the video item may originate from any suitable source. For example, the request may originate from user device 710 as described below in connection with fig. 7 or any other device suitable for playing video content.
In some embodiments, the user request may be associated with and/or include any suitable information. For example, the user request may be associated with and/or include information as described above in connection with 202 of FIG. 2. In another example, the user request may associate and/or include contextual information as described below in connection with 404. In yet another example, the user request may be associated with and/or include information about the user device. In a more specific example, the request may be associated with and/or include information indicating that the request originated from a user device logged into a known user account, information indicating a geographic area of the user device, information indicating a type of user device (e.g., a mobile device, a desktop computer, or any other suitable device type), any other suitable information related to the user device, or any suitable combination thereof.
At 404, process 400 may receive context information related to the request using any suitable technique or combination of techniques. For example, process 400 may receive context information as part of the request (e.g., as described above in connection with 402). In another example, process 400 may send a request for contextual information to a device that sent the request for the video item (e.g., user device 710 described below in connection with fig. 7). In yet another example, process 400 may query a database for contextual information (e.g., a database as described above in connection with fig. 9).
In some embodiments, process 400 may receive any suitable contextual information. For example, process 400 may receive contextual information as described below in connection with 106 of fig. 1 and/or 210 of fig. 2.
At 406, the process 400 may select a user interface for presenting the requested video item based on an intent activity model (e.g., the intent activity model as described above in connection with fig. 1 and 3).
In some embodiments, the process 400 may select a user interface that corresponds to or includes features that correspond to any suitable one or more intended activities predicted by the intended activity model (e.g., any suitable intended activities as described below in connection with 108 of fig. 1). For example, where the intent activity model predicts that the user intends to view the video as an instructional activity, the process 400 may select a user interface (e.g., such as the user interface described below in connection with fig. 6A) corresponding to the instructional activity. In another example, where the intent-to-activity model predicts that the user intends to view the video as a shopping activity, the process 400 may select a user interface that includes features/secondary content items corresponding to the shopping, such as advertisements, price products, product reviews, user reviews, any other suitable user interface features corresponding to the shopping, or any suitable combination thereof. In yet another example, where the intent activity model predicts that the user intends to view the video as part of a casual browsing video, process 400 may select a user interface that includes features corresponding to casual browsing, such as a list of suggested videos, user comments, a user rating, a list of videos with the highest ratings, media content related to the requested video, any other suitable user interface feature corresponding to casual browsing, or any suitable combination thereof.
In some embodiments, the process 400 may select a user interface having two or more features, where each feature corresponds to a different intent activity predicted by the intent activity model. For example, where the intent activity model predicts an entertainment activity and an educational activity, the process 400 may select a user interface that includes a first characteristic corresponding to the entertainment activity and a second characteristic corresponding to the educational activity.
In some embodiments, process 400 may select a user interface based on any suitable indicator of predicted activity produced by the intent activity model. For example, process 400 may select a user interface based on any suitable metric as described above in connection with 106 of FIG. 1. Relatedly, in some embodiments, the process 400 may select the user interface based on any suitable criteria related to the metrics produced by the intent activity model. For example, where the intent activity model produces a first probability indicative of a first intended activity and a second probability indicative of a second intended activity, the process 400 may select a user interface corresponding to a higher probability of predicted activity.
In some embodiments, process 400 may select any suitable user interface. For example, process 400 may select any suitable interface described above in connection with 110 of FIG. 1.
In some embodiments, instead of selecting a user interface based on an intent activity model, the user interface may be selected directly by the intent activity model. For example, the intent activity model may include a predetermined association between the predicted intent activity and a customized user interface. In another example, instead of outputting the predicted intended activity, the intended activity model may output a suggested customized user interface.
In some embodiments, process 400 may select a user interface and/or user interface features that are predetermined to correspond to the predicted intended activity. For example, the process 400 may receive a manual association between a particular intended activity and a user interface customized for the particular intended activity (e.g., an association as received by a user input of an administrator and/or a developer of the mechanisms described herein) and select the customized user interface if the model predicts the particular intended activity. In another example, the process 400 may receive a manual association between a particular intended activity and a particular user interface feature and select the particular user interface feature if the model predicts the particular intended activity.
At 408, process 400 may cause the user device to present the video item using the selected user interface using any suitable technique or combination of techniques. For example, process 400 may present a user interface as described above in connection with 110 of FIG. 1.
Although process 400 has been described herein as generally directed to video items, in some embodiments, process 400 may additionally or alternatively be adapted to select a user interface corresponding to a user's intended use of any suitable type of media content item.
Fig. 5 illustrates an example 500 of a process of presenting a user interface for a predicted instructional activity, according to some embodiments of the present disclosure.
At 502, process 500 may receive a request for a video item using any suitable technique or combination of techniques. For example, process 500 may receive the request as described above in connection with 402 of FIG. 4.
At 504, process 500 may receive context information associated with the request using any suitable technique or combination of techniques. For example, process 500 may receive contextual information as described above in connection with 106 of fig. 1, 210 of fig. 2, and/or 404 of fig. 4.
At 506, the process 500 may predict whether a user associated with the request for the video item requests a video item for an instructional activity.
In some embodiments, process 500 may predict whether a user requests a video item for an instructional activity based on an intent activity model, such as the intent activity model described above in connection with fig. 1 and 3.
In some embodiments, process 500 may predict whether a user requests a video item for an instructional activity based on any suitable information. For example, process 500 may predict whether the user requested the video item for the instructional activity based on metadata associated with the requested video item (e.g., as described above in connection with 406 of fig. 4) and/or contextual information associated with the instructional activity. In a more specific example, process 500 may predict that the requested video is requested for an instructional activity based, at least in part, on metadata associated with the video, wherein the metadata includes a description of the video (e.g., "how" or "instructions") with words indicating that the video is instructional.
In some embodiments, after the predictive user requests a video item for an instructional activity, the process 500 may continue at 508 with the selection of an instructional user interface.
In some embodiments, process 500 may select any user interface suitable for the instructional activity. For example, process 500 may select a user interface as shown in FIG. 6A and described below in connection with FIG. 6A. In another example, process 500 may select a user interface that includes a feature directed to an instructional activity. In more specific examples, the user interface may include a feature to present user comments based on particular times during video playback, a feature to allow a user to take notes during video playback, any other suitable feature directed to an instructional activity, or any suitable combination thereof.
At 510, process 500 may cause the tutorial user interface selected at 508 to be presented to the user using any suitable technique or combination of techniques. For example, process 500 may present a user interface using techniques as described below in connection with 408 of FIG. 4.
At 512, the process 500 may determine whether the user requests a change to the user interface.
In some embodiments, process 500 may determine whether the user requests a change to the user interface based on a request received by the user device. For example, where process 500 causes a user device associated with a request for a video item to present an instructional user interface, if process 500 receives a request from a user device for a different user interface (e.g., a request associated with a user selecting a user interface element configured to change the user interface), process 500 may determine that the user requested that the user interface be changed based on the received request. In a more specific example, where the instructional user interface includes a selectable element configured to project a video item onto the second device, the process 500 can receive a corresponding request (from the second device or from the user device) to project the video item and determine that the user requests to alter the user interface. In another more specific example, where the instructional user interface includes selectable elements for changing user interface preferences, the process 500 can receive a request corresponding to a user selecting a selectable element for changing user interface preferences and determine that the user requests to alter the user interface.
In some embodiments, upon determining that the user requested a change to the user interface at 512, or upon predicting that the user did not request a video item for the instructional activity at 506, process 500 may continue at 514 by selecting another user interface to present to the user using any suitable technique or combination of techniques. For example, process 500 may select a user interface based on user input representing a preference for another user interface. In some embodiments, where the intent activity model provides an indication at 506 that there may be one or more intent activities other than tutorial activities (e.g., by generating a first score related to a tutorial activity and a second score related to a second activity, as described above in connection with 406 of fig. 4), the process 500 may select a user interface corresponding to the one or more intent activities other than tutorial activities.
In some embodiments, in response to receiving a selection at 514 that another user interface should be provided to the user, process 500 may continue at 516 by causing another user interface selected at 514 to be presented. In some embodiments, process 500 may present the other user interfaces using any suitable technique or combination of techniques. For example, process 500 may present the other user interfaces using the techniques described above in connection with 510.
It should be noted that similar to 512, the user may be provided with another opportunity to request a change to the user interface. In response to determining that the user requests a change to the user interface, process 500 may continue by selecting yet another user interface to provide to the user using any suitable technique or combination of techniques. For example, process 500 may select a user interface based on user input indicating a preference for another user interface.
At 518, the process 500 may record behavioral data associated with the presented user interface.
In some embodiments, process 500 may record any suitable behavioral data. For example, process 500 may record behavioral data as described above in connection with 306 of FIG. 3. In another example, process 500 may record behavioral data associated with the request to change the user interface as described above in connection with 514. In yet another example, the process 500 may record the intent activity data as described above in connection with 206 of fig. 2 (e.g., by presenting a query to the user related to the user's intent activity as described above in connection with 206 of fig. 2).
Although process 500 is described herein as generally directed to video items, in some embodiments, process 500 may additionally or alternatively be adapted to select a user interface corresponding to an instructional activity for any suitable type of media content item.
It should be noted that in some embodiments, process 100, process 200, process 300, process 400, and/or process 500 may perform some or all of the blocks described above by a third party device or a third party process.
Fig. 6A illustrates an example 600 of a user interface customized for teaching user activities according to some embodiments of the present disclosure. As shown in FIG. 6A, in some embodiments, the user interface 600 may include a section 602 for presenting the requested video item and elements customized for teaching user activity, such as a section for presenting a video progress bar 604 annotated with step markers 612, 614, and 616 and a section 606 for presenting a list of written steps including a highlighted written step 608 and a user comment 610.
In some embodiments, step markers 612, 614, and 616 may correspond to any suitable point in time and/or span of time in the video item. For example, each of the step markers 612, 614, and 616 may correspond to a point in time in the video item at which a separate step is to be started, discussed, and/or presented. In some embodiments, the step designations 612, 614, and 616 may also correspond to written steps in the written step list 606. In a more specific example, as shown in FIG. 6A, a step mark 612 (represented by "# 1") may correspond to a highlighted written step 608 (represented by "step # 1"). In some embodiments, step markers 612, 614, and 616 may be selectable user interface elements that, when selected by a user, may cause the user interface to take any appropriate corresponding action. For example, the step markup 612 can be configured to cause the written step 608 to expand or collapse, cause the video to jump to a point in time corresponding to the marked location, take any other suitable corresponding action, or any suitable combination thereof, when selected by the user.
In some embodiments, the highlighted written step 608 may correspond to a point in time or a time span of the video associated with the step. For example, the highlighted written step 608 may remain highlighted during the time span in which "step 1" is discussed and/or presented. Additionally or alternatively, the highlighted written step may become un-highlighted when different steps are being discussed and/or demonstrated.
In some embodiments, the user comments 610 may correspond to steps in the step list 606 in the steps section. For example, as shown in FIG. 6A, user comment 610 may correspond to step 608 of highlighting.
FIG. 6B illustrates an example 650 of a user interface customized for an entertainment activity, according to some embodiments of the present disclosure. As shown in fig. 6B, in some embodiments, user interface 650 may include a portion 652 for presenting the requested video item, a portion 654 for presenting a video play control including a projected element 656, and a portion 662 for presenting user comments including user comments 658 and 660. In some embodiments, the cast element 656 may be any suitable interface element that causes the requested video item to be presented by another device. In some embodiments, portion 654 may include any suitable user interface element that controls presentation of the requested video item. For example, portion 654 may include user interface elements for controlling volume, screen size, video resolution, any other suitable playback of the requested video item, and any suitable combination thereof.
Fig. 7 illustrates a schematic diagram of a system 700 suitable for implementing the mechanisms described herein for presenting a user interface customized for predicted user activity, in accordance with some embodiments of the present disclosure. As shown, the system 700 may include one or more servers 702, a communication network 706, and/or one or more user devices 710.
In some embodiments, server 702 may be any server suitable to implement some or all of the mechanisms of the user interface described herein that are customized for predicted user activities. For example, the server 702 may be a server that executes the intent activity model (e.g., as described above with respect to fig. 1 and 3) and/or causes one or more user devices 710 to present a corresponding user interface by sending instructions to the one or more user devices 710 via the communication network 706. In some embodiments, one or more servers 702 may provide media content to one or more user devices 710 via a communication network 706. In some embodiments, one or more servers 702 may host a database of contextual information (e.g., as described above in connection with 106 of fig. 1 and/or below in connection with fig. 9), host a database of behavioral data (e.g., as described above in connection with 306), and/or host a database of user account information (e.g., as described above in connection with 106 of fig. 1).
In some embodiments, the communication network 706 may be any suitable combination of one or more wired and/or wireless networks. For example, the communication network 706 may include any one or more of the internet, an intranet, a Wide Area Network (WAN), a Local Area Network (LAN), a wireless network, a Digital Subscriber Line (DSL) network, a frame relay network, an Asynchronous Transfer Mode (ATM) network, a Virtual Private Network (VPN), and/or any other suitable communication network. The user device 710 may be connected to the communication network 706 by one or more communication links 708, and the communication network 706 may be linked to the server 702 by one or more communication links 704. The communication links 704 and/or 708 may be any communication links suitable for communicating data between the user device 710 and the server 702, such as network links, dial-up links, wireless links, hardwired links, any other suitable communication links, or any suitable combination thereof.
Although two servers 702 are shown in fig. 7 to avoid overcomplicating the drawing, in some embodiments, any suitable number of devices may be used to perform the mechanisms described herein for presenting a user interface tailored for predicted user activity. For example, in some embodiments, the mechanisms may be performed by a single server 702 or multiple servers 702.
Although two user devices 710 are shown in fig. 7 to avoid overcomplicating the drawing, any suitable number or type of devices may be used in some embodiments.
In some embodiments, server 702 and user device 710 may be implemented using any suitable hardware. For example, server 702 and user device 710 may be implemented using hardware as described below in connection with FIG. 8. As another example, in some embodiments, devices 702 and 710 may be implemented using any suitable general purpose or special purpose computer. Any such general purpose or special purpose computer may include any suitable hardware.
Fig. 8 illustrates an example of hardware 800 that may be used in the server and/or user device of fig. 7, according to some embodiments of the present disclosure.
The server 820 may include a hardware processor 822, a display 824, an input device 826, and a memory and/or storage 828 that may be interconnected. In some embodiments, memory and/or storage 828 may include storage for storing data received over communication link 704 or over other links. The storage device may also include a server program for controlling the hardware processor 822. In some embodiments, memory and/or storage 828 may include information stored as a result of user activity (e.g., sharing content, requests for content, etc.), and hardware processor 822 may receive requests for media content and/or requests for a user interface. In some embodiments, the server program may cause hardware processor 822 to perform at least a portion of process 100 described above in connection with fig. 1, process 200 described above in connection with fig. 2, process 300 described above in connection with fig. 3, process 400 described above in connection with fig. 4, and/or process 500 described above in connection with fig. 5, for example
The hardware processor 822 may communicate with the user device 710 using a server program to provide access to and/or replication of the mechanisms described herein. It should also be noted that data received over communication links 704 and/or 708 or any other communication link may be received from any suitable source. In some embodiments, the hardware processor 822 may send and receive data over the communication link 704 or any other communication link using, for example, a transmitter, a receiver, a transmitter/receiver, a transceiver, or any other suitable communication device. In some embodiments, the hardware processor 822 may receive commands and/or values sent by one or more user devices 710, such as a user making changes to adjust settings related to the mechanisms described herein to present a customized user interface. The display 824 may include a touch screen, a flat panel display, a cathode ray tube display, a projector, a single speaker or multiple speakers, and/or any other suitable display and/or presentation device. Input device 826 may be a computer keyboard, a computer mouse, a touchpad, voice recognition circuitry, a touch screen, and/or any other suitable input device.
In some embodiments, hardware 800 may include any other suitable components.
Fig. 9 illustrates an example of a system 900 suitable for implementing the mechanisms described herein for presenting a user interface customized for predicted user activity, in accordance with some embodiments of the present disclosure.
In some embodiments, the total group 902 may include a test set 904. In some embodiments, the general population 902 may include any suitable people. For example, overall group 902 may include users of the social media platform (e.g., as described above in connection with 102 of fig. 1) and/or people that are not currently using the social media platform. In some embodiments, test group 904 may be a test group as described above in connection with fig. 1 and 2.
In some embodiments, the intent activity database 906 may receive intent activity information from the test group 904. In some embodiments, the intent activity database 906 may store any suitable intent activity information, such as the intent activity information described above in connection with fig. 1 and 2. In some embodiments, the intent activity database 906 may be hosted by the server 702 as described above in connection with fig. 7 and 8. In some embodiments, the intent activity information stored in intent activity database 906 may be manipulated and/or refined via system administrator 914 (e.g., as described above in connection with 212 of fig. 2).
In some embodiments, contextual information database 910 may receive contextual information from population group 902 and/or test group 904. In some embodiments, contextual information database 910 may store any suitable contextual information, such as the contextual information described above in connection with fig. 1 and 2. In some embodiments, the intent activity database 910 may be hosted by the server 702 as described above in connection with fig. 7 and 8. In some embodiments, the contextual information stored in the contextual information database 910 may be manipulated and/or refined via a system administrator 914.
In some embodiments, the user interface association 908 may be based on intent activity information received from the intent activity database 906. In some embodiments, user interface associations 908 may include any suitable association between a user interface and/or user interface features and an intended activity. For example, the user interface associations may include predefined user interface associations and/or predefined user interface feature associations as described above in connection with 406 of FIG. 4. In some embodiments, user interface association 908 may be determined and/or input by system administrator 914.
In some embodiments, the intended activity model 912 may be any suitable intended activity model, such as the intended activity model described above in connection with fig. 1 and 3. In some embodiments, the intent activity model 912 can be based on information received from the intent activity database 906 and the contextual information database 910. For example, as described below in connection with fig. 1, 2, 3, and 4, intent activity model 912 may be trained based on intent activity data (subjective data) received from intent activity database 906 and contextual information received from contextual information database 910. In some embodiments, the intent activity model 912 can select a user interface based on the user interface association received by the user interface association 908. In some embodiments, as shown in fig. 9, intent activity model 912 may receive requests (e.g., requests for media content and/or requests for a user interface) from user devices associated with people included in general group 902 and, based on contextual information (e.g., received from context database 910 and/or user devices), send user interface selections ("u.i. selections") to the user devices associated with the people included in general group 902, as shown in fig. 9. In some embodiments, the system administrator 914 may refine the parameters, coefficients, and/or variables of the intent activity model 912 (e.g., as described above in connection with 308 of FIG. 3).
In some embodiments, at least some of the blocks of the processes in fig. 1, 2, 3, 4, and/or 5 described above may be performed or presented in any order or sequence, and are not limited to the order or sequence presented in connection with the figures. Also, some of the blocks in fig. 1, 2, 3, 4, 5, and/or 9 may be executed substantially concurrently or in parallel, as appropriate, to reduce latency and processing time. Additionally or alternatively, in some embodiments, some processes in fig. 1, 2, 3, 4, and/or 5 may be omitted.
In some embodiments, any suitable computer readable medium may be utilized to store instructions for performing the functions and/or processes herein. For example, in some embodiments, the computer-readable medium may be transitory or non-transitory. For example, a non-transitory computer-readable medium may include, for example, magnetic media (e.g., a hard disk, a floppy disk, and/or any other suitable magnetic media), optical media (e.g., a compact disk, a digital video disk, a blu-ray disk, and/or any other suitable optical media), semiconductor media (e.g., a flash memory, an electrically programmable read-only memory (EPROM), an electrically erasable programmable read-only memory (EEPROM), and/or any other suitable semiconductor media), any medium suitable for a transmission without a temporary loss or loss of surface permanence, and/or any suitable tangible media. In another example, a transitory computer-readable medium may include a signal on a network, a wire, a conductor, a fiber, a circuit, any medium suitable for being temporarily lost or missing a permanent signal during transmission, and/or any suitable intangible medium.
Accordingly, methods, systems, and media for presenting a user interface customized for predicted user activity are provided.
While the invention has been described and illustrated in the foregoing illustrative embodiments, it is understood that the present disclosure has been made only by way of example, and that numerous changes in the details of implementation of the invention may be made without departing from the spirit and scope of the invention, which is limited only by the claims which follow. The features of the disclosed embodiments of the invention may be combined and rearranged in various ways.
Claims (19)
1. A method for presenting a customized user interface, the method comprising:
selecting at least a plurality of users of a video content distribution service from among users of the video content distribution service;
for a plurality of user devices associated with the plurality of users:
receiving a request for a video content item;
receiving objective data related to a context in which the request for a video content item was made;
causing each of the plurality of user devices to prompt the associated user to provide subjective data relating to the user's intent when requesting the video content item; and
receiving subjective data generated based on user input in response to the prompt;
receiving, from a first user device, an input mapping each of a plurality of user intents to at least one of a plurality of different user interfaces presenting video content items;
training a predictive model using at least a portion of the objective data received from the plurality of user devices and at least a portion of the subjective data received from the plurality of user devices to identify a subjective intent of the user in requesting a video content item based on objective data received from user devices associated with the user, wherein the predictive model is trained to identify whether to present to the user a first user interface presented with the video content item or a second user interface presented with the video content item, the first user interface having a first set of user interface elements associated with a first user intent, the second user interface having a second set of user interface elements associated with a second user intent;
receiving a request for a first video content item from a second user device;
receiving, from the second user device, objective data related to a context in which the request for the first video content item was made;
providing at least a portion of the objective data received from the second user device to the predictive model;
receiving a first output from the predictive model indicating that the second user device should present the first video content item in the first user interface of the first set of user interface elements having the subjective intent identified based on the predictive model;
in response to receiving the first output from the predictive model, cause the second user device to present the first video content item in the first user interface of the first set of user interface elements having the subjective intent identified based on the predictive model;
receiving a request for the first video content item from a third user device;
receiving, from the third user device, objective data related to a context in which the request for the first video content item was made;
providing at least a portion of the objective data received from the third user device to the predictive model;
receiving a second output from the predictive model indicating that the third user device should present the first video content item in the second user interface of the second set of user interface elements having the subjective intent identified based on the predictive model; and
in response to receiving the second output from the predictive model, cause the third user device to present the first video content item in the second user interface of the second set of user interface elements having the subjective intent identified based on the predictive model.
2. The method of claim 1, wherein a first user intent of the plurality of user intents is an intent to consume the video content item to obtain information included in the video content item.
3. The method of claim 2, wherein a second user intent of the plurality of user intents is an intent to consume the video content item for entertainment.
4. The method of claim 3, wherein causing each of the plurality of user devices to prompt an associated user comprises: causing each of the plurality of user devices to query one of the associated users to determine whether the associated user intends to consume the requested video content item primarily for entertainment or primarily for obtaining information included in the video content item.
5. The method of claim 1, wherein the objective data related to the context in which the request for the first video content item was made comprises information indicating whether the request for the first video content item originated from search results provided by the content distribution service.
6. The method of claim 5, wherein the objective data related to the context in which the request for the first video content item was made comprises a search request for initiating the search.
7. A method for presenting a customized user interface, the method comprising:
identifying, from a plurality of user devices associated with a plurality of users, first contextual information relating to a context in which a request for a video content item was made;
providing a prompt to each of the plurality of user devices to provide intent information related to the user's intent when requesting the video content item;
receiving the intent information in response to the prompt;
generating a trained predictive model that identifies the intent of the user when requesting a video content item using the first contextual information and the received intent information, wherein the trained predictive model determines which version of the user interface is to be presented from a plurality of user interfaces based on a predicted user intent determined based on information relevant to a context in which a request for video content is made, wherein each of the plurality of user interfaces corresponds to a predicted user intent of a plurality of predicted user intents, wherein the first user interface has a different set of user interface elements than the second user interface element in the plurality of user interfaces, and wherein the video content is presented in a determined version of a user interface having a set of user interface elements selected based on the predicted user intent;
receiving a request for a video content item from a second plurality of user devices;
for each request for a video content item received from the second plurality of user devices, identifying second context information relevant to a context in which the request for the video content item was made;
receiving an output from the trained predictive model for each request for a video content item received from the second plurality of user devices, the output indicating which version of the user interface to present based on at least a portion of the second contextual information; and
causing each of the second plurality of user devices to present a version of a user interface in which the video content item was presented based on the output from the trained predictive model, wherein two of the second plurality of user devices are caused to present two different versions of user interfaces in which the same video content item was presented based on the output of the trained predictive model, each of the two different versions of user interfaces having the different set of user interface elements selected based on the predicted user intent.
8. A system for presenting a customized user interface, the system comprising:
a memory storing computer-executable instructions; and
a hardware processor, which when executing the computer-executable instructions stored in the memory, is configured to:
selecting at least a plurality of users of a content delivery service from among users of the content delivery service;
for a plurality of user devices associated with the plurality of users:
receiving a request for a video content item;
receiving objective data related to a context in which the request for a video content item was made;
causing each of the plurality of user devices to prompt the associated user to provide subjective data relating to the user's intent when requesting the video content item; and
receiving subjective data generated based on user input in response to the prompt;
receiving, from a first user device, an input mapping each of a plurality of user intents to at least one of a plurality of different user interfaces presenting video content items;
training a predictive model using at least a portion of the objective data received from the plurality of user devices and at least a portion of the subjective data received from the plurality of user devices to identify a subjective intent of the user in requesting a video content item based on objective data received from user devices associated with the user, wherein the predictive model is trained to identify whether to present to the user a first user interface presented with the video content item or a second user interface presented with the video content item, the first user interface having a first set of user interface elements associated with a first user intent, the second user interface having a second set of user interface elements associated with a second user intent;
receiving a request for a first video content item from a second user device;
receiving, from the second user device, objective data related to a context in which the request for the first video content item was made;
providing at least a portion of the objective data received from the second user device to the predictive model;
receiving a first output from the predictive model indicating that the second user device should present the first video content item in the first user interface of the first set of user interface elements having the subjective intent identified based on the predictive model;
in response to receiving the first output from the predictive model, cause the second user device to present the first video content item in the first user interface of the first set of user interface elements having the subjective intent identified based on the predictive model;
receiving a request for the first video content item from a third user device;
receiving, from the third user device, objective data related to a context in which the request for the first video content item was made;
providing at least a portion of the objective data received from the third user device to the predictive model;
receiving a second output from the predictive model indicating that the third user device should present the first video content item in the second user interface of the second set of user interface elements having the subjective intent identified based on the predictive model; and
in response to receiving the second output from the predictive model, cause the third user device to present the first video content item in the second user interface of the second set of user interface elements having the subjective intent identified based on the predictive model.
9. The system of claim 8, wherein a first user intent of the plurality of user intents is an intent to consume the video content item to obtain information included in the video content item.
10. The system of claim 9, wherein a second user intent of the plurality of user intents is an intent to consume the video content item for entertainment.
11. The system of claim 10, wherein causing each of the plurality of user devices to prompt an associated user comprises: causing each of the plurality of user devices to query one of the associated users to determine whether the associated user intends to consume the requested video content item primarily for entertainment or primarily for obtaining information included in the video content item.
12. The system of claim 8, wherein the objective data related to the context in which the request for the first video content item was made comprises information indicating whether the request for the first video content item originated from search results provided by the content distribution service.
13. The system of claim 12, wherein the objective data related to the context in which the request for the first video content item was made comprises a search request for initiating the search.
14. A non-transitory computer readable storage medium containing computer executable instructions that, when executed by a processor, cause the processor to perform a method of presenting a customized user interface, the method comprising:
selecting at least a plurality of users of a video content distribution service from among users of the video content distribution service;
for a plurality of user devices associated with the plurality of users:
receiving a request for a video content item;
receiving objective data related to a context in which the request for a video content item was made;
causing each of the plurality of user devices to prompt the associated user to provide subjective data relating to the user's intent when requesting the video content item; and
receiving subjective data generated based on user input in response to the prompt;
receiving, from a first user device, an input mapping each of a plurality of user intents to at least one of a plurality of different user interfaces presenting video content items;
training a predictive model using at least a portion of the objective data received from the plurality of user devices and at least a portion of the subjective data received from the plurality of user devices to identify a subjective intent of the user in requesting a video content item based on objective data received from user devices associated with the user, wherein the predictive model is trained to identify whether to present to the user a first user interface presented with the video content item or a second user interface presented with the video content item, the first user interface having a first set of user interface elements associated with a first user intent, the second user interface having a second set of user interface elements associated with a second user intent;
receiving a request for a first video content item from a second user device;
receiving objective data from the second user device relating to making a request for the first video content item;
providing at least a portion of the objective data received from the second user device to the predictive model;
receiving a first output from the predictive model indicating that the second user device should present the first video content item in the first user interface of the first set of user interface elements having the subjective intent identified based on the predictive model;
in response to receiving the first output from the predictive model, cause the second user device to present the first video content item in the first user interface of the first set of user interface elements having the subjective intent identified based on the predictive model;
receiving a request for the first video content item from a third user device;
receiving, from the third user device, objective data related to a context in which the request for the first video content item was made;
providing at least a portion of the objective data received from the third user device to the predictive model;
receiving a second output from the predictive model indicating that the third user device should present the first video content item in the second user interface of the second set of user interface elements having the subjective intent identified based on the predictive model; and
in response to receiving the second output from the predictive model, cause the third user device to present the first video content item in the second user interface of the second set of user interface elements having the subjective intent identified based on the predictive model.
15. The non-transitory computer readable storage medium of claim 14, wherein a first user intent of the plurality of user intents is an intent to consume the video content item to obtain information included in the video content item.
16. The non-transitory computer readable storage medium of claim 15, wherein a second user intent of the plurality of user intents is an intent to consume the video content item for entertainment.
17. The non-transitory computer readable storage medium of claim 16, wherein causing each of the plurality of user devices to prompt an associated user comprises: causing each of the plurality of user devices to query one of the associated users to determine whether the associated user intends to consume the requested video content item primarily for entertainment or primarily for obtaining information included in the video content item.
18. The non-transitory computer-readable storage medium of claim 14, wherein the objective data related to the context in which the request for the first video content item was made includes information indicating whether the request for the first video content item originated from search results provided by the content delivery service.
19. The non-transitory computer-readable storage medium of claim 18, wherein the objective data related to the context in which the request for the first video content item was made comprises a search request for initiating the search.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/234,446 US20180046470A1 (en) | 2016-08-11 | 2016-08-11 | Methods, systems, and media for presenting a user interface customized for a predicted user activity |
US15/234,446 | 2016-08-11 | ||
PCT/US2017/046248 WO2018031743A1 (en) | 2016-08-11 | 2017-08-10 | Methods, systems, and media for presenting a user interface customized for a predicted user activity |
Publications (2)
Publication Number | Publication Date |
---|---|
CN109478142A CN109478142A (en) | 2019-03-15 |
CN109478142B true CN109478142B (en) | 2022-03-01 |
Family
ID=59702846
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780043785.2A Active CN109478142B (en) | 2016-08-11 | 2017-08-10 | Methods, systems, and media for presenting a user interface customized for predicted user activity |
Country Status (5)
Country | Link |
---|---|
US (1) | US20180046470A1 (en) |
EP (1) | EP3469495A1 (en) |
CN (1) | CN109478142B (en) |
DE (1) | DE202017104849U1 (en) |
WO (1) | WO2018031743A1 (en) |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105070288B (en) * | 2015-07-02 | 2018-08-07 | 百度在线网络技术（北京）有限公司 | Vehicle-mounted voice instruction identification method and device |
US10628901B1 (en) * | 2016-09-23 | 2020-04-21 | Accenture Global Solutions Limited | Information management system for connected learning centers |
US10990421B2 (en) | 2018-06-27 | 2021-04-27 | Microsoft Technology Licensing, Llc | AI-driven human-computer interface for associating low-level content with high-level activities using topics as an abstraction |
US11354581B2 (en) * | 2018-06-27 | 2022-06-07 | Microsoft Technology Licensing, Llc | AI-driven human-computer interface for presenting activity-specific views of activity-specific content for multiple activities |
US11449764B2 (en) | 2018-06-27 | 2022-09-20 | Microsoft Technology Licensing, Llc | AI-synthesized application for presenting activity-specific UI of activity-specific content |
CN112150177A (en) * | 2019-06-27 | 2020-12-29 | 百度在线网络技术（北京）有限公司 | Intention prediction method and device |
US11328223B2 (en) * | 2019-07-22 | 2022-05-10 | Panasonic Intellectual Property Corporation Of America | Information processing method and information processing system |
CN112699910A (en) * | 2019-10-23 | 2021-04-23 | 北京达佳互联信息技术有限公司 | Method and device for generating training data, electronic equipment and storage medium |
RU2745362C1 (en) * | 2019-11-27 | 2021-03-24 | Акционерное общество "Лаборатория Касперского" | System and method of generating individual content for service user |
US11921812B2 (en) * | 2022-05-19 | 2024-03-05 | Dropbox, Inc. | Content creative web browser |
Family Cites Families (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20070100650A1 (en) * | 2005-09-14 | 2007-05-03 | Jorey Ramer | Action functionality for mobile content search results |
US20070300185A1 (en) * | 2006-06-27 | 2007-12-27 | Microsoft Corporation | Activity-centric adaptive user interface |
US7877250B2 (en) * | 2007-04-23 | 2011-01-25 | John M Oslake | Creation of resource models |
US20090150541A1 (en) * | 2007-12-06 | 2009-06-11 | Sony Corporation And Sony Electronics Inc. | System and method for dynamically generating user interfaces for network client devices |
US20090187463A1 (en) * | 2008-01-18 | 2009-07-23 | Sony Corporation | Personalized Location-Based Advertisements |
US20120166522A1 (en) * | 2010-12-27 | 2012-06-28 | Microsoft Corporation | Supporting intelligent user interface interactions |
US8744237B2 (en) * | 2011-06-20 | 2014-06-03 | Microsoft Corporation | Providing video presentation commentary |
US20130080968A1 (en) * | 2011-09-27 | 2013-03-28 | Amazon Technologies Inc. | User interface with media content prediction |
US20130159228A1 (en) * | 2011-12-16 | 2013-06-20 | Microsoft Corporation | Dynamic user experience adaptation and services provisioning |
JP6335794B2 (en) * | 2012-01-27 | 2018-05-30 | タッチタイプ リミテッド | Predict user data input |
CN103608759B (en) * | 2012-05-31 | 2019-05-14 | 都特媒体有限公司 | The method of Dynamically Announce personalization main screen in equipment |
US9137332B2 (en) * | 2012-12-21 | 2015-09-15 | Siemens Aktiengesellschaft | Method, computer readable medium and system for generating a user-interface |
US20150169285A1 (en) * | 2013-12-18 | 2015-06-18 | Microsoft Corporation | Intent-based user experience |
US9032321B1 (en) * | 2014-06-16 | 2015-05-12 | Google Inc. | Context-based presentation of a user interface |
CN105354339B (en) * | 2015-12-15 | 2018-08-17 | 成都陌云科技有限公司 | Content personalization providing method based on context |
-
2016
- 2016-08-11 US US15/234,446 patent/US20180046470A1/en not_active Abandoned
-
2017
- 2017-08-10 CN CN201780043785.2A patent/CN109478142B/en active Active
- 2017-08-10 EP EP17757969.5A patent/EP3469495A1/en not_active Ceased
- 2017-08-10 WO PCT/US2017/046248 patent/WO2018031743A1/en unknown
- 2017-08-11 DE DE202017104849.7U patent/DE202017104849U1/en active Active
Also Published As
Publication number | Publication date |
---|---|
CN109478142A (en) | 2019-03-15 |
WO2018031743A1 (en) | 2018-02-15 |
DE202017104849U1 (en) | 2017-10-30 |
US20180046470A1 (en) | 2018-02-15 |
EP3469495A1 (en) | 2019-04-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109478142B (en) | Methods, systems, and media for presenting a user interface customized for predicted user activity | |
US11902626B2 (en) | Control method of playing content and content playing apparatus performing the same | |
US20240086413A1 (en) | Methods, systems, and media for presenting search results | |
US10795929B2 (en) | Interactive music feedback system | |
US9892109B2 (en) | Automatically coding fact check results in a web page | |
CN107430630B (en) | Methods, systems, and media for aggregating and presenting content related to a particular video game | |
US11681750B2 (en) | System and method for providing content to users based on interactions by similar other users | |
KR101908099B1 (en) | Automated click type selection for content performance optimization | |
US20190164069A1 (en) | Method and server for selecting recommendation items for a user | |
US20150347560A1 (en) | Storing and analyzing presentation data | |
US11593432B2 (en) | Methods, systems, and media for providing search suggestions based on content ratings of search results | |
US20130144709A1 (en) | Cognitive-impact modeling for users having divided attention | |
CN104144357B (en) | Video broadcasting method and system | |
KR102340228B1 (en) | Message service providing method for message service linking search service and message server and user device for performing the method | |
CN111436006A (en) | Method, device, equipment and storage medium for displaying information on video | |
US20150319509A1 (en) | Modified search and advertisements for second screen devices | |
US11095930B2 (en) | Methods, systems, and media for indicating viewership of a video | |
KR20170043824A (en) | Method, system and computer-readable recording medium for managing online learnimg | |
US9565224B1 (en) | Methods, systems, and media for presenting a customized user interface based on user actions | |
KR101452414B1 (en) | Method for providing multimedia contents using meta information | |
JP2002108923A (en) | Contents providing method and contents | |
US20130080949A1 (en) | User interactions | |
KR102465853B1 (en) | Method and system for classification and categorization of video paths in interactive video | |
JP2018041509A (en) | Automated click type selection for content performance optimization |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |