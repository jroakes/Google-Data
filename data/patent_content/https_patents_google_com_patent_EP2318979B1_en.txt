EP2318979B1 - Method and system for automated annotation of persons in video content - Google Patents
Method and system for automated annotation of persons in video content Download PDFInfo
- Publication number
- EP2318979B1 EP2318979B1 EP09788910.9A EP09788910A EP2318979B1 EP 2318979 B1 EP2318979 B1 EP 2318979B1 EP 09788910 A EP09788910 A EP 09788910A EP 2318979 B1 EP2318979 B1 EP 2318979B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- face
- models
- module
- images
- video
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 title claims description 21
- 230000001815 facial effect Effects 0.000 claims description 29
- 238000001514 detection method Methods 0.000 claims description 26
- 239000013598 vector Substances 0.000 claims description 13
- 238000012545 processing Methods 0.000 description 9
- 238000013459 approach Methods 0.000 description 4
- 238000004891 communication Methods 0.000 description 3
- 230000008921 facial expression Effects 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 230000014509 gene expression Effects 0.000 description 3
- 230000008569 process Effects 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 206010024825 Loose associations Diseases 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000004075 alteration Effects 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000001914 filtration Methods 0.000 description 1
- 238000005286 illumination Methods 0.000 description 1
- 238000003064 k means clustering Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 230000036544 posture Effects 0.000 description 1
- 230000035755 proliferation Effects 0.000 description 1
- 230000000717 retained effect Effects 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/172—Classification, e.g. identification
- G06V40/173—Classification, e.g. identification face re-identification, e.g. recognising unknown faces across different face tracks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/28—Determining representative reference patterns, e.g. by averaging or distorting; Generating dictionaries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/772—Determining representative reference patterns, e.g. averaging or distorting patterns; Generating dictionaries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
Definitions
- This invention relates to recognizing persons in video content.
- the Internet hosts vast amounts of content of different types including text, images, and video. Leveraging this content requires that the content is searchable and organized. Images are generally searched and organized based on tags that are manually assigned by users. Similarly, video content is generally searched and organized based on tags that are manually assigned.
- each video may be of substantial length and may include many persons appearing in different parts of the video.
- the video may vary according to pose, expression, illumination, occlusion, and quality. It would require a substantial amount of manual effort to accurately tag the video with the name of each person appearing in the video.
- the manual approach of tagging content is not scalable to the large amount of content available on the internet.
- a computer-implemented method that identifies faces in a video includes the stages of: generating one or more face tracks from an input video stream; selecting key face images for each of the one or more face tracks; clustering the face tracks to generate face clusters, where each face cluster is associated with one or more key face images; creating face models from the face clusters; and correlating face models with a face model database.
- a system for identifying faces in a video includes the components: a face model database having face entries with face models and corresponding names; and a video face identifier module.
- the video face identifier module can include: a face detection module that detects faces in an input video stream; a face tracking module that tracks detected faces and generates face tracks; an intra-track face clustering module; an inter-track face clustering module; a detected face model generator module; and a model comparison module that compares detected face models with face entries in a database.
- a system for identifying faces in a video includes a face model generator having as components: a name generating module that generates a name list; an image searching module that locates images corresponding to the name list; a face detection module; a face model generation module; a collection module that pair-wise stores the one or more face models and the corresponding names; and a consistency learning module.
- the collection module may also store face model and name pairs derived from user input.
- FIG. 1 shows a system 100 that can automatically annotate a video, according to an embodiment of the present invention, with information, such as, for example, names of popular persons who appear in the video.
- a video processor module 101 is coupled to a system interface 130 with a connection device 131.
- System interface 130 may be a user interface or an application programming interface located on the same computing platform as video processor module 101, or a remote user interface, such as, for example, a web client.
- connection device 131 may use a connection method, such as, for example, a Peripheral Component Interconnect (PCI) bus, Ethernet, or a wireless communication standard.
- PCI Peripheral Component Interconnect
- Video processor module 101 can also access a video corpus 114, an image corpus 112, and a text corpus 110. Some or all of corpora 114, 112, and 110, may be accessible through a network 140, such as, for example, a wide area network (WAN) like the Internet or a local area network (LAN), or may be located locally on a user's own system. Corpora 114, 112 and 110 may each include one or more corpora that are co-located or distributed. In some embodiments, corpora 114, 112 and 110, may be co-located in part or in whole. Video processor module 101 may be coupled to network 140 through any connection 141 including for example and without limitation, a PCI bus, Ethernet, and a wireless communication standard.
- WAN wide area network
- LAN local area network
- Video processor module 101 may be coupled to network 140 through any connection 141 including for example and without limitation, a PCI bus, Ethernet, and a wireless communication standard.
- Video corpus 114 may include video clips of any length and in any video format including, for example and without limitation, any Moving Picture Experts Group (MPEG) standard, audio video interleave standard (AVI), QuickTime, and Windows Media Video (WMV).
- the video clips include videos having one or more persons.
- Image corpus 112 may include images in any image format, such as, JPEG, TIFF, and PNG. Image corpus 112 includes images of persons.
- Text corpus 110 includes, for example, text archives accessible locally and/or over the Internet. Available text archives may include, for example and without limitation, ASCII text, PDF text, and other forms of text.
- Video processor module 101 is also coupled to a database of face models 121 and a database of annotated video 123, over connections 142.
- Database of face models 121 includes face models generated by the video processor module 101 based at least partly on images available in the image corpus 112. Such generation of face models will be further described with respect to FIG. 5 , below.
- Database 121 may include one or more face models for each person represented. It may also include additional information, such as names or other tags attached to the person or facial images of the person.
- Database of annotated video 123 includes video, primarily from video corpus 114, annotated during the processing in video processor module 101.
- “database” refers to any collection of data elements, and associated storage and access mechanisms.
- Connections 142 may use one or more connection methods, such as, for example, a PCI bus, Ethernet, and wireless communications standards.
- Video processor module 101 can include several components, including a face model generator module 102, a video face recognition module 103, a video annotator module 109, a model updater module 107, and a query module 105.
- Video processor module 101 and some or all of the sub-modules 102, 103, 105, 107, and 109 may be implemented in software, hardware or any combination thereof.
- model generator module 102 may be implemented as executable code on a central processor unit (not shown in FIG. 1 ).
- model generator module 102 may be implemented in a hardware component such as a Field Programmable Gate Array.
- a person skilled in the art would understand that video processor module 101 may be implemented in one or more platforms.
- Face model generator module 102 may build models of faces that are selected from images in image corpus 112 and video corpus 114. Module 102 may also determine a set of people whose facial images are to be modeled. For example, in an embodiment, text corpus 110 is analyzed to derive a list of most popular persons and locate one or more images of the faces of each of them.
- a news archive may be a combined text corpus 110 and image corpus 112, and an analysis of the frequency of occurrence of person names in the news archive can generate a list of most frequently occurring names. Many of the most frequently occurring names may be associated with an image having the face of the named person in one or more news articles, and can therefore be used as a starting point to get models of the facial images of those named people. Face models derived from these and other matched images from image corpus 112 and video corpus 114 can then be stored in database of face models 121.
- Video face recognition module 103 uses face models including face models from database of face models 121 to detect and recognize faces from video corpus 114. Recognition of faces in video streams is explained in more detail with respect to FIG. 7 and FIG. 8 below. As faces are detected and recognized in the video streams of video corpus 114, module 103, together with video annotator module 109, can annotate the video with information known about the person whose face is being recognized. For example, names and tags in database 121 associated with a corresponding image may be used for the annotation. The annotated video, or part thereof, can then be stored in database of annotated video 123.
- a model updater module 107 can be used to update face models in database 121 based on new additions to the image corpus 112 and video corpus 114. It may also update face models in database 121 by adding faces that are recognized by video face recognition module 103. Updating available face models in database 121 according to an increasing number of images may increase the reliability of face recognition for persons having multiple images covering a range of postures, lighting conditions, etc. Also, in some embodiments, a query module 105 may be used for leveraging the video information in annotated video database 123.
- query module 105 may collaborate with external modules to search for a set of video clips or parts of video clips that include an appearance of a specified person, and make those video clips available for access by the external modules.
- a standard browser search for a specific person can be enhanced to present video tracks having at least one appearance of the specified person.
- FIG. 2 shows components of face model generator module 102.
- a name list generator module 201 acquires a list of persons for whom face models will be generated and stored in database of face models 121.
- name list generator module 201 may access external text corpora, for example, text corpus 110, to determine a list of most frequently occurring names.
- An image searcher module 203 associates at least one image with each name in the list of names generated by module 201.
- one or more images in image corpus 112 may be a part of a newspaper article about a celebrity whose name is on the list of names generated by module 201. The inclusion of the image with the article provides an association detected by image searcher module 203.
- face detector module 205 processes each image to detect a face corresponding to the associated name.
- a face modeler module 207 creates one or more face models from the one or more faces detected that correspond to a single name.
- a model collector module 209 gathers all face models corresponding to the same person.
- a consistency learning module 211 selects one or more face models for each associated name, and filters out the face models that are considered weak matches.
- face model database 121 contains one or more face models per person that is included in the database.
- the use of multiple models in face recognition increases the accuracy of the system.
- Multiple face models can represent different appearances of the same person, different light conditions, different environments, etc.
- a very large variation in facial expressions, facial accessories, age, light conditions, etc. can be expected for the face of the same person in a large collection of image and video content.
- Associated with each face model, and/or each group of face models for the same person can be one or more tags including the person's name.
- Video face detector module 301 detects faces in an incoming video. After detector module 301 detects a face, a face tracker module 303 tracks that face in the incoming video stream. Face tracker module 303 can create one or more tracks for each face that is detected by module 301 and then tracked by module 303. An intra-track clustering module 305 then processes the generated tracks to create face clusters based on each track. For example, if the face of a single person undergoes substantial variations in appearance due to facial expressions, facial accessories such as sunglasses, different light conditions, etc., then multiple face models may be required to capture the face accurately because of its many variations even in the duration of the single track.
- Intra-track clustering module 305 collects one or more facial images for each face that is tracked in each video track according to the level of variation detected in a particular face.
- An inter-track clustering module 307 creates clusters using the intra-track clusters of facial images.
- Inter-track clustering module 307 can combine similar clusters from separate tracks, to create one set of face image clusters for each person detected in the video.
- a video face model generator module 309 Taking as input the facial image clusters generated by inter-track clustering module 307, a video face model generator module 309 generates face models corresponding to the selected one or more images for each person.
- the new face models that are generated can be stored in, for example, database of face models 121.
- database of face models 121 For example, having multiple face models encompassing a variety of expressions, lighting conditions, etc., generally makes it easier to reliably detect the occurrence of a face in a video or image corpus.
- a model comparison module 311 takes the newly generated face models, and may determine whether it would be beneficial to store the additional models. Module 311 matches the newly generated face models to one or more models stored in database 121, and thereby associates the newly generated face models with one or more tags stored in database 121 including, possibly, a person's name.
- module 311 may facilitate an annotation module 109 to annotate the video track or segment with data such as the name of the associated person, and/or other information contained in the tags.
- the annotated video may be stored in a database of annotated video 123.
- verification and filtering algorithms such as, for example, consistency learning algorithms can be used to create or update face models.
- FIG. 4 is a flowchart showing two major processing stages according to an embodiment of the present invention.
- a face model generation stage 401 includes building and updating a database of face models, such as database 121.
- a video face recognition stage 402 includes using an existing face model database to detect and recognize faces in video.
- An embodiment of face model generation stage 401 is further dissected into component stages shown in FIG. 5 .
- Stage 401 can be implemented using components including those shown in FIG. 2 .
- An embodiment of video face recognition stage 402 is further dissected to component stages shown in FIG. 7 .
- Stage 402 can be implemented using components, including those shown in FIG. 3 .
- FIG. 5 is a flowchart illustrating exemplary processing stages in creating a database of face models, according to an embodiment of the present invention.
- stage 501 a set of names is determined, where for each name, stage 401 will attempt to determine one or more corresponding face models and store those face models in a database, such as database of face models 121.
- the list of names may be determined based on such criteria as names most frequently occurring in text and image corpora, such as, for example, current news archives. Such a selection criteria may generally yield a list of most popular names, such as, for example, celebrities.
- the automatically generated list of names may be manually or programmatically edited to add new names, to delete existing names, or to modify existing names. For example, in one embodiment, names of a user's closest friends can be added to the list of names. Text analysis methods for determining most frequently occurring names in a collection of sources are well known in the art.
- image corpus 112 is searched to gather multiple images associated with each of the names in the name list.
- embodiments of the present invention may operate with fully or partially integrated text and image corpora.
- image captions or articles in which images are embedded can be used to obtain a substantially unambiguous association between a name and an associated image.
- an association can be assumed between a name and an image, such as, the association of a name appearing frequently in a news article with an image appearing in the same article.
- Loose associations between a name and an image such as in the above described news article, for example, can in general be considered weak and noisy text-image correlation data.
- stage 503 face detection is performed for each image found in stage 502.
- Methods of face detection in images are well known in the art. Where an image yields only one detected face, the association between the detected face and the corresponding name may be unambiguous. When an image yields multiple defected faces, in some embodiments of the present invention, an association may be assumed between the corresponding name and each detected face.
- the resolution of the associations to a single person can be left to, for example, stage 506.
- images having more than a predetermined number of detected faces may be excluded from being used for purposes of this invention.
- methods such as correlating the most prominent facial image with the most frequently occurring name in the corresponding document may be used.
- a user may provide one or more images including the face of a person, and a corresponding name or text label. These user provided image and name pairs may also be added to the face database for further processing.
- Feature vectors describe specific properties of a detected face, such that comparison of two facial images is possible using the corresponding feature vectors.
- Methods of generating feature vectors for a facial image are known in the art. For example, searching of image corpora for named persons and processing the images that are found is described in U.S. Patent Application Publication No. 2008/0130960 titled "Identifying Images Using Face Recognition.”
- the feature vectors are stored in association with the corresponding person name.
- feature vectors and corresponding name pairs, including other associated tag information can be stored in a database, such as database of facial images 121.
- FIG. 6 illustrates an example of an entry in database of facial images 121 in one embodiment of the present invention.
- all feature vectors 601 representing the facial images of the person may be associated with text labels or tags including the name associated with that person A.
- a person skilled in the art will understand that other forms of storing the data are possible.
- stage 506 feature vectors for a specific person are processed to reduce to a set of face models and/or feature sets that can reliably be considered as belonging to that specific person.
- the number of face models retained for each person is variable, and primarily depends on the variation in conditions of the images such as facial expressions, age, facial accessories, lighting conditions, etc.
- a bagging technique such as consistency learning is applied to distil a suitable set of face models for each represented person. Consistency learning is described in detail, for example, in U.S. Patent Application 11/840,139 titled "Graph Based Sampling.”
- Fig. 7 illustrates video face recognition stage 402 in more detail, according to an embodiment of the present invention.
- video is received.
- Video may be received, for example and without limitation, locally, over a network, or from an external source such as a video playback device or a video recording device.
- the teachings of this disclosure apply to video in any video format including, for example, MPEG, AVI, QuickTime, WMV, etc.
- face detection and tracking is performed on the incoming video stream.
- frontal face detection based on an extension of the cascade of boosted classifiers is used. Frontal face detection using a cascade of boosted classifiers is described in P. Viola and M. Jones, "Robust real time object detection," Proceedings of the IEEE ICCV Workshop on Statistical and Computational Theories of Vision, Vancouver, Canada (July 2001 ).
- face detection and face tracking are used alternately and periodically for each detected face. Periodically performing face detection, guided by the face tracking process, helps reduce the computational load, because face detection, in general, is computationally more intensive than face tracking.
- some embodiments of the present invention can use facial feature based tracking, where a selected set of features from the feature set of each facial image is tracked. Facial feature based tracking can reliably determine whether a face can be tracked.
- FIG. 8 is an illustration of detailed operation of stage 702, in one embodiment of the present invention.
- stage 802 each incoming video frame 801 is observed to determine whether it is a shot boundary.
- Shot boundary detection is used to reduce tracking across different shots.
- a shot is a continuous sequence of frames from one camera.
- a cut is an abrupt shot change that occurs in a single frame.
- a fade is a gradual alteration in brightness usually resulting in or starting with a solid black frame. Many other types of gradual transitions are possible. Many techniques are described in the literature for shot boundary detection. If a frame is determined to be a shot boundary frame, that frame will be processed when a face detection stage 803 detects a person's face in that frame.
- a face tracker may be implemented as a separate processing thread primarily dedicated to tracking the assigned face as the video progresses.
- the face trackers, such as the one created in stage 806, yield output and record the tracking results in storage, such as database of face models 121.
- stage 808 decides whether to perform face detection on the video frame.
- face tracking is contemplated to reduce the computational load that can be caused by implementing face detection on each frame. Facial feature based tracking systems can efficiently determine whether tracking can be continued on each frame.
- face detection is initiated in stage 809. If a face is detected in stage 809, it must then be determined in stage 811 whether the detected face is currently being tracked. If the face detected in stage 809 is being currently tracked, then in stage 812 an attempt is made to reactivate an existing corresponding tracker.
- stage 813 If it is determined in stage 813 that an existing tracker cannot be reactivated 813, then a new tracker is created and activated in stage 806. If the face detection fails in stage 809, then an attempt is made to use the existing trackers in stage 814. In stage 815, it is determined whether existing trackers are available to be used in stage 814. Each existing tracker that fails in stage 815 is terminated in stage 816.
- stage 808 If, in stage 808, it is determined that no face detection is necessary for the current video frame, then an attempt is made to reactivate each of the existing face trackers in stage 817. If it is determined in stage 818 that the tracker activation fails, further checks are implemented in stage 819 to determine whether the tracker can be detected. Trackers that cannot be detected in stage 819 are terminated in stage 816. Otherwise, for those trackers that can still be detected in stage 819, new trackers are created and activated in stage 820. The result received in stage 807 for each input video frame 801 can be recorded or stored for further processing.
- a set of representative facial images is selected for each track in stage 702.
- the faces may be represented by face tracks where each track is a sequence of faces of, in an embodiment, the same person in consecutive video frames.
- the selection of key face images in each video track in stage 703 can substantially reduce the effects of the lower quality videos that are included in video corpus 114.
- a clustering-based algorithm for example, hierarchical clustering and/or k-means clustering, can be used in selecting the set of key face images within a video track.
- the distance between two images may be used.
- the distance between two facial images can be based on selected local Gabor features extracted from facial feature points.
- each cluster will include different faces of the same person according to pose, occlusion and quality.
- clusters having less than a predetermined number of facial images may be discarded.
- clusters or facial images having non-frontal facial images can be discarded because recognition based on non-frontal faces is less reliable.
- inter-track clustering is implemented so that tracks having images of the same person can be considered together.
- the distance measure can be based on the similarity of the key faces in each track which were chosen in stage 703.
- the distance between two clusters can be defined by the maximum similarity between a key face of a first track and a key face from the second track.
- a hierarchical clustering algorithm can also be used in this stage for clustering the tracks. After the intra-track clusters are clustered into inter-track clusters, further processing may be done to the set of key face images in each inter-track cluster so that unnecessary or duplicate images are discarded.
- processing stage 705 implements face recognition on each one of the clusters created in stage 704.
- a combination of a majority voting algorithm and a probabilistic voting algorithm can be used for selecting the identity of each facial image.
- a majority voting algorithm the identity within the cluster that occurs most frequently is chosen.
- the confidence of the identity association is also taken into account. Therefore, in a probabilistic voting algorithm, the identity having the strongest confidence score is selected. In one embodiment of the present invention, this may be accomplished by introducing a straw-person that represents some person who does not have corresponding images in the set of face models under consideration.
- each key face f i (where the set of key faces of a cluster of face tracks is ⁇ f 1 , f 2 , ..., f N ⁇ ) that is recognized by the k-nearest neighbor algorithm as person p(f i ) with confidence c(f i ).
- N(p j ) N(p j )
- ⁇ ( p ( f i ) ,p j ) is an indicator function that is 1 when the two arguments match, and 0 otherwise.
- the identity with the maximum N ( p j ) and C ( p j ) can be recognized as the identity of this cluster of tracks if those parameters reach above a predetermined threshold. If the predetermined threshold is not reached, then the identity of the cluster is unknown.
- the associations derived previously for the corresponding face or faces can be used to annotate the video.
- database of face models 121 may associate with each stored model a person's name and/or other information that can be used for annotating the newly processed video segment.
- a module such as, for example, video annotation module 109, can cooperate with video face recognition module 103 to provide annotation.
- the annotated video for example, the video stored in database of annotated video 123, can be used to provide additional information in query responses.
- a search for a person by name can retrieve links to particular segments of the video in which the person appears or an indication of a time of appearance in a full video.
- the stored information can be used to provide thumbnail images to accompany links provided in search responses.
- query module 105 can cooperate with an external search application by providing matching image clips or video clips corresponding to an identified person relevant to a search query.
Description
- This invention relates to recognizing persons in video content.
- The Internet hosts vast amounts of content of different types including text, images, and video. Leveraging this content requires that the content is searchable and organized. Images are generally searched and organized based on tags that are manually assigned by users. Similarly, video content is generally searched and organized based on tags that are manually assigned.
- However, it is impractical to maintain consistency when manually assigning tags to the large amounts of video content available on the Internet. For example, each video may be of substantial length and may include many persons appearing in different parts of the video. The video may vary according to pose, expression, illumination, occlusion, and quality. It would require a substantial amount of manual effort to accurately tag the video with the name of each person appearing in the video. The manual approach of tagging content is not scalable to the large amount of content available on the internet.
- Current approaches to general object recognition include, using an image search engine to find images relevant to a given query and then learn relevant models for various objects that are then used for object detection/recognition in images and in video. However, these approaches do not address the substantial variations that can be presented by the face of a single person in a large data set, and cannot robustly recognize a face belonging to the same person but having substantial variation due to aspects including age, makeup, expression, light conditions, etc. Other approaches automatically extract discriminant coordinates for faces and apply a clustering step to estimate likely labels using news archives. However, clustering directly based on discriminant coordinates for faces does not address issues of noise occurring in sequences of images.
US 2008/0080743 A1 addresses the creation of a face database consisting of (thumbnail) indices of each person appearing in a video stream. Temporarily continuous frames are identified where faces are present. - Therefore, what are needed are methods and systems to automatically annotate video content based on faces of persons appearing in the video.
- In one embodiment according to
claim 1, a computer-implemented method that identifies faces in a video includes the stages of: generating one or more face tracks from an input video stream; selecting key face images for each of the one or more face tracks; clustering the face tracks to generate face clusters, where each face cluster is associated with one or more key face images; creating face models from the face clusters; and correlating face models with a face model database. - In another according to claim 7, a system for identifying faces in a video includes the components: a face model database having face entries with face models and corresponding names; and a video face identifier module. The video face identifier module can include: a face detection module that detects faces in an input video stream; a face tracking module that tracks detected faces and generates face tracks; an intra-track face clustering module; an inter-track face clustering module; a detected face model generator module; and a model comparison module that compares detected face models with face entries in a database.
- In yet another embodiment, a system for identifying faces in a video includes a face model generator having as components: a name generating module that generates a name list; an image searching module that locates images corresponding to the name list; a face detection module; a face model generation module; a collection module that pair-wise stores the one or more face models and the corresponding names; and a consistency learning module. In addition to the detected face model and name pairs, the collection module may also store face model and name pairs derived from user input.
- Further features and advantages of the present invention, as well as the structure and operation of various embodiments thereof, are described in detail below with reference to the accompanying drawings. It is noted that the invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art(s) based on the teachings contained herein.
- Reference will be made to the embodiments of the invention, examples of which may be illustrated in the accompanying figures. These figures are intended to be illustrative, not limiting. Although the invention is generally described in the context of these embodiments, it should be understood that it is not intended to limit the scope of the invention to these particular embodiments.
-
FIG. 1 is a system view according to one embodiment of the present invention. -
FIG. 2 shows components of a face model generator module according to an embodiment of the present invention. -
FIG. 3 shows components of a video face recognition module according to an embodiment of the present invention. -
FIG. 4 shows a high level flowchart of a computer-implemented method implementing an embodiment of the present invention, including a model building stage and a video face recognition stage. -
FIG. 5 shows more detailed operation of the model generation stage ofFIG. 4 , according to an embodiment of the present invention. -
FIG. 6 shows a structure of a face model entry corresponding to one person, according to an embodiment of the present invention. -
FIG. 7 shows more detailed operation of the video face recognition stage ofFIG. 4 , according to an embodiment of the present invention. -
FIG. 8 is a detailed view of the operation of face detection and tracking stage ofFIG. 7 , according to an embodiment of the present invention. - While the present invention is described herein with reference to illustrative embodiments for particular applications, it should be understood that the invention is not limited thereto. Those skilled in the art with access to the teachings herein will recognize additional modifications, applications, and embodiments within the scope thereof and additional fields in which the invention would be of significant utility.
- Increasingly larger collections of video are becoming available with the proliferation of content spurred by the widespread availability of video recording devices and the connectivity offered by the Internet. Through the use of interconnected networks and shared video collections, at any instant, a single user may have access to large collection of video content on various subjects authored by persons spread throughout the world. To leverage the information contained in these large collections, it is necessary that the collections are structured in a manner that facilitates searching. A system that can automatically annotate these large collections of video with information, such as, for example, names of persons appearing in the video would be useful. The methods and systems in this disclosure make use of large text and image corpora available, for example, on the Internet, to automatically associate names and faces with minimal manual intervention, and then to derive a set of face models used for robust recognition of faces in video content. The derived set of models can be used for automatic recognition and annotation of video content to make the video content more searchable. Models may be developed, for example, for celebrities or other popular people for whom there is sufficient information available to make an association.
-
FIG. 1 shows asystem 100 that can automatically annotate a video, according to an embodiment of the present invention, with information, such as, for example, names of popular persons who appear in the video. Avideo processor module 101 is coupled to asystem interface 130 with aconnection device 131.System interface 130 may be a user interface or an application programming interface located on the same computing platform asvideo processor module 101, or a remote user interface, such as, for example, a web client. Accordingly,connection device 131 may use a connection method, such as, for example, a Peripheral Component Interconnect (PCI) bus, Ethernet, or a wireless communication standard. -
Video processor module 101 can also access avideo corpus 114, animage corpus 112, and atext corpus 110. Some or all ofcorpora network 140, such as, for example, a wide area network (WAN) like the Internet or a local area network (LAN), or may be located locally on a user's own system.Corpora corpora Video processor module 101 may be coupled tonetwork 140 through anyconnection 141 including for example and without limitation, a PCI bus, Ethernet, and a wireless communication standard.Video corpus 114 may include video clips of any length and in any video format including, for example and without limitation, any Moving Picture Experts Group (MPEG) standard, audio video interleave standard (AVI), QuickTime, and Windows Media Video (WMV). The video clips include videos having one or more persons.Image corpus 112 may include images in any image format, such as, JPEG, TIFF, and PNG.Image corpus 112 includes images of persons.Text corpus 110 includes, for example, text archives accessible locally and/or over the Internet. Available text archives may include, for example and without limitation, ASCII text, PDF text, and other forms of text. -
Video processor module 101 is also coupled to a database offace models 121 and a database of annotatedvideo 123, overconnections 142. Database offace models 121 includes face models generated by thevideo processor module 101 based at least partly on images available in theimage corpus 112. Such generation of face models will be further described with respect toFIG. 5 , below.Database 121 may include one or more face models for each person represented. It may also include additional information, such as names or other tags attached to the person or facial images of the person. Database of annotatedvideo 123 includes video, primarily fromvideo corpus 114, annotated during the processing invideo processor module 101. As used in this disclosure, "database" refers to any collection of data elements, and associated storage and access mechanisms.Connections 142 may use one or more connection methods, such as, for example, a PCI bus, Ethernet, and wireless communications standards. -
Video processor module 101 can include several components, including a facemodel generator module 102, a videoface recognition module 103, avideo annotator module 109, amodel updater module 107, and aquery module 105.Video processor module 101 and some or all of the sub-modules 102, 103, 105, 107, and 109 may be implemented in software, hardware or any combination thereof. For example,model generator module 102 may be implemented as executable code on a central processor unit (not shown inFIG. 1 ). In another embodiment,model generator module 102 may be implemented in a hardware component such as a Field Programmable Gate Array. A person skilled in the art would understand thatvideo processor module 101 may be implemented in one or more platforms. - Face
model generator module 102 may build models of faces that are selected from images inimage corpus 112 andvideo corpus 114.Module 102 may also determine a set of people whose facial images are to be modeled. For example, in an embodiment,text corpus 110 is analyzed to derive a list of most popular persons and locate one or more images of the faces of each of them. A news archive may be a combinedtext corpus 110 andimage corpus 112, and an analysis of the frequency of occurrence of person names in the news archive can generate a list of most frequently occurring names. Many of the most frequently occurring names may be associated with an image having the face of the named person in one or more news articles, and can therefore be used as a starting point to get models of the facial images of those named people. Face models derived from these and other matched images fromimage corpus 112 andvideo corpus 114 can then be stored in database offace models 121. - Video
face recognition module 103 uses face models including face models from database offace models 121 to detect and recognize faces fromvideo corpus 114. Recognition of faces in video streams is explained in more detail with respect toFIG. 7 andFIG. 8 below. As faces are detected and recognized in the video streams ofvideo corpus 114,module 103, together withvideo annotator module 109, can annotate the video with information known about the person whose face is being recognized. For example, names and tags indatabase 121 associated with a corresponding image may be used for the annotation. The annotated video, or part thereof, can then be stored in database of annotatedvideo 123. - In some embodiments of the present invention, a
model updater module 107 can be used to update face models indatabase 121 based on new additions to theimage corpus 112 andvideo corpus 114. It may also update face models indatabase 121 by adding faces that are recognized by videoface recognition module 103. Updating available face models indatabase 121 according to an increasing number of images may increase the reliability of face recognition for persons having multiple images covering a range of postures, lighting conditions, etc. Also, in some embodiments, aquery module 105 may be used for leveraging the video information in annotatedvideo database 123. For example,query module 105 may collaborate with external modules to search for a set of video clips or parts of video clips that include an appearance of a specified person, and make those video clips available for access by the external modules. In this manner, for example, a standard browser search for a specific person can be enhanced to present video tracks having at least one appearance of the specified person. -
FIG. 2 shows components of facemodel generator module 102. A namelist generator module 201 acquires a list of persons for whom face models will be generated and stored in database offace models 121. For example, namelist generator module 201 may access external text corpora, for example,text corpus 110, to determine a list of most frequently occurring names. Animage searcher module 203 associates at least one image with each name in the list of names generated bymodule 201. For example, one or more images inimage corpus 112 may be a part of a newspaper article about a celebrity whose name is on the list of names generated bymodule 201. The inclusion of the image with the article provides an association detected byimage searcher module 203. Using a list of names of persons of interest and images associated with those names, facedetector module 205 processes each image to detect a face corresponding to the associated name. Aface modeler module 207 creates one or more face models from the one or more faces detected that correspond to a single name. Amodel collector module 209 gathers all face models corresponding to the same person. Aconsistency learning module 211 selects one or more face models for each associated name, and filters out the face models that are considered weak matches. - Returning to
FIG. 1 ,face model database 121 contains one or more face models per person that is included in the database. The use of multiple models in face recognition increases the accuracy of the system. Multiple face models can represent different appearances of the same person, different light conditions, different environments, etc. A very large variation in facial expressions, facial accessories, age, light conditions, etc., can be expected for the face of the same person in a large collection of image and video content. Associated with each face model, and/or each group of face models for the same person, can be one or more tags including the person's name. - In
FIG. 3 components of videoface recognition module 103 are shown according to an embodiment of the present invention. Videoface detector module 301 detects faces in an incoming video. Afterdetector module 301 detects a face, aface tracker module 303 tracks that face in the incoming video stream.Face tracker module 303 can create one or more tracks for each face that is detected bymodule 301 and then tracked bymodule 303. An intra-track clustering module 305 then processes the generated tracks to create face clusters based on each track. For example, if the face of a single person undergoes substantial variations in appearance due to facial expressions, facial accessories such as sunglasses, different light conditions, etc., then multiple face models may be required to capture the face accurately because of its many variations even in the duration of the single track. Intra-track clustering module 305 collects one or more facial images for each face that is tracked in each video track according to the level of variation detected in a particular face. Aninter-track clustering module 307 creates clusters using the intra-track clusters of facial images.Inter-track clustering module 307 can combine similar clusters from separate tracks, to create one set of face image clusters for each person detected in the video. - Taking as input the facial image clusters generated by
inter-track clustering module 307, a video face model generator module 309 generates face models corresponding to the selected one or more images for each person. The new face models that are generated can be stored in, for example, database offace models 121. For example, having multiple face models encompassing a variety of expressions, lighting conditions, etc., generally makes it easier to reliably detect the occurrence of a face in a video or image corpus. Amodel comparison module 311 takes the newly generated face models, and may determine whether it would be beneficial to store the additional models.Module 311 matches the newly generated face models to one or more models stored indatabase 121, and thereby associates the newly generated face models with one or more tags stored indatabase 121 including, possibly, a person's name. Having associated a newly detected face with information previously stored indatabase 121,module 311 may facilitate anannotation module 109 to annotate the video track or segment with data such as the name of the associated person, and/or other information contained in the tags. The annotated video may be stored in a database of annotatedvideo 123. As new face models are added to database offacial models 121, verification and filtering algorithms such as, for example, consistency learning algorithms can be used to create or update face models. -
FIG. 4 is a flowchart showing two major processing stages according to an embodiment of the present invention. A facemodel generation stage 401 includes building and updating a database of face models, such asdatabase 121. A videoface recognition stage 402 includes using an existing face model database to detect and recognize faces in video. An embodiment of facemodel generation stage 401 is further dissected into component stages shown inFIG. 5 .Stage 401 can be implemented using components including those shown inFIG. 2 . An embodiment of videoface recognition stage 402 is further dissected to component stages shown inFIG. 7 .Stage 402 can be implemented using components, including those shown inFIG. 3 . -
FIG. 5 is a flowchart illustrating exemplary processing stages in creating a database of face models, according to an embodiment of the present invention. Instage 501, a set of names is determined, where for each name,stage 401 will attempt to determine one or more corresponding face models and store those face models in a database, such as database offace models 121. The list of names may be determined based on such criteria as names most frequently occurring in text and image corpora, such as, for example, current news archives. Such a selection criteria may generally yield a list of most popular names, such as, for example, celebrities. The automatically generated list of names may be manually or programmatically edited to add new names, to delete existing names, or to modify existing names. For example, in one embodiment, names of a user's closest friends can be added to the list of names. Text analysis methods for determining most frequently occurring names in a collection of sources are well known in the art. - In
stage 502image corpus 112 is searched to gather multiple images associated with each of the names in the name list. As stated earlier, embodiments of the present invention may operate with fully or partially integrated text and image corpora. For example, image captions or articles in which images are embedded can be used to obtain a substantially unambiguous association between a name and an associated image. In embodiments of the present invention, an association can be assumed between a name and an image, such as, the association of a name appearing frequently in a news article with an image appearing in the same article. Loose associations between a name and an image, such as in the above described news article, for example, can in general be considered weak and noisy text-image correlation data. As the size of the corresponding text corpus increases relative to the size of the image corpus, it becomes more difficult to reliably correlate a name to an image. However, the size and diversity of text and image corpora such as what is accessible on the Internet, allows the use of these weak and noisy associations as initial estimates of facial image to name associations. - In
stage 503, face detection is performed for each image found instage 502. Methods of face detection in images are well known in the art. Where an image yields only one detected face, the association between the detected face and the corresponding name may be unambiguous. When an image yields multiple defected faces, in some embodiments of the present invention, an association may be assumed between the corresponding name and each detected face. The resolution of the associations to a single person can be left to, for example,stage 506. In some embodiments, images having more than a predetermined number of detected faces may be excluded from being used for purposes of this invention. In some embodiments, methods such as correlating the most prominent facial image with the most frequently occurring name in the corresponding document may be used. A person of skill in the art will understand that there are multiple ways in which to form a loose initial association between a name appearing in a text sample and a corresponding image. In one embodiment of the present invention, a user may provide one or more images including the face of a person, and a corresponding name or text label. These user provided image and name pairs may also be added to the face database for further processing. - For each face that is detected in
stage 503, one or more feature vectors are generated instage 504. Feature vectors describe specific properties of a detected face, such that comparison of two facial images is possible using the corresponding feature vectors. Methods of generating feature vectors for a facial image are known in the art. For example, searching of image corpora for named persons and processing the images that are found is described in U.S. Patent Application Publication No.2008/0130960 titled "Identifying Images Using Face Recognition." - In
stage 505, the feature vectors are stored in association with the corresponding person name. For example, feature vectors and corresponding name pairs, including other associated tag information can be stored in a database, such as database offacial images 121.FIG. 6 illustrates an example of an entry in database offacial images 121 in one embodiment of the present invention. For a particular person A, all featurevectors 601 representing the facial images of the person may be associated with text labels or tags including the name associated with that person A. A person skilled in the art will understand that other forms of storing the data are possible. - In
stage 506, feature vectors for a specific person are processed to reduce to a set of face models and/or feature sets that can reliably be considered as belonging to that specific person. The number of face models retained for each person is variable, and primarily depends on the variation in conditions of the images such as facial expressions, age, facial accessories, lighting conditions, etc. In some embodiments of the present invention, a bagging technique such as consistency learning is applied to distil a suitable set of face models for each represented person. Consistency learning is described in detail, for example, inU.S. Patent Application 11/840,139 titled "Graph Based Sampling." -
Fig. 7 illustrates videoface recognition stage 402 in more detail, according to an embodiment of the present invention. Instage 701 video is received. Video may be received, for example and without limitation, locally, over a network, or from an external source such as a video playback device or a video recording device. The teachings of this disclosure apply to video in any video format including, for example, MPEG, AVI, QuickTime, WMV, etc. Instage 702, face detection and tracking is performed on the incoming video stream. In one embodiment of the present invention, frontal face detection based on an extension of the cascade of boosted classifiers is used. Frontal face detection using a cascade of boosted classifiers is described in P. Viola and M. Jones, "Robust real time object detection," Proceedings of the IEEE ICCV Workshop on Statistical and Computational Theories of Vision, Vancouver, Canada (July 2001). - In some embodiments of the present invention, face detection and face tracking are used alternately and periodically for each detected face. Periodically performing face detection, guided by the face tracking process, helps reduce the computational load, because face detection, in general, is computationally more intensive than face tracking. To improve the reliability of face tracking, some embodiments of the present invention can use facial feature based tracking, where a selected set of features from the feature set of each facial image is tracked. Facial feature based tracking can reliably determine whether a face can be tracked.
-
FIG. 8 is an illustration of detailed operation ofstage 702, in one embodiment of the present invention. Instage 802, eachincoming video frame 801 is observed to determine whether it is a shot boundary. Shot boundary detection is used to reduce tracking across different shots. A shot is a continuous sequence of frames from one camera. There are several different types of shot boundaries or transitions between shots, for example, including cuts and fades. A cut is an abrupt shot change that occurs in a single frame. A fade is a gradual alteration in brightness usually resulting in or starting with a solid black frame. Many other types of gradual transitions are possible. Many techniques are described in the literature for shot boundary detection. If a frame is determined to be a shot boundary frame, that frame will be processed when aface detection stage 803 detects a person's face in that frame. Whenstage 803 is successful, then a new face feature tracker can be initiated instage 806. A face tracker may be implemented as a separate processing thread primarily dedicated to tracking the assigned face as the video progresses. The face trackers, such as the one created instage 806, yield output and record the tracking results in storage, such as database offace models 121. - When it is determined in
step 802 that the current video frame is not a shot boundary, then stage 808 decides whether to perform face detection on the video frame. The use of face tracking is contemplated to reduce the computational load that can be caused by implementing face detection on each frame. Facial feature based tracking systems can efficiently determine whether tracking can be continued on each frame. When the decision instage 808 is to use face detection, then face detection is initiated instage 809. If a face is detected instage 809, it must then be determined instage 811 whether the detected face is currently being tracked. If the face detected instage 809 is being currently tracked, then instage 812 an attempt is made to reactivate an existing corresponding tracker. If it is determined instage 813 that an existing tracker cannot be reactivated 813, then a new tracker is created and activated instage 806. If the face detection fails instage 809, then an attempt is made to use the existing trackers instage 814. Instage 815, it is determined whether existing trackers are available to be used instage 814. Each existing tracker that fails instage 815 is terminated instage 816. - If, in
stage 808, it is determined that no face detection is necessary for the current video frame, then an attempt is made to reactivate each of the existing face trackers instage 817. If it is determined instage 818 that the tracker activation fails, further checks are implemented instage 819 to determine whether the tracker can be detected. Trackers that cannot be detected instage 819 are terminated instage 816. Otherwise, for those trackers that can still be detected instage 819, new trackers are created and activated instage 820. The result received instage 807 for eachinput video frame 801 can be recorded or stored for further processing. - Returning to
FIG. 7 , the video having been subjected to face detection and tracking, a set of representative facial images is selected for each track instage 702. After face detection and tracking instage 702, the faces may be represented by face tracks where each track is a sequence of faces of, in an embodiment, the same person in consecutive video frames. In considering widely available video corpora such as what is accessible in the Internet, it is often the case that the quality of the video is poor. The selection of key face images in each video track instage 703 can substantially reduce the effects of the lower quality videos that are included invideo corpus 114. A clustering-based algorithm, for example, hierarchical clustering and/or k-means clustering, can be used in selecting the set of key face images within a video track. To separate the face into clusters the distance between two images may be used. The distance between two facial images can be based on selected local Gabor features extracted from facial feature points. After clustering, each cluster will include different faces of the same person according to pose, occlusion and quality. To reduce the noise, in some embodiments of the present invention, clusters having less than a predetermined number of facial images may be discarded. In another embodiment, clusters or facial images having non-frontal facial images can be discarded because recognition based on non-frontal faces is less reliable. - The same person can appear several times in a single video. In
stage 704, inter-track clustering is implemented so that tracks having images of the same person can be considered together. In clustering tracks, the distance measure can be based on the similarity of the key faces in each track which were chosen instage 703. For example, the distance between two clusters can be defined by the maximum similarity between a key face of a first track and a key face from the second track. A hierarchical clustering algorithm can also be used in this stage for clustering the tracks. After the intra-track clusters are clustered into inter-track clusters, further processing may be done to the set of key face images in each inter-track cluster so that unnecessary or duplicate images are discarded. - Next, processing
stage 705 implements face recognition on each one of the clusters created instage 704. In an embodiment of the present invention, a combination of a majority voting algorithm and a probabilistic voting algorithm can be used for selecting the identity of each facial image. In a majority voting algorithm the identity within the cluster that occurs most frequently is chosen. In a probabilistic voting algorithm, the confidence of the identity association is also taken into account. Therefore, in a probabilistic voting algorithm, the identity having the strongest confidence score is selected. In one embodiment of the present invention, this may be accomplished by introducing a straw-person that represents some person who does not have corresponding images in the set of face models under consideration. For example, consider each key face fi (where the set of key faces of a cluster of face tracks is {f1, f2, ..., fN }) that is recognized by the k-nearest neighbor algorithm as person p(fi) with confidence c(fi). Then for each person in all recognized persons pj ∈ {p(fi)}, the number of times the key faces are recognized as pj is N(pj), i.e.,
where ∂(p(fi ),pj ) is an indicator function that is 1 when the two arguments match, and 0 otherwise. The average recognition confidence of pj is C(pj), i.e., - The identity with the maximum N(pj ) and
C (pj ) can be recognized as the identity of this cluster of tracks if those parameters reach above a predetermined threshold. If the predetermined threshold is not reached, then the identity of the cluster is unknown. - Subsequent to identifying a person's face in
stage 705, the associations derived previously for the corresponding face or faces can be used to annotate the video. For example, database offace models 121 may associate with each stored model a person's name and/or other information that can be used for annotating the newly processed video segment. A module such as, for example,video annotation module 109, can cooperate with videoface recognition module 103 to provide annotation. - The annotated video, for example, the video stored in database of annotated
video 123, can be used to provide additional information in query responses. For example, in one embodiment of the present invention, a search for a person by name can retrieve links to particular segments of the video in which the person appears or an indication of a time of appearance in a full video. In another embodiment, the stored information can be used to provide thumbnail images to accompany links provided in search responses. For example,query module 105 can cooperate with an external search application by providing matching image clips or video clips corresponding to an identified person relevant to a search query. - It is to be appreciated that the Detailed Description section, and not the Summary and Abstract sections, is intended to be used to interpret the claims. The Summary and Abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventor(s), and thus, are not intended to limit the present invention and the appended claims in any way.
- The present invention has been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.
- The foregoing description of the specific embodiments will so fully reveal the general nature of the invention that others can, by applying knowledge within the skill of the art, readily modify and/or adapt for various applications such specific embodiments, without undue experimentation, without departing from the general concept of the present invention. Therefore, such adaptations and modifications are intended to be within the meaning and range of the disclosed embodiments, based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation, such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.
- The breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents.
Claims (15)
- A computer-implemented method of identifying faces in a video, comprising:generating one or more face tracks from at least one input video stream, wherein the one or more face track respectively comprise a sequence of face images and respective feature vectors of a person's face appearing in consecutive frames of the video stream;clustering the face images of the one or more face tracks to generate one or more clusters of related face images, wherein the one or more clusters are respectively associated with one or more variations of the face, selecting a face image from each of the clusters of face images to represent the respective clusters as key face images for the respective clusters;creating face models for one or more of the key face images, wherein the face models each comprise a respective reduced set of features which describe specific properties of the person's face in association with the respective key face images;correlating at least one of the face models with a face model included in a face model database; andupdating the face model in the database based on feature vectors associated with the at least one of the face models.
- The computer-implemented method of claim 1, wherein generating one or more face tracks comprises:detecting the person's face in a frame of the consecutive frames of the video stream; andtracking the face in the consecutive frames of the video stream.
- The computer-implemented method of claim 2, wherein the detecting and the tracking is repeated at intervals for the duration of the input video stream.
- The computer-implemented method of claim 1, further comprising:
annotating at least one output video stream using data from the face model database. - The computer-implemented method of claim 1, wherein correlating includes using a majority voting algorithm to correlate the at least one of the face models to the face model included in the face model database.
- The computer-implemented method of claim 1, wherein correlating includes using a probabilistic voting algorithm to correlate the at least one of the face models to the face model included in the face model database.
- A system for identifying faces in a video, comprising:a face model database having face entries that include face models and corresponding names; wherein the face models each comprise a respective reduced set of features which describe specific properties of a detected face associated with the face model, each of the reduced set of features obtained by reducing a set of feature vectors associated with at least one image of the detected face; anda video face identifier module, comprising:a face detection module configured to detect faces in an input video stream;a face tracking module configured to track images of the face appearing in consecutive frames of the input video stream;a face clustering module configured to generate one or more clusters of the images, wherein the one or more clusters are respectively associated with one or more variations of the face;a face model generator configured to generate one or more face models for the face respectively based on the one or more clusters of face images, wherein the one or more face models are based on processed feature vectors of the face, wherein the feature vectors describe specific properties of the face; anda model comparison module that compares the one or more face models with the face entries.
- The system of claim 7, wherein the face model generator comprises:a name generating module that generates a name list; (ii) an image searching module that locates one or more images having at least one face associated with one or more corresponding names in the name list;a face detection module that detects one or more target faces in the one or more images;a generation module that generates one or more face models for at least one of the one or more target faces;a collection module that pair-wise stores the one or more face models and the one or more corresponding names; anda consistency learning module wherein one or more representative face models are selected from the one or more face models, and wherein the one or more representative face models are stored in the face model database.
- The system of claim 8 wherein the name list is generated based on names in documents accessible at one or more remote locations.
- The system of claim 9, wherein documents include web documents.
- The system of claim 7, wherein the face model database includes one or more entries comprising one or more face images and one or more corresponding text labels wherein each entry represents one entity.
- The system of claim 7 wherein the face tracking module includes a facial feature based face tracker.
- The system of claim 7 wherein the face clustering module is further configured to remove noise clusters associated with the one or more clusters.
- The system of claim 7, further comprising:
a video annotation module that annotates the input video stream producing an annotated output video stream. - The system of claim 7, further comprising:
a query module that associates a user query with one or more entries in the face model database.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/172,939 US8213689B2 (en) | 2008-07-14 | 2008-07-14 | Method and system for automated annotation of persons in video content |
PCT/US2009/004061 WO2010008520A1 (en) | 2008-07-14 | 2009-07-14 | Method and system for automated annotation of persons in video content |
Publications (2)
Publication Number | Publication Date |
---|---|
EP2318979A1 EP2318979A1 (en) | 2011-05-11 |
EP2318979B1 true EP2318979B1 (en) | 2018-06-06 |
Family
ID=41060877
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP09788910.9A Active EP2318979B1 (en) | 2008-07-14 | 2009-07-14 | Method and system for automated annotation of persons in video content |
Country Status (6)
Country | Link |
---|---|
US (1) | US8213689B2 (en) |
EP (1) | EP2318979B1 (en) |
JP (2) | JP5602135B2 (en) |
KR (1) | KR101640268B1 (en) |
CN (1) | CN102165464A (en) |
WO (1) | WO2010008520A1 (en) |
Families Citing this family (74)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP4496263B2 (en) * | 2008-10-23 | 2010-07-07 | 株式会社東芝 | Information processing apparatus and content display method |
US8396004B2 (en) * | 2008-11-10 | 2013-03-12 | At&T Intellectual Property Ii, L.P. | Video share model-based video fixing |
JP5361524B2 (en) * | 2009-05-11 | 2013-12-04 | キヤノン株式会社 | Pattern recognition system and pattern recognition method |
US8676725B1 (en) | 2009-06-05 | 2014-03-18 | Google Inc. | Method and system for entropy-based semantic hashing |
US8605956B2 (en) * | 2009-11-18 | 2013-12-10 | Google Inc. | Automatically mining person models of celebrities for visual search applications |
JP5684992B2 (en) * | 2010-02-26 | 2015-03-18 | キヤノン株式会社 | Information processing system, information processing apparatus, processing method, and program |
JP2015038640A (en) * | 2010-04-19 | 2015-02-26 | 株式会社東芝 | Video display device and video display method |
CN101853377B (en) * | 2010-05-13 | 2012-10-17 | 复旦大学 | Method for identifying content of digital video |
JP5375744B2 (en) * | 2010-05-31 | 2013-12-25 | カシオ計算機株式会社 | Movie playback device, movie playback method and program |
US8726161B2 (en) * | 2010-10-19 | 2014-05-13 | Apple Inc. | Visual presentation composition |
US20120200667A1 (en) * | 2011-02-08 | 2012-08-09 | Gay Michael F | Systems and methods to facilitate interactions with virtual content |
US8903198B2 (en) * | 2011-06-03 | 2014-12-02 | International Business Machines Corporation | Image ranking based on attribute correlation |
US8769556B2 (en) * | 2011-10-28 | 2014-07-01 | Motorola Solutions, Inc. | Targeted advertisement based on face clustering for time-varying video |
CN104025117B (en) | 2011-10-31 | 2018-09-28 | 惠普发展公司，有限责任合伙企业 | Time face sequence |
US20130148898A1 (en) * | 2011-12-09 | 2013-06-13 | Viewdle Inc. | Clustering objects detected in video |
US9239848B2 (en) | 2012-02-06 | 2016-01-19 | Microsoft Technology Licensing, Llc | System and method for semantically annotating images |
US8789120B2 (en) * | 2012-03-21 | 2014-07-22 | Sony Corporation | Temporal video tagging and distribution |
US8798401B1 (en) * | 2012-06-15 | 2014-08-05 | Shutterfly, Inc. | Image sharing with facial recognition models |
EP2680189A1 (en) * | 2012-06-26 | 2014-01-01 | Alcatel-Lucent | Method and system for generating multimedia descriptors |
US8965170B1 (en) * | 2012-09-04 | 2015-02-24 | Google Inc. | Automatic transition of content based on facial recognition |
CN103841367A (en) * | 2012-11-21 | 2014-06-04 | 深圳市赛格导航科技股份有限公司 | Monitoring system |
KR101398700B1 (en) * | 2012-12-20 | 2014-05-30 | 인하대학교 산학협력단 | Annotation system and method for video data |
JP2014139733A (en) * | 2013-01-21 | 2014-07-31 | Sony Corp | Information processing device and method, and program |
US9098552B2 (en) * | 2013-02-05 | 2015-08-04 | Google Inc. | Scoring images related to entities |
US9524282B2 (en) * | 2013-02-07 | 2016-12-20 | Cherif Algreatly | Data augmentation with real-time annotations |
US9760803B2 (en) * | 2013-05-15 | 2017-09-12 | Google Inc. | Associating classifications with images |
US9471675B2 (en) * | 2013-06-19 | 2016-10-18 | Conversant Llc | Automatic face discovery and recognition for video content analysis |
US9501693B2 (en) * | 2013-10-09 | 2016-11-22 | Honda Motor Co., Ltd. | Real-time multiclass driver action recognition using random forests |
CN103530652B (en) * | 2013-10-23 | 2016-09-14 | 北京中视广信科技有限公司 | A kind of video categorization based on face cluster, search method and system thereof |
US9852364B2 (en) * | 2014-03-19 | 2017-12-26 | Hulu, LLC | Face track recognition with multi-sample multi-view weighting |
GB2528044B (en) | 2014-07-04 | 2018-08-22 | Arc Devices Ni Ltd | Non-touch optical detection of vital signs |
CN104133875B (en) * | 2014-07-24 | 2017-03-22 | 北京中视广信科技有限公司 | Face-based video labeling method and face-based video retrieving method |
JP6472184B2 (en) | 2014-07-29 | 2019-02-20 | キヤノン株式会社 | Object identification device, object identification method, and program |
US9854973B2 (en) | 2014-10-25 | 2018-01-02 | ARC Devices, Ltd | Hand-held medical-data capture-device interoperation with electronic medical record systems |
CN105893920B (en) * | 2015-01-26 | 2019-12-27 | 阿里巴巴集团控股有限公司 | Face living body detection method and device |
US9704020B2 (en) * | 2015-06-16 | 2017-07-11 | Microsoft Technology Licensing, Llc | Automatic recognition of entities in media-captured events |
CN105357475A (en) * | 2015-10-28 | 2016-02-24 | 小米科技有限责任公司 | Video playing method and device |
CN105681749A (en) * | 2016-01-12 | 2016-06-15 | 上海小蚁科技有限公司 | Method, device and system for previewing videos and computer readable media |
US9811763B2 (en) * | 2016-01-14 | 2017-11-07 | Social Studios Ltd. | Methods and systems for building a media clip |
US9918128B2 (en) * | 2016-04-08 | 2018-03-13 | Orange | Content categorization using facial expression recognition, with improved detection of moments of interest |
KR101827978B1 (en) * | 2016-06-17 | 2018-02-09 | (주)잼투고 | Server for Providing Collaboration Service Based on Performer Object |
US10380429B2 (en) | 2016-07-11 | 2019-08-13 | Google Llc | Methods and systems for person detection in a video feed |
US10957171B2 (en) | 2016-07-11 | 2021-03-23 | Google Llc | Methods and systems for providing event alerts |
KR20180024200A (en) * | 2016-08-29 | 2018-03-08 | 오드컨셉 주식회사 | Method, apparatus and computer program for providing search information from video |
BR102016030449A2 (en) * | 2016-12-23 | 2018-07-17 | Faculdades Catolicas Associacao Sem Fins Lucrativos Mantenedora Da Pontificia Univ Catolica Do Rio D | method for evaluating and selecting facial image samples for face recognition from video sequences |
US11100384B2 (en) | 2017-02-14 | 2021-08-24 | Microsoft Technology Licensing, Llc | Intelligent device user interactions |
US11010601B2 (en) | 2017-02-14 | 2021-05-18 | Microsoft Technology Licensing, Llc | Intelligent assistant device communicating non-verbal cues |
US10467509B2 (en) | 2017-02-14 | 2019-11-05 | Microsoft Technology Licensing, Llc | Computationally-efficient human-identifying smart assistant computer |
US10506926B2 (en) | 2017-02-18 | 2019-12-17 | Arc Devices Limited | Multi-vital sign detector in an electronic medical records system |
US10492684B2 (en) | 2017-02-21 | 2019-12-03 | Arc Devices Limited | Multi-vital-sign smartphone system in an electronic medical records system |
CN106919917A (en) * | 2017-02-24 | 2017-07-04 | 北京中科神探科技有限公司 | Face comparison method |
US10223591B1 (en) | 2017-03-30 | 2019-03-05 | Amazon Technologies, Inc. | Multi-video annotation |
CN108734049A (en) * | 2017-04-13 | 2018-11-02 | 佳能株式会社 | Image processing method and device and image processing system |
US10057644B1 (en) * | 2017-04-26 | 2018-08-21 | Disney Enterprises, Inc. | Video asset classification |
US10410086B2 (en) * | 2017-05-30 | 2019-09-10 | Google Llc | Systems and methods of person recognition in video streams |
EP3410343A1 (en) * | 2017-05-30 | 2018-12-05 | Google LLC | Systems and methods of person recognition in video streams |
US11256951B2 (en) | 2017-05-30 | 2022-02-22 | Google Llc | Systems and methods of person recognition in video streams |
US11783010B2 (en) | 2017-05-30 | 2023-10-10 | Google Llc | Systems and methods of person recognition in video streams |
US10602987B2 (en) | 2017-08-10 | 2020-03-31 | Arc Devices Limited | Multi-vital-sign smartphone system in an electronic medical records system |
US11134227B2 (en) | 2017-09-20 | 2021-09-28 | Google Llc | Systems and methods of presenting appropriate actions for responding to a visitor to a smart home environment |
US10664688B2 (en) | 2017-09-20 | 2020-05-26 | Google Llc | Systems and methods of detecting and responding to a visitor to a smart home environment |
CN108229321B (en) * | 2017-11-30 | 2021-09-21 | 北京市商汤科技开发有限公司 | Face recognition model, and training method, device, apparatus, program, and medium therefor |
US10485431B1 (en) | 2018-05-21 | 2019-11-26 | ARC Devices Ltd. | Glucose multi-vital-sign system in an electronic medical records system |
KR102129843B1 (en) * | 2018-12-17 | 2020-07-03 | 주식회사 크라우드웍스 | Method for verifying real annotation works using test annotation works and apparatus thereof |
KR102192795B1 (en) * | 2019-07-01 | 2020-12-18 | 한국과학기술원 | Method and apparutus for determining a machine learning data label as a vote by a group of third parties and recording the compensation to the block chain |
US11157777B2 (en) | 2019-07-15 | 2021-10-26 | Disney Enterprises, Inc. | Quality control systems and methods for annotated content |
AU2020329148A1 (en) * | 2019-08-09 | 2022-03-17 | Clearview Ai, Inc. | Methods for providing information about a person based on facial recognition |
CN110633677B (en) * | 2019-09-18 | 2023-05-26 | 威盛电子股份有限公司 | Face recognition method and device |
US11893795B2 (en) | 2019-12-09 | 2024-02-06 | Google Llc | Interacting with visitors of a connected home environment |
US11645579B2 (en) | 2019-12-20 | 2023-05-09 | Disney Enterprises, Inc. | Automated machine learning tagging and optimization of review procedures |
US11151386B1 (en) * | 2020-03-04 | 2021-10-19 | Amazon Technologies, Inc. | Automated identification and tagging of video content |
WO2021247300A1 (en) | 2020-06-01 | 2021-12-09 | Arc Devices Limited | Apparatus and methods for measuring blood pressure and other vital signs via a finger |
US11933765B2 (en) * | 2021-02-05 | 2024-03-19 | Evident Canada, Inc. | Ultrasound inspection techniques for detecting a flaw in a test object |
WO2024035442A1 (en) * | 2022-08-11 | 2024-02-15 | Innopeak Technology, Inc. | Methods and systems for image processing |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6272231B1 (en) * | 1998-11-06 | 2001-08-07 | Eyematic Interfaces, Inc. | Wavelet-based facial motion capture for avatar animation |
US6795567B1 (en) * | 1999-09-16 | 2004-09-21 | Hewlett-Packard Development Company, L.P. | Method for efficiently tracking object models in video sequences via dynamic ordering of features |
US7308133B2 (en) * | 2001-09-28 | 2007-12-11 | Koninklijke Philips Elecyronics N.V. | System and method of face recognition using proportions of learned model |
AUPS170902A0 (en) * | 2002-04-12 | 2002-05-16 | Canon Kabushiki Kaisha | Face detection and tracking in a video sequence |
WO2007036892A1 (en) | 2005-09-30 | 2007-04-05 | Koninklijke Philips Electronics, N.V. | Method and apparatus for long term memory model in face detection and recognition |
KR100771244B1 (en) * | 2006-06-12 | 2007-10-29 | 삼성전자주식회사 | Method and apparatus for processing video data |
JP4697106B2 (en) * | 2006-09-25 | 2011-06-08 | ソニー株式会社 | Image processing apparatus and method, and program |
US7881505B2 (en) * | 2006-09-29 | 2011-02-01 | Pittsburgh Pattern Recognition, Inc. | Video retrieval system for human face content |
-
2008
- 2008-07-14 US US12/172,939 patent/US8213689B2/en active Active
-
2009
- 2009-07-14 KR KR1020117003427A patent/KR101640268B1/en active IP Right Grant
- 2009-07-14 CN CN2009801357210A patent/CN102165464A/en active Pending
- 2009-07-14 EP EP09788910.9A patent/EP2318979B1/en active Active
- 2009-07-14 JP JP2011518715A patent/JP5602135B2/en active Active
- 2009-07-14 WO PCT/US2009/004061 patent/WO2010008520A1/en active Application Filing
-
2014
- 2014-04-04 JP JP2014077554A patent/JP2014146367A/en not_active Withdrawn
Non-Patent Citations (1)
Title |
---|
HADID A ET AL: "From still image to video-based face recognition: an experimental analysis", AUTOMATIC FACE AND GESTURE RECOGNITION, 2004. PROCEEDINGS. SIXTH IEEE INTERNATIONAL CONFERENCE ON, IEEE, PISCATAWAY, NJ, USA, 17 May 2004 (2004-05-17), pages 813 - 818, XP010949533, ISBN: 978-0-7695-2122-0 * |
Also Published As
Publication number | Publication date |
---|---|
JP5602135B2 (en) | 2014-10-08 |
JP2014146367A (en) | 2014-08-14 |
WO2010008520A1 (en) | 2010-01-21 |
EP2318979A1 (en) | 2011-05-11 |
JP2011528150A (en) | 2011-11-10 |
KR101640268B1 (en) | 2016-07-15 |
CN102165464A (en) | 2011-08-24 |
KR20110036934A (en) | 2011-04-12 |
US8213689B2 (en) | 2012-07-03 |
US20100008547A1 (en) | 2010-01-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP2318979B1 (en) | Method and system for automated annotation of persons in video content | |
Leng et al. | A survey of open-world person re-identification | |
US8170280B2 (en) | Integrated systems and methods for video-based object modeling, recognition, and tracking | |
Huang et al. | Person search in videos with one portrait through visual and temporal links | |
US11776267B2 (en) | Intelligent cataloging method for all-media news based on multi-modal information fusion understanding | |
US9176987B1 (en) | Automatic face annotation method and system | |
US10025854B2 (en) | Video searching | |
CN110442747B (en) | Video abstract generation method based on keywords | |
Xu et al. | An HMM-based framework for video semantic analysis | |
US20130136319A1 (en) | Computer-implemented method for performing similarity searches | |
CN112738556B (en) | Video processing method and device | |
KR20080058356A (en) | Automated rich presentation of a semantic topic | |
WO2000016243A1 (en) | Method of face indexing for efficient browsing and searching ofp eople in video | |
Chen et al. | Name-face association with web facial image supervision | |
Baghel et al. | Image conditioned keyframe-based video summarization using object detection | |
CN111324768A (en) | Video searching system and method | |
CN109299324B (en) | Method for searching label type video file | |
EP2345978B1 (en) | Detection of flash illuminated scenes in video clips and related ranking of video clips | |
Fischer et al. | Interactive person re-identification in TV series | |
Ćalić et al. | Towards intelligent content based retrieval of wildlife videos | |
Shambharkar et al. | Automatic face recognition and finding occurrence of actors in movies | |
Suguna et al. | State of the art: A summary of semantic image and video retrieval techniques | |
Kavitha et al. | Perceptual Video Summarization using Keyframes Extraction Technique | |
Jadhav et al. | Marking Celebrity Faces Utilizing Annotation by Mining Weakly Labeled Facial Images | |
Song et al. | Large-Scale Video Understanding with Limited Training Labels |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20110214 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: AL BA RS |
|
DAX | Request for extension of the european patent (deleted) | ||
17Q | First examination report despatched |
Effective date: 20130424 |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06K 9/00 20060101AFI20171121BHEPIpc: G06K 9/62 20060101ALI20171121BHEP |
|
INTG | Intention to grant announced |
Effective date: 20171218 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EPRef country code: ATRef legal event code: REFRef document number: 1006881Country of ref document: ATKind code of ref document: TEffective date: 20180615 |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: PLFPYear of fee payment: 10 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602009052686Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: FP |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180906Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180906Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180907Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1006881Country of ref document: ATKind code of ref document: TEffective date: 20180606 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20181006 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602009052686Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20180714 |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20180731 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: MM4A |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20180714Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20180731Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20180731 |
|
26N | No opposition filed |
Effective date: 20190307 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20180731 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MTFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20180714 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: TRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180606Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20090714 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20180606 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R079Ref document number: 602009052686Country of ref document: DEFree format text: PREVIOUS MAIN CLASS: G06K0009000000Ipc: G06V0010000000 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230508 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: NLPayment date: 20230726Year of fee payment: 15 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20230727Year of fee payment: 15 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20230725Year of fee payment: 15Ref country code: DEPayment date: 20230727Year of fee payment: 15 |