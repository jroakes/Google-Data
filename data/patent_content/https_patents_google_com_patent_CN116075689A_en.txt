CN116075689A - Hybrid depth map - Google Patents
Hybrid depth map Download PDFInfo
- Publication number
- CN116075689A CN116075689A CN202180007428.7A CN202180007428A CN116075689A CN 116075689 A CN116075689 A CN 116075689A CN 202180007428 A CN202180007428 A CN 202180007428A CN 116075689 A CN116075689 A CN 116075689A
- Authority
- CN
- China
- Prior art keywords
- depth
- downloaded
- depth map
- depth information
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/04—Texture mapping
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/38—Electronic maps specially adapted for navigation; Updating thereof
- G01C21/3804—Creation or updating of map data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/30—Determination of transform parameters for the alignment of images, i.e. image registration
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10028—Range image; Depth image; 3D point clouds
Abstract
A method includes receiving a request for a depth map, generating a hybrid depth map based on a device depth map and downloaded depth information, and responding to the request for the depth map with the hybrid depth map. The device depth map may be depth data captured on the user device using sensors and/or software. The downloaded depth information may be associated with depth data, map data, image data, and/or the like stored on a server that is remote (to the user device).
Description
Technical Field
Embodiments relate to generating depth images, depth maps, and/or the like for use by applications executing on mobile devices operating in an open air or outdoor environment and/or an indoor environment.
Background
The augmented reality library may use depth images, depth maps, and/or the like to enhance the realism and immersion of applications for features including, for example, occlusion, physical, object placement, re-illumination, and atmospheric rendering effects.
Disclosure of Invention
In a general aspect, a device, system, non-transitory computer-readable medium having computer-executable program code stored thereon that is executable on a computer system, and/or method may perform a process, wherein the method includes receiving a request for a depth map, generating a hybrid depth map based on the device depth map and downloaded depth information, and responding to the request for the depth map with the hybrid depth map.
Implementations may include one or more of the following features. For example, generating the hybrid depth map may include retrieving downloaded depth information from a network device and transforming a format of the downloaded depth information to match a format of the device depth map. The generation of the hybrid depth map may include rendering the device depth map as geometric data, rendering the downloaded depth information as geometric data, and blending the device depth map geometric data with the downloaded depth information geometric data. The geometric data may be surface elements (bins). The downloaded depth information is in a latitude/longitude coordinate system format, the device depth map is in a Cartesian coordinate system format, and generating the hybrid depth map includes transforming the downloaded depth information format into the Cartesian coordinate system format. The method may further include triggering the downloading of the downloaded depth information based on the location of the mobile device.
The method may further include triggering the downloading of the downloaded depth information based on a threshold distance of the mobile device from a location associated with the downloaded depth information on the mobile device. The method may further include filtering the downloaded depth information based on a distance of the data point in the downloaded depth information from a location of the mobile device. The device depth map may be generated by at least one of: using hardware sensors of the mobile device and using software executing on the mobile device. The device depth map may be generated using data captured substantially simultaneously with the mobile device capturing the corresponding image, and the image may be rendered on a display of the mobile device immediately after the image is captured. The downloaded depth information may be captured by a device different from the device generating the device depth map, the downloaded depth information may include data captured earlier in time than the time at which the device depth map was generated, the downloaded depth information may include data convertible into depth data by the device generating the device depth map, and the downloaded depth information may include distance values in a range outside of a range of depth values associated with the device depth map. The hybrid depth map may include distance values in a first range limited in distance by a capability associated with a device generating the device depth map, and the hybrid depth map may include distance values in at least one second range outside the first range.
Drawings
Example embodiments will become more fully understood from the detailed description given herein below and the accompanying drawings, wherein like elements are represented by like reference numerals, which are given by way of illustration only and thus do not limit the example embodiments and wherein:
fig. 1 illustrates a system for generating a hybrid depth map according to an example embodiment.
Fig. 2 illustrates a system for downloading a depth map according to an example embodiment.
Fig. 3 illustrates a system for transforming a depth map format according to an example embodiment.
Fig. 4A illustrates a system for merging depth maps according to an example embodiment.
Fig. 4B illustrates a geometry according to an example embodiment.
Fig. 5 illustrates an apparatus for generating a hybrid depth map according to an example embodiment.
Fig. 6 illustrates a method for generating a hybrid depth map according to an example embodiment.
FIG. 7 illustrates an example of a computer device and a mobile computer device in accordance with at least one example embodiment.
It should be noted that these figures are intended to illustrate general features of methods, structures, and/or materials used in certain exemplary embodiments, and to supplement the written description provided below. However, the drawings are not to scale and may not accurately reflect the precise structural or performance characteristics of any given embodiment, and should not be construed as defining or limiting the scope of the values or characteristics encompassed by the example embodiments. For example, the relative thicknesses and positioning of layers, regions and/or structural elements may be reduced or exaggerated for clarity. The use of similar or identical reference numbers in the various figures is intended to indicate the presence of similar or identical elements or features.
Detailed Description
The depth image, depth map, and/or the like (hereinafter depth map) may be generated from a hardware sensor (e.g., within the mobile device) or by software (e.g., executing on the mobile device). One problem with this system is that current hardware sensors tend to be limited in scope to within about 5 to 10 meters from the mobile device including the hardware sensor. Furthermore, software depth sensing may obtain sparse depth results a few tens of meters from the mobile device executing the software. However, some applications, such as walking navigation, require depth information of tens or hundreds of meters from the mobile device executing the application.
A solution to this problem may be to use depth information external to the mobile device (e.g., generated by some other device and stored on a server). The use of external depth information alone may not be sufficient to completely solve the problem. Thus, external depth information may be used with a mobile device (locally) generated depth map.
Example embodiments may use depth information external to the mobile device. For example, the depth information may be three-dimensional (3D) geometric data representing the world of terrain, roads, and buildings in the real world. By overlaying remote depth information on a user view on a mobile device, a depth map may be generated that may exceed the hardware and/or software capabilities of the mobile device.
Fig. 1 illustrates a system for generating a hybrid depth map according to an example embodiment. As shown in fig. 1, the system 100 includes a download depth information 105 block, a device depth map 110 block, a transform depth information format 115 block, a merge 120 block, and an application 125. The system 100 may be implemented as code segments stored in a memory (e.g., non-transitory memory) for execution by a processor.
The download depth information 105 block may be configured to download depth information from a network storage device (e.g., a server). The downloaded depth information may be associated with the location of the mobile device executing the application 125. The depth information may be two-dimensional (2D) or 3D geometric data representing the world of terrain, roads, and buildings in the real world, a depth map, an image including the depth information, and/or the like. The depth information may be stored in a queue. Depth information may be downloaded based on a predicted (e.g., future) location of the mobile device.
The device depth map 110 block may be configured to obtain (e.g., receive, cause to be captured, and/or the like) depth data of the mobile device. For example, the depth data of the mobile device may be generated using hardware sensors (e.g., within the mobile device) or using software (e.g., executing on the mobile device).
The downloaded depth information may be in a first format a and the depth data of the mobile device may be in a second format B. For example, the downloaded depth information may be optimized for 2D top view (vertical) rendering, 3D top view (vertical) rendering, 2D side view (horizontal) rendering, 3D side view (horizontal) rendering, and/or the like. In other words, the downloaded depth information may be composed mainly of coordinates (latitude/longitude) or formatted. Furthermore, the location associated with the downloaded depth information data may be misplaced (e.g., offset by a few meters) as compared to the depth data of the mobile device. Thus, the downloaded depth information may be reformatted to align or match the depth data of the mobile device. The transform depth information format 115 block may be configured to align the downloaded depth information format a with the depth data of the mobile device format B. In other words, the transform depth information format 115 block may be configured to convert coordinates (e.g., latitude/longitude) of the downloaded depth information into coordinates (e.g., cartesian coordinates, horizontal coordinates, vertical coordinates, and depth) for use on the mobile device (e.g., by the application 125). The transform depth information format 115 block may be configured to shift the converted download depth information to align with the location of the mobile device. The shifted, converted, format C of the downloaded depth information may be aligned with format B of the depth data of the mobile device.
For applications requiring accurate depth information, the data associated with shifted, converted downloaded depth information alone may not be accurate enough. For example, the application 125 may be a first person view application, such as an Augmented Reality (AR) application that requires accurate depth information. Thus, the shifted, translated, downloaded depth information may be combined with the depth data of the mobile device. Thus, the merge 120 block may be configured to merge the shifted, converted download depth information with the depth data of the mobile device. This merged depth data D may be referred to as a blended depth data or blended depth map, which is input to the application 125 as accurate depth information used by the application 125. Downloading depth information is discussed in more detail with respect to fig. 2, transform downloaded depth information is described in more detail with respect to fig. 3, and merging depth data is described in more detail with respect to fig. 4.
Fig. 2 illustrates a system for downloading depth information according to an example embodiment. As shown in fig. 2, the download depth information 105 blocks may include a location 205 block, a get data trigger 210 block, a data filter 215 block, a data queue 220 block, and a select data 225 block. The download depth information 105 block may be implemented as a code segment stored in a memory (e.g., non-transitory memory) executed by a processor.
The location 205 block may be configured to determine a location of a mobile device executing the application 125. In example implementations, the mobile device may operate in an open air or outdoor environment and/or an indoor environment. Thus, the location may be obtained from a Global Positioning System (GPS) of the mobile device, a location tracking (e.g., triangulation) application (e.g., based on provider hardware location), and/or the like. The location may be updated continuously. The location may be based on an anchor point (e.g., hardware location, starting location, and/or the like). The location may be relative to some other location (e.g., a cell tower, a building, another mobile device, and/or the like).
The acquisition data trigger 210 block is configured to trigger the downloading of data representing depth information from a network device (e.g., a server). The downloading of trigger data may be based on location 205. For example, data representing depth information may be stored in the data queue 220 in association with the location 205 (e.g., coordinates). Downloading of the graph data may be initiated based on a threshold distance (e.g., 1m, 2m, 5m, and/or the like) from the location 205 of the depth information stored in the data queue 220. In other words, if the mobile device is moving such that there is no depth information in the data queue 220 within a threshold distance of the (current) location 205 of the mobile device, the acquisition of the data trigger 210 may result in downloading new data representing the depth information. In an example implementation, the downloaded depth information may be limited to a threshold range (e.g., radius) from/around the mobile device. Further, ranges associated with two or more depth information locations may overlap.
The data filter 215 block may be configured to reduce the number of data points (e.g., pixels, geometry, mesh vertices, and/or the like) associated with downloaded and/or queued depth information. The data filter 215 may be based on distance from the mobile device. For example, depth information may be filtered such that more data (e.g., pixels, geometry, mesh vertices, and/or the like) is near the location of the mobile device than is far from the mobile device. The data filter 215 may be applied to the depth information before data associated with the depth information is stored in the data queue 220. The data filter 215 may be applied to the depth information after retrieving data representing the depth information from the data queue 220. For example, data between 0 and 5 meters may not be filtered, 10% of data between 5 and 10 meters is removed, 20% of data between 10 and 15 meters is removed, 30% of data between 15 and 20 meters is removed, and so on.
The select data 225 block may select depth information from the data queue 220 in response to a request for graph data from the application 125. The selection data 225 block may select depth information based on the location 205. For example, data representing depth information may be stored in the data queue 220 in association with a geographic location. Thus, the location 205 may be used to find depth information in the data queue 220. A successful lookup may return depth information having an associated location within a threshold distance of location 205. In an example embodiment, if the lookup is unsuccessful (e.g., no graph data is returned), the get data trigger 210 may cause the requested depth information to be downloaded from a network device (e.g., server).
Fig. 3 illustrates a system for transforming a depth information format (or data format) according to an example embodiment. As shown in fig. 3, the transformed depth information format 115 blocks may include a position 205 block, a depth information 305 block, a format conversion 310 block, a position translation 315 block, and a download depth data 320 block. The transform depth information format 115 block may be implemented as a code segment stored in a memory (e.g., non-transitory memory) that is executed by a processor.
The depth information 305 block may be depth information (e.g., data) to be transformed into depth information formatted the same as a mobile device generated depth map. The depth information 305 may be (or may be equivalent to) the selection data 225. The format conversion 310 block may be configured to convert depth information having data, e.g., located in latitude/longitude coordinates, into depth information data, wherein depth is based on relative altitude (e.g., if the camera is located above the location (vertical)) or based on relative distance (e.g., if the camera is located on the ground (e.g., horizontal)), the depth information data having, e.g., such data: positioned with horizontal/vertical coordinates of depth based on relative distance from the reference position. In an example embodiment, the horizontal/vertical (e.g., cartesian, x, y) coordinates and depth (e.g., z) may be based on the direction of the data point (e.g., latitude/longitude coordinates of the data point based on latitude/longitude coordinates relative to the mobile device location) and the depth (e.g., altitude).
The location translation 315 block may be configured to modify the reformatted depth information data based on the location of the user device and/or the location of a component within the user device. For example, the reformatted depth information data may consider the location 205 without regard to the distance (e.g., altitude) of the mobile device from the ground. The position translation 315 may be configured to modify reformatted depth information data (e.g., x, y, and/or depth) based on the distance of the mobile device from the ground. Furthermore, the GPS system may not be as accurate as required by the application 125. However, for example, based on predetermined factors (e.g., distance and/or direction) determined during mobile device calibration, the position translation 315 may correct for this lack of precision by modifying the reformatted depth information data (e.g., x, y, and/or depth) based. Finally, the location of the GPS within the mobile device may be different from the location of the mobile device depth hardware sensor (and/or equivalent depth location of the software calculated depth). Thus, the position translation 315 may correct for this lack of precision caused by such differences by modifying the reformatted depth information data (e.g., x, y, and/or depth) based on, for example, predetermined positioning factors determined prior to manufacturing the mobile device or before. The downloaded depth data 320 block may include reformatted and translated depth information data as depth information (e.g., 2D or 3D geometry data, depth maps, images including depth information, and/or the like) that may be used by the application 125.
Fig. 4 illustrates a system for merging depth maps according to an example embodiment. As shown in fig. 4, the merge 120 block may include a device depth map 110 block, a downloaded depth data 320 block, a binning 405 block, a binning 410 block, and a blending depth map 415 block. The merge 120 block may be implemented as a code segment stored in a memory (e.g., non-transitory memory) executed by a processor.
The bin conversion 405 block may be configured to convert depth information into bin data. The device depth map 110 block and the downloaded depth data 320 block may each include depth data, which may be elements of a depth map (e.g., geometry data, images including pixel data and depth data associated with pixels) or a data structure representing a depth map. The data structure may include color information, texture information, depth information, position information, and direction information. The depth information may include depth layers, each having a number (e.g., index or z-index) indicating a layer order. The depth information may be a Layered Depth Image (LDI) having a plurality of ordered depths for each pixel in the image. The depth information may include a depth map. Thus, the device depth map 110 block may correspond to a depth image (or data structure) captured by the mobile device, and the downloaded depth data 320 block may correspond to a depth image (or data structure) downloaded from a network device (e.g., server).
The face conversion 405 block may be configured to convert depth maps captured by the mobile device and/or depth information downloaded from the network device into face data. For example, a pixel may be considered a surface element (bin) when projected onto a representation of a surface. The bins may be used to efficiently render complex geometric objects in real-time (at interactive frame rates) in 3D space. A bin may contain one or more samples (points) contained in the original LDI. A bin may be a point bin that does not have any particular connectivity. Thus, the bins can be used to model dynamic geometry, as topology information, such as adjacency information, need not be calculated. Attributes of a bin may include, but are not limited to, depth, texture color, radius, and normalized orientation vector and position.
Each scene image may be assembled into a data structure (e.g., LDI) that may be used in the representation of a simplified version of the scene for rendering (drawing) in 3D real-time in, for example, AR space by an AR application. For example, multiple pixel samples or points may be grouped into multiple partitions. A partition may be a plane or polygon comprising a subset of a plurality of pixel samples or points representing an image of a scene. The partition plane (e.g., partition plane 304) may be at a location in 3D image space where a subset of points are located within 3D space. In some implementations, a quadrilateral algorithm may create a polygonal approximation that may be used to create a partition plane. In some implementations, the iterative partitioning algorithm may create a polygonal approximation that may be used to create a partitioning plane.
In an example embodiment, the hybrid depth map may include distance values within a first range limited in distance by a capability associated with a device that generates the device depth map, and the hybrid depth map may include distance values within at least one second range that is outside of the first range. In other words, the hybrid depth map may include distance values ranging or approximating a range of maximum depth of the mobile device (e.g., a depth sensor in the mobile device). These depth values may be based on a device depth map. The hybrid depth map may also include distance values that exceed the maximum depth of the mobile device. These depth values may be based on downloaded depth information.
Fig. 4B illustrates a geometry according to an example embodiment. The geometry may be stored as a data structure (as described above). For example, the data structure may be an n-tuple, a bin, and/or the like). In an example embodiment, a bin may be generated from an input depth by first estimating a normal orientation associated with each pixel in the input. Once there is a depth value and normal vector associated with each pixel, a bin may be generated by clustering the pixels and generating a disk in world coordinates (as described above). The size of these disks may be based on the number of adjacent pixels sharing the same depth and orientation (and possibly color). These bins may be stored across frames of the AR session, and as each new depth frame is integrated, the bins may be updated and/or merged based on this new information. If the new depth information is inconsistent with the previous face metadata (e.g., something in the scene moves), the original face may be penalized (e.g., confidence reduced), deleted, and/or replaced with the new geometry.
As shown in fig. 4B, grid 420 may represent a portion of a real world geometry (or partition plane) that may be displayed on a display of the mobile device (e.g., as an AR display). On grid 420 are a first object 425-1 (e.g., a building) and a second object 425-2 (e.g., a tree) in the real world. In the illustration, the second object 425-2 may be closer to the user device than the first object 425-1. Circle 430 is a graphical representation of a data structure (e.g., a bin occupancy zone) that may include information about a portion of grid 420 that includes first object 425-1 and second object 425-2. In an example embodiment, the data structure (or data structure, bin, and/or the like) may be stored as a stored geometry. For example, the binning 405 may generate and store a geometry representing the device depth map 110 and/or the downloaded depth data 320.
The bin blend 410 block may be configured to blend the geometry representing the device depth map 110 with the geometry representing the downloaded depth data 320. Blending the geometry representing the device depth map 110 with the geometry representing the downloaded depth data 320 may generate a representation of a real world or real world image. Blending two or more images may include combining portions of each image. For example, data representing missing data (e.g., depth data, color data, pixels, and/or the like) in the geometry of the device depth map 110 may be populated with data from the geometry representing the downloaded depth data 320. For example, pixels having the same location and/or the same location and the same depth may be combined. The location may be based on distance and direction from a reference point (or origin). The location may be based on a coordinate system (e.g., a Cartesian grid, an x, y grid, or grid 420).
For example, a portion of pixels in an image that includes a depth map may be blank or marked as invalid. Thus, in example implementations, pixels (e.g., having depth) that are missing or marked as invalid (e.g., missing depth information or invalid depth information) in the geometry (or rendering) representing the device depth map 110 may be filled with pixels (e.g., depth information) from the geometry representing the downloaded depth data 320, the downloaded depth data 320 having the same location and layer as the missing or invalid pixels, and vice versa. In an example embodiment, pixels from the geometry representing the device depth map 110 and the geometry representing the downloaded depth data 320 have the same location and are at a layer having the same index value. Blending may include selecting pixels from the geometry representing the device depth map 110 and discarding pixels from the geometry representing the downloaded depth data 320 (or vice versa).
Alternatively, blending may include selecting pixels from the geometry representing the downloaded depth data 320 and discarding pixels from the geometry representing the device depth map 110. Alternatively, blending the two images may include averaging the colors and assigning the average colors to the locations and layers. Other techniques for blending two images are within the scope of the invention.
In an example embodiment, the downloaded data may be incorporated into the device data. The motivation for this is that the downloaded data may be static (e.g., the data does not change during the AR session). However, as the user continues to scan the environment, the device data has an opportunity to change. Thus, incorporating the downloaded data into the device data may produce more accurate measurements over time. By incorporating the downloaded data into the device data, the technique may provide an initial estimate for each region, but the device may refine the initially downloaded data if the user happens to scan the region initially covered by the downloaded data. When this occurs, the originally downloaded data can be updated and improved with the new device data.
Furthermore, the images can be projected into each other prior to blending. For example, an image may be captured while the mobile device is moving. The previous frame (e.g., stored in a buffer) may be re-projected into the current frame. This embodiment may enable (or help enable) alignment of objects and/or observed features across frames. The hybrid depth map 415 block may be or may include a resulting blend of the geometry representing the device depth map 110 and the geometry representing the downloaded depth data 320.
Fig. 5 illustrates a system for generating a hybrid depth map according to an example embodiment. As shown in fig. 5, the system 500 includes a server 505 communicatively coupled to a mobile device 515 (or user device). The server 505 includes a memory 510. The memory 510 may be configured to store data structures, geometries, images, data maps, and/or the like, each including depth data and/or depth information associated with a real-world location. The server 505 may be configured to transmit geometry, images, data maps, and/or the like in response to a request from the mobile device 515.
The mobile device 515 may include a processor 520 and memory 525. The memory may include downloaded depth information 105 blocks, device depth map 110 blocks, transformed depth information format 115 blocks, merge 120 blocks, and applications 125, as described above. The processor 520 may be configured to execute code to implement the functions of the downloaded depth information 105 block, the device depth map 110 block, the transform depth information format 115 block, the merge 120 block, and/or the application 125.
Fig. 6 illustrates a method for generating a hybrid depth map according to an example embodiment. As shown in fig. 6, in step S605, a request for a depth map is received from an application. An application (e.g., application 125) may need a depth map to perform an operation. For example, the application may operate on an AR device (e.g., mobile device 515). The application may cause an image including depth information to be rendered on a display of the AR device. In an example embodiment, the depth information may include a hybrid depth map.
In step S610, a device depth map is acquired. The device depth map may be a depth map generated by a device (e.g., mobile device 515) using sensors of the device and/or software executing on the device configured to generate depth information. For example, the device may be an AR device executing an AR application of the requested depth map (and/or depth image, depth data, depth information, and/or the like). The AR device may operate in an open air and/or outdoor environment. The AR application may cause the AR device to trigger generation of the depth map as a device depth map (e.g., using sensors and/or software).
In step S615, depth information is downloaded. The AR device may operate in an open air and/or outdoor environment, and thus, image data (e.g., map data), depth data, depth information, and/or the like may be available based on the location of the AR device. For example, an image captured by a vehicle may be used as map data that may include depth information and/or an image captured by an orbiting satellite may be used as map data that may include depth information. The graph data including depth information (e.g., as metadata) may be stored at a network device (e.g., server 505) communicatively coupled with the AR device (e.g., as mobile device 515). Thus, the AR device (e.g., software executing on the AR device) may request depth information (and/or depth images, depth data, depth maps, and/or the like) from the network device. Depth information may be downloaded from the network device in response to the request.
In step S620, the downloaded depth information is transformed. For example, the downloaded depth information (data or depth map data) may be composed mainly of coordinates (latitude/longitude) or formatted. Furthermore, the locations associated with the downloaded depth information may be misaligned (e.g., a few meters apart) as compared to the device depth map (generated by the AR or mobile device). Thus, data associated with the downloaded depth information may be reformatted to align with the device depth map data. In other words, the downloaded depth information data format may be aligned with the device depth map data format. For example, the coordinates (e.g., latitude/longitude) of the downloaded depth information may be converted to coordinates (e.g., cartesian coordinates, horizontal coordinates, vertical coordinates, and depth) for use on the AR or mobile device (e.g., application 125). The converted download depth information may also be moved (or translated) to align with the AR or mobile device's location.
In step S625, a hybrid depth map is generated by merging the device depth map with the transformed downloaded depth information. For example, the device depth map and the transformed downloaded depth information may be rendered into a bin geometry representing the device depth map and the downloaded depth information. The geometry representing the device depth map may then be blended with the geometry representing the downloaded depth information. Blending the geometry representing the device depth map with the geometry representing the downloaded depth information may generate a representation of the real world or real world image (a view representing the location of the AR or mobile device). Blending two or more geometries (similar to an image) may include combining portions of each geometry. For example, data representing missing geometry of the device depth map (e.g., depth data, color data, pixels, and/or the like) may be populated with data from the geometry representing the downloaded depth information (or vice versa). For example, pixels having the same location and/or the same location and the same depth may be combined. The location may be based on distance and direction from a reference point (or origin). The location may be based on a coordinate system (e.g., an x, y grid). The blending geometry may be or include a blended depth map.
In step S630, the hybrid depth map is transmitted to the requesting application. For example, the hybrid depth map may be transmitted to an AR application (e.g., application 125) for rendering an image on a display of the AR or mobile device.
Fig. 7 illustrates an example of a computer device 700 and a mobile computer device 750 that can be used with the techniques described here (e.g., to implement a client computing device (e.g., mobile device 515), a server computing device (e.g., server 505), and system 100 implemented on mobile device 515). Computing device 700 includes a processor 702, memory 704, storage 706, a high-speed interface 708 connected to memory 704 and high-speed expansion ports 710, and a low-speed interface 708 and storage 706 connected to low-speed bus 714. Each of the components 702, 704, 706, 708, 710, and 712, are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 702 may process instructions for execution within the computing device 700, including instructions stored in the memory 704 or on the storage device 706, to display graphical information for a GUI on an external input/output device (e.g., display 716 coupled to the elevation interface 708). In other embodiments, multiple processors and/or multiple buses, as well as multiple memories and memory types may be used, as appropriate. In addition, multiple computing devices 700 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a blade server bank, or a multiprocessor system).
The storage device 706 is capable of providing mass storage for the computing device 700. In one implementation, the storage device 706 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. The computer program product may be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 704, the storage device 706, or memory on processor 702.
The high speed controller 708 manages bandwidth-intensive operations for the computing device 700, while the low speed controller 712 manages lower bandwidth-intensive operations. This allocation of functions is merely an example. In one embodiment, the high speed controller 708 is coupled to the memory 704, the display 716 (e.g., via a graphics processor or accelerator), and to a high speed expansion port 710 that can accept various expansion cards (not shown). In an implementation, a low speed controller 712 is coupled to the storage device 706 and to a low speed expansion port 714. The low-speed expansion port, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a network device, such as a switch or router, for example, through a network adapter.
As shown, computing device 700 may be implemented in a number of different forms. For example, it may be implemented as a standard server 720, or multiple times in a group of such servers. It may also be implemented as part of a rack server system 724. Furthermore, it may be implemented in a personal computer, such as a laptop computer 722. Alternatively, components from computing device 700 may be combined with other components in a mobile device (not shown), such as device 750. Each such device may contain one or more computing devices 700, 750, and the entire system may be made up of multiple computing devices 700, 750 in communication with each other.
The processor 752 may execute instructions within the computing device 750, including instructions stored in the memory 764. A processor may be implemented as a chipset that includes separate and multiple analog and digital processors. For example, the processor may provide for coordination of the other components of the device 750, such as control of user interfaces, applications run by device 750, and wireless communication of device 750.
The processor 752 may communicate with a user through a control interface 758 and a display interface 756 coupled to a display 754. The display 754 may be, for example, a TFT LCD (thin film transistor liquid crystal display) and LED (light emitting diode) or OLED (organic light emitting diode) display, or other suitable display technology. The display interface 756 may comprise suitable circuitry for driving the display 754 to present graphical and other information to a user. The control interface 758 may receive commands from a user and convert them for submission to the processor 752. In addition, an external interface 762 may be provided in communication with processor 752, to enable device 750 to communicate closely with other devices. For example, the external interface 762 may provide wired communication in some embodiments, or wireless communication in other embodiments, and multiple interfaces may also be used.
The memory may include, for example, flash memory and/or NVRAM memory, as described below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 764, expansion memory 774, or memory on processor 752, which may be received, for example, over transceiver 768 or external interface 762.
As shown, computing device 750 may be implemented in a number of different forms. For example, it may be implemented as a cellular telephone 780. It may also be implemented as part of a smart phone 782, personal digital assistant, or other similar mobile device.
Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments may include embodiments in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. The terms "machine-readable medium," computer-readable medium "and/or" computer program product, apparatus, and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) as used herein refer to any computer program product, apparatus, or device that provides machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (LED or OLED or LCD monitor/screen) for displaying information to the user, a keyboard and a pointing device (e.g., a mouse or trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); input from the user may be received in any form, including acoustic, speech, or tactile input.
The systems and techniques described here can be implemented in a computing system that includes a back-end component (e.g., as a data server) or that includes a middleware component (e.g., an application server) or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the Internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
In some implementations, the computing device depicted in the figures may include a sensor that interfaces with the AR headset/HMD device 790 to generate an enhanced environment for viewing content inserted within the physical space. For example, one or more sensors included on computing device 750 or other computing devices depicted in the figures may provide input to AR headset 790, or generally to AR space. The sensors may include, but are not limited to, touch screens, accelerometers, gyroscopes, pressure sensors, biosensors, temperature sensors, humidity sensors, and ambient light sensors. The computing device 750 may use the sensors to determine an absolute position of the computing device in the AR space and/or a detected rotation, which may then be used as input to the AR space. For example, computing device 750 may incorporate AR space as a virtual object, such as a controller, laser pointer, keyboard, weapon, and the like. The user's positioning of the computing device/virtual object when incorporated into the AR space may allow the user to position the computing device to view the virtual object in some manner in the AR space. For example, if the virtual object represents a laser pointer, the user may manipulate the computing device as if it were an actual laser pointer. The user may move the computing device left and right, up and down, in a circular fashion, etc., and use the device in a manner similar to using a laser pointer. In some implementations, the user may aim at the target location using a virtual laser pointer.
In some implementations, one or more input devices included on computing device 750 or connected to computing device 750 may be used as input to the AR space. The input device may include, but is not limited to, a touch screen, keyboard, one or more buttons, a touch pad, a pointing device, a mouse, a trackball, a joystick, a camera, a microphone, an earphone or headphones with input capabilities, a game controller, or other connectable input devices. When the computing device is incorporated into the AR space, user interaction with an input device included on computing device 750 may cause certain actions to occur in the AR space.
In some implementations, the touch screen of computing device 750 may be rendered as a touch pad in AR space. The user may interact with a touch screen of the computing device 750. For example, in AR headset 790, interactions are rendered as movements on a touch pad rendered in AR space. The act of rendering may control virtual objects in the AR space.
In some implementations, one or more output devices included on computing device 750 may provide output and/or feedback to a user of AR headset 790 in AR space. The output and feedback may be visual, tactile or audible. The output and/or feedback may include, but is not limited to, vibration, turning on and off of one or more lights or flashing and/or blinking, alerting, playing a ring tone, playing a song, and playing an audio file. Output devices may include, but are not limited to, vibration motors, vibration coils, piezoelectric devices, electrostatic devices, light Emitting Diodes (LEDs), flashlights, and speakers.
In some implementations, the computing device 750 may appear as another object in a computer-generated 3D environment. User interactions with computing device 750 (e.g., rotating, panning, touching the touch screen, sliding a finger over the touch screen) may be interpreted as interactions with objects in AR space. In an example of a laser pointer in AR space, computing device 750 appears as a virtual laser pointer in a computer-generated 3D environment. When a user manipulates computing device 750, the user in AR space sees the movement of the laser pointer. The user receives feedback on computing device 750 or AR headset 790 from interactions with computing device 750 in an AR environment. User interactions with the computing device may be translated into interactions with a user interface generated in the AR environment for the controllable appliance.
In some implementations, the computing device 750 may include a touch screen. For example, a user may interact with a touch screen to interact with a user interface of a controllable device. For example, the touch screen may include user interface elements, such as sliders, that may control properties of the controllable device.
Many embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the specification.
Furthermore, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. Further, other steps may be provided from the described flows, or steps may be eliminated, and other components may be added to, or removed from, the described systems. Accordingly, other embodiments are within the scope of the following claims.
In addition to the above description, controls may be provided to the user that allow the user to select as to whether and when the systems, programs, or features described herein are capable of collecting user information (e.g., information about the user's social network, social behavior or activity, profession, user preferences, or the user's current location), and whether to send content or communications from a server to the user. In addition, some data may be processed in one or more ways to delete personal identity information before it is stored or used. For example, the identity of the user may be processed such that the user's personal identity information cannot be determined, or the user's geographic location may be generalized (e.g., to a city, zip code, or state level) in the event that location information is obtained, such that the user's specific location cannot be determined. Thus, the user can control which information is collected about the user, how that information is used, and which information is provided to the user.
While certain features of the described embodiments have been illustrated as described herein, many modifications, substitutions, changes, and equivalents will now occur to those skilled in the art. It is, therefore, to be understood that the appended claims are intended to cover all such modifications and changes as fall within the scope of the embodiments. It is to be understood that they have been presented by way of example only, and not limitation, and various changes in form and details may be made. Any of the portions of the devices and/or methods described herein may be combined in any combination, except combinations that are mutually exclusive. The embodiments described herein may include various combinations and/or sub-combinations of the functions, components, and/or features of the different embodiments described.
While example embodiments are susceptible to various modifications and alternative forms, embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that there is no intent to limit example embodiments to the particular forms disclosed, but on the contrary, example embodiments are to cover all modifications, equivalents, and alternatives falling within the scope of the claims. Like numbers refer to like elements throughout the description of the figures.
Some of the above example embodiments are described as processes or methods illustrated in flowcharts. Although a flowchart depicts operations as a sequential process, many of the operations can be performed in parallel, concurrently, or with other operations. Furthermore, the order of the operations may be rearranged. These processes may terminate when their operations are completed, but may also have additional steps not included in the figures. A process may correspond to a method, a function, a procedure, a subroutine, etc.
The methods discussed above (some of which are illustrated by flowcharts) may be implemented by hardware, software, firmware, middleware, microcode, hardware description languages, or any combination thereof. When implemented in software, firmware, middleware or microcode, the program code or code segments to perform the necessary tasks may be stored in a machine or computer readable medium such as a storage medium. The processor may perform the necessary tasks.
Specific structural and functional details disclosed herein are merely representative for purposes of describing example embodiments. However, the example embodiments are embodied in many alternate forms and should not be construed as limited to only the embodiments set forth herein.
It will be understood that, although the terms first, second, etc. may be used herein to describe various elements, these elements should not be limited by these terms. These terms are only used to distinguish one element from another element. For example, a first element could be termed a second element, and, similarly, a second element could be termed a first element, without departing from the scope of example embodiments. As used herein, the term and/or includes any and all combinations of one or more of the associated listed items.
It will be understood that when an element is referred to as being connected or coupled to another element, it can be directly connected or coupled to the other element or intervening elements may be present. In contrast, when an element is referred to as being directly connected or directly coupled to another element, there are no intervening elements present. Other words used to describe the relationship between elements should be interpreted in a similar fashion (e.g., between and directly between, adjacent and directly adjacent, etc.).
The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of example embodiments. As used herein, the singular forms "a", "an", and "the" are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms comprises, comprising, including, and/or comprising, when used herein, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.
It should also be noted that in some alternative implementations, the functions/acts noted may occur out of the order noted in the figures. For example, two figures shown in succession may in fact be executed concurrently or the figures may sometimes be executed in the reverse order, depending upon the functionality/acts involved.
Unless defined otherwise, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which example embodiments belong. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.
Portions of the above example embodiments and corresponding detailed description are presented in terms of software, or algorithms and symbolic representations of operations on data bits within a computer memory. These descriptions and representations are the ones by which those of ordinary skill in the art effectively convey the substance of their work to others of ordinary skill in the art. An algorithm, as the term is used here, and as it is used generally, is conceived to be a self-consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of optical, electrical, or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like.
In the illustrative embodiments described above, references to acts and symbolic representations of operations that may be implemented as program modules or functional processes (e.g., in the form of flowcharts) include routines, programs, objects, components, data structures, etc., which perform particular tasks or implement particular abstract data types, and which may be described and/or implemented using existing hardware at existing structural elements. Such existing hardware may include one or more Central Processing Units (CPUs), digital Signal Processors (DSPs), application specific integrated circuits, field Programmable Gate Arrays (FPGA) computers, and the like.
It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise, or as is apparent from the discussion, terms such as processing or computing or calculating or determining or displaying or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical, electronic quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices.
It is also noted that the software-implemented aspects of the exemplary embodiments are typically encoded on some form of non-transitory program storage medium or implemented over some type of transmission medium. The program storage medium may be magnetic (e.g., a floppy disk or a hard drive) or optical (e.g., a compact disk read only memory or a CD ROM), and may be read only or random access. Similarly, the transmission medium may be twisted wire pairs, coaxial cable, optical fiber, or some other suitable transmission medium known to the art. The example embodiments are not limited by these aspects of any given implementation.
Finally, it should also be noted that although the appended claims list a particular combination of features described herein, the scope of the disclosure is not limited to the particular combinations claimed, but extends to any combination of features or embodiments disclosed herein, regardless of whether such particular combination is specifically recited in the appended claims at this time.
Claims (36)
1. A method, comprising:
receiving a request for a depth map;
generating a hybrid depth map based on the device depth map and the downloaded depth information; and
responding to the request for depth maps with the hybrid depth maps.
2. The method of claim 1, wherein generating the hybrid depth map comprises
Retrieving the downloaded depth information from a network device
Transforming the format of the downloaded depth information to match the format of the device depth map.
3. The method of claim 1, wherein generating the hybrid depth map comprises:
rendering the device depth map as geometric data,
rendering the downloaded depth information as geometric data, and
blending the device depth map geometry data with the downloaded depth information geometry data.
4. A method according to claim 3, wherein the geometric data is a surface element, (bin).
5. The method of claim 1, wherein,
the downloaded depth information is in a latitude/longitude coordinate system format,
the device depth map is in Cartesian coordinate system format, and
generating the hybrid depth map includes: and transforming the downloaded depth information format into the Cartesian coordinate system format.
6. The method of claim 1, further comprising: the downloading of the downloaded depth information is triggered based on the location of the mobile device.
7. The method of claim 1, further comprising: the downloading of the downloaded depth information is triggered based on a threshold distance of a mobile device from a location associated with the downloaded depth information on the mobile device.
8. The method of claim 1, further comprising: and filtering the downloaded depth information based on the distance between the data point in the downloaded depth information and the position of the mobile device.
9. The method of claim 1, wherein the device depth map is generated by at least one of: using hardware sensors of a mobile device, and using software executing on the mobile device.
10. The method of claim 1, wherein,
the device depth map is generated using data captured substantially simultaneously with the mobile device capturing the corresponding image, an
The image is rendered on a display of the mobile device immediately after the capturing of the image.
11. The method of claim 1, wherein,
the downloaded depth information is captured by a device different from the device generating the device depth map,
the downloaded depth information includes data captured earlier in time than the time at which the device depth map was generated,
the downloaded depth information includes data convertible to depth data by a device generating the device depth map, and
the downloaded depth information includes distance values in a range outside of a range of depth values associated with the device depth map.
12. The method of claim 1, wherein,
the hybrid depth map includes distance values in a first range limited in distance by a capability associated with a device generating the device depth map, an
The hybrid depth map includes distance values in at least one second range outside the first range.
13. A non-transitory computer-readable storage medium comprising instructions stored thereon that, when executed by at least one processor, are configured to cause a computing system to:
receiving a request for a depth map;
generating a hybrid depth map based on the device depth map and the downloaded depth information; and
responding to the request for depth maps with the hybrid depth maps.
14. The non-transitory computer-readable storage medium of claim 13, wherein generating the hybrid depth map comprises:
retrieving the downloaded depth information from a network device
Transforming the format of the downloaded depth information to match the format of the device depth map.
15. The non-transitory computer-readable storage medium of claim 13, wherein generating the hybrid depth map comprises:
rendering the device depth map as geometric data,
Rendering the downloaded depth information as geometric data, and
blending the device depth map geometry data with the downloaded depth information geometry data.
16. The non-transitory computer-readable storage medium of claim 15, wherein the geometric data is a surface element (bin).
17. The non-transitory computer-readable storage medium of claim 13, wherein,
the downloaded depth information is in a latitude/longitude coordinate system format,
the device depth map is in Cartesian coordinate system format, and
generating the hybrid depth map includes: and transforming the downloaded depth information format into the Cartesian coordinate system format.
18. The non-transitory computer-readable storage medium of claim 13, further comprising: the downloading of the downloaded depth information is triggered based on the location of the mobile device.
19. The non-transitory computer-readable storage medium of claim 13, further comprising: the downloading of the downloaded depth information is triggered based on a threshold distance of a mobile device from a location associated with the downloaded depth information on the mobile device.
20. The non-transitory computer-readable storage medium of claim 13, further comprising: and filtering the downloaded depth information based on the distance between the data point in the downloaded depth information and the position of the mobile device.
21. The non-transitory computer-readable storage medium of claim 13, wherein the device depth map is generated by at least one of: using hardware sensors of a mobile device, and using software executing on the mobile device.
22. The non-transitory computer-readable storage medium of claim 13, wherein,
the device depth map is generated using data captured substantially simultaneously with the mobile device capturing the corresponding image, an
The image is rendered on a display of the mobile device immediately after the capturing of the image.
23. The non-transitory computer-readable storage medium of claim 13, wherein,
the downloaded depth information is captured by a device different from the device generating the device depth map,
the downloaded depth information includes data captured earlier in time than the time at which the device depth map was generated,
the downloaded depth information includes data convertible to depth data by a device generating the device depth map, and
the downloaded depth information includes distance values in a range outside of a range of depth values associated with the device depth map.
24. The non-transitory computer-readable storage medium of claim 13, wherein,
the hybrid depth map includes distance values in a first range limited in distance by a capability associated with a device generating the device depth map, an
The hybrid depth map includes distance values in at least one second range outside the first range.
25. An apparatus, comprising:
at least one processor; and
at least one memory including computer program code;
the at least one memory and the computer program code are configured to, with the at least one processor, cause the apparatus to:
receiving a request for a depth map;
generating a hybrid depth map based on the device depth map and the downloaded depth information; and
responding to the request for depth maps with the hybrid depth maps.
26. The apparatus of claim 25, wherein generating the hybrid depth map comprises:
retrieving the downloaded depth information from a network device
Transforming the format of the downloaded depth information to match the format of the device depth map.
27. The apparatus of claim 25, wherein generating the hybrid depth map comprises:
Rendering the device depth map as geometric data,
rendering the downloaded depth information as geometric data, and
blending the device depth map geometry data with the downloaded depth information geometry data.
28. The apparatus of claim 27, wherein the geometric data is a surface element (bin).
29. The apparatus of claim 25, wherein,
the downloaded depth information is in a latitude/longitude coordinate system format,
the device depth map is in Cartesian coordinate system format, and
generating the hybrid depth map includes: and transforming the downloaded depth information format into the Cartesian coordinate system format.
30. The apparatus of claim 25, further comprising: the downloading of the downloaded depth information is triggered based on the location of the mobile device.
31. The apparatus of claim 25, further comprising: the downloading of the downloaded depth information is triggered based on a threshold distance of a mobile device from a location of a location associated with the downloaded depth information on the mobile device.
32. The apparatus of claim 25, further comprising: and filtering the downloaded depth information based on the distance between the data point in the downloaded depth information and the position of the mobile device.
33. The apparatus of claim 25, wherein the device depth map is generated by at least one of: using hardware sensors of a mobile device, and using software executing on the mobile device.
34. The apparatus of claim 25, wherein,
the device depth map is generated using data captured substantially simultaneously with the mobile device capturing the corresponding image, an
The image is rendered on a display of the mobile device immediately after the capturing of the image.
35. The apparatus of claim 25, wherein,
the downloaded depth information is captured by a device different from the device generating the device depth map,
the downloaded depth information includes data captured earlier in time than the time at which the device depth map was generated,
the downloaded depth information includes data convertible to depth data by a device generating the device depth map, and
the downloaded depth information includes distance values in a range outside of a range of depth values associated with the device depth map.
36. The apparatus of claim 25, wherein,
the hybrid depth map includes distance values in a first range limited in distance by a capability associated with a device generating the device depth map, an
The hybrid depth map includes distance values in at least one second range outside the first range.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/071339 WO2023033858A1 (en) | 2021-09-01 | 2021-09-01 | Hybrid depth maps |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116075689A true CN116075689A (en) | 2023-05-05 |
Family
ID=78080605
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180007428.7A Pending CN116075689A (en) | 2021-09-01 | 2021-09-01 | Hybrid depth map |
Country Status (6)
Country | Link |
---|---|
US (1) | US20230274491A1 (en) |
EP (1) | EP4179275A1 (en) |
JP (1) | JP2023544072A (en) |
KR (1) | KR20230035228A (en) |
CN (1) | CN116075689A (en) |
WO (1) | WO2023033858A1 (en) |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20180050823A (en) * | 2016-11-07 | 2018-05-16 | 삼성전자주식회사 | Generating method and apparatus of 3d lane model |
-
2021
- 2021-09-01 EP EP21787274.6A patent/EP4179275A1/en active Pending
- 2021-09-01 JP JP2022555707A patent/JP2023544072A/en active Pending
- 2021-09-01 KR KR1020227040674A patent/KR20230035228A/en unknown
- 2021-09-01 US US18/001,659 patent/US20230274491A1/en active Pending
- 2021-09-01 CN CN202180007428.7A patent/CN116075689A/en active Pending
- 2021-09-01 WO PCT/US2021/071339 patent/WO2023033858A1/en unknown
Also Published As
Publication number | Publication date |
---|---|
KR20230035228A (en) | 2023-03-13 |
JP2023544072A (en) | 2023-10-20 |
WO2023033858A1 (en) | 2023-03-09 |
US20230274491A1 (en) | 2023-08-31 |
EP4179275A1 (en) | 2023-05-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11651561B2 (en) | Real-time shared augmented reality experience | |
US11170741B2 (en) | Method and apparatus for rendering items in a user interface | |
US20210272372A1 (en) | Augmented and virtual reality | |
JP6643357B2 (en) | Full spherical capture method | |
US11288857B2 (en) | Neural rerendering from 3D models | |
CN106133795B (en) | Method and apparatus for visualizing geo-located media content in 3D rendering applications | |
US9514717B2 (en) | Method and apparatus for rendering items in a user interface | |
US9558559B2 (en) | Method and apparatus for determining camera location information and/or camera pose information according to a global coordinate system | |
KR20190108181A (en) | Spherical video editing | |
US20160133230A1 (en) | Real-time shared augmented reality experience | |
EP3832605B1 (en) | Method and device for determining potentially visible set, apparatus, and storage medium | |
CN111771180A (en) | Hybrid placement of objects in augmented reality environment | |
GB2558027A (en) | Quadrangulated layered depth images | |
CN116075689A (en) | Hybrid depth map | |
KR20230007501A (en) | Method and apparatus for combining augmented reality objects with real world images | |
JP7400810B2 (en) | Information processing device, information processing method, and recording medium | |
US11600022B2 (en) | Motion capture calibration using drones | |
EP3923121A1 (en) | Object recognition method and system in augmented reality enviroments | |
KR20240067675A (en) | Method and system for generating 3d street view model using 3d building model and road model | |
WO2022045899A1 (en) | Motion capture calibration using cameras and drones |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |