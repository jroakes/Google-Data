CN117795597A - Joint acoustic echo cancellation, speech enhancement and voice separation for automatic speech recognition - Google Patents
Joint acoustic echo cancellation, speech enhancement and voice separation for automatic speech recognition Download PDFInfo
- Publication number
- CN117795597A CN117795597A CN202180101523.3A CN202180101523A CN117795597A CN 117795597 A CN117795597 A CN 117795597A CN 202180101523 A CN202180101523 A CN 202180101523A CN 117795597 A CN117795597 A CN 117795597A
- Authority
- CN
- China
- Prior art keywords
- encoder
- speech
- asr
- context
- input
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000926 separation method Methods 0.000 title abstract description 13
- 238000012545 processing Methods 0.000 claims abstract description 113
- 238000000034 method Methods 0.000 claims abstract description 52
- 230000005236 sound signal Effects 0.000 claims abstract description 38
- 238000003780 insertion Methods 0.000 claims abstract description 22
- 230000037431 insertion Effects 0.000 claims abstract description 22
- 238000012549 training Methods 0.000 claims description 32
- 230000008569 process Effects 0.000 claims description 20
- 230000003595 spectral effect Effects 0.000 claims description 10
- 238000001514 detection method Methods 0.000 claims description 9
- 238000013528 artificial neural network Methods 0.000 claims description 7
- 230000003993 interaction Effects 0.000 claims description 5
- 230000015654 memory Effects 0.000 description 42
- 238000004891 communication Methods 0.000 description 9
- 238000004590 computer program Methods 0.000 description 8
- 238000010586 diagram Methods 0.000 description 8
- 239000010410 layer Substances 0.000 description 8
- 230000006870 function Effects 0.000 description 7
- 230000004044 response Effects 0.000 description 6
- 238000011156 evaluation Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000002955 isolation Methods 0.000 description 3
- 230000001629 suppression Effects 0.000 description 3
- 238000013518 transcription Methods 0.000 description 3
- 230000035897 transcription Effects 0.000 description 3
- 230000002411 adverse Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 230000002708 enhancing effect Effects 0.000 description 2
- 239000000284 extract Substances 0.000 description 2
- 230000001771 impaired effect Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- PXFBZOLANLWPMH-UHFFFAOYSA-N 16-Epiaffinine Natural products C1C(C2=CC=CC=C2N2)=C2C(=O)CC2C(=CC)CN(C)C1C2CO PXFBZOLANLWPMH-UHFFFAOYSA-N 0.000 description 1
- 230000005534 acoustic noise Effects 0.000 description 1
- 230000003044 adaptive effect Effects 0.000 description 1
- 230000001364 causal effect Effects 0.000 description 1
- 230000006735 deficit Effects 0.000 description 1
- 238000002592 echocardiography Methods 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 239000002243 precursor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 239000002356 single layer Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000001228 spectrum Methods 0.000 description 1
- 230000002194 synthesizing effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
- G10L21/0216—Noise filtering characterised by the method used for estimating noise
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R3/00—Circuits for transducers, loudspeakers or microphones
- H04R3/04—Circuits for transducers, loudspeakers or microphones for correcting frequency response
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
- G10L2021/02082—Noise filtering the noise being echo, reverberation of the speech
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
- G10L2021/02087—Noise filtering the noise being separate speech, e.g. cocktail party
Abstract
A method (600) for automatic speech recognition using joint acoustic echo cancellation, speech enhancement and speech separation, comprising: at a context front-end processing model (200), input speech features (212) corresponding to a target utterance (12) are received. The method further includes receiving, at the context front-end processing model, at least one of: a reference audio signal (154), a contextual noise signal (213) comprising noise preceding the target utterance, or a speaker embedding (215) comprising voice characteristics of a target speaker (10) speaking the target utterance. The method further includes processing at least one of the reference audio signal, the contextual noise signal, or the speaker insertion vector and the input speech features using the contextual front-end processing model to generate enhanced speech features (250).
Description
Technical Field
The present disclosure relates to joint acoustic echo cancellation, speech enhancement, and voice separation for automatic speech recognition.
Disclosure of Invention
One aspect of the present disclosure provides a computer-implemented method for automatic speech recognition using joint acoustic echo cancellation, speech enhancement, and voice separation. The computer-implemented method, when executed on the data processing hardware, causes the data processing hardware to perform operations including receiving, at a context front-end (contextual frontend) processing model, input speech features corresponding to a target utterance. The operations further include receiving at the context front-end processing model at least one of: reference audio signal, contextual noise signal comprising noise preceding the target utterance, or speaker embedding comprising voice characteristics of a target speaker speaking the target utterance. The operations also include processing the input speech feature and at least one of the reference audio signal, the contextual noise signal, or the speaker insertion vector using the contextual front-end processing model to generate an enhanced speech feature.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, the context front-end processing model includes a confromer neural network architecture that combines convolution and self-attention to model short-range and long-range interactions. In some examples, processing the input speech features and at least one of the reference audio signal, the contextual noise signal, or the embedded vector includes: processing the input speech features using a primary encoder to generate a primary input code; and processing the context noise signal using a noise context encoder to generate a context noise code. These examples also include: processing the primary input code and the context noise code using a cross-attention encoder to generate a cross-attention insert; and decoding the cross-attention insert into enhanced speech features corresponding to the target utterance. In these examples, processing the input speech features to generate the primary input encoding may further include processing the input speech features stacked with reference features corresponding to the reference audio signal to generate the primary input encoding. The input speech feature and the reference feature may each comprise a respective sequence of logarithmic mel-filter bank energy (LFBE) features.
In these examples, processing the input features to generate the primary input code may include combining the input speech features with a speaker insertion vector using feature-wise (FiLM) linear modulation to generate the primary input code. Here, processing the primary input code and the contextual noise code to generate cross-attention embedding includes: combining the master input code with the speaker insertion vector using the FiLM to generate a modulated master input code; and processing the modulated primary input code and the contextual noise code to generate a cross-attention insert. Additionally or alternatively, the master encoder may include N modulation precursor blocks; the noise context encoder may include N consumer blocks and be performed in parallel with the primary encoder; and the cross-attention may include M modulated cross-attention Conformer blocks.
In some implementations, the data processing hardware executes a context front-end processing model and resides on the user device. The user device is configured to output a reference audio signal as playback audio via an audio speaker of the user device; and capturing, via one or more microphones of the user device, the target utterance, the reference audio signal, and the contextual noise signal. In some examples, the context front-end processing model is trained jointly with a back-end (background) Automatic Speech Recognition (ASR) model using spectral loss and ASR loss. In these examples, the spectral loss is based on an L1 loss function and an L2 loss function distance between the estimated ratio mask and the ideal ratio mask. Here, the ideal ratio mask is calculated using the reverberated speech and the reverberated noise.
Additionally, in these examples, ASR loss may be calculated by: generating a predicted output of the ASR encoder for the enhanced speech features using an ASR encoder of an ASR model configured to receive as input the enhanced speech features predicted for the training utterance by the context front-end processing model; generating a target output of the ASR encoder for the target speech features using the ASR encoder configured to receive the target speech features for the training utterance as input; and calculating an ASR penalty based on the predicted output of the ASR encoder for the enhanced speech features and the target output of the ASR encoder for the target speech features. In some implementations, the operations further include processing the enhanced speech features corresponding to the target utterance using a back-end speech system. In these implementations, the back-end speech system can include at least one of an Automatic Speech Recognition (ASR) model, a hotword detection model, or an audio or audio-video call application.
Another aspect of the present disclosure provides a context front-end processing model for automatic speech recognition using joint acoustic echo cancellation, speech enhancement, and speech separation, comprising a primary encoder, a noise context encoder, a cross-attention encoder, and a decoder. The primary encoder receives as input speech features corresponding to the target utterance and generates as output a primary input code. The noise context encoder receives as input a context noise signal comprising noise preceding the target utterance and generates as output a context noise code. The cross attention encoder receives as inputs a primary input code generated as an output from the primary encoder and a context noise code generated as an output from the noise context encoder, and generates as an output a cross attention insert. The decoder decodes the cross-attention insert into enhanced input speech features corresponding to the target utterance.
This aspect may include one or more of the following optional features. In some examples, the primary encoder is further configured to receive as input reference features corresponding to the reference audio signal and to generate as output a primary input code by processing the input speech features stacked with the reference features. The input speech feature and the reference feature may each comprise a respective sequence of logarithmic mel-filter bank energy (LFBE) features. In some implementations, the primary encoder is further configured to receive as input a speaker insert including voice characteristics of a target speaker speaking the target utterance, and combine the input speech features with the speaker insert by using linear modulation of feature aspects (FiLM) to generate a primary input code as output.
In some examples, the cross-attention encoder is further configured to receive as input a primary input code modulated by a speaker insertion using linear modulation (FiLM) of feature aspects, the speaker insertion including speech characteristics of a target speaker speaking the target utterance, and process the primary input code modulated by the speaker insertion and the contextual noise code to generate the cross-attention insertion as output. In some implementations, the master encoder includes N modulation shaper blocks, and the context noise encoder includes N shaper blocks and is performed in parallel with the master encoder; and the cross-attention encoder includes M modulated cross-attention Conformer blocks. In some examples, the context front-end processing model executes on data processing hardware residing on the user device. Here, the user device is configured to output the reference audio signal as playback audio via an audio speaker of the user device, and to capture the target utterance, the reference audio signal, and the contextual noise signal via one or more microphones of the user device.
In some implementations, the context front-end processing model is trained in conjunction with a back-end Automatic Speech Recognition (ASR) model using spectral loss and ASR loss. In these embodiments, the spectral loss may be based on an L1 loss function and an L2 loss function distance between the estimated ratio mask and the ideal ratio mask. Here, the ideal ratio mask is calculated using the reverberated speech and the reverberated noise. Additionally, in these embodiments, ASR loss is calculated by: receiving as input enhanced speech features predicted by the context front-end processing model for the training utterance, the ASR encoder generating a target output of the ASR encoder for the target speech features using an ASR encoder configured to receive as input target speech features for the training utterance; and calculating an ASR penalty based on the predicted output of the ASR encoder for the enhanced speech features and the target output of the ASR encoder for the target speech features. In some examples, the back-end speech system is configured to process enhanced input speech features corresponding to the target utterance. In these implementations, the back-end speech system can include at least one of an Automatic Speech Recognition (ASR) model, a hotword detection model, or an audio or audio-video call application.
The details of one or more embodiments of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
Background
With the advent of neural network-based end-to-end models, large-scale training data, and improved strategies for enhancing training data, the robustness of Automatic Speech Recognition (ASR) systems has improved significantly over the years. However, various conditions such as echo, more severe background noise, and competing (competing) speech significantly degrade the performance of ASR systems. While separate ASR models may be trained to handle these conditions, it is impractical to maintain multiple task/condition-specific ASR models and to switch between models instantaneously during use.
Drawings
FIG. 1 is a schematic diagram of an example speech environment including a user delivering a spoken target utterance to a speech-enabled user device.
FIG. 2 is a schematic diagram of the context front-end processing model of FIG. 1.
Fig. 3 is a schematic diagram of a modulation shaper block.
Fig. 4 is a schematic diagram of a modulation shaper block architecture implemented by a cross-attention encoder of a context front-end processing model.
FIG. 5 is a schematic diagram of an example training process for jointly training a context front-end processing model and an automatic speech recognition model.
FIG. 6 is an example flow diagram of an example arrangement of the operation of a method for automatic speech recognition using a context front-end processing model.
FIG. 7 is a schematic diagram of an example computing device that may be used to implement the systems and methods described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
With the advent of neural network-based end-to-end models, large-scale training data, and improved strategies for enhancing training data, the robustness of Automatic Speech Recognition (ASR) systems has improved significantly over the years. However, background interference can significantly degrade the ability of an ASR system to accurately recognize speech directed to the ASR system. Background interference can be broadly divided into three groups: echo of equipment; background noise; and competing for the speech. While separate ASR models can be trained to handle each of these sets of background interference in isolation, it is impractical to maintain multiple task/condition-specific ASR models and to switch between models instantaneously during use.
The device echo may correspond to playback audio output from a device (such as a smart home speaker), whereby the playback audio is recorded as an echo and can affect the performance of a back-end speech system (such as an ASR system). In particular, if the playback audio contains audible speech (e.g., text-to-speech (TTS) responses from a digital assistant), the performance impairment of the back-end speech system is particularly severe. This problem is typically solved via Acoustic Echo Cancellation (AEC) techniques. A unique feature of AECs is that the reference signal corresponding to playback audio is generally available and can be used for suppression.
Data enhancement strategies like multi-lattice training (MTR) of ASR models are typically used to handle well background noise with non-speech characteristics. Here, the room simulator is used to add noise to the training data and then carefully weight with the clean data during training to achieve a good performance balance between clean and noisy conditions. Thus, large-scale ASR models are robust to moderate levels of non-speech noise. However, background noise can still affect the performance of the back-end speech system in the presence of low signal-to-noise ratio (SNR) conditions.
In contrast to non-speech background noise, competing for speech is quite challenging for ASR models trained to recognize individual speakers. Training an ASR model using multi-speaker speech can itself be problematic because it is difficult to tell (disambiguate) which speaker is concerned during inference. The use of a model that recognizes multiple speakers is also suboptimal because it is difficult to know in advance how many users to support. Furthermore, such multi-speaker models typically have impaired performance in the case of a single speaker, which is undesirable.
Three types of background interference are typically addressed in isolation from each other, with each type using a separate modeling strategy. Speech separation has received a lot of attention in recent literature using techniques such as deep clustering, permutation invariance training, and using speaker embedding. When speaker embedding is used, it is assumed that the target speaker of interest is known a priori. Techniques developed for speaker separation have also been applied to remove non-speech noise, with modifications to the training data. AECs were also studied in isolation or together in the presence of background noise. It is well known that improving speech quality does not always improve ASR performance, as distortion introduced by non-linear processing may adversely affect ASR performance. One way to mitigate this is to co-train the enhanced front-end with the back-end ASR model.
Furthermore, as the application of large-scale multi-domain and multi-language ASR models continues to gain attention, the training data of these ASR models typically cover various acoustic and language usage scenarios (e.g., voice search and video captioning), making it challenging to address more severe noise conditions at the same time. Thus, it is often convenient to train and maintain a separate front-end feature processing model that can handle adverse conditions without having to combine it with the back-end ASR model.
Embodiments herein are directed to a context front-end processing model for improving the robustness of ASR by jointly implementing Acoustic Echo Cancellation (AEC), speech enhancement, and speech separation modules as a single model. A single joint model is realistic from the following standpoint: it is difficult, if not impossible, to know in advance which type of background interference to address, especially in the case of streaming ASR. In particular, the context front-end processing model includes a Context Enhanced Neural Network (CENN) that can optionally utilize the following three different types of side (side) context inputs: a reference signal associated with playback audio; noise context; and speaker embedding that represents voice characteristics of the target speaker of interest. As will become apparent, the reference signal associated with playback audio is necessary to provide echo cancellation, while the noise context is useful for speech enhancement. Additionally, speaker embedding (when available) that represents the voice characteristics of the target speaker is not only critical for speech separation, but also helps for echo cancellation and speech enhancement. For speech enhancement and separation, the noise context (i.e., the audio a few seconds before the target utterance to be recognized) carries useful information about the acoustic context. CENN employs a respective neural network architecture configured to ingest each corresponding contextual edge input to produce enhanced input speech features that can be passed to a back-end speech system, such as an ASR model that can process the enhanced input speech features to generate speech recognition results for a target utterance. Notably, since the noise context and reference features are optional context side inputs, the noise context and reference features are assumed by the CENN to be corresponding non-informative silence signals when not available.
Referring to fig. 1, in some implementations, a speech environment 100 includes a user 10 transmitting a spoken target utterance 12 to a speech-enabled user device 110 (also referred to as device 110 or user device 110). User 10 (i.e., the speaker of utterance 12) may speak target utterance 12 as a query or command requesting a response from device 110. The device 110 is configured to capture sound from one or more users 10, 11 within the speech environment 100. Here, audio sounds may refer to spoken utterances 12 of user 10 that act as audible queries, commands to device 110, or audible communications captured by device 110. The voice-enabled system of the device 110 or associated with the device 110 may respond to a query or command by answering the query and/or causing the command to be executed.
Various types of background interference may interfere with the ability of the back-end speech system 180 to process target utterances 12 that specify queries or commands for the device 110. As described above, the background interference may include device echoes corresponding to playback audio 154 output from the user device (e.g., smart speaker) 110, competing speech 13, such as utterances 13 different from the target utterance 12 that are not spoken by one or more other users 111 directed to the user device 110, and background noise having non-speech characteristics. Embodiments herein employ a context front-end processing model 200 that executes on the user device 110 and is configured to receive as inputs input speech features corresponding to the target utterance 12 and one or more context signals 213, 214, 215, and to generate as output enhanced input speech features 250 corresponding to the target utterance 12 by processing the input speech features 212 and the one or more contexts 213, 214, 215. The back-end speech system 180 may process the enhanced speech features 250 to generate the output 182. Notably, when user 10 speaks target utterance 12, contextual front-end processing model 200 effectively removes the presence of background interference recorded by device 110 such that enhanced speech features 250 provided to back-end speech system 180 convey speech intended for device 110 (i.e., target utterance 12) such that back-end speech system 180 is not impaired by the background interference.
In the illustrated example, the back-end speech system 180 includes an ASR system that employs an ASR model to process the enhanced input speech features 250 to generate speech recognition results (e.g., transcription) of the target utterance 12. The ASR system may also include a Natural Language Understanding (NLU) module that performs semantic interpretation of the transcription of the target utterance 12 to identify a query/command directed to the user device 110. As such, the output 180 from the back-end speech system 180 may include transcription and/or instructions to fulfill the queries/commands identified by the NLU module.
The back-end speech system 180 may additionally or alternatively include a hotword detection model configured to detect whether the enhanced input speech feature 250 includes the presence of one or more hotwords/hotwords, the hotword detection model being trained to detect the one or more hotwords/hotwords. For example, the hotword detection model may output a hotword detection score that indicates a likelihood that enhanced input speech features 250 corresponding to target utterance 12 include a particular hotword/warm word. The detection of a hotword may trigger a wake-up process that wakes up the device 110 from a sleep state. For example, the device 110 may wake up and process the hotword and/or one or more items before/after the hotword.
In additional examples, the background speech system 180 includes an audio or audio-video call application (e.g., a video conferencing application). Here, enhanced input speech features 250 corresponding to target utterance 12 are used by an audio or audio-video call application to filter the voice of target speaker 10 for communication with a recipient during an audio or audio-video communication session. Background speech system 180 may additionally or alternatively include a speaker identification model configured to perform speaker identification using enhanced input speech features 250 to identify user 10 speaking target utterance 12.
In the illustrated example, user device 110 captures a noisy audio signal 202 (also referred to as audio data) of target utterance 12 spoken by user 10 in the presence of background interference emanating from one or more sources other than user 10. Device 110 may correspond to any computing device associated with user 10 and capable of receiving noisy audio signal 202. Some examples of user devices 110 include, but are not limited to, mobile devices (e.g., mobile phones, tablet computers, laptops, etc.), computers, wearable devices (e.g., smartwatches), smart appliances and internet of things (IoT) devices, smart speakers, etc. The device 110 includes data processing hardware 112 and memory hardware 114, the memory hardware 114 being in communication with the data processing hardware 112 and storing instructions that, when executed by the data processing hardware 112, cause the data processing hardware 112 to perform one or more operations. The context front-end processing model 200 may execute on the data processing hardware 112. In some examples, the back-end speech system 180 executes on the data processing hardware 112.
In some examples, the device 110 includes one or more applications (i.e., software applications), where each application can utilize the enhanced input speech features 250 generated by the context front-end processing model 200 to perform various functions within the application. For example, the device 110 includes an assistant application configured to communicate the synthesized playback audio 154 to the user 10 to assist the user 10 in various tasks.
The device 110 also includes an audio subsystem having an audio capture device (e.g., microphone) 116 for capturing and converting spoken utterances 12 within the speech environment 100 into electrical signals and a speech output device (e.g., audio speaker) 118 for delivering audible audio signals (e.g., synthesized playback signal 154 from the device 110). Although device 110 implements a single audio capture device 116 in the illustrated example, device 110 may implement an array of audio capture devices 116 without departing from the scope of the disclosure, whereby one or more audio capture devices 116 in the array may not physically reside on device 110, but communicate with an audio subsystem (e.g., a peripheral device of device 110). For example, device 110 may correspond to a vehicle infotainment system that utilizes an array of microphones positioned throughout the vehicle.
In some examples, device 110 is configured to communicate with remote system 130 via a network (not shown). The remote system 130 may include remote resources 132, such as remote data processing hardware 134 (e.g., a remote server or CPU) and/or remote memory hardware 136 (e.g., a remote database or other storage hardware). Device 110 may utilize remote resource 132 to perform various functionalities related to speech processing and/or synthetic playback communications. The contextual front-end processing model 200 and the back-end speech system 180 may reside on the device 110 (referred to as a system-on-device) or remotely (e.g., on the remote system 130), but in communication with the device 110. In some examples, one or more back-end voice systems 180 reside locally or on the device, while one or more other back-end voice systems 180 reside remotely. In other words, one or more back-end speech systems 180 utilizing the enhanced input speech features 250 output from the contextual front-end processing model 200 may be local or remote in any combination. For example, when the system 180 is substantial in size or processing requirements, the system 180 may reside in the remote system 130. However, when the device 110 may support the size or processing requirements of one or more systems 180, one or more systems 180 may reside on the device 110 using the data processing hardware 112 and/or the memory hardware 114. Alternatively, one or more systems 180 may reside both locally/on the device and remotely. For example, the back-end voice system 180 may execute on the remote system 130 by default when a connection between the device 110 and the remote system 130 is available, but the system 180 executes locally on the device 110 instead when the connection is lost or unavailable.
In some implementations, the device 110 or a system associated with the device 110 recognizes text that the device 110 will transmit to the user 10 as a response to a query spoken by the user 10. Device 110 may then use a text-to-speech (TTS) system to convert the text into corresponding synthesized playback audio 154 for communication by device 110 to user 10 as a response to the query (e.g., audibly to user 10). Once generated, the TTS system transmits the synthesized playback audio 154 to the device 110 to allow the device 110 to output the synthesized playback audio 154. For example, in response to the user 10 providing a verbal query for a weather forecast today, the device 110 outputs a synthesized playback audio 154 of "today is sunny" at the speaker 118 of the device 110.
With continued reference to fig. 1, as the device 110 outputs the synthesized playback audio 154, the synthesized playback audio 154 generates an echo 156 that is captured by the audio capture device 116. The synthesized playback audio 154 corresponds to a reference audio signal. Although the synthesized playback audio 154 depicts the reference audio signal in the example of fig. 1, the reference audio signal may include other types of playback audio 154 including media content output from the speaker 118 or communications from a remote user with whom the user 10 is talking through the device 110 (e.g., a voice over IP call or a video conference call). Unfortunately, in addition to the echo 156, the audio capture device 116 may also simultaneously capture a target utterance 12 spoken by the user 10, the target utterance 12 being spoken by the statement "what about tomorrow? (tomorrow. For example, fig. 1 depicts that when the device 110 outputs the synthesized playback audio 154, the user 10 is presented with the statement "what about tomorrow? By way of "(more weather related content is queried in the spoken utterance 12 to the device 110). Here, both the spoken utterance 12 and the echo 156 are captured at the audio capture device 116 simultaneously to form a noisy audio signal 202. In other words, the audio signal 202 comprises an overlapping audio signal in which portions of the target utterance 12 spoken by the user 10 overlap with portions of the reference audio signal (e.g., synthesized playback audio) 154 output from the speaker 118 of the device 110. In addition to synthesizing the playback audio 154, the competing speech 13 spoken by another user 11 in the environment may also be captured by the audio capture device 116 and contribute to the background interference that overlaps with the target utterance 12.
In fig. 1, back-end speech system 180 may have processing and follow-up weather query "what about tomorrow" in noisy audio signal 202 due to the presence of background interference due to at least one of playback audio 154, competing speech 13, or non-speech background noise interfering with target utterance 12? "problem of the corresponding target utterance 12". The context front-end processing model 200 is used to improve the robustness of the back-end speech system 180 by implementing Acoustic Echo Cancellation (AEC), speech enhancement, and speech separation models/modules jointly into a single model.
To perform Acoustic Echo Cancellation (AEC), the single model 200 uses the reference signal 154 being played back by the device as an input to the model 200. It is assumed that reference signals 154 are aligned in time with target utterance 12 and have the same length. In some examples, a feature extractor (not shown) extracts reference features 214 corresponding to the reference audio signal 154. The reference features 214 may include logarithmic mel-filter bank energy (LFBE) features of the reference audio signal 154. Similarly, feature extractor may extract speech input features 212 corresponding to target utterance 12. Voice input features 212 may include LFBE features. As described in more detail below, the speech input features 212 may be stacked with the reference features 214 and provided as input to the main encoder 210 (fig. 2) of the single model 200 to perform AEC. When the device is not playing the reference audio signal 154, an all-zero reference signal may be used such that only the speech input feature 212 is received as input to the primary encoder 210.
The single model 200 may additionally perform speech enhancement in parallel with AEC by applying noise context modeling, wherein the single model 200 processes the contextual noise signal 213 associated with noise segments of a predetermined duration captured by the audio capture device 116 prior to the target utterance 12 spoken by the user 10. In some examples, the predetermined duration includes a noise segment of six (6) seconds. Thus, the contextual noise signal 213 provides a noise context. In some examples, the contextual noise signal 213 includes LFBE features of the noise contextual signal that are used as contextual information.
Alternatively, a single model 200 may additionally perform targeted speaker modeling for speech separation in conjunction with AEC and speech enhancement. Here, speaker embedment 215 is received as input by the single model 200. Speaker embedding 215 may include voice characteristics of target speaker 10 speaking target utterance 12. Speaker embedding 215 may include a d-vector. In some examples, the speaker embedding 215 is calculated using a text-independent speaker recognition (TI-SID) model trained with generalized end-to-end expansion set softmax loss. The TI-SID may include three Long Short Term Memory (LSTM) layers having 768 nodes and a projected size of 256. The output of the final frame of the last LSTM layer is then linearly transformed into a final 256-dimensional d vector.
For training and evaluation, each target utterance may be paired with a separate "registration" utterance from the same speaker. The registered utterance may be randomly selected from a pool of available utterances of the target speaker. The d-vector is then calculated over the registration utterance. For most practical applications, the registration utterance is typically obtained via a separate offline process.
Fig. 2 illustrates the context front-end processing model 200 of fig. 1. The context front-end processing model 200 uses a modified version of the former neural network architecture that combines convolution and self-attention to model short-range and long-range interactions. Model 200 includes a main encoder 210, a noise context encoder 220, a cross-attention encoder 400, and a decoder 240. The primary encoder 210 may include N modulation consumer blocks. The noise context encoder 220 may include N Conformer blocks. The cross-attention encoder 230 may include M modulated cross-attention Conformer blocks. The main context Wen Bianma encoder 210 and the noise context encoder 220 may be executed in parallel. As used herein, each con block may use local, causal self-attention to allow streaming capability.
The primary encoder 210 may be configured to receive as input the input speech features 212 corresponding to the target utterance and to generate as output a primary input code 218. When the reference audio signal 154 is available, the primary encoder 210 is configured to receive as input the input speech features 212 of the stack of reference features 214 corresponding to the reference audio signal and to generate a primary input code by processing the input speech features 212 of the stack of reference features 214. The input speech feature and the reference feature may each comprise a respective sequence of LFBE features.
The primary encoder 210 may also be configured to receive as input a speaker insert 215 that includes the voice characteristics of the target speaker 10 speaking the target utterance 12 (i.e., when available), and to generate as output a primary input code by combining the input speech features 212 (or input speech features stacked with the reference features 214) using linear modulation (palm) combinations of feature aspects. Fig. 3 provides an example modulation shaper block 300 employed by the primary encoder 210. Here, before each consumer block at the primary encoder 210, a speaker insert 215 (e.g., d-vector) is combined with the input speech features 212 (or a stack of input speech and reference features 214) using a FiLM layer. The FiLM allows the master encoder 210 to adjust its encoding based on the speaker embedding 215 of the targeted speaker 10. A residual connection is added after the FiLM layer to ensure that the architecture performs well when no speaker embedding is present. Mathematically, the modulation complex block 300 transforms the input features x using the modulation features m to produce output features y as follows:
x″＝x′+Conv(x′)
x″′＝x″+MHSA(x″)
here, h (·) and r (·) are affine transformations. FFN, conv and MHSA represent a feed forward module, a convolution module and a multi-headed self-attention module, respectively. Equation 1 shows a linear modulation (FiLM) layer with the characteristic aspect of the residual connection.
Referring back to fig. 2, the noise context encoder 220 is configured to receive as input a context noise signal 213 comprising noise preceding the target utterance and to generate as output a context noise code 222. The context noise signal 213 may comprise LFBE features of the context noise signal. Unlike the main encoder 210 and the cross-attention encoder 400, the noise context encoder 220 includes standard Conformer blocks that are not modulated by the speaker insertion 215. Noise context encoder 220 does not use speaker embedding 215 to modulate context noise signal 213 because context noise signal 213 is associated with the acoustic noise context prior to speaking target utterance 12 and is therefore assumed to contain information that should be passed forward to cross-attention encoder 400 to assist in noise suppression.
With continued reference to fig. 2, the cross-attention encoder 400 may be configured to receive as inputs the primary input code 218 generated as an output from the primary encoder 210 and the contextual noise code 222 generated as an output from the noise context encoder 220, and to generate as an output a cross-attention insert 480. Thereafter, decoder 240 is configured to decode cross-attention insert 480 into enhanced input speech features 250 corresponding to target utterance 12. The contextual noise code 222 may correspond to an auxiliary input. The decoder may comprise a simple projection decoder with a single layer, a fully connected network with Sigmoid activated frame aspects.
As shown in fig. 4, the cross-attention encoder 400 may employ a respective set of M modulation conster blocks that each receive as input a master input code 218 modulated by the speaker inlay 215 using the palm as described in fig. 3 and the contextual noise codes 222 output from the noise context encoder 220. The cross-attention complex encoder first independently processes the modulation input 218 and the auxiliary input 222 using a half feed forward network and convolution block. The cross-attention block is then used to summarize the auxiliary input using the processed input as a query vector. Intuitively, the role of the cross-attention block is to summarize the noise context separately for each input frame to be enhanced. The summarized assist features are then merged with the input using a FiLM layer, followed by a second cross-attention layer to further process the merged features. Mathematically, if x, m and n are the encoded input from the previous layer, the d vector and the encoded noise context, the cross-attention encoder is as follows:
x″＝x′+MHCA(x′,n′)
x″′＝x′⊙r(x″)+h(x″)
x″″＝x′+MHCA(x′,x″′)
thus, the input of each of the M converger blocks is modulated by both the speaker embedding 215 and the noise context encoding 222 associated with the target speaker.
In some implementations, the contextual front-end processing model is trained jointly with a back-end Automatic Speech Recognition (ASR) model using spectral loss and ASR loss. The training objectives for training the context front-end processing model 200 use an ideal ratio mask. The IRM is calculated using reverberated speech and reverberated noise based on the assumption that speech and noise are uncorrelated in mel-frequency spectrum space as follows.
Here, X and N are reverberant speech and reverberant noise Mel spectrograms, respectively. t and c represent binary indexes of time and mel frequency. We choose to estimate IRM because the target is at 0,1]The inter-definition simplifies the estimation process. Furthermore, the ASR model used for evaluation is trained on real and simulated reverberation data, making it relatively robust to reverberated speech. Thus, IRMs derived using reverberant speech as a target still provide substantial gain in performance. The spectral loss during training is based on the IRM and the estimated IRM,the L1 and L2 losses in between are calculated as follows.
Wherein the method comprises the steps ofAnd->
During the inference, the estimated IRM is scaled and rounded down (floor) to reduce speech distortion at the cost of reduced noise suppression. This is particularly important because ASR models are sensitive to speech distortion and nonlinear front-end processing, which is one of the main challenges in improving the performance of robust ASR models using enhanced front-ends. The enhancement features are derived as follows.
Here, Y is a noisy Mel spectrogram,is an estimate of the clean Mel spectrogram, α and β are the exponent mask scalar and the rounding result under mask (mask floor), and ∈are dot products. In our evaluation, α was set to 0.5, and β was set to 0.01. The enhancement features are logarithmically compressed, i.e. +. >And passed to the ASR model for evaluation.
FIG. 5 illustrates an example training process 500 for calculating ASR loss when using an ASR model joint training context front-end processing model 200. Here, only the encoder of the ASR model is used to calculate the penalty. The penalty is calculated as the l2 distance between the outputs of the ASR encoder 510 for the target feature and the enhanced feature. The ASR encoder 510 is not updated during training. In detail, the training process 500 calculates ASR loss by: generating a prediction output of the ASR encoder 510 for the enhanced speech features using the ASR encoder 510 configured to receive as input an ASR model the enhanced speech features predicted for the training utterance by the context front-end processing model 200; and generating a target output of the ASR encoder 510 for the target speech feature using the ASR encoder 510 configured to receive the target speech feature for the training utterance as an input. The predicted enhanced speech feature and the target speech feature may each comprise a respective sequence of LFBE features. Thereafter, the training process 500 calculates ASR losses based on the predicted output of the ASR encoder 510 for the enhanced speech features and the target output of the ASR encoder 510 for the target speech features via the loss module 520. The goal of using ASR penalty is to make the enhancement more adaptive to the ASR model, which is critical to obtain optimal performance from the enhancement front-end. By keeping the parameters of the ASR models fixed, the ASR models are decoupled from the context front-end processing model 200, allowing each ASR model to be trained and deployed independently of each other.
FIG. 6 includes a flowchart of an example arrangement of operations of a method 600 for performing automatic speech recognition using the context front-end processing model 200. At operation 602, the method 600 includes: input speech features 212 corresponding to the target utterance 12 are received at the context front-end processing model 200. The method 600 further comprises: at operation 604, at least one of the following is received at the context front-end processing model 200: reference audio signal 154, contextual noise signal 213 that includes noise preceding target utterance 12, or speaker embedded vector 215 that includes voice characteristics of target speaker 10 speaking target utterance 12. At operation 606, the method 600 further comprises: using the context front-end processing model 200, at least one of the reference audio signal 154, the context noise signal 213, or the speaker insertion vector 215, and the input speech feature 212 are processed to generate the enhanced speech feature 250.
FIG. 7 is a schematic diagram of an example computing device 700 that may be used to implement the systems and methods described in this document. Computing device 700 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the disclosures described and/or claimed in this document.
Computing device 700 includes a processor 710, memory 720, storage device 730, high-speed interface/controller 740 coupled to memory 720 and high-speed expansion ports 750, and low-speed interface/controller 760 coupled to low-speed bus 770 and storage device 730. Each of the components 710, 720, 730, 740, 750, and 760 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 710 (i.e., the data processing hardware 710, which may include either of the data processing hardware 112, 134) is capable of processing instructions for execution within the computing device 700, including instructions stored in the memory 720 or on the storage device 730, to display graphical information for a Graphical User Interface (GUI) on an external input/output device, such as a display 780 coupled to a high speed interface 740. In other embodiments, multiple processors and/or multiple buses, as well as multiple memories and memory types may be used, as appropriate. In addition, multiple computing devices 700 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a set of blade servers, or a multiprocessor system).
Memory 720 (i.e., memory hardware 720, which may include either of memory hardware 114, 136) non-transitory stores information within computing device 700. Memory 720 may be a computer-readable medium, volatile memory unit(s), or non-volatile memory unit(s). Non-transitory memory 720 may be a physical device for temporarily or permanently storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by computing device 700. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., commonly used for firmware such as a boot strap). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and magnetic disk or tape.
Storage device 730 is capable of providing mass storage for computing device 700. In some implementations, the storage device 730 is a computer-readable medium. In various different implementations, storage device 730 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional embodiments, the computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as memory 720, storage device 730, or memory on processor 710.
The high speed controller 740 manages bandwidth-intensive operations for the computing device 700, while the low speed controller 760 manages lower bandwidth-intensive operations. This allocation of responsibilities is merely exemplary. In some implementations, high-speed controller 740 is coupled to memory 720, display 780 (e.g., via a graphics processor or accelerator), and high-speed expansion port 750, high-speed expansion port 750 may accept various expansion cards (not shown). In some implementations, a low-speed controller 760 is coupled to the storage device 730 and the low-speed expansion port 790. The low-speed expansion port 790, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a network device such as a switch or router, for example, through a network adapter.
Computing device 700 may be implemented in a number of different forms, as shown. For example, it may be implemented as a standard server 700a or multiple times in a group of such servers 700a, as a laptop 700b, or as part of a rack server system 700 c.
Various implementations of the systems and techniques described here can be realized in digital electronic and/or optical circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments can include embodiments in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform tasks. In some examples, a software application may be referred to as an "application," application program (app) "or" program. Example applications include, but are not limited to, system diagnostic applications, system management applications, software maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
The non-transitory memory may be a physical device for temporarily or permanently storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by the computing device. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., commonly used for firmware such as a boot strap). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and magnetic disk or tape.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, non-transitory computer-readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
The processes and logic flows described in this specification can be performed by one or more programmable processors (also referred to as data processing hardware) executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such a device. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen) for displaying information to the user and optionally a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices can also be used to provide interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and can receive input from a user in any form, including acoustic, speech, or tactile input. In addition, the computer is able to interact with the user by sending and receiving documents to and from a device used by the user; for example, by sending a web page to a web browser on a user's client device in response to a request received from the web browser.
Various embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other embodiments are within the scope of the following claims.
Claims (25)
1. A computer-implemented method (600), the computer-implemented method (600), when executed on data processing hardware (710), causing the data processing hardware (710) to perform operations comprising:
at a context front-end processing model (200), input speech features (212) corresponding to a target utterance (12) and at least one of:
a reference audio signal (154);
a contextual noise signal (213), the contextual noise signal (213) comprising noise preceding the target utterance (12); or (b)
A speaker insertion vector (215), the speaker insertion vector (215) comprising voice characteristics of a target speaker (10) speaking the target utterance (12); and processing at least one of the reference audio signal (154), the contextual noise signal (213), or the speaker insertion vector (215), and the input speech feature (212) using the contextual front end processing model (200) to generate an enhanced speech feature (250).
2. The computer-implemented method (600) of claim 1, wherein the contextual front-end processing model (200) includes a confromer neural network architecture that combines convolution and self-attention to model short-range and long-range interactions.
3. The computer-implemented method (600) of claim 1 or 2, wherein processing at least one of the reference audio signal (154), the contextual noise signal (213), or the speaker-embedded vector (215), and the input speech feature (212) comprises:
processing the input speech features (212) using a primary encoder (210) to generate a primary input code (218);
processing the context noise signal (213) using a noise context encoder (220) to generate a context noise code (222);
processing the primary input code (218) and the contextual noise code (222) using a cross-attention encoder (400) to generate a cross-attention insert (480); and
-decoding (480) the cross-attention-embedding into the enhanced speech features (250) corresponding to the target utterance (12).
4. The computer-implemented method (600) of claim 3, wherein processing the input speech feature (212) to generate the primary input code (218) further comprises: the input speech features (212) stacked with reference features (214) corresponding to the reference audio signal (154) are processed to generate the primary input encoding (218).
5. The computer-implemented method (600) of claim 3 or 4, wherein the input speech feature (212) and the reference feature (214) each comprise a respective sequence of logarithmic mel-filter bank energy (LFBE) features.
6. A computer-implemented method (600) according to claim 3, wherein:
processing the input speech features (212) to generate the primary input code (218) includes: -combining the input speech features (212) with the speaker insertion vector (215) using feature-wise linear modulation (palm) to generate the primary input code (218); and
processing the primary input code (218) and the contextual noise code (222) to generate the cross-attention insert (480) includes:
combining the primary input code (218) with the speaker insertion vector (215) using a FiLM to generate a modulated primary input code (218); and
the modulated primary input code (218) and the contextual noise code (222) are processed to generate the cross-attention insert (480).
7. The computer-implemented method (600) of any of claims 3-6, wherein:
the primary encoder (210) includes N modulation consumer blocks;
the noise context encoder (220) comprises N consumer blocks and is executed in parallel with the main encoder (210); and
the cross-attention encoder (400) includes M modulated cross-attention Conformer blocks.
8. The computer-implemented method (600) of any of claims 1-7, wherein the data processing hardware (710) executes the context front-end processing model (200) and resides on a user device (110), the user device (110) configured to:
-outputting the reference audio signal (154) as playback audio via an audio speaker (118) of the user device (110); and
the target utterance (12), the reference audio signal (154), and the contextual noise signal (213) are captured via one or more microphones (116) of the user device (110).
9. The computer-implemented method (600) of any of claims 1-8, wherein the contextual front-end processing model (200) is trained in conjunction with a back-end Automatic Speech Recognition (ASR) model using spectral loss and ASR loss.
10. The computer-implemented method (600) of claim 9, wherein the spectral loss is based on an L1 loss function and an L2 loss function distance between an estimated ratio mask and an ideal ratio mask, the ideal ratio mask being calculated using reverberant speech and reverberant noise.
11. The computer-implemented method (600) of claim 9 or 10, wherein the ASR penalty is calculated by:
generating a prediction output of the ASR encoder (510) for the enhanced speech features (250) using an ASR encoder (510) of the ASR model configured to receive as input enhanced speech features (250) predicted for a training utterance by the context front-end processing model (200);
Generating a target output of the ASR encoder (510) for target speech features of the training utterance using the ASR encoder (510) configured to receive the target speech features as input; and
the ASR penalty is calculated based on the predicted output of the ASR encoder (510) for the enhanced speech features (250) and the target output of the ASR encoder (510) for the target speech features.
12. The computer-implemented method (600) of any of claims 1-11, wherein the operations further comprise: -processing the enhanced speech features (250) corresponding to the target utterance (12) using a back-end speech system (180).
13. The computer-implemented method (600) of claim 12, wherein the back-end speech system (180) comprises at least one of:
an Automatic Speech Recognition (ASR) model;
a hotword detection model; or (b)
Audio or audio-video call applications.
14. A context front-end processing model (200), comprising:
a primary encoder (210), the primary encoder (210) configured to:
receiving as input an input speech feature (212) corresponding to a target utterance (12); and
Generating a primary input code (218) as an output;
a noise context encoder (220), the noise context encoder (220) being configured to:
receiving as input a contextual noise signal (213) comprising noise preceding the target utterance (12); and
generating as output a context noise code (222); and
a cross-attention encoder (400), the cross-attention encoder (400) configured to:
receiving as inputs the primary input code (218) generated as output from the primary encoder (210) and the context noise code (222) generated as output from the noise context encoder (220); and
generating a cross-attention insert (480) as an output; and
-a decoder (240), the decoder (240) being configured to decode the cross-attention embedding (480) into enhanced speech features (250) corresponding to the target utterance (12).
15. The context front-end processing model (200) of claim 14, wherein the master encoder (210) is further configured to:
receiving as input a reference feature (214) corresponding to a reference audio signal (154); and
-generating the primary input code (218) as output by processing the input speech features (212) stacked together with the reference features (214).
16. The context front-end processing model (200) of claim 14 or 15, wherein the input speech features (212) and the reference features (214) each comprise a respective sequence of logarithmic mel-filter bank energy (LFBE) features.
17. The context front-end processing model (200) of any of claims 14 to 16, wherein the master encoder (210) is further configured to:
receiving as input a speaker insertion vector (215), the speaker insertion vector (215) comprising voice characteristics of a target speaker (10) speaking the target utterance (12); and
-generating the primary input code (218) as output by: the input speech features (212) are combined with the speaker insertion vector (215) using linear modulation in Feature (FiLM).
18. The context front-end processing model (200) of any of claims 14 to 17, wherein the cross-attention encoder (400) is further configured to:
receiving as input the primary input code (218) modulated by a speaker insertion vector (215) using linear modulation of feature aspects (palm), the speaker insertion vector (215) including voice characteristics of a target speaker (10) speaking the target utterance (12); and
-processing the primary input code (218) modulated by the speaker insertion vector (215) and the contextual noise code (222) to generate the cross-attention insertion (480) as output.
19. The contextual front end processing model (200) according to any of claims 14 to 18, wherein:
the primary encoder (210) includes N modulation consumer blocks;
the noise context encoder (220) comprises N consumer blocks and is executed in parallel with the main encoder (210); and
the cross-attention encoder (400) includes M modulated cross-attention Conformer blocks.
20. The contextual front end processing model (200) according to any of claims 14 to 19, wherein the contextual front end processing model (200) is executed on data processing hardware (710) residing on a user device (110), the user device (110) being configured to:
outputting a reference audio signal (154) as playback audio via an audio speaker (118) of the user device (110); and
the target utterance (12), the reference audio signal (154), and the contextual noise signal (213) are captured via one or more microphones (116) of the user device (110).
21. The contextual front-end processing model (200) according to any of claims 14 to 20, wherein the contextual front-end processing model (200) is trained jointly with a back-end Automatic Speech Recognition (ASR) model using spectral loss and ASR loss.
22. The context front-end processing model (200) of claim 21, wherein the spectral loss is based on an L1 loss function and an L2 loss function distance between an estimated ratio mask and an ideal ratio mask, the ideal ratio mask being calculated using reverberant speech and reverberant noise.
23. The context front-end processing model (200) of claim 21 or 22, wherein the ASR penalty is calculated by:
generating a prediction output of the ASR encoder (510) for the enhanced speech features (250) using an ASR encoder (510) of the ASR model configured to receive as input enhanced speech features (250) predicted for a training utterance by the context front-end processing model (200);
generating a target output of the ASR encoder (510) for target speech features of the training utterance using the ASR encoder (510) configured to receive the target speech features as input; and
The ASR penalty is calculated based on the predicted output of the ASR encoder (510) for the enhanced speech features (250) and the target output of the ASR encoder (510) for the target speech features.
24. The contextual front-end processing model (200) of any of claims 14 to 23, wherein a back-end speech system (180) is configured to process the enhanced speech features (250) corresponding to the target utterance (12).
25. The contextual front end processing model (200) of claim 24, wherein the back end speech system (180) comprises at least one of:
automatic speech recognition ASR model;
a hotword detection model; or (b)
Audio or audio-video call applications.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163260100P | 2021-08-09 | 2021-08-09 | |
US63/260,100 | 2021-08-09 | ||
PCT/US2021/063196 WO2023018434A1 (en) | 2021-08-09 | 2021-12-14 | Joint acoustic echo cancelation, speech enhancement, and voice separation for automatic speech recognition |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117795597A true CN117795597A (en) | 2024-03-29 |
Family
ID=79425569
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180101523.3A Pending CN117795597A (en) | 2021-08-09 | 2021-12-14 | Joint acoustic echo cancellation, speech enhancement and voice separation for automatic speech recognition |
Country Status (5)
Country | Link |
---|---|
US (1) | US20230038982A1 (en) |
EP (1) | EP4367664A1 (en) |
KR (1) | KR20240033265A (en) |
CN (1) | CN117795597A (en) |
WO (1) | WO2023018434A1 (en) |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2008501991A (en) * | 2004-06-04 | 2008-01-24 | コーニンクレッカ フィリップス エレクトロニクス エヌ ヴィ | Performance prediction for interactive speech recognition systems. |
US10074380B2 (en) * | 2016-08-03 | 2018-09-11 | Apple Inc. | System and method for performing speech enhancement using a deep neural network-based signal |
US10803881B1 (en) * | 2019-03-28 | 2020-10-13 | Samsung Electronics Co., Ltd. | System and method for acoustic echo cancelation using deep multitask recurrent neural networks |
CN111261146B (en) * | 2020-01-16 | 2022-09-09 | 腾讯科技（深圳）有限公司 | Speech recognition and model training method, device and computer readable storage medium |
US11521635B1 (en) * | 2020-12-01 | 2022-12-06 | Amazon Technologies, Inc. | Systems and methods for noise cancellation |
US11562734B2 (en) * | 2021-01-04 | 2023-01-24 | Kwai Inc. | Systems and methods for automatic speech recognition based on graphics processing units |
US11715480B2 (en) * | 2021-03-23 | 2023-08-01 | Qualcomm Incorporated | Context-based speech enhancement |
-
2021
- 2021-12-14 CN CN202180101523.3A patent/CN117795597A/en active Pending
- 2021-12-14 WO PCT/US2021/063196 patent/WO2023018434A1/en active Application Filing
- 2021-12-14 EP EP21841088.4A patent/EP4367664A1/en active Pending
- 2021-12-14 KR KR1020247005088A patent/KR20240033265A/en unknown
- 2021-12-14 US US17/644,108 patent/US20230038982A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2023018434A1 (en) | 2023-02-16 |
EP4367664A1 (en) | 2024-05-15 |
KR20240033265A (en) | 2024-03-12 |
US20230038982A1 (en) | 2023-02-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11823679B2 (en) | Method and system of audio false keyphrase rejection using speaker recognition | |
US10950249B2 (en) | Audio watermark encoding/decoding | |
US10373609B2 (en) | Voice recognition method and apparatus | |
EP3782151B1 (en) | Hotword suppression | |
EP4004906A1 (en) | Per-epoch data augmentation for training acoustic models | |
US10978081B2 (en) | Audio watermark encoding/decoding | |
Pan et al. | USEV: Universal speaker extraction with visual cue | |
US20230298593A1 (en) | Method and apparatus for real-time sound enhancement | |
EP4139816B1 (en) | Voice shortcut detection with speaker verification | |
JP2023162265A (en) | Text echo cancellation | |
US11727926B1 (en) | Systems and methods for noise reduction | |
CN117795597A (en) | Joint acoustic echo cancellation, speech enhancement and voice separation for automatic speech recognition | |
US20230298609A1 (en) | Generalized Automatic Speech Recognition for Joint Acoustic Echo Cancellation, Speech Enhancement, and Voice Separation | |
WO2020068401A1 (en) | Audio watermark encoding/decoding | |
US20230298612A1 (en) | Microphone Array Configuration Invariant, Streaming, Multichannel Neural Enhancement Frontend for Automatic Speech Recognition | |
US20230079828A1 (en) | STFT-Based Echo Muter | |
JP6169526B2 (en) | Specific voice suppression device, specific voice suppression method and program |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |