US20170293671A1 - Post-hoc management of datasets - Google Patents
Post-hoc management of datasets Download PDFInfo
- Publication number
- US20170293671A1 US20170293671A1 US15/480,971 US201715480971A US2017293671A1 US 20170293671 A1 US20170293671 A1 US 20170293671A1 US 201715480971 A US201715480971 A US 201715480971A US 2017293671 A1 US2017293671 A1 US 2017293671A1
- Authority
- US
- United States
- Prior art keywords
- collection
- data sets
- dataset
- data
- metadata
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G06F17/30598—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6218—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/21—Design, administration or maintenance of databases
- G06F16/211—Schema design and management
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/21—Design, administration or maintenance of databases
- G06F16/215—Improving data quality; Data cleansing, e.g. de-duplication, removing invalid entries or correcting typographical errors
-
- G06F17/30292—
Definitions
- This specification relates to managing data sets.
- This specification describes methods and systems, including computer programs encoded on computer storage media, for organizing structured data sets at scale.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of accessing a plurality of extant data sets, the plurality of extant data sets including data sets that are generated independent of each other and structurally dissimilar; organizing the data sets into a plurality of collections, each collection including two or more data sets from the plurality of data sets, each data set in each collection belonging to the collection based on collection data associated with the data set, and each collection corresponding to collection data that is different from the collection data that corresponds to the other collections; for each collection of data sets: determining, from a subset of the data sets that belong to the collection, metadata that describe the data sets that belong to the collection, wherein the metadata does not include the collection data; attributing, to other data sets in the collection that are not included in the subset of data sets in the collection, the metadata determined from the subset of data sets; generating, from the collections of data sets and the metadata determined from the respective subsets of datasets, a catalog for the plurality of datasets, wherein
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of software, firmware, hardware, or any combination thereof installed on the system that in operation may cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- the collection data is a path of a location of a dataset.
- the datasets that belong to a collection include different instances of a particular dataset stored at a location identified by the path.
- the datasets that belong to a collection include datasets that each have at least a sub-path name in its respective path that is common to each path of each dataset included in the collection.
- the metadata include timestamp, file format, owners, and access permissions of the datasets.
- the metadata includes provenance data that describes one or more of dataset production in a workflow, dataset consumption in a workflow, dataset parent dependencies, and dataset child dependencies.
- the metadata include a schema of the dataset.
- the metadata include a content summary that describes the content stored in a dataset.
- the metadata include user-defined annotations.
- the method further comprises receiving one or more keyword search queries; matching the one or more keyword search queries to one or more datasets included in the plurality of datasets in the generated data catalog; ranking the matched one or more datasets using a scoring function, wherein the scoring function ranks each dataset based on one or more of (i) the type of dataset, (ii) the index section of the dataset, (iii) lineage fan-out of the dataset or (iv) a description of the dataset; and providing a predetermined number of highest ranking datasets for output in response to receiving the one or more keyword search queries.
- a data cataloging system implementing post-hoc management of datasets addresses the problem of how to organize structured datasets at scale, in a setting where teams use diverse and often idiosyncratic ways to produce the datasets and where there is no centralized system for storing and querying them.
- the data cataloging system extracts metadata ranging from salient information about each dataset (e.g., owners, timestamps, schema) to relationships among datasets, such as similarity and provenance. It then exposes this metadata through services that allow engineers to find datasets within the company, to monitor datasets, to annotate them in order to enable others to use their datasets, and to analyze relationships between them.
- the attribution of metadata determined from a subset of data sets within a collection of data sets improves data set indexing technologies by reducing the computing resources and time required to index a large set of data sets.
- a dataset cataloging system implementing post-hoc dataset management provides a principled and flexible approach to dataset management, reducing the risk of internal siloing of datasets, thus reducing losses in productivity and opportunities, duplication of work and mishandling of data.
- FIG. 1 is a schematic overview of an example data cataloging system.
- FIG. 2 is a flow diagram of an example process for generating a catalog for one or more data sets.
- FIG. 3 illustrates an example of attributing metadata to an unanalyzed dataset through a collection's representative element.
- This specification describes a post-hoc system for organizing datasets at scale. Specifically, the system collects and aggregates metadata about datasets after the datasets were created, accessed, or updated by various pipelines, without interfering with dataset owners or users. Users are thus able to continue to generate and access datasets using the tools of their choice, and the system works in the background, in a nonintrusive manner, to gather the metadata about datasets and their usage. The system then uses this metadata to power services that enable engineers to organize and find their datasets in a principled manner.
- FIG. 1 is a schematic overview of an example data cataloging system 100 .
- the data cataloging system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.
- the system 100 crawls different storage systems and production infrastructure, e.g., logs from running pipelines, to discover which datasets exist and to gather metadata about each dataset, e.g., owners, times of access, content features, accesses by production pipelines.
- the system aggregates the metadata in a central catalog and correlates the metadata about a specific dataset with information about other datasets.
- the system 100 includes one or more storage systems 102 , additional sources of metadata 104 , dataset catalog 106 , index 108 and dataset organizing tools 110 .
- the one or more storage systems may include a large database storage system 112 , one or more file systems 114 a and 114 b, and a data access services/APIs storage system 118 . Other storage systems may also be used.
- the system 100 accesses the storage systems 102 to collect metadata about data sets stored in the storage systems 102 .
- the system may also infer metadata about data sets by processing other sources of metadata 104 such as logs and information about dataset owners and their projects, by analyzing content of the datasets, and by collecting input from the system users.
- the system stores the collected metadata in dataset catalog 106 .
- the information stored in dataset catalog 106 is used to build dataset organizing tools 110 .
- the dataset organizing tools 110 may include a search tool 120 , a dashboard 122 , a dataset monitoring tool 124 , a provenance visualization module 126 and an annotation tool 128 .
- the dataset organizing tools 110 may be used to search, monitor and visualize the flow of data.
- the resulting catalog uses a Path/Identifier to collect data sets, and then uses metadata that is different from the path/Identifier to describe the data sets.
- the path/identifier is an example of collection data that is data that is used to collect data sets into a collection, and that is also separate from the metadata that is used to describe the data sets.
- FIG. 2 is a flow diagram of an example process 200 for generating a catalog for one or more data sets.
- the process 200 will be described as being performed by a system of one or more computers located in one or more locations.
- a data cataloging system e.g., the data cataloging system 100 of FIG. 1 , appropriately programmed in accordance with this specification, can perform the process 200 .
- the system accesses a plurality of extant data sets (step 202 ).
- the plurality of extant data sets may include billions of data sets.
- the plurality of extant data sets includes data sets that are generated independent of each other and are structurally dissimilar.
- the extant data sets may be stored in many formats, e.g., text files, csv files, and storage systems, e.g., particular file systems or database servers, where each data set may be associated with different types of metadata or access characteristics.
- the relationships between datasets may vary.
- the data cataloging systems may exhibit a high level of churn of catalog entries, since every day production jobs generate new data sets, and old data sets are deleted—either explicitly or because their designated time-to-live has expired.
- the system is able to prioritize and specify which data sets are to be included in the data set catalog. For example, many of the datasets that have a limited Time-To-Live (TTL) from their creation are intermediate results of a large production pipeline that are garbage collected after a several data set accessing iterations (e.g., several days, if a collection iteration is done daily).
- TTL Time-To-Live
- the system organizes the extant data sets into a plurality of collections (step 204 ).
- Each collection in the plurality of collections includes two or more data sets from the plurality of data sets, with each data set in each collection belonging to the collection based on collection data associated with the data set.
- the collection data is a path of a location of a dataset.
- Each collection in the plurality of collections corresponds to collection data that is different from the collection data that corresponds to the other collections.
- Other ways of organizing collections can also be used. For example, data sets that belong to a particular entity, such as a particular user, a particular company, or a particular division within a company may be grouped as a collection.
- the datasets that belong to a collection include different instances of a particular dataset stored at a location identified by the path.
- the datasets that belong to a collection include datasets that each have at least a sub-path name in its respective path that is common to each path of each dataset included in the collection.
- Paths of the datasets give hints on how to cluster or organize the datasets, e.g., via embedded identifiers for timestamps, versions, and so on. As an example, consider a dataset that is produced daily and let “/dataset/2015-10-10/daily_scan” be the path for one of its instances.
- the day portion of the date may be abstracted out to get a generic representation of all datasets produced in a month: “/dataset/2015-10- ⁇ day>/daily_scan,” representing all instances from October 2015.
- the hierarchy may be ascended to create abstract paths that represent all datasets produced in the same year: “/dataset/2015- ⁇ month>- ⁇ day>/daily_scan.”
- the system determines, from a subset of the data sets that belong to the collection, metadata that describe the data sets that belong to the collection (step 206 ).
- the metadata does not include the collection data.
- the metadata is basic metadata and includes timestamps, file formats, owners, and access permissions of the datasets.
- the system may obtain basic metadata by crawling storage systems.
- the metadata includes provenance data that describes one or more of dataset production in a workflow, dataset consumption in a workflow, dataset parent dependencies, and dataset child dependencies.
- Datasets are produced and consumed by code.
- This code may include analysis tools that are used to query datasets, serving infrastructures that provide access to datasets through APIs, or ETL pipelines that transform it into other datasets. Often, a dataset may be understood better through these surrounding pieces of software that produce and use it. Moreover, this information helps in tracking how data flows through the enterprise as well as across boundaries of teams and organizations within the company. Therefore, for each dataset, the system maintains the provenance of how the dataset is produced, how it is consumed, what datasets this dataset depends on, and what other datasets depend on this dataset.
- the metadata include a schema of the dataset.
- Schema may be a core type of metadata that helps understand a data set.
- commonly used data set formats may not be self-describing, and the schema must be inferred.
- records within structured datasets may be encoded as serialized protocol buffers. A difficulty lies in determining which protocol buffer was used to encode records in a given dataset. Protocol buffers may be checked into a central code repository such that the system has a full list of protocol buffers that may be made available to match against datasets that have been crawled. Matching may be performed by scanning a few records from the file, and going through each protocol message definition to determine whether it could conceivably have generated the bytes seen in those records.
- Protocol buffers encode multiple logical types as the same physical type, notably string and nested messages are both encoded as variable-length byte strings. Consequently, the matching procedure may be speculative and can produce multiple candidate protocol buffers. All the candidate protocol buffers, along with heuristic scores for each candidate, may become part of the metadata.
- the metadata include a content summary that describes the content stored in a dataset.
- the system may record frequent tokens that are found by sampling the content.
- the system may analyze some of the data fields to determine if they contain keys for the data, individually or in combination.
- the system may use an algorithm, e.g., a HyperLogLog algorithm, to estimate the cardinality of values in individual fields and combinations of fields. The cardinality may be compared with the number of records to find potential keys.
- the system may also collect fingerprints that have checksums for the individual fields and locality-sensitive hash values for the content.
- the fingerprints may be used to find datasets with content that is similar or identical to the given data set, or columns from other datasets that are similar or identical to columns in the current dataset.
- the checksums may also be used to identify which data fields are populated in the records of the dataset.
- the metadata include user-defined annotations.
- the system may enable data set owners to provide text descriptions of their data sets.
- the system may also collect identifiers for teams that own the data set, a description of the project to which a data set belongs, and the history of the changes to the metadata of the data set.
- the cost of determining metadata that describes data sets may vary depending on the type and size of the data set and the type of metadata.
- the system may therefore determine metadata that describes the data sets using a differential process that identifies which datasets are important to cover and performs metadata inference based on the cost and benefit for having the particular type of metadata.
- the system attributes For each collection of data sets, the system attributes, to other data sets in the collection that are not included in the subset of data sets in the collection, the metadata determined from the subset of data sets (step 208 ).
- the plurality of extant data sets may include billions of data sets. At this scale, gathering metadata for all datasets becomes infeasible—even spending one second per dataset (although it is noted that many datasets may be too large to process in one second), going through a catalog with 26 billion datasets using a thousand parallel machines may still require around 300 days.
- the system may therefore collect metadata only for a few datasets in a collection, or a subset of the data datasets, and attribute the metadata to other data sets in the collection.
- the system then propagates the metadata across the other datasets in the collection. For instance, if the same job generates versions of a dataset daily, these datasets are likely to have the same schema. Thus, the system does not need to infer the schema for each version.
- a user provides a description for a dataset, it usually applies to all members of the collection and not just the one version. When the collections are large, the computational savings that the system obtains by avoiding analysis of each member of the cluster can be significant.
- An example of propagating owner's metadata to an unanalyzed dataset through a collection's representative element is illustrated below in FIG. 3 .
- the system generates, from the collections of data sets and the metadata determined from the respective subsets of datasets, a catalog for the plurality of datasets (step 210 ).
- the catalog includes an entry for each dataset in the plurality of datasets, and each entry describes the collection to which the dataset belongs and the metadata for the dataset.
- the catalog provides a uniform way for users to access and query information about all types of data sets, since the variety and complexity of the different data sets, as described above with respect to step 202 , is hidden from the user.
- the catalog for the plurality of datasets may exclude types of uninteresting data sets, e.g., “marker” files that are content free, and may normalize paths to avoid obvious redundancies, e.g., normalize paths corresponding to different shards of the same data set to a common path and not storing the paths separately in the catalog.
- markers e.g., “marker” files that are content free
- the generated data catalog provides a variety of services enabled by the determined metadata.
- the generated data catalog may allow for the exportation of metadata for a specific dataset in a profile page for the data set.
- a profile page for a data set may accept as input the path of a data set or a data set collection and generate an HTML page from the metadata stored in the catalog.
- This service may provide methods to edit specific parts of the metadata, for example to allow users to either augment or correct the information stored in the catalog.
- a profile page may provide a single place where a user can inspect the information about a data set and understand a context in which the data set can be used in production, making the profile page a natural handle for sharing a data set among users or linking to data set information from other tools.
- the system may receive one or more keyword search queries and match the one or more keyword search queries to one or more datasets included in the plurality of datasets in the generated data catalog.
- the system may rank the matched one or more datasets using a scoring function, wherein the scoring function ranks each dataset based on one or more of (i) the type of dataset, (ii) the index section of the dataset, (iii) lineage fan-out of the dataset or (iv) a description of the dataset, and provide a predetermined number of highest ranking datasets for output in response to receiving the one or more keyword search queries.
- the generate data catalog may allow users to query the catalog and find data sets of interest.
- the generated data catalog may allow for users to find data sets using simple keyword queries.
- the service may be backed by a conventional inverted index for document retrieval, where each dataset becomes a “document” and the indexing tokens for each document are derived from a subset of the dataset's metadata.
- each token can be associated with a specific section of the index, e.g., a token derived from the path of the dataset may be associated with the “path” section of the index.
- the search atom “path:x” may match keyword “x” on dataset paths only, whereas the unqualified atom “x” will match the keyword in any part of a dataset's metadata.
- indexing tokens follows from the type of queries that the index must cover.
- it may be desirable to support partial matches on the dataset path where a user may search for “x/y” to match a dataset with path “a/x/y/b” (but not one with “a/y/x/b”).
- the system may break up the dataset's path along common separators and then associate each resulting token with its position in the path. For example, the path “a/x/y/b” may get mapped to the indexing tokens “a”,“x”, “y”, and “b”, in that sequence.
- the service parses the partial path the same way and matches the query's tokens against consecutive tokens in the index.
- Matching search keywords to datasets may only be one part of the search task.
- An additional part may include deriving a scoring function to rank the matching datasets, so that the top results are relevant for the user's search.
- the scoring function may score data sets and be tuned based on a user's experience. For example, the scoring function may score data sets based on how important the dataset is. In some examples the importance of a dataset may depend on the type of dataset. As another example, the importance of a keyword match may depend on the index section of the dataset. For instance, a keyword match on a path of the dataset may be more important that a match on jobs that read or write the dataset. As a further example, lineage fan-out may be an indicator of dataset importance.
- the dataset is importance.
- a dataset that carries an owner sourced description may be important. If a keyword match occurs in a description for a dataset, e.g., provided by the dataset owner, the dataset may be weighted higher.
- FIG. 3 illustrates an example 300 of attributing metadata to an unanalyzed dataset through a collection's representative element.
- owner's metadata is propagated to an unanalyzed data set through a collection's representative element.
- the schema 304 “prod.model” is known for several members of the collection 302 , and is propagated 306 for the collection 302 as a whole.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory program carrier for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. The computer storage medium is not, however, a propagated signal.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- an “engine,” or “software engine,” refers to a software implemented input/output system that provides an output that is different from the input.
- An engine can be an encoded block of functionality, such as a library, a platform, a software development kit (“SDK”), or an object.
- SDK software development kit
- Each engine can be implemented on any appropriate type of computing device, e.g., servers, mobile phones, tablet computers, notebook computers, music players, e-book readers, laptop or desktop computers, PDAs, smart phones, or other stationary or portable devices, that includes one or more processors and computer readable media. Additionally, two or more of the engines may be implemented on the same computing device, or on different computing devices.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- special purpose logic circuitry e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g. an HTML page, to a user device, e.g. for purposes of displaying data to and receiving user input from a user interacting with the user device, which acts as a client.
- Data generated at the user device e.g. as a result of the user interaction, can be received from the user device at the server.
Abstract
Description
- This application claims priority to U.S. Provisional Application Ser. No. 62/319,914, filed on Apr. 8, 2016, the entire contents of which are hereby incorporated by reference.
- This specification relates to managing data sets.
- Enterprises increasingly rely on structured datasets to run their businesses. These datasets take a variety of forms, such as structured files, databases, spreadsheets, or even services that provide access to the data. The datasets often reside in different storage systems, may vary in their formats, may change every day.
- Most large enterprises today witness an explosion in the number of datasets that they generate internally for use in ongoing research and development. The reason behind this explosion is simple: by allowing engineers and data scientists to consume and generate datasets in an unfettered manner, enterprises promote fast development cycles, experimentation, and ultimately innovation that drives their competitive edge. As a result, these internally generated datasets often become a prime asset of the company, on par with source code and internal infrastructure. However, while enterprises have developed a strong culture on how to manage the latter, with source-code development tools and methodologies that are now considered “standard” in the industry (e.g., code versioning and indexing, reviews, or testing), similar approaches do not generally exist for managing datasets.
- This specification describes methods and systems, including computer programs encoded on computer storage media, for organizing structured data sets at scale.
- In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of accessing a plurality of extant data sets, the plurality of extant data sets including data sets that are generated independent of each other and structurally dissimilar; organizing the data sets into a plurality of collections, each collection including two or more data sets from the plurality of data sets, each data set in each collection belonging to the collection based on collection data associated with the data set, and each collection corresponding to collection data that is different from the collection data that corresponds to the other collections; for each collection of data sets: determining, from a subset of the data sets that belong to the collection, metadata that describe the data sets that belong to the collection, wherein the metadata does not include the collection data; attributing, to other data sets in the collection that are not included in the subset of data sets in the collection, the metadata determined from the subset of data sets; generating, from the collections of data sets and the metadata determined from the respective subsets of datasets, a catalog for the plurality of datasets, wherein the catalog includes an entry for each dataset in the plurality of datasets, and each entry describes the collection to which the dataset belongs and the metadata for the dataset.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods. A system of one or more computers can be configured to perform particular operations or actions by virtue of software, firmware, hardware, or any combination thereof installed on the system that in operation may cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- The foregoing and other embodiments can each optionally include one or more of the following features, alone or in combination. In some implementations the collection data is a path of a location of a dataset.
- In some implementations the datasets that belong to a collection include different instances of a particular dataset stored at a location identified by the path.
- In other implementations the datasets that belong to a collection include datasets that each have at least a sub-path name in its respective path that is common to each path of each dataset included in the collection.
- In some cases the metadata include timestamp, file format, owners, and access permissions of the datasets.
- In other cases the metadata includes provenance data that describes one or more of dataset production in a workflow, dataset consumption in a workflow, dataset parent dependencies, and dataset child dependencies.
- In some implementations the metadata include a schema of the dataset.
- In other implementations the metadata include a content summary that describes the content stored in a dataset.
- In some cases the metadata include user-defined annotations.
- In some implementations the method further comprises receiving one or more keyword search queries; matching the one or more keyword search queries to one or more datasets included in the plurality of datasets in the generated data catalog; ranking the matched one or more datasets using a scoring function, wherein the scoring function ranks each dataset based on one or more of (i) the type of dataset, (ii) the index section of the dataset, (iii) lineage fan-out of the dataset or (iv) a description of the dataset; and providing a predetermined number of highest ranking datasets for output in response to receiving the one or more keyword search queries.
- Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages. A data cataloging system implementing post-hoc management of datasets addresses the problem of how to organize structured datasets at scale, in a setting where teams use diverse and often idiosyncratic ways to produce the datasets and where there is no centralized system for storing and querying them. The data cataloging system extracts metadata ranging from salient information about each dataset (e.g., owners, timestamps, schema) to relationships among datasets, such as similarity and provenance. It then exposes this metadata through services that allow engineers to find datasets within the company, to monitor datasets, to annotate them in order to enable others to use their datasets, and to analyze relationships between them. The attribution of metadata determined from a subset of data sets within a collection of data sets improves data set indexing technologies by reducing the computing resources and time required to index a large set of data sets.
- A dataset cataloging system implementing post-hoc dataset management provides a principled and flexible approach to dataset management, reducing the risk of internal siloing of datasets, thus reducing losses in productivity and opportunities, duplication of work and mishandling of data.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1 is a schematic overview of an example data cataloging system. -
FIG. 2 is a flow diagram of an example process for generating a catalog for one or more data sets. -
FIG. 3 illustrates an example of attributing metadata to an unanalyzed dataset through a collection's representative element. - Like reference numbers and designations in the various drawings indicate like elements.
- This specification describes a post-hoc system for organizing datasets at scale. Specifically, the system collects and aggregates metadata about datasets after the datasets were created, accessed, or updated by various pipelines, without interfering with dataset owners or users. Users are thus able to continue to generate and access datasets using the tools of their choice, and the system works in the background, in a nonintrusive manner, to gather the metadata about datasets and their usage. The system then uses this metadata to power services that enable engineers to organize and find their datasets in a principled manner.
-
FIG. 1 is a schematic overview of an example data cataloging system 100. The data cataloging system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented. - The system 100 crawls different storage systems and production infrastructure, e.g., logs from running pipelines, to discover which datasets exist and to gather metadata about each dataset, e.g., owners, times of access, content features, accesses by production pipelines. The system aggregates the metadata in a central catalog and correlates the metadata about a specific dataset with information about other datasets.
- The system 100 includes one or
more storage systems 102, additional sources ofmetadata 104,dataset catalog 106,index 108 anddataset organizing tools 110. For example, the one or more storage systems may include a largedatabase storage system 112, one ormore file systems APIs storage system 118. Other storage systems may also be used. - The system 100 accesses the
storage systems 102 to collect metadata about data sets stored in thestorage systems 102. The system may also infer metadata about data sets by processing other sources ofmetadata 104 such as logs and information about dataset owners and their projects, by analyzing content of the datasets, and by collecting input from the system users. - The system stores the collected metadata in
dataset catalog 106. The information stored indataset catalog 106 is used to builddataset organizing tools 110. For example, thedataset organizing tools 110 may include asearch tool 120, adashboard 122, adataset monitoring tool 124, aprovenance visualization module 126 and anannotation tool 128. Thedataset organizing tools 110 may be used to search, monitor and visualize the flow of data. - As will be described in more detail below, the resulting catalog uses a Path/Identifier to collect data sets, and then uses metadata that is different from the path/Identifier to describe the data sets. The path/identifier is an example of collection data that is data that is used to collect data sets into a collection, and that is also separate from the metadata that is used to describe the data sets. The generation of the catalog is described with reference to
FIG. 2 , which is a flow diagram of anexample process 200 for generating a catalog for one or more data sets. For convenience, theprocess 200 will be described as being performed by a system of one or more computers located in one or more locations. For example, a data cataloging system, e.g., the data cataloging system 100 ofFIG. 1 , appropriately programmed in accordance with this specification, can perform theprocess 200. - The system accesses a plurality of extant data sets (step 202). In some cases the plurality of extant data sets may include billions of data sets. The plurality of extant data sets includes data sets that are generated independent of each other and are structurally dissimilar. For example, the extant data sets may be stored in many formats, e.g., text files, csv files, and storage systems, e.g., particular file systems or database servers, where each data set may be associated with different types of metadata or access characteristics. In addition, the relationships between datasets may vary.
- The data cataloging systems may exhibit a high level of churn of catalog entries, since every day production jobs generate new data sets, and old data sets are deleted—either explicitly or because their designated time-to-live has expired. By accessing extant data sets, the system is able to prioritize and specify which data sets are to be included in the data set catalog. For example, many of the datasets that have a limited Time-To-Live (TTL) from their creation are intermediate results of a large production pipeline that are garbage collected after a several data set accessing iterations (e.g., several days, if a collection iteration is done daily).
- Another option is to ignore these transient datasets for metadata extraction, or even to exclude them from the catalog. However, there are two limitations on excluding data sets. First, for the datasets have long times to live (TTLs) (e.g., measured in weeks), their value to users can be high when the datasets are just created. Accordingly, these data sets will be included in the collection. Second, some of these transient datasets link non-transient datasets to each other and are thus required for some computations, such as data set provenance. Thus, such data sets are also included in the collection, even if they have a relatively short TTL.
- The system organizes the extant data sets into a plurality of collections (step 204). Each collection in the plurality of collections includes two or more data sets from the plurality of data sets, with each data set in each collection belonging to the collection based on collection data associated with the data set. In some implementations the collection data is a path of a location of a dataset. Each collection in the plurality of collections corresponds to collection data that is different from the collection data that corresponds to the other collections. Other ways of organizing collections can also be used. For example, data sets that belong to a particular entity, such as a particular user, a particular company, or a particular division within a company may be grouped as a collection.
- In some implementations the datasets that belong to a collection include different instances of a particular dataset stored at a location identified by the path. In other implementations the datasets that belong to a collection include datasets that each have at least a sub-path name in its respective path that is common to each path of each dataset included in the collection. Paths of the datasets give hints on how to cluster or organize the datasets, e.g., via embedded identifiers for timestamps, versions, and so on. As an example, consider a dataset that is produced daily and let “/dataset/2015-10-10/daily_scan” be the path for one of its instances. The day portion of the date may be abstracted out to get a generic representation of all datasets produced in a month: “/dataset/2015-10-<day>/daily_scan,” representing all instances from October 2015. By abstracting out the month as well, the hierarchy may be ascended to create abstract paths that represent all datasets produced in the same year: “/dataset/2015-<month>-<day>/daily_scan.”
- For each collection of data sets, the system determines, from a subset of the data sets that belong to the collection, metadata that describe the data sets that belong to the collection (step 206). The metadata does not include the collection data. In some implementations the metadata is basic metadata and includes timestamps, file formats, owners, and access permissions of the datasets. For example, the system may obtain basic metadata by crawling storage systems.
- In further implementations the metadata includes provenance data that describes one or more of dataset production in a workflow, dataset consumption in a workflow, dataset parent dependencies, and dataset child dependencies. Datasets are produced and consumed by code. This code may include analysis tools that are used to query datasets, serving infrastructures that provide access to datasets through APIs, or ETL pipelines that transform it into other datasets. Often, a dataset may be understood better through these surrounding pieces of software that produce and use it. Moreover, this information helps in tracking how data flows through the enterprise as well as across boundaries of teams and organizations within the company. Therefore, for each dataset, the system maintains the provenance of how the dataset is produced, how it is consumed, what datasets this dataset depends on, and what other datasets depend on this dataset.
- In other implementations the metadata include a schema of the dataset. Schema may be a core type of metadata that helps understand a data set. In some implementations commonly used data set formats may not be self-describing, and the schema must be inferred. For example, records within structured datasets may be encoded as serialized protocol buffers. A difficulty lies in determining which protocol buffer was used to encode records in a given dataset. Protocol buffers may be checked into a central code repository such that the system has a full list of protocol buffers that may be made available to match against datasets that have been crawled. Matching may be performed by scanning a few records from the file, and going through each protocol message definition to determine whether it could conceivably have generated the bytes seen in those records. Protocol buffers encode multiple logical types as the same physical type, notably string and nested messages are both encoded as variable-length byte strings. Consequently, the matching procedure may be speculative and can produce multiple candidate protocol buffers. All the candidate protocol buffers, along with heuristic scores for each candidate, may become part of the metadata.
- In some implementations the metadata include a content summary that describes the content stored in a dataset. For example, the system may record frequent tokens that are found by sampling the content. The system may analyze some of the data fields to determine if they contain keys for the data, individually or in combination. To find the potential keys, the system may use an algorithm, e.g., a HyperLogLog algorithm, to estimate the cardinality of values in individual fields and combinations of fields. The cardinality may be compared with the number of records to find potential keys. The system may also collect fingerprints that have checksums for the individual fields and locality-sensitive hash values for the content. The fingerprints may be used to find datasets with content that is similar or identical to the given data set, or columns from other datasets that are similar or identical to columns in the current dataset. The checksums may also be used to identify which data fields are populated in the records of the dataset.
- In some implementations the metadata include user-defined annotations. For example, the system may enable data set owners to provide text descriptions of their data sets.
- In addition to the metadata described above, the system may also collect identifiers for teams that own the data set, a description of the project to which a data set belongs, and the history of the changes to the metadata of the data set.
- The cost of determining metadata that describes data sets may vary depending on the type and size of the data set and the type of metadata. The system may therefore determine metadata that describes the data sets using a differential process that identifies which datasets are important to cover and performs metadata inference based on the cost and benefit for having the particular type of metadata.
- For each collection of data sets, the system attributes, to other data sets in the collection that are not included in the subset of data sets in the collection, the metadata determined from the subset of data sets (step 208). As described above with reference to step 202, in some cases the plurality of extant data sets may include billions of data sets. At this scale, gathering metadata for all datasets becomes infeasible—even spending one second per dataset (although it is noted that many datasets may be too large to process in one second), going through a catalog with 26 billion datasets using a thousand parallel machines may still require around 300 days. Instead of collecting expensive metadata for each individual dataset, the system may therefore collect metadata only for a few datasets in a collection, or a subset of the data datasets, and attribute the metadata to other data sets in the collection. The system then propagates the metadata across the other datasets in the collection. For instance, if the same job generates versions of a dataset daily, these datasets are likely to have the same schema. Thus, the system does not need to infer the schema for each version. Similarly, if a user provides a description for a dataset, it usually applies to all members of the collection and not just the one version. When the collections are large, the computational savings that the system obtains by avoiding analysis of each member of the cluster can be significant. An example of propagating owner's metadata to an unanalyzed dataset through a collection's representative element is illustrated below in
FIG. 3 . - The system generates, from the collections of data sets and the metadata determined from the respective subsets of datasets, a catalog for the plurality of datasets (step 210). The catalog includes an entry for each dataset in the plurality of datasets, and each entry describes the collection to which the dataset belongs and the metadata for the dataset. The catalog provides a uniform way for users to access and query information about all types of data sets, since the variety and complexity of the different data sets, as described above with respect to step 202, is hidden from the user. By construction, the catalog for the plurality of datasets may exclude types of uninteresting data sets, e.g., “marker” files that are content free, and may normalize paths to avoid obvious redundancies, e.g., normalize paths corresponding to different shards of the same data set to a common path and not storing the paths separately in the catalog.
- The generated data catalog provides a variety of services enabled by the determined metadata. For example, the generated data catalog may allow for the exportation of metadata for a specific dataset in a profile page for the data set. For example, a profile page for a data set may accept as input the path of a data set or a data set collection and generate an HTML page from the metadata stored in the catalog. This service may provide methods to edit specific parts of the metadata, for example to allow users to either augment or correct the information stored in the catalog. Generally, a profile page may provide a single place where a user can inspect the information about a data set and understand a context in which the data set can be used in production, making the profile page a natural handle for sharing a data set among users or linking to data set information from other tools.
- In some implementations the system may receive one or more keyword search queries and match the one or more keyword search queries to one or more datasets included in the plurality of datasets in the generated data catalog. The system may rank the matched one or more datasets using a scoring function, wherein the scoring function ranks each dataset based on one or more of (i) the type of dataset, (ii) the index section of the dataset, (iii) lineage fan-out of the dataset or (iv) a description of the dataset, and provide a predetermined number of highest ranking datasets for output in response to receiving the one or more keyword search queries.
- For example, the generate data catalog may allow users to query the catalog and find data sets of interest. For example, the generated data catalog may allow for users to find data sets using simple keyword queries. The service may be backed by a conventional inverted index for document retrieval, where each dataset becomes a “document” and the indexing tokens for each document are derived from a subset of the dataset's metadata. For example, each token can be associated with a specific section of the index, e.g., a token derived from the path of the dataset may be associated with the “path” section of the index. Accordingly, the search atom “path:x” may match keyword “x” on dataset paths only, whereas the unqualified atom “x” will match the keyword in any part of a dataset's metadata. The extraction of indexing tokens follows from the type of queries that the index must cover. As an example, it may be desirable to support partial matches on the dataset path, where a user may search for “x/y” to match a dataset with path “a/x/y/b” (but not one with “a/y/x/b”). The system may break up the dataset's path along common separators and then associate each resulting token with its position in the path. For example, the path “a/x/y/b” may get mapped to the indexing tokens “a”,“x”, “y”, and “b”, in that sequence. When the user issues a search query with a partial path, the service parses the partial path the same way and matches the query's tokens against consecutive tokens in the index. Matching search keywords to datasets may only be one part of the search task. An additional part may include deriving a scoring function to rank the matching datasets, so that the top results are relevant for the user's search. The scoring function may score data sets and be tuned based on a user's experience. For example, the scoring function may score data sets based on how important the dataset is. In some examples the importance of a dataset may depend on the type of dataset. As another example, the importance of a keyword match may depend on the index section of the dataset. For instance, a keyword match on a path of the dataset may be more important that a match on jobs that read or write the dataset. As a further example, lineage fan-out may be an indicator of dataset importance. For instance, if many production pipelines access the dataset, then it is likely the dataset is importance. As a further example, a dataset that carries an owner sourced description may be important. If a keyword match occurs in a description for a dataset, e.g., provided by the dataset owner, the dataset may be weighted higher.
-
FIG. 3 illustrates an example 300 of attributing metadata to an unanalyzed dataset through a collection's representative element. As illustrated inFIG. 3 , owner's metadata is propagated to an unanalyzed data set through a collection's representative element. Theschema 304 “prod.model” is known for several members of the collection 302, and is propagated 306 for the collection 302 as a whole. - Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory program carrier for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. The computer storage medium is not, however, a propagated signal.
- The term “data processing apparatus” encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program (which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- As used in this specification, an “engine,” or “software engine,” refers to a software implemented input/output system that provides an output that is different from the input. An engine can be an encoded block of functionality, such as a library, a platform, a software development kit (“SDK”), or an object. Each engine can be implemented on any appropriate type of computing device, e.g., servers, mobile phones, tablet computers, notebook computers, music players, e-book readers, laptop or desktop computers, PDAs, smart phones, or other stationary or portable devices, that includes one or more processors and computer readable media. Additionally, two or more of the engines may be implemented on the same computing device, or on different computing devices.
- The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g. an HTML page, to a user device, e.g. for purposes of displaying data to and receiving user input from a user interacting with the user device, which acts as a client. Data generated at the user device, e.g. as a result of the user interaction, can be received from the user device at the server.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous.
Claims (19)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/480,971 US10417439B2 (en) | 2016-04-08 | 2017-04-06 | Post-hoc management of datasets |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662319914P | 2016-04-08 | 2016-04-08 | |
US15/480,971 US10417439B2 (en) | 2016-04-08 | 2017-04-06 | Post-hoc management of datasets |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170293671A1 true US20170293671A1 (en) | 2017-10-12 |
US10417439B2 US10417439B2 (en) | 2019-09-17 |
Family
ID=59999432
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/480,971 Active 2037-09-18 US10417439B2 (en) | 2016-04-08 | 2017-04-06 | Post-hoc management of datasets |
Country Status (1)
Country | Link |
---|---|
US (1) | US10417439B2 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11436237B2 (en) | 2020-12-17 | 2022-09-06 | International Business Machines Corporation | Ranking datasets based on data attributes |
US11520764B2 (en) * | 2019-06-27 | 2022-12-06 | International Business Machines Corporation | Multicriteria record linkage with surrogate blocking keys |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20070162436A1 (en) * | 2006-01-12 | 2007-07-12 | Vivek Sehgal | Keyword based audio comparison |
US20140115012A1 (en) * | 2012-10-23 | 2014-04-24 | Oracle International Corporation | Data model optimization using multi-level entity dependencies |
US20140181505A1 (en) * | 2012-12-25 | 2014-06-26 | Carrier Iq, Inc. | Unified Mobile Security System and Method of Operation |
US20140343957A1 (en) * | 2013-05-14 | 2014-11-20 | Zynx Health Incorporated | Clinical content analytics engine |
US20150178167A1 (en) * | 2013-12-23 | 2015-06-25 | Symantec Corporation | Systems and methods for generating catalogs for snapshots |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120016901A1 (en) | 2010-05-18 | 2012-01-19 | Google Inc. | Data Storage and Processing Service |
US8793215B2 (en) | 2011-06-04 | 2014-07-29 | Recommind, Inc. | Systems and methods for publishing datasets |
US8983954B2 (en) | 2012-04-10 | 2015-03-17 | Microsoft Technology Licensing, Llc | Finding data in connected corpuses using examples |
US9582554B2 (en) | 2013-11-08 | 2017-02-28 | Business Objects Software Ltd. | Building intelligent datasets that leverage large-scale open databases |
WO2015117074A1 (en) | 2014-01-31 | 2015-08-06 | Global Security Information Analysts, LLC | Document relationship analysis system |
US20150242407A1 (en) | 2014-02-22 | 2015-08-27 | SourceThought, Inc. | Discovery of Data Relationships Between Disparate Data Sets |
-
2017
- 2017-04-06 US US15/480,971 patent/US10417439B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20070162436A1 (en) * | 2006-01-12 | 2007-07-12 | Vivek Sehgal | Keyword based audio comparison |
US20140115012A1 (en) * | 2012-10-23 | 2014-04-24 | Oracle International Corporation | Data model optimization using multi-level entity dependencies |
US20140181505A1 (en) * | 2012-12-25 | 2014-06-26 | Carrier Iq, Inc. | Unified Mobile Security System and Method of Operation |
US20140343957A1 (en) * | 2013-05-14 | 2014-11-20 | Zynx Health Incorporated | Clinical content analytics engine |
US20150178167A1 (en) * | 2013-12-23 | 2015-06-25 | Symantec Corporation | Systems and methods for generating catalogs for snapshots |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11520764B2 (en) * | 2019-06-27 | 2022-12-06 | International Business Machines Corporation | Multicriteria record linkage with surrogate blocking keys |
US11436237B2 (en) | 2020-12-17 | 2022-09-06 | International Business Machines Corporation | Ranking datasets based on data attributes |
Also Published As
Publication number | Publication date |
---|---|
US10417439B2 (en) | 2019-09-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Halevy et al. | Goods: Organizing google's datasets | |
CN106547809B (en) | Representing compound relationships in a graph database | |
US9569506B2 (en) | Uniform search, navigation and combination of heterogeneous data | |
US9256422B2 (en) | Systems and methods for finding project-related information by clustering applications into related concept categories | |
US8983947B2 (en) | Augmenting search with association information | |
KR20090010185A (en) | Method and system for managing single and multiple taxonomies | |
WO2012129149A2 (en) | Aggregating search results based on associating data instances with knowledge base entities | |
US20210089822A1 (en) | Content type embeddings | |
US20150081718A1 (en) | Identification of entity interactions in business relevant data | |
Kelley et al. | A framework for creating knowledge graphs of scientific software metadata | |
Gui et al. | IFC-based partial data model retrieval for distributed collaborative design | |
Lehmann et al. | Managing Geospatial Linked Data in the GeoKnow Project. | |
Buneman et al. | Data citation and the citation graph | |
US20160086499A1 (en) | Knowledge brokering and knowledge campaigns | |
Truică et al. | TextBenDS: a generic textual data benchmark for distributed systems | |
WO2018226255A1 (en) | Functional equivalence of tuples and edges in graph databases | |
Valentine et al. | EarthCube Data Discovery Studio: A gateway into geoscience data discovery and exploration with Jupyter notebooks | |
US10503743B2 (en) | Integrating search with application analysis | |
US10417439B2 (en) | Post-hoc management of datasets | |
US20160085850A1 (en) | Knowledge brokering and knowledge campaigns | |
US9679066B2 (en) | Search results based on an environment context | |
US9286349B2 (en) | Dynamic search system | |
US10417230B2 (en) | Transforming and evaluating missing values in graph databases | |
Truică et al. | Benchmarking top-k keyword and top-k document processing with T2K2 and T2K2D2 | |
US10409871B2 (en) | Apparatus and method for searching information |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KORN, PHILIP;WHANG, STEVEN EUIJONG;NOY, NATALYA FRIDMAN;AND OTHERS;SIGNING DATES FROM 20160415 TO 20160426;REEL/FRAME:042399/0431 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
CC | Certificate of correction | ||
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |