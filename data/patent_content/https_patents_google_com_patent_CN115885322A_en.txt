CN115885322A - Determining a viewing time loss region in a media content item - Google Patents
Determining a viewing time loss region in a media content item Download PDFInfo
- Publication number
- CN115885322A CN115885322A CN202180046570.2A CN202180046570A CN115885322A CN 115885322 A CN115885322 A CN 115885322A CN 202180046570 A CN202180046570 A CN 202180046570A CN 115885322 A CN115885322 A CN 115885322A
- Authority
- CN
- China
- Prior art keywords
- viewing time
- video
- time loss
- region
- features
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/25—Management operations performed by the server for facilitating the content distribution or administrating data related to end-users or client devices, e.g. end-user or client device authentication, learning user preferences for recommending movies
- H04N21/251—Learning process for intelligent management, e.g. learning user preferences for recommending movies
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/45—Management operations performed by the client for facilitating the reception of or the interaction with the content or administrating data related to the end-user or to the client device itself, e.g. learning user preferences for recommending movies, resolving scheduling conflicts
- H04N21/466—Learning process for intelligent management, e.g. learning user preferences for recommending movies
- H04N21/4662—Learning process for intelligent management, e.g. learning user preferences for recommending movies characterized by learning algorithms
- H04N21/4666—Learning process for intelligent management, e.g. learning user preferences for recommending movies characterized by learning algorithms using neural networks, e.g. processing the feedback provided by the user
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23418—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/774—Generating sets of training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/80—Fusion, i.e. combining data from various sources at the sensor level, preprocessing level, feature extraction level or classification level
- G06V10/806—Fusion, i.e. combining data from various sources at the sensor level, preprocessing level, feature extraction level or classification level of extracted features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23424—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving splicing one content stream with another content stream, e.g. for inserting or substituting an advertisement
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/25—Management operations performed by the server for facilitating the content distribution or administrating data related to end-users or client devices, e.g. end-user or client device authentication, learning user preferences for recommending movies
- H04N21/262—Content or additional data distribution scheduling, e.g. sending additional data at off-peak times, updating software modules, calculating the carousel transmission frequency, delaying a video stream transmission, generating play-lists
- H04N21/26208—Content or additional data distribution scheduling, e.g. sending additional data at off-peak times, updating software modules, calculating the carousel transmission frequency, delaying a video stream transmission, generating play-lists the scheduling operation being performed under constraints
- H04N21/26241—Content or additional data distribution scheduling, e.g. sending additional data at off-peak times, updating software modules, calculating the carousel transmission frequency, delaying a video stream transmission, generating play-lists the scheduling operation being performed under constraints involving the time of distribution, e.g. the best time of the day for inserting an advertisement or airing a children program
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/44—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs
- H04N21/44008—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics in the video stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/442—Monitoring of processes or resources, e.g. detecting the failure of a recording device, monitoring the downstream bandwidth, the number of times a movie has been viewed, the storage space available from the internal hard disk
- H04N21/44213—Monitoring of end-user related data
- H04N21/44222—Analytics of user selections, e.g. selection of programs or purchase activity
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/60—Network structure or processes for video distribution between server and client or between remote clients; Control signalling between clients, server and network components; Transmission of management data between server and client, e.g. sending from server to client commands for recording incoming content stream; Communication details between server and client
- H04N21/65—Transmission of management data between client and server
- H04N21/658—Transmission by the client directed to the server
- H04N21/6582—Data stored in the client, e.g. viewing habits, hardware capabilities, credit card number
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/85—Assembly of content; Generation of multimedia applications
- H04N21/854—Content authoring
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
- G06N3/0442—Recurrent networks, e.g. Hopfield networks characterised by memory or gating, e.g. long short-term memory [LSTM] or gated recurrent units [GRU]
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for determining a viewing time loss region in a media content item. In one aspect, features of a video are input into a trained model trained to output a viewing time loss region. The trained model is trained using labels corresponding to known viewing time loss regions in the training video and features of the training video corresponding to the known viewing time loss regions. The viewing time loss region defines a time window of the video during which a likelihood that a user will stop playback of the video is greater than a threshold likelihood. In response to inputting the features of the first video into the trained model, data regarding the viewing time lost region of the video is obtained from the model and provided to an entity participating in providing the video to the user.
Description
Background
This specification relates generally to data processing and determining a viewing time loss region in a media content item.
The video streamed to the user may include one or more digital components that are typically overlaid on top of the original video stream. The overlaid content may be provided to the user within a rectangular area that overlays a portion of the original video screen. The digital component may also include in-stream content that is played before, during, or after the original video stream.
As used throughout this document, the phrase "digital component" refers to a discrete unit of digital content or digital information (e.g., a video clip, an audio clip, a multimedia clip, an image, text, or another unit of content). The digital component may be electronically stored in the physical memory device as a single file or collection of files, and the digital component may take the form of a video file, an audio file, a multimedia file, an image file, or a text file. For example, the digital component may be content intended to supplement the content of a video or other resource. More specifically, the digital component may include digital content related to the asset content (e.g., the digital component may relate to a subject that is the same as or otherwise related to the subject/content on the video). Thus, the provision of the digital component may supplement and generally enhance the web page or application content.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of: receiving a first video; determining a set of features of a first video; inputting the set of features of the first video into the trained model, wherein (1) the trained model is trained to output a viewing time lost region in a particular video, (2) the trained model is trained using labels corresponding to known viewing time lost regions in a plurality of training videos and features of the plurality of training videos corresponding to known viewing time lost regions, and (3) the viewing time lost region of the particular video defines a time window of the particular video during which a likelihood that a user stops playback of the particular video is greater than a threshold likelihood; in response to inputting the set of features of the first video into the trained model, obtaining data about a viewing time lost region of the first video from the trained model; and providing data regarding a viewing time lost region of the first video to an entity participating in providing the first video to the user. Other embodiments of this aspect include corresponding methods, apparatus, and computer programs configured to perform the actions of the methods and encoded on computer storage devices. These and other embodiments may each optionally include one or more of the following features.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The techniques described in this specification enable identification of viewing time loss information in a video, which may be used to determine where digital components within a video stream are positioned so that the digital components are more likely to be viewed/played by a user during video playback. Without considering the viewing time loss region, the digital component may be positioned within a portion of the video that appears after the viewing time loss region, which increases the likelihood that the digital component is not played. This in turn results in wasted computational resources to provide and receive digital components that are not ultimately consumed by the user. In contrast, by taking into account the viewing time lost region of the video when placing the digital component to be displayed/played during the video, the probability of the digital component being played during video playback increases, resulting in that the computational resources required to provide and receive the digital component are not wasted. In more detail, the media content system achieves resource savings by selecting a point in time of playback of the digital component prior to the determined loss of viewing time region. By selecting where the playback of the digital components is located before viewing the time loss region, the likelihood of providing unconsumed digital components is reduced (or conversely, the likelihood of consuming digital components is increased).
The viewing time lost region information may be used for other purposes. For example, viewing time loss information may be provided to the media content item owner. The media content item owner may be interested in knowing during which portion(s) of the content item the user is more likely to abandon playback of the content item. By presenting the viewing time loss information to the content item owner, the content item owner may know the viewing time loss region. Thus, the content item owner may decide to rework or adjust the media content item to reduce or change the viewing time loss area of the media content item. Over time, the adjusted media content item including fewer or shorter viewing time loss regions will be consumed by the user in a greater proportion than media content items having more or longer viewing time loss regions. Accordingly, providing media content item assets with fewer and/or shorter viewing time loss zones is more efficient, as the waste of assets (e.g., providing assets that are not consumed) will decrease over time.
Further, when placing and providing digital components within a video, accounting for the loss-in-viewing-time region of the video may result in an increase in the rating of the video and associated digital components (as compared to the rating provided and resulting from the placement of the digital components without accounting for the loss-in-viewing-time region). This in turn may result in more revenue from the digital components as they are actually consumed/viewed by the user (as opposed to being ignored or missed by the user as they are placed in the video in areas where viewing time is not a concern).
The viewing time loss region information may also be used to optimize the serving or other processing of the media content and the digital components to be presented with the media content. For example, a first set of digital components scheduled to be played with the video before the determined viewing time loss region may be sent to the client device by the content provider based on priority (e.g., even possibly provided with the video before the first set of digital components is scheduled to be played). The other set(s) of digital components that are scheduled to be played after the first determined viewing time loss zone may be provided by the content provider on an on-demand basis (e.g., exactly when the digital components are scheduled to be played). For example, since some users tend to stop watching video during each viewing time loss zone, the content provider may provide some high priority digital components before the first viewing time loss zone and lower priority digital components before each subsequent viewing time loss zone. It will be appreciated that this results in a more efficient use of resources, as the content provider can provide digital components that are scheduled to play after a viewing time loss zone only if the video is played back, or only if the user does not stop playback before the particular digital component is required.
The machine learning methods described in this specification may achieve technical advantages that are otherwise impossible to achieve. A media content system may provide billions of content items, for example. It is not feasible to manually determine the viewing time loss region and determine where the digital components of the billions of content items are located. Rather than using a manual method to determine the viewing time loss region, the machine learning system described in this specification can be used to analyze a media content item and automatically determine the viewing time loss region in the media content item. Further, the machine learning system may automatically predict a viewing time loss region of the media content item where little or no user interaction data exists. Without the use of machine learning systems, a large amount of resources must be used to collect a large amount of user interaction data in order to be able to directly calculate viewing time loss information. Using a machine learning system may enable prediction of viewing time loss information even for newly created videos without waiting or expending resources for user interaction data collection and processing of real world data.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example environment for distributing and providing digital content for display on a client device.
FIG. 2 illustrates an example system for determining a viewing time loss zone in a media content item.
Fig. 3 illustrates an example viewing time loss graph for a video.
FIG. 4 illustrates example pseudo code for determining a viewing time lost region.
FIG. 5 illustrates an example architecture of a viewing time loss model.
FIG. 6 is a flow diagram of an example process for determining a viewing time loss zone in a media content item.
FIG. 7 is a block diagram of an example computer system that may be used to perform the described operations.
Detailed Description
The specification relates generally to determining a viewing time loss zone in a media content item. The media content system may provide media content (e.g., video content, audio content) to viewers. In general, a user may not consume a media content item all the way through. For various reasons, a user may leave or abandon playback of a media content item at a point in time before the media content item is completed. The time frame (timeframe) in which a user may give up playback of a media content item for playback may be referred to as a viewing time lost area. As another definition, a watch time lost region of a particular video may describe a time window in which a user of the particular video has a greater than threshold likelihood of stopping playback of the particular video in the time window.
As summarized below and described in this specification, the techniques described in this application enable the use of trained viewing time loss models to automatically determine predicted viewing time loss regions for input video. The model trainer may train the viewing time loss model using a set of training videos. For each training video, the viewing time loss curve generator may generate a viewing time loss curve for the training video based on user interaction data (e.g., video session start/stop times) of the training video. The viewing time loss region may be determined by a viewing time loss region identifier in a viewing time loss curve generated for the training video. The viewing time loss region identifier may determine an average slope of the viewing time loss curve and identify the viewing time loss region by determining a region of the viewing time loss curve having a slope greater than the average slope. The viewing time loss model may be trained based on viewing time loss regions determined for the training video. In particular, the viewing time loss model may be trained based on a viewing time loss region determined for the training video and features of the training video that occur within a predetermined time window of the viewing time loss region (e.g., as may be determined by a feature extractor).
The trained viewing time loss model may be used to predict a viewing time loss region of the input video. For example, the input video may be a video where little or no user interaction data exists. The feature extractor may determine a set of features of the input video and input the set of features of the input video into the trained viewing time loss model. The trained viewing time loss model may output data regarding predicted viewing time loss regions of input features of the input video. The data regarding the predicted viewing time loss zone may include start and stop times of the predicted viewing time zone, a count of the viewing time zones in the video, a total duration of the viewing time loss zones in the video, or a percentage of a length of the video corresponding to the identified viewing time loss zone.
Data regarding the predicted viewing time loss region of the input video may be used for various purposes. For example, data regarding the predicted viewing time loss region of the input video may be provided to a content provider, which uses the data regarding the viewing time loss region of the first video to determine the point in time at which the digital component is played in the input video. For example, the content provider may select a digital component playback time point in the input video that precedes the determined viewing time loss region, which may enable various types of resource savings, as described above. As another example, data regarding the predicted viewing time loss region may be provided to the owner of the input video. The owner of the input video may choose to modify the input video to reduce the count or duration of the viewing time loss region. Thus, the updated input video may be more likely to be consumed to completion or a later point in time, which may result in resource efficiency with respect to expending resources in providing the actually consumed content. These features and additional features and benefits are further described in more detail below with reference to fig. 1-7.
In addition to the description throughout this document, a user may be provided with controls that allow the user to select whether and when the systems, programs, or features described herein may be able to collect user information (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the user's current location) and whether to send content or communications from a server to the user. Further, some data may be processed in one or more ways before it is stored or used, thereby removing personally identifiable information. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized where location information is obtained (e.g., to a city, zip code, or state level) such that a particular location of the user cannot be determined. Thus, the user may have control over what information is collected about the user, how the information is used, and what information is provided to the user.
FIG. 1 is a block diagram of an example environment 100 for distributing and providing digital content for display on a client device. The example environment 100 includes a network 104. The network 104 may include a Local Area Network (LAN), a Wide Area Network (WAN), the Internet, or a combination thereof. The network 104 may also include any type of wired and/or wireless network, satellite network, cable network, wi-Fi network, mobile communication network (e.g., 3G, 4G, etc.), or any combination thereof. Network 104 may utilize communication protocols, including packet-based and/or datagram-based protocols such as Internet Protocol (IP), transmission Control Protocol (TCP), user Datagram Protocol (UDP), or other types of protocols. The network 104 may also include a plurality of devices, such as switches, routers, gateways, access points, firewalls, base stations, repeaters, or combinations thereof, that facilitate network communications and/or form the basis of network hardware.
The network 104 connects the client device 102, the content platform 106, the content provider 108, and the video processing system 110. The example environment 100 may include many different content platforms 106, video processing systems 110, client devices 102, and content providers 108.
The content platform 106 may publish and make available its own content. For example, the content platform 106 may be a news platform that publishes its own news articles. The content platform 106 may also display content provided by one or more content providers 108 that are not part of the content platform 106. In the above example, the news platform may also display third-party content provided by one or more content providers 108. As another example, the content platform 106 may be a data aggregator platform that does not publish its own content, but aggregates and displays third party content provided by different content providers 108.
In some implementations, the content platform 106 may store some information about the client device (e.g., device preference information, content consumption information, etc.). Such user information may be used by the content platform, for example, to customize content provided to the client device 102 or to enable easy access to particular content frequently accessed by the client device 102. In some implementations, content platform 106 may not store such device information on the platform; however, the content platform 106 may still provide such information for storage on a particular server (separate from the content platform). Thus, content platform 106 (also referred to herein as content platform/server 106, or simply server) refers to a content platform that stores such device information or a server (separate from the content platform) that stores such device information.
In some implementations, the content platform 106 is a video service through which users can view streamed video content. The video streamed to the user may include one or more digital components overlaid on top of the initial video stream (e.g., provided by the content provider 108). For example, for a given video streaming bandwidth, it is often desirable to provide overlay content on the underlying video stream to provide the digital component(s) to the viewer of the video stream and to increase the amount of content delivered within the viewing area. In addition to or in lieu of a video streaming scenario, the content platform 106 may include a video processor that processes a video file to modify the video file to include overlay content, the processed video file with the overlay content being provided to the client device 102 for display on the client device 102.
The video processing system 110 may determine a predicted viewing time loss area for a given video and provide the viewing time loss area information to different entities or systems (as outlined above and further described with reference to fig. 2-6). For example, the video processing system 110 may provide information/data regarding the viewing time loss zone to the content provider 108 so that the content provider may arrange the digital component in advance of the predicted viewing time loss zone. As another example, the viewing time loss information may be provided to the client device 102 of the video owner as feedback to the video. For example, the owner may adjust the video based on the viewing time loss information.
In some implementations, the video processing system 110 uses the trained models to generate viewing time loss information (as outlined herein and further described with reference to fig. 2, 5, and 6). That is, the trained model may be trained to output a viewing time lost region in a particular input video. The trained model may be trained using known viewing time lost regions of the plurality of training videos and features of the plurality of training videos corresponding to the known viewing time lost regions. Once the model is trained, the video processing system 110 may process the input video using the trained model. A set of features (e.g., video features, audio features, semantic features) may be determined for the input video and the features may be provided to the trained model. The trained model may identify viewing time lost regions in the input video. Additional structural and operational aspects of these components of the example environment 100 are described below with reference to fig. 2-7.
Fig. 2 illustrates an example system 200 for determining a viewing time loss zone in a media content item. The system 200 depicts the below-described components of the video processing system 110, such as described above. Each component of system 200 is a software component that includes instructions executed by a processing entity, such as a processor.
A video repository 202 (e.g., a database or other storage device) includes videos managed by the content platform 106, for example. A set of training videos 204 may be selected from the video repository 202 (e.g., by an administrator or training video selector 205) for training the viewing time loss model 206. In some implementations, the training video 204 may be selected from the video warehouse 202 based on the number of views (e.g., by an administrator or the training video selector 205). For example, the more a particular video is viewed, the more interaction data (described below) exists for that video (e.g., stored in the user activity log 208). Thus, for example, the training video 204 may include the top X number of most viewed videos, or may include videos that have been viewed at least Y times. By selecting the most viewed video, the viewing time loss curve generated by the viewing time loss curve generator 210 may be based on the maximum amount of interaction data (as further described below with reference to fig. 3-4).
The interaction data included in the user activity log 208 may include start and stop time information that indicates, for each video viewing session of the video, a start time at which the user starts viewing the video and an end time at which the user ends their video viewing session. The end time may correspond to the end of the video or may be a point in time before the end of the video. In addition to start and stop time information, the user activity log may also include other types of data, such as user interactions with digital components in the video. The information included in the user activity log 208 may be anonymized so that no PIN (personally identifiable information) is stored.
The viewing time loss curve generator 210 may create a viewing time loss curve for each respective one of the training videos 204 based on the aggregated information for the training videos included in the user activity log 208. An example viewing time loss curve generated by the viewing time loss curve generator 210 is described and depicted below with reference to fig. 3.
Fig. 3 illustrates an example viewing time loss graph 300 for a video. The video may be one of the training videos 204 described above with respect to fig. 2.
As shown in fig. 3, the X-axis 302 corresponds to a video length ratio. For example, a value of 0 (labeled 304) on the X-axis corresponds to the beginning of the video, while a value of 1 (labeled 306) on the X-axis corresponds to the end of the video. The Y-axis 308 corresponds to the ratio of viewers of a video session of a video that are still watching that video at a given video point in time. For example, for point 310 on the viewing time loss curve 312, a value of 1 on the y-axis and a value of 0 on the X-axis indicate that 100% of the viewers are viewing video at the beginning of their video viewing session. For point 314, a value of 0.5 on the y-axis and a value of 0.48 on the X-axis indicate that by the time 48% of the video has ended/elapsed, 50% of the viewers remain and continue to watch the video. In other words, this point on the viewing time loss curve also indicates that 50% of the viewers have stopped viewing the video by the time 48% of the video has ended/passed. For point 316, a value of 0.08 on the y-axis and a value of 1 on the X-axis indicate that 8% of the viewers watched the video until the end of the video.
Referring briefly again to fig. 2, the viewing time loss region identifier 212 may identify a viewing time loss region of the training video 204 based on the viewing time loss curve created by the viewing time loss curve generator 210. The identified viewing time lost region information may be stored by the viewing time lost region identifier 212 in the viewing time lost region repository 213. For a given viewing time curve of the video, the viewing time lost region identifier 212 may identify the viewing time lost region as a portion of the curve having a slope that is greater than the average slope of the entire curve.
For example, and referring again to fig. 3, viewing time loss region identifier 212 has determined an average slope for viewing time loss curve 312. The average slope is represented in graph 300 by line 318. The identified viewing time loss regions 320, 322, and 324 correspond to regions of the viewing time loss curve 312 where the slope of the viewing time loss curve 312 is greater than the average slope. For example, a viewing time loss region 320, 322, or 324 that is greater than the average slope and identified may correspond to a significant drop in viewers at those time windows of the video. The viewing time lost region identifier 212 may store information of the identified viewing time lost regions, such as information of the identified viewing time lost regions 320, 322, and 324, in the viewing time lost region repository 213.
Additional details regarding the determination of the viewing time lost region by the viewing time lost region identifier 212 are discussed below with reference to fig. 4.
Fig. 4 illustrates example pseudo code 400 for determining a viewing time loss region. For example, code corresponding to the pseudo code 400 may be executed by a processor and may be provided by the viewing time lost region identifier 212 described above with reference to fig. 2. The first section 402 includes pseudo code for finding a viewing time loss region in a video using an average slope method.
The code line 404 includes code for determining an average viewing time. Viewing time refers to the value of the y-axis of a viewing time loss curve, such as viewing time loss curve 312. The viewing time corresponds to the ratio of the remaining persons (e.g. persons still viewing the video at a certain point in time). The ratio of remaining people (e.g., the count of remaining people divided by the total number of people) has a starting value of 1 at the beginning of the video and a value of 0 at the end of the curve. Thus, the slope of the line connecting the start and end points of the curve may be calculated as 1 (e.g., the change in y) divided by the video length (e.g., the change in x).
In code line 408, it is determined whether the difference between the viewing time at the current point in time and the viewing time at the immediately preceding point in time is greater than the average viewing time. The code line 408 is equivalent to determining whether the slope of the viewing time loss curve between the current point in time and the immediately preceding point in time is greater than the average slope. Code line 408 also corresponds to determining whether the local derivative of the view time loss curve at the current point in time is less than the average slope. At line 410, if the difference between the viewing time at the current point in time and the viewing time at the immediately preceding point in time is greater than the average viewing time, then that point in time is added to the viewing time lost area. For example, when a first time point is to be added to the viewing time lost region, a new viewing time lost region may be "opened" as the current viewing time lost region, and the time point may be added to the current viewing time lost region. As long as the determination in code line 408 returns true for those time point(s), subsequent consecutive time points may be added to the current viewing time lost region. Once the determination in code line 408 returns false for a point in time, the current viewing time lost region may be closed. The viewing time lost region identifier 212 may store information of the closed viewing time lost region in the viewing time lost region repository 213, corresponding to the identified viewing time lost region. If the determination in code line 408 returns true for a subsequent point in time, another viewing time lost region may be opened as the current viewing time lost region.
Referring again to fig. 3, for general video content (e.g., for most, all, or nearly all video), a relatively sharp drop in slope may occur in the viewing time loss curve at points in time corresponding to the start and end of the video as compared to the average slope. For example, if a video does not include expected or desired content, some portion of the users may abandon the video near the beginning of the video. As another example, some portion of the users may view the video to or near the end, but may typically abandon the video as it nears completion. As shown in fig. 3, the curve portions 326 and 328 at the beginning and end of the view-time-loss curve 312 have relatively steep slopes, respectively, compared to the average slope. The slope of the flatter middle portion 330 of the viewing time loss curve 312 is generally lower than the average slope of the entire viewing time loss curve 312 due to the greater slopes at the beginning and end of the viewing time loss curve 312. Thus, if the slope increases in the regions of the flatter middle portion 330 (e.g., as in the viewing time loss regions 320, 322, and 324) and becomes higher than the average slope, the viewing time loss region identifier 212 may identify these regions as viewing time loss regions. A user who abandons the video in the lost viewing time area in the flatter middle portion 330 may do so due to the characteristics of the video. These features will be described in more detail below.
Because most, if not all or nearly all, of the viewing time loss curves for all videos exhibit a sharp slope drop at the beginning and end compared to the average slope, the beginning and ending video portions may be considered uninteresting for viewing time loss region identification, since most of all of the viewing time loss curves exhibit such behavior. Thus, for the purpose of training the viewing time loss model 206, the viewing time loss region identifier 212 may be excluded from identifying viewing time loss regions at the beginning and end of the video. Additional details regarding excluding the viewing time lost region are discussed below with reference to fig. 4.
Referring again to fig. 4, section 412 corresponds to excluding (e.g., removing) the continuous viewing time lost region from the beginning of the video, while section 422 corresponds to removing the continuous viewing time lost region from the end of the video. The viewing time loss curve typically has a relatively sharp drop at the beginning and end of the video. In some implementations, as the identified loss of viewing time regions, a first loss of viewing time region occurring at the beginning of the video and a last loss of viewing time region occurring at the end of the video are removed by the loss of viewing time region identifier 212. In other implementations, point-in-time analysis may be used to remove the initial and end viewing time loss regions, as described below for rows 412-422.
For example, line 414 begins an iterative construct that analyzes the point in time from the beginning of the video (i.e., t) i ＝t 1 ). At line 416, for the current point in time (t) i ) Determining the next time point (t) i+1 ) Whether in a first viewing time lost region (e.g., a region that has been identified by the viewing time lost region identifier 212 with the operation of code lines 406-410 as a viewing time lost region). At row 418, if the next point in time is in the lost viewing time region, the current point in time is removed from the lost viewing time region. At line 420, the iterative construction exits once the next point in time is not in the viewing time lost region. Removing start time from initial viewing time lost regionThe point may effectively result in delaying the point in time at which the viewing time loss region begins to be considered until the viewing time loss curve begins to flatten after the initial sharp drop. Line 422 begins an iterative construct that includes similar logic as the iterative construct that begins at line 414, but for the point in time that the video was processed in reverse, starting from the last point in time.
In some implementations and/or for some videos, different methods may be used to reduce the number of identified viewing time loss regions. For example, when an area is identified for a video that exceeds a predetermined threshold, a viewing time lost area reduction method may be used. In some implementations, high percentile values of the derivatives may be used to filter out points in time from the viewing time loss region to create a more refined list of viewing time region candidates that includes only the region with the steepest slope. For example, a predetermined number of regions having the steepest slope may be identified as viewing time loss regions, as long as each of those regions is steeper than the average slope. As another example, a second derivative may be calculated (e.g., at each time point) and used to create a more refined set of identified viewing time loss regions. For example, a viewing time loss curve region in which the second derivative is negative may be selected as a viewing time loss region, and a viewing time loss curve region in which the second derivative is not negative may remain unselected as a viewing time loss region.
When model trainer 215 trains viewing time loss model 206, the viewing time loss regions identified by viewing time loss region identifier 212 may be used by model trainer 215 as labels 214. To define the label 214, the following equation may be used, where i represents a given region of the viewing time loss curve generated by the viewing time loss curve generator 210:
target labeling: if it is a viewing time loss region, y (i) =1; otherwise is 0
The viewing time loss model 206 may also be trained by a model trainer 215 using features 216 of the training video 204 retrieved from a video feature database 218 and/or determined by a feature extractor 219. Features 216 used to train viewing time loss model 206 may include features that occur in training video 204 within a predetermined time window of the time points included in the identified viewing time loss region. For example, the predetermined time window may include a configurable point in time of X seconds (e.g., X may be 15 seconds, 6 seconds, 9 seconds, or some other value) before and after the point in time in the viewing time loss region. Model trainer 215 may train viewing time loss model 206 based on the assumption that features within a predetermined time window of the viewing time loss region may predict the viewing time loss region. In general, different machine learning methods may be used in the viewing time loss model 206 to generate the predicted viewing time loss region. For example, the viewing time loss model 206 may be a supervised offline model. In such a model, the supervised machine learning model is trained using the determined features of the video and corresponding labels identifying viewing time loss regions in the video (additional details regarding model training are described below with reference to fig. 5 and 6). In this way, one or more machine learning methods that result in the most accurate predictions may be used. An example architecture of the view time loss model 206 is described below with reference to fig. 5.
Features 216 used by model trainer 215 to train viewing time loss model 206 may include various types of features, such as audio features and video features. The feature extractor 219 may identify features per second of the video and/or each frame of the video based on different feature extraction models 220. For example, the feature extraction model 220 may include an image model and an audio model. The audio features may correspond to various types of sounds and may be determined by the feature extractor 219 using one or more audio models. The audio features may include embeddings and annotations related to the audio information in the video. The video data may be processed, for example, using color space methods, object detection, and/or other methods. The video features may correspond to color space characteristics, the presence or absence of objects, and the like. Features 216 may include annotation features that identify semantic information about a particular frame(s), such as objects or semantic entities that have been identified in audio and/or video data. For example, the video features identified by feature extractor 219 may include frame annotations generated using a semantic model included in feature extraction model 220. For example, for a video frame in which a person is cycling, the semantic model may generate annotations for "cycling" and "athletes. Other features may include derivative features such as annotations that are most common in videos. Some features may be original embedded features. The original embedded features are features describing the appearance of the video generated by the image understanding model. The original embedded features may capture visual/semantic concepts and similarities in a frame or image.
In some implementations, rather than having the feature extractor 219 extract features using a separate feature extraction model 220 and provide the extracted features to the viewing time loss model 206, the training video 204 is provided as input to the viewing time loss model 206, and the viewing time loss model 206 analyzes each frame of the training video 204 and corresponding audio using machine learning to learn which features correspond to the viewing time loss model. However, using such an approach to the viewing time loss model 206 may result in unacceptable use of resources for training and analysis. A complete evaluation of all pixels of all frames and corresponding audio may add unacceptable complexity, size, and processing to the viewing time loss model 206. Having the feature extractor 219 use a separate feature extraction model 220 and provide the extracted features to the view time loss model 206 may improve the efficiency of the view time loss model 206 and enable the view time loss model 206 to use less training and processing resources for prediction.
Once trained by model trainer 215 as described above, viewing time loss model 206 may be applied to input video 221 to generate predicted viewing time loss region 222 of input video 221. The input video 221 may be, for example, a recently uploaded video that stores little or no interaction data in the user activity log 208. As described in more detail below with reference to fig. 5, features 224 of the input video 221 may be extracted by the feature extractor 219 and provided as input to the viewing time loss model 206. The features 224 of the input video 221 may be similar types of features to the features 216 determined for the training video 204. The features 224 determined for the input video 221 may then be input into the trained viewing time loss model 206, which uses the features 224 to generate a predicted viewing time loss region 222 of the input video 221. The viewing time loss model 206 may generate a confidence score for each predicted viewing time loss region 222 that represents a likelihood that the predicted viewing time loss region 222 is a viewing time loss region. For example, the predicted viewing time loss region 222 may represent a portion of the video where the confidence score of the input video 221 is greater than a predetermined threshold.
The predicted viewing time lost region 222 may be used for various purposes and/or provided to various entities. For example, the predicted viewing time loss region 222 may be used to select a particular point in time of a video presenting a digital component. For example, a point in time outside the predicted viewing time loss region 222 (i.e., a previous point in time) may be selected as a point in time for rendering the digital component in association with playback of the input video 221. Thus, the likelihood of the user abandoning playback of the digital component (and input video 221) is reduced relative to playback of the digital component that is completed after the predicted loss of viewing time region. In this way, resource consumption may be more efficient, as the likelihood of spending resources on unviewed content is reduced. Other benefits of presenting the digital components before the predicted loss of viewing time region 222 may be realized, such as additional revenue that may arise from a large number of actual views of the digital components that may occur by placing the digital components before the predicted loss of viewing time region 222.
As another example, the video processing system 110 may provide the predicted viewing time loss region 222 (and possibly corresponding confidence values) to a video creator or owner (e.g., in a video studio, such as after the video creator uploads the input video 221 to the content platform 106). The presentation of the viewing time loss information may also include derived, aggregated, or presented information generated by the viewing time loss information generator 226. For example, the viewing time loss information generator 226 may generate derived viewing time loss information 228 for the input video 221 that may include a count of predicted viewing time loss regions 222, information describing a timeline view or other presentation of where the predicted viewing time loss regions 222 exist in the input video 221, and/or various other statistics or information about the predicted viewing time loss regions 222. For example, the viewing time loss information generator 226 may calculate a percentage of the running time of the input video 221 included in the predicted viewing time loss region 222 (e.g., the total length of time/video included in the predicted viewing time region). As yet another example, the video quality score of the input video 221 may be calculated by the viewing time loss information generator 226 based at least on the predicted viewing time loss region 222 (such as based on a total count and/or a total duration of the predicted viewing time loss region 222). Other examples may include the viewing time loss information generator 226 determining information indicative of how the predicted viewing time loss region 222 of the input video 221 compares to viewing time loss predictions or calculations for other videos (e.g., relative to a total count and/or duration of the viewing time loss region, compared to other videos that are similar in category, duration, or other video characteristics). The derived viewing time loss information 228 may be presented with the predicted viewing time loss region 222 to, for example, a video creator or a content provider.
The video creator may use the received information of the predicted viewing time loss region 222 to adjust the input video 221 to improve the quality of the input video 221. For example, the video creator may adjust or replace the video content in the predicted viewing time loss region 222 and re-upload the input video 221 to be processed again using the viewing time loss model 206. The viewing time loss model 206 may be used to generate a new predicted viewing time loss region 222 for the input video 221. The video creator may repeatedly adjust, upload, and view the predicted loss of viewing time region 222 information until the video creator is satisfied with the reduction in the count and/or duration of the predicted loss of viewing time region 222 of the input video 221.
By reducing the count and/or duration of the predicted viewing time lost regions 222 in the input video 221, the video creator can create an improved video that is more efficient in terms of resource utilization. For example, a video with fewer lost viewing time regions will be more likely to be viewed for a longer period of time than a video with a larger number of lost viewing time regions. Therefore, for a video having a less viewing time loss area, resources for providing the video to the user are less likely to be wasted. Furthermore, for videos with less viewing time lost regions, resources for providing digital components that are provided for playback associated with video playback will be less likely to be wasted. As there are fewer viewing time lost regions in the video, the likelihood of playing back a digital component in the video at a time point corresponding to the viewing time lost region decreases.
The viewing time loss model 206 may be refined over time using a feedback loop. For example, in processing the input video 221 using the viewing time loss model 206, as described above, a predicted viewing time loss region 222 of the input video 221 may be determined. Over time, the input video 221 will likely be viewed by more and more users, and the user activity log 208 may include an increasing amount of interaction data of the input video 221. As the interaction data of the input video 221 grows, the viewing time lost region identifier 212 may identify the actual viewing time lost region of the input video 221 (e.g., once a sufficient amount of interaction data has been collected for the input video 221) (in the same manner as described above). For example, model trainer 215 may compare the actual viewing time loss region with the predicted viewing time loss region 222, and model trainer 215 may feed the comparison into viewing time loss model 206 for updating viewing time loss model 206. For example, the confidence value or weight used by the viewing time loss model 206 may be increased (e.g., by the model trainer 215) when the actual viewing time loss region matches the predicted viewing time loss region 222. As another example, when the actual viewing time loss region does not match the predicted viewing time loss region 222, the model trainer 215 may reduce a confidence value or weight used by the viewing time loss model 206, or the model trainer 215 may otherwise adjust the viewing time loss model 206, and/or the model trainer 215 may retrain the viewing time loss model 206 using at least the input video 221.
As another example, in some implementations, the features 216 used to train the viewing time loss model 206 may be refined over time (i.e., as training progresses) by the feature refiner 230. For example, the feature importance calculator 231 may calculate the feature importance for each type of feature that may be included in the features 216 (e.g., various types of video, audio, or semantic features that may be included in the features 216). The feature importance may be determined by the feature importance calculator 231 based on which features of the training video 204 correspond to the viewing time lost regions identified by the viewing time lost region identifier 212, with a higher number of viewing time lost regions corresponding to features resulting in higher feature importance and a lower number of viewing time lost regions corresponding to features resulting in lower feature importance. The feature refiner 230 may rank the features by feature importance and the feature refiner 230 may remove features with feature importance below a threshold from the features 216 used to train the viewing time loss model 206.
As another example, a feature ablation (ablation) procedure may be performed by feature ablator 232. For example, in each training video 204, each feature 216 may be iteratively and individually ablated (e.g., with noise) by a feature ablator 232. Feature extractor 219 may extract features from each of the training videos of ablations (e.g., including features that ignore ablations) and provide the features extracted from each of the training videos of ablations to viewing time loss model 206. The feature refiner 230 may compare the predicted viewing time loss region 222 generated by the viewing time loss model 206 based on features extracted from the ablated training video with the viewing time loss region previously generated by the viewing time loss region identifier 212 for the corresponding unablated training video 204. If the predicted viewing time loss region 222 generated by the viewing time loss model 206 based on features extracted from the ablated training video is less than (e.g., in count and/or duration) the predicted viewing time loss region of the corresponding unablated training video 204, the feature refiner 230 may determine that the ablated features are important to identify the viewing time loss region. That is, in the absence of this feature, fewer lost viewing time regions are identified. If the predicted viewing time loss region 222 generated by the viewing time loss model 206 based on features extracted from the ablated training video is not less than (e.g., in count and/or duration) the predicted viewing time loss region of the corresponding unablated training video 204, the feature refiner 230 may determine that the ablated features are not important to identify the viewing time loss region. That is, even if the feature is ablated, the lost-view region of the training video is still identified. The feature refiner 230 may remove features that are determined to be unimportant by the feature refiner 230 after ablation from the features 216 used to train the viewing time loss model 206.
Fig. 5 illustrates an example architecture of a viewing time loss model 500. For example, the viewing time loss model 500 may be the viewing time loss model 206 described above with reference to fig. 2. The viewing time loss model 500 includes a bidirectional RNN (recurrent neural network) 516. The bi-directional RNN 516 may include a set of Gated Round Units (GRUs), including a GRU 518 and a GRU 520. For each time T in the input video, the concatenated features 502 and 504 may be provided (e.g., by the feature extractor 219) as inputs to the bidirectional RNN 516, and in particular, as inputs to the respective GRUs of the bidirectional RNN 516. Feature 502, which may comprise a concatenation of original embedded feature 506, visual feature 508, and audio feature 510, includes features from, for example, 15 seconds before time T to time T. Similarly, features 504, which may include a concatenation of original embedded features 512, visual features 514, and audio features 515, include features from, for example, time T to 15 seconds after time T. Although a 15 second time window is described, other time window lengths (e.g., 6 seconds, 9 seconds, 20 seconds) may also be used. Practical results indicate that selecting time windows both before time T and after time T can facilitate model training and prediction by providing more context about the video to the view time loss model 500. Selecting the time window length may involve a trade-off decision. A longer period of time may provide more context for the view time loss model with respect to the video, but may require more data for training and/or more model complexity (e.g., more GRUs).
The bi-directional RNN 516 may include a series of steps, each step having a feature at a one second time stamp in the video. For example, with respect to the 15 second time window example described above, the bi-directional RNN 516 may include 31 steps (e.g., corresponding to 15 seconds before time T, time T itself, and 15 seconds after time T). As another example, in some implementations, the bidirectional RNN 516 may use an LSTM (long short term memory) method.
Above the bi-directional RNN 516, a fully connected deep neural network 522 may be used as an additional layer to the viewing time loss model 500. For example, the outputs of the GRUs of the bi-directional RNN 516 (e.g., a vector of 31 scaler values) may be fed to a fully-connected ReLu (corrected linear units) layer of the fully-connected deep neural network 522. For example, the fully-connected deep neural network 522 may use ReLU as the activation function. Various numbers of layers may be used in the fully-connected deep neural network 522.
The output layer of the viewing time loss model 500 may use a sigmoid (sigmoid) function 524, which may convert the output from the fully-connected deep neural network 522 into a real prediction value 526 between 0 and 1, the prediction value 526 representing the likelihood that time T is a viewing time loss region in the input video. The larger the prediction value 526, the more likely T is a viewing time loss region in the input video. In more detail and as other examples, sigmoid function 524 may generate a score (e.g., a binary score of 0 or 1, or a score that occurs in a range such as from 0 to 1) for each region of the input video. The score for each region indicates the likelihood that the region is a viewing time loss region. Sigmoid function 524 may determine that a particular region is a viewing time lost region if its score meets (e.g., meets or exceeds) a certain threshold (e.g., 1 or greater than 0.5). On the other hand, if the score of a particular region does not meet (e.g., is less than) a certain threshold (e.g., 1 or 0.5), the sigmoid function may determine that the particular region is not a viewing time lost region. For example, the administrator may set the certain threshold. As another example, the certain threshold may be automatically determined.
Other types of architectures may be used to view the time loss model 500. As described above, the LSTM model may be used instead of the GRU. As another example, in some implementations, both LSTM models and GRUs may be used.
FIG. 6 is a flow diagram of an example process 600 for determining a viewing time loss region in a media content item. The operations of process 600 are described below as being performed by components of the systems described and depicted in fig. 1-5. The following description of the operation of process 600 is for illustration purposes only. The operations of process 600 may be performed by any suitable device or system, such as any suitable data processing apparatus. The operations of process 600 may also be implemented as instructions stored on a computer-readable medium, which may be non-transitory. Execution of the instructions causes one or more data processing apparatus to perform the operations of process 600.
The video processing system 110 receives a first video (at 602). For example, as described above with reference to fig. 2, the input video 221 may be received, such as in response to a user input from an administrator selecting the input video 221 or a user input from a batch process that triggers processing of the input video 221 (and other videos). Although video content is described, process 600 may also be used for other media content, such as audio content.
The video processing system 110 determines a set of features of the first video (at 604). For example, as described above with reference to fig. 2, the feature extractor 219 may determine the features 224 of the input video 221. The set of features may include video features, audio features, or semantic features.
The video processing system 110 inputs the set of features of the first video into the trained model (at 606). For example, as described above with reference to fig. 2, the features 224 may be provided as input to a trained model such as the viewing time loss model 206. As discussed above with respect to the view time loss model 500, the trained model may be or include a bidirectional recurrent neural network, a fully connected deep neural network, an LSTM network, and/or other machine learning engines or systems (as described above with reference to fig. 5). The trained model is trained to output viewing time lost regions in a particular video, as described above with reference to fig. 2-5 and summarized below. The watch time lost region of a particular video defines a time window of the particular video in which a likelihood that a user stops playback of the particular video is greater than a threshold likelihood. The trained model is trained using the known viewing time loss region of the plurality of training videos and features of the plurality of training videos corresponding to the known viewing time loss region. For example, as described above with reference to fig. 2, the viewing time loss model 206 is trained on the features 216 of the training video 204 using the tags 214, the tags 214 being based on the viewing time loss regions of the training video 204 identified by the viewing time loss region identifier 212.
Training the viewing time loss model may include: identifying a set of training videos; generating a viewing time loss curve for each training video; determining a viewing time loss region in each viewing time loss curve; and training the trained model using the viewing time loss region in the viewing time loss curve. For example, as described above with reference to fig. 2, the viewing time loss curve generator 210 may generate a viewing time loss curve for each training video 204. Viewing time loss model 206 may be trained by model trainer 215 using labels 214, labels 214 being based on viewing time loss regions identified in a viewing time loss curve by viewing time loss region identifier 212. A view time loss curve for a particular training video may be generated by view time loss curve generator 210 based on interaction data from a user activity log that includes, for example, start and stop times of a user viewing session for the particular training video. For example, as described above with reference to fig. 2, the viewing time loss curve generator 210 may generate a viewing time loss curve based on the interaction data in the user activity log 208.
In some implementations, determining a viewing time region in the viewing time loss curve may include determining an average slope of the viewing time loss curve. For example, as described above with reference to fig. 3, line 322 depicts the average slope of the view-time-loss curve 312. The average slope of the viewing time loss curve may correspond to a threshold likelihood for determining the viewing time loss region. For example, a region of the viewing time loss curve having a slope greater than the average slope may be identified as a viewing time loss region in the viewing time loss curve. For example, as described above with reference to fig. 3, viewing time loss regions 324, 326, and 328 have been identified as viewing time loss regions based on having a slope greater than the average slope of viewing time loss curve 312.
In some implementations, the first viewing time loss region at the beginning of the viewing time loss curve and the last viewing time loss region at the end of the viewing time loss curve are not used to train the trained model. For example, as described above with reference to fig. 2 and 3, the viewing time loss regions corresponding to curve portions 318 and 320 are not used to train viewing time loss model 206, although the slopes of those portions of the viewing time loss curve are greater than the average slope.
Training the trained model using the viewing time loss region in the viewing time loss curve may include: for each viewing time loss region determined for a particular training video, determining features of the particular training video that occur within a predetermined time window of the viewing time loss region; and training the trained model using features of the particular training video that occur within a predetermined time window of the viewing time loss region determined for the particular training video. For example, as described above with reference to fig. 2 and 3, the feature extractor 219 may generate as the features 216 the features of the training video 204 within the predetermined time window of the viewing time loss regions 324, 326, and 328.
The video processing system 110 obtains data about the viewing time lost region of the first video from the trained model (at 608). For example, as described above with reference to fig. 2, the viewing time loss model 206 may output a predicted viewing time loss region 222 of the input video 221.
The video processing system 110 provides data regarding the identified viewing time loss region to the entity participating in providing the first video to the user (at 610). For example, as described above with reference to fig. 1, data regarding the identified viewing time loss zone may be provided to the client device 102 of the owner of the first video. As another example, data regarding the viewing time loss region may be provided to the content provider 108 for determining a point in time within the first video at which the digital component is displayed. For example, as described above with reference to fig. 1, the content provider 108 may determine to schedule presentation of the digital component prior to the determined viewing time loss region. As yet another example, the content provider 108 may use the data regarding the identified viewing time loss region to determine a service decision of when to provide the digital component to the client device 102. For example, a first set of digital components scheduled to play before the determined viewing time loss zone may be sent to client device 102 based on priority (e.g., before the first set of digital components is scheduled to play, possibly even with the video being provided), and a second set of digital components scheduled to play after the first determined viewing time loss zone may be configured (e.g., by content provider 108) to be provided on-demand (e.g., just as the digital components are scheduled to play). That is, since some users may tend to stop watching video during the first determined loss of viewing time region, the content provider 108 may provide some high priority digital components before the first loss of viewing time region and lower priority digital components before each subsequent loss of viewing time region.
The data regarding the viewing time loss region provided to the client device 102 of the owner of the first video may be presented as quality information of the first video. The quality information may indicate a count and/or a duration of the determined viewing time loss region of the first video. If the quality information indicates a low or otherwise unacceptable quality (e.g., the count or duration of the viewing time zone is above a predetermined threshold), the owner may use the information of the viewing time lost zone to adjust those portions of the first video and re-upload the first video (e.g., to the content platform 106) as an updated version of the first video. The video processing system 110 may process the updated version of the first video using the trained model to generate updated viewing time loss information for the updated version of the first video and provide the updated viewing time loss information for the updated version of the first video to the owner of the first video. The owner can repeat the adjustment and upload until an acceptable quality level is reached.
FIG. 7 is a block diagram of an example computer system 700 that may be used to perform the operations described above. The system 700 includes a processor 710, a memory 720, a storage device 730, and an input/output device 740. Each of the components 710, 720, 730, and 740 can be interconnected, e.g., using a system bus 750. Processor 710 is capable of processing instructions for execution within system 700. In some implementations, the processor 710 is a single-threaded processor. In another implementation, the processor 710 is a multi-threaded processor. Processor 710 is capable of processing instructions stored in memory 720 or on storage device 730.
The storage device 730 is capable of providing mass storage for the system 700. In some implementations, the storage device 730 is a computer-readable medium. In various different implementations, the storage device 730 may include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices over a network (e.g., a cloud storage device), or some other mass storage device.
The input/output device 740 provides input/output operations for the system 700. In some implementations, the input/output devices 740 can include one or more of a network interface device (e.g., an ethernet card), a serial communication device (e.g., an RS-232 port), and/or a wireless interface device (e.g., an 802.11 card). In another implementation, the input/output devices can include driver devices configured to receive input data and send output data to peripheral devices 760 (e.g., keyboards, printers, and display devices). However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and so forth.
Although an example processing system has been described in FIG. 7, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage medium (or media) for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus. The computer storage medium may be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, while a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be included in one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification can be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones or combinations of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). In addition to hardware, the apparatus can include code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with the instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Further, the computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having: a display device for displaying information to a user, such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor; and a keyboard and pointing device, such as a mouse or trackball, by which a user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on the user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server; or include middleware components, such as application servers; or include a front-end component, such as a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification; or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the client device (e.g., for the purpose of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) may be received at the server from the client device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Some features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some implementations, multitasking and parallel processing may be advantageous.
Claims (13)
1. A computer-implemented method, comprising:
receiving a first video;
determining a set of features of a first video;
inputting the set of features of the first video into the trained model, wherein (1) the trained model is trained to output a viewing time lost region in a particular video, (2) the trained model is trained using labels corresponding to known viewing time lost regions in a plurality of training videos and features of the plurality of training videos corresponding to known viewing time lost regions, and (3) the viewing time lost region of the particular video defines a time window of the particular video during which a likelihood that a user stops playback of the particular video is greater than a threshold likelihood;
in response to inputting the set of features of the first video into the trained model, obtaining data about a viewing time loss region of the first video from the trained model; and
data regarding a viewing time loss region of a first video is provided to an entity participating in providing the first video to a user.
2. The computer-implemented method of claim 1, wherein the data regarding the loss of viewing time region of the first video is provided to a content provider, the content provider using the data regarding the loss of viewing time region of the first video to determine a point in time in the first video at which to play a digital component comprising content different from the content in the first video.
3. The computer-implemented method of claim 2, wherein the content provider uses the data regarding the viewing time loss region of the first video to determine a service decision of when to provide the digital content to the client device during playback of the first video.
4. The computer-implemented method of any preceding claim, wherein data regarding a loss of viewing time region of the first video is provided to an owner of the first video, the method further comprising:
receiving an updated version of the first video that has been modified by an owner of the first video;
processing the updated version of the first video using the trained model to generate update data for the viewing time lost region of the updated version of the first video; and
update data regarding a viewing time lost region of an updated version of the first video is provided to an owner of the first video.
5. The computer-implemented method of any preceding claim, further comprising training the trained model using a set of training videos, wherein training the trained model comprises:
for each training video in the set of training videos:
generating a viewing time loss curve of the training video;
determining a viewing time loss region in a viewing time loss curve; and
the trained model is trained using the viewing time loss region in the viewing time loss curve for the set of training videos.
6. The computer-implemented method of claim 5, wherein determining a viewing time region in a viewing time loss curve comprises:
determining an average slope of a viewing time loss curve, wherein the average slope corresponds to a threshold likelihood; and
a region of the viewing time loss curve having a slope greater than the average slope is determined as a viewing time loss region in the viewing time loss curve.
7. The computer-implemented method of claim 5 or 6, wherein training the trained model using the viewing time loss region in the viewing time loss curve for the set of training videos comprises: the trained model is trained using a different viewing time loss region than the first viewing time loss region at the beginning of the viewing time loss curve and the last viewing time loss region at the end of the viewing time loss curve.
8. The computer-implemented method of any of claims 5 to 7, wherein training the trained model using a viewing time loss region in a viewing time loss curve comprises:
for each viewing time loss region determined for a particular training video, determining features of the particular training video that occur within a predetermined time window of the viewing time loss region; and
the trained model is trained using the determined features of the particular training video.
9. The computer-implemented method of any of claims 5 to 8, wherein the view time loss curve for a particular training video is generated from interaction data obtained from a user activity log that includes start and stop times of user viewing sessions for the particular training video.
10. The computer-implemented method of any preceding claim, wherein the trained model comprises a bi-directional recurrent neural network and a fully-connected deep neural network.
11. The computer-implemented method of any preceding claim, wherein the set of features of the first video and the features of the plurality of training videos corresponding to known viewing time loss regions comprise video features, audio features, or semantic features.
12. A system, comprising:
one or more memory devices to store instructions; and
one or more data processing apparatus configured to interact with the one or more memory devices and, when executing instructions, to carry out operations according to any one of claims 1 to 11.
13. A computer-readable medium storing instructions that, when executed by one or more data processing apparatus, cause the one or more data processing apparatus to perform operations according to any one of claims 1 to 11.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/043487 WO2023009114A1 (en) | 2021-07-28 | 2021-07-28 | Determining watch time loss regions in media content items |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115885322A true CN115885322A (en) | 2023-03-31 |
Family
ID=77412360
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180046570.2A Pending CN115885322A (en) | 2021-07-28 | 2021-07-28 | Determining a viewing time loss region in a media content item |
Country Status (6)
Country | Link |
---|---|
US (1) | US11949924B2 (en) |
EP (1) | EP4154545A1 (en) |
JP (1) | JP2023538802A (en) |
KR (1) | KR20230018453A (en) |
CN (1) | CN115885322A (en) |
WO (1) | WO2023009114A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116775752B (en) * | 2023-08-25 | 2023-11-17 | 广东南方电信规划咨询设计院有限公司 | Method and device for carrying out visualization processing on data |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8959540B1 (en) | 2009-05-27 | 2015-02-17 | Google Inc. | Predicting engagement in video content |
US9820004B1 (en) | 2013-12-03 | 2017-11-14 | Google Inc. | Optimizing timing of display of a video overlay |
US9736503B1 (en) | 2014-09-12 | 2017-08-15 | Google Inc. | Optimizing timing of display of a mid-roll video advertisement based on viewer retention data |
US9659218B1 (en) * | 2015-04-29 | 2017-05-23 | Google Inc. | Predicting video start times for maximizing user engagement |
US10856022B2 (en) | 2017-10-02 | 2020-12-01 | Facebook, Inc. | Dynamically providing digital content to client devices by analyzing insertion points within a digital video |
-
2021
- 2021-07-28 JP JP2022580975A patent/JP2023538802A/en active Pending
- 2021-07-28 KR KR1020227045961A patent/KR20230018453A/en unknown
- 2021-07-28 CN CN202180046570.2A patent/CN115885322A/en active Pending
- 2021-07-28 US US17/928,230 patent/US11949924B2/en active Active
- 2021-07-28 WO PCT/US2021/043487 patent/WO2023009114A1/en unknown
- 2021-07-28 EP EP21758238.6A patent/EP4154545A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230319326A1 (en) | 2023-10-05 |
KR20230018453A (en) | 2023-02-07 |
WO2023009114A1 (en) | 2023-02-02 |
EP4154545A1 (en) | 2023-03-29 |
JP2023538802A (en) | 2023-09-12 |
US11949924B2 (en) | 2024-04-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20220417567A1 (en) | Computerized system and method for automatic highlight detection from live streaming media and rendering within a specialized media player | |
US11290775B2 (en) | Computerized system and method for automatically detecting and rendering highlights from streaming videos | |
TWI409691B (en) | Comment filters for real-time multimedia broadcast sessions | |
US20170055014A1 (en) | Processing video usage information for the delivery of advertising | |
US11153619B2 (en) | Cognitively derived multimedia streaming preferences | |
CN109829064B (en) | Media resource sharing and playing method and device, storage medium and electronic device | |
CN113742567A (en) | Multimedia resource recommendation method and device, electronic equipment and storage medium | |
CN113766268B (en) | Video processing method and device, electronic equipment and readable medium | |
US11949924B2 (en) | Determining watch time loss regions in media content items | |
JP6823170B2 (en) | Prediction of interruptions in the content stream | |
US20230142444A1 (en) | Generating breakpoints in media playback | |
US11564013B2 (en) | Content recommendation generation using content based on bi-directional prediction | |
CN111343483B (en) | Method and device for prompting media content segment, storage medium and electronic device | |
WO2022060956A1 (en) | User interfaces for refining video group packages | |
US20230222151A1 (en) | Determining types of digital components to provide | |
Reveiu et al. | Using content-based multimedia data retrieval for multimedia content adaptation | |
WO2022060930A1 (en) | Digital video analysis | |
CN112840667A (en) | Method and system for classification and categorization of video paths in interactive video |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |