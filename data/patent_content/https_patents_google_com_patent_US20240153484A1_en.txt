US20240153484A1 - Massive multilingual speech-text joint semi-supervised learning for text-to-speech - Google Patents
Massive multilingual speech-text joint semi-supervised learning for text-to-speech Download PDFInfo
- Publication number
- US20240153484A1 US20240153484A1 US18/494,324 US202318494324A US2024153484A1 US 20240153484 A1 US20240153484 A1 US 20240153484A1 US 202318494324 A US202318494324 A US 202318494324A US 2024153484 A1 US2024153484 A1 US 2024153484A1
- Authority
- US
- United States
- Prior art keywords
- tts
- speech
- utterances
- utterance
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012549 training Methods 0.000 claims abstract description 240
- 238000000034 method Methods 0.000 claims abstract description 97
- 238000013518 transcription Methods 0.000 claims description 45
- 230000035897 transcription Effects 0.000 claims description 45
- 230000015654 memory Effects 0.000 claims description 33
- 238000012545 processing Methods 0.000 claims description 16
- 230000001143 conditioned effect Effects 0.000 claims description 6
- 230000000306 recurrent effect Effects 0.000 claims description 6
- 238000004891 communication Methods 0.000 claims description 5
- 230000001537 neural effect Effects 0.000 claims description 5
- 230000008569 process Effects 0.000 description 62
- 239000013598 vector Substances 0.000 description 29
- 230000000873 masking effect Effects 0.000 description 10
- 230000006870 function Effects 0.000 description 9
- 201000007201 aphasia Diseases 0.000 description 8
- 238000004590 computer program Methods 0.000 description 8
- 230000004913 activation Effects 0.000 description 7
- 238000001994 activation Methods 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 6
- 238000005070 sampling Methods 0.000 description 4
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 239000000284 extract Substances 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 230000001737 promoting effect Effects 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 230000002123 temporal effect Effects 0.000 description 2
- ATJFFYVFTNAWJD-UHFFFAOYSA-N Tin Chemical compound [Sn] ATJFFYVFTNAWJD-UHFFFAOYSA-N 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 229940082150 encore Drugs 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000003786 synthesis reaction Methods 0.000 description 1
- 230000002194 synthesizing effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/04—Details of speech synthesis systems, e.g. synthesiser structure or memory management
- G10L13/047—Architecture of speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
Abstract
A method includes receiving training data that includes a plurality of sets of text-to-speech (TTS) spoken utterances each associated with a respective language and including TTS utterances of synthetic speech spoken that includes a corresponding reference speech representation paired with a corresponding input text sequence. For each TTS utterance in each set of the TTS spoken training utterances of the received training data, the method includes generating a corresponding TTS encoded textual representation for the corresponding input text sequence, generating a corresponding speech encoding for the corresponding TTS utterance of synthetic speech, generating a shared encoder output, generating a predicted speech representation for the corresponding TTS utterance of synthetic speech, and determining a reconstruction loss. The method also includes training a TTS model based on the reconstruction losses determined for the TTS utterances in each set of the TTS spoken training utterances.
Description
- This U.S. Patent Application claims priority under 35 U.S.C. § 119(e) to U.S. Provisional Application 63/381,077, filed on Oct. 26, 2022. The disclosure of this prior application is considered part of the disclosure of this application and is hereby incorporated by reference in its entirety.
- This disclosure relates to massive multilingual speech-text joint semi-supervised learning for text-to-speech.
- Text-to-speech (TTS) systems read aloud digital text to a user and are becoming increasingly popular on mobile devices. Certain TTS models aim to synthesize various aspects of speech, such as speaking styles and languages, to produce human-like, natural sounding speech. Some TTS models are multilingual such that the TTS model outputs synthetic speech in multiple different languages. However, even these multilingual TTS models are only compatible with a relatively small portion of all the languages spoken in the world. Particularly, a lack of sufficient training data in other languages, especially low-resource languages, inhibits TTS models from learning to generate synthetic speech in these other languages. As such, training a multilingual TTS model to generate synthetic speech in many different languages, even for low-resource languages, would further increase the use of TTS models.
- One aspect of the disclosure provides a computer-implemented method that when executed on data processing hardware causes the data processing hardware to perform operations for massive multilingual speech-text joint semi-supervised learning for text-to-speech. The operations include receiving training data that includes a plurality of sets of text-to-speech (TTS) spoken utterances. Each set of the TTS spoken utterances is associated with a respective language from among a plurality of different languages that is different than the respective languages associated with each other set of the TTS spoken utterances and includes TTS utterances of synthetic speech spoken in the respective language. Each TTS utterance of synthetic speech includes a corresponding reference speech representation paired with a corresponding input text sequence. For each TTS utterance in each set of the TTS spoken utterances of the received training data, the operations include: generating a corresponding TTS encoded textual representation for the corresponding input text sequence using a text encoder, generating a corresponding speech encoding for the corresponding TTS utterance of synthetic speech using a speech encoder, generating a shared encoder output using a shared encoder configured to receive the corresponding TTS encoded textual representation or the corresponding speech encoding, and determining a reconstruction loss based on the predicted speech representation and the corresponding reference speech representation for the corresponding TTS utterance. The operations also include training a TTS model based on the reconstruction losses determined for the TTS utterances in each set of the TTS spoken training utterances to teach the TTS model to learn how to synthesize speech in each of the plurality of different languages.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, for each TTS utterance in each set of the TTS spoken training utterances of the received training data, the operations further include obtaining a corresponding speaker embedding characterizing speaker characteristics of a corresponding speaker that spoke the TTS utterance of synthetic speech in the respective language and obtaining a corresponding variational embedding that specifies an intended prosody/style for the predicted speech representation generated for the corresponding TTS utterance of synthetic speech. In these implementations, the text encoder is configured to receive a concatenation of the corresponding input text sequence and the corresponding speaker embedding when generating the corresponding TTS encoded textual representation for the corresponding input text sequence and the speech decoder is conditioned on the corresponding variational embedding and the corresponding speaker embedding when generating the predicted speech representation for the corresponding TTS utterance of synthetic speech. In some examples, for each TTS utterance in each set of the TTS spoken training utterances of the received training data, the operations further include generating, using an automatic speech recognition (ASR) decoder configured to receive the shared encoder output, a sequence of speech recognition hypotheses representing a candidate transcription for the corresponding TTS utterance of synthetic speech and determining an ASR loss based on the sequence of speech recognition hypotheses and the corresponding input text sequence. Here, training the TTS model is further based on the ASR losses determined for the TTS utterances in each set of the TTS spoken training utterances.
- The training data may further include a plurality of sets of automatic speech recognition (ASR) transcribed utterances each associated with a respective language that is different than the respective language associated with each other set of the ASR transcribed utterances and including ASR utterances of non-synthetic speech spoken in the respective language where each ASR utterance of non-synthetic speech is paired with a corresponding transcription and training the TTS model includes training the TTS model on the plurality of sets of ASR transcribed utterances. The speech decoder may include a recurrent neural network-transducer (RNN-T) architecture. In some implementations, the operations further include determining consistency losses between the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and the TTS encoded textual representations generated for the input text sequences. In these implementations, training the TTS model is further based on the consistency losses.
- In some examples, the operations further include determining modality matching losses between the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and the TTS encoded textual representations generated for the input text sequences. In these examples, training the TTS model is further based on the modality matching losses. In some implementations, for each TTS utterance in each set of the TTS spoken training utterances of the received training data, the operations further include obtaining a sequence representation of the corresponding input text sequence concatenated with a variational embedding, using a duration model network to predict a duration of the input text sequence based on the sequence representation and upsample the sequence representation into a upsampled output specifying a number of frames, and determining a duration loss based on the predicted duration of the input text sequence and a ground-truth duration. In these implementations, generating the predicted speech representation for the corresponding TTS utterance of synthetic speech using the speech decoder configured to receive the shared encoder output is based on the upsampled output and training the TTS model further includes training the TTS model on the duration losses determined the TTS utterances in each set of the TTS spoken training utterances.
- The operations may further include obtaining a masked language modeling (MLM) loss for the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and obtaining an aligned MLM loss for the TTS encoded textual representations generated for the input text sequences using the text encoder. Here, training the TTS model further includes training the TTS model on the MLM loss and the aligned MLM loss. In some examples, the training data further includes unspoken textual utterances in a respective plurality of different languages where each unspoken textual utterance is not paired with any corresponding spoken utterance of synthetic speech and, for each unspoken textual utterance, the operations further include generating a corresponding unspoken encoded textual representation for the corresponding unspoken textual utterance using the text encoder and obtaining an aligned masked language modeling (MLM) loss for the corresponding unspoken encoded textual representation generated for the corresponding unspoken textual utterance. In these examples, training the TTS model further includes training the TTS model based on the aligned MLM loss obtained for the unspoken encoded textual representation.
- In some implementations, the training data further includes un-transcribed non-synthetic speech utterances in a respective plurality of different languages where each un-transcribed non-synthetic speech utterance is not paired with a corresponding transcription and, for each un-transcribed non-synthetic speech utterance, the operations further include generating a corresponding speech encoding for the corresponding un-transcribed non-synthetic speech utterance using the speech encoder and obtaining a masked language modeling (MLM) loss for the corresponding speech encoding generated for the corresponding un-transcribed non-synthetic speech utterance. In these implementations, training the TTS model further includes training the TTS model based on the MLM loss obtained for the corresponding speech encoding. The TTS model may include the text encoder and the speech decoder. In some examples, each corresponding input text sequence includes a sequence of graphemes, word-piece-model units, phonemes, or bytes.
- Another aspect of the disclosure provides a system that includes data processing hardware and memory hardware storing instructions that when executed on the data processing hardware causes the data processing hardware to perform operations. The operations include receiving training data that includes a plurality of sets of text-to-speech (TTS) spoken utterances. Each set of the TTS spoken utterances is associated with a respective language from among a plurality of different languages that is different than the respective languages associated with each other set of the TTS spoken utterances and includes TTS utterances of synthetic speech spoken in the respective language. Each TTS utterance of synthetic speech includes a corresponding reference speech representation paired with a corresponding input text sequence. For each TTS utterance in each set of the TTS spoken utterances of the received training data, the operations include: generating a corresponding TTS encoded textual representation for the corresponding input text sequence using a text encoder, generating a corresponding speech encoding for the corresponding TTS utterance of synthetic speech using a speech encoder, generating a shared encoder output using a shared encoder configured to receive the corresponding TTS encoded textual representation or the corresponding speech encoding, and determining a reconstruction loss based on the predicted speech representation and the corresponding reference speech representation for the corresponding TTS utterance. The operations also include training a TTS model based on the reconstruction losses determined for the TTS utterances in each set of the TTS spoken training utterances to teach the TTS model to learn how to synthesize speech in each of the plurality of different languages.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, for each TTS utterance in each set of the TTS spoken training utterances of the received training data, the operations further include obtaining a corresponding speaker embedding characterizing speaker characteristics of a corresponding speaker that spoke the TTS utterance of synthetic speech in the respective language and obtaining a corresponding variational embedding that specifies an intended prosody/style for the predicted speech representation generated for the corresponding TTS utterance of synthetic speech. In these implementations, the text encoder is configured to receive a concatenation of the corresponding input text sequence and the corresponding speaker embedding when generating the corresponding TTS encoded textual representation for the corresponding input text sequence and the speech decoder is conditioned on the corresponding variational embedding and the corresponding speaker embedding when generating the predicted speech representation for the corresponding TTS utterance of synthetic speech. In some examples, for each TTS utterance in each set of the TTS spoken training utterances of the received training data, the operations further include generating, using an automatic speech recognition (ASR) decoder configured to receive the shared encoder output, a sequence of speech recognition hypotheses representing a candidate transcription for the corresponding TTS utterance of synthetic speech and determining an ASR loss based on the sequence of speech recognition hypotheses and the corresponding input text sequence. Here, training the TTS model is further based on the ASR losses determined for the TTS utterances in each set of the TTS spoken training utterances.
- The training data may further include a plurality of sets of automatic speech recognition (ASR) transcribed utterances each associated with a respective language that is different than the respective language associated with each other set of the ASR transcribed utterances and including ASR utterances of non-synthetic speech spoken in the respective language where each ASR utterance of non-synthetic speech is paired with a corresponding transcription and training the TTS model includes training the TTS model on the plurality of sets of ASR transcribed utterances. The speech decoder may include a recurrent neural network-transducer (RNN-T) architecture. In some implementations, the operations further include determining consistency losses between the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and the TTS encoded textual representations generated for the input text sequences. In these implementations, training the TTS model is further based on the consistency losses.
- In some examples, the operations further include determining modality matching losses between the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and the TTS encoded textual representations generated for the input text sequences. In these examples, training the TTS model is further based on the modality matching losses. In some implementations, for each TTS utterance in each set of the TTS spoken training utterances of the received training data, the operations further include obtaining a sequence representation of the corresponding input text sequence concatenated with a variational embedding, using a duration model network to predict a duration of the input text sequence based on the sequence representation and upsample the sequence representation into a upsampled output specifying a number of frames, and determining a duration loss based on the predicted duration of the input text sequence and a ground-truth duration. In these implementations, generating the predicted speech representation for the corresponding TTS utterance of synthetic speech using the speech decoder configured to receive the shared encoder output is based on the upsampled output and training the TTS model further includes training the TTS model on the duration losses determined the TTS utterances in each set of the TTS spoken training utterances.
- The operations may further include obtaining a masked language modeling (MLM) loss for the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and obtaining an aligned MLM loss for the TTS encoded textual representations generated for the input text sequences using the text encoder. Here, training the TTS model further includes training the TTS model on the MLM loss and the aligned MLM loss. In some examples, the training data further includes unspoken textual utterances in a respective plurality of different languages where each unspoken textual utterance is not paired with any corresponding spoken utterance of synthetic speech and, for each unspoken textual utterance, the operations further include generating a corresponding unspoken encoded textual representation for the corresponding unspoken textual utterance using the text encoder and obtaining an aligned masked language modeling (MLM) loss for the corresponding unspoken encoded textual representation generated for the corresponding unspoken textual utterance. In these examples, training the TTS model further includes training the TTS model based on the aligned MLM loss obtained for the unspoken encoded textual representation.
- In some implementations, the training data further includes un-transcribed non-synthetic speech utterances in a respective plurality of different languages where each un-transcribed non-synthetic speech utterance is not paired with a corresponding transcription and, for each un-transcribed non-synthetic speech utterance, the operations further include generating a corresponding speech encoding for the corresponding un-transcribed non-synthetic speech utterance using the speech encoder and obtaining a masked language modeling (MLM) loss for the corresponding speech encoding generated for the corresponding un-transcribed non-synthetic speech utterance. In these implementations, training the TTS model further includes training the TTS model based on the MLM loss obtained for the corresponding speech encoding. The TTS model may include the text encoder and the speech decoder. In some examples, each corresponding input text sequence includes a sequence of graphemes, word-piece-model units, phonemes, or bytes.
- The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a schematic view of an example speech recognition system. -
FIG. 2 is a schematic view of an example automatic speech recognition model. -
FIGS. 3A-3C are schematic views of an example training process for training a text-to-speech (TTS) model using sets of ASR transcribed utterances. -
FIG. 4 is a schematic view of an example alignment model used during the example training process. -
FIGS. 5A-5C are schematic views of an example training process for training the TTS model using sets of TTS transcribed utterances. -
FIG. 6 is a flowchart of an example arrangement of operations for a method of training a massive multilingual TTS model. -
FIG. 7 is a schematic view of an example computing device that may be used to implement the systems and methods described herein. - Like reference symbols in the various drawings indicate like elements.
- Text-to-speech is the process generating synthetic speech based on input textual data. In some instances, TTS models are multilingual whereby the TTS model may receive a text input and generate synthetic speech corresponding to the text input in multiple different languages. Recently, TTS models have made significant advances in synthesizing human-like high-quality speech in multiple languages. Yet, even multilingual TTS models are only capable of generating synthetic speech in a few different languages. A major obstacle preventing TTS models from scaling to hundreds or even thousands of different languages is the difficulty in collecting a large quantity of high-quality paired training data in each of the different languages that is required to train the TTS model. In particular, low-resource languages have a very scarce amount of (or even zero) paired training data thereby further increasing the difficulty of scaling TTS models to these low-resource languages.
- Accordingly, implementations herein are directed towards methods and systems for training a massive multilingual TTS model using speech-text joint semi-supervised learning. That is, a training process may receive training data that includes a plurality of sets of TTS spoken utterances. Each set of TTS spoken utterances is associated with a respective language different than the respective languages associated with each other set of TTS spoken utterances. Moreover, each set of TTS spoken utterances includes TTS utterances of synthetic speech in the respective language. Here, each TTS utterance of synthetic speech includes a corresponding reference speech representation paired with a corresponding input text sequence. For each TTS utterance in each set of the TTS spoken training utterances, the training process generates a corresponding TTS encoded textual representation using a text encoder, generates a corresponding speech encoding using a speech encoder, generates a shared encoder output based on the corresponding TTS encoded textual representation or the corresponding speech encoding using a shared encoder, generates a predicted speech representation based on the shared encoder output using a speech decoder, and determines a reconstruction loss based on the predicted speech representation and the corresponding reference speech representation.
- Notably, the training process may employ one or more components (e.g., speech encoder and/or text encoder) of an automatic speech recognition (ASR) model to train the multilingual TTS model. In some examples, the ASR model and the TTS model share the same text encoder. In other examples, the ASR model and the TTS model each include a respective text encoder. Finally, the training process trains the multilingual TTS model based on the reconstruction losses determined for the TTS utterances in each set of the TTS spoken training utterances to teach the TTS model to learn how to synthesize speech in each of the plurality of different languages. More specifically, the training process may update parameters of the text encoder of the TTS model based on the reconstruction losses.
-
FIG. 1 illustrates anexample system 100 implementing an automated speech recognition (ASR)model 200 and a text-to-speech (TTS)model 501 that resides on auser device 102 of auser 104 and/or on a remote computing device 201 (e.g., one or more servers of a distributed system executing in a cloud-computing environment) in communication with theuser device 102. Although theuser device 102 is depicted as a mobile computing device (e.g., a smart phone), theuser device 102 may correspond to any type of computing device such as, without limitation, a tablet device, a laptop/desktop computer, a wearable device, a digital assistant device, a smart speaker/display, a smart appliance, an automotive infotainment system, or an Internet-of-Things (IoT) device, and is equipped withdata processing hardware 111 andmemory hardware 113. - The
user device 102 includes anaudio subsystem 108 configured to receive anutterance 106 spoken by the user 104 (e.g., theuser device 102 may include one or more microphones for recording the spoken utterance 106) and convert theutterance 106 into a corresponding digital format associated with inputacoustic frames 110 capable of being processed by theASR system 100. In the example shown, the user speaks arespective utterance 106 in a natural language of English for the phrase “What is the weather in New York City?” and theaudio subsystem 108 converts theutterance 106 into correspondingacoustic frames 110 for input to theASR system 100. Thereafter, theASR model 200 receives, as input, theacoustic frames 110 corresponding to theutterance 106, and generates/predicts, as output, a corresponding transcription 120 (e.g., recognition result/hypothesis) of theutterance 106. In the example shown, theuser device 102 and/or theremote computing device 201 also executes a user interface generator 107 configured to present a representation of thetranscription 120 of theutterance 106 to theuser 104 of theuser device 102. In some configurations, thetranscription 120 output from theASR system 100 is processed, e.g., by a natural language understanding (NLU) module executing on theuser device 102 or theremote computing device 201, to execute a user command. Additionally or alternatively, the TTS model 501 (e.g., executing on any combination of theuser device 102 or the remote computing device 201) may convert the transcription into synthesized speech for audible output by theaudio subsystem 108 or another device. For instance, theoriginal utterance 106 may correspond to a message theuser 104 is sending to a friend in which thetranscription 120 is converted to synthesized speech for audible output to the friend to listen to the message conveyed in theoriginal utterance 106. - The
TTS model 501 receives, as input, atextual input 112 corresponding to a word or sequence of words and generates, as output, acorresponding speech representation 520 for the textual input. In particular, theTTS model 501 may generate textual encodings based on thetextual input 112 and decode thetextual encodings 520 to produce thespeech representation 520. Theuser 104 may provide thetextual input 112 via the user input to theuser device 102. In some examples, theuser 104 provides thetextual input 112 directly by typing on a screen of theuser device 102. In other examples, theuser 104 may speak anutterance 106 such that theASR model 200 generates thetranscription 120 based on theutterance 106 which serves as thetextual input 112. Without departing from the scope of the present disclosure, thetextual input 112 may correspond to a response, notification, or other communication that a digital assistant is conveying to theuser 104. Theuser 104 may also select a target embedding for use by theTTS model 501 in generating synthetic speech having speaker characteristics of a target speaker. Additionally or alternatively, theuser 104 may further specify an intended prosody/style of the resulting synthetic speech. Theaudio subsystem 108 including a vocoder may receive thespeech representation 520 and generate an audible output (e.g., via one or more speakers of the user device 102) of thetextual input 112. - Referring to
FIG. 2 , in some examples, theASR model 200 includes a Recurrent Neural Network-Transducer (RNN-T) model architecture which adheres to latency constraints associated with interactive applications. The use of the RNN-T model architecture is exemplary, and theASR model 200 may include other architectures such as transformer-transducer and conformer-transducer model architectures among others. The RNN-T model architecture provides a small computation footprint and utilizes less memory requirements than conventional ASR architectures, making the RNN-T model architecture suitable for performing speech entirely on the user device 102 (e.g., no communication with a remote server is required). The RNN-T model architecture of theASR model 200 includes anencoder network 210, a prediction network 220, and ajoint network 230. Theencoder network 210, which is roughly analogous to an acoustic model (AM) in a traditional ASR system, includes a stack of self-attention layers (e.g., Conformer or Transformer layers) or a recurrent network of stacked Long Short-Term Memory (LSTM) layers. For instance, theencoder network 210 reads a sequence of d-dimensional feature vectors (e.g., acoustic frames 110 (FIG. 1 )) x=x1, x2, . . . , xT), where xt∈ - Similarly, the prediction network 220 is also an LSTM network, which, like a language model (LM), processes the sequence of non-blank symbols output by a
final Softmax layer 240 so far, y0, . . . , yui−1, into a dense representation pui . Finally, with the RNN-T model architecture, the representations produced by the encoder and prediction/decoder networks 210, 220 are combined by thejoint network 230. The prediction network 220 may be replaced by an embedding look-up table to improve latency by outputting looked-up sparse embeddings in lieu of processing dense representations. The joint network then predicts P(yi|xti , y0, . . . , yui−1 ), which is a distribution over the next output symbol. Stated differently, thejoint network 230 generates, at each output step (e.g., time step), a probability distribution over possible speech recognition hypotheses. Here, the “possible speech recognition hypotheses” correspond to a set of output labels each representing a symbol/character in a specified natural language. For example, when the natural language is English, the set of output labels may include twenty-seven (27) symbols, e.g., one label for each of the 24-letters in the English alphabet and one label designating a space. Accordingly, thejoint network 230 may output a set of values indicative of the likelihood of occurrence of each of a predetermined set of output labels. This set of values can be a vector and can indicate a probability distribution over the set of output labels. In some cases, the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited. For example, the set of output labels can include wordpieces, phonemes, and/or entire words, in addition to or instead of graphemes. The output distribution of thejoint network 230 can include a posterior probability value for each of the different output labels. Thus, if there are 100 different output labels representing different graphemes or other symbols, the output yi of thejoint network 230 can include 100 different probability values, one for each output label. The probability distribution can then be used to select and assign scores to candidate orthographic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process (e.g., by the Softmax layer 240) for determining thetranscription 120. - The
Softmax layer 240 may employ any technique to select the output label/symbol with the highest probability in the distribution as the next output symbol predicted by theASR model 200 at the corresponding output step. In this manner, the RNN-T model architecture of theASR model 200 does not make a conditional independence assumption, rather the prediction of each symbol is conditioned not only on the acoustics but also on the sequence of labels output so far. TheASR model 200 does assume an output symbol is independent of futureacoustic frames 110, which allows the RNN-T model architecture of theASR model 200 to be employed in a streaming fashion. - In some examples, the encoder network (i.e., audio encoder) 210 of the
ASR model 200 includes a stack of self-attention layers/blocks, such as conformer blocks. Here, each conformer block includes a series of multi-headed self-attention, depth wise convolution and feed-forward layers. The prediction network 220 may have two 2,048-dimensional LSTM layers, each of which is also followed by 440-dimensional projection layer. Alternatively, the prediction network 220 may include a stack of transformer or conformer blocks, or an embedding look-up table in lieu of LSTM layers. Finally, thejoint network 230 may also have 440 hidden units. TheSoftmax layer 240 may be composed of a unified word piece or grapheme set that is generated using all unique word pieces or graphemes in a plurality of training data sets. -
FIGS. 3A-3C illustrate anexample training process 300 for training theTTS model 501 using sets of ASR transcribed utterances 310. In particular, thetraining process 300 may train atext encoder 202 of theTTS model 501. TheTTS model 501 and theASR model 200 may share thetext encoder 204. As will become apparent, thetraining process 300 may train theTTS model 400 usingtraining data 301 that includes a plurality of sets of ASR utterances 310. More specifically, each set of ASR utterances 310 of the plurality of sets of ASR utterances 310 includes a set of unspoken textual utterances (Xtext) 308, a set of transcribed non-synthetic speech utterances (Xsup) 304, and/or un-transcribed non-synthetic speech utterances (Xunsup) 306. Each unspokentextual utterance 308 includes text-only data (i.e., unpaired data) such that each unspokentextual utterance 308 is not paired with any corresponding spoken audio representation (i.e., speech) of the utterance. The unspokentextual utterance 308 may include any sequence text chunks including words, word-pieces, phonemes, bytes, and/or graphemes. Each un-transcribed non-synthetic speech utterance 306 (also referred to as simply “un-transcribed speech utterance 306”) includes audio-only data (i.e., unpaired data) such that theun-transcribed speech utterance 306 is not paired with any corresponding transcription. On the other hand, each transcribed non-synthetic speech utterance 304 (also referred to as simply “transcribedspeech utterance 304”) includes acorresponding transcription 302 paired with a corresponding non-synthetic speech representation of the corresponding transcribedspeech utterance 304. - Moreover, each set of ASR utterances 310 is associated with a respective language that is different than the respective language associated with each other set of the ASR utterances 310 and includes ASR utterances of non-synthetic speech spoken in the respective language. For instance, in the example shown, the
training data 301 includes a first set of ASR utterances 310, 310 a includingtranscriptions 302, transcribedspeech utterances 304,un-transcribed speech utterances 306, and unspokentextual utterances 308 each associated with a first respective language (e.g., English). Continuing with the example shown, thetraining data 301 also includes a second set of ASR utterances 310, 310b including transcriptions 302, transcribedspeech utterances 304,un-transcribed speech utterances 306, and unspokentextual utterances 308 each associated with a second respective language (e.g., Chinese). The example shown includes two sets of ASR utterances 310 associated with two respective languages for the sake of clarity only, as it is understood that thetraining data 301 may include a number of sets of ASR utterances 310 associated with any number of languages. - For simplicity, the
training process 300 includes a contrastive self-supervisedloss part 300 a (FIG. 3A ), an ASRsupervised loss part 300 b (FIG. 3B ), and a consistency regularization part 300 c (FIG. 3C ). Thetraining process 300 trains theTTS model 501 on a total loss based on: contrastive losses (Lw2v) 316 derived using the contrastive self-supervisedloss part 300 a from the unspoken training text utterances (Xtext) 308, a corpus of transcribed non-synthetic speech utterances (Xsup) 304, and un-transcribed non-synthetic speech utterances (Xunsup) 306; supervised losses (Laux) 342, 344 derived using the ASR supervisedloss part 300 b from the unspoken training text utterances (Xtext) 306 and the transcribed non-synthetic speech utterances (Xsup) 304; and consistency losses ( - In some examples, the
training process 300 employs analignment model 400 that is configured to generate, at each of a plurality of output steps, alignment outputs (i.e., textual representation) 402 for a respective one of the plurality of unspokentraining text utterances 308, thetranscriptions 302, and/or theinput text sequences 502. Accordingly, thealignment model 400 may generate acorresponding alignment output 402 for each one of the unspokentextual utterances 308, thetranscriptions 302, and/or theinput text sequences 502. Thereafter, thetraining process 300 trains theTTS model 501 using the generated alignment outputs 402. - Referring now to
FIG. 4 , in some examples, thealignment model 400 includes an embeddingextractor 410,duration predictor 420, and anupsampler 430. The embeddingextractor 410 receives a respective one of the unspokentextual utterances 308,transcriptions 302, and/orinput text sequences 502. Here, the unspokentextual utterances 308,transcriptions 302, andinput text sequences 502 may each include a sequence of text chunks including words, word-pieces, phonemes, bytes, and/or graphemes. As such, the embeddingextractor 410 extracts a corresponding initial textual representation (et) 412 for the respective one of the unspokentextual utterances 308,transcriptions 302, and/orinput text sequences 502. For example, the embeddingextractor 410 may receive a respectiveinput text sequence 502 and extract the initialtextual representation 412 from the respectiveinput text sequence 502. The initialtextual representation 412 includes embedding lexical information from the sequence of text chunks. Theduration predictor 420 receives the initialtextual representation 412 from the embeddingextractor 410 and predicts a corresponding text chunk duration (i.e., word, word-piece, phoneme, and/or grapheme duration) 422. Thetext chunk duration 422 indicates a duration the corresponding text chunk would be spoken if a human (or text-to-speech system) spoke the unspokentextual utterance 308. For example, theinput text sequence 502 may include a sequence of phonemes and theduration predictor 420 predicts aphoneme duration 422 for each phoneme in the sequence of phonemes. In this example, theduration predictor 420 predicts thephoneme duration 422 by predicting a probability of non-zero duration for each phoneme and predicting a probability of continuous phoneme duration for each phoneme. As the sequence of phonemes includes regular phonemes, silences between word boundaries, and punctuation marks, only the regular phonemes are associated with non-zero duration while the silences and punctuation marks are generally associated with the continuous phoneme duration. Accordingly, theduration predictor 420 may use a sigmoid activation following a first one of two independent activations to predict the probability of non-zero duration and use a soft plus activation following a second one of the two independent projections to predict the continuoustext chunk duration 422 for each text chunk. Theduration predictor 420 determines, for each text chunk, whether the probability of non-zero duration is less than a threshold value, and when the probability of non-zero duration is less than the threshold value, a multiplier may zero-out the continuoustext chunk duration 422 predicted by the softplus activation for the corresponding text chunk. Otherwise, when the probability of non-zero duration is not less than the threshold value, the predictedtext chunk duration 422 may be set equal to the continuous phoneme duration predicted by the softplus activation. - The
upsampler 430 receives each corresponding initialtextual representation 412 output by the embeddingextractor 410 and the corresponding predictedtext chunk duration 422, and generates an alignment output (êt) 402 that has a number of frames by upsampling the initialtextual representation 412 using the corresponding predictedtext chunk duration 422. In some examples, thealignment model 400 sends thealignment output 402 to thetext encoder 202. In other examples (not shown), thealignment model 400 sends thealignment output 402 to a shared encoder 250 (e.g., bypassing the text encoder 202) of theencoder 210. In these other examples, thealignment output 402 serves as the encodedtextual representation 312 such that the sharedencoder 250 may receive thealignment output 402 directly from the alignment model. In some additional examples, paired training data is available and theupsampler 430 generates thealignment output 402 as follows. -
ê t=σRefiner (Resample(e t,AlignRNN−T(e S ,t))) (1) - Here, the upsampler includes resampler and refiner layers that align the initial textual embedding 412 to align with a corresponding encoded
audio representation 314 directly. In other examples, paired training data is not available and theupsampler 430 generates thealignment output 402 as follows. -
ê t=θRefiner (Resample(e t, θduration(e t))) (2) - In particular, the number of frames of the
alignment output 402 indicates a predicted speech duration of the respective one of the unspokentextual utterances 308,transcriptions 302, orinput text sequences 502. Stated differently, the number of frames of thealignment output 402 maps (i.e., aligns) the sequence of text chunks of the text input to speech frames. Here, theupsampler 430 includes resampler and refiner layers that replicate the initial textual embedding 412 to match the predicted text chunk duration 422 (i.e., speech duration). As such, thealignment output 402 includes a textual representation of the text input (e.g., the unspokentextual utterances 308,transcriptions 302, and/or input text sequences 502) having a timing component that aligns with how a human would speak the text input. - Notably, in most instances, a TTS system (i.e., an auxiliary TTS system) generates an audible output to give text input the timing component of human speech such that a training process may use the audible output (i.e., synthetic speech) to train the
encoder 210. Thus, since thealignment model 400 generates thealignment output 402 that maps the sequence of text chunks to speech frames directly, thetraining process 300 does not require speech synthesis of speech to generate the alignment outputs 402. That is, thealignment model 400 does not convert the input text into synthetic speech. - Referring now specifically to
FIG. 3A , in some implementations, theencoder 210 includes aspeech encoder 204 and thetext encoder 202, described in more detail with reference toFIGS. 3B and 3C . In the example shown, thespeech encoder 204 processes audio input (e.g., transcribedspeech utterance 304 and un-transcribed speech utterances 306) and the text encoder 206 processes text input (e.g., unspoken text 308). Each of thespeech encoder 204 and thetext encoder 202 includes a Conformer encoder including a stack of conformer blocks each of which includes a series of multi-headed self attention, depth wise convolution, and feed-forward layers. Alternatively, theaudio encoder 210 may include another type of encoder having a stack of self-attention layers/blocks, such as a transformer encoder. Each of thespeech encoder 204 and thetext encoder 202 may naturally be split into a feature encoder, including a convolutionsub sampling block 212, and a context network, including alinear layer 214 and a stack of Conformer blocks 216. In some implementations, theconvolution subsampling block 212 has two two-dimensional-convolution layers, both with strides (2, 2), resulting in a 4× reduction in the feature sequence length. Theconvolution subsampling block 212 receives, as input, a sequence of input features/vectors (e.g., mel-frequency spectrograms such as theacoustic frames 110 ofFIG. 1 ) associated with each transcribednon-synthetic speech utterance 304 and each un-transcribednon-synthetic speech utterance 306, and generates, as output, for each of a plurality of output steps, an encoded audio feature 211 that corresponds to a respective one of the transcribednon-synthetic speech utterances 304 or a respective one of the un-transcribednon-synthetic speech utterances 306. Theconvolution subsampling block 212 may receive, as input, eachalignment output 402 and generate, as output, for each of the plurality of output steps, an encoded textual feature 213 that corresponds to a respective one of the alignment outputs 402. - The encoded audio and textual features 211, 213 (i.e., interchangeably referred to as “encoded features 211, 213”) output from the
convolution subsampling block 212 may be fed to amasking module 218 where some of the encoded features 211, 213 are randomly chosen and replaced with a trained feature vector shared between all masked time steps to provide corresponding masked encoded audio features 211, 211 m and masked encodedtextual features 213, 213 m. In some examples, themasking module 218 masks the randomly chosen encoded features 211, 213 for masking by randomly sampling without replacement a certain proportion p of all time steps to be start indices and then masks the subsequent M consecutive time steps from every sample index, whereby some spans may overlap. After masking is applied, thelinear layer 214 and the Conformer blocks 216 of the context network receives the masked encodedfeatures 211 m (or encoded features 211, 213 not chosen by the masking module 218) and outputs corresponding contrastive context vectors (i.e., encoded representation) 215 from masked encodedfeatures quantizer 217 receives the encoded features 211, 213 as input, and generates quantized vectors (i.e., target context vectors) 219 as output. Thereafter, acontrastive loss module 315 derives a contrastive loss (Lw2v) 316 between thecontrastive context vectors 215 at the masked positions and thetarget context vectors 219 as follows. -
- where ct is
contrastive context vector 215 centered over a masked time step t and qt represents atarget context vector 219 at the time step tin a set of K+1 candidatetarget context vectors 219 which includes qt and K distractors. Distractors may be uniformly sampled from other masked time steps of the same utterance. - The
contrastive loss 316 is optimized between thecontrastive context vectors 215 at the masked positions and thetarget context vectors 219. After theencoder 210 converges on the un-transcribednon-synthetic speech utterances 306, the training procedure is repeated on both the alignment outputs 402 corresponding to the unspokentextual utterance 308 and the transcribednon-synthetic speech utterances 304. Thus, the contrastive loss (Lw2v) is optimized for both real/human (non-synthetic) and the unspokentextual utterances 308 represented byalignment outputs 402, with additional auxiliary losses on the transcribednon-synthetic speech utterances 304 and the alignment outputs 402 as described in greater detail below with reference toFIG. 3B . Accordingly, thecontrastive part 300 a of thetraining process 300 trains thespeech encoder 204 and thetext encoder 202 on the derivedcontrastive loss 316 applied on the corresponding encoded features 211, 213 associated with eachalignment output 402, each transcribednon-synthetic speech utterance 304, and each un-transcribednon-synthetic speech utterance 306 provided as input to theencoder 210. Training theencoder 210 may include updating parameters of theencoder 210 based on thecontrastive losses 316. In some implementations, thecontrastive loss module 315 determines a masked language modeling (MLM)loss 318 for the speech input (e.g., transcribedspeech utterance 304 and un-transcribed speech utterances 306) by comparing thecontrastive context vector 215 generated from masked encoded features to contrastivecontext vectors 215 generated from corresponding unmasked encoded features. Thus, theMLM loss 318 compares the encodings generated for masked and unmasked encoded features. - Referring now to
FIG. 3B , the ASR supervisedloss part 300 b of thetraining process 300 is configured to inject lexical information into thetext encoder 204 of theTTS model 501 during pre-training based onsupervised loss terms non-synthetic speech utterances 304 and the alignment outputs 402 corresponding to unspokentextual utterances 308 output by thealignment model 400. Notably, the ASR supervisedloss part 300 b leverages one ormore ASR decoders 390 for generating the supervised loss terms (i.e., ASR loss) 342, 344. TheASR decoders 390 may include Connectionist Temporal Classification (CTC) decoders, Listen Attend Spell (LAS) decoders, or RNN-T decoders. TheseASR decoders 390 may include at least one of a phoneme decoder configured to decode a sequence of phonemes or a wordpiece decoder configured to decode a sequence of word pieces. TheASR decoders 390 could also include a grapheme decoder configured to decode a sequence of graphemes. - During the ASR supervised
loss part 300 b, thetext encoder 202 is configured to receive alignment outputs 402 (i.e., text embeddings) from thealignment model 400 and thespeech encoder 204 is configured to receive transcribednon-synthetic speech utterances 304. That is, thetext encoder 202 generates encodedtextual representations 312 for alignment outputs 402 (e.g., corresponding to an unspoken textual utterance 308) and thespeech encoder 204 of theencoder 210 generates encodedaudio representations 314 for speech inputs (i.e., transcribed non-synthetic speech utterances 304). Here, the encodedtextual representations 312 and the encodedaudio representations 314 may not both be compatible with theASR decoders 390. In some examples, thetext encoder 202 obtains a corresponding speaker embedding 326 that characterizes speaker characteristics of a corresponding speaker that spoke the ASR utterance in the respective language and generates the corresponding encodedtextual representation 312 based on a concatenation of thecorresponding alignment output 402 and the corresponding speaker embedding 326. - Thus, the ASR supervised
loss part 300 b may employ a sharedencoder 250 that receives the encodedtextual representations 312 as input, and generates a first encoded shared representation 322 (etext) as output. Similarly to thetext encoder 202, theTTS model 501 and theASR model 200 may share the sharedencoder 250. Moreover, the sharedencoder 250 receives the encodedaudio representations 314 as input, and generates a second encoded shared representation (esup) 324 as output. Accordingly, the sharedencoder 250 generates the first and second encoded sharedrepresentations 322, 324 into a shared latent representation space compatible with theASR decoder 390. - In particular, the shared
encoder 250 receives, as input, each encodedtextual representation 312 that corresponds to thealignment output 402 generated from the unspokentextual utterance 308 and generates, as output, for each of a plurality of time steps, the first encoded shared representation (etext) 322 that corresponds to thealignment output 402 at the corresponding output step. TheASR decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each first encoded sharedrepresentation 332 output from the sharedencoder 250 and generates, as output, afirst probability distribution 392 over possible speech recognition hypotheses for thecorresponding alignment output 402 at the corresponding output step. In some examples, thefirst probability distribution 392 over possible speech recognition hypotheses includes one of possible phoneme labels, possible word piece labels, or possible grapheme labels. Thereafter, an ASRsupervised loss module 340 may determine an alignmentoutput loss term 342 based on thefirst probability distribution 392 over possible speech recognition hypotheses for thealignment output 402 corresponding to the unspokentextual utterance 308. Here, the corresponding unspokentextual utterance 308 in which thealignment output 402 is generated from also serves as a ground-truth transcription 302. Since thealignment output 402 may be masked, the alignmentoutput loss term 342 also serves as an aligned MLM loss. The ASR supervisedloss part 300 b may train thetext encoder 202 and/orspeech encoder 204 on the alignmentoutput loss term 342 by updating parameters of thetext encoder 202 and/or thespeech encoder 204 based on the alignmentoutput loss term 342. - Similarly, during the ASR supervised
loss part 300 b, the sharedencoder 250 receives, as input, each transcribed encodedaudio representation 314 that corresponds to thenon-synthetic speech utterance 304 and generates, as output, for each of a plurality of time steps, a second encoded shared representation (esup) 334 that corresponds to the transcribednon-synthetic speech utterance 304 at the corresponding time step. TheASR decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each second encoded sharedrepresentation 334 output from the sharedencoder 250 and generates, as output, asecond probability distribution 394 over possible non-synthetic speech recognition hypotheses for the corresponding transcribednon-synthetic speech utterance 304 at the corresponding time step. In some examples, thesecond probability distribution 394 over possible non-synthetic speech recognition hypotheses includes the one of possible phoneme labels, the possible word piece labels, or the possible grapheme labels. Thereafter, the ASR supervisedloss module 340 may determine a non-syntheticspeech loss term 344 based on thesecond probability distribution 394 over possible non-synthetic speech recognition hypotheses and thecorresponding transcription 302 paired with the transcribednon-synthetic speech utterance 304. Here, the correspondingtranscription 302 serves as a ground-truth transcription and may include a sequence of target phonemes, target word pieces, and/or target graphemes. The ASR supervisedloss part 300 b may train thetext encoder 202 and/or speech encode 204 on the non-syntheticspeech loss term 344 by updating parameters of thetext encore 202 and/orspeech encoder 204 based on the non-syntheticspeech loss term 344. - The un-transcribed
non-synthetic speech utterances 306 and the unspokentextual utterances 308 each correspond to “unpaired” training data whereby the contrastive loss (Lw2v) derived from the unspoken textual utterances (Xtext) 308 may be combined with the supervised lossoutput loss term 342 to obtain an unspoken textual loss function, ≈text, as follows. - During training of the
text encoder 202 and thespeech encoder 204, the alignment outputs 402 and the un-transcribednon-synthetic utterances 306 may be separated or mixed within each batch. In order to force thetext encoder 202 to learn representations that are effective for bothalignment outputs 402 corresponding to unspokentextual utterances 308 and non-synthetic (human/real) speech, the loss mask σ is applied when combining the loss functions - The transcribed
non-synthetic speech utterances 304 corresponds to “paired” and “supervised” training data whereby the derived contrastive loss Lw2v and the derived supervised loss - Referring to
FIG. 3C , the consistency regularization part (i.e., modality matching part) 300 c of thetraining process 300 is configured to promote thetext encoder 202 and thespeech encoder 204 to learn consistent predictions between non-synthetic speech (e.g., real/human speech) andalignment outputs 402 corresponding to unspokentextual utterances 308 by generating a consistent loss term (non-synthetic speech utterance 304. As such, thenon-synthetic speech utterance 304 and the paired alignment output 404 of each training utterance pair 303 is associated with a same ground-truth transcription. In short, theconsistent loss term 352 between the transcribednon-synthetic speech utterance 304 and paired alignment output 404 of the same training utterance provides an unsupervised training aspect by encouraging theencoder 210 to behave consistently regardless of whether the training utterance belongs to non-synthetic speech (i.e., speech training data) or the alignment output (i.e., text training data) and independent of supervised loss terms between the ground-truth transcription 302 and each of: non-synthetic speech recognition hypotheses output by theauxiliary decoder 390; and speech recognition hypothesis output by theauxiliary decoder 390. - Similar to the alignment outputs 402 generated from the unspoken
textual utterances 308 inFIG. 3B , thealignment model 400 may generate each paired alignment output 404 using the correspondingtranscription 302 that is paired with the transcribednon-synthetic speech utterance 304. Here, thenon-synthetic speech representation 304 is associated with paired alignment output 404 generated by thealignment model 400 mapping the unspokentextual utterance 308 into speech frames. - During the consistency regularization part 300 c, the
text encoder 202 receives, - as input, each paired alignment output 404 and generates, as output, for each of a plurality of time steps, an encoded
textual representation 313 that corresponds to the paired alignment output 404 at the corresponding output step. In some examples, thetext encoder 202 obtains a corresponding speaker embedding 326 that characterizes speaker characteristics of a corresponding speaker that spoke the ASR utterance in the respective language and generates the corresponding encodedtextual representation 312 based on a concatenation of thecorresponding alignment output 402 and the corresponding speaker embedding 326. The sharedencoder 250 receives, as input, the encodedtextual representation 313 and generates, as output, a first encoded shared representation (e*sup) 323. Theauxiliary decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each first encoded sharedrepresentation 323 output from the sharedencoder 250 and generates, as output, afirst probability distribution 311 over possible speech recognition hypotheses for the corresponding paired alignment output 404 at the corresponding output step. In some examples, thefirst probability distribution 311 over possible speech recognition hypotheses includes one of possible phoneme labels or possible word piece labels. - Similarly, the
speech encoder 204 receives, as input, each transcribednon-synthetic speech utterance 304 as a sequence of features/vectors (e.g., mel-frequency spectrograms such as theacoustic frames 110 ofFIG. 1 ) and generates, as output, for each of a plurality of time steps, a encodedaudio representation 314 that corresponds to the transcribednon-synthetic speech utterance 304 at the corresponding output step. The sharedencoder 250 receives, as input, the encodedaudio representation 314 and generates, as output, a second encoded shared representation (esup) 324. Theauxiliary decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each second encoded sharedrepresentation 324 output from the sharedencoder 250 and generates, as output, asecond probability distribution 394 over possible non-synthetic speech recognition hypotheses for the corresponding transcribednon-synthetic speech utterance 304 at the corresponding time step. In some examples, thesecond probability distribution 394 over possible non-synthetic speech recognition hypotheses includes the one of the possible phoneme labels or the possible word piece labels. - With continued reference to
FIG. 3C , the consistency regularization part 300 c of thetraining process 300 further determines, at each of the plurality of output steps for eachtraining utterance pair 301, the consistent loss term (training utterance pair 301 based on thefirst probability distribution 311 over possible speech recognition hypotheses and thesecond probability distribution 394 over possible non-synthetic speech recognition hypotheses. For instance, thetraining process 300 may employ a consistencyloss term module 350 configured to receive, at each time step, the corresponding non-synthetic speech and speech recognition results 311, 394 output by theauxiliary decoder 390, and determine theconsistency loss term 352 for the correspondingtraining utterance pair 301 at the time step. - In some examples, the consistency regularization part 300 c of the
training process 300 determines theconsistent loss term 352 based on a Kullback-Leibler divergence (DKL) between thefirst probability distribution 311 over possible speech recognition hypotheses and thesecond probability distribution 394 over possible non-synthetic speech recognition hypotheses. Theconsistent loss term 352 based on DKL may be expressed by the following equation. - Here, the
consistent loss term 352 determined for thetraining utterance pair 301 at each time step provides an “unsupervised” loss term that is independent of the accuracy of the auxiliary decoder 390 (e.g., independent of thesupervised loss terms FIG. 3B ), and thus, may be employed to update parameters of theencoder 210 for promoting consistency between non-synthetic speech representations and alignment outputs of the same utterances. In batch training, theconsistent loss term 352 may correspond to an average loss term obtained for the batch. In other words, theconsistent loss term 352 permits thetext encoder 202 and thespeech encoder 204 to learn to behave the same, e.g., make consistent encoded representation predictions on both non-synthetic speech (e.g., real/human speech) and alignment outputs of a same training utterance, regardless of whether the training utterance belongs to non-synthetic speech or alignment outputs. -
- where λ1 may be equal to 1.0 and λ2 is equal to 0.1. The training process 300 may pre-train the audio encoder speech encoder 204 and the text encoder 202 using the overall loss term,
speech encoder 204 and thetext encoder 202 to effectively teach thespeech encoder 204 and thetext encoder 202 to learn shared representations between speech and text. After pre-training thespeech encoder 204 and thetext encoder 202, thetraining process 300 may fine-tune thepre-trained speech encoder 204 and thetext encoder 202 on transcribed speech utterances that may include supervised training samples of both alignment outputs corresponding to unspokentextual utterance 308 and non-synthetic (e.g., human speech). - In some implementations, the
training process 300 for pre-training thespeech encoder 204 and thetext encoder 202 applies encoder consistency regularization. Unlike decoder consistency regularization applied to auxiliary decoder(s) during the consistency regularization part 300 c that requires hypothesized labels (e.g.,transcripts 302 and unspoken textual utterances 308), encoder consistency regularization does not require hypothesized labels and therefore has the advantage being allowed to be applied to all thetraining data -
- Specific to HCCR, a Convolutional Neural Network (CNN) projection network may calculate projections over increasing length segments of encoder activations e (30, 50, 120 ms) to yield 3 views (V) and draw negative examples from the same utterance for short segments, and from other utterances in the batches with 120 ms segments. Accordingly, an HCCR loss may be calculated over the transcribed non-synthetic speech utterances 304 (paired speech), the un-transcribed non-synthetic speech utterances 306 (unpaired speech), and the alignment outputs 402 generated from the unspoken
textual utterances 308 as follows. -
- In short, the
training process 300 trains theTTS model 501 using the sets of ASR utterances 310 by training thespeech decoder 204, thetext encoder 202, and/or the sharedencoder 250 based on any of the losses derived by thetraining process 300. Even though thespeech decoder 204 and the sharedencoder 240 may not be employed by theTTS model 501 during inference, thetraining process 300 trains these components to learn better shared representations between speech and text thereby further training the TTS model 501 (e.g.,text encoder 202 of the TTS model 501) to generate encodings that accurately represent human speech. -
FIGS. 5A-5C illustrate anexample training process 500 for training theTTS model 501 using sets of TTS spoken utterances 510. Similar to the training process 300 (FIGS. 3A-3C ), thetraining process 500 trains thetext encoder 202 of theTTS model 501, however, thetraining process 500 also trains aspeech decoder 520 of theTTS model 501. As will become apparent, thetraining process 500 may train theTTS model 501 using thetraining data 301 that also includes a plurality of sets of TTS spoken utterances 510. In contrast to the ASR utterance 310, the TTS spoken utterances 510 may include synthetic speech while the ASR utterance 510 include non-synthetic or human speech. - Each set of TTS spoken utterances 510 of the plurality of sets of TTS spoken utterances 510 includes TTS utterances of synthetic speech spoken in a respective language. In particular, each TTS utterance of non-synthetic speech includes a corresponding
reference speech representation 504 paired with a correspondinginput text sequence 502. Here, thereference speech representation 504 includes audio data paired with the correspondinginput text sequence 502 thereby forming labeled training data for training theTTS model 501. Thereference speech representation 504 and theTTS utterance 504 may be used interchangeably. In some examples, thereference speech representations 504 and theinput text sequences 502 are the same as the transcribedspeech utterances 304 and the transcriptions 302 (FIGS. 3A-3C ). In other examples, thereference speech representations 504 and theinput text sequences 502 are different from the transcribedspeech utterances 304 and thetranscriptions 302. Each TTS utterance of non-synthetic speech may include the speaker embedding 326 characterizing speaker characteristics of a corresponding speaker that spoke the TTS utterance in the respective language. - Moreover, each set of TTS spoken utterances 510 is associated with a
- respective language from among a plurality of different languages that is different than the respective language associated with each other set of TTS spoken utterances 510. For instance, in the example shown, the
training data 301 includes a first set of TTS spoken utterances 510, 510 a includinginput text sequences 502 andreference speech representations 504 each associated with the first respective language (e.g., English) and a second set of TTS spoken utterances 510, 510 binput text sequences 502 andreference speech representations 504 each associated with the second respective language (e.g., Chinese). The example shown includes two sets of TTS spoken utterances 510 associated with two respective languages for the sake of clarity only, as it is understood that thetraining data 301 may include a number of sets of TTS spoken utterances 510 associated with any number of languages. Each set of TTS spoken utterances 510 may include the corresponding speaker embedding 326. - For simplicity, the
training process 501 includes a contrastive self-supervised loss part 500 a (FIG. 5A ), a TTS supervised loss part 500 b (FIG. 5B ), a consistency regularization part 500 c (FIG. 5C ). Thetraining process 500 trains theTTS model 501 on a total loss based on: contrastive losses (Lw2v) 516 derived using the contrastive self-supervised loss part 500 a from thereference speech representations 504 and theinput text sequences 502; supervised losses (Laux) 542, 544 and areconstruction loss 545 derived using the TTS supervised loss part 500 b derived from thereference speech representations 504 and theinput text sequences 502; and consistency losses (training process 500 may employ thealignment model 400 to generate, at each of the plurality of output steps,alignment outputs 402 for theinput text sequences 502. - Referring now specifically to
FIG. 5A , in some implementations, thespeech encoder 204 processes audio input (e.g., reference speech representations 504) and thetext encoder 202 processes text input (e.g., input text sequences 502). Each of thespeech encoder 204 and thetext encoder 202 includes a Conformer encoder including a stack of conformer blocks each of which includes a series of multi-headed self attention, depth wise convolution, and feed-forward layers. Each of thespeech encoder 204 and thetext encoder 202 may naturally be split into a feature encoder, including the convolutionsub sampling block 212, and a context network, including thelinear layer 214 and the stack of Conformer blocks 216. In some implementations, theconvolution subsampling block 212 has two two-dimensional-convolution layers, both with strides (2, 2), resulting in a 4× reduction in the feature sequence length. Theconvolution subsampling block 212 receives, as input, a sequence of input features/vectors (e.g., mel-frequency spectrograms such as theacoustic frames 110 ofFIG. 1 ) associated with each transcribednon-synthetic speech utterance 304 and eachreference speech representations 504 andinput text sequence 502, and generates, as output, for each of a plurality of output steps, the encoded audio feature 211 that corresponds to a respective one of thereference speech representations 504. Theconvolution subsampling block 212 may receive, as input, eachalignment output 402 and generate, as output, for each of the plurality of output steps, the encoded textual feature 213 that corresponds to a respective one of the alignment outputs 402. - The encoded audio and textual features 211, 213 (i.e., interchangeably referred to as “encoded features 211, 213”) output from the
convolution subsampling block 212 may be fed to amasking module 218 where some of the encoded features 211, 213 are randomly chosen and replaced with a trained feature vector shared between all masked time steps to provide corresponding masked encoded audio features 211, 211 m and masked encodedtextual features 213, 213 m. In some examples, themasking module 218 masks the randomly chosen encoded features 211, 213 for masking by randomly sampling without replacement a certain proportion p of all time steps to be start indices and then masks the subsequent M consecutive time steps from every sample index, whereby some spans may overlap. After masking is applied, thelinear layer 214 and the Conformer blocks 216 of the context network receives the masked encodedfeatures 211 m (or encoded features 211, 213 not chosen by the masking module 218) and outputs corresponding contrastive context vectors (i.e., encoded representation) 215 from masked encodedfeatures quantizer 217 receives the encoded features 211, 213 as input, and generates quantized vectors (i.e., target context vectors) 219 as output. Thereafter, acontrastive loss module 515 derives a contrastive loss (Lw2v) 516 between thecontrastive context vectors 215 at the masked positions and thetarget context vectors 219 as follows according to Equation 3. - The
contrastive loss 516 is optimized between thecontrastive context vectors 215 at the masked positions and thetarget context vectors 219. The contrastive loss (Lw2v) is optimized for both synthetic speech and theinput text sequences 502 represented byalignment outputs 402. Accordingly, the contrastive part 500 a of thetraining process 500 trains thespeech encoder 204 and thetext encoder 202 on the derivedcontrastive loss 516 applied on the corresponding encoded features 211, 213 associated with eachalignment output 402 and eachreference speech representations 504 provided as input to thespeech encoder 204 or thetext encoder 202. Training thespeech encoder 204 and/or thetext encoder 202 may include updating parameters of thespeech encoder 204 and/or thetext encoder 202 based on thecontrastive losses 516. In some implementations, thecontrastive loss module 515 determines a masked language modeling (MLM)loss 518 for the speech input (e.g., reference speech representations 504) by comparing thecontrastive context vector 215 generated from masked encoded features to contrastivecontext vectors 215 generated from corresponding unmasked encoded features. Thus, theMLM loss 518 compares the encodings generated for masked and unmasked encoded features. - Referring now to
FIG. 5B , the TTS supervised loss part 500 b of thetraining process 500 is configured to inject lexical information into thetext encoder 202 of theTTS model 501 during training based onsupervised loss terms reference speech representations 504 and the alignment outputs 402 corresponding to inputtext sequences 502. In contrast, to the ASR supervisedloss part 300 b (FIG. 3B ), the TTS supervisedloss part 500 also employs aspeech decoder 520 and determines areconstruction loss 545. The TTS supervised loss part 500 b leverages one ormore ASR decoders 390 for generating the supervised loss terms (i.e., ASR loss) 542, 544. TheASR decoders 390 may include Connectionist Temporal Classification (CTC) decoders, Listen Attend Spell (LAS) decoders, or RNN-T decoders. TheseASR decoders 390 may include at least one of a phoneme decoder configured to decode a sequence of phonemes or a wordpiece decoder configured to decode a sequence of word pieces. TheASR decoders 390 could also include a grapheme decoder configured to decode a sequence of graphemes. - During the TTS supervised loss part 500 b, the
text encoder 202 is configured to receive alignment outputs 402 (i.e., text embeddings) from thealignment model 400 and thespeech encoder 204 is configured to receive thereference speech representations 504. That is, thetext encoder 202 generates encodedtextual representations 512 for alignment outputs 402 (e.g., corresponding to an input text sequence 502) and thespeech encoder 204 generates encodedaudio representations 514 for speech inputs (i.e.,reference speech representations 504 of the TTS utterances). Here, the encodedtextual representations 512 and the encodedaudio representations 514 may not both be compatible with theASR decoders 390. In some examples, thetext encoder 202 obtains a corresponding speaker embedding 326 that characterizes speaker characteristics of a corresponding speaker that spoke the ASR utterance in the respective language and generates the corresponding encodedtextual representation 512 based on a concatenation of the corresponding alignment output 402 (or the corresponding input text sequence 502) and the corresponding speaker embedding 326. - Thus, the TTS supervised loss part 500 b may employ the shared
encoder 250 that receives the encodedtextual representations 512 as input, and generates a first encoded shared representation 532 (etext) as output. Similarly to thetext encoder 202, theTTS model 501 and theASR model 200 may share the sharedencoder 250. Moreover, the sharedencoder 250 receives the encodedaudio representations 514 as input, and generates a second encoded shared representation (esup) 534 as output. Accordingly, the sharedencoder 250 generates the first and second encoded sharedrepresentations ASR decoder 390. - In particular, the shared
encoder 250 receives, as input, each encodedtextual representation 512 that corresponds to thealignment output 402 generated from theinput text sequence 502 and generates, as output, for each of a plurality of time steps, the first encoded shared representation (etext) 532 that corresponds to thealignment output 402 at the corresponding output step. TheASR decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each first encoded sharedrepresentation 532 output from the sharedencoder 250 and generates, as output, afirst probability distribution 592 over possible speech recognition hypotheses for thecorresponding alignment output 402 at the corresponding output step. Thefirst probability distribution 592 may represent a candidate transcription for the corresponding TTS utterance. In some examples, thefirst probability distribution 592 over possible speech recognition hypotheses includes one of possible phoneme labels, possible word piece labels, or possible grapheme labels. Thereafter, a TTSsupervised loss module 540 may determine an alignmentoutput loss term 542 based on thefirst probability distribution 592 over possible speech recognition hypotheses for thealignment output 402 corresponding to theinput text sequence 502. Here, the correspondinginput text sequence 502 in which thealignment output 402 is generated from also serves as a ground-truth transcription. Since thealignment output 402 may be masked (FIG. 4 ), the alignmentoutput loss term 542 also serves as an aligned MLM loss. The TTS supervised loss part 500 b may train thetext encoder 202 and/orspeech encoder 204 on the alignment output loss term (i.e., ASR loss) 542 by updating parameters of thetext encoder 202 and/or thespeech encoder 204 based on the alignmentoutput loss term 542. - Similarly, during the TTS supervised loss part 500 b, the shared
encoder 250 receives, as input, each transcribed encodedaudio representation 514 that corresponds to thereference speech representation 504 and generates, as output, for each of a plurality of time steps, a second encoded shared representation (esup) 534 that corresponds to thereference speech representation 504 at the corresponding time step. TheASR decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each second encoded sharedrepresentation 534 output from the sharedencoder 250 and generates, as output, asecond probability distribution 594 over possible synthetic speech recognition hypotheses for the correspondingreference speech representation 504 at the corresponding time step. Thefirst probability distribution 592 may represent a candidate transcription for the corresponding TTS utterance. In some examples, thesecond probability distribution 594 over possible synthetic speech recognition hypotheses includes the one of possible phoneme labels, the possible word piece labels, or the possible grapheme labels. Thereafter, the TTS supervisedloss module 540 may determine a syntheticspeech loss term 544 based on thesecond probability distribution 594 over possible synthetic speech recognition hypotheses and the correspondinginput text sequence 502 paired with the transcribedreference speech representation 504. Here, the correspondinginput text sequence 502 serves as a ground-truth transcription and may include a sequence of target phonemes, target word pieces, and/or target graphemes. The TTS supervised loss part 500 b may train thetext encoder 202 and/or speech encode 204 on the synthetic speech loss term (i.e., ASR loss) 544 by updating parameters of thetext encoder 202 and/orspeech encoder 204 based on the syntheticspeech loss term 544. - In some examples, the TTS supervised loss part 500 b determines
modality matching losses 505 between the speech encodings 514 generated for the TTS utterances using thespeech encoder 204 and the TTS encodedtextual representations 512 generated for theinput text sequences 502. That is, the TTS supervised loss part 500 b compares thespeech encodings 514 and the TTS encodedtextual representations 512 that each correspond to a same utterance to determine themodality matching loss 505. Thereafter, the supervised loss part 500 b trains thespeech encoder 204 and/ortext encoder 202 based on themodality matching losses 505. - The TTS supervised loss part also employs the
speech decoder 520 that may include a RNN-T architecture. Thespeech decoder 520 may be part of theTTS model 501 whereby thespeech decoder 520 is configured to receive the first or second encoded sharedrepresentation 532, 534 (collectively referred to as the sharedencoder output 532, 534) and generate a predictedspeech representation 522 for the corresponding TTS utterance of synthetic speech represented by thereference speech representation 504 or thealignment output 402 generated from theinput text sequence 502. In some examples, thespeech decoder 520 obtains a corresponding variational embedding 528 that specifies an intended prosody/style for the predictedspeech representation 522 whereby the speech encoder is 520 is conditioned on the corresponding variational embedding 528 and the corresponding speaker embedding 326. The predictedspeech representation 522 represents features of synthetic speech theTTS model 501 would generate for the TTS utterance 510. Thus, thereconstruction loss 545 based on the predictedspeech representation 522 and the correspondingreference speech representation 504 which serves as a ground-truth label from which the predictedspeech representation 522 was generated from. Thetraining process 500 trains thespeech encoder 202, thetext encoder 204, the sharedencoder 250, and/or thespeech decoder 520 based on thereconstruction losses 545 generated for each TTS utterance 510. - Referring to
FIG. 5C , the consistency regularization part (i.e., modality matching part) 500 c of thetraining process 500 is configured to promote thetext encoder 202 and thespeech encoder 204 to learn consistent predictions between synthetic speech andalignment outputs 402 corresponding to theinput text sequences 502 by generating a consistent loss term (reference speech representations 504 and a paired alignment output 404 of the same utterance as the correspondingreference speech representation 504. As such, the reference speech representation and the paired alignment output 404 of each training utterance pair 503 is associated with a same ground-truth transcription. In short, theconsistent loss term 552 between thereference speech representation 504 and paired alignment output 404 of the same training utterance provides an unsupervised training aspect by encouraging thespeech encoder 204 and thetext encoder 202 to behave consistently regardless of whether the training utterance belongs to synthetic speech or the alignment output (i.e., text training data) and independent of supervised loss terms between the ground-truth transcription (i.e., input text sequence) 502 and each of the speech recognition hypothesis output by theauxiliary decoder 390. - Similar to the alignment outputs 402 generated from the
input text sequences 502 inFIG. 5B , thealignment model 400 may generate each paired alignment output 404 using the correspondinginput text sequence 502 that is paired with the reference speech representation 503. Here, thereference speech representation 504 is associated with paired alignment output 404 generated by thealignment model 400 mapping theinput text sequence 502 into speech frames. - During the consistency regularization part 500 c, the
text encoder 202 receives, as input, each paired alignment output 404 and generates, as output, for each of a plurality of time steps, an encodedtextual representation 513 that corresponds to the paired alignment output 404 at the corresponding output step. In some examples, thetext encoder 202 obtains a corresponding speaker embedding 326 that characterizes speaker characteristics of a corresponding speaker that spoke the ASR utterance in the respective language and generates the corresponding encodedtextual representation 512 based on a concatenation of the corresponding alignment output 402 (or the corresponding input text sequence 502) and the corresponding speaker embedding 326. The sharedencoder 250 receives, as input, the encodedtextual representation 513 and generates, as output, a first encoded shared representation (e*sup) 523. Theauxiliary decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each first encoded sharedrepresentation 523 output from the sharedencoder 250 and generates, as output, afirst probability distribution 511 over possible speech recognition hypotheses for the corresponding paired alignment output 404 at the corresponding output step. In some examples, thefirst probability distribution 511 over possible speech recognition hypotheses includes one of possible phoneme labels or possible word piece labels. - Similarly, the
speech encoder 204 receives, as input, eachreference speech representation 504 as a sequence of features/vectors (e.g., mel-frequency spectrograms such as theacoustic frames 110 ofFIG. 1 ) and generates, as output, for each of a plurality of time steps, an encodedaudio representation 514 that corresponds to thereference speech representation 504 at the corresponding output step. The sharedencoder 250 receives, as input, the encodedaudio representation 514 and generates, as output, a second encoded shared representation (esup) 534. Theauxiliary decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each second encoded sharedrepresentation 534 output from the sharedencoder 250 and generates, as output, asecond probability distribution 594 over possible non-synthetic speech recognition hypotheses for the correspondingreference speech representation 504 at the corresponding time step. In some examples, thesecond probability distribution 594 over possible non-synthetic speech recognition hypotheses includes the one of the possible phoneme labels or the possible word piece labels. - With continued reference to
FIG. 5C , the consistency regularization part 500 c of thetraining process 500 further determines, at each of the plurality of output steps for each training utterance pair 503, the consistent loss term (first probability distribution 511 over possible speech recognition hypotheses and thesecond probability distribution 594 over possible non-synthetic speech recognition hypotheses. For instance, thetraining process 500 may employ a consistencyloss term module 550 configured to receive, at each time step, the corresponding non-synthetic speech and speech recognition results 511, 594 output by theauxiliary decoder 390, and determine theconsistency loss term 552 for the corresponding training utterance pair 503 at the time step. - In some examples, the consistency regularization part 500 c of the
training process 500 determines theconsistent loss term 552 based on a Kullback-Leibler divergence (DKL) between thefirst probability distribution 511 over possible speech recognition hypotheses and thesecond probability distribution 594 over possible non-synthetic speech recognition hypotheses. Theconsistent loss term 552 based on DKL may be expressed by Equation 8. Here, theconsistent loss term 552 determined for the training utterance pair 503 at each time step provides an “unsupervised” loss term that is independent of the accuracy of theauxiliary decoder 390, and thus, may be employed to update parameters of thespeech encoder 204 and/or thetext encoder 202 for promoting consistency between synthetic speech representations and alignment outputs of the same utterances. In batch training, theconsistent loss term 552 may correspond to an average loss term obtained for the batch. In other words, theconsistent loss term 552 permits thetext encoder 202 and thespeech encoder 204 to learn to behave the same, e.g., make consistent encoded representation predictions on both synthetic speech and alignment outputs of a same training utterance, regardless of whether the training utterance belongs to non-synthetic speech or alignment outputs. - In short, the training processes 300 and 500 train the
TTS model 500 that includes thetext encoder 202 and thespeech decoder 520 during inference. Thetraining process 300 trains theTTS model 500 using ASR utterances of non-synthetic speech including transcribed speech utterance, un-transcribed speech utterances, and unspoken text. Thetraining process 500 trains theTTS model 500 using TTS utterances of synthetic speech including speech representations paired with input text sequences. Moreover, the training processes 300, 500 train theTTS model 500 with training data from multiple different languages such that the training processes 300, 500 train theTTS model 500 to be multilingual. By training theTTS model 500 on each of the losses (or any combination of losses) derived from the training processes 300, 500, theTTS model 500 may scale to a massive multilingual TTS model even for languages with little or no training data. In particular, the training processes 300, 500 utilize textual input training data to train theTTS model 500 by generating the alignment outputs 402. That is, the alignment outputs 402 enable training for theTTS model 500 on text inputs without having to synthesize the text input. -
FIG. 6 is flowchart of an example arrangement of operations for a computer-implementedmethod 600 of massive multilingual speech-text joint semi-supervised learning for text-to-speech. Themethod 600 may execute on data processing hardware 710 (FIG. 7 ) using instructions stored on memory hardware 720 (FIG. 7 ). Thedata processing hardware 710 and thememory hardware 720 may reside on theuser device 102 and/or theremote computing device 201 ofFIG. 1 each corresponding to a computing device 700 (FIG. 7 ). - At
operation 602, themethod 600 includes receivingtraining data 301 that includes a plurality of sets of TTS spoken utterances 510. Each set of the TTS spoken utterances 510 is associated with a respective language from among a plurality of different languages that is different than the respective language associated with each other set of the TTS spoken utterances 510. Moreover, each set of the TTS spoken utterances includes TTS utterances 510 of synthetic speech spoken in the respective language. Each TTS utterance 510 of synthetic speech includes a correspondingreference speech representation 504 paired with a correspondinginput text sequence 502. For each TTS utterance 510 in each set of the TTS spoken training utterances 510 of the receivedtraining data 301, themethod 600 performs operations 604-612. Atoperation 604, themethod 600 includes generating a corresponding TTS encodedtextual representation 512 for the correspondinginput text sequence 502 using atext encoder 202. Atoperation 604, themethod 600 includes generating a corresponding speech encoding 514 for the corresponding TTS utterance 510 of synthetic speech using aspeech encoder 204 and, atoperation 608, themethod 600 includes generating a sharedencoder output encoder 250 configured to receive the corresponding TTS encodedtextual representation 512 or thecorresponding speech encoding 514. Atoperation 610, themethod 600 includes generating a predictedspeech representation 522 for the corresponding TTS utterance 510 of synthetic speech using aspeech decoder 520 configured to receive the sharedencoder output operation 612, themethod 600 includes determining areconstruction loss 545 based on the predictedspeech representation 522 and the correspondingreference speech representation 504 for the corresponding TTS utterance 510. Atoperation 614, themethod 600 includes training aTTS model 501 based on thereconstruction losses 545 determined for the TTS utterances 510 in each set of the TTS spoken training utterances 510 to teach theTTS model 501 to learn how to synthesize speech in each of the plurality of different languages. -
FIG. 7 is a schematic view of anexample computing device 700 that may be used to implement the systems and methods described in this document. Thecomputing device 700 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 700 includes aprocessor 710,memory 720, astorage device 730, a high-speed interface/controller 740 connecting to thememory 720 and high-speed expansion ports 750, and a low speed interface/controller 740 connecting to a low speed bus 770 and astorage device 730. Each of thecomponents processor 710 can process instructions for execution within thecomputing device 700, including instructions stored in thememory 720 or on thestorage device 730 to display graphical information for a graphical user interface (GUI) on an external input/output device, such asdisplay 780 coupled tohigh speed interface 740. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 700 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 720 stores information non-transitorily within thecomputing device 700. Thememory 720 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). Thenon-transitory memory 720 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by thecomputing device 700. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes. - The
storage device 730 is capable of providing mass storage for thecomputing device 700. In some implementations, thestorage device 730 is a computer-readable medium. In various different implementations, thestorage device 730 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 720, thestorage device 730, or memory onprocessor 710. - The
high speed controller 740 manages bandwidth-intensive operations for thecomputing device 700, while thelow speed controller 740 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 740 is coupled to thememory 720, the display 780 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 750, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 740 is coupled to thestorage device 730 and a low-speed expansion port 790. The low-speed expansion port 790, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 700 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 700 a or multiple times in a group ofsuch servers 700 a, as alaptop computer 700 b, or as part of arack server system 700 c. - Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” and “computer-readable medium” refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (24)
1. A computer-implemented method that when executed on data processing hardware causes the data processing hardware to perform operations comprising:
receiving training data comprising a plurality of sets of text-to-speech (TTS) spoken utterances, each set of the TTS spoken utterances associated with a respective language from among a plurality of different languages that is different than the respective language associated with each other set of the TTS spoken utterances and comprising TTS utterances of synthetic speech spoken in the respective language, each TTS utterance of synthetic speech comprising a corresponding reference speech representation paired with a corresponding input text sequence;
for each TTS utterance in each set of the TTS spoken utterances of the received training data:
generating, using a text encoder, a corresponding TTS encoded textual representation for the corresponding input text sequence;
generating, using a speech encoder, a corresponding speech encoding for the corresponding TTS utterance of synthetic speech;
generating, using a shared encoder configured to receive the corresponding TTS encoded textual representation or the corresponding speech encoding, a shared encoder output;
generating, using a speech decoder configured to receive the shared encoder output, a predicted speech representation for the corresponding TTS utterance of synthetic speech; and
determining a reconstruction loss based on the predicted speech representation and the corresponding reference speech representation for the corresponding TTS utterance; and
training a TTS model based on the reconstruction losses determined for the TTS utterances in each set of the TTS spoken training utterances to teach the TTS model to learn how to synthesize speech in each of the plurality of different languages.
2. The computer-implemented method of claim 1 , wherein the operations further comprise, for each TTS utterance in each set of the TTS spoken training utterances of the received training data:
obtaining a corresponding speaker embedding characterizing speaker characteristics of a corresponding speaker that spoke the TTS utterance of synthetic speech in the respective language; and
obtaining a corresponding variational embedding that specifies an intended prosody/style for the predicted speech representation generated for the corresponding TTS utterance of synthetic speech,
wherein, when generating the corresponding TTS encoded textual representation for the corresponding input text sequence, the text encoder is configured to receive a concatenation of the corresponding input text sequence and the corresponding speaker embedding, and
wherein, when generating the predicted speech representation for the corresponding TTS utterance of synthetic speech, the speech decoder is conditioned on the corresponding variational embedding and the corresponding speaker embedding.
3. The computer-implemented method of claim 1 , wherein the operations further comprise, for each TTS utterance in each set of the TTS spoken training utterances of the received training data:
generating, using an automatic speech recognition (ASR) decoder configured to receive the shared encoder output, a sequence of speech recognition hypotheses representing a candidate transcription for the corresponding TTS utterance of synthetic speech; and
determining an ASR loss based on the sequence of speech recognition hypotheses and the corresponding input text sequence,
wherein training the TTS model is further based on the ASR losses determined for the TTS utterances in each set of the TTS spoken training utterances.
4. The computer-implemented method of claim 1 , wherein:
the training data further comprises a plurality of sets of automatic speech recognition (ASR) transcribed utterances, each set of the ASR transcribed utterances associated with a respective language that is different than the respective language associated with each other set of the ASR transcribed utterances and comprising ASR utterances of non-synthetic speech spoken in the respective language, each ASR utterance of non-synthetic speech paired with a corresponding transcription; and
training the TTS model further comprises training the TTS model on the plurality of sets of ASR transcribed utterances.
5. The computer-implemented method of claim 1 , wherein the speech decoder comprises a recurrent neural network-transducer (RNN-T) architecture.
6. The computer-implemented method of claim 1 , wherein the operations further comprise:
determining consistency losses between the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and the TTS encoded textual representations generated for the input text sequences,
wherein training the TTS model is further based on the consistency losses.
7. The computer-implemented method of claim 1 , wherein the operations further comprise:
determining modality matching losses between the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and the TTS encoded textual representations generated for the input text sequences,
wherein training the TTS model is further based on the modality matching losses.
8. The computer-implemented method of claim 1 , wherein the operations further comprise:
obtaining a masked language modeling (MLM) loss for the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder; and
obtaining an aligned MLM loss for the TTS encoded textual representations generated for the input text sequences using the text encoder,
wherein training the TTS model further comprises training the TTS model on the MLM loss and the aligned MLM loss.
9. The computer-implemented method of claim 1 , wherein:
the training data further comprises unspoken textual utterances in a respective plurality of different languages, each unspoken textual utterance not paired with any corresponding spoken utterance of synthetic speech; and
the operations further comprise, for each unspoken textual utterance:
generating, using the text encoder, a corresponding unspoken encoded textual representation for the corresponding unspoken textual utterance; and
obtaining an aligned masked language modeling (MLM) loss for the corresponding unspoken encoded textual representation generated for the corresponding unspoken textual utterance,
wherein training the TTS model further comprises training the TTS model based on the aligned MLM loss obtained for the unspoken encoded textual representation.
10. The computer-implemented method of claim 1 , wherein:
the training data further comprises un-transcribed non-synthetic speech utterances in a respective plurality of different languages, each un-transcribed non-synthetic speech utterance not paired with a corresponding transcription; and
the operations further comprise, for each un-transcribed non-synthetic speech utterance:
generating, using the speech encoder, a corresponding speech encoding for the corresponding un-transcribed non-synthetic speech utterance; and
obtaining a masked language modeling (MLM) loss for the corresponding speech encoding generated for the corresponding un-transcribed non-synthetic speech utterance,
wherein training the TTS model further comprises training the TTS model based on the MLM loss obtained for the corresponding speech encoding.
11. The computer-implemented method of claim 1 , wherein the TTS model comprises the text encoder and the speech decoder.
12. The computer-implemented method of claim 1 , wherein each corresponding input text sequence comprises a sequence of graphemes, word-piece-model units, phonemes, or bytes.
13. A system comprising:
data processing hardware; and
memory hardware in communication with the data processing hardware, the memory hardware storing instructions that when executed on the data processing hardware cause the data processing hardware to perform operations comprising:
receiving training data comprising a plurality of sets of text-to-speech (TTS) spoken utterances, each set of the TTS spoken utterances associated with a respective language from among a plurality of different languages that is different than the respective language associated with each other set of the TTS spoken utterances and comprising TTS utterances of synthetic speech spoken in the respective language, each TTS utterance of synthetic speech comprising a corresponding reference speech representation paired with a corresponding input text sequence;
for each TTS utterance in each set of the TTS spoken training utterances of the received training data:
generating, using a text encoder, a corresponding TTS encoded textual representation for the corresponding input text sequence;
generating, using a speech encoder, a corresponding speech encoding for the corresponding TTS utterance of synthetic speech;
generating, using a shared encoder configured to receive the corresponding TTS encoded textual representation or the corresponding speech encoding, a shared encoder output;
generating, using a speech decoder configured to receive the shared encoder output, a predicted speech representation for the corresponding TTS utterance of synthetic speech; and
determining a reconstruction loss based on the predicted speech representation and the corresponding reference speech representation for the corresponding TTS utterance; and
training a TTS model based on the reconstruction losses determined for the TTS utterances in each set of the TTS spoken training utterances to teach the TTS model to learn how to synthesize speech in each of the plurality of different languages.
14. The system of claim 13 , wherein the operations further comprise, for each TTS utterance in each set of the TTS spoken training utterances of the received training data:
obtaining a corresponding speaker embedding characterizing speaker characteristics of a corresponding speaker that spoke the TTS utterance of synthetic speech in the respective language; and
obtaining a corresponding variational embedding that specifies an intended prosody/style for the predicted speech representation generated for the corresponding TTS utterance of synthetic speech,
wherein, when generating the corresponding TTS encoded textual representation for the corresponding input text sequence, the text encoder is configured to receive a concatenation of the corresponding input text sequence and the corresponding speaker embedding, and
wherein, when generating the predicted speech representation for the corresponding TTS utterance of synthetic speech, the speech decoder is conditioned on the corresponding variational embedding and the corresponding speaker embedding.
15. The system of claim 13 , wherein the operations further comprise, for each TTS utterance in each set of the TTS spoken training utterances of the received training data:
generating, using an automatic speech recognition (ASR) decoder configured to receive the shared encoder output, a sequence of speech recognition hypotheses representing a candidate transcription for the corresponding TTS utterance of synthetic speech; and
determining an ASR loss based on the sequence of speech recognition hypotheses and the corresponding input text sequence,
wherein training the TTS model is further based on the ASR losses determined for the TTS utterances in each set of the TTS spoken training utterances.
16. The system of claim 13 , wherein:
the training data further comprises a plurality of sets of automatic speech recognition (ASR) transcribed utterances, each set of the ASR transcribed utterances associated with a respective language that is different than the respective language associated with each other set of the ASR transcribed utterances and comprising ASR utterances of non-synthetic speech spoken in the respective language, each ASR utterance of non-synthetic speech paired with a corresponding transcription; and
training the TTS model further comprises training the TTS model on the plurality of sets of ASR transcribed utterances.
17. The system of claim 13 , wherein the speech decoder comprises a recurrent neural network-transducer (RNN-T) architecture.
18. The system of claim 13 , wherein the operations further comprise:
determining consistency losses between the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and the TTS encoded textual representations generated for the input text sequences,
wherein training the TTS model is further based on the consistency losses.
19. The system of claim 13 , wherein the operations further comprise:
determining modality matching losses between the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder and the TTS encoded textual representations generated for the input text sequences,
wherein training the TTS model is further based on the modality matching losses.
20. The system of claim 13 , wherein the operations further comprise:
obtaining a masked language modeling (MLM) loss for the speech encodings generated for the TTS utterances of synthetic speech using the speech encoder; and
obtaining an aligned MLM loss for the TTS encoded textual representations generated for the input text sequences using the text encoder,
wherein training the TTS model further comprises training the TTS model on the MLM loss and the aligned MLM loss.
21. The system of claim 13 , wherein:
the training data further comprises unspoken textual utterances in a respective plurality of different languages, each unspoken textual utterance not paired with any corresponding spoken utterance of synthetic speech; and
the operations further comprise, for each unspoken textual utterance:
generating, using the text encoder, a corresponding unspoken encoded textual representation for the corresponding unspoken textual utterance; and
obtaining an aligned masked language modeling (MLM) loss for the corresponding unspoken encoded textual representation generated for the corresponding unspoken textual utterance,
wherein training the TTS model further comprises training the TTS model based on the aligned MLM loss obtained for the unspoken encoded textual representation.
22. The system of claim 13 , wherein:
the training data further comprises un-transcribed non-synthetic speech utterances in a respective plurality of different languages, each un-transcribed non-synthetic speech utterance not paired with a corresponding transcription; and
the operations further comprise, for each un-transcribed non-synthetic speech utterance:
generating, using the speech encoder, a corresponding speech encoding for the corresponding un-transcribed non-synthetic speech utterance; and
obtaining a masked language modeling (MLM) loss for the corresponding speech encoding generated for the corresponding un-transcribed non-synthetic speech utterance,
wherein training the TTS model further comprises training the TTS model based on the MLM loss obtained for the corresponding speech encoding.
23. The system of claim 13 , wherein the TTS model comprises the text encoder and the speech decoder.
24. The system of claim 13 , wherein each corresponding input text sequence comprises a sequence of graphemes, word-piece-model units, phonemes, or bytes.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/494,324 US20240153484A1 (en) | 2022-10-26 | 2023-10-25 | Massive multilingual speech-text joint semi-supervised learning for text-to-speech |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202263381077P | 2022-10-26 | 2022-10-26 | |
US18/494,324 US20240153484A1 (en) | 2022-10-26 | 2023-10-25 | Massive multilingual speech-text joint semi-supervised learning for text-to-speech |
Publications (1)
Publication Number | Publication Date |
---|---|
US20240153484A1 true US20240153484A1 (en) | 2024-05-09 |
Family
ID=88863319
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US18/494,324 Pending US20240153484A1 (en) | 2022-10-26 | 2023-10-25 | Massive multilingual speech-text joint semi-supervised learning for text-to-speech |
Country Status (2)
Country | Link |
---|---|
US (1) | US20240153484A1 (en) |
WO (1) | WO2024091564A1 (en) |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2020242662A1 (en) * | 2019-05-31 | 2020-12-03 | Google Llc | Multilingual speech synthesis and cross-language voice cloning |
-
2023
- 2023-10-25 WO PCT/US2023/035908 patent/WO2024091564A1/en unknown
- 2023-10-25 US US18/494,324 patent/US20240153484A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2024091564A1 (en) | 2024-05-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11605368B2 (en) | Speech recognition using unspoken text and speech synthesis | |
US11929060B2 (en) | Consistency prediction on streaming sequence models | |
US11961515B2 (en) | Contrastive Siamese network for semi-supervised speech recognition | |
US20230104228A1 (en) | Joint Unsupervised and Supervised Training for Multilingual ASR | |
US20220310065A1 (en) | Supervised and Unsupervised Training with Contrastive Loss Over Sequences | |
US20220122581A1 (en) | Using Speech Recognition to Improve Cross-Language Speech Synthesis | |
US20230317059A1 (en) | Alignment Prediction to Inject Text into Automatic Speech Recognition Training | |
US11823697B2 (en) | Improving speech recognition with speech synthesis-based model adapation | |
US20240153484A1 (en) | Massive multilingual speech-text joint semi-supervised learning for text-to-speech | |
US20240029715A1 (en) | Using Aligned Text and Speech Representations to Train Automatic Speech Recognition Models without Transcribed Speech Data | |
US20230017892A1 (en) | Injecting Text in Self-Supervised Speech Pre-training | |
US20230013587A1 (en) | Advancing the Use of Text and Speech in ASR Pretraining With Consistency and Contrastive Losses | |
US20230103722A1 (en) | Guided Data Selection for Masked Speech Modeling | |
US20230298565A1 (en) | Using Non-Parallel Voice Conversion for Speech Conversion Models | |
US20240013777A1 (en) | Unsupervised Data Selection via Discrete Speech Representation for Automatic Speech Recognition | |
US20240153495A1 (en) | Multi-Output Decoders for Multi-Task Learning of ASR and Auxiliary Tasks | |
CN117597729A (en) | Use of advanced text and speech in ASR pre-training with consistency and contrast loss | |
WO2024086265A1 (en) | Context-aware end-to-end asr fusion of context, acoustic and text representations | |
KR20240068699A (en) | Guided data selection for masked speech modeling |