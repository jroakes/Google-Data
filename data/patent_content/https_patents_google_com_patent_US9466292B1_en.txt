US9466292B1 - Online incremental adaptation of deep neural networks using auxiliary Gaussian mixture models in speech recognition - Google Patents
Online incremental adaptation of deep neural networks using auxiliary Gaussian mixture models in speech recognition Download PDFInfo
- Publication number
- US9466292B1 US9466292B1 US13/886,620 US201313886620A US9466292B1 US 9466292 B1 US9466292 B1 US 9466292B1 US 201313886620 A US201313886620 A US 201313886620A US 9466292 B1 US9466292 B1 US 9466292B1
- Authority
- US
- United States
- Prior art keywords
- audio signal
- speaker
- neural network
- transformed
- feature vectors
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 138
- 239000000203 mixture Substances 0.000 title claims abstract description 29
- 230000006978 adaptation Effects 0.000 title abstract description 39
- 230000005236 sound signal Effects 0.000 claims abstract description 158
- 238000000034 method Methods 0.000 claims abstract description 74
- 239000013598 vector Substances 0.000 claims description 221
- 238000012545 processing Methods 0.000 claims description 45
- 230000006870 function Effects 0.000 claims description 42
- 238000003860 storage Methods 0.000 claims description 38
- 238000013518 transcription Methods 0.000 claims description 14
- 230000035897 transcription Effects 0.000 claims description 14
- 238000013500 data storage Methods 0.000 claims description 10
- 230000002123 temporal effect Effects 0.000 claims description 9
- 230000003595 spectral effect Effects 0.000 claims description 8
- 230000008569 process Effects 0.000 abstract description 20
- 239000011159 matrix material Substances 0.000 description 20
- 238000012549 training Methods 0.000 description 20
- 238000003491 array Methods 0.000 description 17
- 238000004891 communication Methods 0.000 description 17
- 238000013459 approach Methods 0.000 description 14
- 238000004458 analytical method Methods 0.000 description 11
- 241000282326 Felis catus Species 0.000 description 10
- 230000001419 dependent effect Effects 0.000 description 10
- 238000009826 distribution Methods 0.000 description 9
- 230000007704 transition Effects 0.000 description 9
- 238000013507 mapping Methods 0.000 description 6
- 238000004422 calculation algorithm Methods 0.000 description 5
- 238000010586 diagram Methods 0.000 description 4
- 238000007476 Maximum Likelihood Methods 0.000 description 3
- 238000013461 design Methods 0.000 description 3
- 238000012048 forced swim test Methods 0.000 description 3
- 238000012417 linear regression Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000015572 biosynthetic process Effects 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 238000007620 mathematical function Methods 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 239000011295 pitch Substances 0.000 description 2
- 238000001228 spectrum Methods 0.000 description 2
- 238000003786 synthesis reaction Methods 0.000 description 2
- PXFBZOLANLWPMH-UHFFFAOYSA-N 16-Epiaffinine Natural products C1C(C2=CC=CC=C2N2)=C2C(=O)CC2C(=CC)CN(C)C1C2CO PXFBZOLANLWPMH-UHFFFAOYSA-N 0.000 description 1
- 235000007688 Lycopersicon esculentum Nutrition 0.000 description 1
- 240000003768 Solanum lycopersicum Species 0.000 description 1
- 244000061456 Solanum tuberosum Species 0.000 description 1
- 235000002595 Solanum tuberosum Nutrition 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000000295 complement effect Effects 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 238000005538 encapsulation Methods 0.000 description 1
- 230000003203 everyday effect Effects 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 238000013515 script Methods 0.000 description 1
- 238000012163 sequencing technique Methods 0.000 description 1
- 238000010183 spectrum analysis Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000001052 transient effect Effects 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/065—Adaptation
- G10L15/07—Adaptation to the speaker
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/14—Speech classification or search using statistical models, e.g. Hidden Markov Models [HMMs]
Definitions
- ASR Automatic speech recognition
- Some ASR systems use “training” where an individual speaker reads sections of text into the speech recognition system. These systems analyze a specific voice of a person and use the voice to fine tune recognition of that speech for that person resulting in more accurate transcription.
- Systems that do not use training may be referred to as “Speaker Independent” systems.
- Systems that use training may be referred to as “Speaker Dependent” systems.
- a framework for facilitating real-time speech recognition performance using neural networks is provided.
- An online incremental feature-space adaptation may be performed for audio signals having speech content using an acoustic model.
- Speaker-adapted features can be used to improve the speech recognition performance of the NNs models.
- a method in one aspect, includes receiving, by a computing device, an audio signal at a first time and a subsequent audio signal at a second time different from the first time, where the audio signal and the subsequent audio signal include speech content.
- the method further includes applying a speaker-specific feature transform to the audio signal to obtain a transformed audio signal.
- the speaker-specific feature transform may be determined based on one or more speaker-specific speech characteristics of a speaker-profile relating to the speech content.
- the method also includes processing, by the computing device, the transformed audio signal using a neural network trained to estimate a given speech content of the audio signal.
- the method further includes modifying the speaker-specific feature transform based on an output of the neural network; and applying the modified speaker-specific feature transform to a subsequent audio signal of the audio signals to obtain a respective transformed audio signal to be processed by the neural network to estimate a respective speech content of the subsequent audio signal.
- a computer readable medium having stored thereon instructions that, when executed by a computing device, cause the computing device to perform functions.
- the functions comprise receiving an audio signal at a first time and a subsequent audio signal at a second time different from the first time, where the audio signal and the subsequent audio signal include speech content.
- the functions further comprise applying a speaker-specific feature transform to the audio signal to obtain a transformed audio signal.
- the speaker-specific feature transform may be determined based on one or more speaker-specific speech characteristics of a speaker-profile relating to the speech content.
- the functions also comprise processing the transformed audio signal using a neural network trained to estimate a given speech content of the audio signal.
- the functions also comprise modifying the speaker-specific feature transform based on an output of the neural network to obtain a modified speaker-specific feature transform; and applying the modified speaker-specific feature transform to a subsequent audio signal of the audio signals to obtain a respective transformed audio signal to be processed by the neural network to estimate a respective speech content of the subsequent audio signal.
- a system comprises at least one processor, and data storage comprising instructions executable by the at least one processor to cause the system to perform functions.
- the functions comprise receiving an audio signal at a first time and a subsequent audio signal at a second time different from the first time, where the audio signal and the subsequent audio signal include speech content.
- the functions further comprise applying a speaker-specific feature transform to the audio signal to obtain a transformed audio signal.
- the speaker-specific feature transform may be determined based on one or more speaker-specific speech characteristics of a speaker-profile relating to the speech content.
- the functions also comprise processing the transformed audio signal using a neural network trained to estimate a given speech content of the audio signal.
- the functions also comprise modifying the speaker-specific feature transform based on an output of the neural network to obtain a modified speaker-specific feature transform; and applying the modified speaker-specific feature transform to a subsequent audio signal to obtain a respective transformed audio signal to be processed by the neural network to estimate a respective speech content of the subsequent audio signal.
- a system comprising a means for receiving an audio signal at a first time and a subsequent audio signal at a second time different from the first time, where the audio signal and the subsequent audio signal include speech content.
- the system also includes a means for applying a speaker-specific feature transform to the audio signal to obtain a transformed audio signal.
- the speaker-specific feature transform may be determined based on one or more speaker-specific speech characteristics of a speaker-profile relating to the speech content.
- the system further includes a means for processing the transformed audio signal using a neural network trained to estimate a given speech content of the audio signal.
- the system also includes a means for modifying the speaker-specific feature transform based on an output of the neural network to obtain a modified speaker-specific feature transform; and a means for applying the modified speaker-specific feature transform to a subsequent audio signal to obtain a respective transformed audio signal to be processed by the neural network to estimate a respective speech content of the subsequent audio signal.
- FIG. 1 illustrates an example Automatic Speech Recognition (ASR) system, in accordance with an example embodiment
- FIG. 2 illustrates aspects of an example acoustic model, in accordance with an embodiment.
- FIG. 3 is a schematic illustration of processing of feature vectors with a neural network (NN) to determine NN-based emission probabilities for hidden Markov models, in accordance with an example embodiment.
- NN neural network
- FIG. 4 is a schematic illustration of applying NN-based emission probabilities determined by a neural network to hidden Markov models to determine speech content represented in feature vectors, in accordance with an example embodiment.
- FIG. 5 is a schematic illustration of processing of feature vectors with a Gaussian mixture model (GMM) to determine GMM-based emission probabilities for hidden Markov models, in accordance with an example embodiment.
- GMM Gaussian mixture model
- FIG. 6 is a schematic illustration of applying GMM-based emission probabilities determined by a Gaussian mixture model to hidden Markov models to determine speech content represented in feature vectors, in accordance with an example embodiment.
- FIG. 7 illustrates an ASR system with speaker adaptation, in accordance with an example embodiment
- FIG. 8 is a block diagram illustrating a system for online incremental adaptation for a neural network, in accordance with an embodiment.
- FIG. 9 is a flowchart of an example method for online incremental adaptation of neural networks using auxiliary Gaussian mixture models in speech recognition, in accordance with an embodiment.
- FIG. 10 illustrates speaker-adapted state-level score combination of deep neural network and Gaussian mixture model, in accordance with an embodiment.
- FIG. 11 illustrates an example distributed computing architecture, in accordance with an example embodiment.
- FIG. 12A is a block diagram of an example computing device, in accordance with an example embodiment illustrates.
- FIG. 12B illustrates a cloud-based server system, in accordance with an example embodiment.
- ASR automatic speech recognition
- voice control can be used to convert a voice search query into a text string that can be sent to a search engine to obtain search results.
- ASR can be performed at the device that receives utterances from a speaker.
- each user device may be configured with an ASR module.
- speech recognition can be performed at a remote network server (e.g., a server or cluster of servers on the Internet). While, in this example, speech recognition may not incorporate ASR into user devices, the user devices may still be configured to have a communication path with the remote ASR system (e.g., through Internet access).
- speech recognition can be performed by use of a local ASR system that offloads performance of at least some aspects of ASR to remote devices.
- the local ASR system may be a dedicated device or devices performing ASR, or software configured to operate, for instance, on a general purpose computing platform.
- This local ASR system may be physically located in a residence, business, vehicle, etc., and may operate even if the user devices do not have Internet access.
- a user device may receive an utterance from a speaker, and transmit a representation of the utterance to the local ASR system.
- the local ASR system may transcribe the representation of the utterance into a textual representation of the utterance, and transmit this textual representation to the user device.
- the local ASR system may instead transmit a command based on a transcription of the utterance to the user device. This command may be based on a transcribed textual representation of the utterance, or may be derived more directly from the representation of the utterance.
- the command may also be of a command set or command language supported by the user device.
- the utterance may represent a voice search query
- the local ASR system may be configured to transmit the transcription of the voice search query to a search engine to obtain respective search results that can be communicated to the user device.
- FIG. 1 illustrates an example ASR system, in accordance with an embodiment.
- input to the ASR system may include an utterance 100
- the output may include one or more text strings and possibly associated confidence levels 101 .
- the ASR system may include a feature analysis module 102 that may be configured to produce feature vectors 104 , a pattern classification module 106 , an acoustic model 108 , a dictionary 110 , and a language model 112 .
- the pattern classification module 106 may incorporate various aspects of the acoustic model 108 , the dictionary 110 , and the language model 112 .
- the feature analysis module 102 may be configured to receive the utterance 100 .
- the utterance 100 may include an analog or digital representation of human speech, and may possibly contain background noise as well.
- the feature analysis module 102 may be configured to convert the utterance 100 to a sequence of one or more feature vectors 104 .
- Each feature vector of the feature vectors 104 may include temporal and/or spectral representations of acoustic features of at least a portion of the utterance 100 .
- a feature vector may include Mel Filter Cepstral coefficients, Perceptual Linear Predictive coefficients, Relative Spectral coefficients, or Filterbank log-energy coefficients of such a portion.
- the mel-frequency cepstrum coefficients may represent the short-term power spectrum of a portion of the utterance 100 . They may be based on, for example, a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. (A mel scale may be a scale of pitches subjectively perceived by listeners to be about equally distant from one another, even though the actual frequencies of these pitches are not equally distant from one another).
- the feature analysis module 102 may be configured to sample and quantize the utterance 100 , divide the utterance 100 into overlapping or non-overlapping frames of 15 milliseconds, and perform spectral analysis on the frames to derive the spectral components of each frame.
- the feature analysis module 102 may further be configured to perform noise removal, convert the standard spectral coefficients to mel-frequency cepstrum coefficients, and calculate first-order and second-order cepstral derivatives of the mel-frequency cepstrum coefficients.
- the first-order cepstral coefficient derivatives may be calculated based on the slopes of linear regressions performed over windows of two or more consecutive frames.
- the second-order cepstral coefficient derivatives may be calculated based on the slopes of linear regressions performed over windows of two or more consecutive sets of first-order cepstral coefficient derivatives.
- one or more frames of the utterance 100 may be represented by a feature vector of mel-frequency cepstrum coefficients, first-order cepstral coefficient derivatives, and second-order cepstral coefficient derivatives.
- the feature vector may contain 13 coefficients, 13 first-order derivatives, and 13 second-order derivatives, therefore having a length of 39.
- feature vectors may use different combinations of features in other possible examples.
- the pattern classification module 106 may be configured to receive a sequence of the feature vectors 104 from the feature analysis module 102 and produce, as output, one or more text string transcriptions 101 of the utterance 100 .
- Each transcription 101 may be accompanied by a respective confidence level indicating an estimated likelihood that the transcription is correct (e.g., 80% confidence, 90% confidence, etc.).
- the pattern classification module 106 may be configured to include, or incorporate aspects of the acoustic model 108 , the dictionary 110 , and/or the language model 112 . In some examples, the pattern classification module 106 may also be configured to use a search graph that represents sequences of word or sub-word acoustic features that appear in spoken utterances.
- the acoustic model 108 may be configured to determine probabilities that the feature vectors 104 may have been derived from a particular sequence of spoken words and/or sub-word sounds. This may involve mapping sequences of the feature vectors 104 to one or more phonemes, and then mapping sequences of phonemes to one or more words.
- a phoneme may be considered to be the smallest segment of an utterance that encompasses a meaningful contrast with other segments of utterances.
- a word typically includes one or more phonemes.
- phonemes may be thought of as utterances of letters; however, some phonemes may present multiple letters.
- An example phonemic spelling for the American English pronunciation of the word “cat” may be /k/ /ae/ /t/, including the phonemes /k/, /ae/, and/t/.
- Another example phonemic spelling for the word “dog” may be /d/ /aw/ /g/, including the phonemes /d/, /aw/, and/g/.
- the letter “a” may be represented by the phoneme /ae/ for the sound in “cat,” by the phoneme /ey/ for the sound in “ate,” and by the phoneme /ah/ for the sound in “beta.”
- Other phonemic representations are possible.
- the acoustic model 108 may be configured to estimate the phoneme(s) in a feature vector by comparing the feature vector to the distributions for each of the 40 phonemes, and finding one or more phonemes that are most likely represented by the feature vector.
- the acoustic model 108 may include a hidden Markov model (HMM).
- HMM hidden Markov model
- An HMM may model a system as a Markov process with unobserved (i.e., hidden) states.
- Each HMM state may be represented as a multivariate Gaussian distribution that characterizes the statistical behavior of the state.
- each state may also be associated with one or more state transitions that specify a probability of making a transition from a current state to another state.
- the combination of the multivariate Gaussian distribution and the state transitions for each state may define a time sequence of feature vectors over the duration of one or more phonemes.
- the HMM may model the sequences of phonemes that define words.
- some HMM-based acoustic models may also take into account phoneme context when mapping a sequence of feature vectors to one or more words.
- FIG. 2 illustrates aspects of an example acoustic model 200 , in accordance with an embodiment.
- the acoustic model 200 defines a sequence of phonemes that make up the word “cat.”
- Each phoneme is represented by a 3-state HMM with an initial state, a middle state, and an end state representing the statistical characteristics at the beginning of phoneme, the middle of the phoneme, and the end of the phoneme, respectively.
- Each state e.g., state /k/1, state /k/2, etc.
- the acoustic model 200 may represent a word by concatenating the respective 3-state HMMs for each phoneme in the word together, with appropriate transitions. These concatenations may be performed based on information in the dictionary 110 . In some implementations, more or fewer states per phoneme may be used in the acoustic model 200 .
- the acoustic model 200 may be trained using recordings of each phoneme in numerous contexts (e.g., various words and sentences) so that a representation for each of the phoneme's states can be obtained. These representations may encompass the multivariate Gaussian distributions discussed above.
- a possibly large number of utterances containing spoken phonemes may each be associated with transcriptions. These utterances may be words, sentences, and so on, and may be obtained from recordings of everyday speech or some other source.
- the transcriptions may be automatic or manual (human-made) text strings of the utterances.
- the utterances may be segmented according to their respective transcriptions. For instance, training of the acoustic model 200 may involve segmenting spoken strings into units (e.g., using either a Baum-Welch and/or Viterbi alignment method), and then using the segmented utterances to build statistical distributions for each phoneme state.
- acoustic model 200 may involve segmenting spoken strings into units (e.g., using either a Baum-Welch and/or Viterbi alignment method), and then using the segmented utterances to build statistical distributions for each phoneme state.
- a more accurate acoustic model can be produced.
- a well-trained acoustic model may have limited accuracy when used for ASR in a domain for which it was not trained. For instance, if a given acoustic model is trained by utterances from a number of speakers of American English, this acoustic model may perform well when used for ASR of American English, but may be less accurate when used for ASR of, e.g., British English.
- the acoustic model 200 when trained in this fashion may represent the pronunciation and usage of a hypothetical average speaker, rather than any particular speaker.
- the dictionary 110 may define a pre-established mapping between phonemes and words. This mapping may include a list of tens or hundreds of thousands of phoneme-pattern-to-word mappings, for example.
- the dictionary 110 may include a lookup table, such as Table 1 shown below. Table 1 illustrates how the dictionary 110 may list phonemic sequences that the pattern classification module 106 may be configured to identify for corresponding words that the ASR system is attempting to recognize. Therefore, the dictionary 110 may be used when developing phonemic state representations of words that are illustrated by the acoustic model 200 .
- the language model 112 may be configured to assign probabilities to sequences of phonemes or words, based on a likelihood of that sequence of phonemes or words occurring in an input utterance to the ASR system.
- the language model 112 may define a conditional probability of w n (for n th word in a phrase transcribed from an utterance), given values of a pattern of n ⁇ 1 previous words in the phrase.
- An example conditional probability may be expressed as: P ( w n
- a language model may operate on n-grams, which, for example, may be sequences of n phonemes or words that are represented in the pattern classification module 106 .
- Language models with values of n greater than 5 can require a large memory or storage space; therefore, smaller n-grams (e.g., 3-grams, which are also referred to as tri-grams) may be used to yield acceptable results efficiently.
- Tri-grams are used herein for purposes of illustration. Nonetheless, any value of n may be may be used with the examples herein.
- Language models may be trained through analysis of a corpus of text strings or sequences of words.
- This corpus may contain a large number of words, e.g., hundreds, thousands, millions or more. These words may be derived from utterances spoken by users of an ASR system and/or from written documents.
- the language model 112 can be determined or developed based on word patterns occurring in human speech, written text (e.g., emails, web pages, reports, academic papers, word processing documents, etc.), search queries, and so on.
- tri-gram probabilities can be estimated based on their respective number of appearances in the corpus.
- C(w 1 , w 2 , w 3 ) is the number of occurrences of a sequence of words w 1 , w 2 , w 3 in the corpus
- a probability of occurrence for the sequence of words can be expressed as:
- the language model 112 may be represented as a table of conditional probabilities.
- Table 2 illustrates an example of such a table that could form the basis of the language model 112 .
- Table 2 contains tri-gram conditional probabilities.
- Table 2 indicates that, based on observed occurrences in the corpus, 50% of the time the next 1-gram is “dog.” Likewise, 35% of the time, the next 1-gram is “mouse,” 14% of the time the next 1-gram is “bird,” and 1% of the time the next 1-gram is “fiddle.” In a fully-trained ASR system, the language model 112 would contain many more entries, and these entries may include more than just one 2-gram prefix.
- the feature analysis model 102 and the pattern classification module 106 may be configured to perform ASR.
- the ASR system can search the space of valid word sequences from the language model 112 to find the word sequence with the maximum likelihood of having been spoken in the utterance 100 .
- size of search space can be quite large, and methods to reduce the search space may cause such search to be more computationally efficient.
- heuristic techniques that can be used to reduce the complexity of the search, potentially by orders of magnitude. Other methods of limiting the search space are possible.
- the search space can be constrained to popular phrases in a given period of time.
- a finite state transducer can be used to compactly represent multiple phoneme patterns that map to a single word. Some words, such as “data,” “either,” “tomato,” and “potato,” have multiple pronunciations. The phoneme sequences for these pronunciations can be represented in a single FST per word.
- This process of creating efficient phoneme-level FSTs can be carried out for each word in the dictionary 110 , and the resulting word FSTs can be combined into sentence FSTs using the language model 112 .
- a network of states for phonemes, words, and sequences of words can be developed and represented in a compact search graph.
- the acoustic model 108 may include a neural network (NN) and one or more hidden Markov models (HMMs).
- NN neural network
- HMMs hidden Markov models
- Such an implementation is referred to herein as a “hybrid neural network/hidden Markov model,” and is abbreviated as “HNN/HMM” (or “HNN/HMMs” in reference to a plurality of HMMs).
- HNN/HMM hidden neural network/hidden Markov model
- one or more HMMs are used to model the fundamental speech units (e.g., phonemes, triphones, etc.), while the neural network is used to determine emission probabilities to apply to the models, based on the observed data (e.g., sequence of feature vectors 104 in the example ASR system in FIG. 1 ).
- the fundamental speech units of HMMs will be taken to be triphones, because this is the case for certain ASR systems.
- the principles discussed are not limited to triphones, and that other fundamental speech units can be used (e.g. phonemes, quinphones, clusters of similar and/or related speech units, etc.).
- a triphone may be modeled as temporally evolving according to a sequence of temporal phases.
- triphones may be manifested in speech across three acoustic phases: a start, a middle, and an end.
- the HMM for a given triphone therefore can be constructed having three states, one corresponding to each acoustic phase. Transitions between states are governed by transition probabilities of the model, and one or more states could include self-transitions that “loop” back to themselves.
- each state has an associated emission probability for emitting an output corresponding to the acoustic phase of the triphone.
- the HMM for a given triphone is characterized by probabilities of transitioning from a current state to a next state, and upon transitioning, a respective probability of producing (emitting) the acoustic phase associated with the next state.
- the emission probabilities may be determined by the neural network, based on the observed utterance as represented in feature vectors derived from a given utterance.
- the triphone sequence could be modeled with three HMM states each.
- the triphone “#[d]ae” could be modeled according to states corresponding to “#[d]ae.1,” “#[d]ae.2,” and “#[d]ae.3,“where the”0.1,” “0.2,“and”0.3” signify a temporal order of the states in the HMM for the triphone “#[d]ae.”
- the triphone “d[ae]d” could be modeled with a HMM having states corresponding to “d[ae]d.1,” “d[ae]d.2,” and “d[ae]d.3,” and the triphone “ae[d]#” could be modeled with a HMM having states corresponding to “ae[d]#1,” “ae[d]#2,” “ae[d]#3.” This description could be generalized to different number of acoustic phases of triphones (as well
- the sequential feature vectors 104 derived from the utterance 100 represent a stream of observed acoustic data, while sequential states of one or more concatenated HMMs represent sequences of acoustic phases of triphones in the corpus that probabilistically correspond to the observed acoustic data. While the possible states and their associated transition and emission probabilities of the HMMs may be known, the specific state sequences associated with any given observed sequence of feature vectors is not a priori known (hence the term “hidden”). Recognition of speech in the input utterance 100 therefore involves determining the most probable sequence (or sequences) of states of one or more concatenated HMMs that would produce the observed feature vectors 104 . The most probable sequence of states then corresponds to the most probable sequence of triphones (including acoustic phases), from which the output 101 can be determined.
- the determination of the most probable sequences of HMMs and states is carried out one step at a time, where each step corresponds to a feature vector in the sequence of feature vectors 104 , and by extension to a frame of sampled audio data.
- the process can be guided at each new step by the results of the previous step, because the most probable state determined for the previous step may constrain the possible (allowed) states that can be transitioned to on the next step.
- the NN determines a conditional probability that the particular feature vector would be emitted given the allowed next state.
- the NN may be trained before run time to recognize feature vectors as input, and to generate associated conditional probabilities as output. Then, at each time step corresponding to a frame at run time, the NN, based on what NN has “learned” during training, generates a posterior conditional probability of being in the particular allowed next state, given the observed run-time feature vector.
- the emission probability for each particular allowed next state is a prior conditional probability of emitting the observed feature vector, given that the HMM is in the particular allowed next state.
- the prior conditional probability i.e., the emission probability—can be related to the posterior conditional probability through Bayes rule, for example.
- the NN may be trained to be able to produce, at run time, the posterior conditional probability p(q k
- the training of the NN may take place before run time, using training data (e.g., from the corpus).
- Bayes rule can be expressed as:
- the probabilities p(x j ) are the same for all states at run time, and so may be treated as a scaling constant in the expression for Bayes rule. Therefore, the a priori emission probabilities p(x j
- q k ) for the q k , k 1, . . . , K states follow from Bayes rule (equation [1] above) applied at run time for the HMM states.
- the most probable next state for that time step can be determined as the one that maximizes the combined likelihood of being transitioned to, and emitting the observed feature vector.
- the most probable sequence of states corresponding to a sequence of feature vectors is determined, and from which follows the most probable sequence of fundamental speech units in the corpus and a reconstruction of the utterance in the audio input signal.
- One of the aspects of using a neural network for determining the emission probabilities is that correlations among feature vectors are accounted for naturally in the “learning” process during training. Consequently, categorization of feature vectors corresponding to the speech samples of the corpus can avoid simplifying assumptions often required by other analysis techniques, such as Gaussian mixture models, to deal with statistical complexities. Moreover, the ability of neural networks to naturally account for correlations among feature vectors also enables determination of the probabilities for a given input feature vector to include input from a sub-sequence of feature vectors preceding and/or following the given feature vector. Feature vectors preceding and/or following a given feature vector can provide additional context for the neural network.
- ANNs Artificial neural networks
- feed-forward networks may take the form of a multiplicity of interconnected “layers,” each including a set of “nodes.”
- a typical architecture may include an input layer, and output layer, and one or more intervening layers, commonly referred to as “hidden” layers.
- Each node in a given layer may correspond to a mathematical function for computing a scalar output of one or more inputs.
- the nodes of the input layer may each receive just one input at a given computational step (e.g., time step), the total number of inputs to the neural network being the total number of nodes in the input layer.
- the computed outputs of each input-layer node may then serve as input to each node of the next (forward) layer.
- the nodes of the output layer deliver the output of the neural network, the total number of outputs of neural network being the total number of nodes in the output layer.
- All of the nodes may be the same scalar function, differing according to possibly different parameter values, for example.
- the mathematical function could take the form of a sigmoid function, in which case each node could compute a sigmoidal nonlinearity of a weighted sum of its inputs.
- Other functional forms could be used as well.
- Training a neural network may involve adjusting parameter values to achieve, to a given level of confidence, known results from known input data.
- a variety of techniques may be used to train a neural network, including stochastic gradient descent, batch gradient descent, second order methods, Hessian-free optimization, and gradient boost, among possibly others.
- a neural network to speech recognition involves providing one or more feature vectors as input, and delivering emission probabilities as output.
- the effectiveness and/or accuracy of a neural network may depend, at least in part, on the number of nodes per layer, and the number of hidden layers between the input and output layers.
- DNN Deep Neural Network
- a HNN/HMM speech recognition system may include a DNN for generation of emission probabilities.
- a DNN for generation of emission probabilities.
- predicted emission probabilities for a given sequence of input feature vectors may be accurately predicted, correspondingly supporting accurate speech recognition.
- a DNN can learn to accurately predict emission probabilities given run-time feature vectors.
- a neural network e.g., a DNN
- layers, nodes, and connections between nodes may be implemented as executable instructions stored in one or another form of non-transient computer readable media, and executed by one of more processors of speech synthesis system, for example.
- FIG. 3 is a schematic illustration of processing of feature vectors with a neural network (e.g., a DNN) to determine emission probabilities for hidden Markov models, in accordance with an example embodiment.
- a time sequence of feature vectors 301 is represented by a “staircase” of overlapping rectangles labeled, by way of example, N ⁇ 2, N ⁇ 1, . . . , N, N+1, . . . , N+7, where each label corresponds to a frame time step at which the input audio data was acquired (e.g., digitally sampled).
- each feature vector corresponds to a frame of sampled audio input (e.g., an utterance), and that each frame may be acquired in a sliding time window.
- each frame i.e., the time window
- the time increment between successive windows is 10 ms.
- each next frame overlaps the previous frame by 15 ms.
- the time increment between frames e.g., 10 ms in the present example
- the frame period can be referred to as the frame period
- the inverse of the frame period can be referred as the frame rate ( 100 frames per second in the present example).
- the feature vectors 301 in FIG. 3 may be the output of sampling and digital processing, such as by the feature analysis module 102 shown in FIG. 1 .
- the frame-like representation of the feature vectors 301 may thus be taken as a visual cue that digital samples of the input utterance 100 may be acquired in time frames using a sliding window, and then subject to feature extraction.
- t ack,i may also be considered the time at which feature extraction is performed, although this is not necessarily a restriction of embodiments described herein.
- the legend at lower right of FIG. 3 reiterates the meanings of t i and t ack,i .
- feature vectors corresponding to frame times t ack,N , t ack,N+1 , . . . , t ack,N+5 are shown as being input to the neural network at neural network time steps t N , t N+1 , . . . , t N+5 .
- each feature vector is shown as being accompanied by two preceding (left-context) and two following (right-context) feature vectors corresponding to preceding and following frame acquisition times.
- the input to the neural network 302 at neural network time step t N includes the feature vector labeled N, together with feature vectors labeled N ⁇ 2, N ⁇ 1, N+1, and N+2, corresponding to frame acquisition times t ack,N-2 , t ack,N-1 , t ack,N , t ack,N+1 , and t ackN+2 .
- the input to the neural network 302 at neural network time step t N+1 includes the feature vector labeled N+1, together with feature vectors labeled N ⁇ 1, N, N+2, and N+3, corresponding to frame acquisition times t ackN ⁇ 1 , t ackN , t ack,N+1 , t ack,N+2 , and t ackN+3 .
- This pattern is extended in the figure up to neural network time step t N+5 for the feature vector labeled N+5, together with feature vectors labeled N+3, N+4, N+6, and N+7, corresponding to frame acquisition times t ack,N+3 , t ack,N+4 , t ack,N+5 , t ack,N+6 , and t ackN+7 .
- Other arrangements of multiple feature vector input could be used. For instance, each feature vector could be accompanied by four preceding and four following feature vectors. In addition, the number of preceding and following feature vectors need not be equal.
- q k ) for q k , k 1, . . . , K HMM states according, for example, to equation [1].
- the neural network 302 may be considered as operating at the input frame rate.
- q k ) for q k , k 1, . . . , K HMM states according, for example, to equation [1].
- the neural network 302 may be considered as operating at the input frame rate.
- a set of K emission probabilities 303 is generated at each of neural network time steps t N , t N+1 , t N+2 , t N+3 , t N+4 , and t N+5 .
- the output set of emission probabilities may apply to the HMM states at just one frame time, corresponding to just one frame of input audio data.
- FIG. 4 is a schematic illustration of applying NN-based emission probabilities determined by a neural network to hidden Markov models to determine speech content represented in feature vectors, in accordance with an example embodiment.
- FIG. 4 depicts a graph of observed acoustic data along a time (horizontal) axis versus HMM states along a vertical axis.
- an example utterance 401 of “cat sat” is input to an audio processing module 402 , which samples the input in frames and outputs a time sequence of feature vectors 403 .
- the feature vectors 403 are then input to a neural network 404 , which outputs respective sets of emission probabilities at each neural network time step.
- the feature vectors 403 may be considered analogous to the feature vectors 301 shown in FIG.
- Output of the emission probabilities at neural network time steps is represented as a series of short vertical arrows at times marked along the horizontal time axis, and occurs at the frame rate.
- a multiplicity of HMMs 405 - 1 , 405 - 2 , 405 - 3 , 405 - 4 , 405 - 5 , and 405 - 6 is represented as a portion of a concatenation of HMM states pictured along the vertical axis in FIG. 4 .
- Each HMM is used to model a respective triphone, and includes three states corresponding to three acoustic phases of the respective triphone.
- Each state is represented as a circle enclosing a state label q k , such as q 1 , q 2 , q 3 , etc.
- An arrow connecting adjacent states signifies a transition between the connected states, while a loop-shaped arrow signifies a “self-transition” that leaves a state unchanged after a given time step.
- the HMM 405 - 1 includes states q 1 , q 2 , and q 3 for modeling the triphone states #[k]ae.1, #[k]ae.2, and #[k]ae.3 of the triphone #[k]ae.
- the HMM 405 - 2 includes states q 4 , q 5 , and q 6 for modeling the triphone states k[ae]t.1, k[ae]t.2, and k[ae]t.3 of the triphone k[ae]t.
- the HMM 405 - 3 includes states q 7 , q 8 , and q 9 for modeling the triphone states ae[t]#.1, ae[t]#2, and ae[t]#3 of the triphone ae[t]#;
- the HMM 405 - 4 includes states q 10 , q 11 , and q 12 for modeling the triphone states #[s]ae.1, #[s]ae.2, and #[s]ae.3 of the triphone #[s]ae;
- the HMM 405 - 5 includes states q 4 , q 5 , and q 6 for modeling the triphone states s[ae]t.1, s[ae]t.2, and s[ae]t.3 of the triphone s[ae]t;
- the HMM 405 - 6 includes states q 7 , q 8 , and q 9 for modeling the triphone states ae[t]#.1, ae[t
- the HMM 405 - 2 for k[ae]t and the HMM 405 - 5 for s[ae]t are made up of the same states q 4 , q 5 , and q 6 .
- This repetition of states is meant to represent how HMM and HMM states may be shared among similar triphones.
- the HMM 405 - 3 for ae[t]# and the HMM 405 - 6 also for ae[t]# are made up of the same states q 7 , q 8 , and q 9 .
- the sharing of states is an example of “clustering” of similar triphones, which may help reduce the number of states that needs to be considered at each time step, as described below.
- the neural network 404 outputs of K emission probabilities for the states of the HMMs at each neural network time step (i.e., at the frame rate).
- K emission probabilities By applying the K emission probabilities to the K HMM states, one of the K states is determined to be most probable at each neural network time step.
- a path 409 through the graph of observed acoustic data versus HMM states is mapped out by connecting successive points in the graph, also at the frame rate.
- the path 409 then represents the most likely sequence of HMMs and HMM states, and thereby yields the sequence of triphones in the corpus that most probably corresponds to the input utterance 401 , as represented in the feature vectors 403 .
- a set of emission probabilities 407 is shown as being output from the neural network 404 at a current neural network time step t N .
- the emission probabilities 407 are labeled as p 1 , p 2 , p 3 , p 4 , . . . , and may be applied to similarly indexed HMM states. Note that p 4 , p 5 , and p 6 are repeated for the HMMs 405 - 2 and 405 - 5 . Similarly, p 7 , p 8 , and p 9 are repeated for the HMMs 405 - 3 and 405 - 6 .
- the HMM state q 4 of the HMM 405 - 5 is the most probable next state in this example.
- the immediately preceding state in this example was also q 4 of the HMM 405 - 5 .
- a legend at the lower right of FIG. 4 reiterates the proportional relation between the a priori emission probabilities and the a posteriori conditional probabilities generated by the neural network.
- HMM 405 - 6 there may be additional HMMs (and states) available to model the input utterance 401 .
- HMMs and states
- Clustering of similar triphones and/or triphone acoustic phases, plus constraints that may rule out certain sequences of states, may help reduce this number to approximately 8,000 HMM states. Clustering is represented in FIG.
- the acoustic model 108 may also include an implementation (e.g., one or more coded algorithms) of a Gaussian mixture model (GMM).
- GMM Gaussian mixture model
- the singular term GMM applies to a collection or one or more mixtures of Gaussian distributions.
- the same one or more HMMs are used to model the fundamental speech units (e.g., phonemes, triphones, etc.).
- the GMM is used to determine emission probabilities to apply to the models, based on the observed data (i.e., sequence of feature vectors 104 in the example ASR system shown in FIG. 1 ).
- the fundamental speech units of HMMs will be taken to be triphones, because this is the case in practice for certain ASR systems.
- the principles discussed herein are not limited to triphones, and that other fundamental speech units can be used (e.g. phonemes, quinphones, clusters of similar and/or related speech units etc.).
- the determination of the most probable sequences of HMMs and states is carried out one step at a time, where each step corresponds to a feature vector in the sequence 104 , and by extension to a frame of sampled audio data.
- the particular set of defining features in the feature vectors used in the GMM approach may not necessarily be the same as the set used in NN approach.
- any such distinction between the defining features in feature vectors used in the GMM and NN approaches may not be apparent in the sequence 104 , which can be considered as input to either approach.
- the process of determining the most probable sequence of states corresponding to the input sequence of feature vectors can be guided at each new step by the results of the previous step, since the most probable state determined for the previous step may constrain the possible (allowed) states that can be transitioned to on the next step.
- the GMM determines a conditional probability that the particular feature vector would be emitted given the allowed next state.
- the GMM may be trained before run time to associate feature vectors with conditional probabilities as output. That is, at each time step corresponding to a frame at run time, GMM is used to determine, for each respective HMM state, a conditional probability of observing the feature vector at that time step given the respective HMM state. Thus for each frame, a plurality of GMM-based conditional probabilities is computed, one for each HMM state.
- a Gaussian mixture model can be described as a weighted sum of M Gaussian densities, given by the expression: p ( x
- x is a D-dimensional continuous-valued vector (i.e., features)
- Each component density is a D-variate Gaussian function of the form:
- ⁇ i , ⁇ i ) 1 ( 2 ⁇ ⁇ ) D / 2 ⁇ ⁇ ⁇ i ⁇ 1 / 2 ⁇ exp ⁇ ⁇ - 1 2 ⁇ ( x - ⁇ 1 ) ′ ⁇ ⁇ i - 1 ⁇ ( x - ⁇ i ) ⁇ , [ 3 ] with mean vector ⁇ i and covariance matrix ⁇ i .
- the parameters are adjusted for known fundamental speech units to produce mixtures that probabilistically represent the features as observed in the known fundamental speech units.
- the fundamental speech units could be phonemes, triphones, or clustering of similar triphones and/or triphone acoustic phases.
- the most probable next state for that time step can be determined in the same manner as in the HNN/HMM approach. Namely, most probable next state is the one that maximizes the combined likelihood of being transitioned to, and emitting the observed feature vector.
- the most probable sequence of states corresponding to a sequence of feature vectors is determined, and from which follows the most probable sequence of fundamental speech units in the corpus and a reconstruction of the utterance in the audio input signal.
- the GMM approach may include simplifying assumptions, such as assuming negligible and/or ignorable correlations of features and/or feature vectors, the implementation costs, both in terms of computational complexity and processing resources, may be significantly smaller than those of the HNN/HMM approach. Moreover, in spite of the simplifying assumptions, the GMM approach may nevertheless yield largely equivalent, or even better, predictive results than the HNN/HMM for a certain subset of fundamental speech units.
- FIG. 5 is a schematic illustration of processing of feature vectors with a Gaussian mixture model (GMM) to determine GMM-based emission probabilities for hidden Markov models, in accordance with an example embodiment.
- a time sequence of feature vectors 501 is represented again by a “staircase” of overlapping rectangles labeled, by way of example, N ⁇ 2, N ⁇ 1, . . . , N, N+1, . . . , N+7, where each label corresponds to a frame time step at which the input audio data was acquired (e.g., digitally sampled).
- the representation of the feature vectors 501 is the same as that of feature vectors 301 in FIG. 3 , and the previous explanation of feature vectors 301 apply to feature vectors 501 as well. As noted above, however, the specific defining features of the feature vectors 501 may be different than those feature vectors 301 .
- the legend at lower right of FIG. 5 reiterates the meanings of t i and t ack,i .
- feature vectors corresponding to frame times t ack,N , t ack,N+1 , . . . , t ack,N+5 are shown at being input to the GMM at neural GMM steps t N , t N+1 , . . . , t N+5 .
- each feature vector is shown as being “accompanied” by two preceding (left-context) and two following (right-context) feature vectors corresponding to preceding and following frame acquisition times.
- each feature vector in sequence 501 could be accompanied by four left-context and four right-context feature vectors.
- the number of left-context and right-context feature vectors need not be equal even within just sequence 501 .
- q k ) for q k , k 1, . . . , K HMM states according, for example. As such, the GMM 502 may be considered as operating at the input frame rate.
- a set of K emission probabilities 503 is generated at each of GMM time steps t N , t N+1 , t N+2 , t N+3 , t N+4 , and t N+5 .
- the output set of emission probabilities may apply to the HMM states at just one frame time, corresponding to just one frame of input audio data.
- FIG. 6 is a schematic illustration of applying GMM-based emission probabilities determined by a Gaussian mixture model to hidden Markov models to determine speech content represented in feature vectors, in accordance with an example embodiment.
- FIG. 6 may be considered analogous to FIG. 4 .
- an example utterance 601 of “cat sat” is input to an audio processing module 602 , which samples the input in frames and outputs a time sequence of feature vectors 603 .
- the feature vectors 603 are then input to a GMM 604 , which outputs respective sets of emission probabilities at each neural network time step.
- the feature vectors 603 may be considered analogous to the feature vectors 501 shown in FIG. 5
- the GMM 604 may be considered analogous to the GMM 502 also in FIG. 5 .
- Output of the emission probabilities at GMM time steps is again represented as a series of short vertical arrows at times marked along the horizontal time axis, and occurs at the frame rate.
- a multiplicity of HMMs 605 - 1 , 605 - 2 , 605 - 3 , 605 - 4 , 605 - 5 , and 605 - 6 is again represented as a portion of a concatenation of HMM states pictured along the vertical axis in FIG. 6 .
- each HMM is used to model a respective triphone, and includes three states corresponding to three acoustic phases of the respective triphone.
- the format of the HMMs is the same as in FIG. 4 .
- the GMM 604 outputs of K emission probabilities for the states of the HMMs at each GMM time step (i.e., at the frame rate).
- K emission probabilities for the states of the HMMs at each GMM time step (i.e., at the frame rate).
- applying the K emission probabilities to the K HMM states determines the most probable next state at each GMM time step.
- a path 609 through the graph of observed acoustic data versus HMM states is mapped out by connecting successive points in the graph, also at the frame rate.
- the path 609 then represents the most likely sequence of HMMs and HMM states, and yields the sequence of triphones in the corpus that most probably corresponds to the input utterance 601 , as represented in the feature vectors 603 .
- the path 609 is depicted as being slightly different than the path 409 .
- a set of emission probabilities 607 is shown as being output from the GMM 604 at a current neural network time step t N .
- the emission probabilities 607 are labeled as b 1 , b 2 , b 3 , b 4 , . . . , and may be applied to similarly indexed HMM states. Note that b 4 , b 5 , and b 6 are repeated for the HMMs 605 - 2 and 605 - 5 . Similarly, b 7 , b 8 , and b 9 are repeated for the HMMs 605 - 3 and 605 - 6 .
- the HMM state q 6 of the HMM 605 - 5 is the most probable next state in this example.
- the immediately preceding state in this example was q 5 of the HMM 605 - 5 .
- a legend at the lower right of FIG. 6 defines the conditional probabilities generated by the GMM.
- Acoustic models may be trained with utterances from multiple speakers in multiple environments.
- a given acoustic model may represent a hypothetical average speaker, and might not perform well when applied to utterances from a speaker whose vocal characteristics differ from those of the hypothetical average speaker. Therefore, ASR systems may attempt to compensate for these differences through speaker adaptation
- FIG. 7 depicts an ASR system with speaker adaptation, in accordance with an example embodiment.
- an utterance is provided as input to feature analysis module 102 , which produces one or more feature vectors based on the utterance.
- These feature vectors may be provided to speaker adaptation module 700 , which may be configured to modify the feature vectors according to one or more of speaker adaptation profile(s) 702 .
- the modified feature vectors may be provided to the pattern classification module 106 , which in turn may produce one or more text string transcriptions of the utterance.
- Speaker adaptation profiles 702 may include default, gender-specific, and/or speaker-dependent profiles.
- a default profile may be a profile that the ASR system uses when no other profile has been selected. For instance, when the ASR system begins speech recognition, the ASR may be configured to apply the default profile to feature vectors. Additionally, the default profile may be applied to utterances received after the ASR system has been idle for some period of time (e.g., 1-10 minutes or more) or after the ASR system detects that the speaker has changed. In some examples, the default profile may not perform any speaker adaptation. Thus, for some default profiles, the modified feature vectors may be the same as the feature vectors.
- Speaker-adaptation profiles 702 may also include one or more environment-specific, speaker-dependent profiles. Each of these profiles may be associated with a particular speaker speaking in a particular environment or location. For example, one such profile might be based on the particular speaker speaking in a quiet location, with little background noise. Another such profile might be based on the particular speaker speaking in an environment with a given type of background noise, such as an office or a car. Thus, an environment-specific, speaker-dependent speaker adaptation profile for the speaker may be based on the characteristics of the input utterances, location of the speaker, and/or the user device that receives the utterance.
- each of the feature vectors may be of a particular length (e.g., n entries), and may include representations of the temporal and/or spectral acoustic features of at least a portion of the utterance.
- the speaker adaptation parameters may take the form of a matrix, for instance, an n ⁇ n matrix.
- the speaker adaptation module 700 may be configured to multiply each feature vector received by the matrix, resulting in updated feature vectors. These updated feature vectors may be transmitted to the pattern classification module 106 .
- the acoustic model used by pattern classification module 106 may be speaker-independent, and the speaker adaptation module 700 may use the matrix to adapt speaker-dependent feature vectors so that they are more likely to be properly recognized by the acoustic model.
- the matrix may be a diagonal matrix (i.e., for each entry (i,j) in the matrix, the entry takes on a non-zero value if i is equal to j, but takes on a value of zero if i is not equal to j). Since at least half of the entries in a 2 ⁇ 2 or greater diagonal matrix contain zeroes, less computation is required to multiply a feature vector by a diagonal matrix than a non-diagonal matrix.
- a non-diagonal matrix refers to a matrix in which at least one entry for which i is not equal to j contains a non-zero value.
- the ASR system may compare the characteristics of received utterances to speech models associated with one or more speaker adaptation profiles. Based on the outcome of this comparison, a new speaker adaptation profile may be selected, or the current speaker adaptation profile may continue to be applied.
- the speech models may be represented as Gaussian mixture models (GMMs).
- GMM may probabilistically represent the likelihood that a particular speaker is speaking based on feature vectors derived from an input utterance.
- a GMM may be a weighted sum of M Gaussian random variables, each with potentially different mean and covariance parameters.
- An example GMM is given by: p ( x
- ⁇ i , ⁇ i ), i 1 . . .
- x is an n-dimensional feature vector
- GMMs can be used to approximate arbitrarily-shaped probability density functions. Thus, GMMs are powerful tools for representing distributions of feature vectors in ASR systems. In some implementations, full covariance matrices are not used, as partial covariance matrices (e.g., diagonal matrices wherein each non-zero entry represents the variance of a particular component Gaussian function) can provide suitable results
- ASR system has access to a set of S speaker adaptation profiles represented by speech models ⁇ 1 , ⁇ 2 , . . . , ⁇ S , respectively.
- a goal of speaker adaptation is to select the profile, ⁇ , with a speech model that has the maximum a posteriori probability of being the closest fit for a series of feature vectors.
- ⁇ circumflex over ( S ) ⁇ argmax 1 ⁇ k ⁇ S p ( ⁇ k
- any value of T may be used. For instance, assuming that a feature vector is derived from 10 milliseconds of an input utterance, anywhere from one to several thousand feature vectors may be evaluated according to the equations above, and a profile that fits a majority of the feature vectors may be selected.
- a speech model for a particular speaker adaptation profile may be trained based on input utterances. For instance, a female-specific speech model may be trained with utterances from various female speakers in various environments.
- a speaker-dependent speech model may be trained with utterances from a particular speaker in various environments.
- An environment-specific, speaker-dependent speech model may be trained with utterances from a particular speaker in a particular environment.
- x t , ⁇ ) ⁇ x t ⁇ t 1 T ⁇ p ⁇ ( i
- x t , ⁇ ) ⁇ x i 2 ⁇ t 1 T ⁇ p ⁇ ( i
- these equations calculate the variances, ⁇ i 2 , rather than the full covariance matrix, ⁇ i .
- these variances can be used to form a diagonal covariance matrix that is sufficient for this example embodiment.
- NNs such as DNNs
- the GMMs have the advantages of easily parallelizable training, fast and efficient speaker adaptation, and being more computationally efficient than DNNs.
- GMMs exhibit different error patterns compared to the DNNs.
- DNNs and GMMs may be complementary.
- DNNs and GMMs may be combined to improve speech recognition performance.
- target distributions for DNN training may be obtained from forced alignments generated by a baseline GMM system, the GMMs can be used directly at run-time or in real-time without training computational overhead.
- Speaker adaptation may be utilized to reduce mismatch between training and decoding conditions.
- Various adaptation techniques can be used with GMM-based acoustic models. These techniques can be divided into two categories: model-space adaptation, and feature-space adaptation.
- Feature-space adaptation may not require modifying the entire acoustic model.
- feature-space adaptation may be suited for real-time ASR server systems.
- Constrained maximum likelihood linear regression (CMLLR), also called feature-space MLLR (fMLLR) is an example of feature-space adaptation that can be used with a GMM-based acoustic model for real-time speech recognition.
- CMLLR Constrained maximum likelihood linear regression
- fMLLR feature-space MLLR
- Unsupervised online fMLLR may be configured to estimate and update a speaker-specific affine transformation of the feature vectors during decoding.
- o t be the n-dimensional feature vector at time frame t
- A is n ⁇ n rotation/scaling matrix
- b is n ⁇ 1 bias term.
- fMLLR may improve the recognition accuracy with 5 seconds of speech data, for example. In examples, with sufficient amount of adaptation data, fMLLR may achieve 10% to 20% relative improvement in terms of word error rate (WER), compared to a speaker independent baseline. Because fMLLR may be configured to adapt input feature vectors, fMLLR may be used to transform the feature vector for DNNs, if the DNNs share the same feature space as the underlying GMMs.
- WER word error rate
- FIG. 8 is a block diagram illustrating a system 800 for online incremental adaptation for a neural network, in accordance with an embodiment.
- a computing device may be configured to extract feature vectors 801 , o 1:T .
- the computing device may be configured to apply the transform at block 806 to feature vectors 801 to transform the feature vectors 801 to transformed feature vectors 808 , ô 1,T , adapted to a specific speaker.
- the computing device may be configured to process the transformed feature vectors 808 by a DNN decoding block 810 to determine speech content of the input utterances.
- outputs 812 e.g., alignments
- the GMM fMLLR block 802 may be configured to utilize the outputs 812 to modify the transform 804 .
- the computing device may be configured to apply the modified transform to a consecutive utterance or audio signal.
- the transform 804 may be initialized to an identify transform (e.g., identity matrix), and may be modified or updated upon processing each utterance or audio signal. Each modified transform is applied to feature vectors of a following or consecutive utterance.
- Components of the system 800 may be configured to work in an interconnected fashion with each other and/or with other components coupled to respective systems.
- One or more of the described functions, components, or blocks of the system 800 may be divided up into additional functional or physical components, or combined into fewer functional or physical components.
- additional functional and/or physical components may be added to the examples illustrated by FIG. 8 .
- the system 800 may include or be provided in the form of a processor (e.g., a microprocessor, a digital signal processor (DSP), etc.) configured to execute program code including one or more instructions for implementing logical functions described at the GMM fMLLR block 802 , the feature transform block 806 , and the DNN decoding block 810 .
- the system 800 may further include any type of computer readable medium (non-transitory medium) or memory, for example, such as a storage device including a disk or hard drive, to store the program code.
- the system 800 may be included within other systems.
- FIG. 9 is a flowchart of an example method 900 for online incremental adaptation of neural networks using auxiliary Gaussian mixture models in speech recognition, in accordance with an embodiment.
- the method 900 shown in FIG. 9 presents an example method that, for example, can be used with the system 800 , and may be performed by a device, a server, or a combination of the device and the server.
- the method 900 may include one or more operations, functions, or actions as illustrated by one or more of blocks 902 - 910 . Although the blocks are illustrated in a sequential order, these blocks may in some instances be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation
- each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process.
- the program code may be stored on any type of computer readable medium or memory, for example, such as a storage device including a disk or hard drive.
- the computer readable medium may include a non-transitory computer readable medium or memory, for example, such as computer-readable media that stores data for short periods of time like register memory, processor cache and Random Access Memory (RAM).
- the computer readable medium may also include non-transitory media or memory, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- the computer readable medium may be considered a computer readable storage medium, a tangible storage device, or other article of manufacture, for example.
- each block in FIG. 9 may represent circuitry that is wired to perform the specific logical functions in the process.
- the method 900 includes receiving, by a computing device, an audio signal at a first time and a subsequent audio signal at a second time different from the first time, wherein the audio signal and the subsequent audio signal include speech content.
- the computing device can be, for example, a mobile telephone, personal digital assistant (PDA), laptop, notebook, or netbook computer, tablet computing device, a wearable computing device, a server in a cloud-based computing system, etc.
- the computing device may be configured to receive, over time, input audio signals associated with utterances by a given speaker.
- the computing device may be a mobile phone, and the speaker may be a user of the phone.
- the user may utter, over time, a series of commands to the phone.
- the computing device may be configured to receive, over time, a series of audio signals corresponding to the series of utterances or commands, for example, and may be configured to recognize respective speech content of each audio signal.
- the computing device may be configured to determine a sequence of feature vectors for each audio signal.
- Each respective feature vector of the sequence may correspond to a respective temporal frame of a sequence of temporal frames of a respective audio signal.
- each feature vector may bear quantitative measures of acoustic properties of the corresponding temporal frame.
- Each temporal frame may contain a portion of the audio signal digitally sampled within a sliding time window, for example.
- Each feature vector may include quantitative measures of acoustic properties of the digitally sampled signal within the corresponding time frame.
- the feature vectors may include Mel Filter Cepstral (MFC) coefficients.
- MFC Mel Filter Cepstral
- Other possible types of quantitative measures of acoustic properties may include Perceptual Linear Predictive (PLP) coefficients, Relative Spectral (RASTA) coefficients, and Filterbank log-energy coefficients. These types of coefficients are examples for illustration only, and the feature vectors may include other types of quantitative measures, and may also include more than one type.
- FIG. 10 illustrates a system 1000 for speaker-adapted state-level score combination of deep neural network and Gaussian mixture model, in accordance with an embodiment.
- the system 1000 may for example be configured to implement the method 900 , for example.
- the computing device may be configured to extract the feature vectors o t corresponding to each audio signal.
- the method 900 includes applying a speaker-specific feature transform to the audio signal to obtain a transformed audio signal, where the speaker-specific feature transform may be determined based on one or more speaker-specific speech characteristics of a speaker-profile relating to the speech content.
- the computing device at block 1004 , may be configured to use a GMM fMLLR technique and speaker-specific adaptation data (e.g., speaker profile or speech characteristics as described in Section II) to determine a transform W.
- the speaker-specific feature transform may be initialized to an identify matrix.
- the computing device may be configured to modify or update the transform with speaker-specific characteristics as described at block 908 below.
- the computing device may be configured to apply the transform W to the feature vectors o t as shown by equation [10] to determine transformed feature vectors ô t .
- the method 900 includes processing, by the computing device, the transformed audio signal using a neural network trained to estimate a given speech content of the audio signal.
- the computing device may be configured to adapt the transformed feature vectors ô t to determine corresponding features that can be processed by a neural network.
- the computing device may be configured to adapt the transformed feature vectors ô t to match time step of the neural network. In this manner, the transformed feature vectors ô t may be adapted to correspond to the time sequence of feature vectors 301 in FIG. 3 , for example.
- the computing device may be configured to process the transformed vectors ô t (after adaptation at block 1008 ) through a neural network, such as Deep Neural Network (DNN) 1010 .
- DNN Deep Neural Network
- the DNN 1010 may correspond, for example, to the neural network 302 in FIG. 3 or the neural network 404 in FIG. 4 .
- the DNN 1010 may be trained to predict targets of context dependent (CD) HMM states. For each observation vector of and CD-HMM state s j , the posterior probability P(o t ′
- s j ) P ⁇ ( s j
- s j is the j-th HMM state
- observation vectors of are acoustic feature vectors augmented with neighbor frames.
- P(s j ) is the prior probability of state s j , which can be estimated from the frequency of the state in training alignments.
- P(o t ′) is a constant that is independent of state s j , and can thus be ignored in a likelihood computation. In other examples, the likelihood computation is not ignored.
- the computing device may be configured to process the transformed feature vectors ô t , in parallel, using a GMM. As shown in FIG. 10 , at block 1012 , the computing device may be configured to adapt the transformed feature vectors ô t to determine corresponding features that can be processed by the GMM. For instance, the computing device may be configured to adapt the transformed feature vectors ô t to match time step of the GMM. In this manner, the transformed feature vectors ô t may be adapted to correspond to the time sequence of feature vectors 501 in FIG. 5 , for example.
- the computing device may be configured to process the transformed vectors ô t (after adaptation at block 1012 ) through GMM 1014 .
- the GMM 1014 may correspond, for example, to the GMM 502 in FIG. 5 or the GMM 604 in FIG. 6 .
- state emission likelihoods may be determined as: P gmm ( o t
- w jm is a mixture weight of m-th Gaussian component in state s j
- ⁇ jm is a mean vector
- ⁇ jm is covariance
- N(.; ⁇ , ⁇ ) denotes a Gaussian density with mean ⁇ and covariance ⁇ .
- both the DNN and GMM models may share the same transformed feature vectors ô t .
- the DNN and GMM may be trained from the same type of source features.
- both models can be trained using perceptual linear predictive (PLP) features.
- Phone classification error patterns for DNN 1010 and GMM 1014 may be different, and combining outputs of DNN 1010 and GMM 1014 may achieve improved classification and ASR performance.
- the computing device may be configured to combine the outputs at the state level for every frame.
- the computing device may be configured to determine DNN scores at block 1016 A and GMM score at block 1016 B in FIG.
- a final acoustic score for frame t and state s j as a linear combination of DNN and GMM acoustic scores: log ⁇ circumflex over ( p ) ⁇ ( o t ,o t ′
- s j ) ⁇ j log p dnn ( o t ′
- the parameter ⁇ j is a state-dependent weight of DNN log likelihood score may be given a value between 0 and 1.
- the state-dependent weights may be learned by minimizing a discriminative criterion such as phone or state classification error rate.
- the weight ⁇ j can be optimized by grid search on a development set.
- the method 900 includes modifying the speaker-specific feature transform based on an output of the neural network to obtain a modified speaker-specific feature transform, and at block 910 the method 900 includes applying the modified speaker-specific feature transform to a subsequent audio signal of the audio signals to obtain a respective transformed audio signal to be processed by the neural network to estimate a respective speech content of the subsequent audio signal.
- the computing device may be configured to use alignments resulting from the decoder 1018 to accumulate sufficient statistics specific to a speaker of the input audio signal. The alignments contain information related to probability of each Gaussian in each frame. Therefore, statistics can be accumulated for all Gaussians. Then, the speaker-specific feature transform is re-estimated (or modified) at block 1004 based on the accumulated statistics and the feature vectors obtained from block 1002 to maximize the likelihood.
- the computing device may be configured to receive, over time, three consecutive audio signals, each audio signal having respective speech content from a given speaker.
- the computing device may be configured to extract, at block 1002 , feature vectors from the first audio signal and apply, at block 1006 , a transform W to the feature vectors.
- the transform W may be initialized to an identity matrix, for example, or may initially be determined using speaker-specific characteristics to which the computing device may have access.
- the transformed feature vectors may be adapted to be processed by both the DNN 1010 and the GMM 1014 .
- the outputs of the DNN 1010 and the GMM 1014 are merged at block 1014 and decoded by the decoder 1018 to determine hypotheses or estimated respective speech content of the first audio signal.
- the hypotheses, alignments, or the estimated respective speech content are feedback to the GMM fMLLR block 1004 to modify the transform W.
- the modified transform W may also be modified (or re-estimated) based on feature vectors extracted from the subsequent audio signal, i.e., the second audio signal.
- the modified transform W is thus adapted to characteristics of the speaker by integrating parameters of the estimated speech of the speaker.
- the modified transform W may then be applied to respective feature vectors extracted from the second audio signal.
- the transform W is modified again using respective outputs of the decoding process.
- the transform W modified by outputs associated with the first audio signal and outputs associated with the second audio signal is applied to respective feature vectors of the third audio signal. This process is continued such that the transform W is modified sequentially or incrementally upon processing each audio signal.
- the computing device may be configured to continuously update the speaker-specific characteristics based on outputs of the decoding process, for example.
- the system 1000 may be configured to run in real-time.
- the system 1000 may be implemented using multi-core central processing units, and the DNN 1010 and GMM 1014 may be implemented on separate threads to improve computational efficiency.
- Components of the system 1000 may be configured to work in an interconnected fashion with each other and/or with other components coupled to respective systems.
- One or more of the described functions or components of the system 1000 may be divided up into additional functional or physical components, or combined into fewer functional or physical components.
- additional functional and/or physical components may be added to the examples illustrated by FIG. 10 .
- the system 1000 may include or be provided in the form of a processor (e.g., a microprocessor, a digital signal processor (DSP), etc.) configured to execute program code including one or more instructions for implementing logical functions described at the various blocks of the system 1000 .
- the system 1000 may further include any type of computer readable medium (non-transitory medium) or memory, for example, such as a storage device including a disk or hard drive, to store the program code.
- the system 1000 may be included within other systems.
- FIG. 11 illustrates an example distributed computing architecture, in accordance with an example embodiment.
- FIG. 11 shows server devices 1102 and 1104 configured to communicate, via network 1106 , with programmable devices 1108 a , 1108 b , and 1108 c .
- the network 1106 may correspond to a LAN, a wide area network (WAN), a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices.
- the network 1106 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet.
- FIG. 11 shows three programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices.
- the programmable devices 1108 a , 1108 b , and 1108 c may be any sort of computing device, such as an ordinary laptop computer, desktop computer, network terminal, wireless communication device (e.g., a tablet, a cell phone or smart phone, a wearable computing device, etc.), and so on.
- the programmable devices 1108 a , 1108 b , and 1108 c may be dedicated to the design and use of software applications.
- the programmable devices 1108 a , 1108 b , and 1108 c may be general purpose computers that are configured to perform a number of tasks and may not be dedicated to software development tools.
- the server devices 1102 and 1104 can be configured to perform one or more services, as requested by programmable devices 1108 a , 1108 b , and/or 1108 c .
- server device 1102 and/or 1104 can provide content to the programmable devices 1108 a - 1108 c .
- the content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video.
- the content can include compressed and/or uncompressed content.
- the content can be encrypted and/or unencrypted. Other types of content are possible as well.
- the server device 1102 and/or 1104 can provide the programmable devices 1108 a - 1108 c with access to software for database, search, computation, graphical, audio (e.g. speech recognition), video, World Wide Web/Internet utilization, and/or other functions.
- server devices Many other examples are possible as well.
- the server devices 1102 and/or 1104 can be cloud-based devices that store program logic and/or data of cloud-based applications and/or services.
- the server devices 1102 and/or 1104 can be a single computing device residing in a single computing center.
- the server device 1102 and/or 1104 can include multiple computing devices in a single computing center, or multiple computing devices located in multiple computing centers in diverse geographic locations.
- FIG. 11 depicts each of the server devices 1102 and 1104 residing in different physical locations.
- data and services at the server devices 1102 and/or 1104 can be encoded as computer readable information stored in non-transitory, tangible computer readable media (or computer readable storage media) and accessible by programmable devices 1108 a , 1108 b , and 1108 c , and/or other computing devices.
- data at the server device 1102 and/or 1104 can be stored on a single disk drive or other tangible storage media, or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations.
- FIG. 12A is a block diagram of a computing device (e.g., system) in accordance with an example embodiment.
- computing device 1200 shown in FIG. 12A can be configured to perform one or more functions of the server devices 1202 , 1204 , network 1206 , and/or one or more of the programmable devices 1208 a , 1208 b , and 1208 c .
- the computing device 1200 may include a user-interface module 1202 , a network communications interface module 1204 , one or more processors 1206 , and data storage 1208 , all of which may be linked together via a system bus, network, or other connection mechanism 1210 .
- the user-interface module 1202 can be operable to send data to and/or receive data from external user input/output devices.
- user-interface module 1202 can be configured to send and/or receive data to and/or from user input devices such as a keyboard, a keypad, a touchscreen, a computer mouse, a track ball, a joystick, a camera, a voice recognition/synthesis module, and/or other similar devices.
- the user-interface module 1202 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCD), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later developed.
- the user-interface module 1202 can also be configured to generate recognized speech or audible output(s), and may include a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices.
- the network communications interface module 1204 can include one or more wireless interfaces 1212 and/or one or more wireline interfaces 1214 that are configurable to communicate via a network, such as network 1106 shown in FIG. 11 .
- the wireless interfaces 1212 can include one or more wireless transmitters, receivers, and/or transceivers, such as a Bluetooth transceiver, a Zigbee transceiver, a Wi-Fi transceiver, a LTE transceiver, and/or other similar type of wireless transceiver configurable to communicate via a wireless network.
- the wireline interfaces 1214 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- wireline transmitters such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- USB Universal Serial Bus
- the network communications interface module 1204 can be configured to provide reliable, secured, and/or authenticated communications. For each communication described herein, information for ensuring reliable communications (i.e., guaranteed message delivery) can be provided, perhaps as part of a message header and/or footer (e.g., packet/message sequencing information, encapsulation header(s) and/or footer(s), size/time information, and transmission verification information such as CRC and/or parity check values). Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, DES, AES, RSA, Diffie-Hellman, and/or DSA. Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications.
- cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications.
- the processors 1206 can include one or more general purpose processors and/or one or more special purpose processors (e.g., digital signal processors, application specific integrated circuits, etc.).
- the processors 1206 can be configured to execute computer-readable program instructions 1215 that are contained in the data storage 1208 and/or other instructions as described herein (e.g., the method 900 ).
- the data storage 1208 can include one or more computer-readable storage media that can be read and/or accessed by at least one of processors 1206 .
- the one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one of the processors 1206 .
- the data storage 1208 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other examples, the data storage 1208 can be implemented using two or more physical devices.
- the data storage 1208 can include computer-readable program instructions 1215 and perhaps additional data, such as but not limited to data used by one or more processes and/or threads of a software application.
- data storage 1208 can additionally include storage required to perform at least part of the herein-described methods (e.g., the method 900 ) and techniques and/or at least part of the functionality of the herein-described devices and networks.
- FIG. 12B depicts a cloud-based server system, in accordance with an example embodiment.
- functions of the server device 1102 and/or 1104 can be distributed among three computing clusters 1216 a , 1216 b , and 1216 c .
- the computing cluster 1216 a can include one or more computing devices 1218 a , cluster storage arrays 1220 a , and cluster routers 1222 a connected by a local cluster network 1224 a .
- the computing cluster 1216 b can include one or more computing devices 1218 b , cluster storage arrays 1220 b , and cluster routers 1222 b connected by a local cluster network 1224 b .
- computing cluster 1216 c can include one or more computing devices 1218 c , cluster storage arrays 1220 c , and cluster routers 1222 c connected by a local cluster network 1224 c.
- each of the computing clusters 1216 a , 1216 b , and 1216 c can have an equal number of computing devices, an equal number of cluster storage arrays, and an equal number of cluster routers. In other examples, however, each computing cluster can have different numbers of computing devices, different numbers of cluster storage arrays, and different numbers of cluster routers. The number of computing devices, cluster storage arrays, and cluster routers in each computing cluster can depend on the computing task or tasks assigned to each computing cluster.
- the computing devices 1218 a can be configured to perform various computing tasks of the server device 1102 .
- the various functionalities of the server device 1102 can be distributed among one or more of computing devices 1218 a , 1218 b , and 1218 c .
- the computing devices 1218 b and 1218 c in the computing clusters 1216 b and 1216 c can be configured similarly to the computing devices 1218 a in computing cluster 1216 a .
- the computing devices 1218 a , 1218 b , and 1218 c can be configured to perform different functions.
- computing tasks and stored data associated with server devices 1102 and/or 1104 can be distributed across computing devices 1218 a , 1218 b , and 1218 c based at least in part on the processing requirements of the server devices 1102 and/or 1104 , the processing capabilities of computing devices 1218 a , 1218 b , and 1218 c , the latency of the network links between the computing devices in each computing cluster and between the computing clusters themselves, and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency, and/or other design goals of the overall system architecture.
- the cluster storage arrays 1220 a , 1220 b , and 1220 c of the computing clusters 1216 a , 1216 b , and 1216 c can be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives.
- the disk array controllers alone or in conjunction with their respective computing devices, can also be configured to manage backup or redundant copies of the data stored in the cluster storage arrays to protect against disk drive or other cluster storage array failures and/or network failures that prevent one or more computing devices from accessing one or more cluster storage arrays.
- cluster storage arrays 1220 a , 1220 b , and 1220 c can be configured to store the data of the server device 1102
- other cluster storage arrays can store data of the server device 1104
- some cluster storage arrays can be configured to store backup versions of data stored in other cluster storage arrays.
- the cluster routers 1222 a , 1222 b , and 1222 c in computing clusters 1216 a , 1216 b , and 1216 c can include networking equipment configured to provide internal and external communications for the computing clusters.
- the cluster routers 1222 a in computing cluster 1216 a can include one or more internet switching and routing devices configured to provide (i) local area network communications between the computing devices 1218 a and the cluster storage arrays 1220 a via the local cluster network 1224 a , and (ii) wide area network communications between the computing cluster 1216 a and the computing clusters 1216 b and 1216 c via the wide area network connection 1226 a to network 1206 .
- the cluster routers 1222 b and 1222 c can include network equipment similar to the cluster routers 1222 a , and the cluster routers 1222 b and 1222 c can perform similar networking functions for the computing clusters 1216 b and 1216 c that the cluster routers 1222 a perform for the computing cluster 1216 a.
- the configuration of the cluster routers 1222 a , 1222 b , and 1222 c can be based at least in part on the data communication requirements of the computing devices and cluster storage arrays, the data communications capabilities of the network equipment in the cluster routers 1222 a , 1222 b , and 1222 c , the latency and throughput of the local networks 1224 a , 1224 b , 1224 c , the latency, throughput, and cost of wide area network links 1226 a , 1226 b , and 1226 c , and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency and/or other design goals of the moderation system architecture.
- the configurations illustrated in FIGS. 11 and 12A-12B can be used for implementations described with respect to the method 900 .
- the computing device implementing the method 900 can be a cloud-based device (e.g., server devices 1102 and/or 1104 ), or one of the programmable devices 1108 a - c in FIG. 11 , or the computing devices 1218 a - c of FIG. 12B .
- a computing device such as one of the programmable devices 1108 a - c in FIG. 11 , may be configured to sequentially receive the audio signals, and provide the audio signals or information associated with the audio signals to server devices 1102 and/or 1104 .
- the server devices 1102 and/ 1104 may be configured to process the audio signals incrementally as described in FIGS. 8, 9, and 10 .
- the server devices 1102 and/ 1104 may provide the estimate speech content back to the computing device (one of the programmable devices 1108 a - c in FIG. 11 ), for example.
Abstract
Description
TABLE 1 | |||
Word | Phonemic Interpretation | ||
cat | /k/ /ae/ /t/ | ||
and | /ay/ /n/ /d/ | ||
dog | /d/ /aw/ /g/ | ||
P(w n |w 1 ,w 2 , . . . ,w n-1)
TABLE 2 |
Tri-gram Conditional Probabilities |
P(dog|cat,and) = 0.50 | ||
P(mouse|cat,and) = 0.35 | ||
P(bird|cat,and) = 0.14 | ||
P(fiddle|cat,and) = 0.01 | ||
where p(qk) gives the prior probabilities for the qk states, and p(xj) gives the probabilities for the acoustic features. Before run time, the ASR system may also be trained to generate expected output (e.g., text strings) from known input speech (e.g., utterances), from which relative frequencies of the qk, k=1, . . . , K states, and correspondingly the prior probabilities p(qk) for the qk states may be determined. In addition, the probabilities p(xj) are the same for all states at run time, and so may be treated as a scaling constant in the expression for Bayes rule. Therefore, the a priori emission probabilities p(xj|qk) for the qk, k=1, . . . , K states follow from Bayes rule (equation [1] above) applied at run time for the HMM states.
p(x|λ)=Σi=1 M w i g(x|μ i,Σi), [2]
with mean vector μi and covariance matrix Σi.
p(x|w i,μi,Σi)=Σi=1 M w i g(x|μ i,Σi),i=1 . . . M [4]
where x is an n-dimensional feature vector, wi are weights such that Σi=1 Mwi=1, and g(x|μi,Σi) is an n-dimensional Gaussian function with a mean of μi and a covariance matrix of Σi. The speech model for a given profile may be represented as
λ={w i,μi,Σi },i=1 . . . M [5]
{circumflex over (S)}=argmax1≦k≦S p(λk |X)=argmax1≦k≦SΣt=1 T log p(x t,λk) [6]
where X is a series of T feature vectors, xt, 1≦t≦T. The final equation can be derived from argmax1≦k≦S p(λk|X) through application of Bayes Rule and some simplifying assumptions. Note that p(xt|λk)=p(xt|wk,μk,Σk), and thus the solution to this term is provided by equations [4] and [5].
where,
ô t =Ao t +b [10]
Where A is n×n rotation/scaling matrix, b is n×1 bias term. The transform parameters W=[A b] can be estimated by optimizing an auxiliary Q-function and can be solved iteratively, for example.
where sj is the j-th HMM state, and observation vectors of are acoustic feature vectors augmented with neighbor frames. P(sj) is the prior probability of state sj, which can be estimated from the frequency of the state in training alignments. In one example, in a likelihood computation, P(ot′) is a constant that is independent of state sj, and can thus be ignored in a likelihood computation. In other examples, the likelihood computation is not ignored.
P gmm(o t |s j)=Σm=1 M w jm N(o t;μjm,Σjm) [12]
Where wjm is a mixture weight of m-th Gaussian component in state sj, μjm is a mean vector, Σjm is covariance, and N(.; μ, Σ) denotes a Gaussian density with mean μ and covariance Σ.
log {circumflex over (p)}(o t ,o t ′|s j)=αj log p dnn(o t ′|s j)+(1−αj)log p gmm(o t |s j) [13]
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/886,620 US9466292B1 (en) | 2013-05-03 | 2013-05-03 | Online incremental adaptation of deep neural networks using auxiliary Gaussian mixture models in speech recognition |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/886,620 US9466292B1 (en) | 2013-05-03 | 2013-05-03 | Online incremental adaptation of deep neural networks using auxiliary Gaussian mixture models in speech recognition |
Publications (1)
Publication Number | Publication Date |
---|---|
US9466292B1 true US9466292B1 (en) | 2016-10-11 |
Family
ID=57046362
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/886,620 Active 2034-10-03 US9466292B1 (en) | 2013-05-03 | 2013-05-03 | Online incremental adaptation of deep neural networks using auxiliary Gaussian mixture models in speech recognition |
Country Status (1)
Country | Link |
---|---|
US (1) | US9466292B1 (en) |
Cited By (27)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170110115A1 (en) * | 2015-10-16 | 2017-04-20 | Samsung Electronics Co., Ltd. | Apparatus and method for normalizing input data of acoustic model and speech recognition apparatus |
CN106782520A (en) * | 2017-03-14 | 2017-05-31 | 华中师范大学 | Phonetic feature mapping method under a kind of complex environment |
US20170287465A1 (en) * | 2016-03-31 | 2017-10-05 | Microsoft Technology Licensing, Llc | Speech Recognition and Text-to-Speech Learning System |
US10008197B2 (en) * | 2015-11-24 | 2018-06-26 | Fujitsu Limited | Keyword detector and keyword detection method |
CN109256144A (en) * | 2018-11-20 | 2019-01-22 | 中国科学技术大学 | Sound enhancement method based on integrated study and noise perception training |
CN109299270A (en) * | 2018-10-30 | 2019-02-01 | 云南电网有限责任公司信息中心 | A kind of text data unsupervised clustering based on convolutional neural networks |
US10235994B2 (en) * | 2016-03-04 | 2019-03-19 | Microsoft Technology Licensing, Llc | Modular deep learning model |
CN110277088A (en) * | 2019-05-29 | 2019-09-24 | 平安科技（深圳）有限公司 | Intelligent voice recognition method, device and computer readable storage medium |
US20190311711A1 (en) * | 2018-04-10 | 2019-10-10 | Futurewei Technologies, Inc. | Method and device for processing whispered speech |
US10553218B2 (en) * | 2016-09-19 | 2020-02-04 | Pindrop Security, Inc. | Dimensionality reduction of baum-welch statistics for speaker recognition |
US20200082817A1 (en) * | 2018-09-10 | 2020-03-12 | Ford Global Technologies, Llc | Vehicle language processing |
CN111354344A (en) * | 2020-03-09 | 2020-06-30 | 第四范式（北京）技术有限公司 | Training method and device of voice recognition model, electronic equipment and storage medium |
US10706856B1 (en) * | 2016-09-12 | 2020-07-07 | Oben, Inc. | Speaker recognition using deep learning neural network |
US10854205B2 (en) | 2016-09-19 | 2020-12-01 | Pindrop Security, Inc. | Channel-compensated low-level features for speaker recognition |
US10855455B2 (en) * | 2019-01-11 | 2020-12-01 | Advanced New Technologies Co., Ltd. | Distributed multi-party security model training framework for privacy protection |
US10923110B2 (en) | 2017-08-25 | 2021-02-16 | International Business Machines Corporation | Priors adaptation for conservative training of acoustic model |
US10930271B2 (en) * | 2013-07-31 | 2021-02-23 | Google Llc | Speech recognition using neural networks |
US10937438B2 (en) | 2018-03-29 | 2021-03-02 | Ford Global Technologies, Llc | Neural network generative modeling to transform speech utterances and augment training data |
US11019201B2 (en) | 2019-02-06 | 2021-05-25 | Pindrop Security, Inc. | Systems and methods of gateway detection in a telephone network |
US11100932B2 (en) * | 2017-02-10 | 2021-08-24 | Synaptics Incorporated | Robust start-end point detection algorithm using neural network |
US11302303B2 (en) * | 2018-12-18 | 2022-04-12 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and device for training an acoustic model |
US11646018B2 (en) | 2019-03-25 | 2023-05-09 | Pindrop Security, Inc. | Detection of calls from voice assistants |
US11670304B2 (en) | 2016-09-19 | 2023-06-06 | Pindrop Security, Inc. | Speaker recognition in the call center |
US11842748B2 (en) | 2016-06-28 | 2023-12-12 | Pindrop Security, Inc. | System and method for cluster-based audio event detection |
US11853884B2 (en) | 2017-02-10 | 2023-12-26 | Synaptics Incorporated | Many or one detection classification systems and methods |
US11961524B2 (en) | 2021-05-27 | 2024-04-16 | Honeywell International Inc. | System and method for extracting and displaying speaker information in an ATC transcription |
US11977974B2 (en) | 2017-11-30 | 2024-05-07 | International Business Machines Corporation | Compression of fully connected / recurrent layers of deep network(s) through enforcing spatial locality to weight matrices and effecting frequency compression |
Citations (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020087325A1 (en) * | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Dialogue application computer platform |
US20020156626A1 (en) * | 2001-04-20 | 2002-10-24 | Hutchison William R. | Speech recognition system |
US20030009333A1 (en) * | 1996-11-22 | 2003-01-09 | T-Netix, Inc. | Voice print system and method |
US20060235696A1 (en) * | 1999-11-12 | 2006-10-19 | Bennett Ian M | Network based interactive speech recognition system |
US20080077404A1 (en) * | 2006-09-21 | 2008-03-27 | Kabushiki Kaisha Toshiba | Speech recognition device, speech recognition method, and computer program product |
US20080255835A1 (en) * | 2007-04-10 | 2008-10-16 | Microsoft Corporation | User directed adaptation of spoken language grammer |
US20080312926A1 (en) * | 2005-05-24 | 2008-12-18 | Claudio Vair | Automatic Text-Independent, Language-Independent Speaker Voice-Print Creation and Speaker Recognition |
US20080316888A1 (en) * | 2007-06-25 | 2008-12-25 | Eli Reifman | Device Method and System for Communication Session Storage |
US20090216528A1 (en) * | 2005-06-01 | 2009-08-27 | Roberto Gemello | Method of adapting a neural network of an automatic speech recognition device |
US20090287489A1 (en) * | 2008-05-15 | 2009-11-19 | Palm, Inc. | Speech processing for plurality of users |
US20100057453A1 (en) * | 2006-11-16 | 2010-03-04 | International Business Machines Corporation | Voice activity detection system and method |
US7769588B2 (en) * | 2002-07-25 | 2010-08-03 | Sony Deutschland Gmbh | Spoken man-machine interface with speaker identification |
US20100217657A1 (en) * | 1999-06-10 | 2010-08-26 | Gazdzinski Robert F | Adaptive information presentation apparatus and methods |
US20110238407A1 (en) * | 2009-08-31 | 2011-09-29 | O3 Technologies, Llc | Systems and methods for speech-to-speech translation |
US20120010887A1 (en) * | 2010-07-08 | 2012-01-12 | Honeywell International Inc. | Speech recognition and voice training data storage and access methods and apparatus |
US20120116772A1 (en) * | 2010-11-10 | 2012-05-10 | AventuSoft, LLC | Method and System for Providing Speech Therapy Outside of Clinic |
US20130080165A1 (en) * | 2011-09-24 | 2013-03-28 | Microsoft Corporation | Model Based Online Normalization of Feature Distribution for Noise Robust Speech Recognition |
US20130166279A1 (en) * | 2010-08-24 | 2013-06-27 | Veovox Sa | System and method for recognizing a user voice command in noisy environment |
US8515750B1 (en) * | 2012-06-05 | 2013-08-20 | Google Inc. | Realtime acoustic adaptation using stability measures |
US20130238337A1 (en) * | 2011-07-14 | 2013-09-12 | Panasonic Corporation | Voice quality conversion system, voice quality conversion device, voice quality conversion method, vocal tract information generation device, and vocal tract information generation method |
US8554559B1 (en) * | 2012-07-13 | 2013-10-08 | Google Inc. | Localized speech recognition with offload |
US8571859B1 (en) * | 2012-05-31 | 2013-10-29 | Google Inc. | Multi-stage speaker adaptation |
US20140114655A1 (en) * | 2012-10-19 | 2014-04-24 | Sony Computer Entertainment Inc. | Emotion recognition using auditory attention cues extracted from users voice |
US20140195232A1 (en) * | 2013-01-04 | 2014-07-10 | Stmicroelectronics Asia Pacific Pte Ltd. | Methods, systems, and circuits for text independent speaker recognition with automatic learning features |
US20140257803A1 (en) * | 2013-03-06 | 2014-09-11 | Microsoft Corporation | Conservatively adapting a deep neural network in a recognition system |
US20140282956A1 (en) * | 2013-03-12 | 2014-09-18 | Sony Corporation | System and method for user authentication |
US8983844B1 (en) * | 2012-07-31 | 2015-03-17 | Amazon Technologies, Inc. | Transmission of noise parameters for improving automatic speech recognition |
US20150149174A1 (en) * | 2012-05-08 | 2015-05-28 | Nuance Communications, Inc. | Differential acoustic model representation and linear transform-based adaptation for efficient user profile update techniques in automatic speech recognition |
-
2013
- 2013-05-03 US US13/886,620 patent/US9466292B1/en active Active
Patent Citations (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030009333A1 (en) * | 1996-11-22 | 2003-01-09 | T-Netix, Inc. | Voice print system and method |
US20100217657A1 (en) * | 1999-06-10 | 2010-08-26 | Gazdzinski Robert F | Adaptive information presentation apparatus and methods |
US20060235696A1 (en) * | 1999-11-12 | 2006-10-19 | Bennett Ian M | Network based interactive speech recognition system |
US20020087325A1 (en) * | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Dialogue application computer platform |
US20020156626A1 (en) * | 2001-04-20 | 2002-10-24 | Hutchison William R. | Speech recognition system |
US7769588B2 (en) * | 2002-07-25 | 2010-08-03 | Sony Deutschland Gmbh | Spoken man-machine interface with speaker identification |
US20080312926A1 (en) * | 2005-05-24 | 2008-12-18 | Claudio Vair | Automatic Text-Independent, Language-Independent Speaker Voice-Print Creation and Speaker Recognition |
US20090216528A1 (en) * | 2005-06-01 | 2009-08-27 | Roberto Gemello | Method of adapting a neural network of an automatic speech recognition device |
US20080077404A1 (en) * | 2006-09-21 | 2008-03-27 | Kabushiki Kaisha Toshiba | Speech recognition device, speech recognition method, and computer program product |
US20100057453A1 (en) * | 2006-11-16 | 2010-03-04 | International Business Machines Corporation | Voice activity detection system and method |
US20080255835A1 (en) * | 2007-04-10 | 2008-10-16 | Microsoft Corporation | User directed adaptation of spoken language grammer |
US20080316888A1 (en) * | 2007-06-25 | 2008-12-25 | Eli Reifman | Device Method and System for Communication Session Storage |
US20090287489A1 (en) * | 2008-05-15 | 2009-11-19 | Palm, Inc. | Speech processing for plurality of users |
US20110238407A1 (en) * | 2009-08-31 | 2011-09-29 | O3 Technologies, Llc | Systems and methods for speech-to-speech translation |
US20120010887A1 (en) * | 2010-07-08 | 2012-01-12 | Honeywell International Inc. | Speech recognition and voice training data storage and access methods and apparatus |
US20130166279A1 (en) * | 2010-08-24 | 2013-06-27 | Veovox Sa | System and method for recognizing a user voice command in noisy environment |
US20120116772A1 (en) * | 2010-11-10 | 2012-05-10 | AventuSoft, LLC | Method and System for Providing Speech Therapy Outside of Clinic |
US20130238337A1 (en) * | 2011-07-14 | 2013-09-12 | Panasonic Corporation | Voice quality conversion system, voice quality conversion device, voice quality conversion method, vocal tract information generation device, and vocal tract information generation method |
US20130080165A1 (en) * | 2011-09-24 | 2013-03-28 | Microsoft Corporation | Model Based Online Normalization of Feature Distribution for Noise Robust Speech Recognition |
US20150149174A1 (en) * | 2012-05-08 | 2015-05-28 | Nuance Communications, Inc. | Differential acoustic model representation and linear transform-based adaptation for efficient user profile update techniques in automatic speech recognition |
US8571859B1 (en) * | 2012-05-31 | 2013-10-29 | Google Inc. | Multi-stage speaker adaptation |
US8515750B1 (en) * | 2012-06-05 | 2013-08-20 | Google Inc. | Realtime acoustic adaptation using stability measures |
US8554559B1 (en) * | 2012-07-13 | 2013-10-08 | Google Inc. | Localized speech recognition with offload |
US8983844B1 (en) * | 2012-07-31 | 2015-03-17 | Amazon Technologies, Inc. | Transmission of noise parameters for improving automatic speech recognition |
US20140114655A1 (en) * | 2012-10-19 | 2014-04-24 | Sony Computer Entertainment Inc. | Emotion recognition using auditory attention cues extracted from users voice |
US20140195232A1 (en) * | 2013-01-04 | 2014-07-10 | Stmicroelectronics Asia Pacific Pte Ltd. | Methods, systems, and circuits for text independent speaker recognition with automatic learning features |
US20140257803A1 (en) * | 2013-03-06 | 2014-09-11 | Microsoft Corporation | Conservatively adapting a deep neural network in a recognition system |
US20140282956A1 (en) * | 2013-03-12 | 2014-09-18 | Sony Corporation | System and method for user authentication |
Non-Patent Citations (2)
Title |
---|
Frank Seide et al., Feature Engineering in Context-Dependent Deep Neural Networks for Conversational Speech Transcription, 978-1-4673-03657-5/11, 2011 IEEE (ASRU 2011). |
M.J.F. Gales, Maximum Likelihood Linear Transformations for HMM-Based Speech Recognition, CUES/F-INFENG/TR 291, May 1997. |
Cited By (39)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11620991B2 (en) | 2013-07-31 | 2023-04-04 | Google Llc | Speech recognition using neural networks |
US10930271B2 (en) * | 2013-07-31 | 2021-02-23 | Google Llc | Speech recognition using neural networks |
US20170110115A1 (en) * | 2015-10-16 | 2017-04-20 | Samsung Electronics Co., Ltd. | Apparatus and method for normalizing input data of acoustic model and speech recognition apparatus |
US9972305B2 (en) * | 2015-10-16 | 2018-05-15 | Samsung Electronics Co., Ltd. | Apparatus and method for normalizing input data of acoustic model and speech recognition apparatus |
US10008197B2 (en) * | 2015-11-24 | 2018-06-26 | Fujitsu Limited | Keyword detector and keyword detection method |
US10235994B2 (en) * | 2016-03-04 | 2019-03-19 | Microsoft Technology Licensing, Llc | Modular deep learning model |
US20170287465A1 (en) * | 2016-03-31 | 2017-10-05 | Microsoft Technology Licensing, Llc | Speech Recognition and Text-to-Speech Learning System |
US10089974B2 (en) * | 2016-03-31 | 2018-10-02 | Microsoft Technology Licensing, Llc | Speech recognition and text-to-speech learning system |
US11842748B2 (en) | 2016-06-28 | 2023-12-12 | Pindrop Security, Inc. | System and method for cluster-based audio event detection |
US10706856B1 (en) * | 2016-09-12 | 2020-07-07 | Oben, Inc. | Speaker recognition using deep learning neural network |
US10854205B2 (en) | 2016-09-19 | 2020-12-01 | Pindrop Security, Inc. | Channel-compensated low-level features for speaker recognition |
US10553218B2 (en) * | 2016-09-19 | 2020-02-04 | Pindrop Security, Inc. | Dimensionality reduction of baum-welch statistics for speaker recognition |
US11670304B2 (en) | 2016-09-19 | 2023-06-06 | Pindrop Security, Inc. | Speaker recognition in the call center |
US11657823B2 (en) | 2016-09-19 | 2023-05-23 | Pindrop Security, Inc. | Channel-compensated low-level features for speaker recognition |
US11853884B2 (en) | 2017-02-10 | 2023-12-26 | Synaptics Incorporated | Many or one detection classification systems and methods |
US11100932B2 (en) * | 2017-02-10 | 2021-08-24 | Synaptics Incorporated | Robust start-end point detection algorithm using neural network |
CN106782520A (en) * | 2017-03-14 | 2017-05-31 | 华中师范大学 | Phonetic feature mapping method under a kind of complex environment |
US10923110B2 (en) | 2017-08-25 | 2021-02-16 | International Business Machines Corporation | Priors adaptation for conservative training of acoustic model |
US10991363B2 (en) * | 2017-08-25 | 2021-04-27 | International Business Machines Corporation | Priors adaptation for conservative training of acoustic model |
US11977974B2 (en) | 2017-11-30 | 2024-05-07 | International Business Machines Corporation | Compression of fully connected / recurrent layers of deep network(s) through enforcing spatial locality to weight matrices and effecting frequency compression |
US10937438B2 (en) | 2018-03-29 | 2021-03-02 | Ford Global Technologies, Llc | Neural network generative modeling to transform speech utterances and augment training data |
US20190311711A1 (en) * | 2018-04-10 | 2019-10-10 | Futurewei Technologies, Inc. | Method and device for processing whispered speech |
US10832660B2 (en) * | 2018-04-10 | 2020-11-10 | Futurewei Technologies, Inc. | Method and device for processing whispered speech |
CN111902862A (en) * | 2018-04-10 | 2020-11-06 | 华为技术有限公司 | Ear voice processing method and device |
US10891949B2 (en) * | 2018-09-10 | 2021-01-12 | Ford Global Technologies, Llc | Vehicle language processing |
US20200082817A1 (en) * | 2018-09-10 | 2020-03-12 | Ford Global Technologies, Llc | Vehicle language processing |
CN109299270A (en) * | 2018-10-30 | 2019-02-01 | 云南电网有限责任公司信息中心 | A kind of text data unsupervised clustering based on convolutional neural networks |
CN109256144A (en) * | 2018-11-20 | 2019-01-22 | 中国科学技术大学 | Sound enhancement method based on integrated study and noise perception training |
CN109256144B (en) * | 2018-11-20 | 2022-09-06 | 中国科学技术大学 | Speech enhancement method based on ensemble learning and noise perception training |
US11302303B2 (en) * | 2018-12-18 | 2022-04-12 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and device for training an acoustic model |
US10855455B2 (en) * | 2019-01-11 | 2020-12-01 | Advanced New Technologies Co., Ltd. | Distributed multi-party security model training framework for privacy protection |
US11019201B2 (en) | 2019-02-06 | 2021-05-25 | Pindrop Security, Inc. | Systems and methods of gateway detection in a telephone network |
US11870932B2 (en) | 2019-02-06 | 2024-01-09 | Pindrop Security, Inc. | Systems and methods of gateway detection in a telephone network |
US11646018B2 (en) | 2019-03-25 | 2023-05-09 | Pindrop Security, Inc. | Detection of calls from voice assistants |
CN110277088B (en) * | 2019-05-29 | 2024-04-09 | 平安科技（深圳）有限公司 | Intelligent voice recognition method, intelligent voice recognition device and computer readable storage medium |
CN110277088A (en) * | 2019-05-29 | 2019-09-24 | 平安科技（深圳）有限公司 | Intelligent voice recognition method, device and computer readable storage medium |
CN111354344B (en) * | 2020-03-09 | 2023-08-22 | 第四范式（北京）技术有限公司 | Training method and device of voice recognition model, electronic equipment and storage medium |
CN111354344A (en) * | 2020-03-09 | 2020-06-30 | 第四范式（北京）技术有限公司 | Training method and device of voice recognition model, electronic equipment and storage medium |
US11961524B2 (en) | 2021-05-27 | 2024-04-16 | Honeywell International Inc. | System and method for extracting and displaying speaker information in an ATC transcription |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9466292B1 (en) | Online incremental adaptation of deep neural networks using auxiliary Gaussian mixture models in speech recognition | |
US11145293B2 (en) | Speech recognition with sequence-to-sequence models | |
US9240184B1 (en) | Frame-level combination of deep neural network and gaussian mixture models | |
US8484022B1 (en) | Adaptive auto-encoders | |
US8589164B1 (en) | Methods and systems for speech recognition processing using search query information | |
US8996366B2 (en) | Multi-stage speaker adaptation | |
US10629185B2 (en) | Statistical acoustic model adaptation method, acoustic model learning method suitable for statistical acoustic model adaptation, storage medium storing parameters for building deep neural network, and computer program for adapting statistical acoustic model | |
US8805684B1 (en) | Distributed speaker adaptation | |
US9620145B2 (en) | Context-dependent state tying using a neural network | |
US20200168208A1 (en) | Systems and methods for speech recognition in unseen and noisy channel conditions | |
US10714076B2 (en) | Initialization of CTC speech recognition with standard HMM | |
US20220319506A1 (en) | Method and system for performing domain adaptation of end-to-end automatic speech recognition model | |
US20230104228A1 (en) | Joint Unsupervised and Supervised Training for Multilingual ASR | |
Chen et al. | Automatic transcription of broadcast news | |
Becerra et al. | Speech recognition in a dialog system: From conventional to deep processing: A case study applied to Spanish | |
Park et al. | Korean grapheme unit-based speech recognition using attention-ctc ensemble network | |
Razavi et al. | An HMM-based formalism for automatic subword unit derivation and pronunciation generation | |
Russell et al. | A multiple-level linear/linear segmental HMM with a formant-based intermediate layer | |
Tabibian | A survey on structured discriminative spoken keyword spotting | |
Rasipuram | Probabilistic lexical modeling and grapheme-based automatic speech recognition | |
Hain | Hidden model sequence models for automatic speech recognition | |
Sun et al. | Integrated exemplar-based template matching and statistical modeling for continuous speech recognition | |
Savitha | Deep recurrent neural network based audio speech recognition system | |
Imseng et al. | Applying multi-and cross-lingual stochastic phone space transformations to non-native speech recognition | |
EP4068279B1 (en) | Method and system for performing domain adaptation of end-to-end automatic speech recognition model |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LEI, XIN;ALEKSIC, PETAR;SIGNING DATES FROM 20130501 TO 20130503;REEL/FRAME:030346/0158 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044097/0658Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |