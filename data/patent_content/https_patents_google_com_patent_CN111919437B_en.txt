CN111919437B - Stereoscopic knitting for head tracking autostereoscopic displays - Google Patents
Stereoscopic knitting for head tracking autostereoscopic displays Download PDFInfo
- Publication number
- CN111919437B CN111919437B CN201980021850.0A CN201980021850A CN111919437B CN 111919437 B CN111919437 B CN 111919437B CN 201980021850 A CN201980021850 A CN 201980021850A CN 111919437 B CN111919437 B CN 111919437B
- Authority
- CN
- China
- Prior art keywords
- mask
- image
- display device
- pixels
- user
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/366—Image reproducers using viewer tracking
- H04N13/383—Image reproducers using viewer tracking for tracking with gaze detection, i.e. detecting the lines of sight of the viewer's eyes
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/302—Image reproducers for viewing without the aid of special glasses, i.e. using autostereoscopic displays
- H04N13/305—Image reproducers for viewing without the aid of special glasses, i.e. using autostereoscopic displays using lenticular lenses, e.g. arrangements of cylindrical lenses
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/012—Head tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/013—Eye tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/04815—Interaction with a metaphor-based environment or interaction object displayed as three-dimensional, e.g. changing the user viewpoint with respect to the environment or object
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/30003—Arrangements for executing specific machine instructions
- G06F9/3004—Arrangements for executing specific machine instructions to perform operations on memory
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/122—Improving the 3D impression of stereoscopic images by modifying image signal contents, e.g. by filtering or adding monoscopic depth cues
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/302—Image reproducers for viewing without the aid of special glasses, i.e. using autostereoscopic displays
- H04N13/31—Image reproducers for viewing without the aid of special glasses, i.e. using autostereoscopic displays using parallax barriers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/366—Image reproducers using viewer tracking
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/302—Image reproducers for viewing without the aid of special glasses, i.e. using autostereoscopic displays
- H04N13/307—Image reproducers for viewing without the aid of special glasses, i.e. using autostereoscopic displays using fly-eye lenses, e.g. arrangements of circular lenses
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/366—Image reproducers using viewer tracking
- H04N13/376—Image reproducers using viewer tracking for tracking left-right translational head movements, i.e. lateral movements
Abstract
Systems and methods are described for performing the following steps: determining a tracked orientation associated with viewing an emission interface of a display device; generating a first mask representing a first set of values associated with a transmission interface of a display device using the tracked orientation; generating a second mask representing a second set of values associated with a transmission interface of the display device using the tracked orientation; and generating an output image using the first mask and the second mask.
Description
Cross Reference to Related Applications
This application claims priority to U.S. application No.16/249,551 filed on 16.1.2019, which claims priority to U.S. provisional application No.62/696,188 filed on 10.7.2018, the disclosure of which is incorporated herein by reference in its entirety.
Technical Field
This specification relates generally to methods, devices and algorithms that may be used in generating content for presentation on an autostereoscopic display.
Background
Experiencing traditional three-dimensional (3D) content may include accessing a Head Mounted Display (HMD) device to properly view and interact with such content. Specific optics may be calculated and manufactured for the HMD device in order to provide realistic 3D images for display. However, the use of HMD devices can be cumbersome for the user to wear continuously. Thus, a user may utilize an autostereoscopic display to access a user experience with 3D perception without using an HMD device (e.g., goggles or headwear). Autostereoscopic displays employ optical components to achieve a 3D effect for a variety of different images on the same plane and provide such images from multiple viewpoints to create the illusion of 3D space.
Disclosure of Invention
A system of one or more computers can be configured to perform particular operations or actions by virtue of installing software, firmware, hardware, or a combination thereof on the system that, in operation, causes or causes the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by a data processing apparatus, cause the apparatus to perform the actions.
In one general aspect, systems and methods are described for performing the following steps: determining a tracked orientation associated with viewing an emission interface of a display device; generating a first mask (mask) representing a first set of values associated with a transmission interface of a display device using the tracked position; generating a second mask representing a second set of values associated with a transmission interface of the display device using the tracked orientation; and generating an output image using the first mask and the second mask.
Generating the output image may include: obtaining a left image having a first set of pixels and a right image having a second set of pixels, assigning the first set of values to the first set of pixels in the left image, assigning the second set of values to the second set of pixels in the right image, and interleaving the left image with the right image according to the assigned first set of values and the assigned second set of values.
Systems and methods may include and/or utilize a tracking module, a display panel coupled to a lenticular array, and at least one processing device having access to a memory storing instructions executable by the system.
Implementations may include one or more of the following features. Implementations may include providing an output image to a display device. The output image may be configured to provide a left image to a left eye of a user viewing the emissive interface of the display device while simultaneously providing a right image to a right eye of the user viewing the emissive interface of the display device.
In some implementations, the first mask and the second mask are derived from a terrain four-dimensional surface, and the first mask and the second mask are generated by applying a non-linear mapping to a fractional portion of the surface. In some implementations, the first mask represents a first subset of pixels for a detected head orientation (tracked orientation) of a user viewing the display device, and the second mask represents a second subset of pixels for the detected head orientation of the user. In some implementations, the first mask and the second mask are used to obtain a value for each pixel associated with the display device for a plurality of changeable head orientations of the user.
In some implementations, the first mask and the second mask are color images having red, green, and blue color components. Each color component may be used to determine a corresponding color component of the output image. In some implementations, the tracked orientation is a head and/or eye orientation of the user, and wherein the first mask and the second mask are updated based on detected movement of the head and/or eye orientation. In some implementations, the first mask and the second mask include representations of at least one color index for each pixel of the display device. In some implementations, the tracked orientation varies for each row of the display device.
In some implementations, the tracked position is a head and/or eye position of the user, and the first mask and the second mask are updated based on detected movement of the head and/or eye position. In some implementations, the tracked orientation varies for each row of the display device. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
In another general aspect, a computer-implemented method is described. The method may include using at least one processing device and a memory, the memory storing instructions that, when executed, cause the processing device to perform operations comprising: determining a tracked orientation associated with viewing an emission interface of a display device; generating a first mask representing a first set of values associated with a transmission interface of a display device using the tracked orientation; generating a second mask representing a second set of values associated with a transmission interface of the display device using the tracked orientation; and generating an output image using the first mask and the second mask.
Generating the output image may include: obtaining a left image having a first set of pixels and a right image having a second set of pixels, assigning the first set of values to the first set of pixels in the left image, assigning the second set of values to the second set of pixels in the right image, and interleaving the left image with the right image according to the assigned first set of values and the assigned second set of values.
Implementations may include one or more of the following features. In some implementations, the method can include providing the output image to a display device. The output image may be configured to provide a left image to a left eye of a user viewing an emissive interface of the display device while simultaneously providing a right image to a right eye. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
In some implementations, at least a portion of the first set of pixels is viewable from a first location and the first location is associated with a left eye of a user viewing the display device, and at least a portion of the second set of pixels is viewable from a second location and the second location is associated with a right eye of the user viewing the display device.
In some implementations, the first mask and the second mask are derived from a topographic four-dimensional surface having plot lines representing positions of light projected through a plurality of lenticular lenses associated with an emissive interface of the display device. The first mask and the second mask may be generated by applying a non-linear mapping to a fractional portion of the surface.
In some implementations, the first mask and the second mask are color images having red, green, and blue color components, each color component for determining a respective color component of the output image. In some implementations, the first mask and the second mask include representations of at least one color index for each pixel of the display device. Implementations of the described technology may include hardware, methods or processes, or computer software on a computer-accessible medium.
The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
Drawings
Fig. 1 is a block diagram illustrating an example output image providing stereoscopic knitted content in a stereoscopic display according to an implementation described throughout this disclosure.
Fig. 2 is a block diagram of an example system for generating content for display in an autostereoscopic display device according to an implementation described throughout this disclosure.
FIG. 3 is a diagram of an example image defined on a u-v plane from an orientation associated with a viewer of the image on a display screen according to an implementation described throughout this disclosure.
FIG. 4 is a diagram illustrating an example geometric relationship between a viewer's head orientation and position in an example three-dimensional plane representing a display screen.
Fig. 5A-5C illustrate example optical features provided by implementations described throughout this disclosure.
6A-6C illustrate example masks sampled at various pixel locations for an autostereoscopic display according to implementations described throughout this disclosure.
7A-7B illustrate an example mask for generating the image in FIG. 8C according to an implementation described throughout this disclosure.
Fig. 8A-8C illustrate example images provided on a stereoscopic display according to implementations described throughout this disclosure.
FIG. 9 is a diagram representing an example mesh and geometry of a sampled four-dimensional (4D) surface.
Fig. 10 is an example diagram of a function for determining the illumination or extinction of a pixel according to an implementation described throughout this disclosure.
FIG. 11 is an exemplary diagram of another function for determining the illumination or extinction of a pixel according to an implementation described throughout this disclosure.
FIG. 12 is an exemplary diagram of another function for determining the illumination or extinction of a pixel according to an implementation described throughout this disclosure.
FIG. 13 is an exemplary diagram of yet another function for determining the illumination or extinction of a pixel according to an implementation described throughout this disclosure.
Fig. 14 is a flow diagram illustrating one embodiment of a process of generating stereoscopic braided image content according to an implementation described throughout this disclosure.
Fig. 15 is an example technique for determining a mask based on each row of a lenticular display according to an implementation described throughout this disclosure.
Fig. 16 illustrates an example function of weaving left and right images together to generate stereoscopic woven image content according to implementations described throughout this disclosure.
Fig. 17 is a flow chart illustrating one example of a process for using a lookup table to determine view-dependent pixel values based on a determined head orientation according to an implementation described throughout this disclosure.
FIG. 18 shows an example of a general-purpose computer device and a general-purpose mobile computer device that can be used with the techniques described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed description of the invention
Autostereoscopic displays are capable of providing imagery that approximates three-dimensional (3D) optical characteristics of physical objects in the real world without the use of Head Mounted Display (HMD) devices. Typically, autostereoscopic displays include a flat panel display, a lenticular lens (e.g., a microlens array), and/or a parallax barrier for redirecting an image to multiple different viewing regions associated with the display.
In some example autostereoscopic displays, there may be a single location that provides a 3D view of image content provided by such a display. The user may sit in this single position to experience a 3D image that is properly parallax, little distorted, and realistic. If the user moves to a different physical location (or changes head orientation or eye gaze orientation), the image content may begin to appear less realistic, 2D, and/or distorted. The systems and methods described herein may reconfigure image content projected from a display to ensure that a user is able to move around, but still experience a suitable parallax, low distortion rate, and realistic 3D image in real time. Thus, the systems and methods described herein provide the advantage of maintaining and providing 3D image content to a user despite user movement that occurs while the user is viewing a display.
Systems and methods described herein may evaluate how to display image content on an autostereoscopic display in response to detecting movement of a user proximate to the display. For example, if the user (or the user's head or eyes) moves left or right, the systems and methods described herein can detect such movement to display image content in a manner that determines how to provide the user (and for each eye of the user) with the 3D depth, appropriate disparity, and 3D perception of the image content.
Determining how to display the image content may include generating masks (e.g., sub-pixel mask images) that can be calculated for the left and right eyes of a user viewing the image content on the display. In some implementations, the mask may represent an image stored in memory on the computing device. The mask may indicate which particular pixels or sub-pixels of the display are to be illuminated (and which sub-pixels are to be extinguished) to properly display the 3D effect and image content to both the left and right eyes of a user viewing the image content on the display. In short, the systems and methods described herein may generate two masks for an image for a user when movement of the user is detected. The first mask may be used to display image content intended for viewing by the left eye of the user, whereas the second mask may be used to display image content intended for viewing by the right eye of the user.
In some implementations, the mask may be used to appropriately orient or redirect the image content for display to each eye of a user viewing the image content on the autostereoscopic display. For example, the systems described herein can track head orientation, eye movement, and/or body movement and use the tracked movement to generate masks (e.g., mask images) to adjust how image content is provided for display on display devices described throughout this disclosure.
In some implementations, generating the mask may include using a variety of algorithms and techniques to determine which portions of pixels in the image content may be provided to each eye to ensure a 3D effect. The mask can be used to combine a left eye image (e.g., a left image) and a right eye image (e.g., a right image) to produce an output image that can be provided to a display device. The output image may represent image content configured to be displayed to the left and right eyes, respectively.
The left image and the right image may be combined to generate an output image. Combining the left image with the right image may be described as stereoscopic weaving (e.g., swizzling) of pixels using a stereoscopic weaving algorithm that utilizes masks generated by a mask generation algorithm. Systems and methods described herein may provide one or more stereo weaving algorithms configured to interleave pixels from a generated left image and pixels from a generated right image such that a left eye of a user views the left image while a right eye of the user views the right image.
In some implementations, the systems and methods described herein may use any number of stereoscopic weaving algorithms to determine which set of pixels is to be assigned as viewable within an image being presented by an autostereoscopic display. For example, masks (e.g., sub-pixel masks) that operate as an eye assignment function to ensure that each eye views an appropriate 3D version of the output image may be computed using the various techniques described herein. For example, a left mask may be calculated and generated to represent the left eye assignment function. Similarly, a right eye mask may be calculated and generated to represent the right eye assignment function. Left and right masks may be used together to provide appropriate depth perception, 3D effects, and appropriate parallax for a particular set of images. Each generated mask may include a plurality of values assigned to each red, green, and blue (RGB) sub-pixel for a predefined display size (or display portion). The mask may be used to compute and generate an image to display to each eye of a user viewing the autostereoscopic display.
In general, the systems and methods described herein may utilize a variety of techniques and algorithms adapted to configure image content displayed on autostereoscopic displays described throughout this disclosure to ensure that a user viewing such content experiences realistic 3D content with appropriate parallax and minimal distortion. For example, the systems and methods described herein may determine and/or calculate masks used to assign an output image provided to each of the user's left and right eyes based on determining the position of the user (or the position of the user's head or eyes). In some implementations, the output image is provided relative to a particular portion of pixels being provided by the display. In particular, the system may determine that a left eye of a user (e.g., a viewer) may view a first set of pixels in the output image while a right eye of the user may view a second set of pixels in the output image. Each group of pixels can be determined based on the position of the user relative to the display (or relative to a pixel location on the display).
Fig. 1 is a block diagram illustrating an example of using one or more masks 100 (e.g., mask 100A and mask 100B) to provide stereoscopic woven content in an autostereoscopic display assembly 102 according to an implementation described throughout this disclosure. Stereoscopic weaving content may refer to interleaving a left image 104A with a right image 104B to obtain an output image 105. The autostereoscopic display assembly 102 shown in fig. 1 represents an assembled display that includes a high resolution display panel 107 coupled to (e.g., bonded to) at least a lenticular lens array 106. Further, the assembly 102 may include one or more glass spacers 108 between the lenticular lens array and the high-resolution display panel 107. In operation of the display assembly 102, the lens array 106 (e.g., a microlens array) and the glass spacers 108 may be designed such that under certain viewing conditions, the user's left eye views a first subset of pixels associated with an image, as shown by viewing rays 110, while the user's right eye views a mutually exclusive second subset of pixels, as shown by viewing rays 112.
A mask may be calculated and generated for each of the left and right eyes. The mask 100 may be different for each eye. For example, mask 100A may be calculated for the left eye, whereas mask 100B may be calculated for the right eye. In some implementations, the mask 100A may be a shifted version of the mask 100B. That is, the mask 100A may be shifted in one or more of the five dimensions of space (e.g., x, y, z, u, and v as shown in fig. 9). In some implementations, the mask 100A may be determined using pixel values associated with the mask 100B. For mathematical convenience throughout this disclosure, a mask 100 (e.g., mask 100A and/or mask 100B) may be labeled as a single mask m, where m is R M, and mL 1-m. Each mask 100A/100B may be represented in the (x-y-z) plane to correlate to the display assembly 102 in the (u-v) plane.
In some implementations, the mask 100 may include a single mask (mask image) m, which may be a two-dimensional (2D) plane represented by a plurality of pixel values (also having sub-pixel values). For example, each mask 100 (e.g., mask 100A for the left eye and mask 100B for the right eye) represents a surface s of a calculated sub-pixel having a value assigned to each RGB sub-pixel within an image to be displayed from the display component 102. Mask for the right eye (e.g., m) R ) May include a definition of one (e.g., m) R 1) to indicate the sub-pixel for which the particular sub-pixel is visible to the right eye of the user. Similarly, if the mask for the left eye (e.g., m) L ) Including being defined as one (e.g., m) L 1), then those subpixels are visible to the left eye of the user.
In some implementations, image I is output (e.g., output image 105)) Can be calculated by a system described throughout this disclosure as a right mask m shown by the following equation R (e.g., right mask image 100A) by right image I R (e.g., right image 104B) and left mask m L (e.g., left mask image 100A) by left image I L (e.g., left image 104A) combine: (I ═ m) R I R +m L I L ). Such calculations may be calculated on a per pixel, per color basis. Furthermore, a left mask m may be used L And a right mask m R The combination being equal to one (e.g., m) R +m L 1) constraint.
Typically, a mask (m) R )100B and (m) L )100A and a right image (I) R )104B and left image (I) L )104A may represent a 2D image including at least one component per color. For example, an example representation for a mask image may include m R (u, v) where the mask for the right eye is denoted m R And u and v represent spatial coordinates in the plane of the screen of the display assembly 102. The mask value m (u, v) may be calculated for each pixel based on whether the pixel is viewable by the left eye (e.g., m-0) or the right eye (e.g., m-1).
The mask image m (u, v) may depend on a variable (x, y, z) representing the 3D position of the user's head (e.g., viewing the display component 102). For example, the system described herein may track, for example, a center point between two eyes of a user whose position and movement are to be learned while viewing the component 102. The tracked position may be denoted as M (u, v, x, y, z) to show that the scalar value (M) depends on at least five variables u, v, x, y, and z. The scalar value (M) may also depend on the color index (c), where c ═ red, green, blue }. The lower case letter (M) may represent a 2D function (as an image), whereas the upper case letter (M) may represent a 5D function. Therefore, M (u, v) ═ M (u, v, x, y, z), is evaluated as a specific position x, y, and z.
In some implementations, a color index may be calculated for each mask image m (u, v) by determining, for each pixel in the mask, which of the red, green, and blue sub-pixels will become visible when displayed on the display. In general, each of the red, green, and blue sub-pixels may be sampled separately and independently of the other pixel. Further, a color index may be calculated for each eye of the user.
In some implementations, the location representing the user may be selected as the midpoint between the two eyes of the user to take advantage of symmetry between the eyes of the user. A left eye mask and a right eye mask can be calculated from the midpoint. Alternatively, a mask for the left eye (or right eye) can be calculated and a mask for the right eye (or left eye) can be interpolated from the calculation.
Consistent with implementations described herein, the autostereoscopic display assembly 102 may be a glasses-free lenticular three-dimensional display that includes a plurality of lenticules. In some implementations, the array 106 can include microlenses in a microlens array. In some implementations, the 3D image can be generated by projecting a portion (e.g., a first set of pixels) of a first image through at least one microlens (e.g., to the left eye of the user) in a first direction and projecting a portion (e.g., a second set of pixels) of a second image through at least one other microlens (e.g., to the right eye of the user) in a second direction. The second image may be similar to the first image, but the second image may be shifted from the first image to simulate parallax, thereby simulating a 3D stereoscopic image for a user viewing the autostereoscopic display assembly 102.
Fig. 2 is a block diagram of an example system 200 for generating content for display in an autostereoscopic display device 202 according to an implementation described throughout this disclosure. Although system 200 is shown separate from display 202, in some implementations, system 200 may be included as part of display 202. In general, the display 202 can comprise a high resolution and glasses-free lenticular three-dimensional display. For example, the display 202 can include a microlens array 106 that includes a plurality of lenses (e.g., microlenses) with glass spacers coupled (e.g., bonded) to the microlenses of the display. The microlenses may be designed such that, from a selected viewing orientation, a left eye of a user of the display may view the first set of pixels while a right eye of the user may view the second set of pixels (e.g., where the second set of pixels is mutually exclusive from the first set of pixels).
In some implementations, the microlenses of microlens array 106 can receive images 204 from display 202, which can be analyzed according to content and location of a user associated with viewing display 202. The image 204 may be processed by the system 200 into a pixel 208 and a sub-pixel 210 having a specific RGB sub-pixel 212 for the left eye and a RGB sub-pixel 214 for the right eye. System 200 may use pixel 208 (and sub-pixels 210, 212, and 214) to generate a first image (e.g., left image 216) configured for 3D stereoscopic viewing from a first location. Similarly, system 200 may use pixel 208 (and sub-pixels 210, 212, and 214) to generate a second image (e.g., right image 218) configured for 3D stereoscopic viewing from a second location. In some implementations, the first position may correspond to a position of a left eye of the user, whereas the second position may correspond to a position of a right eye of the user. In some implementations, the symmetry of the user can be exploited by performing calculations and evaluations for the user using offsets from the center of the eye position. That is, the left image 216 may be calculated using an offset distance from the tracked location between the user's eyes, while the right image 218 may be calculated using the same offset distance from the tracked location between the user's eyes in the opposite direction (i.e., toward the right eye).
The output image 226 represents a 3D stereoscopic image having an appropriate parallax and viewing configuration associated with the user's proximity to the display for both eyes based at least in part on the tracked position of the user's head. The system 200 may be used to determine the output image 226 whenever the user moves the head orientation while viewing the display 202.
In some implementations, the processor may include (or be in communication with) a Graphics Processing Unit (GPU) 221. In operation, the processor 221 may include (or have access to) memory, storage, and other processors (e.g., CPUs). To facilitate graphics and image generation, the processor 221 may communicate with the GPU 221 to display images on the display device 202. The CPU (e.g., processor 220) and GPU 221 may be connected by a high-speed bus such as PCI, AGP, or PCI-Express. The GPU 221 may be connected to the display 202 through another high-speed interface, such as HDMI, DVI, or a display port. Generally, the GPU 221 may render image content in the form of pixels. The display device 202 receives image content from the GPU 221 and displays the image content on a display screen.
In general, the system 200 can utilize a processor 220 or 228 (e.g., a CPU) and/or a GPU 221 to generate and render stereoscopic three-dimensional images (e.g., output images 226 using one or more masks 206) on a display panel of the display 202. For example, system 200 can generate and render left image 216 on a portion of the pixel grid for viewing through microlens array 106 from a first location corresponding to a location of a first eye of the user. Similarly, system 200 can generate and render right image 218 on a portion of the pixel grid for viewing through microlens array 106 from a second location corresponding to a location of a second eye of the user. In general, a left eye image 216 and a right image 218 may be generated and rendered to simulate parallax and depth perception for a user. That is, the left image 216 may represent a depth shift from the right image 218. For example, the grid of pixels 208 may display a first display image intended to be seen by the left eye of the user through the microlens array 106, and the grid of pixels 208 may display a second display image intended to be seen by the right eye of the participant through the microlens array 106. The first and second positions can be based on a position of the user relative to the display 202 (e.g., lateral/vertical position, orientation, depth, position of the left or right eye, etc.). In some implementations, the first and second directions for generating the first and second display images can be determined by selecting certain pixels from an array of pixels associated with the microlens array 106.
In some implementations, the microlens array 106 can include a plurality of microlens pairs including two microlenses. Display 202 may use at least two of these microlenses for displaying an image. In some implementations, the processing device 228 may be utilized by the display 202 to select a set of outgoing rays through which the image may be viewed through the microlenses to display the left and right images 216, 218 based on positional information corresponding to the orientation of the user relative to the display 202. In some implementations, the orientation (e.g., the user's location) can be detected by a tracking system 230 associated with the display 202 and/or the system 200. In some implementations, the orientation can be determined using one or more camera devices (not shown) associated with the system 200 and/or the display 202.
In some implementations, each of the plurality of microlenses can cover (e.g., can be disposed over or associated with) a plurality of discrete pixels such that each pixel is visible from some limited subset of directions in front of the display 202. If the location of the user viewing the display is known, a subset of pixels visible from the user's left eye under each lens of the display 202 and a subset of pixels visible from the user's right eye across the display 202 can be identified. By selecting for each pixel an appropriately rendered image corresponding to the virtual view to be seen from the eye position of the user, each eye is able to view the correct image. In some implementations, for example, the position of the user relative to the display 202 can be used to determine a direction for simultaneously projecting at least two images to the user of the display 202 via the microlenses 106.
The display 202 may include one or more processing devices 228, which may include one or more central processing units, graphics processing units, other types of processing units, or combinations thereof. The processing device 228 may perform functions and operations to command (e.g., trigger) the display 202 to display an image. The processing device 228 may analyze the captured visible and/or infrared light and determine image data (e.g., data corresponding to RGB values for a set of pixels that can be rendered as an image) and/or depth data (e.g., data corresponding to a depth for each of the RGB values for a set pixel in the rendered image). If computing system 200 is incorporated into display 202, processor 228 may be the same as processor 220. Similarly, if system 230 is incorporated into either or both of system 200 or display 202, tracking system 230 may utilize such resources.
In some implementations, the display 202 can include a memory 234. Depending on the implementation, the memory 234 may be one or more volatile memory units or nonvolatile memory units. The memory 234 may be any form of computer readable medium, such as a magnetic or optical disk or solid state memory. According to some implementations, the memory 234 may store instructions that cause the processing device 228 to perform functions and operations consistent with the disclosed implementations.
In some implementations, the display 202 can include Organic Light Emitting Diodes (OLEDs) that are small enough that the human eye or camera lens is not easily detectable, thereby making the display 202 effectively transparent. Such OLEDs may also have sufficient brightness such that when illuminated, the area for emitted light is significantly larger than their respective areas. As a result, the OLED, while not readily visible to the human eye or camera lens, is bright enough to illuminate the display 202 with the rendered image without gaps in the displayed image. In some implementations, the display 202 may be a switchable transparent lenticular three-dimensional display. In such an example, the OLEDs may be embedded in the glass substrate such that the glass is disposed between successive rows of OLEDs. This arrangement results in the display 202 being transparent when the OLED is not illuminated, but opaque when illuminated (due to the image displayed on the display 202).
In a switchable transparent lenticular three-dimensional display implementation, the lenticules 106 of the lenticular array can be made of a first material and a second material. For example, at least some of the microlenses 106 can be made of a first material and at least some of the microlenses 106 can be made of a second material. The first material may be a material that is unaffected (e.g., substantially unaffected) by the electrical current, whereas the second material may be affected (e.g., substantially affected) by the electrical current. For example, the first material and the second material may have different refractive indices when no current is applied to the second material. Such example components may produce refraction at the boundaries between the microlenses of the first material and the microlenses of the second material, creating a lenticular display. When a current is applied to the second material, the current may change the refractive index of the second material to be the same as the refractive index of the first material, thereby counteracting the lenticular nature of the display 202, such that the two materials form a single rectangular flat plate that refracts uniformly, thereby allowing the image on the display to pass through without distortion.
In some implementations, the current is applied to both the first material and the second material, where the current has the above-described effect on the second material and no effect on the first material. Thus, when the display 202 projects an image (e.g., the device OLED is illuminated), the processing device 228 may not apply current to the microlens array and the display 202 may act as a lenticular array (e.g., when turned on). When the OLEDs of the display 202 are not illuminated and the processing device 228 detects visible and infrared light, the processing device 228 may cause an electrical current to be applied to the display 202, thereby affecting the microlenses made of the second material. Application of an electrical current can change the refractive index of the microlenses made of the second material, and the display 202 may not act as a lenticular array. For example, the display 202 may be transparent or act as a clear glass sheet without a lenticular effect.
According to some implementations, the display 202 can include speaker components, I/O devices, and/or other interface mechanisms. The processing device 228 may be used to collect, receive, and/or generate image data, depth data, and/or position data to render stereoscopic three-dimensional images on the display 202. The processing device 228 can interpret the audio data to command the speaker assembly to project audio corresponding to the detected audio data. In some implementations, image data, depth data, audio data, and/or position data may be compressed or encoded, and the processing device 228 may perform functions and operations to decompress or decode the data. In some implementations, the image data can be in a standard image format such as JPEG or MPEG, for example. In some implementations, the depth data may be a matrix that specifies a depth value for each pixel of the image data, e.g., in a one-to-one correspondence.
The tracking system 230 may include sensors, cameras, detectors, and/or markers to track the position of all or a portion of the user. In some implementations, the tracking system 230 can track the location of the user in the room. In some implementations, the tracking system 230 may track the position of the user's eyes. In some implementations, the tracking system 230 can track the position of the user's head.
In some implementations, the tracking system 230 may track the position of the user (or the position of the user's eyes or head) relative to the display device 202, e.g., may be used to configure masks for the user's left and right eyes to display images with appropriate depth and disparity. In some implementations, for example, a head position associated with the user can be detected and used as a direction for simultaneously projecting at least two images to the user of the display device 202 via the microlenses 106.
In some implementations, the tracking system 230 can include (or utilize) a black and white camera. A black and white camera may return a black and white (i.e., colorless) image. Such cameras may be sensitive to light of all colors including red, green, blue, and infrared.
In some implementations, the tracking system 230 can include an infrared sensor and can employ one or more markers coupled to the user (e.g., reflective markers attached to the user) to accurately find the orientation of the user's head. As another example, an infrared camera can be used to accurately find the position of the user's head. For example, an infrared camera can be configured with a relatively fast face detector that can be used to position the user's eyes in at least two images and triangulate position in 3D. As yet another example, color pixels (e.g., RGB pixels) and depth sensors can be used to determine (e.g., directly determine) location information of a user.
FIG. 3 is a diagram of an example image 302 defined on a u-v plane from an orientation 304 of a head of a viewer of the image on a display screen 302. In this example, the orientation 306 is being viewed on the display screen 302. The user's eyes are represented at position 304. Here, the relationship between position 304 and position 306 is defined as a function in five dimensions: m (u, v, x, y, z). The value of the function M is a scalar value calculated by the system 200 for image content each time a user head movement is detected. If a particular calculated pixel (e.g., the pixel at position 306) is not viewable by the user's left eye at position 304, the value of M may be zero. If a particular calculated pixel (e.g., the pixel at position 306) is viewable by the user's left eye at position 304, the value of M may be one. Similar calculations and assignments of ones or zeros can be performed for the right eye of the user. In some implementations, additional calculations of the function M for both the left and right eyes may be performed for each representable color of each pixel (e.g., red, green, and blue (RGB)).
In the example of FIG. 3, for example, the screen 302 is shown as being defined on a u-v plane and dependent on the orientation (e.g., x, y, and z) of the head of a user viewing the display 302 depicting the image. For any one color, the scalar value of the function M depends on at least five variables, so the function M is five-dimensional. However, if each pixel on the display depicting the image on the screen 302 is known and if each head orientation in a particular plane is known, the system 200 may determine masks for both the left and right eyes of a viewer of the image 302. Thus, the system 200 is able to determine a mask for properly displaying an image to a user on the display screen 302 by using four of the five dimensions. For example, the function M can be reduced to a four-dimensional function instead of a five-dimensional function.
FIG. 4 is a diagram illustrating an example geometric relationship between a viewer's head position (represented at position 304) and a position (e.g., position 306) in a three-dimensional plane of a display screen. Fig. 4 graphically depicts data in four dimensions, which is simplified in one dimension from the data shown in fig. 3 in five dimensions. Reducing the dimension may include the system 200 at a known distance z 0 An image M (e.g., an image on screen 302) is defined at 404.
FIG. 4 also illustrates that any head orientation (x ', y ', z ')304 (corresponding to position 402) is associated with any plane (x, y, z) 0 )408, the geometric relationship between the locations 306. The origin of the plane 408 is shown at position 409. The u-v plane 302, screen and plane z-0 all coincide, whereas plane z-z 0 coincides with the x-y plane 408.
In general, head orientation may be calculated and stored for plane 408, and can be correlated to any head orientation in which plane 408 may be viewed. The system 200 is able to determine a change in head orientation for any orientation (e.g., orientation 304) based on the stored head orientations corresponding to the parallel planes 408. Because the rays travel in straight lines, the stored head orientation in plane (x, y)408 can be used to calculate the head orientation in plane (x ', y', z ')304 that holds the head orientation (x', y ', z'). For example, plane 302 may represent a screen of display 202. To determine whether the pixel at location 306 on plane 302 is visible from a particular orientation (e.g., orientation 304), system 200 may determine whether the pixel at location 306 is visible from an orientation 402 in the parallel plane.
As used herein, a capital M notation may represent the 4D function illustrated in fig. 4 (e.g., M (u, v, x, y)). The algorithm described herein may determine such a 4D function M (u, v, x, y) ═ M (u, v, x, y, z 0 ). In determining the 4D function, the system described herein (e.g., system 200) may represent the function as a terrain contour map (e.g., a surface) to ensure that the function is low frequency. Representing the function as a low frequency surface enables sub-sampling in an efficient manner. Once sub-sampling occurs, system 200 can perform intra-sampling on the representative surfaceThe restoration function M is inserted.
For example, the system 200 may use the function M as a contour map of the low frequency function S. To simplify the calculation, the 4D function M may be simplified to a 2D image M which is a contour map of the 2D image s. In general, both m and s may vary with head orientation.
For example, the position of the lenticular lens of the microlens array 106 projected onto the screen of the display 202 may be represented as the contour line of the low-frequency image or the topographic surface s (u, v). This low frequency surface is 4D because the surface depends on the point in the x-y plane from which the screen is being viewed. The capital letter S may be used throughout this disclosure to indicate the 4D surface S (u, v, x, y).
To facilitate the further concepts described below, the optical features associated with an autostereoscopic display device will now be described. Fig. 5A-5C illustrate example optical features provided throughout implementations described in this disclosure. As shown in fig. 5A, a cross-sectional view of an autostereoscopic display device 500 is shown. Each pixel 502 may emit red, green, and blue light via respective red, green, and blue sub-pixels (sub-pixel 502a, sub-pixel 502b, and sub-pixel 502 c). The color of each pixel 502 may be selected by a processor associated with the display 500. Each pixel 502 has a directional and multi-modal transmit beam.
The display device 500 is here shown adjacent to a row of cylindrical lenses 504 positioned in the front emission surface. The cylindrical lens may be a lenticular lens (or lens array) that receives the emitted light from the display device 500 and transmits the light to a viewer, as shown by beam 506. In some implementations, the lens array 504 is coupled to the lenticular display apparatus 500 via glass spacers. Lens array 504 may be used to focus light from each emission pixel, as shown by beam 506. The lens array 504 ensures that each beam is spaced apart by a distance in the gap between each beam emitted from the array 504. The system 200 can ensure that a particular pixel can inject light into one eye of a user while avoiding injecting light into the other eye of the user.
Fig. 5B illustrates an example front view of a portion of display device 500. The subpixels (e.g., subpixels 502a, 502b, and 502c) are shown in columns, and the lens array 504 is in front of the emissive pixels of the display device 500. In some implementations, the angle of the lens array 504 may be offset from line a by about 15 degrees. Line a is perpendicular to the bottom edge of display 500. Other angles are possible.
Fig. 5C illustrates another example front view of a portion of display device 500. In some implementations, a lenticular lens array (e.g., a lens array) may function similar to barrier display 510 to filter out image content (e.g., pixels) so that the system can provide specific image content for each eye of a user viewing display device 500. Similar to the lenticular array, the barrier display 510 blocks certain pixels and shows other pixels. If the gap in the barrier display 510 coincides with the central optical axis of the cylindrical lens, the same pixel is blocked or shown.
Typically, the array of lenses is selected to provide a specific distance between the beams emitted by each lens in the array 504. For a lenticular lens array (not shown), the barrier 510 and the barrier 512 may be configured to block all light except for the pixels 514, 516, 518, 520, 522, and 524.
The unblocked pixels may be aligned directly in the center position of a particular lenticular lens in the lens array. Similar configurations can be arranged to block and show other pixels in the display 500. In these configurations, the user may view the pixels shown by the gap 526. Barriers 510 and 512 may be used to display a shifted version of pixels from an image intended to be viewed by the left eye as a version of pixels from an image intended to be viewed by the right eye.
To simulate such a barrier as shown in fig. 5C, the system 200 may calculate a mask for each eye of a user viewing the display device from a location (x, y, z). In some implementations, for example, each mask (e.g., a shifted version of mask 100A for the left eye and mask 100B for the right eye) may represent a 3-color-per-pixel value that provides an appropriate view of a particular image from display 500. Example values for each pixel in the mask may include 1.0 or 0.0. A value of 1.0 indicates that the pixel in the mask is visible. A value of 0.0 indicates that the pixels in the mask are not visible.
6A-6C illustrate example masks sampled at various pixel locations for an autostereoscopic display according to implementations described throughout this disclosure. Fig. 6A depicts an example mask 602 (e.g., M (u, v) ═ M (u, v, x, y, z)) representing a 50 × 50 region of the selected set of values for orientation (x, y, z) on the display screen. Mask 602 includes an RGB image as displayed on a screen of a computing device. Mask 602 indicates which pixels should be illuminated from a particular determined location on display 202 for a particular image and which pixels should be extinguished (or remain extinguished). In particular, mask 602 indicates which pixels should be illuminated or extinguished based on the detected orientation of the user viewing display 202, for example. Mask 602 depicts half of the pixels being illuminated and half of the pixels being extinguished.
Fig. 6B depicts the mask 604 as viewed on a lenticular display screen. Mask 604 (e.g., M (u, v) ═ M (u, v, x, y, z)) illustrates the RGB sub-pixel structure. A mask 604 may be calculated for each of the left and right eyes. Typically, the right eye mask generates a shifted version of the left eye mask based on a known symmetry in the distance between the eyes of the user.
Fig. 6C depicts mask 606. Mask 606 is a version of mask 604 with the color filters removed. Mask 606 illustrates lenticular stripes throughout. For example, when the mask 606 is used to display content on the display 202, lenticular lines viewed by the left eye of the user are aligned with white stripes, while lenticular lines viewed by the right eye of the user are aligned with black stripes.
7A-7B illustrate an example mask for generating the image in FIG. 8C. For example, fig. 7A is a mask (m)702 determined by the system 200 for the left eye of a user proximate to the display 202. For example, fig. 7B is a mask 704 determined by the system 200 for the right eye of a user proximate to the display 202. The bi-convex lines 706 and 708 are depicted as alternating color stripes.
The masks 702 and 704 may change rapidly with u and v, but may also change rapidly with changes in head orientation. For example, a change in x position of about 60 millimeters may reverse the lenticular pattern. The masks 702 and 704 may be calculated and generated by the system 200 using the algorithm 224 and the tracking system 230. The masks 702 and 704 are determined and/or calculated to correspond to a head orientation of a user viewing the display device 202 and a plurality of pixels on the display 202.
Fig. 8A-8C illustrate example images provided on a stereoscopic display according to implementations described throughout this disclosure. The images 802 and 804 in fig. 8A and 8B may be provided to the display device 202 along with the masks 702 and 704. For example, once the mask 702 and the mask 704 are calculated for each of the left and right eyes, the system 200 may use the left mask 702 and multiply it by the left image 802. Similarly, system 200 may use right mask 704 and multiply it by right image 810. For example, the sum of left mask 702 times left image 802 and right mask 704 times right image 810 generates image 226 that is output for display to a user viewing display 202 (e.g., I ═ m) R I R +m L I L ). In some implementations, the display 202 may include circuitry for computing the output image, as well as processing software and hardware. In some implementations, the system 200 retrieves the images 802 and 810, computes the masks 702 and 704, and provides the resulting output image (I)226 to the display device 202.
As shown in fig. 8A, white image content 804 corresponds to pixels that are illuminated and visible to the user's left eye, while dark content 806 corresponds to off pixels. A number of features including feature 808 may be depicted. As shown in fig. 8B, right image 810 includes white image content 812 corresponding to pixels that are illuminated and visible to the user's right eye, while dark content 814 corresponds to off pixels that are considered black by the user. The image 810 includes several image features 816, 818, 820, and 822.
Fig. 8C illustrates an example output image 824 that is stereoscopically woven by the systems and techniques described herein. Output image 824 is a blended image that is stereoscopically woven by system 200 using image 802, image 810, mask 702, and mask 704. The features may appear to overlap because the output image is configured to properly display content to both the right and left eyes of a user viewing the display 202. In particular, the features 808 in fig. 8C correspond to the same features 808 shown in fig. 8A. Similarly, the features 816 in fig. 8C correspond to the same features 816 shown in fig. 8B. Further, features 818 and 820 in FIG. 8C correspond to corresponding features 818 and 820 in FIG. 8B. The image features 804 shown in FIG. 8C correspond to the features and locations 804 in FIG. 8A.
Fig. 9 is a diagram representing an example mesh 902 of a sparsely sampled four-dimensional (4D) surface. As described throughout this disclosure, a surface may represent a (valued) mask associated with a left or right eye. The grid 902 is in the (x, y) plane and represents samples of pixel values of a lenticular display device, such as display 202, here represented as grid 904 in the (u, v) plane.
Returning to the calculation of the mask as surface S, the 2D image S (u, v) appears as a plane from a fixed head orientation (e.g., fixed (x, y)). For example, S can be expressed as S ═ C 1 u+C 2 v+C 3 In which C is 1 、C 2 And C 3 Is a constant. In this regard, the system 200 may interpolate the planes to determine the mask for the left eye and the mask for the right image. For example, a display device may have a screen that may be represented as a plane using values at the corners of the screen. System 200 may bilinear interpolate to obtain all values of one that indicate which pixels are visible from a particular head orientation (e.g., orientation 906). In general, the slope (C) of the plane varies with x and y 1 And C 2 ) May remain unchanged but offset (C) 3 ) May vary. This offset varies linearly with x and y. Thus, the system 200 may store values at the four corners of the x-y plane and may later interpolate the pixel values to generate a mask according to a particular head orientation.
If four values at the corners of the screen of the system storage 904, sixteen values are utilized (e.g., 2 × 2 × 2 × 16). This is because each of the four variables can have two possible values. Compensation can be made by sub-sampling at a lower rate or size to account for defects in the lenticular lens or other manufacturing portion of the lenticular display device. For example, to compensate for equipment imperfections, the system 200 may sub-sample the 4D surface S at, for example, 17 × 10 × 5 × 3 points. A surface S (e.g., S (u, v, x, y)) is stored on sparse grid 902. Grid 902 is then linearly interpolated to obtain the value of s for each pixel on the screen of device 202, e.g., as represented by grid 904, and for any value of head orientation (x, y).
As used herein, square brackets may indicate discrete sampling functions defined for integer input variables. For example, the surface function S [ U, V, X, Y]＝S(UΔ u ,VΔ v ,XΔ x ,YΔ y ) Wherein capital letters U, V, X and Y represent integers and Δ u 、Δ v 、Δ x And Δ y The spacing of the sampling grid shown in fig. 9 is shown. In some implementations, the width of the display device is calculated to be 16 Δ u 。
In some example implementations, the sub-sampled version of the surface function S may be stored in memory by the processor 220, for example. Such a subsampled version may use an array of size (17 × 10 × 5 × 3 ═ 2550) floating point numbers. The (17 × 10) matrix represents the u-v plane of grid 904 (e.g., representing display 202), and the (5 × 3) matrix represents the x-y plane of grid 902. In some implementations, three colors (e.g., RGB) in the mask and final output image are represented with an additional factor of 3.
In operation, the system 200 may determine (or predict) a head position 906 (e.g., position (x ', y ', z ')). For example, the tracking system 230 may determine the head position of a user proximate to the display 202. For example, the tracking system 230 may predict that the head position may be in different locations based on the detected speed of the user. For example, the head orientation may be used to calculate a 2D image (e.g., 17 × 10 samples) s [ U, V ] using processor 220. The processor 220 may send the 2D image to the GPU 221. For example, the user's head may be determined at a fixed position 906 (x', y ', z'). The system 200 can calculate the individual values of s U, V one at a time. For example, the system 200 may calculate the pixel values of s [ U, V ] with U-3 and V-6. That is, the system may calculate s [3,6] as indicated by block 908 in FIG. 9.
To calculate the pixel values of the example box 908, the system 200 may anchor points 910. Point 910 may be tabulatedOn the x-y plane, on a straight line passing through (x ', y ', z ') and (u, v) ═ 3 Δ u ,6Δ v ) The position of (a). As an example, if point 910 is located at (x, y) ═ 1.6 Δ x ,1.3Δ y ) Then the system 200 can take the first corner 912 (e.g., S3, 6, 1)]) A second corner 914 (e.g., S [3,6,2,1]]) A third corner 916 (e.g., S3, 6,1, 2)]) And a fourth corner 918 (e.g., S3, 6,2]) To calculate s [3,6] by a weighted sum of the four values in]. The values at corners 912, 914, 916, and 918 represent s [3,6] as viewed from each respective corner 912 and 918]The value of (c).
The system 200 may utilize a weighting factor that follows normal bilinear interpolation using the values at corners 912 and 918. For example, s [3,6] can be calculated at each respective corner 912-]And s [3,6]]Can be equivalent to 0.4 × 0.7 × S [3,6, 1]]+0.6×0.7×S[3,6,2,1]+0.4×0.3×S[3,6,1,2]+0.6×0.3×S[3,6,2,2]. Can be used for s [ U, V ]]Each of the (17 × 10) samples of (a) performs such calculation. Upon completion, the processor 220 may send the subsampled image to the GPU 221. Thus, from any (x ', y ', z ') location 906 to z-z occurs in the processor 220 (e.g., at the CPU) 0 Mapping of planes and when z equals z 0 Bilinear interpolation in-plane, and (17 × 10) images are uploaded to the GPU 221. For example, GPU 221 may support automatic hardware bilinear interpolation to ensure that s (u, v) may be quickly determined for each pixel on display 202.
Next, the value calculated for s may be converted into an actual mask value m. For example, the mask generation algorithm 224 may perform a non-linear mapping on the values of the fractional portion of the surface s (e.g., frac (s)) to determine the value of each pixel (and sub-pixel) represented in the mask m. That is, for a number s the integer part of the number is the largest integer not greater than s and the fractional part of s is the difference between the integer part and s. These values may include one and zero to indicate an illuminated state or an extinguished state, respectively.
In general, mask m may be based on frac(s) and the relationships are depicted in FIGS. 10-13. When frac(s) is 0, this corresponds to white line 706, indicating those pixels are visible to the user. When frac(s) is as far away from 1 as possible (e.g., frac(s) ═ 0.5), this corresponds to black line 708, which indicates those pixels are not visible to the user. An example is shown in fig. 10, where frac(s) is 0 and m is 1, and when frac(s) is 0.5, m is 0.
The system 200 is able to calculate a value for each pixel of the display 202 using frac(s). Once frac(s) is calculated, the mask generation algorithm 224 may calculate normalized texture coordinates corresponding to the user's current viewpoint. Four neighboring blocks can be selected to calibrate the image and bilinear interpolation can be performed to calculate the view-dependent calibration pixel values.
The algorithm 224 may generate one or more 4D lookup tables for determining the stereoscopic weave of pixels using the mask 206. The 4D lookup table in the above example may comprise a lookup table of size 17 × 10 × 5 × 3 pixels. In operation, the system 200 is able to track a user and determine a position of the user's head and/or eyes in the x-y plane using the tracking system 230. A straight line can be interpolated from this position to the sample point in the u-v plane (representing the display 202). A mask for the left eye and a mask for the right eye can be retrieved from the masks 206. A lookup table may be used to retrieve the values at the four corner locations 912, 914, 916, and 918. The retrieved values may be used to generate left and right eye masks for weaving the left and right images together. Such a weave of images can provide appropriate parallax and depth to a 3D video image.
The graphs shown in fig. 10-13 provide a representation of the manner in which low frequency images (e.g., surface S and function S) can be sub-sampled and interpolated to recover the 4D high frequency function M. Mask m may be used to determine which portions of pixels are to be supplied to each eye of a user viewing image content on a display device described herein. In general, the mask generation algorithm 224 may generate a mask for the left eye of a user viewing the display 202 and a mask for the right eye of a user viewing the display 202. The two masks are added to one. Errors in tracking can cause visible flicker and afterimages observed by the user. Thus, rather than tracking two separate positions representing the left and right eyes, the system 200 may use the tracking system 230 to track the midpoint position between the eyes of a user viewing the display 202. From the midpoint location, the system 200 may utilize a single value of s, which may be provided as an output to two non-linear functions shown in the exemplary diagrams in fig. 11-13.
FIG. 10 is an example illustration 1000 of a function for determining the illumination or extinction of a pixel. Diagram 1000 shows the relationship between the value of s and the value of m. In this example, s is a two-dimensional image representing the terrain surface. For example, if the s value at a particular pixel is 6.37, then the pixel may be above the third column and below the seventh row relative to the lenticular display. To determine which mask image m to use for this same pixel, the system can determine a value of 6.37 and use that value to find a fractional fraction (e.g., 0.37). The non-linear function shown in fig. 10 can be applied to find the value of m at that pixel. In this example, the system 200 may determine that the value of m at the determined pixel is zero (as shown at line 1004). This non-linear function 1000 is applied to all pixel values in s on a pixel-by-pixel basis to obtain a value in m.
Because pixels may switch abruptly with the vertical slope shown at locations 1002 and 1006, diagram 1000 may not be desirable to display 3D content. Sudden switching can cause distortion and flicker. Thus, the system 200 may utilize functions that include gradual transitions, as shown in FIGS. 11-13.
FIG. 11 is an exemplary illustration 1100 of a function for determining the illumination or extinction of a pixel. Because a gradual transition may provide a less distorted and smoother image than the switching ratio shown in fig. 10, a gradual slope corresponding approximately to one pixel in width is introduced at the transition point 1102. In this example, curve 1104 corresponds to m L (e.g., left mask). Curve 1106 corresponds to m R (e.g., right mask).
FIG. 12 is another example illustration 1200 of a function for determining the illumination or extinction of a pixel. In this example, curve 1202 corresponds to m L (e.g., left eye mask). Curve 1204 corresponds to m R (e.g., right eye mask). As shown at location 1206, the value of m may be allowed to be less than zero. By allowing the value of m to become negative, the system 200 canA portion of left image mask 206A is subtracted from right image mask 206B to compensate for (i.e., cancel) signal crosstalk that may be introduced by scattering of light. The non-linear function shown in fig. 12 may be used to compensate for this signal crosstalk. Analyzing crosstalk cancellation using such techniques can enable the introduction of non-linear functions that can apply different amounts of crosstalk cancellation depending on the interpupillary distance (IPD) of the viewer. One example non-linear function that may be used to eliminate cross-talk between pixels is shown in fig. 13.
Fig. 13 is an example illustration 1300 of a non-linear function for determining the illumination or extinction of a pixel. The mask generation algorithm 224 can implement such a function in software or hardware (e.g., in an FPGA or ASIC) to map s to mask m. Diagram 1300 depicts a left eye function with non-linear movement at positions 1302, 1304, 1306 and 1308. Similarly, right eye functions are depicted with non-linear movement at positions 1310, 1312, 1314 and 1316. Function 1300 can be used to cancel crosstalk without knowing an interpupillary distance (IPD) associated with the user. This may provide robust error detection or avoid tracking errors.
Figure 14 is a flow diagram illustrating one embodiment of a process 1400 for generating stereoscopic braided image content. In general, for example, process 1400 may be performed by one or more processors on a system communicatively coupled to display device 202. In some implementations, the one or more processors may be within the device 202. In some implementations, the one or more processors can be external to the device 202 and can communicate with the display 202 to provide imaging on a screen of the display 202.
The display associated with process 1400 may be a high resolution display having a lenticular array of lenses coupled to the front of the display. The user may be viewing image content provided on the display 202. Process 1400 may ensure that the left eye view for the user includes certain pixels while the right eye view for the user includes a mutually exclusive subset of pixels. Process 1400 may generate an output image using a mask (e.g., a mask image) that may be used to stereoscopically weave left and right images together. For example, the process 1400 may obtain a left image and a right image and interleave pixels (from a mask image) in such a way as to enable a left eye of the user to view the left image and a right eye of the user to view the right image when viewed through a lenticular array coupled to the display 202.
At block 1402, process 1400 can include determining a tracked position associated with viewing a transmission interface of a display device. For example, the tracking system 230 may determine a location associated with, for example, a user who is viewing the device 202. That is, the tracking system 230 may determine the head orientation or eye gaze of the user. In some implementations, the system 200 can track and use the physical location of other portions of the user in order to provide image content constructed for the left and right eyes of the user viewing the display 202.
At block 1404, process 1400 includes generating a first mask representing a first set of values associated with a transmission interface of a display device using the tracked position of the user. For example, the system 200 can use the mask generation algorithm 224 to determine a left mask associated with viewing an image by the left eye of a user viewing the display 202. A first mask (e.g., left mask 206A) may be computed using a processor and algorithm as described herein. Left mask 206A may include values for each RGB sub-pixel 212 corresponding to a particular image 204. The left mask 206A may be used as a left eye assignment function.
At block 1406, the process 1400 includes generating a second mask representing a second set of values associated with the emissive interface of the display device using the tracked orientation. For example, the system 200 can use the mask generation algorithm 224 to determine a right mask associated with viewing an image by the right eye of a user viewing the display 202. A second mask (e.g., right mask 206B) may be computed using a processor and algorithm as described herein. Right mask 206B may include values for each RGB sub-pixel 214 corresponding to a particular image 204. The right mask 206B may be used as a right eye assignment function.
In some implementations, the left mask 206A and the right mask 206B are derived from a terrain 4D surface, where the plotted lines represent locations of light projected through a plurality of lenticular lenses associated with an emissive interface of the display device 202. For example, the mask may serve as a topographic contour map to map the locations of the lenticular lens associated with the display 202 and the image being provided on the display 202. The first mask 206A and the second mask 206B may be generated by applying a non-linear mapping to the fractional portion of the surface.
In some implementations, the first mask 206A represents a first sampled portion of pixels for a detected head orientation of a user viewing the display device 202, and the second mask 206B represents a second sampled portion of pixels for the detected head orientation of the user. For example, each mask may be calculated for a particular eye to view a different respective portion of the image displayed on the display 202. In some implementations, the first mask 206A and the second mask 206B are used to obtain a value for each pixel associated with the display device 202 for a plurality of changeable head orientations of the user.
In some implementations, the tracked orientation is a head orientation of a user viewing the display 202 and the first mask (e.g., the left mask 206A) and the second mask (e.g., the right mask 206B) are updated based on detected movement of the head orientation. In some implementations, as described in detail in the description of fig. 15, the tracked orientation varies for each row of the display device.
In some implementations, at least a portion of the first set of pixels is viewable from a first location associated with a left eye of a user viewing the display device 202. Similarly, at least a portion of the second set of pixels is viewable from a second location associated with a right eye of a user viewing the display device 202.
At block 1408, process 1400 includes generating output image 226 using the first mask (e.g., left mask 206A) and the second mask (e.g., right mask 206B). Generating output image 226 may include obtaining left image 216 having a first set of pixels and right image having a second set of pixels, the first and second sets of pixels representing subpixels 212 and 214, respectively. For example, processor 220 may request and/or obtain any portion of pixels from image 204 for use in generating left image 216 and right image 218.
Next, process 1400 may include: assigning a first set of values (from the calculated left mask 206A) to a first set of pixels in the left image 216; and a second set of values (from the calculated right mask 206B) to a second set of pixels in the right image 218. Left image 216 and right image 218 generated using left mask 206A and right mask 206B, respectively, may then be interleaved according to the assigned first set of values and the assigned second set of values. In operation, processor 220 may blend pixels of left image 216 and pixels of right image 218 together according to left mask 206A and right mask 206B.
The output image 226 may be provided to a display device. For example, the blended (i.e., stereo-woven) image may be provided by processor 220 to GPU 221. GPU 221 may provide the output images to autostereoscopic display 202 for display to a user tracked to the detected orientation. In general, the output image 226 may be configured to provide the left image 216 to the left eye of a user viewing the emissive interface of the display device 202 while simultaneously providing the right image 218 to the right eye of a user viewing the emissive interface of the display device 202.
In some implementations, the process 1400 may also include calculating a color index for each of the first mask and the second mask. For example, color indices may be calculated for subpixels in RGB-L212 and RGB-R214. Process 1400 may determine, for each value in each of the first and second masks, which of the red, green, and blue subpixels is to be indicated as being illuminated in the respective mask. The indication as illuminated may refer to a one (1) value of a pixel or sub-pixel in the mask 206. In some implementations, the first mask (e.g., left mask 206A) and the second mask (e.g., right mask 206B) can include representations of at least one color index for each pixel of the display device. For example, the color index may add additional dimensions to the 2D structure described herein.
FIG. 15 is an example technique for determining a mask based on each row of a lenticular display. u-v grid 1502 represents the display screen of display 202. An x-y grid 1504 represents locations a predetermined distance from the screen of the display 202. The user may move from the orientation 1506 to the orientation 1508 and/or to another orientation 1510. The system 200 may compute images and masks that can provide appropriate 3D image content to mobile users.
In general, a lenticular display, such as autostereoscopic display 202, may receive and display an output image. For example, the output image 226 provided to the display 202 may be updated to be displayed by one row of pixels at a time. Thus, different lines may be provided on the screen of the display 202 at slightly different times for one rendered frame of content. If the user is moving, the position of the user's head may be different for different rows of the content frame being displayed. The system 200 may account for differences in display times. For example, as described above, a 17 × 10 image s [ U, V ] may be determined. The image may be sent to the GPU 221. However, when calculating the image s [ U, V ], the system 200 may allow the head orientation to vary with each row V of the display 202. The amount of computation remains the same, but for each computation the head orientation is updated after one line of s [ U, V ] is computed. For example, if it is determined that the user has moved, the user's head position may be updated, which may be more frequent than the rate at which particular image frames are rendered and/or updated. Each new row of s U, V may potentially use a different head orientation.
Fig. 16 illustrates an example function 1600 for weaving left and right images together to generate stereo woven image content. In this example, the left eye function 1602 is woven with the right eye function 1604. The fill factor is 1.0 with a spacing of 0.25. This function enables left mask 206A and right mask 206B to sum to 1.0.
Fig. 17 is a flow diagram illustrating one example of a process 1700 for using a lookup table to determine view-dependent pixel values based on a determined head orientation. In short, the process 1700 may provide an example of using values in a lookup table to find a surface s of calculated pixel values and doing so after determining the head orientation (or change in head orientation).
At block 1702, the process 1700 includes determining coordinates corresponding to a current viewpoint. For example, the system 200 can utilize the tracking system 230 to determine the head position of the user. From the head orientation, a normalized set of coordinates corresponding to the current viewpoint may be calculated.
At block 1704, the process 1700 includes selecting a neighboring block in the image. For example, four adjacent blocks of a particular calibration image can be selected. For a particular image, four corner locations can be selected, as described with respect to fig. 9 and locations 912, 914, 916, and 918.
At block 1706, the process 1700 may include performing bilinear interpolation of neighboring blocks to determine a plurality of view-dependent calibration pixel values. For example, using the values at corners 912 and 918 shown in FIG. 9, the system 200 can interpolate the calibration pixels. In this example, S [3,6] can be calculated at each respective inflection point 912-.
At block 1708, the process 1700 may include providing a plurality of view-dependent calibration pixel values. The plurality of view-dependent calibration pixel values may correspond to the determined head orientation. Each time the head orientation changes, the system 200 may re-determine calibration pixel values for the changed head orientation and provide new calibration pixel values for use in generating a new output image for display to the user.
In some implementations, the systems and techniques described herein can be used to represent a mask image as a 4D surface. A 4D surface may be generated and used with a non-linear mapping applied to the fractional portion of the surface to produce a mask for assigning pixels in an image to either the left or right eye. The mask may be calculated based on a head orientation of a user viewing a display depicting the image content.
In some implementations, additional surfaces can be added and used with the systems described herein to, for example, eliminate or remove crosstalk, as crosstalk can change as the viewing angle associated with the user and any imagery changes. The system may additionally use such surface representations to perform backlight uniformity correction, as backlight uniformity may vary with orientation on the display screen and with viewing angle.
FIG. 18 shows an example of a general purpose computer device 1800 and a general purpose mobile computer device 1850 that may be used with the techniques described herein. Computing device 1800 is intended to represent various forms of digital computers, such as laptops, desktops, tablets, workstations, personal digital assistants, televisions, servers, blade servers, mainframes, and other suitable computing devices. Computing device 1850 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
The memory 1804 stores information within the computing device 1800. In one implementation, the memory 1804 is a volatile memory unit or units. In another implementation, the memory 1804 is a non-volatile memory unit or units. The memory 1804 may also be another form of computer-readable medium, such as a magnetic or optical disk.
The storage device 1806 is capable of providing mass storage for the computing device 1800. In one implementation, the storage device 1806 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices (including devices in a storage area network or other configurations). The computer program product may be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 1804, the storage device 1806, or memory on processor 1802.
The high-speed controller 1808 manages bandwidth-intensive operations for the computing device 1800, while the low-speed controller 1812 manages lower bandwidth-intensive operations. Such allocation of functions is merely exemplary. In one embodiment, the high-speed controller 1808 is coupled to memory 1804, display 1816 (e.g., through a graphics processor or accelerator), and high-speed expansion ports 1810, which may accept various expansion cards (not shown). In an embodiment, low-speed controller 1812 is coupled to storage device 1806 and low-speed expansion port 1814. A low-speed expansion port, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled, e.g., through a network adapter, to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device (such as a switch or router).
As shown in the figure, computing device 1800 may be implemented in a number of different forms. For example, the computing device can be implemented as a standard server 1820 or multiple times in a group of such servers. The computing device may also be implemented as part of a rack server system 1824. Additionally, the computing device may be implemented in a personal computer (such as laptop computer 1822). Alternatively, components from computing device 1800 may be combined with other components in a mobile device (not shown), such as device 1850. Each of such devices may contain one or more of computing device 1800, 1850, and an entire system may be made up of multiple computing devices 1800, 1850 communicating with each other.
The processor 1852 may execute instructions within the computing device 1850, including instructions stored in the memory 1864. The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor may provide, for example, for coordination of the other components of the device 1850, such as control of user interfaces, applications run by device 1850, and wireless communication by device 1850.
The processor 1852 can communicate with a user through a control interface 1858 and a display interface 1856 coupled to a display 1854. For example, the display 1854 may be a TFT LCD (thin film transistor liquid crystal display) or OLED (organic light emitting diode) display or other suitable display technology. The display interface 1856 may include suitable circuitry for driving the display 1854 to present graphical and other information to a user. The control interface 1858 may receive commands from a user and convert the commands for submission to the processor 1852. In addition, an external interface 1862 may be provided in communication with processor 1852, so as to enable device 1850 to communicate with other devices in close proximity. External interface 1862 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
The memory may include, for example, flash memory and/or NVRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 1864, expansion memory 1874, or memory on processor 1852, which may be received through transceiver 1868 or external interface 1862, for example.
As shown in the figure, computing device 1850 may be implemented in a number of different forms. For example, the computing device may be implemented as a cellular telephone 1880. The computing device may also be implemented as part of a smart phone 1882, personal digital assistant, or other similar mobile device.
Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, application specific ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments may include: embodiments in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium," "computer-readable medium" refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having: a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to a user; and a keyboard and a pointing device (e.g., a mouse or a trackball) by which a user may provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user may be received in any form, including acoustic, speech, or tactile input.
The systems and techniques described here can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include: a local area network ("LAN"), a wide area network ("WAN"), and the Internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Various embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the invention.
In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other embodiments are within the scope of the following claims.
Claims (19)
1. A computer-implemented method, comprising:
determining a tracked orientation of a user associated with viewing an emission interface of a display device;
generating a first mask representing a first set of values associated with the emissive interface of the display device using the tracked orientation;
generating a second mask representing a second set of values associated with the emissive interface of the display device using the tracked orientation;
wherein the first mask and the second mask are derived from a grid representing samples of pixel values of the emission interface of the display device based on a predefined distance from the emission interface of the display device; and
generating an output image using the first mask and the second mask, the generating the output image comprising:
a left image having a first set of pixels and a right image having a second set of pixels are obtained,
assigning the first set of values to the first set of pixels in the left image,
assigning the second set of values to the second set of pixels in the right image, an
Interleaving the left image with the right image according to the assigned first set of values and the assigned second set of values.
2. The method of claim 1, further comprising:
providing the output image to the display device, the output image configured to provide the left image to a left eye of a user viewing the emissive interface of the display device while simultaneously providing the right image to a right eye of the user viewing the emissive interface of the display device.
3. The method of claim 1, wherein:
the first mask represents a first subset of pixels of a detected head orientation for a user viewing the display device;
the second mask represents a second subset of pixels for the detected head orientation of the user; and is
The first mask and the second mask are used to obtain a value for each pixel associated with the display device for a plurality of changeable head orientations of the user.
4. The method of claim 1, wherein the first mask and the second mask are color images having red, green, and blue color components, each color component being used to determine a respective color component of the output image.
5. The method of claim 1, wherein the tracked position is a head position of a user, and wherein the first mask and the second mask are updated based on detected movement of the head position.
6. The method of claim 1, wherein the first mask and the second mask comprise representations of at least one color index for each pixel of the display device.
7. The method of any of claims 1-6, wherein the tracked orientation varies for each row of the display device.
8. The method of claim 1, wherein:
generating the first mask comprises: applying a non-linear function to map a fractional portion of the emission interface of the display device to determine to assign the first set of values to the first set of pixels in the left image; and
generating the second mask comprises: applying the non-linear function to map different fractional portions of the emission interface of the display apparatus to determine to assign the second set of values to the second set of pixels in the right image.
9. The method of claim 1, wherein the grid is defined by four corner positions retrieved from a 4D lookup table, the 4D lookup table comprising samples of the transmission interface of the display device.
10. The method of claim 1, wherein:
the tracked orientation represents at least one detected head orientation of the user viewing the display device; and
the grid represents samples of pixel values of the emission interface of the display device sampled from the detected head orientation.
11. A computing device, comprising, in combination,
a memory storing executable instructions; and
a processor configured to execute the instructions, which when executed, cause the computing device to:
determining a tracked orientation of a user associated with viewing an emission interface of a display device;
generating a first mask representing a first set of values associated with the emissive interface of the display device using the tracked orientation;
generating a second mask representing a second set of values associated with the emissive interface of the display device using the tracked orientation;
wherein the first mask and the second mask are derived from a grid representing samples of pixel values of the emission interface of the display device based on a predefined distance from the emission interface of the display device; and is
Obtaining a left eye image having a first set of pixels;
obtaining a right eye image having a second set of pixels;
assigning the first set of values to the first set of pixels;
assigning the second set of values to the second set of pixels; and is
Generating an output image generated by interleaving the left-eye image and the right-eye image according to the assigned first set of values and the assigned second set of values.
12. The computing device of claim 11, wherein:
at least a portion of the first set of pixels is viewable from a first location and the first location is associated with a left eye of a user viewing the display device; and
at least a portion of the second set of pixels is viewable from a second location and the second location is associated with a right eye of the user viewing the display device.
13. The computing device of claim 11, wherein:
the first mask represents a first subset of pixels of a detected head orientation for a user viewing the display device;
the second mask represents a second subset of pixels for the detected head orientation of the user; and is
The first mask and the second mask are used to obtain a value for each pixel associated with the display device for a plurality of changeable head orientations of the user.
14. The computing device of claim 11, wherein the first mask and the second mask are color images having red, green, and blue color components, each color component being used to determine a respective color component of the output image.
15. The computing device of any of claims 11 to 14, wherein the first mask and the second mask comprise representations of at least one color index for each pixel of the display device.
16. A display device system, comprising:
a tracking module;
a display panel coupled to a lenticular lens array;
at least one processing device; and
a memory storing instructions that, when executed, cause the system to perform operations comprising:
determining a tracked orientation of a user associated with viewing an emission interface of a display device;
generating a first mask representing a first set of values associated with the emissive interface of the display device using the tracked orientation;
generating a second mask representing a second set of values associated with the emissive interface of the display device using the tracked orientation;
wherein the first mask and the second mask are derived from a grid representing samples of pixel values of the emission interface of the display device based on predefined distances from the emission interface of the display device; and
generating an output image using the first mask and the second mask, the generating the output image comprising:
a left image having a first set of pixels and a right image having a second set of pixels are obtained,
assigning the first set of values to the first set of pixels in the left image,
assigning the second set of values to the second set of pixels in the right image, an
Interleaving the left image with the right image according to the assigned first set of values and the assigned second set of values.
17. The system of claim 16, wherein the operations further comprise:
providing the output image to the display device, the output image configured to provide the left image to a left eye of a user viewing the emissive interface of the display device while simultaneously providing the right image to a right eye of the user viewing the emissive interface of the display device.
18. The system of claim 16, wherein the first mask and the second mask are color images having red, green, and blue color components, each color component for determining a respective color component of the output image.
19. The system of claim 16, wherein the tracked orientation varies for each row of the display device.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862696158P | 2018-07-10 | 2018-07-10 | |
US62/696,158 | 2018-07-10 | ||
US16/249,551 US11172190B2 (en) | 2018-07-10 | 2019-01-16 | Stereo weaving for head-tracked autostereoscopic displays |
US16/249,551 | 2019-01-16 | ||
PCT/US2019/040810 WO2020014126A1 (en) | 2018-07-10 | 2019-07-08 | Autostereoscopic display with viewer tracking |
Publications (2)
Publication Number | Publication Date |
---|---|
CN111919437A CN111919437A (en) | 2020-11-10 |
CN111919437B true CN111919437B (en) | 2022-08-09 |
Family
ID=69138604
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980021850.0A Active CN111919437B (en) | 2018-07-10 | 2019-07-08 | Stereoscopic knitting for head tracking autostereoscopic displays |
Country Status (4)
Country | Link |
---|---|
US (1) | US11172190B2 (en) |
EP (1) | EP3747191A1 (en) |
CN (1) | CN111919437B (en) |
WO (1) | WO2020014126A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
NL2020216B1 (en) * | 2017-12-30 | 2019-07-08 | Zhangjiagang Kangde Xin Optronics Mat Co Ltd | Method for reducing crosstalk on an autostereoscopic display |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP2835974A1 (en) * | 2013-08-05 | 2015-02-11 | TP Vision Holding B.V. | Multi-view 3D display system and method |
CN107124607A (en) * | 2017-05-08 | 2017-09-01 | 上海大学 | The naked-eye stereoscopic display device and method of a kind of combination visual fatigue detection |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9445072B2 (en) * | 2009-11-11 | 2016-09-13 | Disney Enterprises, Inc. | Synthesizing views based on image domain warping |
KR101860435B1 (en) * | 2011-05-12 | 2018-06-29 | 프라운호퍼 게젤샤프트 쭈르 푀르데룽 데어 안겐반텐 포르슝 에. 베. | An autostereoscopic screen and method for reproducing image information |
CN104685423B (en) | 2012-10-23 | 2017-07-28 | 李阳 | Dynamic solid and holographic display device |
US9013564B2 (en) * | 2013-05-07 | 2015-04-21 | Elwha Llc | Controllable lenticular lenslets |
WO2016007976A1 (en) * | 2014-07-15 | 2016-01-21 | Novomatic Ag | Method for the representation of a three-dimensional scene on an auto-stereoscopic monitor |
KR101975246B1 (en) * | 2014-10-10 | 2019-05-07 | 삼성전자주식회사 | Multi view image display apparatus and contorl method thereof |
EP3350649B1 (en) * | 2015-09-17 | 2024-05-08 | Fathom Optics Inc. | Multi-view displays and associated systems and methods |
US10560689B2 (en) * | 2017-11-28 | 2020-02-11 | Paul Lapstun | Viewpoint-optimized light field display |
-
2019
- 2019-01-16 US US16/249,551 patent/US11172190B2/en active Active
- 2019-07-08 WO PCT/US2019/040810 patent/WO2020014126A1/en unknown
- 2019-07-08 CN CN201980021850.0A patent/CN111919437B/en active Active
- 2019-07-08 EP EP19749488.3A patent/EP3747191A1/en active Pending
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP2835974A1 (en) * | 2013-08-05 | 2015-02-11 | TP Vision Holding B.V. | Multi-view 3D display system and method |
CN107124607A (en) * | 2017-05-08 | 2017-09-01 | 上海大学 | The naked-eye stereoscopic display device and method of a kind of combination visual fatigue detection |
Also Published As
Publication number | Publication date |
---|---|
CN111919437A (en) | 2020-11-10 |
US11172190B2 (en) | 2021-11-09 |
WO2020014126A1 (en) | 2020-01-16 |
EP3747191A1 (en) | 2020-12-09 |
US20200021796A1 (en) | 2020-01-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR102185130B1 (en) | Multi view image display apparatus and contorl method thereof | |
JP3966830B2 (en) | 3D display device | |
KR102140080B1 (en) | Multi view image display apparatus and controlling method thereof | |
CN102246085B (en) | Method of improved three dimensional display technique | |
KR102121389B1 (en) | Glassless 3d display apparatus and contorl method thereof | |
US20130127861A1 (en) | Display apparatuses and methods for simulating an autostereoscopic display device | |
US9224366B1 (en) | Bendable stereoscopic 3D display device | |
KR20160010169A (en) | Curved multiview image display apparatus and control method thereof | |
KR101975246B1 (en) | Multi view image display apparatus and contorl method thereof | |
CN102510515A (en) | Grating-type multi-viewpoint stereo image synthesis method | |
CN105430369A (en) | Autostereoscopic 3d display device | |
CN109782452B (en) | Stereoscopic image generation method, imaging method and system | |
CN113900273B (en) | Naked eye 3D display method and related equipment | |
JP4703635B2 (en) | Stereoscopic image generation method, apparatus thereof, and stereoscopic image display apparatus | |
TW201320719A (en) | Three-dimensional image display device, image processing device and image processing method | |
US10939092B2 (en) | Multiview image display apparatus and multiview image display method thereof | |
CN111919437B (en) | Stereoscopic knitting for head tracking autostereoscopic displays | |
KR102139746B1 (en) | Transparent display apparatus and method thereof | |
CN116508066A (en) | Three-dimensional (3D) facial feature tracking for an autostereoscopic telepresence system | |
Jurk et al. | A new type of multiview display | |
KR20150118325A (en) | multi view image display apparatus and display method thereof | |
WO2023219916A1 (en) | Predictive head-tracking multiview display and method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |