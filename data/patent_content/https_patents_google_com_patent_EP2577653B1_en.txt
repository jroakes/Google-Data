EP2577653B1 - Acoustic model adaptation using geographic information - Google Patents
Acoustic model adaptation using geographic information Download PDFInfo
- Publication number
- EP2577653B1 EP2577653B1 EP11723813.9A EP11723813A EP2577653B1 EP 2577653 B1 EP2577653 B1 EP 2577653B1 EP 11723813 A EP11723813 A EP 11723813A EP 2577653 B1 EP2577653 B1 EP 2577653B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- geographic location
- mobile device
- audio signals
- acoustic models
- adapting
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/065—Adaptation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
Definitions
- This specification relates to speech recognition.
- a user of a mobile device may enter text by, for example, typing on a keyboard or speaking into a microphone.
- an Automated Search Recognition (“ASR") engine may have difficulty accurately recognizing spoken words when the sounds associated with a particular language vary based on an accent of the user. As spoken by a New Yorker or a Bostonian, for example, a typical ASR engine may recognize the word “park,” as the words “pork” or "pack,” respectively.
- aspects of the subject matter described in this specification may be embodied in methods for automatically training, selecting, generating, or otherwise adapting, by an ASR engine, one or more acoustic models that are geographic-location specific (or "geo-specific") to one or more geographic areas.
- the acoustic models are applied to audio signals (or “samples,” or “waveforms") that are "geotagged” with location information, to perform speech recognition by comparing the audio signals to statistical representations of the sounds that make up each word of a particular language.
- the one or more acoustic models may include a single acoustic model that is geographic-location specific to a single geographic area or to multiple geographic areas.
- geotagged audio signals refer to signals that have been associated, or “tagged,” with location metadata (e.g., geographic location metadata, or relative location metadata) or geospatial metadata.
- location metadata e.g., geographic location metadata, or relative location metadata
- the location metadata may include navigational coordinates, such as latitude and longitude, altitude information, bearing or heading information, name or address information associated with the location, relative position or direction information, or information that references a type of a location.
- the methods for adapting the acoustic models include receiving geotagged audio signals recorded by multiple mobile devices in multiple geographic locations, and adapting one or more acoustic models using at least a portion of the geotagged audio signals.
- the ASR engine may perform speech recognition on the utterance using the adapted acoustic model or models.
- the acoustic models may be adapted before, during, or after the utterance is received.
- the utterance may correspond to any type of voice input, such as an input to a voice search query system, a dictation system, or a dialog system.
- a "search query” includes one or more query terms that a user submits to a search engine when the user requests the search engine to execute a search query, where a "term” or a “query term” includes one or more whole or partial words, characters, or strings of characters.
- a "result" (or a "search result") of the search query includes a Uniform Resource Identifier (URI) that references a resource that the search engine determines to be responsive to the search query.
- the search result may include other things, such as a title, preview image, user rating, map or directions, description of the corresponding resource, or a snippet of text that has been automatically or manually extracted from, or otherwise associated with, the corresponding resource.
- aspects of the subject matter described in this specification may be embodied in methods that include the actions of receiving an audio signal that corresponds to an utterance recorded by a mobile device, determining a geographic location associated with the mobile device, adapting one or more acoustic models for the geographic location, and performing speech recognition on the audio signal using the one or more acoustic models model that are adapted for the geographic location.
- adapting one or more acoustic models further include adapting one or more acoustic models before receiving the audio signal that corresponds to the utterance; adapting one or more acoustic models further include adapting one or more acoustic models after receiving the audio signal that corresponds to the utterance; the actions further include receiving geotagged audio signals that correspond to audio recorded by multiple mobile devices in multiple geographic locations; and adapting one or more acoustic models for the geographic location further includes adapting one or more acoustic models for the geographic location using a subset of the geotagged audio signals; the actions further include determining, for each of the geotagged audio signals, a distance between the geographic location associated with the mobile device and a geographic location associated the geotagged audio signal, and selecting, as the subset of the geotagged audio signals, the geotagged audio signals that are associated with geographic locations which are within a predetermined distance of the geographic location associated with the mobile device, or that are associated with geographic
- Speech recognition accuracy may be improved.
- Acoustic models may be adapted using utterances that accurately reflect the differences in accents, dialects, or speech patterns that exist within a given language, and that may occur across different geographic regions.
- Speech recognition may be performed at the server side, instead of on the client device, to allow for enhanced process optimization and to increase computational efficiency.
- FIG. 1 is a diagram of an example system 100 that uses geotagged audio to enhance speech recognition accuracy.
- FIG. 1 also illustrates a flow of data within the system 100 during states (a) to (i), as well as a user interface 101 that is displayed on a mobile device 102 of the system 100 during state (i).
- the system 100 adapts one or more acoustic models that are geo-specific to one or more geographic areas.
- the acoustic models are applied to audio signals that are geotagged with location information, to perform speech recognition by comparing the audio signals to statistical representations of the sounds that make up each word of a particular language.
- the system 100 includes the mobile device 102, which is in communication with a server 104 and an ASR engine 105 over one or more networks 106.
- the server 104 may be a search engine, a dictation engine, a dialogue system, or any other engine or system that uses transcribed speech, or that invokes a software application that uses transcribed speech, to perform some action.
- the networks 106 may include a wireless cellular network, a wireless local area network (WLAN) or Wi-Fi network, a Third Generation (3G) or Fourth Generation (4G) mobile telecommunications network, a private network such as an intranet, a public network such as the Internet, or any appropriate combination thereof.
- the states (a) through (i) depict a flow of data that occurs when an example process is performed by the system 100.
- the states (a) to (i) may be time-sequenced states, or they may occur in a sequence that is different than the illustrated sequence.
- the ASR engine 105 receives geotagged audio signals 107 to 109 from various devices (e.g., the mobile device 102 or other mobile or non-mobile devices), and adapts one or more geo-specific acoustic models 111 for one or more multiple geographic locations using the geotagged audio signals 107 to 109.
- the geo-specific acoustic models 111 may include one, single acoustic model that is adapted to be geo-specific to one geographic location or more than one geographic location, or the geo-specific acoustic models 111 may include two or more acoustic models that are collectively adapted to be geo-specific to one geographic location, or that are each adapted to be geo-specific to a different geographic location.
- one or more geographic locations associated with the mobile device 102 are determined.
- the ASR engine 105 transcribes the utterance 113 using the geo-specific acoustic models 111 that match, or that the ASR engine 105 determines to be suitable for, the geographic locations associated with the mobile device 102 (or the user 114 of the mobile device 102).
- One or more candidate transcriptions 115 are communicated from the ASR engine 105 to the server 104.
- the server 104 is a search engine
- the server 104 executes one or more search queries using the candidate transcriptions 115, generates search results 116, and communicates the search results 116 to the mobile device 102 for display.
- geotagged audio signals 107 to 109 are communicated to the ASR engine 105 over the networks 106.
- one or more of the geotagged audio signals 107 to 109 include the voices of different users. Fewer or more geotagged audio signals may be communicated to the ASR engine 105 during state (a).
- the geographic locations that are associated with the audio signals 107 to 109 may be used to cluster the audio signals by geographic region (and thus by accent, dialect, or speech pattern), and to adapt the one or more acoustic models 111 to better recognize speech that exhibit particular, geo-correlated accents, dialects, or speech patterns.
- the geotagged audio signals 107 to 109 may also include ambient sounds or environmental noises that occur (naturally or otherwise) at a particular location.
- the ASR engine 105 receives the geotagged audio signals 107 to 109, and stores the geotagged audio signals 107 to 109 (or portions thereof) in a collection of audio signals (e.g., on a computer-readable storage medium).
- the collection of audio signals stored by the ASR engine 105 is used for training, building, generating, or otherwise adapting one or more geo-specific acoustic models 111 that are used to perform speech recognition on geo-tagged audio signals and utterances.
- the ASR engine 105 receives an audio signal 107 that has been tagged with metadata 117 that references the location "New York City.” Further, the ASR engine 105 receives an audio signal 108 that has been tagged with metadata 118 that references the location "Boston,” and metadata 119 that references the "city” geographic location type ( i.e ., because "Boston” is a "city”). Additionally, the ASR engine 105 receives an audio signal 109 that has been tagged with metadata 120 that references the location "New England,” and metadata 121 that references the location "Boston” (“Boston” is a city in "New England”).
- the geo-tagged locations associated with the respective audio signals my refer to a location of a mobile device, a user, a location referenced by the utterance, a default location, the ASR engine 105, the networks 106 or a portion of the networks 106, or some other location
- the metadata 117 to 121 may, as illustrated, be associated with the audio signals 107 to 109 by the devices that communicate the metadata 117 to 121 to the ASR engine 105.
- the metadata 117 to 121 may be associated with the audio signals 107 to 109 by the ASR engine 105, the search engine 104, or by another server, based upon inferring a location of a mobile device 102 (or of the user 114 of the mobile device 102) after receiving untagged audio signals.
- the audio signals 107 to 109 may each include a two-second (or more) snippet of relatively high quality audio, such as sixteen kilohertz lossless audio.
- the metadata may reference the location of a device (or of a user a device) when audio was recorded, captured, generated or otherwise obtained, or the metadata may reference a location of the a device (or of the user of the device) at a time before or after the audio was recorded, captured, generated, or otherwise obtained.
- the audio signals 107 to 109 may be manually uploaded to the ASR engine 105 or, for users who opt to participate, the audio signals 107 to 109 may be automatically obtained and communicated to the ASR engine 105 without requiring an explicit, user actuation before each audio signal is communicated to the ASR engine 105.
- the metadata 117 to 121 may describe locations in any number of different formats or levels of detail or granularity.
- the metadata 117 to 121 may include a two dimensional coordinates (e.g., latitude and longitude), an address, or information that identifies a geographic region.
- the metadata 117 to 121 may describe a path of the vehicle (e.g., including a start point and an end point, and motion data).
- the metadata 117 to 121 may describe locations in terms of location type (e.g., “moving vehicle,” “on a beach,” “in a restaurant,” “in tall building,” “South Asia,” “rural area,” “someplace with construction noise,” “amusement park,” “on a boat,” “indoors,” “underground,” “on a street,” “forest”).
- location type e.g., "moving vehicle,” “on a beach,” “in a restaurant,” “in tall building,” “South Asia,” “rural area,” “someplace with construction noise,” “amusement park,” “on a boat,” “indoors,” “underground,” “on a street,” “forest”).
- the metadata 117 to 121 may describe locations in terms of a bounded area (e.g., expressed as a set of coordinates that define the bounded area), or may use a region identifier, such as a state name or identifier, city name, idiomatic name (e.g., "Central Park,” “Chinatown,” “TriBeCa”), a country name, or the identifier of arbitrarily defined region (e.g., "cell/region ABC123").
- a single audio signal may be associated with metadata that describes one location or location type, or more than one location and/or location type.
- the ASR engine 105 or the mobile device 102 may process the metadata 117 to 121 to adjust the level of detail of the location information (e.g., to determine a state associated with a particular set of coordinates), or the location information may be discretized ( e.g., by selecting a specific point along the path, or a region associated with the path).
- the level of detail of the metadata may also be adjusted by specifying or adding location type metadata, for example by adding an "on the beach" tag to an audio signal whose associated geographic coordinates are associated with a beach location, or by adding a "someplace with lots of people" tag to an audio signal that includes the sounds of multiple people talking in the background.
- the ASR engine 105 may filter the audio signal by removing metadata that references one or more of the locations.
- the geographic locations referenced by the metadata can also be converted into discretized features to reduce the number of possible distinct locations. This could be done, for example, by reducing the resolution of latitude and longitude coordinates (e.g., from 0.001 degrees to 1 degree, or to 5 degrees), or by converting the latitude and longitude coordinates into a name of a geographic location (e.g., by using regions defined by the boundaries between countries, states, cities or provinces).
- the ASR engine 105 adapts the one or more acoustic models 111 to enhance the recognition of speech that includes different accents
- the audio signals that are used to adapt the one or more acoustic models 111 should include samples of different users' voices, accents, and dialects in different geographic locations.
- the ASR engine 105 may use a voice activity detector to verify that the collection of audio signals stored by the ASR engine 105 includes audio signals in which voices are present, and to filter out or otherwise identify or exclude audio signals (or portions of the audio signals) that include ambient noise or environmental sounds only.
- the ASR engine 105 may remove portions of the audio signals that correspond to background noise that is occurs before or after a user speaks, or that occurs during pauses between words.
- the collection of the audio signals stored by the ASR engine 105 may include tens, hundreds, thousands, millions, or hundreds of millions of audio signals.
- the decision by the ASR engine 105 to store or not store a particular audio signal (or portion thereof) may be based on determining that the user's voice is or is not encoded in the audio signal, respectively.
- storing an audio signal by the ASR engine 105 may include identifying a portion of the audio signal that includes the user's voice, altering the audio signal by removing the portion that does not include the user's voice or by associating metadata which references the portion that includes the user's voice, and storing the altered audio signal.
- Ambient noise or environmental sound portions of the audio signals may be stored by the ASR engine 105 for other purposes, for example to build geo-specific noise models.
- the audio signals stored by the ASR engine 105 can, in some implementations, include other metadata tags, such as tags that indicate whether background voices (e.g., cafeteria chatter) are present within the audio signal, tags that identify the date on which a particular audio signal was obtained ( e.g., used to determine a sample age), tags that identify an accent of the user 114 of the mobile device 102, tags that identify a locale set by the user 114 of the mobile device 102 (e.g., tags that identify that the user 114 prefers British English or American English), or tags that identify whether a particular audio signal deviates in some way from other audio signals of the collection that were obtained in the same or similar location.
- tags that indicate whether background voices (e.g., cafeteria chatter) are present within the audio signal e.g., used to determine a sample age
- tags that identify an accent of the user 114 of the mobile device 102 e.g., used to determine a sample age
- the tags may identify that a user that has no accent, or has a strong accent (e.g., a South African accent), and is using a mobile device in a geographic area that is associated with a different strong accent (e.g., an Australian accent), to avoid adapting an acoustic model using audio signals that do not accurately reflect an accent associated with a particular geographic area.
- a strong accent e.g., a South African accent
- a different strong accent e.g., an Australian accent
- the ASR engine 105 may optionally filter audio signals to exclude particular audio signals that satisfy or that do not satisfy other criteria. For example, the ASR engine 105 may decide to not store audio signals that are older than a certain age, or that include background chatter that may uniquely identify an individual or that may otherwise be proprietary or private in nature.
- data referencing whether the audio signals stored by the ASR engine 105 were manually or automatically uploaded may be tagged in metadata associated with the audio signals, and the one or more acoustic models 111 may be adapted using only those audio signals that were automatically uploaded, or only those that were manually uploaded, or different weightings may be assigned to each category of upload during the adaptation of the acoustic models.
- an explicit tag may be applied to the audio signals stored by the ASR engine 105 to reference a particular geographic location
- an explicit tag is not required or is not used.
- a geographic location may be implicitly associated with an audio signal by processing search logs (e.g., stored with the server 104) to infer a geographic location for a particular audio signal.
- 'receipt' of a geo-tagged audio signals by the ASR engine 105 may include obtaining an audio signal that does is not expressly tagged with a geographic location, and deriving and associating one or more geo-tags for the audio signal.
- an audio signal 112 is communicated from the mobile device 102 to the ASR engine 105 over the networks 106.
- the audio signal 112 includes an utterance 113 ("Pahk yah kah," a phonetic transcription of the term “Park your car,” as might be spoken by a native Bostonian such as "Boston Bob") recorded by the mobile device 102 ( e.g., when the user implicitly or explicitly initiates a voice search query).
- the audio signal 112 includes metadata 123 that references the geographic location "Boston.”
- the audio signal 112 may also include a snippet of environmental audio, such as a two second snippet of audio that was recorded before or after the utterance 113 was spoken. While the utterance 113 is described an illustrated in FIG. 1 as a voice query, in other example implementations the utterance may be an voice input to dictation system or to a dialog system.
- the geographic location (“Boston") associated with the audio signal 112 may be defined using a same or different level of detail as the geographic locations associated with the audio signals stored by the ASR engine 105.
- the geographic locations associated with the audio signals stored by the ASR engine 105 may be expressed as geographic regions, while the geographic location associated with the audio signal 112 may be expressed as geographic coordinates.
- the ASR engine 105 may process the geographic metadata 123 or the metadata 117 to 121 to align the respective levels of detail, so that a subset selection process may be performed more easily.
- the metadata 123 may be associated with the audio signal 112 by the mobile device 102 (or the user 114 of the mobile device 102) based on a current geographic location when the utterance 113 is recorded, and may be communicated with the audio signal 112 from the mobile device 102 to the ASR engine 105.
- the metadata may be associated with the audio signal 112 by the ASR engine 105, based on a geographic location that the ASR engine 105 infers for the mobile device 102 (or the user 114 of the mobile device 102).
- the ASR engine 105 or the mobile device 102 may infer the geographic location using the user's calendar schedule, user preferences (e.g., as stored in a user account of the ASR engine 105 or the server 104, or as communicated from the mobile device 102), a default location, a past location (e.g., the most recent location calculated by a GPS module of the mobile device 102), information explicitly provided by the user when submitting the voice search query, from the utterance 113 themselves, triangulation (e.g., WiFi or cell tower triangulation), a GPS module in the mobile device 102, or dead reckoning.
- user preferences e.g., as stored in a user account of the ASR engine 105 or the server 104, or as communicated from the mobile device 102
- a default location e.g., a past location calculated by a GPS module of the mobile device 102
- a past location e.g., the most recent location calculated by a GPS module of the mobile device 102
- the metadata 123 may include accuracy information that specifies an accuracy of the geographic location determination, signifying a likelihood that the mobile device 102 (or the user 114 of the mobile device 102) was actually in the particular geographic location specified by the metadata 123 at the time when the utterance 113 was recorded.
- the ASR engine 105 or the mobile device 102 may infer the geographic location using the user's average location over all his utterances, the user's "home location", (e.g. where the user currently lives, or where he grew up and his accent came from, as specified explicitly by the user or inferred from the accent), a 'smoothed' location that represents the location of the user over some recent period of time, a combination of the current location and the user's home location (e.g., a four-dimensional signal derived from the two, two-dimensional latitude and longitude), or the current location, as a continuous two-dimensional latitude and longitude signal.
- the user's "home location” e.g. where the user currently lives, or where he grew up and his accent came from, as specified explicitly by the user or inferred from the accent
- a 'smoothed' location that represents the location of the user over some recent period of time
- a combination of the current location and the user's home location
- Metadata included with the audio signals may include a location or locale associated with the respective mobile device 102.
- the locale may describe, among other selectable parameters, a region in which the mobile device 102 is registered, or the language or dialect of the user 114 of the mobile device 102.
- the speech recognition module 124 may use this information to select, train, generate or otherwise adapt noise, speech, acoustic, popularity, or other models that match the context of the mobile device 102.
- the ASR engine 105 selects a subset of the audio signals that have been received by the ASR engine 105, and uses an acoustic model adaptation module 125 to train, generate, or otherwise adapt one or more acoustic models 111 (e.g., Gaussian Mixture Models (GMMs)) using the subset of the audio signals.
- acoustic models 111 e.g., Gaussian Mixture Models (GMMs)
- GMMs Gaussian Mixture Models
- the subset may include all, or fewer than all of the audio signals stored by the ASR engine 105.
- Machine learning techniques such as k-means may be used to select the subset. This selection may occur by comparing acoustic information from the audio signals 107 to 109 with acoustic information from the audio signal 112, to result in a subset that more accurately reflects actual geographical boundaries between different accents.
- the one or more acoustic models 111 are applied to the audio signal 112 to translate or transcribe the spoken utterance 113 into one or more textual, candidate transcriptions 115, and to generate speech recognition confidence scores to the candidate transcriptions 115.
- the one or more acoustic models 111 include statistical representations of the sounds that make up each word of a particular language, and the noise models are used for noise suppression or noise compensation. Both models enhance the intelligibility of the spoken utterance 113 to the ASR engine 105.
- the acoustic model adaptation module 125 may adapt an acoustic model for the geographic location ("Boston") associated with the audio signal 112 using the audio signals 108 and 109, because the audio signals 108 and 109 were geotagged as having been recorded at or near that geographic location, or at a same or similar type of location. Furthermore, the audio signal 112 may itself be used to adapt the one or more acoustic models 111, in addition to or instead of using the audio signals 108 and 109. In adapting an acoustic model for a particular geographic location, the acoustic model adaptation module 125 adapts an acoustic model based on criteria that may correlate to a particular accent, dialect, or pattern of speech.
- the acoustic model adaptation module 125 may adapt an acoustic model for another geographic location (e.g., "New York City"), using the audio signal 107 that was geotagged as having been recorded at or near that other geographic location, or at a same or similar type of location. If the acoustic model adaptation module 125 is configured to select audio signals that were geotagged as having been recorded near (e.g., within a predefined distance) the geographic location associated with the audio signal 112, the acoustic model adaptation module 125 may also adapt the one or more acoustic models 111 for "Boston” using the audio signal 107 that tagged "New York City,” if "New York City” is within the predefined distance of "Boston.”
- another geographic location e.g., "New York City”
- context data may be used to select the subset of the audio signals that the ASR engine 105 uses to adapt the one or more acoustic models 111, or to adjust a weight or effect that a particular audio signal has upon the adaptation of the one or more acoustic models 111.
- the ASR engine 105 may select a subset of the audio signals whose context data indicates that they are longer than or shorter than a predetermined period of time, or whose context data indicates that they satisfy certain quality or recency criteria.
- the ASR engine 105 may select, as the subset, audio signals whose context data indicates that they were recorded using a mobile device that has a similar audio subsystem as the mobile device 102.
- context data which may be used to select the subset of the audio signals may include, in some examples, time information, date information, data referencing a speed or an amount of motion measured by the particular mobile device during recording, other device sensor data, device state data (e.g., Bluetooth headset, speaker phone, or traditional input method), a user identifier (if the user opts to provide one), or information identifying the type or model of mobile device.
- the context data may provide an indication of the conditions surrounding the recording of the audio signal 112.
- context data supplied with the audio signal 112 by the mobile device 102 may indicate that the mobile device 102 is traveling above walking speeds in an area that is associated with a body of water.
- the ASR engine 105 may infer that the audio signal 112 was recorded on a boat, and may select a subset of the audio signals that are associated with an "on a boat" location type, to better recognize an accent, dialect, or speech pattern that is common to an "on a boat” location type, such as an accent, dialect, or speech pattern used by fishermen or sailors.
- context data supplied with the audio signal 112 by the mobile device 102 may indicate that the mobile device 102 is in a rural area. Based on this context data, the ASR engine 105 may infer that that the accuracy of the speech recognition would not be improved if the subset included audio signals that were recorded in urban areas. Accordingly, the context data may be used by the acoustic model adaptation module 125 to select audio signals that are to be used to adapt the one or more acoustic models 111, or to select the appropriate acoustic models 111 to use to recognize a particular utterance. In some implementations, the acoustic model adaptation module 125 may select a weighted combination of the audio signals stored by the ASR engine 105 based upon the proximity of the geographic locations associated with the audio signals to the geographic location associated with the audio signal 112.
- the acoustic model adaptation module 125 may also adapt the one or more acoustic models 111 using audio included in the audio signal 112 itself. For instance, the acoustic model adaptation module 125 may determine the quality of the audio signals stored by the ASR engine 105 relative to the quality of the audio signal 112, and may choose to adapt the one or more acoustic models 111 using the audio signals stored by the ASR engine 105 only, using the audio signal 112 only, or using any appropriate weighted or unweighted combination thereof.
- the acoustic model adaptation module 125 may determine that the audio signal 112 includes very few utterances, or that other high quality audio signals that include multiple utterances are stored by the ASR engine 105 for that particular geographic location, and may choose to adapt the acoustic model without using (or giving little weight to) the audio signal 112.
- the acoustic model adaptation module 125 selects, as the subset, the audio signals that are associated with the N (e.g., five, twenty, or fifty) closest geographic locations to the geographic location associated with the audio signal 112.
- the geographic location associated with the audio signal 112 describes a point or a place (e.g., coordinates)
- a geometric shape e.g., a circle or square
- the acoustic model adaptation module 125 may select, as the subset, audio signals stored by the ASR engine 105 that are associated with geographic regions that are wholly or partially located within the defined geometric shape.
- the acoustic model adaptation module 125 may select, as the subset, audio signals stored by the ASR engine 105 that are associated with geographic regions that are within a predetermined distance of any point of the area.
- the ASR engine 105 may select audio signals that are associated with a same or a similar location type, even if the physical geographic locations associated with the selected audio signals are not physically near the geographic location associated with the audio signal 112. For instance, because surfers across the world may use a similar accent or dialect, an acoustic model for an audio signal that was recorded on the beach in Florida may be tagged with "on the beach” metadata. In doing so, the acoustic model adaptation module 125 may select, as the subset, audio signals whose associated metadata indicate that they were also recorded on beaches, despite the fact that they may have been recorded on beaches in Australia, Hawaii, or in Iceland.
- the acoustic model adaptation module 125 may select the subset of audio signals based on matching location types, instead of matching actual, physical geographic locations, if the geographic location associated with the audio signal 112 does not match, or does not have a high quality match (i.e., the match does not satisfy a predetermined quality threshold) with any physical geographic location associated with an audio signal stored by the ASR engine 105.
- Other matching processes such as clustering algorithms, may be used to match the audio signal 112 with audio signals stored by the ASR engine 105.
- the acoustic model adaptation module 125 may adapt geo-specific acoustic models that are targeted or specific to other criteria as well, such as geo-specific acoustic models that are further specific to different device types or times of day.
- a targeted, acoustic sub-model may be adapted based upon detecting that a threshold criteria has been satisfied, such as determining that a threshold number of audio signals stored by the ASR engine 105 refer to the same geographic location and share another same or similar context ( e.g., time of day, day of the week, motion characteristics, device type, etc.).
- the one or more acoustic models 111 may be adapted before, during, or after the utterance 113 has been recorded by the mobile device 102.
- multiple audio signals, incoming from a same or similar location as the utterance 113 may be processed in parallel with the processing of the utterance 113, and may be used to adapt the one or more acoustic models 111 in real time or near real time, to better approximate the accent, dialect, or others speech patterns of the people who live in the geographic area which surrounds the mobile device 102 when the utterance 113 is recorded.
- Adaptation of the one or more acoustic models 111 may occur using at least four approaches. For instance, separate acoustic models may be built for each geographic location, geographic region, or locale. According to this approach, adaptation of the acoustic models 111 includes selecting the particular, geo-specific acoustic model that matches the geographic location associated with the audio signal 112, from among multiple acoustic models that have been built by the ASR engine 105 for multiple geographic locations.
- location information is directly incorporated into an acoustic model.
- the two-dimensional, continuous latitude and longitude coordinate vector can be directly stacked into the feature space used by the acoustic model, which already includes acoustic features such as Mel-frequency Cepstral Coefficients ("MFCCs").
- MFCCs Mel-frequency Cepstral Coefficients
- the audio signals that are used to adapt the model are divided into frames (e.g., 25 millisecond frames).
- a cepstral representation of each frame is derived using, for example, ten to forty MFCCs to describe the sounds of each particular frame.
- a data set that includes both the MFCCs associated with the particular frame, and values that refer to a geographic location (e.g., geographic coordinates) is used to represent the frame.
- discretized location information may be incorporated as part of the state information included in the acoustic model.
- the acoustic model maps states to probability distributions over the feature space so that, in addition to current phoneme and some contextual information about the preceding and following phonemes, the state can be augmented to include location information.
- the state may not be known exactly; sometimes, only a probability distribution over the states is known; in this case, a smoothed continuous location distribution or probability density function over the discretized location can be incorporated into the probability distribution over the states. Accordingly, location information is stored by the model at the phoneme level, instead of the acoustic feature level.
- a single acoustic model is used for all locations within a language, however the acoustic model is adapted in a lightweight manner based on the geographic location.
- One such known technique for adapting acoustic models uses a Maximum-Likelihood Linear Regression (“MLLR”), which derives a transformation matrix that is applied to the Gaussian coefficients in the acoustic model space, or to the input features of the acoustic model, to adjust the model to match a set of adaptation utterances.
- MLLR Maximum-Likelihood Linear Regression
- the geographic location of the audio signal 112 may be used to define a geographic region, and all of the training audio signals stored by the ASR engine 105 that are associated with the region can be fed into the MLLR adaptation algorithm, to produce a matrix that may be used to transform the acoustic model to match the accent found in that region.
- a single, universal acoustic model may be generated for a particular region and/or language, such as an acoustic model that represents "United States English.”
- the audio signals that are used to adapt the model e.g., the audio signals 107 to 109 are used to generate linear transformations that transform the universal model to match the accent in a particular sub-region, by matrix multiplying the coefficients of the universal acoustic model by an appropriate linear transformation.
- the generation of the linear transformations and the adaptation of the universal acoustic model through matrix multiplication may occur on-the-fly, for example after the audio signal 112 has been received by the ASR engine 105.
- the speech recognition module 124 of the ASR engine 105 performs speech recognition on the audio signal 112 using the one or more geo-specific acoustic models 111 for the geographic location associated with the audio signal 112.
- the ASR engine 105 may apply an acoustic model that is specific to both the geographic location associated with the audio signal, and to the device type of the mobile device 102.
- the speech recognition module 124 may generate one or more candidate transcriptions 115 that match the utterance encoded in the audio signal 112, and speech recognition confidence values for the candidate transcriptions.
- one or more of the candidate transcriptions 115 generated by the speech recognition module 124 are communicated from the ASR engine 105 to the server 104.
- the candidate transcriptions 115 may be used as candidate query terms that are used by the search engine to execute one or more search queries.
- the ASR engine 105 may rank the candidate transcriptions 115 based at least on their respective speech recognition confidence scores before transmission to the server 104. By transcribing spoken utterances and providing candidate transcriptions to the server 104, the ASR engine 105 may provide a voice search query capability, a dictation capability, or a dialogue system capability to the mobile device 102.
- the server 104 may execute one or more search queries using the candidate query terms, and may generate a file 116 that references search results 126 and 127.
- the file 116 may be a markup language file, such as an extensible Markup Language (XML) or HyperText Markup Language (HTML) file.
- XML extensible Markup Language
- HTML HyperText Markup Language
- the server 104 may include a web search engine used to find references within the Internet, a phone book type search engine used to find businesses or individuals, or another specialized search engine (e.g., a search engine that provides references to entertainment listings such as restaurants and movie theater information, medical and pharmaceutical information, etc.).
- a web search engine used to find references within the Internet
- a phone book type search engine used to find businesses or individuals
- another specialized search engine e.g., a search engine that provides references to entertainment listings such as restaurants and movie theater information, medical and pharmaceutical information, etc.
- the server 104 provides the file 116 that references the search results 126 and 127 to the mobile device 102.
- the mobile device 102 displays the search results 126 and 127 on the user interface 101.
- the user interface 101 includes a search box 129 that displays the candidate query term with the highest speech recognition confidence score ("Park your car"), an alternate query term suggestion region 130 that displays another of the candidate query term that may have been intended by the utterance 113 ("Parker Cole” and "Parka Card”), a search result 126 that includes a link to a resource for "Boston Parking,” and a search result 127 that includes a link to a resource for "Cambridge Car Park.”
- FIG. 2 is a flowchart of an example process 200.
- the process 200 includes receiving an audio signal that corresponds to an utterance recorded by a mobile device, determining a geographic location associated with the mobile device, adapting one or more acoustic models for the geographic location, and performing speech recognition on the audio signal using the one or more acoustic models model that are adapted for the geographic location.
- an audio signal that corresponds to an utterance recorded by a mobile device is received (202).
- the utterance may include a voice search query, or may be an input to a dictation or dialog application or system.
- the utterance may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- a geographic location associated with the mobile device is determined (204). For example, data referencing the particular geographic location may be received from the mobile device, or a past geographic location or a default geographic location associated with the mobile device may be identified.
- One or more acoustic models are adapted for the geographic location (206).
- a subset of geotagged audio signals used for adapting the acoustic model may be selected by determining, for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal, and selecting those geotagged audio signals which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location.
- Adapting the one or more acoustic models may include selecting the one or more acoustic models generated for the geographic location associated with the mobile device, from among multiple acoustic models that have been generated for multiple geographic locations, or incorporating data that references the geographic location (e.g., geographic coordinates) into a feature space used by the one or more acoustic models, in accordance with the invention.
- adapting the one or more acoustic models may include incorporating data that references the geographic location into state information included in the acoustic model, or deriving a transformation matrix associated with the geographic location; and applying the transformation matrix to a universal acoustic model.
- the subset of geotagged audio signals may be selected by identifying the geotagged audio signals associated with the particular geographic location, and/or by identifying the geotagged audio signals that are acoustically similar to the utterance.
- the subset of geotagged audio signals may be selected based both on the particular geographic location and on context data associated with the utterance.
- Generating the acoustic model may include training a GMM using the subset of geotagged audio signals as a training set.
- Speech recognition is performed on the audio signal (208). Performing the speech recognition may include generating one or more candidate transcriptions of the utterance.
- a search query may be executed using the one or more candidate transcriptions, or one or more of the candidate transcriptions may be provided as an output of a digital dictation application.
- one or more of the candidate transcriptions may be provided as an input to a dialog system, to allow a computer system to converse with the user of the particular mobile device.
- FIG. 3 is a flowchart of another example process 300.
- the process 300 includes receiving geotagged audio signals and generating multiple acoustic models based, in part, upon particular geographic locations associated with each of the geotagged audio signals.
- One or more of these acoustic models may be selected when performing speech recognition upon an utterance based, in part, upon a geographic location associated with the utterance.
- geotagged audio signal corresponding to audio is received (302).
- the geotagged audio signal may be recorded by a mobile device in a particular geographic location.
- the received geotagged audio signal may be processed to exclude portions of the audio that do not include the voice of the user of the mobile device.
- Multiple geotagged audio signals recorded in one or more geographic locations may be received and stored.
- context data associated with the geotagged audio signal is received (304).
- the geotagged audio signal may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- Each acoustic model may be adapted for a particular geographic location or, optionally, a location type, using a subset of geotagged audio signals.
- the subset of geotagged audio signals may be selected by determining, for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal and selecting those geotagged audio signals which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location.
- the subset of geotagged audio signals may be selected by identifying the geotagged audio signals associated with the particular geographic location.
- the subset of geotagged audio signals may be selected based both on the particular geographic location and on context data associated with the geotagged audio signals.
- Generating the acoustic model may include training a Gaussian Mixture Model (GMM) using the subset of geotagged audio signals.
- GMM Gaussian Mixture Model
- the utterance may include a voice search query.
- the utterance may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- a geographic location is determined (310). For example, data referencing the particular geographic location may be received from a GPS module of the mobile device.
- An acoustic model is selected (312).
- the acoustic model may be selected from among multiple acoustic models adapted for multiple geographic locations.
- Context data may optionally contribute to selection of a particular acoustic model among multiple acoustic models for the particular geographic location.
- Speech recognition is performed on the utterance using the selected acoustic model (314). Performing the speech recognition may include generating one or more candidate transcriptions of the utterance. A search query may be executed using the one or more candidate transcriptions.
- FIG. 4 shows a swim lane diagram of an example of a process 400 for enhancing speech recognition accuracy using geotagged audio.
- the process 400 may be implemented by a mobile device 402, an ASR engine 404, and a search engine 406.
- the mobile device 402 may provide audio signals, such as audio signals or audio signals that correspond to an utterance, to the ASR engine 404. Although only one mobile device 402 is illustrated, the mobile device 402 may represent a large quantity of mobile devices 402 contributing audio signals and voice queries to the process 400.
- the ASR engine 404 may adapt acoustic models based upon the audio signals, and may apply one or more acoustic models to an incoming voice search query when performing speech recognition.
- the ASR engine 404 may provide transcriptions of utterances within a voice search query to the search engine 406 to complete the voice search query request.
- the process 400 begins with the mobile device 402 providing 408 a geotagged audio signal to the ASR engine 404.
- the audio signal may include audio of a voice of the mobile device 402, along with an indication regarding the location at which the audio was recorded.
- the geotagged audio signal may include context data, for example in the form of metadata.
- the ASR engine 404 may store the geotagged audio signal in an audio data store.
- the mobile device 402 provides 410 an utterance to the ASR engine 404.
- the utterance may include a voice search query.
- the recording of the utterance may optionally include a sample of audio, for example recorded briefly before or after the recording of the utterance.
- the mobile device 402 provides 412 a geographic location to the ASR engine 404.
- the mobile device may provide navigational coordinates detected using a GPS module, a most recent (but not necessarily concurrent with recording) GPS reading, a default location, a location derived from the utterance previously provided, or a location estimated through dead reckoning or triangulation of transmission towers.
- the mobile device 402 may optionally provide context data, such as sensor data, device model identification, or device settings, to the ASR engine 404.
- the ASR engine 404 adapts 414 an acoustic model.
- the acoustic model may be adapted, in part, by training a GMM.
- the acoustic model may be adapted based upon the geographic location provided by the mobile device 402. For example, geotagged audio signals submitted from a location at or near the location of the mobile device 402 may contribute to an acoustic model.
- context data provided by the mobile device 402 may be used to filter geotagged audio signals to select those most appropriate to the conditions in which the utterances were recorded. For example, the geotagged audio signals near the geographic location provided by the mobile device 402 may be filtered by a day of the week or a time of day. If a sample of audio was included with the utterance provided by the mobile device 402, the audio sample may optionally be included in the acoustic model.
- the ASR engine 404 performs speech recognition 416 upon the provided utterance.
- the utterance provided by the mobile device 402 may be transcribed into one or more sets of query terms using the acoustic model adapted by the ASR engine 404.
- the ASR engine 404 forwards 418 the generated transcription(s) to the search engine 406. If the ASR engine 404 generated more than one transcription, the transcriptions may optionally be ranked in order of confidence.
- the ASR engine 404 may optionally provide context data to the search engine 406, such as the geographic location, which the search engine 406 may use to filter or rank search results.
- the search engine 406 performs 420 a search operation using the transcription(s).
- the search engine 406 may locate one or more URIs related to the transcription term(s).
- the search engine 406 provides 422 search query results to the mobile device 402.
- the search engine 406 may forward HTML code which generates a visual listing of the URI(s) located.
- Embodiments and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable medium may be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files ( e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- FPGA field programmable gate array
- ASIC application specific integrated circuit
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- the processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation, or any combination of one or more such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- HTML file In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.
Description
- This application claims priority to
U.S. Application Serial No. 12/787,568, filed on May 26, 2010 - This specification relates to speech recognition.
- A user of a mobile device may enter text by, for example, typing on a keyboard or speaking into a microphone. In the context of voice input, an Automated Search Recognition ("ASR") engine may have difficulty accurately recognizing spoken words when the sounds associated with a particular language vary based on an accent of the user. As spoken by a New Yorker or a Bostonian, for example, a typical ASR engine may recognize the word "park," as the words "pork" or "pack," respectively.
- In Bocchieri et al: "Use of geographical meta-data in ASR language and acoustic models", IEEE International Conference on Acoustics, Speech and Signal Processing, 14 March 2010, pages 5118-5121, adaptation of local acoustic models based on geographically labelled audio data is described
- The invention is defined by the appended claims.
- In general, aspects of the subject matter described in this specification may be embodied in methods for automatically training, selecting, generating, or otherwise adapting, by an ASR engine, one or more acoustic models that are geographic-location specific (or "geo-specific") to one or more geographic areas. The acoustic models are applied to audio signals (or "samples," or "waveforms") that are "geotagged" with location information, to perform speech recognition by comparing the audio signals to statistical representations of the sounds that make up each word of a particular language. The one or more acoustic models may include a single acoustic model that is geographic-location specific to a single geographic area or to multiple geographic areas.
- As used by this specification, "geotagged" audio signals refer to signals that have been associated, or "tagged," with location metadata (e.g., geographic location metadata, or relative location metadata) or geospatial metadata. Among other things, the location metadata may include navigational coordinates, such as latitude and longitude, altitude information, bearing or heading information, name or address information associated with the location, relative position or direction information, or information that references a type of a location.
- The methods for adapting the acoustic models include receiving geotagged audio signals recorded by multiple mobile devices in multiple geographic locations, and adapting one or more acoustic models using at least a portion of the geotagged audio signals. Upon receiving an utterance recorded by a mobile device within or near one of the geographic locations, the ASR engine may perform speech recognition on the utterance using the adapted acoustic model or models. Notably, the acoustic models may be adapted before, during, or after the utterance is received.
- The utterance may correspond to any type of voice input, such as an input to a voice search query system, a dictation system, or a dialog system. In the context of a voice search query system, a "search query" includes one or more query terms that a user submits to a search engine when the user requests the search engine to execute a search query, where a "term" or a "query term" includes one or more whole or partial words, characters, or strings of characters. Among other things, a "result" (or a "search result") of the search query includes a Uniform Resource Identifier (URI) that references a resource that the search engine determines to be responsive to the search query. The search result may include other things, such as a title, preview image, user rating, map or directions, description of the corresponding resource, or a snippet of text that has been automatically or manually extracted from, or otherwise associated with, the corresponding resource.
- In general, further aspects of the subject matter described in this specification may be embodied in methods that include the actions of receiving an audio signal that corresponds to an utterance recorded by a mobile device, determining a geographic location associated with the mobile device, adapting one or more acoustic models for the geographic location, and performing speech recognition on the audio signal using the one or more acoustic models model that are adapted for the geographic location.
- Other embodiments of these aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
- These and other embodiments may each optionally include one or more of the following features. In various examples, adapting one or more acoustic models further include adapting one or more acoustic models before receiving the audio signal that corresponds to the utterance; adapting one or more acoustic models further include adapting one or more acoustic models after receiving the audio signal that corresponds to the utterance; the actions further include receiving geotagged audio signals that correspond to audio recorded by multiple mobile devices in multiple geographic locations; and adapting one or more acoustic models for the geographic location further includes adapting one or more acoustic models for the geographic location using a subset of the geotagged audio signals; the actions further include determining, for each of the geotagged audio signals, a distance between the geographic location associated with the mobile device and a geographic location associated the geotagged audio signal, and selecting, as the subset of the geotagged audio signals, the geotagged audio signals that are associated with geographic locations which are within a predetermined distance of the geographic location associated with the mobile device, or that are associated with geographic locations which are among an N closest geographic locations to the geographic location associated with the mobile device; the actions further include selecting, as the subset of the geotagged audio signals, the geotagged audio signals that are associated with the geographic location that is also associated with the mobile device; the actions further include selecting the subset of the geotagged audio signals based on the geographic location associated with the mobile device, and based on context data associated with the utterance; the context data includes data that references a time or a date when the utterance was recorded by the mobile device, data that references a speed or an amount of motion measured by the mobile device when the utterance was recorded, data that references settings of the mobile device, or data that references a type of the mobile device; adapting the acoustic model include training a Gaussian Mixture Model (GMM) using the subset of the geotagged audio signals as a training set; the utterance represents a voice search query, or an input to a digital dictation application or a dialog system; determining the geographic location further includes receiving data referencing the geographic location from the mobile device; determining the geographic location further includes determining a past geographic location or a default geographic location associated with the mobile device; the actions further include generating one or more candidate transcriptions of the utterance, and executing a search query using the one or more candidate transcriptions; adapting one or more acoustic models for the geographic location further includes selecting, from among multiple acoustic models that have been generated for multiple geographic locations, the one or more acoustic models generated for the geographic location associated with the mobile device; adapting one or more acoustic models for the geographic location further includes incorporating data that references the geographic location into a feature space used by a single acoustic model; the values incorporated into the feature space are Mel-frequency Cepstral Coefficients and geographic coordinates; adapting one or more acoustic models for the geographic location further includes incorporating data that references the geographic location into state information included in a single acoustic model; and/or adapting one or more acoustic models for the geographic location further includes deriving a transformation matrix associated with the geographic location, and applying the transformation matrix to a single, universal acoustic model.
- Particular embodiments of the subject matter described in this specification may be implemented to realize one or more of the following advantages. Speech recognition accuracy may be improved. Acoustic models may be adapted using utterances that accurately reflect the differences in accents, dialects, or speech patterns that exist within a given language, and that may occur across different geographic regions. Speech recognition may be performed at the server side, instead of on the client device, to allow for enhanced process optimization and to increase computational efficiency.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims, which define the invention.
-
-
FIG. 1 is a diagram of an example system that uses geotagged audio to enhance speech recognition accuracy. -
FIGS. 2 and3 are flowcharts of example processes. -
FIG. 4 is a swim lane diagram of an example process. - Like reference symbols in the various drawings indicate like elements.
-
FIG. 1 is a diagram of an example system 100 that uses geotagged audio to enhance speech recognition accuracy.FIG. 1 also illustrates a flow of data within the system 100 during states (a) to (i), as well as auser interface 101 that is displayed on amobile device 102 of the system 100 during state (i). Briefly, the system 100 adapts one or more acoustic models that are geo-specific to one or more geographic areas. The acoustic models are applied to audio signals that are geotagged with location information, to perform speech recognition by comparing the audio signals to statistical representations of the sounds that make up each word of a particular language. - In more detail, the system 100 includes the
mobile device 102, which is in communication with aserver 104 and anASR engine 105 over one ormore networks 106. Theserver 104 may be a search engine, a dictation engine, a dialogue system, or any other engine or system that uses transcribed speech, or that invokes a software application that uses transcribed speech, to perform some action. Thenetworks 106 may include a wireless cellular network, a wireless local area network (WLAN) or Wi-Fi network, a Third Generation (3G) or Fourth Generation (4G) mobile telecommunications network, a private network such as an intranet, a public network such as the Internet, or any appropriate combination thereof. The states (a) through (i) depict a flow of data that occurs when an example process is performed by the system 100. The states (a) to (i) may be time-sequenced states, or they may occur in a sequence that is different than the illustrated sequence. - According the example process illustrated in
FIG. 1 , the ASRengine 105 receivesgeotagged audio signals 107 to 109 from various devices (e.g., themobile device 102 or other mobile or non-mobile devices), and adapts one or more geo-specificacoustic models 111 for one or more multiple geographic locations using thegeotagged audio signals 107 to 109. The geo-specificacoustic models 111 may include one, single acoustic model that is adapted to be geo-specific to one geographic location or more than one geographic location, or the geo-specificacoustic models 111 may include two or more acoustic models that are collectively adapted to be geo-specific to one geographic location, or that are each adapted to be geo-specific to a different geographic location. - When an
audio signal 112 that corresponds to anutterance 113 recorded by themobile device 102 is received, one or more geographic locations associated with the mobile device 102 (or the user 114 ("Boston Bob") of the mobile device 102) are determined. The ASRengine 105 transcribes theutterance 113 using the geo-specificacoustic models 111 that match, or that theASR engine 105 determines to be suitable for, the geographic locations associated with the mobile device 102 (or theuser 114 of the mobile device 102). One ormore candidate transcriptions 115 are communicated from theASR engine 105 to theserver 104. When theserver 104 is a search engine, theserver 104 executes one or more search queries using thecandidate transcriptions 115, generatessearch results 116, and communicates thesearch results 116 to themobile device 102 for display. - In more detail, during state (a),
geotagged audio signals 107 to 109 are communicated to theASR engine 105 over thenetworks 106. In general, one or more of thegeotagged audio signals 107 to 109 include the voices of different users. Fewer or more geotagged audio signals may be communicated to theASR engine 105 during state (a). - Although several of the voices encoded in the
geotagged audio signals 107 to 109 may share a common language, different voices speaking a common language may have different accents that correlate to different geographic regions (i.e., that are "geo-correlated" accents). Accordingly, the geographic locations that are associated with theaudio signals 107 to 109 may be used to cluster the audio signals by geographic region (and thus by accent, dialect, or speech pattern), and to adapt the one or moreacoustic models 111 to better recognize speech that exhibit particular, geo-correlated accents, dialects, or speech patterns. In addition to voices, thegeotagged audio signals 107 to 109 may also include ambient sounds or environmental noises that occur (naturally or otherwise) at a particular location. - During state (b), the ASR
engine 105 receives thegeotagged audio signals 107 to 109, and stores thegeotagged audio signals 107 to 109 (or portions thereof) in a collection of audio signals (e.g., on a computer-readable storage medium). As described below, the collection of audio signals stored by the ASRengine 105 is used for training, building, generating, or otherwise adapting one or more geo-specificacoustic models 111 that are used to perform speech recognition on geo-tagged audio signals and utterances. - In
FIG. 1 , the ASRengine 105 receives anaudio signal 107 that has been tagged withmetadata 117 that references the location "New York City." Further, the ASRengine 105 receives anaudio signal 108 that has been tagged withmetadata 118 that references the location "Boston," andmetadata 119 that references the "city" geographic location type (i.e., because "Boston" is a "city"). Additionally, the ASRengine 105 receives anaudio signal 109 that has been tagged withmetadata 120 that references the location "New England," and metadata 121 that references the location "Boston" ("Boston" is a city in "New England"). The geo-tagged locations associated with the respective audio signals my refer to a location of a mobile device, a user, a location referenced by the utterance, a default location, theASR engine 105, thenetworks 106 or a portion of thenetworks 106, or some other location - The
metadata 117 to 121 (or some portion thereof) may, as illustrated, be associated with theaudio signals 107 to 109 by the devices that communicate themetadata 117 to 121 to theASR engine 105. Alternatively, themetadata 117 to 121 (or some portion thereof) may be associated with theaudio signals 107 to 109 by theASR engine 105, thesearch engine 104, or by another server, based upon inferring a location of a mobile device 102 (or of theuser 114 of the mobile device 102) after receiving untagged audio signals. - The audio signals 107 to 109 may each include a two-second (or more) snippet of relatively high quality audio, such as sixteen kilohertz lossless audio. The metadata may reference the location of a device (or of a user a device) when audio was recorded, captured, generated or otherwise obtained, or the metadata may reference a location of the a device (or of the user of the device) at a time before or after the audio was recorded, captured, generated, or otherwise obtained. The audio signals 107 to 109 may be manually uploaded to the
ASR engine 105 or, for users who opt to participate, theaudio signals 107 to 109 may be automatically obtained and communicated to theASR engine 105 without requiring an explicit, user actuation before each audio signal is communicated to theASR engine 105. - The
metadata 117 to 121 may describe locations in any number of different formats or levels of detail or granularity. For example, themetadata 117 to 121 may include a two dimensional coordinates (e.g., latitude and longitude), an address, or information that identifies a geographic region. When an audio signal is recorded in a moving vehicle, themetadata 117 to 121 may describe a path of the vehicle (e.g., including a start point and an end point, and motion data). Additionally, themetadata 117 to 121 may describe locations in terms of location type (e.g., "moving vehicle," "on a beach," "in a restaurant," "in tall building," "South Asia," "rural area," "someplace with construction noise," "amusement park," "on a boat," "indoors," "underground," "on a street," "forest"). - Furthermore, the
metadata 117 to 121 may describe locations in terms of a bounded area (e.g., expressed as a set of coordinates that define the bounded area), or may use a region identifier, such as a state name or identifier, city name, idiomatic name (e.g., "Central Park," "Chinatown," "TriBeCa"), a country name, or the identifier of arbitrarily defined region (e.g., "cell/region ABC123"). A single audio signal may be associated with metadata that describes one location or location type, or more than one location and/or location type. - The
ASR engine 105 or themobile device 102 may process themetadata 117 to 121 to adjust the level of detail of the location information (e.g., to determine a state associated with a particular set of coordinates), or the location information may be discretized (e.g., by selecting a specific point along the path, or a region associated with the path). The level of detail of the metadata may also be adjusted by specifying or adding location type metadata, for example by adding an "on the beach" tag to an audio signal whose associated geographic coordinates are associated with a beach location, or by adding a "someplace with lots of people" tag to an audio signal that includes the sounds of multiple people talking in the background. Where a particular audio signal is associated with metadata referencing two or more locations, theASR engine 105 may filter the audio signal by removing metadata that references one or more of the locations. - The geographic locations referenced by the metadata can also be converted into discretized features to reduce the number of possible distinct locations. This could be done, for example, by reducing the resolution of latitude and longitude coordinates (e.g., from 0.001 degrees to 1 degree, or to 5 degrees), or by converting the latitude and longitude coordinates into a name of a geographic location (e.g., by using regions defined by the boundaries between countries, states, cities or provinces).
- Because the
ASR engine 105 adapts the one or moreacoustic models 111 to enhance the recognition of speech that includes different accents, the audio signals that are used to adapt the one or moreacoustic models 111 should include samples of different users' voices, accents, and dialects in different geographic locations. In this regard, theASR engine 105 may use a voice activity detector to verify that the collection of audio signals stored by theASR engine 105 includes audio signals in which voices are present, and to filter out or otherwise identify or exclude audio signals (or portions of the audio signals) that include ambient noise or environmental sounds only. For example theASR engine 105 may remove portions of the audio signals that correspond to background noise that is occurs before or after a user speaks, or that occurs during pauses between words. The collection of the audio signals stored by theASR engine 105 may include tens, hundreds, thousands, millions, or hundreds of millions of audio signals. - The decision by the
ASR engine 105 to store or not store a particular audio signal (or portion thereof) may be based on determining that the user's voice is or is not encoded in the audio signal, respectively. Alternatively, storing an audio signal by theASR engine 105 may include identifying a portion of the audio signal that includes the user's voice, altering the audio signal by removing the portion that does not include the user's voice or by associating metadata which references the portion that includes the user's voice, and storing the altered audio signal. Ambient noise or environmental sound portions of the audio signals may be stored by theASR engine 105 for other purposes, for example to build geo-specific noise models. - Other context data or metadata associated with the
audio signals 107 to 109 may also be stored by theASR engine 105. For example, the audio signals stored by theASR engine 105 can, in some implementations, include other metadata tags, such as tags that indicate whether background voices (e.g., cafeteria chatter) are present within the audio signal, tags that identify the date on which a particular audio signal was obtained (e.g., used to determine a sample age), tags that identify an accent of theuser 114 of themobile device 102, tags that identify a locale set by theuser 114 of the mobile device 102 (e.g., tags that identify that theuser 114 prefers British English or American English), or tags that identify whether a particular audio signal deviates in some way from other audio signals of the collection that were obtained in the same or similar location. For example, the tags may identify that a user that has no accent, or has a strong accent (e.g., a South African accent), and is using a mobile device in a geographic area that is associated with a different strong accent (e.g., an Australian accent), to avoid adapting an acoustic model using audio signals that do not accurately reflect an accent associated with a particular geographic area.. - The
ASR engine 105 may optionally filter audio signals to exclude particular audio signals that satisfy or that do not satisfy other criteria. For example, theASR engine 105 may decide to not store audio signals that are older than a certain age, or that include background chatter that may uniquely identify an individual or that may otherwise be proprietary or private in nature. In an additional example, data referencing whether the audio signals stored by theASR engine 105 were manually or automatically uploaded may be tagged in metadata associated with the audio signals, and the one or moreacoustic models 111 may be adapted using only those audio signals that were automatically uploaded, or only those that were manually uploaded, or different weightings may be assigned to each category of upload during the adaptation of the acoustic models. - Although an explicit tag may be applied to the audio signals stored by the
ASR engine 105 to reference a particular geographic location, in other implementations, such as where the association between an audio signal and a geographic location may be derived, an explicit tag is not required or is not used. For example, a geographic location may be implicitly associated with an audio signal by processing search logs (e.g., stored with the server 104) to infer a geographic location for a particular audio signal. Accordingly, 'receipt' of a geo-tagged audio signals by theASR engine 105 may include obtaining an audio signal that does is not expressly tagged with a geographic location, and deriving and associating one or more geo-tags for the audio signal. - During state (c), an
audio signal 112 is communicated from themobile device 102 to theASR engine 105 over thenetworks 106. Theaudio signal 112 includes an utterance 113 ("Pahk yah kah," a phonetic transcription of the term "Park your car," as might be spoken by a native Bostonian such as "Boston Bob") recorded by the mobile device 102 (e.g., when the user implicitly or explicitly initiates a voice search query). Theaudio signal 112 includesmetadata 123 that references the geographic location "Boston." In addition to including theutterance 113, theaudio signal 112 may also include a snippet of environmental audio, such as a two second snippet of audio that was recorded before or after theutterance 113 was spoken. While theutterance 113 is described an illustrated inFIG. 1 as a voice query, in other example implementations the utterance may be an voice input to dictation system or to a dialog system. - The geographic location ("Boston") associated with the
audio signal 112 may be defined using a same or different level of detail as the geographic locations associated with the audio signals stored by theASR engine 105. For example, the geographic locations associated with the audio signals stored by theASR engine 105 may be expressed as geographic regions, while the geographic location associated with theaudio signal 112 may be expressed as geographic coordinates. Where the level of detail is different, theASR engine 105 may process thegeographic metadata 123 or themetadata 117 to 121 to align the respective levels of detail, so that a subset selection process may be performed more easily. - The
metadata 123 may be associated with theaudio signal 112 by the mobile device 102 (or theuser 114 of the mobile device 102) based on a current geographic location when theutterance 113 is recorded, and may be communicated with theaudio signal 112 from themobile device 102 to theASR engine 105. Alternatively, the metadata may be associated with theaudio signal 112 by theASR engine 105, based on a geographic location that theASR engine 105 infers for the mobile device 102 (or theuser 114 of the mobile device 102). - The
ASR engine 105 or themobile device 102 may infer the geographic location using the user's calendar schedule, user preferences (e.g., as stored in a user account of theASR engine 105 or theserver 104, or as communicated from the mobile device 102), a default location, a past location (e.g., the most recent location calculated by a GPS module of the mobile device 102), information explicitly provided by the user when submitting the voice search query, from theutterance 113 themselves, triangulation (e.g., WiFi or cell tower triangulation), a GPS module in themobile device 102, or dead reckoning. Themetadata 123 may include accuracy information that specifies an accuracy of the geographic location determination, signifying a likelihood that the mobile device 102 (or theuser 114 of the mobile device 102) was actually in the particular geographic location specified by themetadata 123 at the time when theutterance 113 was recorded. - In additional examples, the
ASR engine 105 or themobile device 102 may infer the geographic location using the user's average location over all his utterances, the user's "home location", (e.g. where the user currently lives, or where he grew up and his accent came from, as specified explicitly by the user or inferred from the accent), a 'smoothed' location that represents the location of the user over some recent period of time, a combination of the current location and the user's home location (e.g., a four-dimensional signal derived from the two, two-dimensional latitude and longitude), or the current location, as a continuous two-dimensional latitude and longitude signal. - Other metadata may also be included with the
audio signal 112. For example, metadata included with the audio signals may include a location or locale associated with the respectivemobile device 102. For example, the locale may describe, among other selectable parameters, a region in which themobile device 102 is registered, or the language or dialect of theuser 114 of themobile device 102. Thespeech recognition module 124 may use this information to select, train, generate or otherwise adapt noise, speech, acoustic, popularity, or other models that match the context of themobile device 102. - During state (d), the
ASR engine 105 selects a subset of the audio signals that have been received by theASR engine 105, and uses an acoustic model adaptation module 125 to train, generate, or otherwise adapt one or more acoustic models 111 (e.g., Gaussian Mixture Models (GMMs)) using the subset of the audio signals. For example the subset of the audio signals that are selected by theASR engine 105 may be used as a training set for the one or moreacoustic models 111. - The subset may include all, or fewer than all of the audio signals stored by the
ASR engine 105. Machine learning techniques such as k-means may be used to select the subset. This selection may occur by comparing acoustic information from theaudio signals 107 to 109 with acoustic information from theaudio signal 112, to result in a subset that more accurately reflects actual geographical boundaries between different accents. - In general, the one or more
acoustic models 111, along with noise models, language models, and/or other models, are applied to theaudio signal 112 to translate or transcribe the spokenutterance 113 into one or more textual,candidate transcriptions 115, and to generate speech recognition confidence scores to thecandidate transcriptions 115. In particular, the one or moreacoustic models 111 include statistical representations of the sounds that make up each word of a particular language, and the noise models are used for noise suppression or noise compensation. Both models enhance the intelligibility of the spokenutterance 113 to theASR engine 105. - In more detail, the acoustic model adaptation module 125 may adapt an acoustic model for the geographic location ("Boston") associated with the
audio signal 112 using the audio signals 108 and 109, because theaudio signals audio signal 112 may itself be used to adapt the one or moreacoustic models 111, in addition to or instead of using the audio signals 108 and 109. In adapting an acoustic model for a particular geographic location, the acoustic model adaptation module 125 adapts an acoustic model based on criteria that may correlate to a particular accent, dialect, or pattern of speech. - In another example, the acoustic model adaptation module 125 may adapt an acoustic model for another geographic location (e.g., "New York City"), using the
audio signal 107 that was geotagged as having been recorded at or near that other geographic location, or at a same or similar type of location. If the acoustic model adaptation module 125 is configured to select audio signals that were geotagged as having been recorded near (e.g., within a predefined distance) the geographic location associated with theaudio signal 112, the acoustic model adaptation module 125 may also adapt the one or moreacoustic models 111 for "Boston" using theaudio signal 107 that tagged "New York City," if "New York City" is within the predefined distance of "Boston." - In addition to using location criteria, other context data may be used to select the subset of the audio signals that the
ASR engine 105 uses to adapt the one or moreacoustic models 111, or to adjust a weight or effect that a particular audio signal has upon the adaptation of the one or moreacoustic models 111. For example, theASR engine 105 may select a subset of the audio signals whose context data indicates that they are longer than or shorter than a predetermined period of time, or whose context data indicates that they satisfy certain quality or recency criteria. Furthermore, theASR engine 105 may select, as the subset, audio signals whose context data indicates that they were recorded using a mobile device that has a similar audio subsystem as themobile device 102. - Other context data which may be used to select the subset of the audio signals may include, in some examples, time information, date information, data referencing a speed or an amount of motion measured by the particular mobile device during recording, other device sensor data, device state data (e.g., Bluetooth headset, speaker phone, or traditional input method), a user identifier (if the user opts to provide one), or information identifying the type or model of mobile device. The context data, for example, may provide an indication of the conditions surrounding the recording of the
audio signal 112. - In one example, context data supplied with the
audio signal 112 by themobile device 102 may indicate that themobile device 102 is traveling above walking speeds in an area that is associated with a body of water. Using this context data, theASR engine 105 may infer that theaudio signal 112 was recorded on a boat, and may select a subset of the audio signals that are associated with an "on a boat" location type, to better recognize an accent, dialect, or speech pattern that is common to an "on a boat" location type, such as an accent, dialect, or speech pattern used by fishermen or sailors. - In another example, context data supplied with the
audio signal 112 by themobile device 102 may indicate that themobile device 102 is in a rural area. Based on this context data, theASR engine 105 may infer that that the accuracy of the speech recognition would not be improved if the subset included audio signals that were recorded in urban areas. Accordingly, the context data may be used by the acoustic model adaptation module 125 to select audio signals that are to be used to adapt the one or moreacoustic models 111, or to select the appropriateacoustic models 111 to use to recognize a particular utterance. In some implementations, the acoustic model adaptation module 125 may select a weighted combination of the audio signals stored by theASR engine 105 based upon the proximity of the geographic locations associated with the audio signals to the geographic location associated with theaudio signal 112. - The acoustic model adaptation module 125 may also adapt the one or more
acoustic models 111 using audio included in theaudio signal 112 itself. For instance, the acoustic model adaptation module 125 may determine the quality of the audio signals stored by theASR engine 105 relative to the quality of theaudio signal 112, and may choose to adapt the one or moreacoustic models 111 using the audio signals stored by theASR engine 105 only, using theaudio signal 112 only, or using any appropriate weighted or unweighted combination thereof. For instance, the acoustic model adaptation module 125 may determine that theaudio signal 112 includes very few utterances, or that other high quality audio signals that include multiple utterances are stored by theASR engine 105 for that particular geographic location, and may choose to adapt the acoustic model without using (or giving little weight to) theaudio signal 112. - In some implementations, the acoustic model adaptation module 125 selects, as the subset, the audio signals that are associated with the N (e.g., five, twenty, or fifty) closest geographic locations to the geographic location associated with the
audio signal 112. When the geographic location associated with theaudio signal 112 describes a point or a place (e.g., coordinates), a geometric shape (e.g., a circle or square) may be defined relative to that that geographic location, and the acoustic model adaptation module 125 may select, as the subset, audio signals stored by theASR engine 105 that are associated with geographic regions that are wholly or partially located within the defined geometric shape. When the geographic location associated with theaudio signal 112 describes an area, the acoustic model adaptation module 125 may select, as the subset, audio signals stored by theASR engine 105 that are associated with geographic regions that are within a predetermined distance of any point of the area. - If the geographic location associated with the
audio signal 112 has been defined in terms of a location type (i.e., "on the beach," "city"), theASR engine 105 may select audio signals that are associated with a same or a similar location type, even if the physical geographic locations associated with the selected audio signals are not physically near the geographic location associated with theaudio signal 112. For instance, because surfers across the world may use a similar accent or dialect, an acoustic model for an audio signal that was recorded on the beach in Florida may be tagged with "on the beach" metadata. In doing so, the acoustic model adaptation module 125 may select, as the subset, audio signals whose associated metadata indicate that they were also recorded on beaches, despite the fact that they may have been recorded on beaches in Australia, Hawaii, or in Iceland. - The acoustic model adaptation module 125 may select the subset of audio signals based on matching location types, instead of matching actual, physical geographic locations, if the geographic location associated with the
audio signal 112 does not match, or does not have a high quality match (i.e., the match does not satisfy a predetermined quality threshold) with any physical geographic location associated with an audio signal stored by theASR engine 105. Other matching processes, such as clustering algorithms, may be used to match theaudio signal 112 with audio signals stored by theASR engine 105. - In addition to generating 'generic,' geo-specific
acoustic models 111, the acoustic model adaptation module 125 may adapt geo-specific acoustic models that are targeted or specific to other criteria as well, such as geo-specific acoustic models that are further specific to different device types or times of day. A targeted, acoustic sub-model may be adapted based upon detecting that a threshold criteria has been satisfied, such as determining that a threshold number of audio signals stored by theASR engine 105 refer to the same geographic location and share another same or similar context (e.g., time of day, day of the week, motion characteristics, device type, etc.). - The one or more
acoustic models 111 may be adapted before, during, or after theutterance 113 has been recorded by themobile device 102. For example, multiple audio signals, incoming from a same or similar location as theutterance 113, may be processed in parallel with the processing of theutterance 113, and may be used to adapt the one or moreacoustic models 111 in real time or near real time, to better approximate the accent, dialect, or others speech patterns of the people who live in the geographic area which surrounds themobile device 102 when theutterance 113 is recorded. - Adaptation of the one or more
acoustic models 111 may occur using at least four approaches. For instance, separate acoustic models may be built for each geographic location, geographic region, or locale. According to this approach, adaptation of theacoustic models 111 includes selecting the particular, geo-specific acoustic model that matches the geographic location associated with theaudio signal 112, from among multiple acoustic models that have been built by theASR engine 105 for multiple geographic locations. - According to a second approach, in accordance with the invention, location information is directly incorporated into an acoustic model. For example, the two-dimensional, continuous latitude and longitude coordinate vector can be directly stacked into the feature space used by the acoustic model, which already includes acoustic features such as Mel-frequency Cepstral Coefficients ("MFCCs"). According to this approach, fewer acoustic models need to be built, since the location information is considered as part of the regular training process for a single acoustic model.
- In more detail, the audio signals that are used to adapt the model (e.g., the
audio signals 107 to 109) are divided into frames (e.g., 25 millisecond frames). A cepstral representation of each frame is derived using, for example, ten to forty MFCCs to describe the sounds of each particular frame. When training a model using a particular frame, a data set that includes both the MFCCs associated with the particular frame, and values that refer to a geographic location (e.g., geographic coordinates) is used to represent the frame. - According to a third approach, discretized location information may be incorporated as part of the state information included in the acoustic model. Specifically, the acoustic model maps states to probability distributions over the feature space so that, in addition to current phoneme and some contextual information about the preceding and following phonemes, the state can be augmented to include location information. During training, for each frame, the state may not be known exactly; sometimes, only a probability distribution over the states is known; in this case, a smoothed continuous location distribution or probability density function over the discretized location can be incorporated into the probability distribution over the states. Accordingly, location information is stored by the model at the phoneme level, instead of the acoustic feature level.
- According to a fourth approach, which does not require the retraining an acoustic model, a single acoustic model is used for all locations within a language, however the acoustic model is adapted in a lightweight manner based on the geographic location. One such known technique for adapting acoustic models uses a Maximum-Likelihood Linear Regression ("MLLR"), which derives a transformation matrix that is applied to the Gaussian coefficients in the acoustic model space, or to the input features of the acoustic model, to adjust the model to match a set of adaptation utterances. The geographic location of the
audio signal 112 may be used to define a geographic region, and all of the training audio signals stored by theASR engine 105 that are associated with the region can be fed into the MLLR adaptation algorithm, to produce a matrix that may be used to transform the acoustic model to match the accent found in that region. - In more detail, a single, universal acoustic model may be generated for a particular region and/or language, such as an acoustic model that represents "United States English." The audio signals that are used to adapt the model (e.g., the
audio signals 107 to 109) are used to generate linear transformations that transform the universal model to match the accent in a particular sub-region, by matrix multiplying the coefficients of the universal acoustic model by an appropriate linear transformation. The generation of the linear transformations and the adaptation of the universal acoustic model through matrix multiplication may occur on-the-fly, for example after theaudio signal 112 has been received by theASR engine 105. - During state (e), the
speech recognition module 124 of theASR engine 105 performs speech recognition on theaudio signal 112 using the one or more geo-specificacoustic models 111 for the geographic location associated with theaudio signal 112. When theaudio signal 112 includes metadata that describes a device type of themobile device 102, theASR engine 105 may apply an acoustic model that is specific to both the geographic location associated with the audio signal, and to the device type of themobile device 102. Thespeech recognition module 124 may generate one ormore candidate transcriptions 115 that match the utterance encoded in theaudio signal 112, and speech recognition confidence values for the candidate transcriptions. - During state (f), one or more of the
candidate transcriptions 115 generated by thespeech recognition module 124 are communicated from theASR engine 105 to theserver 104. When theserver 104 is a search engine, thecandidate transcriptions 115 may be used as candidate query terms that are used by the search engine to execute one or more search queries. TheASR engine 105 may rank thecandidate transcriptions 115 based at least on their respective speech recognition confidence scores before transmission to theserver 104. By transcribing spoken utterances and providing candidate transcriptions to theserver 104, theASR engine 105 may provide a voice search query capability, a dictation capability, or a dialogue system capability to themobile device 102. - The
server 104 may execute one or more search queries using the candidate query terms, and may generate afile 116 that referencessearch results file 116 may be a markup language file, such as an extensible Markup Language (XML) or HyperText Markup Language (HTML) file. - The
server 104, in some examples, may include a web search engine used to find references within the Internet, a phone book type search engine used to find businesses or individuals, or another specialized search engine (e.g., a search engine that provides references to entertainment listings such as restaurants and movie theater information, medical and pharmaceutical information, etc.). During state (h), theserver 104 provides thefile 116 that references the search results 126 and 127 to themobile device 102. - During state (i), the
mobile device 102 displays the search results 126 and 127 on theuser interface 101. Theuser interface 101 includes asearch box 129 that displays the candidate query term with the highest speech recognition confidence score ("Park your car"), an alternate queryterm suggestion region 130 that displays another of the candidate query term that may have been intended by the utterance 113 ("Parker Cole" and "Parka Card"), asearch result 126 that includes a link to a resource for "Boston Parking," and asearch result 127 that includes a link to a resource for "Cambridge Car Park." -
FIG. 2 is a flowchart of anexample process 200. Briefly, theprocess 200 includes receiving an audio signal that corresponds to an utterance recorded by a mobile device, determining a geographic location associated with the mobile device, adapting one or more acoustic models for the geographic location, and performing speech recognition on the audio signal using the one or more acoustic models model that are adapted for the geographic location. - In more detail, when
process 200 begins, an audio signal that corresponds to an utterance recorded by a mobile device is received (202). The utterance may include a voice search query, or may be an input to a dictation or dialog application or system. The utterance may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal. - A geographic location associated with the mobile device is determined (204). For example, data referencing the particular geographic location may be received from the mobile device, or a past geographic location or a default geographic location associated with the mobile device may be identified.
- One or more acoustic models are adapted for the geographic location (206). A subset of geotagged audio signals used for adapting the acoustic model may be selected by determining, for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal, and selecting those geotagged audio signals which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location.
- Adapting the one or more acoustic models may include selecting the one or more acoustic models generated for the geographic location associated with the mobile device, from among multiple acoustic models that have been generated for multiple geographic locations, or incorporating data that references the geographic location (e.g., geographic coordinates) into a feature space used by the one or more acoustic models, in accordance with the invention. Alternatively, adapting the one or more acoustic models may include incorporating data that references the geographic location into state information included in the acoustic model, or deriving a transformation matrix associated with the geographic location; and applying the transformation matrix to a universal acoustic model.
- The subset of geotagged audio signals may be selected by identifying the geotagged audio signals associated with the particular geographic location, and/or by identifying the geotagged audio signals that are acoustically similar to the utterance. The subset of geotagged audio signals may be selected based both on the particular geographic location and on context data associated with the utterance. Generating the acoustic model may include training a GMM using the subset of geotagged audio signals as a training set.
- Speech recognition is performed on the audio signal (208). Performing the speech recognition may include generating one or more candidate transcriptions of the utterance. A search query may be executed using the one or more candidate transcriptions, or one or more of the candidate transcriptions may be provided as an output of a digital dictation application. Alternatively, one or more of the candidate transcriptions may be provided as an input to a dialog system, to allow a computer system to converse with the user of the particular mobile device.
-
FIG. 3 is a flowchart of anotherexample process 300. Briefly, theprocess 300 includes receiving geotagged audio signals and generating multiple acoustic models based, in part, upon particular geographic locations associated with each of the geotagged audio signals. One or more of these acoustic models may be selected when performing speech recognition upon an utterance based, in part, upon a geographic location associated with the utterance. - In more detail, when
process 300 begins, geotagged audio signal corresponding to audio is received (302). The geotagged audio signal may be recorded by a mobile device in a particular geographic location. The received geotagged audio signal may be processed to exclude portions of the audio that do not include the voice of the user of the mobile device. Multiple geotagged audio signals recorded in one or more geographic locations may be received and stored. - Optionally, context data associated with the geotagged audio signal is received (304). The geotagged audio signal may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- One or more acoustic models are adapted (306). Each acoustic model may be adapted for a particular geographic location or, optionally, a location type, using a subset of geotagged audio signals. The subset of geotagged audio signals may be selected by determining, for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal and selecting those geotagged audio signals which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location. The subset of geotagged audio signals may be selected by identifying the geotagged audio signals associated with the particular geographic location. The subset of geotagged audio signals may be selected based both on the particular geographic location and on context data associated with the geotagged audio signals. Generating the acoustic model may include training a Gaussian Mixture Model (GMM) using the subset of geotagged audio signals.
- An utterance recorded by a particular mobile device is received (308). The utterance may include a voice search query. The utterance may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- A geographic location is determined (310). For example, data referencing the particular geographic location may be received from a GPS module of the mobile device.
- An acoustic model is selected (312). The acoustic model may be selected from among multiple acoustic models adapted for multiple geographic locations. Context data may optionally contribute to selection of a particular acoustic model among multiple acoustic models for the particular geographic location.
- Speech recognition is performed on the utterance using the selected acoustic model (314). Performing the speech recognition may include generating one or more candidate transcriptions of the utterance. A search query may be executed using the one or more candidate transcriptions.
-
FIG. 4 shows a swim lane diagram of an example of aprocess 400 for enhancing speech recognition accuracy using geotagged audio. Theprocess 400 may be implemented by amobile device 402, anASR engine 404, and asearch engine 406. Themobile device 402 may provide audio signals, such as audio signals or audio signals that correspond to an utterance, to theASR engine 404. Although only onemobile device 402 is illustrated, themobile device 402 may represent a large quantity ofmobile devices 402 contributing audio signals and voice queries to theprocess 400. TheASR engine 404 may adapt acoustic models based upon the audio signals, and may apply one or more acoustic models to an incoming voice search query when performing speech recognition. TheASR engine 404 may provide transcriptions of utterances within a voice search query to thesearch engine 406 to complete the voice search query request. - The
process 400 begins with themobile device 402 providing 408 a geotagged audio signal to theASR engine 404. The audio signal may include audio of a voice of themobile device 402, along with an indication regarding the location at which the audio was recorded. Optionally, the geotagged audio signal may include context data, for example in the form of metadata. TheASR engine 404 may store the geotagged audio signal in an audio data store. - The
mobile device 402 provides 410 an utterance to theASR engine 404. The utterance, for example, may include a voice search query. The recording of the utterance may optionally include a sample of audio, for example recorded briefly before or after the recording of the utterance. - The
mobile device 402 provides 412 a geographic location to theASR engine 404. The mobile device, in some examples, may provide navigational coordinates detected using a GPS module, a most recent (but not necessarily concurrent with recording) GPS reading, a default location, a location derived from the utterance previously provided, or a location estimated through dead reckoning or triangulation of transmission towers. Themobile device 402 may optionally provide context data, such as sensor data, device model identification, or device settings, to theASR engine 404. - The
ASR engine 404 adapts 414 an acoustic model. The acoustic model may be adapted, in part, by training a GMM. The acoustic model may be adapted based upon the geographic location provided by themobile device 402. For example, geotagged audio signals submitted from a location at or near the location of themobile device 402 may contribute to an acoustic model. Optionally, context data provided by themobile device 402 may be used to filter geotagged audio signals to select those most appropriate to the conditions in which the utterances were recorded. For example, the geotagged audio signals near the geographic location provided by themobile device 402 may be filtered by a day of the week or a time of day. If a sample of audio was included with the utterance provided by themobile device 402, the audio sample may optionally be included in the acoustic model. - The
ASR engine 404 performsspeech recognition 416 upon the provided utterance. The utterance provided by themobile device 402 may be transcribed into one or more sets of query terms using the acoustic model adapted by theASR engine 404. - The
ASR engine 404 forwards 418 the generated transcription(s) to thesearch engine 406. If theASR engine 404 generated more than one transcription, the transcriptions may optionally be ranked in order of confidence. TheASR engine 404 may optionally provide context data to thesearch engine 406, such as the geographic location, which thesearch engine 406 may use to filter or rank search results. - The
search engine 406 performs 420 a search operation using the transcription(s). Thesearch engine 406 may locate one or more URIs related to the transcription term(s). - The
search engine 406 provides 422 search query results to themobile device 402. For example, thesearch engine 406 may forward HTML code which generates a visual listing of the URI(s) located. - A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the scope of the disclosure. For example, various forms of the flows shown above may be used, with steps re-ordered, added, or removed. Accordingly, other implementations are within the scope of the following claims.
- Embodiments and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus. The computer readable medium may be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them. The term "data processing apparatus" encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- A computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both.
- The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, embodiments may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer. Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation, or any combination of one or more such back end, middleware, or front end components. The components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), e.g., the Internet.
- The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- While this specification contains many specifics, these should not be construed as limitations on the scope of the disclosure or of what may be claimed, but rather as descriptions of features specific to particular embodiments. The invention is ultimately defined by the appended claims. Certain features that are described in this specification in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment may also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination may in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems may generally be integrated together in a single software product or packaged into multiple software products.
- In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.
- Thus, particular embodiments have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims may be performed in a different order and still achieve desirable results.
Claims (15)
- A system comprising:one or more computers; anda computer-readable medium coupled to the one or more computers having instructions stored thereon which, when executed by the one or more computers, cause the one or more computers to perform operations comprising:receiving an audio signal (112) that corresponds to an utterance (113) recorded by a mobile device (102),determining a geographic location (123) associated with the mobile device (102),adapting one or more acoustic models (111) for the geographic location (123), wherein the adapting of the one or more acoustic models for the geographic location further comprises:incorporating data that references the geographic location (123) into a feature space used by a single acoustic model, wherein the geographic location information is directly incorporated into the single acoustic model by directly stacking a two-dimensional, continuous latitude and longitude coordinate vector into the feature space, wherein the geographic location information is considered as part of the regular training process for a single acoustic model,andperforming speech recognition on the audio signal (112) using the one or more acoustic models model (111) that are adapted for the geographic location (123).
- The system of claim 1, wherein adapting one or more acoustic models further comprises adapting one or more acoustic models before receiving the audio signal that corresponds to the utterance, or
wherein adapting one or more acoustic models further comprises adapting one or more acoustic models after receiving the audio signal that corresponds to the utterance. - The system of claim 1, wherein:the operations further comprise receiving geotagged audio signals (107-109) that correspond to audio recorded by multiple mobile devices in multiple geographic locations (117-121); andadapting one or more acoustic models (111) for the geographic location (123) further comprises adapting one or more acoustic models for the geographic location using a subset of the geotagged audio signals (107-109).
- The system of claim 3, wherein the operations further comprise:determining, for each of the geotagged audio signals, a distance between the geographic location associated with the mobile device and a geographic location associated the geotagged audio signal; andselecting, as the subset of the geotagged audio signals, the geotagged audio signals that are associated with geographic locations which are within a predetermined distance of the geographic location associated with the mobile device, or that are associated with geographic locations which are among an N closest geographic locations to the geographic location associated with the mobile device.
- The system of claim 3, wherein the operations further comprise:selecting, as the subset of the geotagged audio signals, the geotagged audio signals that are associated with the geographic location that is also associated with the mobile device.
- The system of claim 3, wherein the operations further comprise selecting the subset of the geotagged audio signals based on the geographic location associated with the mobile device, and based on context data associated with the utterance,
wherein the context data comprises data that references a time or a date when the utterance was recorded by the mobile device, data that references a speed or an amount of motion measured by the mobile device when the utterance was recorded, data that references settings of the mobile device, or data that references a type of the mobile device. - The system of claim 3, wherein adapting the acoustic model comprises training a Gaussian Mixture Model (GMM) using the subset of the geotagged audio signals as a training set.
- The system of claim 1, wherein determining the geographic location further comprises receiving data referencing the geographic location from the mobile device, or
wherein determining the geographic location further comprises determining a past geographic location or a default geographic location associated with the mobile device. - The system of claim 1, wherein the operations further comprise:generating one or more candidate transcriptions of the utterance; andexecuting a search query using the one or more candidate transcriptions.
- The system of claim 1, wherein adapting one or more acoustic models for the geographic location further comprises:selecting, from among multiple acoustic models that have been generated for multiple geographic locations, the one or more acoustic models generated for the geographic location associated with the mobile device.
- The system of claim 1, wherein incorporating data that references the geographic location into a feature space used by the single acoustic model further comprises incorporating values into a feature space used by the single acoustic model, wherein the values comprise Mel-frequency Cepstral Coefficients and geographic coordinates.
- The system of claim 1, wherein adapting one or more acoustic models for the geographic location further comprises incorporating data that references the geographic location into state information included in a single acoustic model.
- The system of claim 1, wherein adapting one or more acoustic models for the geographic location further comprises:deriving a transformation matrix associated with the geographic location; andapplying the transformation matrix to a single, universal acoustic model.
- A computer storage medium encoded with a computer program, the program comprising instructions that when executed by one or more computers cause the one or more computers to perform operations comprising:receiving an audio signal (112) that corresponds to an utterance (113) recorded by a mobile device (102),determining a geographic location (123) associated with the mobile device (102),adapting one or more acoustic models (111) for the geographic location (123), wherein the adapting of the one or more acoustic models for the geographic location further comprises:incorporating data that references the geographic location (123) into a feature space used by a single acoustic model, wherein the geographic location information is directly incorporated into the single acoustic model by directly stacking a two-dimensional, continuous latitude and longitude coordinate vector into the feature space, wherein the geographic location information is considered as part of the regular training process for a single acoustic model;andperforming speech recognition on the audio signal (112) using the one or more acoustic models model (111) that are adapted for the geographic location (102).
- A computer-implemented method comprising:receiving an audio signal (112) that corresponds to an utterance (113) recorded by a mobile device (102),determining a geographic location (123) associated with the mobile device (102),adapting one or more acoustic models (111) for the geographic location (123), wherein the adapting of the one or more acoustic models for the geographic location further comprises:incorporating data that references the geographic location (123) into a feature space used by a single acoustic model, wherein the geographic location information is directly incorporated into the single acoustic model by directly stacking a two-dimensional, continuous latitude and longitude coordinate vector into the feature space, wherein the geographic location information is considered as part of the regular training process for a single acoustic model;andperforming speech recognition on the audio signal (112) using the one or more acoustic models model (111) that are adapted for the geographic location (102).
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/787,568 US8468012B2 (en) | 2010-05-26 | 2010-05-26 | Acoustic model adaptation using geographic information |
PCT/US2011/037558 WO2011149837A1 (en) | 2010-05-26 | 2011-05-23 | Acoustic model adaptation using geographic information |
Publications (2)
Publication Number | Publication Date |
---|---|
EP2577653A1 EP2577653A1 (en) | 2013-04-10 |
EP2577653B1 true EP2577653B1 (en) | 2015-03-11 |
Family
ID=44276070
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP11723813.9A Active EP2577653B1 (en) | 2010-05-26 | 2011-05-23 | Acoustic model adaptation using geographic information |
Country Status (5)
Country | Link |
---|---|
US (3) | US8468012B2 (en) |
EP (1) | EP2577653B1 (en) |
CN (2) | CN104575493B (en) |
AU (2) | AU2011258531B2 (en) |
WO (1) | WO2011149837A1 (en) |
Families Citing this family (358)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
AU6630800A (en) * | 1999-08-13 | 2001-03-13 | Pixo, Inc. | Methods and apparatuses for display and traversing of links in page character array |
US8645137B2 (en) * | 2000-03-16 | 2014-02-04 | Apple Inc. | Fast, language-independent method for user authentication by voice |
ITFI20010199A1 (en) | 2001-10-22 | 2003-04-22 | Riccardo Vieri | SYSTEM AND METHOD TO TRANSFORM TEXTUAL COMMUNICATIONS INTO VOICE AND SEND THEM WITH AN INTERNET CONNECTION TO ANY TELEPHONE SYSTEM |
US7669134B1 (en) | 2003-05-02 | 2010-02-23 | Apple Inc. | Method and apparatus for displaying information during an instant messaging session |
US8677377B2 (en) | 2005-09-08 | 2014-03-18 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US7633076B2 (en) | 2005-09-30 | 2009-12-15 | Apple Inc. | Automated response to and sensing of user activity in portable devices |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US20080129520A1 (en) * | 2006-12-01 | 2008-06-05 | Apple Computer, Inc. | Electronic device with enhanced audio feedback |
US7912828B2 (en) * | 2007-02-23 | 2011-03-22 | Apple Inc. | Pattern searching methods and apparatuses |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
ITFI20070177A1 (en) | 2007-07-26 | 2009-01-27 | Riccardo Vieri | SYSTEM FOR THE CREATION AND SETTING OF AN ADVERTISING CAMPAIGN DERIVING FROM THE INSERTION OF ADVERTISING MESSAGES WITHIN AN EXCHANGE OF MESSAGES AND METHOD FOR ITS FUNCTIONING. |
US9053089B2 (en) * | 2007-10-02 | 2015-06-09 | Apple Inc. | Part-of-speech tagging using latent analogy |
US8165886B1 (en) | 2007-10-04 | 2012-04-24 | Great Northern Research LLC | Speech interface system and method for control and interaction with applications on a computing system |
US8595642B1 (en) | 2007-10-04 | 2013-11-26 | Great Northern Research, LLC | Multiple shell multi faceted graphical user interface |
US8364694B2 (en) * | 2007-10-26 | 2013-01-29 | Apple Inc. | Search assistant for digital media assets |
US8620662B2 (en) | 2007-11-20 | 2013-12-31 | Apple Inc. | Context-aware unit selection |
US10002189B2 (en) | 2007-12-20 | 2018-06-19 | Apple Inc. | Method and apparatus for searching using an active ontology |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US8327272B2 (en) | 2008-01-06 | 2012-12-04 | Apple Inc. | Portable multifunction device, method, and graphical user interface for viewing and managing electronic calendars |
US8065143B2 (en) | 2008-02-22 | 2011-11-22 | Apple Inc. | Providing text input using speech data and non-speech data |
US8289283B2 (en) * | 2008-03-04 | 2012-10-16 | Apple Inc. | Language input interface on a device |
US8996376B2 (en) | 2008-04-05 | 2015-03-31 | Apple Inc. | Intelligent text-to-speech conversion |
US10496753B2 (en) | 2010-01-18 | 2019-12-03 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US8464150B2 (en) | 2008-06-07 | 2013-06-11 | Apple Inc. | Automatic language identification for dynamic text processing |
US20100030549A1 (en) | 2008-07-31 | 2010-02-04 | Lee Michael M | Mobile device having human language translation capability with positional feedback |
WO2010019831A1 (en) * | 2008-08-14 | 2010-02-18 | 21Ct, Inc. | Hidden markov model for speech processing with training method |
US8768702B2 (en) | 2008-09-05 | 2014-07-01 | Apple Inc. | Multi-tiered voice feedback in an electronic device |
US8898568B2 (en) * | 2008-09-09 | 2014-11-25 | Apple Inc. | Audio user interface |
US8352272B2 (en) * | 2008-09-29 | 2013-01-08 | Apple Inc. | Systems and methods for text to speech synthesis |
US8352268B2 (en) * | 2008-09-29 | 2013-01-08 | Apple Inc. | Systems and methods for selective rate of speech and speech preferences for text to speech synthesis |
US8355919B2 (en) * | 2008-09-29 | 2013-01-15 | Apple Inc. | Systems and methods for text normalization for text to speech synthesis |
US8583418B2 (en) | 2008-09-29 | 2013-11-12 | Apple Inc. | Systems and methods of detecting language and natural language strings for text to speech synthesis |
US8712776B2 (en) * | 2008-09-29 | 2014-04-29 | Apple Inc. | Systems and methods for selective text to speech synthesis |
US20100082328A1 (en) * | 2008-09-29 | 2010-04-01 | Apple Inc. | Systems and methods for speech preprocessing in text to speech synthesis |
US8396714B2 (en) * | 2008-09-29 | 2013-03-12 | Apple Inc. | Systems and methods for concatenation of words in text to speech synthesis |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
WO2010067118A1 (en) | 2008-12-11 | 2010-06-17 | Novauris Technologies Limited | Speech recognition involving a mobile device |
US8862252B2 (en) | 2009-01-30 | 2014-10-14 | Apple Inc. | Audio user interface for displayless electronic device |
US8380507B2 (en) * | 2009-03-09 | 2013-02-19 | Apple Inc. | Systems and methods for determining the language to use for speech generated by a text to speech engine |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US10540976B2 (en) | 2009-06-05 | 2020-01-21 | Apple Inc. | Contextual voice commands |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US9431006B2 (en) * | 2009-07-02 | 2016-08-30 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US20110010179A1 (en) * | 2009-07-13 | 2011-01-13 | Naik Devang K | Voice synthesis and processing |
JP2011033680A (en) * | 2009-07-30 | 2011-02-17 | Sony Corp | Voice processing device and method, and program |
US20110066438A1 (en) * | 2009-09-15 | 2011-03-17 | Apple Inc. | Contextual voiceover |
US8682649B2 (en) * | 2009-11-12 | 2014-03-25 | Apple Inc. | Sentiment prediction from textual data |
WO2011071484A1 (en) | 2009-12-08 | 2011-06-16 | Nuance Communications, Inc. | Guest speaker robust adapted speech recognition |
US11416214B2 (en) | 2009-12-23 | 2022-08-16 | Google Llc | Multi-modal input on an electronic device |
EP2339576B1 (en) | 2009-12-23 | 2019-08-07 | Google LLC | Multi-modal input on an electronic device |
US8600743B2 (en) * | 2010-01-06 | 2013-12-03 | Apple Inc. | Noise profile determination for voice-related feature |
US20110167350A1 (en) * | 2010-01-06 | 2011-07-07 | Apple Inc. | Assist Features For Content Display Device |
US8311838B2 (en) | 2010-01-13 | 2012-11-13 | Apple Inc. | Devices and methods for identifying a prompt corresponding to a voice input in a sequence of prompts |
US8381107B2 (en) | 2010-01-13 | 2013-02-19 | Apple Inc. | Adaptive audio feedback system and method |
US10705794B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10679605B2 (en) | 2010-01-18 | 2020-06-09 | Apple Inc. | Hands-free list-reading by intelligent automated assistant |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US10553209B2 (en) | 2010-01-18 | 2020-02-04 | Apple Inc. | Systems and methods for hands-free notification summaries |
US8682667B2 (en) | 2010-02-25 | 2014-03-25 | Apple Inc. | User profiling for selecting user specific voice input processing information |
US8265928B2 (en) | 2010-04-14 | 2012-09-11 | Google Inc. | Geotagged environmental audio for enhanced speech recognition accuracy |
US8639516B2 (en) | 2010-06-04 | 2014-01-28 | Apple Inc. | User-specific noise suppression for voice quality improvements |
US8442827B2 (en) | 2010-06-18 | 2013-05-14 | At&T Intellectual Property I, L.P. | System and method for customized voice response |
US8713021B2 (en) | 2010-07-07 | 2014-04-29 | Apple Inc. | Unsupervised document clustering using latent semantic density analysis |
US9104670B2 (en) | 2010-07-21 | 2015-08-11 | Apple Inc. | Customized search or acquisition of digital media assets |
US8521526B1 (en) * | 2010-07-28 | 2013-08-27 | Google Inc. | Disambiguation of a spoken query term |
US8719006B2 (en) | 2010-08-27 | 2014-05-06 | Apple Inc. | Combined statistical and rule-based part-of-speech tagging for text-to-speech synthesis |
US8719014B2 (en) | 2010-09-27 | 2014-05-06 | Apple Inc. | Electronic device with text error correction based on voice recognition data |
US20120109649A1 (en) * | 2010-11-01 | 2012-05-03 | General Motors Llc | Speech dialect classification for automatic speech recognition |
US20120155663A1 (en) * | 2010-12-16 | 2012-06-21 | Nice Systems Ltd. | Fast speaker hunting in lawful interception systems |
US10515147B2 (en) | 2010-12-22 | 2019-12-24 | Apple Inc. | Using statistical language models for contextual lookup |
US10762293B2 (en) | 2010-12-22 | 2020-09-01 | Apple Inc. | Using parts-of-speech tagging and named entity recognition for spelling correction |
US8352245B1 (en) | 2010-12-30 | 2013-01-08 | Google Inc. | Adjusting language models |
KR101791907B1 (en) * | 2011-01-04 | 2017-11-02 | 삼성전자주식회사 | Acoustic processing apparatus and method based on position information |
US8296142B2 (en) | 2011-01-21 | 2012-10-23 | Google Inc. | Speech recognition using dock context |
US20120197630A1 (en) * | 2011-01-28 | 2012-08-02 | Lyons Kenton M | Methods and systems to summarize a source text as a function of contextual information |
US9674328B2 (en) * | 2011-02-22 | 2017-06-06 | Speak With Me, Inc. | Hybridized client-server speech recognition |
US8781836B2 (en) | 2011-02-22 | 2014-07-15 | Apple Inc. | Hearing assistance system for providing consistent human speech |
US8660581B2 (en) | 2011-02-23 | 2014-02-25 | Digimarc Corporation | Mobile device indoor navigation |
US9270807B2 (en) * | 2011-02-23 | 2016-02-23 | Digimarc Corporation | Audio localization using audio signal encoding and recognition |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
US9202465B2 (en) * | 2011-03-25 | 2015-12-01 | General Motors Llc | Speech recognition dependent on text message content |
US10672399B2 (en) | 2011-06-03 | 2020-06-02 | Apple Inc. | Switching between text data and audio data based on a mapping |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
US8812294B2 (en) | 2011-06-21 | 2014-08-19 | Apple Inc. | Translating phrases from one language into another using an order-based set of declarative rules |
US8706472B2 (en) | 2011-08-11 | 2014-04-22 | Apple Inc. | Method for disambiguating multiple readings in language conversion |
US9576573B2 (en) * | 2011-08-29 | 2017-02-21 | Microsoft Technology Licensing, Llc | Using multiple modality input to feedback context for natural language understanding |
US8994660B2 (en) | 2011-08-29 | 2015-03-31 | Apple Inc. | Text correction processing |
US8762156B2 (en) | 2011-09-28 | 2014-06-24 | Apple Inc. | Speech recognition repair using contextual information |
US20170221093A1 (en) * | 2011-12-07 | 2017-08-03 | Google Inc. | Dynamically Generating Video / Animation, in Real-Time, in a Display or Electronic Advertisement Based on User Data |
GB201200831D0 (en) * | 2012-01-18 | 2012-02-29 | Sensewhere Ltd | Improved positioning system |
US10134385B2 (en) | 2012-03-02 | 2018-11-20 | Apple Inc. | Systems and methods for name pronunciation |
US9483461B2 (en) | 2012-03-06 | 2016-11-01 | Apple Inc. | Handling speech synthesis of content for multiple languages |
US9224383B2 (en) * | 2012-03-29 | 2015-12-29 | Educational Testing Service | Unsupervised language model adaptation for automated speech scoring |
US8838448B2 (en) * | 2012-04-05 | 2014-09-16 | Nuance Communications, Inc. | Forced/predictable adaptation for speech recognition |
US8473293B1 (en) * | 2012-04-17 | 2013-06-25 | Google Inc. | Dictionary filtering using market data |
US8374865B1 (en) * | 2012-04-26 | 2013-02-12 | Google Inc. | Sampling training data for an automatic speech recognition system based on a benchmark classification distribution |
US9280610B2 (en) | 2012-05-14 | 2016-03-08 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US8775442B2 (en) | 2012-05-15 | 2014-07-08 | Apple Inc. | Semantic search using a single-source semantic model |
US8805684B1 (en) * | 2012-05-31 | 2014-08-12 | Google Inc. | Distributed speaker adaptation |
US11023520B1 (en) | 2012-06-01 | 2021-06-01 | Google Llc | Background audio identification for query disambiguation |
WO2013185109A2 (en) | 2012-06-08 | 2013-12-12 | Apple Inc. | Systems and methods for recognizing textual identifiers within a plurality of words |
US9721563B2 (en) | 2012-06-08 | 2017-08-01 | Apple Inc. | Name recognition system |
US9043205B2 (en) * | 2012-06-21 | 2015-05-26 | Google Inc. | Dynamic language model |
US9502029B1 (en) * | 2012-06-25 | 2016-11-22 | Amazon Technologies, Inc. | Context-aware speech processing |
WO2014005055A2 (en) * | 2012-06-29 | 2014-01-03 | Elwha Llc | Methods and systems for managing adaptation data |
US9495129B2 (en) | 2012-06-29 | 2016-11-15 | Apple Inc. | Device, method, and user interface for voice-activated navigation and browsing of a document |
US9966064B2 (en) | 2012-07-18 | 2018-05-08 | International Business Machines Corporation | Dialect-specific acoustic language modeling and speech recognition |
US8831957B2 (en) * | 2012-08-01 | 2014-09-09 | Google Inc. | Speech recognition models based on location indicia |
US9946699B1 (en) * | 2012-08-29 | 2018-04-17 | Intuit Inc. | Location-based speech recognition for preparation of electronic tax return |
US9576574B2 (en) | 2012-09-10 | 2017-02-21 | Apple Inc. | Context-sensitive handling of interruptions by intelligent digital assistant |
US9460716B1 (en) * | 2012-09-11 | 2016-10-04 | Google Inc. | Using social networks to improve acoustic models |
US9547647B2 (en) | 2012-09-19 | 2017-01-17 | Apple Inc. | Voice-based media searching |
US8935167B2 (en) | 2012-09-25 | 2015-01-13 | Apple Inc. | Exemplar-based latent perceptual modeling for automatic speech recognition |
US9190057B2 (en) * | 2012-12-12 | 2015-11-17 | Amazon Technologies, Inc. | Speech model retrieval in distributed speech recognition systems |
US9065971B2 (en) * | 2012-12-19 | 2015-06-23 | Microsoft Technology Licensing, Llc | Video and audio tagging for active speaker detection |
US9495955B1 (en) * | 2013-01-02 | 2016-11-15 | Amazon Technologies, Inc. | Acoustic model training |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US9734819B2 (en) * | 2013-02-21 | 2017-08-15 | Google Technology Holdings LLC | Recognizing accented speech |
US10229701B2 (en) * | 2013-02-28 | 2019-03-12 | Nuance Communications, Inc. | Server-side ASR adaptation to speaker, device and noise condition via non-ASR audio transmission |
US20140270249A1 (en) | 2013-03-12 | 2014-09-18 | Motorola Mobility Llc | Method and Apparatus for Estimating Variability of Background Noise for Noise Suppression |
US9237225B2 (en) | 2013-03-12 | 2016-01-12 | Google Technology Holdings LLC | Apparatus with dynamic audio signal pre-conditioning and methods therefor |
US20140278393A1 (en) | 2013-03-12 | 2014-09-18 | Motorola Mobility Llc | Apparatus and Method for Power Efficient Signal Conditioning for a Voice Recognition System |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US9977779B2 (en) | 2013-03-14 | 2018-05-22 | Apple Inc. | Automatic supplementation of word correction dictionaries |
US10572476B2 (en) | 2013-03-14 | 2020-02-25 | Apple Inc. | Refining a search based on schedule items |
US9368114B2 (en) | 2013-03-14 | 2016-06-14 | Apple Inc. | Context-sensitive handling of interruptions |
US9733821B2 (en) | 2013-03-14 | 2017-08-15 | Apple Inc. | Voice control to diagnose inadvertent activation of accessibility features |
US10642574B2 (en) | 2013-03-14 | 2020-05-05 | Apple Inc. | Device, method, and graphical user interface for outputting captions |
WO2014144579A1 (en) | 2013-03-15 | 2014-09-18 | Apple Inc. | System and method for updating an adaptive speech recognition model |
EP2973002B1 (en) | 2013-03-15 | 2019-06-26 | Apple Inc. | User training by intelligent digital assistant |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
KR102057795B1 (en) | 2013-03-15 | 2019-12-19 | 애플 인크. | Context-sensitive handling of interruptions |
KR101759009B1 (en) | 2013-03-15 | 2017-07-17 | 애플 인크. | Training an at least partial voice command system |
US9582608B2 (en) | 2013-06-07 | 2017-02-28 | Apple Inc. | Unified ranking with entropy-weighted information for phrase-based semantic auto-completion |
WO2014197336A1 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US20140365218A1 (en) * | 2013-06-07 | 2014-12-11 | Microsoft Corporation | Language model adaptation using result selection |
WO2014197334A2 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
WO2014197335A1 (en) | 2013-06-08 | 2014-12-11 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
KR101959188B1 (en) | 2013-06-09 | 2019-07-02 | 애플 인크. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
WO2014200731A1 (en) | 2013-06-13 | 2014-12-18 | Apple Inc. | System and method for emergency calls initiated by voice command |
US20140372027A1 (en) * | 2013-06-14 | 2014-12-18 | Hangzhou Haicun Information Technology Co. Ltd. | Music-Based Positioning Aided By Dead Reckoning |
US9727129B2 (en) * | 2013-06-28 | 2017-08-08 | Harman International Industries, Incorporated | System and method for audio augmented reality |
KR102084646B1 (en) * | 2013-07-04 | 2020-04-14 | 삼성전자주식회사 | Device for recognizing voice and method for recognizing voice |
US9786296B2 (en) * | 2013-07-08 | 2017-10-10 | Qualcomm Incorporated | Method and apparatus for assigning keyword model to voice operated function |
KR101749009B1 (en) | 2013-08-06 | 2017-06-19 | 애플 인크. | Auto-activating smart responses based on activities from remote devices |
US9299340B2 (en) | 2013-10-07 | 2016-03-29 | Honeywell International Inc. | System and method for correcting accent induced speech in an aircraft cockpit utilizing a dynamic speech database |
US9530416B2 (en) | 2013-10-28 | 2016-12-27 | At&T Intellectual Property I, L.P. | System and method for managing models for embedded speech and language processing |
US9666188B2 (en) | 2013-10-29 | 2017-05-30 | Nuance Communications, Inc. | System and method of performing automatic speech recognition using local private data |
CN104637495B (en) * | 2013-11-08 | 2019-03-26 | 宏达国际电子股份有限公司 | Electronic device and acoustic signal processing method |
US10296160B2 (en) | 2013-12-06 | 2019-05-21 | Apple Inc. | Method for extracting salient dialog usage from live data |
CN103680493A (en) * | 2013-12-19 | 2014-03-26 | 百度在线网络技术（北京）有限公司 | Voice data recognition method and device for distinguishing regional accents |
TWI506458B (en) * | 2013-12-24 | 2015-11-01 | Ind Tech Res Inst | Apparatus and method for generating recognition network |
US9589564B2 (en) | 2014-02-05 | 2017-03-07 | Google Inc. | Multiple speech locale-specific hotword classifiers for selection of a speech locale |
US9842592B2 (en) | 2014-02-12 | 2017-12-12 | Google Inc. | Language models using non-linguistic context |
US9412365B2 (en) | 2014-03-24 | 2016-08-09 | Google Inc. | Enhanced maximum entropy models |
US9633649B2 (en) | 2014-05-02 | 2017-04-25 | At&T Intellectual Property I, L.P. | System and method for creating voice profiles for specific demographics |
US9620105B2 (en) | 2014-05-15 | 2017-04-11 | Apple Inc. | Analyzing audio input for efficient speech and music recognition |
US10592095B2 (en) | 2014-05-23 | 2020-03-17 | Apple Inc. | Instantaneous speaking of content on touch devices |
KR102225404B1 (en) * | 2014-05-23 | 2021-03-09 | 삼성전자주식회사 | Method and Apparatus of Speech Recognition Using Device Information |
US9502031B2 (en) | 2014-05-27 | 2016-11-22 | Apple Inc. | Method for supporting dynamic grammars in WFST-based ASR |
US9734193B2 (en) | 2014-05-30 | 2017-08-15 | Apple Inc. | Determining domain salience ranking from ambiguous words in natural speech |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US10289433B2 (en) | 2014-05-30 | 2019-05-14 | Apple Inc. | Domain specific language for encoding assistant dialog |
US10078631B2 (en) | 2014-05-30 | 2018-09-18 | Apple Inc. | Entropy-guided text prediction using combined word and character n-gram language models |
US9430463B2 (en) | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9842101B2 (en) | 2014-05-30 | 2017-12-12 | Apple Inc. | Predictive conversion of language input |
US9633004B2 (en) | 2014-05-30 | 2017-04-25 | Apple Inc. | Better resolution when referencing to concepts |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9785630B2 (en) | 2014-05-30 | 2017-10-10 | Apple Inc. | Text prediction using combined word N-gram and unigram language models |
US9760559B2 (en) | 2014-05-30 | 2017-09-12 | Apple Inc. | Predictive text input |
US9904851B2 (en) | 2014-06-11 | 2018-02-27 | At&T Intellectual Property I, L.P. | Exploiting visual information for enhancing audio signals via source separation and beamforming |
US20150371628A1 (en) * | 2014-06-23 | 2015-12-24 | Harman International Industries, Inc. | User-adapted speech recognition |
US9858920B2 (en) * | 2014-06-30 | 2018-01-02 | GM Global Technology Operations LLC | Adaptation methods and systems for speech systems |
US10659851B2 (en) | 2014-06-30 | 2020-05-19 | Apple Inc. | Real-time digital assistant knowledge updates |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
WO2016016863A1 (en) * | 2014-08-01 | 2016-02-04 | Maluuba Inc. | Speech recognition using models associated with a geographic location |
US10446141B2 (en) | 2014-08-28 | 2019-10-15 | Apple Inc. | Automatic speech recognition based on user feedback |
US9953646B2 (en) | 2014-09-02 | 2018-04-24 | Belleau Technologies | Method and system for dynamic speech recognition and tracking of prewritten script |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10789041B2 (en) | 2014-09-12 | 2020-09-29 | Apple Inc. | Dynamic thresholds for always listening speech trigger |
KR102348084B1 (en) * | 2014-09-16 | 2022-01-10 | 삼성전자주식회사 | Image Displaying Device, Driving Method of Image Displaying Device, and Computer Readable Recording Medium |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
US9646609B2 (en) | 2014-09-30 | 2017-05-09 | Apple Inc. | Caching apparatus for serving phonetic pronunciations |
US9886432B2 (en) | 2014-09-30 | 2018-02-06 | Apple Inc. | Parsimonious handling of word inflection via categorical stem + suffix N-gram language models |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US9530408B2 (en) | 2014-10-31 | 2016-12-27 | At&T Intellectual Property I, L.P. | Acoustic environment recognizer for optimal speech processing |
US10552013B2 (en) | 2014-12-02 | 2020-02-04 | Apple Inc. | Data detection |
US9711141B2 (en) | 2014-12-09 | 2017-07-18 | Apple Inc. | Disambiguating heteronyms in speech synthesis |
US9898170B2 (en) | 2014-12-10 | 2018-02-20 | International Business Machines Corporation | Establishing user specified interaction modes in a question answering dialogue |
US9865280B2 (en) | 2015-03-06 | 2018-01-09 | Apple Inc. | Structured dictation using intelligent automated assistants |
US10152299B2 (en) | 2015-03-06 | 2018-12-11 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US9805713B2 (en) * | 2015-03-13 | 2017-10-31 | Google Inc. | Addressing missing features in models |
US9899019B2 (en) | 2015-03-18 | 2018-02-20 | Apple Inc. | Systems and methods for structured stem and suffix language models |
US10134394B2 (en) | 2015-03-20 | 2018-11-20 | Google Llc | Speech recognition using log-linear model |
US9842105B2 (en) | 2015-04-16 | 2017-12-12 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US10460227B2 (en) | 2015-05-15 | 2019-10-29 | Apple Inc. | Virtual assistant in a communication session |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US10127220B2 (en) | 2015-06-04 | 2018-11-13 | Apple Inc. | Language identification from short strings |
US10101822B2 (en) | 2015-06-05 | 2018-10-16 | Apple Inc. | Language input correction |
US9578173B2 (en) | 2015-06-05 | 2017-02-21 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US10186254B2 (en) | 2015-06-07 | 2019-01-22 | Apple Inc. | Context-based endpoint detection |
US10255907B2 (en) * | 2015-06-07 | 2019-04-09 | Apple Inc. | Automatic accent detection using acoustic models |
WO2016200381A1 (en) * | 2015-06-10 | 2016-12-15 | Nuance Communications, Inc. | Motion adaptive speech recognition for enhanced voice destination entry |
DE102015211101A1 (en) * | 2015-06-17 | 2016-12-22 | Volkswagen Aktiengesellschaft | Speech recognition system and method for operating a speech recognition system with a mobile unit and an external server |
US10274911B2 (en) * | 2015-06-25 | 2019-04-30 | Intel Corporation | Conversational interface for matching text of spoken input based on context model |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
CN105094364B (en) * | 2015-07-13 | 2018-07-20 | 小米科技有限责任公司 | Vocabulary display methods and device |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US9787819B2 (en) * | 2015-09-18 | 2017-10-10 | Microsoft Technology Licensing, Llc | Transcription of spoken communications |
US10319369B2 (en) * | 2015-09-22 | 2019-06-11 | Vendome Consulting Pty Ltd | Methods for the automated generation of speech sample asset production scores for users of a distributed language learning system, automated accent recognition and quantification and improved speech recognition |
US9697820B2 (en) | 2015-09-24 | 2017-07-04 | Apple Inc. | Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10468016B2 (en) | 2015-11-24 | 2019-11-05 | International Business Machines Corporation | System and method for supporting automatic speech recognition of regional accents based on statistical information and user corrections |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10133821B2 (en) * | 2016-01-06 | 2018-11-20 | Google Llc | Search result prefetching of voice queries |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US9978367B2 (en) | 2016-03-16 | 2018-05-22 | Google Llc | Determining dialog states for language models |
CN105872687A (en) * | 2016-03-31 | 2016-08-17 | 乐视控股（北京）有限公司 | Method and device for controlling intelligent equipment through voice |
US11138987B2 (en) * | 2016-04-04 | 2021-10-05 | Honeywell International Inc. | System and method to distinguish sources in a multiple audio source environment |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
DK179309B1 (en) | 2016-06-09 | 2018-04-23 | Apple Inc | Intelligent automated assistant in a home environment |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
DK179343B1 (en) | 2016-06-11 | 2018-05-14 | Apple Inc | Intelligent task discovery |
DK179049B1 (en) | 2016-06-11 | 2017-09-18 | Apple Inc | Data driven natural language event detection and classification |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
CN105957516B (en) * | 2016-06-16 | 2019-03-08 | 百度在线网络技术（北京）有限公司 | More voice identification model switching method and device |
CN106128462A (en) * | 2016-06-21 | 2016-11-16 | 东莞酷派软件技术有限公司 | Audio recognition method and system |
CN106205622A (en) | 2016-06-29 | 2016-12-07 | 联想(北京)有限公司 | Information processing method and electronic equipment |
CN106251859B (en) * | 2016-07-22 | 2019-05-31 | 百度在线网络技术（北京）有限公司 | Voice recognition processing method and apparatus |
CN106293600A (en) * | 2016-08-05 | 2017-01-04 | 三星电子（中国）研发中心 | A kind of sound control method and system |
US10832664B2 (en) | 2016-08-19 | 2020-11-10 | Google Llc | Automated speech recognition using language models that selectively use domain-specific model components |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
WO2018085893A1 (en) | 2016-11-10 | 2018-05-17 | Mark Andrew Englund | Acoustic method and system for providing digital data |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US11144683B2 (en) | 2016-12-06 | 2021-10-12 | General Electric Company | Real-time adaptation of system high fidelity model in feature space |
US10163451B2 (en) * | 2016-12-21 | 2018-12-25 | Amazon Technologies, Inc. | Accent translation |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US10311860B2 (en) | 2017-02-14 | 2019-06-04 | Google Llc | Language model biasing system |
US11024302B2 (en) * | 2017-03-14 | 2021-06-01 | Texas Instruments Incorporated | Quality feedback on user-recorded keywords for automatic speech recognition systems |
KR102380717B1 (en) * | 2017-04-30 | 2022-03-31 | 삼성전자주식회사 | Electronic apparatus for processing user utterance and controlling method thereof |
DK201770383A1 (en) | 2017-05-09 | 2018-12-14 | Apple Inc. | User interface for correcting recognition errors |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10446136B2 (en) * | 2017-05-11 | 2019-10-15 | Ants Technology (Hk) Limited | Accent invariant speech recognition |
DK201770439A1 (en) | 2017-05-11 | 2018-12-13 | Apple Inc. | Offline personal assistant |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
DK180048B1 (en) | 2017-05-11 | 2020-02-04 | Apple Inc. | MAINTAINING THE DATA PROTECTION OF PERSONAL INFORMATION |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
DK201770432A1 (en) | 2017-05-15 | 2018-12-21 | Apple Inc. | Hierarchical belief states for digital assistants |
DK201770431A1 (en) | 2017-05-15 | 2018-12-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
DK179560B1 (en) | 2017-05-16 | 2019-02-18 | Apple Inc. | Far-field extension for digital assistant services |
CN107274885B (en) * | 2017-05-31 | 2020-05-26 | Oppo广东移动通信有限公司 | Speech recognition method and related product |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
CN107016996B (en) * | 2017-06-06 | 2020-11-10 | 广东小天才科技有限公司 | Audio data processing method and device |
US10769138B2 (en) | 2017-06-13 | 2020-09-08 | International Business Machines Corporation | Processing context-based inquiries for knowledge retrieval |
KR102426717B1 (en) * | 2017-06-27 | 2022-07-29 | 삼성전자주식회사 | System and device for selecting a speech recognition model |
EP3662470B1 (en) * | 2017-08-01 | 2021-03-24 | Dolby Laboratories Licensing Corporation | Audio object classification based on location metadata |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10468019B1 (en) * | 2017-10-27 | 2019-11-05 | Kadho, Inc. | System and method for automatic speech recognition using selection of speech models based on input characteristics |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10616853B2 (en) | 2017-12-29 | 2020-04-07 | Sonitor Technologies As | Location determination using acoustic-contextual data |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
CN110047478B (en) * | 2018-01-16 | 2021-06-08 | 中国科学院声学研究所 | Multi-channel speech recognition acoustic modeling method and device based on spatial feature compensation |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10803843B2 (en) | 2018-04-06 | 2020-10-13 | Microsoft Technology Licensing, Llc | Computationally efficient language based user interface event sound selection |
WO2019203795A1 (en) | 2018-04-16 | 2019-10-24 | Google Llc | Automatically determining language for speech recognition of spoken utterance received via an automated assistant interface |
EP3622507B1 (en) * | 2018-04-16 | 2020-10-21 | Google LLC | Automatically determining language for speech recognition of spoken utterance received via an automated assistant interface |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
WO2019227290A1 (en) * | 2018-05-28 | 2019-12-05 | Beijing Didi Infinity Technology And Development Co., Ltd. | Systems and methods for speech recognition |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
DK201870355A1 (en) | 2018-06-01 | 2019-12-16 | Apple Inc. | Virtual assistant operation in multi-device environments |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US11011162B2 (en) | 2018-06-01 | 2021-05-18 | Soundhound, Inc. | Custom acoustic models |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10867067B2 (en) * | 2018-06-07 | 2020-12-15 | Cisco Technology, Inc. | Hybrid cognitive system for AI/ML data privacy |
CN108766414B (en) * | 2018-06-29 | 2021-01-15 | 北京百度网讯科技有限公司 | Method, apparatus, device and computer-readable storage medium for speech translation |
CN109243461B (en) * | 2018-09-21 | 2020-04-14 | 百度在线网络技术（北京）有限公司 | Voice recognition method, device, equipment and storage medium |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
CN109377990A (en) * | 2018-09-30 | 2019-02-22 | 联想(北京)有限公司 | A kind of information processing method and electronic equipment |
CN109215688B (en) * | 2018-10-10 | 2020-12-22 | 麦片科技（深圳）有限公司 | Same-scene audio processing method, device, computer readable storage medium and system |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
CN109599112B (en) * | 2019-01-02 | 2021-07-06 | 珠海格力电器股份有限公司 | Voice control method and device, storage medium and air conditioner |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
JP6745465B1 (en) * | 2019-03-06 | 2020-08-26 | パナソニックＩｐマネジメント株式会社 | Vehicle and camera module |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
CN110082726B (en) * | 2019-04-10 | 2021-08-10 | 北京梧桐车联科技有限责任公司 | Sound source positioning method and device, positioning equipment and storage medium |
CN110033765A (en) * | 2019-04-11 | 2019-07-19 | 中国联合网络通信集团有限公司 | A kind of method and terminal of speech recognition |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11227599B2 (en) | 2019-06-01 | 2022-01-18 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
CN110534112B (en) * | 2019-08-23 | 2021-09-10 | 王晓佳 | Distributed speech recognition error correction method based on position and time |
KR20190107622A (en) * | 2019-09-02 | 2019-09-20 | 엘지전자 주식회사 | Method and Apparatus for Updating Real-time Voice Recognition Model Using Moving Agent |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11061543B1 (en) | 2020-05-11 | 2021-07-13 | Apple Inc. | Providing relevant data items based on context |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
WO2021258240A1 (en) * | 2020-06-22 | 2021-12-30 | Qualcomm Incorporated | Voice or speech recognition in noisy environments |
US11490204B2 (en) | 2020-07-20 | 2022-11-01 | Apple Inc. | Multi-device audio adjustment coordination |
US11438683B2 (en) | 2020-07-21 | 2022-09-06 | Apple Inc. | User identification using headphones |
CN113506565A (en) * | 2021-07-12 | 2021-10-15 | 北京捷通华声科技股份有限公司 | Speech recognition method, speech recognition device, computer-readable storage medium and processor |
CN114165819A (en) * | 2021-11-26 | 2022-03-11 | 珠海格力电器股份有限公司 | Range hood, control method and module thereof and computer readable medium |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040230420A1 (en) * | 2002-12-03 | 2004-11-18 | Shubha Kadambe | Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments |
Family Cites Families (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
DE19533541C1 (en) | 1995-09-11 | 1997-03-27 | Daimler Benz Aerospace Ag | Method for the automatic control of one or more devices by voice commands or by voice dialog in real time and device for executing the method |
US6778959B1 (en) | 1999-10-21 | 2004-08-17 | Sony Corporation | System and method for speech verification using out-of-vocabulary models |
US7219058B1 (en) | 2000-10-13 | 2007-05-15 | At&T Corp. | System and method for processing speech recognition results |
US7457750B2 (en) * | 2000-10-13 | 2008-11-25 | At&T Corp. | Systems and methods for dynamic re-configurable speech recognition |
US6876966B1 (en) | 2000-10-16 | 2005-04-05 | Microsoft Corporation | Pattern recognition training method and apparatus using inserted noise followed by noise reduction |
US6915262B2 (en) * | 2000-11-30 | 2005-07-05 | Telesector Resources Group, Inc. | Methods and apparatus for performing speech recognition and using speech recognition results |
US6959276B2 (en) | 2001-09-27 | 2005-10-25 | Microsoft Corporation | Including the category of environmental noise when processing speech signals |
US6950796B2 (en) | 2001-11-05 | 2005-09-27 | Motorola, Inc. | Speech recognition by dynamical noise model adaptation |
US7224981B2 (en) | 2002-06-20 | 2007-05-29 | Intel Corporation | Speech recognition of mobile devices |
JP4109063B2 (en) | 2002-09-18 | 2008-06-25 | パイオニア株式会社 | Speech recognition apparatus and speech recognition method |
JP4352790B2 (en) | 2002-10-31 | 2009-10-28 | セイコーエプソン株式会社 | Acoustic model creation method, speech recognition device, and vehicle having speech recognition device |
US7533023B2 (en) * | 2003-02-12 | 2009-05-12 | Panasonic Corporation | Intermediary speech processor in network environments transforming customized speech parameters |
US7392188B2 (en) | 2003-07-31 | 2008-06-24 | Telefonaktiebolaget Lm Ericsson (Publ) | System and method enabling acoustic barge-in |
JP4548646B2 (en) | 2003-09-12 | 2010-09-22 | 株式会社エヌ・ティ・ティ・ドコモ | Noise model noise adaptation system, noise adaptation method, and speech recognition noise adaptation program |
US7634095B2 (en) | 2004-02-23 | 2009-12-15 | General Motors Company | Dynamic tuning of hands-free algorithm for noise and driving conditions |
US8041568B2 (en) * | 2006-10-13 | 2011-10-18 | Google Inc. | Business listing search |
US7890326B2 (en) * | 2006-10-13 | 2011-02-15 | Google Inc. | Business listing search |
US7941189B2 (en) | 2007-02-07 | 2011-05-10 | Denso Corporation | Communicating road noise control system, in-vehicle road noise controller, and server |
US20090030687A1 (en) | 2007-03-07 | 2009-01-29 | Cerra Joseph P | Adapting an unstructured language model speech recognition system based on usage |
US9405823B2 (en) * | 2007-07-23 | 2016-08-02 | Nuance Communications, Inc. | Spoken document retrieval using multiple speech transcription indices |
US8255224B2 (en) * | 2008-03-07 | 2012-08-28 | Google Inc. | Voice recognition grammar selection based on context |
US8121837B2 (en) | 2008-04-24 | 2012-02-21 | Nuance Communications, Inc. | Adjusting a speech engine for a mobile computing device based on background noise |
US9646025B2 (en) * | 2008-05-27 | 2017-05-09 | Qualcomm Incorporated | Method and apparatus for aggregating and presenting data associated with geographic locations |
US8548807B2 (en) * | 2009-06-09 | 2013-10-01 | At&T Intellectual Property I, L.P. | System and method for adapting automatic speech recognition pronunciation by acoustic model restructuring |
US8589163B2 (en) * | 2009-12-04 | 2013-11-19 | At&T Intellectual Property I, L.P. | Adapting language models with a bit mask for a subset of related words |
US8265928B2 (en) * | 2010-04-14 | 2012-09-11 | Google Inc. | Geotagged environmental audio for enhanced speech recognition accuracy |
-
2010
- 2010-05-26 US US12/787,568 patent/US8468012B2/en active Active
-
2011
- 2011-05-23 AU AU2011258531A patent/AU2011258531B2/en not_active Ceased
- 2011-05-23 WO PCT/US2011/037558 patent/WO2011149837A1/en active Application Filing
- 2011-05-23 CN CN201410723927.8A patent/CN104575493B/en active Active
- 2011-05-23 EP EP11723813.9A patent/EP2577653B1/en active Active
- 2011-05-23 CN CN201180021722.XA patent/CN103038817B/en active Active
- 2011-09-30 US US13/250,690 patent/US8219384B2/en active Active
-
2013
- 2013-04-12 US US13/862,219 patent/US20130297313A1/en not_active Abandoned
-
2014
- 2014-05-21 AU AU2014202785A patent/AU2014202785B2/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040230420A1 (en) * | 2002-12-03 | 2004-11-18 | Shubha Kadambe | Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments |
Also Published As
Publication number | Publication date |
---|---|
EP2577653A1 (en) | 2013-04-10 |
CN103038817A (en) | 2013-04-10 |
AU2014202785B2 (en) | 2015-10-29 |
CN104575493B (en) | 2019-03-26 |
AU2011258531B2 (en) | 2014-03-06 |
AU2011258531A1 (en) | 2012-10-04 |
US8219384B2 (en) | 2012-07-10 |
US20110295590A1 (en) | 2011-12-01 |
AU2014202785A1 (en) | 2014-06-12 |
US8468012B2 (en) | 2013-06-18 |
CN104575493A (en) | 2015-04-29 |
WO2011149837A1 (en) | 2011-12-01 |
CN103038817B (en) | 2015-01-14 |
US20130297313A1 (en) | 2013-11-07 |
US20120022869A1 (en) | 2012-01-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
AU2014202785B2 (en) | Acoustic model adaptation using geographic information | |
US8682659B2 (en) | Geotagged environmental audio for enhanced speech recognition accuracy | |
AU2011267982B2 (en) | Speech and noise models for speech recognition | |
US9858917B1 (en) | Adapting enhanced acoustic models | |
AU2014200999B2 (en) | Geotagged environmental audio for enhanced speech recognition accuracy |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20121019 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
DAX | Request for extension of the european patent (deleted) | ||
17Q | First examination report despatched |
Effective date: 20131004 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R079Ref document number: 602011014599Country of ref document: DEFree format text: PREVIOUS MAIN CLASS: G10L0015060000Ipc: G10L0015065000 |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
INTG | Intention to grant announced |
Effective date: 20141015 |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G10L 15/30 20130101ALI20141006BHEPIpc: G10L 15/065 20130101AFI20141006BHEP |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 715704Country of ref document: ATKind code of ref document: TEffective date: 20150415 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602011014599Country of ref document: DEEffective date: 20150423 |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: VDEPEffective date: 20150311 |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: VDEPEffective date: 20150311 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150611Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 715704Country of ref document: ATKind code of ref document: TEffective date: 20150311 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150612Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150713Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150711 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602011014599Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20150531Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20150531Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: LUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150523 |
|
26N | No opposition filed |
Effective date: 20151214 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: MM4A |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20150523 |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: PLFPYear of fee payment: 6 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R082Ref document number: 602011014599Country of ref document: DERepresentative=s name: MAIKOWSKI & NINNEMANN PATENTANWAELTE PARTNERSC, DE |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: PLFPYear of fee payment: 7 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20110523Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: TRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R082Ref document number: 602011014599Country of ref document: DERepresentative=s name: MAIKOWSKI & NINNEMANN PATENTANWAELTE PARTNERSC, DERef country code: DERef legal event code: R081Ref document number: 602011014599Country of ref document: DEOwner name: GOOGLE LLC (N.D.GES.D. STAATES DELAWARE), MOUN, USFree format text: FORMER OWNER: GOOGLE, INC., MOUNTAIN VIEW, CALIF., US |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: CDOwner name: GOOGLE INC., USEffective date: 20180213Ref country code: FRRef legal event code: CJEffective date: 20180213 |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: PLFPYear of fee payment: 8 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20150311 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20220525Year of fee payment: 12 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230505 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20230530Year of fee payment: 13 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20230529Year of fee payment: 13 |