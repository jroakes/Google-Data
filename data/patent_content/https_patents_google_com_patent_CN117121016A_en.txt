CN117121016A - Granular neural network architecture search on low-level primitives - Google Patents
Granular neural network architecture search on low-level primitives Download PDFInfo
- Publication number
- CN117121016A CN117121016A CN202280026335.3A CN202280026335A CN117121016A CN 117121016 A CN117121016 A CN 117121016A CN 202280026335 A CN202280026335 A CN 202280026335A CN 117121016 A CN117121016 A CN 117121016A
- Authority
- CN
- China
- Prior art keywords
- neural network
- input
- architecture
- sequence
- layer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 240
- 238000000034 method Methods 0.000 claims abstract description 69
- 238000010801 machine learning Methods 0.000 claims abstract description 36
- 238000010845 search algorithm Methods 0.000 claims abstract description 19
- 238000003860 storage Methods 0.000 claims abstract description 12
- 238000012549 training Methods 0.000 claims description 40
- 230000008569 process Effects 0.000 claims description 38
- 230000006870 function Effects 0.000 claims description 32
- 230000009466 transformation Effects 0.000 claims description 31
- 230000007246 mechanism Effects 0.000 claims description 30
- 230000001537 neural effect Effects 0.000 claims description 29
- 239000013598 vector Substances 0.000 claims description 26
- 238000012545 processing Methods 0.000 claims description 21
- 230000004913 activation Effects 0.000 claims description 12
- 238000010606 normalization Methods 0.000 claims description 10
- 230000036541 health Effects 0.000 claims description 9
- 230000009471 action Effects 0.000 claims description 8
- 239000003795 chemical substances by application Substances 0.000 claims description 8
- 230000035772 mutation Effects 0.000 claims description 8
- 238000000844 transformation Methods 0.000 claims description 8
- 238000013519 translation Methods 0.000 claims description 8
- 238000004422 calculation algorithm Methods 0.000 claims description 7
- 239000012634 fragment Substances 0.000 claims description 7
- 230000004044 response Effects 0.000 claims description 7
- 108091028043 Nucleic acid sequence Proteins 0.000 claims description 4
- 230000000750 progressive effect Effects 0.000 claims description 4
- 238000003745 diagnosis Methods 0.000 claims description 3
- 239000011159 matrix material Substances 0.000 claims description 3
- 238000013518 transcription Methods 0.000 claims description 3
- 230000035897 transcription Effects 0.000 claims description 3
- 230000001419 dependent effect Effects 0.000 claims 1
- 238000005070 sampling Methods 0.000 claims 1
- 238000004590 computer program Methods 0.000 abstract description 18
- 238000004891 communication Methods 0.000 description 6
- 238000010200 validation analysis Methods 0.000 description 4
- 230000006872 improvement Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000008901 benefit Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 238000011156 evaluation Methods 0.000 description 2
- 238000005259 measurement Methods 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000005192 partition Methods 0.000 description 2
- 230000002787 reinforcement Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000013526 transfer learning Methods 0.000 description 2
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 1
- 241001522296 Erithacus rubecula Species 0.000 description 1
- 239000008186 active pharmaceutical agent Substances 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 230000004888 barrier function Effects 0.000 description 1
- 230000002457 bidirectional effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 230000000295 complement effect Effects 0.000 description 1
- 230000003750 conditioning effect Effects 0.000 description 1
- 238000013480 data collection Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000008451 emotion Effects 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- JEIPFZHSYJVQDO-UHFFFAOYSA-N iron(III) oxide Inorganic materials O=[Fe]O[Fe]=O JEIPFZHSYJVQDO-UHFFFAOYSA-N 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000000873 masking effect Effects 0.000 description 1
- 230000011987 methylation Effects 0.000 description 1
- 238000007069 methylation reaction Methods 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/086—Learning methods using evolutionary algorithms, e.g. genetic algorithms or genetic programming
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/12—Computing arrangements based on biological models using genetic models
- G06N3/126—Evolutionary algorithms, e.g. genetic algorithms or genetic programming
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for determining neural network architecture. One of the methods includes receiving initial neural network architecture data; generating search space data defining a plurality of sub-model architectures from the initial neural network architecture data, each sub-model architecture comprising an ordered set of primitive neural network operations, each primitive neural network operation being associated with one or more operating parameters; and determining a final architecture of the neural network for performing the machine learning task, including running an evolutionary architecture search algorithm on the search space data to identify a respective optimized value for each of one or more operating parameters of the primitive neural network operation in at least one of the plurality of sub-model architectures.
Description
Cross Reference to Related Applications
The present application claims priority from U.S. provisional application No.63/194,889 filed 5/28 at 2021. The disclosure of the previous application is considered to be part of the disclosure of the present application and is incorporated by reference into the disclosure of the present application.
Background
The present description relates to determining architecture of a neural network.
Neural networks are machine-learning models that employ one or more layers of nonlinear units to predict the output of a received input. Some neural networks include one or more hidden layers in addition to the output layer. The output of each hidden layer is used as an input to the next layer (i.e., the next hidden layer or output layer) in the network. Each layer of the network generates an output from the received input in accordance with the current value of the corresponding parameter set.
Disclosure of Invention
The specification describes a neural architecture search system implemented as a computer program on one or more computers in one or more locations that determines a network architecture of a neural network configured to perform a particular machine learning task.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
The described techniques allow a neural network architecture search system to construct an open search space comprised of largely degenerate (degenerated) neural network architecture components from any of a variety of existing neural network architectures, and thereafter automatically and efficiently determine from the search space the final architecture of the neural network for performing a given machine learning task. In particular, the described techniques allow the system to focus on improvements to primitive (private) components of the final architecture during the search process rather than advanced building blocks.
As one example of an improvement to the architecture of a neural network that includes an attention mechanism, inserting a squaring function after a ReLU activation function and thereby creating a new type of feed-forward sub-layer for use in the network may improve nonlinear transformations within the network without additional parameters. As another example of improvement, inserting a depth-wise convolution (depth-wise convolution) function after each key, query, or value linear transformation function and thereby creating a new type of attention sub-layer for use in the network may increase the representational capacity of the network.
Thus, training of the neural network with the final architecture may require less computational resources, such as reduced processor cycles, reduced wall clock time, reduced power consumption, and thus increased computational efficiency of training. Furthermore, neural networks with final architectures may exhibit improved data efficiency, requiring fewer training steps and/or examples of training data to achieve a desired level of performance than existing architectures. Furthermore, trained examples of neural networks are lightweight (i.e., parameter efficient) and still high performance. Once trained, neural networks, despite having relatively small model sizes, are capable of performing on a variety of machine learning tasks that rival or even exceed the most advanced models, and thus are suitable for deployment on hardware platforms with limited computing resources, including, for example, mobile devices and embedded systems.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example neural architecture search system.
Fig. 2 shows an example illustration of an architecture of a neural network.
FIG. 3 is a flow chart of an example process for searching for an architecture of a neural network.
FIG. 4 illustrates an example neural network system.
Fig. 5A-B illustrate examples of operations performed by the feed-forward sub-layer of the attention layer.
FIG. 6 is a flow chart of an example process for generating an output sequence of an attention layer from an input sequence of interest.
FIG. 7 is a flow chart of an example process for generating an input sequence of interest from an input sequence.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
The specification describes a neural architecture search system implemented as a computer program on one or more computers in one or more locations that determines a network architecture of a neural network configured to perform a particular machine learning task.
Some examples of machine learning tasks that a system may be configured to perform are as follows.
As one example, the task may be a neural machine translation task. For example, if the input to the neural network is a sequence of text in one language, e.g., a sequence of words, phrases, characters, or fields, the output generated by the neural network may be a translation of the sequence of text to another language, i.e., a sequence of text in another language that is a translation of the sequence of text input. As a specific example, the task may be a multilingual machine translation task in which a single neural network is configured to translate between a plurality of different source-target language pairs. In this example, the source language text may be enhanced to have an identifier that indicates the target language into which the neural network should translate the source language text.
As another example, the task may be an audio processing task. For example, if the input to the neural network is a sequence representing a spoken utterance, e.g., a spectrogram or waveform or a feature of a spectrogram or waveform, the output generated by the neural network may be a segment of text that is a transcription of the utterance. As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may indicate whether a particular word or phrase ("hotword") was spoken in the utterance. As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may recognize the natural language of the spoken utterance.
As another example, a task may be a natural language processing or understanding task, such as an implication (entailment) task, a paraphrase (paraphrase) task, a text similarity task, a emotion (sentime) task, a sentence completion (completion) task, a grammatical task, etc., that operates on a text sequence in a natural language.
As another example, the task may be a text-to-speech task in which the input is text in natural language or features of text in natural language and the network output is a spectrogram, waveform, or other data defining audio of text spoken in natural language.
As another example, the task may be a health prediction task, where the input is a sequence derived from electronic health record data of the patient and the output is a prediction related to the future health of the patient, e.g., a predicted treatment that should be prescribed to the patient, a likelihood that the patient will have an adverse health event, or a predicted diagnosis of the patient. The sequence derived from the electronic health record data of the patient may include physiological measurements, such as a sequence of physiological measurements.
As another example, the task may be a text generation task, where the input is a text sequence and the output is another text sequence, such as a complement of the input text sequence, an answer to a question posed in the input sequence, or a text sequence related to a topic specified by the first text sequence. As another example, the input of the text generation task may be an input other than text, such as an image, and the output sequence may be text describing the input.
As another example, the task may be an image generation task, where the input is a conditioning (input) and the output is a sequence of intensity value inputs for pixels of the image.
As another example, the task may be a computer vision task, where the input is an image or a point cloud, and the output is a computer vision output for the image or the point cloud, such as a classification output, that includes a respective score for each of a plurality of categories, each score representing a likelihood that the image or the point cloud includes an object belonging to the category. When the input is an image or a point cloud, the neural network may include an embedding sub-network that generates a respective embedding for each of a plurality of tiles (patches) of the image or point cloud, and the input to the first tile of the neural network may be a sequence including the respective embedding (and optionally one or more additional embeddings, e.g., at predetermined locations that will later be used to generate the output). Each tile includes intensity values for pixels in different regions of the input image.
As another example, the task may be an agent control task, where the input is a sequence of observations or other data characterizing the state of the environment, and the output defines an action performed by the agent in response to the most recent data in the sequence. The agent may be, for example, a real world or simulated robot, a control system for an industrial facility, or a control system controlling different types of agents.
As another example, a task may be a genomics task, where inputs are sequences that identify fragments of DNA sequences or other molecular sequences, and outputs are, for example, embedding of the fragments of DNA sequences or outputs of downstream tasks by using unsupervised learning techniques on the data sets of the fragments. Examples of downstream tasks include promoter site prediction, methylation analysis, prediction of the functional effects of non-coding variants, and the like.
In some cases, the machine learning task is a combination of multiple separate machine learning tasks, i.e., the system is configured to perform multiple different separate machine learning tasks, e.g., two or more of the machine learning tasks mentioned above. For example, the system may be configured to perform a plurality of separate natural language understanding tasks, wherein the network input includes identifiers of the separate natural language understanding tasks to be performed on the network input.
FIG. 1 illustrates an example neural architecture search system 100. The neural architecture search system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations, in which the systems, components, and techniques described below may be implemented.
The neural architecture search system 100 is a system that obtains initial neural network architecture data 102, generates a neural architecture search space 110 from the initial neural network architecture data 102, and then determines an architecture 150 of the neural network by searching over the neural architecture search space 110. Search space 110 is comprised of a plurality of primitive neural network operations 134, where each primitive neural network operation is associated with one or more tunable operating parameters 136. For each primitive neural network operation, the tunable operating parameters generally define how the operation should be applied. Each of the tunable operating parameters may have a predetermined set of possible values. Some operating parameters may have values that are any value within a certain range, such as a real value constant value, while other operating parameters may take only one of a small number of possible values, such as an integer index value.
As used herein, primitive neural network operations are operations that correspond to basic mathematical concepts. For example, primitive neural network operations that make up the neural architecture search space 110 may include maximization, sine, sigmoid, logarithm, addition, multiplication (including matrix multiplication), convolution (including deep convolution), shifting, scaling, masking, and the like. In contrast, more complex operations that may be performed by the neural network, such as layer normalization and residual connection, are not primitive neural network operations, but may be constructed from a combination of two or more primitive neural network operations.
For each primitive neural network operation, the tunable operating parameters may include one or more of the following:
input1 (Input 1): a first input parameter is identified that should be used as data for a first input of an operation. For example, input1 may be an index of another operation that generates the first Input as an output. Any operation that receives at least one input may use the parameter.
Input2 (Input 2): a second input parameter is identified that should be used as data for a second input of the operation. An example of an operation using this parameter is Addition (ADD).
Constant (Constant): real value constant. An example of an operation using it is Maximization (MAX). For example, reLU activation may be represented by MAX (x, C), where c=0 is a constant.
Dimension Size: an integer representing the size of the output dimension of the transformation using the weight matrix. An example of an operation using this parameter is 1×1 convolution (CONV 1X 1).
Each primitive neural network operation may be mapped to one or more lines of a set of computer program code, i.e., a function, and the tunable operating parameters may be mapped to the arguments of the function. For example, as shown in FIG. 1, there is an operating parameter "Input 1: h0: input 2: h1: constant:0.781: dimension Size: the operation "CONV 1X1" of 512 "may be mapped to the following TensorFlow (TF) code (with the unsuitable operating parameters discarded):
tf.layers.dense(inputs＝hidden_state_0,units＝512)。
From the search space 110, the system uses the neural architecture search engine 120 to determine the architecture 140 of the neural network configured to perform a particular machine learning task.
In particular, the system 100 is configured to determine a network architecture of the neural network by determining an architecture of each of a plurality of sub-model architectures 138, each of the plurality of sub-model architectures 138 being stackable in a given order to perform successive operations on the network input to generate the network output. Each sub-model architecture in turn comprises an ordered set of primitive neural network operations. The number of primitive neural network operations within the multiple sub-model architectures may be different. The same sub-model structure may be repeated in the network architecture, i.e., the neural network may have multiple instances of the same sub-model architecture.
The system may determine a number of sub-model architectures in the neural network. The system may also determine the operations performed by each sub-model architecture, i.e., which primitive neural network operations are included in the sub-model architecture, how the primitive neural network operations are ordered in the sub-model architecture, or both. The system may also determine the connection relationships between the primitive neural network operations in each sub-model architecture, namely: which primitive neural network operations in the sub-model architecture receive input from which other primitive neural network operations. The system may also determine connection relationships between sub-model architectures in the neural network, i.e., which sub-model architecture in the neural network receives input from which other sub-model architectures.
In general, the system 100 determines the architecture of the neural network by repeatedly modifying multiple sub-model architectures, thereby generating a set of candidate architectures for the neural network and evaluating the performance of the neural network with each candidate architecture in the set with respect to the task.
Fig. 2 illustrates an architecture of an example neural network 200. The neural network 200 is configured to receive a network input 202 and generate a network output 220 of the input 202.
The neural network 200 includes a stack of sub-model architectures. The stack includes multiple sub-model architectures, e.g., sub-model architectures 204, 206, and 208 ("sub-program 0", "sub-program 1", and "sub-program 2"), which are stacked one after the other in a given order. Although fig. 2 shows a total of three sub-model architectures, the neural network 200 may have fewer or more sub-model architectures. The sub-model architectures in the stack may have the same or different number of primitive neural network operations included in each sub-model architecture.
Although the sub-model architectures are illustrated as being arranged in a sequence (with the output of any sub-model architecture being the input of another sub-model architecture except the last sub-model architecture), the sub-model architectures may also be arranged in parallel, where two or more sub-model architectures receive the same input.
For example, the submodel architecture 206 includes an ordered set of four primitive neural network operations: a 1X1 convolution operation (CONV 1X 1), followed by a sine operation (SIN), followed by a multiplication operation (MUL), followed by an addition operation (ADD). The primitive neural network operations may be indexed as h2-h5 according to their position in the ordered set, with indices h0-h1 reserved for the first and second inputs of the submodel architecture 206 itself. Here, each index may have a non-negative integer value.
In this example, the system uses the neural architecture search engine 120 to determine a value for each of one or more operating parameters of the primitive neural network operations included in the sub-model architecture 206. For example, the system may determine inputs for a given primitive neural network operation, such as inputs for a multiplication operation, by: an index of one or more previous primitive neural network operations in the ordered set of inputs is determined, for example by determining h2 and h3 as values of operating parameters associated with the multiplication operation.
The sub-model architecture 206 may then be converted into the actual computer program code 212 that defines a portion of the architecture of the neural network. The sub-model architecture 206 may also be mapped to a computation graph 214, the computation graph 214 having nodes representing primitive neural network operations and edges between nodes representing data flows between primitive neural network operations. For example, the computing graphic may be a computing graphic for execution by a graphic-based machine learning framework (e.g., a TensorFlow framework).
Depending on the particular goal and purpose of the neural architecture search process, the neural architecture search engine 120 may run any of a variety of neural architecture search algorithms, including random search, bayesian optimization, evolutionary algorithms, reinforcement Learning (RL), and gradient-based algorithms.
For example, where the goal is to find the most training efficient architecture in the search space (i.e., neural network architecture may achieve the most advanced level with less training cost), the neural architecture search engine 120 may run an evolutionary architecture search algorithm. Example evolutionary architecture search algorithms include regularized evolutionary search algorithms and evolutionary algorithms with progressive dynamic barrier (progressive dynamic hurdles), which are described in detail in U.S. patent No.11,144,831B2 entitled "Regularized Neural Network Architecture Search (regularized neural network architecture search)" submitted by month 6, month 12 of 2021 and U.S. patent No.10,997,503B2 entitled "Computationally Efficient Neural Network Architecture Search (computationally efficient neural network architecture search)" submitted by month 6, month 20 of 2019 and issued by day 4 of 2021, respectively, which are incorporated herein by reference.
Some other examples of search algorithms that may be run by the neural architecture search engine 120 include the reinforcement learning-based search algorithm described in Pham et al Efficient Neural Architecture Search via Parameter Sharing, arXiv:1802.03268 (effective neural architecture search via parameter sharing, arXiv: 1802.03268) and the gradient descent-based search algorithm described in Liu et al DARTS: differentiable Architecture Search, arXiv:1806.09055 (DARTS: differentiable architecture search, arXiv: 1806.09055), both of which are incorporated herein by reference.
The system 100 also obtains training data 104 for training the neural network to perform a particular task, and in some cases, a validation set for evaluating the performance of the neural network with respect to the particular task.
Typically, both the training data and the validation set comprise a set of neural network inputs, and include, for each network input, a respective target output that should be generated by the neural network performing the particular task. For example, a large set of training data may have been randomly partitioned to generate training data and a validation set.
The system may receive the initial neural network architecture data 102 and the training data 104 in any of a variety of ways. For example, the system may receive training data from a remote user of the system over a data communication network as an upload, e.g., using an Application Programming Interface (API) available to the system, and randomly divide the uploaded data into training data and a validation set. As another example, the system may receive input from a user specifying which data that has been maintained by the system or by another system accessible to the system should be used as the initial neural network architecture data 102, training data 104, or both.
In some cases, the initial neural network architecture data 102 received by the system 100 may include data defining the vocabulary of the primitive neural network operations, and the system uses only the received data as is to construct the neural architecture search space 110. In other cases, the initial architecture data 102 may include data defining one or more known network architectures, and the system may first convert the known architecture into a plurality of primitive operations that make up the architecture, and then use the converted primitive operations to construct the neural architecture search space 110.
For example, the initial neural network architecture data 102 may define an architecture of an attention-based neural network, namely: a neural network comprising one or more attention layers. As used herein, an attention layer is a neural network layer that includes an attention mechanism (e.g., a self-attention mechanism, such as a multi-headed self-attention mechanism). Further details regarding the mechanism of attention will be described further below with reference to fig. 4.
In this and other similar examples where the initial neural network architecture data 102 defines an architecture of a convolutional neural network, a round robin neural network, or the like, the system may partition the known architecture into a plurality of sub-model architectures, each having an initial ordered set of primitive neural network operations and each corresponding to a functional component of the architecture. For example, the system may generate one sub-model architecture corresponding to the attention layer, one sub-model architecture corresponding to the ReLU activation layer, and another sub-model architecture corresponding to the layer normalization layer. Each sub-model architecture includes an initial ordered set of one or more primitive neural network operations that facilitate the primitive operations performed by the corresponding functional architecture components.
After the search process has terminated, for example, after a specified amount of time has elapsed since the start of the search process, or after the performance of the neural network with the determined architecture has reached a specified threshold, the neural network search system 100 may then output the final architecture data 150 for the neural network. For example, the neural network search system 100 may output data specifying the final neural network architecture 150 to a user submitting the initial architecture data 102. For example, the architecture data may specify a neural network operator as part of the neural network, a connection relationship between the neural network operations, and operations performed by the neural network operator.
In some implementations, instead of or in addition to output architecture data 150, system 100 instantiates an instance of a neural network having the determined architecture and having trained parameters, for example, either trained by the system starting from zero start after determining the final architecture by using parameter values generated as a result of a search process or generated by fine tuning parameter values generated as a result of a search process, and system 100 then uses the trained neural network to process requests made by users, for example, received through APIs provided by the system. That is, the system 100 may receive input to be processed, process the input using a trained neural network, and provide output generated by the trained neural network or data derived from the generated output in response to the received input.
FIG. 3 is a flow chart of an example process 300 for searching for an architecture of a neural network. For convenience, process 300 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural architecture search system, such as the neural architecture search system 100 of fig. 1, may perform the process 300.
The system receives initial neural network architecture data (step 302). In some cases, the initial neural network architecture data received by the system may include data defining a vocabulary for primitive neural network operations, while in other cases, the initial architecture data may include data defining one or more known network architectures (e.g., known architectures of attention-based neural networks, convolutional neural networks, recurrent neural networks, etc.).
The system generates search space data from the initial neural network architecture data (step 304). The search space refers to a space of candidate network architectures from which the final architecture of the neural network can be determined. In particular, the system generates search space data defining a plurality of sub-model architectures. Each sub-model architecture includes an ordered set of one or more primitive neural network operations. Each elementary neural network operation is associated with one or more operating parameters. Each operating parameter has a predetermined set of possible values.
Where the initial architecture data includes data defining one or more known network architectures, the system may first transform (e.g., partition) the known architecture into a plurality of sub-model architectures that make up the architecture, and then construct a neural architecture search space using the transformed sub-model architectures.
The system determines the final architecture of the neural network for performing the machine learning task (step 306). In general, the system may run any of a variety of neural architecture search algorithms on the search space data to search the space of candidate network architectures to determine the final architecture. By running an appropriate neural architecture search algorithm, the system may identify each of the one or more operating parameters that operate for the primitive neural networks included in one or more of the plurality of sub-model architectures, identify a respective optimized value, which is then used to construct the final architecture of the neural network.
In some implementations, the system can run an evolutionary architecture search algorithm. For example, the evolutionary architecture search algorithm may include a regularized evolutionary algorithm or an evolutionary algorithm with progressive dynamic impediments.
Running the evolutionary architecture search algorithm involves generating a population of candidate neural network architectures from the search space data and maintaining, for each candidate neural network architecture in the population, a fitness metric representing performance of the neural network with the candidate neural network architecture with respect to the machine learning task (measure of fitness). To determine the fitness metric, after generating each new candidate architecture, the system trains instances of the neural network with the new candidate architecture on a training subset of the training data. The system trains an instance of the neural network for a particular amount of time until a termination criteria for training is met, e.g., until a next training checkpoint is reached.
The fitness metric may be determined by any metric that is appropriate for the machine learning task and that measures the performance of the neural network with respect to the machine learning task (i.e., measures the degree to which the trained new neural network performs the machine learning task). For example, if the task is an image processing task, the metrics may include various classification errors, overlap-over-unit metrics, rewards or rewards metrics, and the like. As another example, if the task is a natural language processing task, the metrics may include a bilingual evaluation replacement (BLEU) score, a recall-oriented Replacement (ROUGE) score for primary evaluation, confusion, and so forth.
During the search process, the fitness metric of each candidate architecture may be used to determine the amount of computing resources to be devoted to training the neural network with the candidate architecture, e.g., if the fitness metric of the selected architecture fails to meet a particular threshold upon reaching a training checkpoint, training of the neural network may be terminated prematurely. Once the search process is complete, the fitness metric of each candidate architecture may also be used to determine the final architecture of the neural network used to perform the machine learning task, e.g., the candidate architecture with the best fitness metric may be selected as the final architecture.
To generate a population of candidate neural network architectures, the system may first sample candidate sub-model architectures from the search space data with uniform randomness and select candidate primitive neural network operations for the sampled candidate sub-model architectures that are included in the candidate sub-model architectures. The system may then determine a candidate neural network architecture that includes at least the sampled candidate sub-model architectures and is configured to perform at least the selected candidate primitive neural network operations. To create a new candidate neural network architecture during the search process, the system may repeatedly modify the architecture in the candidate architecture set by applying one or more mutations (mutations) to the candidate architecture.
Applying these mutations to the candidate neural network architecture may include: modifying a candidate operating parameter associated with candidate primitive neural network operation, or modifying a value associated with the candidate operating parameter. The modified value may be selected from a predetermined set of possible values associated with the candidate operating parameter.
Applying the mutation may include deleting candidate primitive neural network operations.
Applying the mutation may include inserting additional primitive neural network operations sampled at uniform randomness from the search space data into the candidate sub-model architecture.
Applying the mutation may also include modifying the candidate primitive neural network operation using another primitive neural network operation included in the candidate sub-model architecture. That is, the locations of the two primitive operations included in the candidate sub-model architecture are swapped to create a new candidate architecture.
Fig. 4 illustrates an example neural network system 400. Neural network system 400 is an example of a system implemented as a computer program on one or more computers in one or more locations, in which the systems, components, and techniques described below may be implemented.
The neural network system 400 includes an attention neural network 450 having an architecture determined by using the neural architecture search technique described in this specification.
The neural network system 400 may receive the input 402 and perform a machine learning task on the input 402 to generate an output 452. The machine learning task may be any of the tasks described above.
The attention neural network 450 includes a plurality of attention layers 410. Each attention layer 410 operates on the input sequence 404 and generates a corresponding output sequence 434.
Although one attention layer is depicted in fig. 4 for convenience, as described above, the attention neural network 450 typically includes many other layers including, for example, an embedded layer, an output layer, and other attention layers.
Specifically, input sequence 404 has a respective layer input at each of one or more input locations in input order, and output sequence 434 has a respective layer output at each of one or more output locations in output order. That is, the input sequence 402 has one or more layer inputs arranged according to an input order, and the output sequence 434 has one or more layer outputs arranged according to an output order.
In general, the input sequence 404 may be any intermediate sequence data generated by the attention neural network 450 when performing machine learning tasks on the input 402. For example, the input sequence 404 may be an embedded (i.e., digital) representation of the input 402 generated by the embedding layer. As another example, the input sequence 404 may be an output sequence generated by a preceding attention layer or other layer in the attention neural network 450. As another example, when the attention neural network 450 autoregressively generates a network output, the input sequence 404 may be an embedded representation of the currently generated network output as the current time step.
To generate the output sequence 434 from the input sequence 404, each attention layer 410 includes an attention sub-layer 420 and a feed-forward sub-layer 430.
The attention sub-layer 420 receives the input sequence 404 for the layer 410 and applies an attention mechanism to the input sequence for the layer to generate an input sequence of interest 424.
Typically, to apply the attention mechanism, the attention sub-layer 420 uses one or more attention headers. Each attention header generates a set of queries Q, a set of keys K, and a set of values V, and then applies specific variants of query-key-value (QKV) attention using the queries, keys, and values to generate an output. Each query, key, or value may be in the form of a vector. In particular, the attention sub-layer 420 includes one or more depth convolution layers 422, e.g., one or more depth convolution layers per attention header. Thus, the particular variant of attention applied by the attention header differs from conventional attention in that the attention header additionally processes the set of queries, keys and values by using a deep convolution layer before directly applying the attention mechanism to the set of queries, keys and values to generate an output. When there are multiple attention heads, the attention sub-layer 120 then combines the outputs of the multiple attention heads, for example by concatenating the outputs and optionally processing the concatenated outputs through a linear layer.
Fig. 5A illustrates operations performed by the attention sub-layer 420 for the input sequence 404. The attention sub-layer 420 includes a plurality of attention headers that may perform these operations on the input sequence (e.g., in parallel) to generate an attention sub-layer's input sequence of interest 424.
To achieve multi-headed attention, the attention sub-layer 420 applies n attention mechanisms in parallel. In other words, the attention sub-layer includes n attention headers, where each attention header receives the same original query Q405, original key K406, and original value V407. Different attention heads in the attention sub-layer will typically apply different attention mechanisms. These original query Q405, original key K406, and original value V407 are derived from the input sequence 404 of the attention sub-layer 420.
Each attention header is configured to transform the original query, key, and value using the learned transformation, and then apply an attention mechanism to the transformed query, key, and value. Each attention head will typically learn a different transformation from each other.
As used in this specification, the term "learned" means that the operation or value has been adjusted during training of the attentive neural network 450.
In conventional query-key-value (QKV) attention, each attention header is configured to apply a learned query Q linear transformation to each original query to generate an attention header-specific query for each original query, to apply a learned key K linear transformation to each original key to generate an attention header-specific key for each original key, and to apply a learned value V linear transformation to each original value to generate an attention header-specific value for each original value. The attention header then uses these head-specific queries, keys, and values to apply the attention mechanism to generate an initial output of the attention sub-layer 420.
The attention mechanism applied by the attention sub-layer 420 depends on the configuration of the attention neural network 450.
As one example, when the network input is an input sequence, the attention neural network 450 includes an encoder neural network that includes a subset of the plurality of layers and encodes the input sequence to generate a respective encoded representation of each input in the sequence. In this example, the attention mechanism applied by the attention sub-layer 420 in the encoder is a self-attention mechanism, such as a multi-headed self-attention mechanism, in which queries, keys, and values are all generated from an input sequence to the attention sub-layer.
As another example, the attention neural network 450 includes a decoder neural network that includes different subsets of the multiple layers and processes the network input or an encoded representation of the network input to generate the network output. In some of these examples, when the network output is an output sequence, the decoder neural network autoregressively operates, and the attention sub-layer 420 within some or all of the layers of the decoder applies masked autoregressions to the partially generated output sequence, wherein the query, key, and value are all generated from the input sequence of the attention sub-layer 420.
When the neural network 450 includes both an encoder and a decoder, some of the layers in the decoder apply cross-attention to the encoded representation, while other layers apply self-attention to the (masked or unmasked) output sequence. In cross-attention, a query is generated from an input sequence to the attention sub-layer 420, while keys and values are generated from encoded representations of network inputs.
When the attention neural network 450 includes a decoder neural network that directly operates on an input sequence, the attention sub-layer 420 within the decoder may apply a self-attention mechanism to the input sequence.
More details of conventional query-key-value (QKV) attention are described in Attention Is All You Need, arXiv, of Vaswani et al: 1706.03762 (attention is you needed, arXiv: 1706.03762), raffel et al Exploring the Limits of Transfer Learning with aUnified Text-to-Text transducer, arXiv:1910.10683 (exploration of constraints for transfer learning using unified text-to-text converters, arXiv: 1910.10683), BERT by Devlin et al: pre-training of Deep Bidirectional Transformers for Language Understanding, arXiv:1810.04805 (BERT: pre-training of deep bi-directional transformers for language understanding, arXiv: 1810.04805), which are incorporated herein by reference.
However, in the described attention neural network 450, each attention header additionally applies a respective depth convolution function to generate an attention header-specific query, key, or value that is then used by the attention mechanism to generate an initial output of the attention sub-layer. In other words, the attention head first applies the learned query Q linear transformation to each original query to generate transformed queries 408, and then applies the learned deep convolution transformation to the transformed queries to generate attention head-specific queries 409 for each original query. The use of convolution transforms increases the representation capabilities of the attention neural network. Similarly, by using the learned linear transformation and the learned deep convolution transformation, the attention-header-specific keys and values are generated.
In the example of fig. 5A, the depth convolution layer is a depth 2-D convolution layer having a convolution kernel of size 3 x 1, where 3 is width and 1 is height, but in other examples the depth convolution layer may have a smaller or larger convolution kernel size, such as a 1 x 1 kernel or a 5 x 1 kernel.
The attention sub-layer 420 then combines the initial outputs of the attention header to generate a final output of the attention sub-layer 420. For example, the attention sub-layer concatenates the initial outputs of the attention head and applies the learned linear transformation to the concatenated outputs to generate an output of the attention sub-layer. In some cases, the input sequence of interest 424 is the final output of the attention mechanism. In some other cases, the attention sub-layer applies one or more other operations (e.g., residual connection, layer normalization, or both) to the final output to generate the input sequence of interest 424.
The feed-forward sub-layer 430 then operates on the input sequence of interest 424 to generate an output sequence 434 of the attention layer 410. The feed forward sub-layer 430 is configured to receive an input sequence of interest and apply a transform sequence to the input sequence of interest to generate an output sequence from the input sequence of interest.
In conventional attention neural networks, the transformation sequence typically comprises one linear transformation applied to the input sequence of interest, followed by an activation function, e.g. a nonlinear element-wise activation function, e.g. a linear rectification unit (ReLU) activation function, followed by another linear transformation.
However, in the described attention neural network 450, the feed forward sub-layer 430 applies a different set of transforms.
Fig. 5B illustrates operations performed by the feed-forward sub-layer 430 for the input sequence of interest 424. As shown, the feed-forward sublayer generates a first transformed input sequence 425 by applying the learned first linear transformation to the input sequence of interest 424.
The feed-forward sublayer applies, for example, a square ReLU activation function to the first transformed input sequence instead of the previously more common ReLU activation function to generate an activated input sequence 426. In particular, the squaring ReLU activation function comprises a square function f (x) =x followed by 2 ReLU activation function f (x) =max (0, x). The use of squaring functions improves the nonlinear transformation within the attention neural network without requiring additional parameters.
Feedforward sub-layerThe second transformed input sequence 427 is generated by applying the learned second linear transformation to the activated input sequence. In some cases, the second transformed input sequence 427 is used as an output sequence 434 of the attention layer 410. In other cases, the feed forward sub-layer 430 applies one or more other operations (e.g., residual connection, layer normalization, or both) to the second transformed input sequence 427 to generate an output sequence 434. In some of these cases, feed-forward sublayer 430 may apply a modified version of the layer normalization using x (x- μ) to the second transformed input sequence instead of (x- μ) that was previously more common in layer normalization 2 。
The output sequence 434 may be provided as an input to a next attention layer or other component of the attention neural network 450 for further processing or may be used to generate an output 452 of the neural network system 400.
The attention neural network 450 may include one or more output layers configured to receive an output of a final attention layer of the plurality of attention layers in the attention neural network 450. The one or more output layers are configured to process the output of the final attention layer to generate an output 452 of the neural network system 400.
Fig. 6 is a flow chart of an example process 600 for generating an output sequence of an attention layer from an input sequence of interest. For convenience, process 600 will be described as being performed by a system of one or more computers located at one or more locations. For example, a neural network system, such as the neural network system 400 of fig. 4, suitably programmed in accordance with the present description, may perform the process 600.
Generally, the system receives an input sequence of interest at a feed-forward sublayer included in an attention layer, the input sequence of interest including a respective input of interest at each of one or more locations (step 602).
The input sequence of interest has been generated by an attention sub-layer within the attention layer, the attention sub-layer being configured to receive an input sequence of the attention layer comprising a respective layer input at each of the one or more positions and to generate the input sequence of interest at least in part by applying an attention mechanism to the input sequence of the layer.
The system then performs the following steps 604-610 to generate an output sequence of the attention layer including a respective layer output at each of the one or more positions from the input sequence of interest.
In some implementations, the feed-forward sublayer can be configured to operate individually at each position in the input sequence of interest, i.e., in a position-wise (position-wise) manner. In these embodiments, the transformations applied by the feed-forward sub-layers will typically be the same for each location (however, different feed-forward sub-layers in the attention neural network will apply different transformations).
The system generates a first transformed input for each of the one or more locations through the feed-forward sub-layer at least in part by applying a first linear transformation to the layer of interest input at that location (step 604);
The system generates, for each of the one or more locations, a square ReLU-activated input at that location through a feed-forward sublayer (step 606). In particular, for each of the one or more locations, the system first applies a ReLU activation function to the first transformed input to generate a ReLU-activated input, and then applies a squaring function to the ReLU-activated input to generate a squared ReLU-activated input.
The system generates a second transformed input for each of the one or more locations through the feed-forward sublayer at least in part by applying a second linear transformation to the squared ReLU-activated input at that location (step 608).
The system generates a layer output at each of the one or more locations from the second transformed input for that location through the feed-forward sub-layer (step 610).
FIG. 7 is a flow chart of an example process 700 for generating an input sequence of interest from an input sequence. For convenience, process 700 will be described as being performed by a system of one or more computers located at one or more locations. For example, a neural network system, such as the neural network system 400 of fig. 4, suitably programmed in accordance with the present description, may perform process 700.
The system receives an input sequence at an attention layer. The input sequence has a respective layer input at each of one or more input positions in an input order. In some implementations, the input sequence may be derived from the output of a previous attention layer of the attention neural network. In some other embodiments, the input sequence may be derived from an input of an attentive neural network.
The system then performs the following steps 702-706 to generate an input sequence of interest at least in part by applying an attention mechanism to the input sequence, the input sequence of interest including a respective layer of interest input at each of the one or more output positions in output order.
The system applies a plurality of query linear transformations to the layer input or data derived from the layer input through an attention sub-layer included in the attention layer to generate a plurality of query vectors (step 702). The attention sub-layer may generate one or more query vectors for respective layer inputs at each input position in the input sequence. In some implementations, the query linear transformation applied by the attention sub-layers is the same for each input location (however, different attention sub-layers in the attention neural network will apply different transformations).
Similarly, the system applies a plurality of key linear transforms to the layer input or data derived from the layer input to generate a plurality of key vectors. The system applies a plurality of value linear transformations to layer inputs or data derived from layer inputs to generate a plurality of value vectors
The system applies a first depth convolution function to the plurality of query vectors through an attention sub-layer included in the attention layer to generate a plurality of modified query vectors (step 704).
Similarly, the system applies a second depth convolution function to the plurality of key vectors to generate a plurality of modified key vectors. The system applies a third depth convolution function to the plurality of value vectors to generate a plurality of modified value vectors.
In some implementations, the first, second, and third depth convolution layers are each a depth 2-D convolution function having a convolution kernel of finite size (e.g., size 3 x 1), where 3 is width and 1 is height, respectively.
The system applies an attention mechanism on a respective layer input in the input sequence using the plurality of modified query vectors, the plurality of modified key vectors, and the plurality of modified value vectors through an attention sub-layer included in the attention layer (step 706).
The system generates an input sequence of interest from the output of the mechanism.
For each attention layer in the attention neural network, the system may repeatedly (i.e., at each of the one or more feedforward sublayers included in the attention layer) perform process 600 to update the input sequence of interest for that layer. Likewise, the system may repeatedly (i.e., at each of the one or more attention sub-layers included in the attention layer) perform process 700 to update the input sequence of that layer. By repeatedly performing processes 600 and 700 for all of the attention layers in the attention neural network and then processing at least a portion of the output sequence generated by the last attention layer in the attention neural network using one or more output layers, the system can generate a network output for the received network input.
That is, processes 600 and 700 may be performed as part of predicting output for such inputs: for this input, the desired output, i.e. the output that should be generated by the system for the input sequence, is unknown.
The processes 600 and 700 may also be used as processing inputs derived from a set of training data-i.e., inputs derived from such a set of inputs: for the set of inputs, the output that should be generated by the system is part of a known-to train the attentive neural network to determine a training value for the parameter of the attentive neural network. The system may repeatedly perform processes 600 and 700 on inputs selected from a set of training data as part of a conventional machine learning training technique that trains the attention and output layers of the attention neural network, e.g., gradient descent using a back propagation training technique that uses a conventional optimizer (e.g., a random gradient descent, RMSprop, adafactor optimizer, or Adam optimizer), to optimize an objective function suitable for the task that the attention neural network is configured to perform. During training, the system may incorporate any number of techniques to increase the speed, effectiveness, or both of the training process. For example, the system may use a walk-off method (dropout), label smoothing, or both to reduce overfitting. As another example, the system may perform training using a distributed architecture that trains multiple instances of the attention neural network in parallel. Furthermore, the system may first pre-train the attention neural network on a larger set of unsupervised data by unsupervised learning, e.g., to minimize BERT loss or other unsupervised loss, and then fine tune the attention neural network on task-specific training data to optimize the objective function for the task.
The term "configured" is used in this specification in connection with systems and computer program components. By a system of one or more computers to be configured to perform a particular operation or action, it is meant that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operation or action. By one or more computer programs to be configured to perform a particular operation or action is meant the one or more programs comprising instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operation or action.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangible computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium, for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise dedicated logic circuitry, e.g. an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for a computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, such as in one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, such as files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of the following data: the data need not be structured in any particular way, or structured at all, and may be stored on a storage device in one or more locations. Thus, for example, an index database may include a plurality of data collections, each of which may be organized and accessed differently.
Similarly, in this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or in combination with, special purpose logic circuitry (e.g., an FPGA or ASIC).
A computer suitable for executing a computer program may be based on a general-purpose or special-purpose microprocessor or both, or may be any other kind of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or carrying out the instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such a device. Furthermore, a computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disk; CD ROM disks and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by: transmitting and receiving documents to and from devices used by the user; for example, by sending a web page to a web browser on the user's device in response to a request received from the web browser. In addition, the computer may interact with the user by sending text messages or other forms of messages to a personal device, such as a smart phone, that is running a messaging application, and in response receiving response messages from the user.
The data processing means for implementing the machine learning model may also comprise, for example, a dedicated hardware accelerator unit for handling public and computationally intensive parts of machine learning training or production, i.e. inference, workload.
The machine learning model may be implemented and deployed using a machine learning framework, such as a TensorFlow framework, microsoft Cognitive Toolkit framework, apache Single framework, or Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server; or the computing system includes a middleware component, such as an application server; or the computing system includes a front-end component, e.g., a client computer having a graphical user interface, web browser, or application through which a user may interact with embodiments of the subject matter described in this specification; or any combination including one or more such back-end components, middleware components, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data, such as HTML pages, to the user device, e.g., for the purpose of displaying data to and receiving user input from a user interacting with the device acting as a client. Data generated at the user device, e.g., results of a user interaction, may be received at the server from the user device.
While this specification contains many specifics, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings and described in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying drawings do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (28)
1. A method, comprising:
receiving initial neural network architecture data;
generating search space data defining a plurality of sub-model architectures from the initial neural network architecture data, each sub-model architecture comprising an ordered set of primitive neural network operations, each primitive neural network operation being associated with one or more operating parameters; and
determining a final architecture of a neural network for performing machine learning tasks, comprising: an evolutionary architecture search algorithm is run on the search space data to identify a respective optimized value for each of one or more operating parameters of the primitive neural network operation in at least one of the plurality of sub-model architectures.
2. The method of claim 1, wherein the evolutionary architecture search algorithm comprises a regularized evolutionary algorithm or an evolutionary algorithm with progressive dynamic impediments.
3. The method of any of claims 1-2, wherein running the evolutionary architecture search algorithm comprises:
sampling candidate sub-model architectures from the search space data with uniform randomness and selecting candidate primitive neural network operations included in the candidate sub-model architectures for the sampled candidate sub-model architectures; and
A candidate neural network architecture is determined, the candidate neural network architecture including at least the sampled candidate sub-model architecture and configured to perform at least the selected candidate primitive neural network operations.
4. The method of claim 3, wherein running the evolutionary architecture search algorithm further comprises applying mutations to the candidate neural network architecture.
5. The method of claim 4, wherein the mutation comprises modifying a candidate operating parameter associated with the candidate primitive neural network operation or modifying a value associated with the candidate operating parameter.
6. The method of claim 4, wherein the mutating comprises deleting the candidate primitive neural network operation.
7. The method of claim 4, wherein the mutating comprises inserting additional primitive neural network operations sampled at uniform randomness from the search space data into the candidate submodel architecture.
8. The method of claim 4, wherein the mutating comprises modifying the candidate primitive neural network operation using another primitive neural network operation included in the candidate sub-model architecture.
9. The method of any of claims 1-8, wherein the final architecture comprises an ordered stack of sub-model architectures.
10. The method of any of claims 1-9, wherein the primitive neural network operations comprise one or more of: maximization, sine, logarithm, addition, multiplication-the multiplication comprising matrix multiplication-or convolution-the convolution comprising a deep convolution operation.
11. The method of any of claims 1-7, further comprising: training data for training the neural network to perform the machine learning task is received, the training data including a plurality of training examples and a respective target output for each of the training examples.
12. A method according to any preceding claim when dependent on claim 3, further comprising: for the candidate neural network architecture, a fitness metric is maintained that represents performance of the neural network with the candidate neural network architecture with respect to the machine learning task.
13. The method of claim 12, wherein determining the final architecture of the neural network for performing the machine learning task further comprises:
The candidate architecture with the best fitness metric is selected as the final architecture from a population of candidate architectures that have been generated as a result of running the evolving architecture search algorithm on the search space data.
14. The method of any preceding claim, further comprising: the evolutionary architecture search algorithm is run under a graph-based machine learning framework that represents each sub-model architecture as a computational graph.
15. The method of any preceding claim, wherein the machine learning task comprises:
an audio processing task, wherein the input to the neural network is a sequence representing a spoken utterance, and the output generated by the neural network is a piece of text that is a transcription of the utterance, an indication of whether a particular word or phrase was spoken in the utterance, or an identification of a natural language of the spoken utterance;
a text-to-speech task, wherein the input to the neural network is text in natural language or features of text in natural language, and the output generated by the neural network is a spectrogram, waveform, or other data defining audio of the text spoken in the natural language;
A computer vision task, wherein the input to the neural network is an image or a point cloud, and the output generated by the neural network is a computer vision output of the image or the point cloud, and optionally wherein the computer vision output is a classification output comprising a score for each of a plurality of categories, each score representing a likelihood that the image or the point cloud includes an object belonging to that category;
an image generation task, wherein the input to the neural network is an adjustment input, and the output generated by the neural network is a sequence of intensity value inputs for pixels of an image;
a neural machine translation task, wherein the input to the neural network is a sequence of text in one language, and the output generated by the neural network is a translation of the sequence of text to another language;
an agent control task, wherein the input to the neural network is a sequence of observations or other data characterizing a state of an environment, and the output generated by the neural network defines an action performed by the agent in response to the most recent data in the sequence;
a health prediction task, wherein the input to the neural network is a sequence derived from electronic health record data of a patient, and the output generated by the neural network is a predictive diagnosis of the patient; or alternatively
A genomic task, wherein the input to the neural network is a sequence representing a fragment of a DNA sequence or other molecular sequence, and the output generated by the neural network is an embedding of the fragment for a downstream task or an output for the downstream task.
16. A system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform the operations of the respective method of any preceding claim.
17. A computer storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the respective method of any preceding claim.
18. A system for performing machine learning tasks on network inputs to generate network outputs, the system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to implement:
an attention neural network configured to perform the machine learning task, the attention neural network comprising one or more layers, each layer comprising an attention sub-layer and a feed-forward sub-layer, the attention sub-layer configured to:
Receiving an input sequence of the layers, the input sequence comprising a respective layer input at each of a plurality of locations; and
generating an input sequence of interest at least in part by applying an attention mechanism on the input sequence of the attention sub-layer, the input sequence of interest comprising a respective layer of interest input at each of the plurality of positions, and the feed-forward sub-layer configured to:
receiving the input sequence of interest; and
generating an output sequence of the feed-forward sub-layer from the input sequence of interest, the output sequence comprising a respective layer output at each of the plurality of positions, and the generating comprising, for each of the plurality of positions:
generating a first transformed input comprising applying a first linear transformation to the layer of interest input at the location;
the square ReLU activated input at the location is generated by:
applying a ReLU activation function to the first transformed input to generate a ReLU activated input, an
Applying a squaring function to the ReLU-activated input to generate a squared ReLU-activated input;
Generating a second transformed input comprising applying a second linear transformation to the squared ReLU-activated input at the location; and
the layer output at the location is generated from the second transformed input.
19. The system of claim 18, wherein generating the input sequence of interest at least in part by applying the attention mechanism to the input sequence of the attention sub-layer comprises:
applying a plurality of query linear transformations to the layer input or data derived from the layer input to generate a plurality of query vectors;
applying a first depth convolution function to the plurality of query vectors to generate a plurality of modified query vectors; and
the attention mechanism is applied on respective layer inputs in the input sequence using the plurality of modified query vectors.
20. The system of any of claims 18-19, wherein generating the input sequence of interest at least in part by applying the attention mechanism to the input sequence of the attention sub-layer comprises:
applying a plurality of key linear transforms to the layer input or data derived from the layer input to generate a plurality of key vectors;
Applying a second depth convolution function to the plurality of key vectors to generate a plurality of modified key vectors; and
the attention mechanism is applied on a respective layer input in the input sequence using the plurality of modified key vectors.
21. The system of any of claims 18-20, wherein generating the input sequence of interest at least in part by applying the attention mechanism to the input sequence of the attention sub-layer comprises:
applying a plurality of value linear transformations to the layer input or data derived from the layer input to generate a plurality of value vectors;
applying a third depth convolution function to the plurality of value vectors to generate a plurality of modified value vectors; and
the attention mechanism is applied on a respective layer input in the input sequence using the plurality of modified value vectors.
22. The system of any of claims 19-21, wherein the first, second, and third depth convolution functions each have a corresponding convolution kernel of finite size.
23. The system of claim 22, wherein the limited size comprises a width of 3.
24. The system of any of claims 18-23, wherein each layer of the attention neural network further comprises a normalization layer that applies layer normalization to the second transformed output generated by the feed forward sub-layer.
25. The system of claim 24, wherein the layer normalization comprises a modified layer normalization.
26. The system of any of claims 18-25, wherein the machine learning task comprises:
an audio processing task, wherein the network input is a sequence representing a spoken utterance and the network output is a text segment that is a transcription of the utterance, an indication of whether a particular word or phrase was spoken in the utterance, or an identification of a natural language of the spoken utterance;
a text-to-speech task, wherein the network input is a text in natural language or a feature of a text in natural language, and the network output is a spectrogram, waveform, or other data defining audio of the text spoken in the natural language;
a computer vision task, wherein the network input is an image or a point cloud and the network output is a computer vision output of the image or the point cloud, and optionally wherein the computer vision output is a classification output comprising a score for each of a plurality of categories, each score representing a likelihood that the image or the point cloud includes an object belonging to that category;
An image generation task, wherein the network input is an adjustment input and the network output is a sequence of intensity value inputs for pixels of an image;
neural machine translation tasks, wherein the network input is a sequence of text in one language and the network output is a translation of the sequence of text to another language;
an agent control task, wherein the network input is a sequence of observations or other data characterizing a state of an environment, and the network output defines an action performed by the agent in response to most recent data in the sequence;
a health prediction task, wherein the network input is a sequence derived from electronic health record data for a patient, and the network output is a predictive diagnosis of the patient; or alternatively
A genomics task, wherein the network inputs are sequences representing fragments of DNA sequences or other molecular sequences, and the network outputs are embeddings of the fragments for downstream tasks or outputs for the downstream tasks.
27. One or more computer-readable storage media storing instructions that, when executed by one or more computers, cause the one or more computers to implement the attentive neural network of any one of claims 18-26.
28. A method performed by one or more computers, the method comprising:
receiving a network input; and
use of an attention neural network according to any one of claims 18-26 to process the network input to generate a network output.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163194889P | 2021-05-28 | 2021-05-28 | |
US63/194,889 | 2021-05-28 | ||
PCT/US2022/031468 WO2022251719A1 (en) | 2021-05-28 | 2022-05-27 | Granular neural network architecture search over low-level primitives |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117121016A true CN117121016A (en) | 2023-11-24 |
Family
ID=82458449
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280026335.3A Pending CN117121016A (en) | 2021-05-28 | 2022-05-27 | Granular neural network architecture search on low-level primitives |
Country Status (4)
Country | Link |
---|---|
US (1) | US20220383119A1 (en) |
EP (1) | EP4298556A1 (en) |
CN (1) | CN117121016A (en) |
WO (1) | WO2022251719A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11922314B1 (en) * | 2018-11-30 | 2024-03-05 | Ansys, Inc. | Systems and methods for building dynamic reduced order physical models |
CN117315001B (en) * | 2023-11-27 | 2024-02-23 | 江苏房城建设工程质量检测有限公司 | Method and system for rapidly detecting building area based on laser scanning |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111602148B (en) | 2018-02-02 | 2024-04-02 | 谷歌有限责任公司 | Regularized neural network architecture search |
US10997503B2 (en) | 2019-06-20 | 2021-05-04 | Google Llc | Computationally efficient neural network architecture search |
US10685286B1 (en) * | 2019-07-30 | 2020-06-16 | SparkCognition, Inc. | Automated neural network generation using fitness estimation |
-
2022
- 2022-05-27 WO PCT/US2022/031468 patent/WO2022251719A1/en active Application Filing
- 2022-05-27 US US17/827,362 patent/US20220383119A1/en active Pending
- 2022-05-27 EP EP22738777.6A patent/EP4298556A1/en active Pending
- 2022-05-27 CN CN202280026335.3A patent/CN117121016A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022251719A1 (en) | 2022-12-01 |
US20220383119A1 (en) | 2022-12-01 |
EP4298556A1 (en) | 2024-01-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3711000B1 (en) | Regularized neural network architecture search | |
US11113479B2 (en) | Utilizing a gated self-attention memory network model for predicting a candidate answer match to a query | |
JP6790286B2 (en) | Device placement optimization using reinforcement learning | |
US11544536B2 (en) | Hybrid neural architecture search | |
US10546066B2 (en) | End-to-end learning of dialogue agents for information access | |
US20230049747A1 (en) | Training machine learning models using teacher annealing | |
WO2019155064A1 (en) | Data compression using jointly trained encoder, decoder, and prior neural networks | |
WO2018156373A1 (en) | Sequence processing using online attention | |
CN111652378B (en) | Learning to select vocabulary for category features | |
CN117121016A (en) | Granular neural network architecture search on low-level primitives | |
CN111008689B (en) | Using SOFTMAX approximation to reduce neural network inference time | |
CN111832699A (en) | Computationally efficient expressive output layer for neural networks | |
US20230107409A1 (en) | Ensembling mixture-of-experts neural networks | |
US20230029590A1 (en) | Evaluating output sequences using an auto-regressive language model neural network | |
WO2018204706A2 (en) | Recurrent neural networks for online sequence generation | |
CN115485694A (en) | Machine learning algorithm search | |
US11886976B1 (en) | Efficient decoding of output sequences using adaptive early exiting | |
US20240112027A1 (en) | Neural network architecture search over complex block architectures | |
US20240005131A1 (en) | Attention neural networks with tree attention mechanisms | |
US20240078379A1 (en) | Attention neural networks with n-grammer layers | |
JP2024519265A (en) | Neural network with feedforward spatial transformation units | |
EP4302237A1 (en) | Generating neural network outputs by cross attention of query embeddings over a set of latent embeddings | |
CN118043818A (en) | Self-attention-based neural network for processing network metrics from multiple modalities | |
JP2024506597A (en) | Generating neural network output by enriching latent embeddings using self- and mutual-attention behaviors |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |