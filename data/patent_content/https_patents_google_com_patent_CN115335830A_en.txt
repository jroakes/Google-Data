CN115335830A - Neural architecture search with weight sharing - Google Patents
Neural architecture search with weight sharing Download PDFInfo
- Publication number
- CN115335830A CN115335830A CN202180022733.3A CN202180022733A CN115335830A CN 115335830 A CN115335830 A CN 115335830A CN 202180022733 A CN202180022733 A CN 202180022733A CN 115335830 A CN115335830 A CN 115335830A
- Authority
- CN
- China
- Prior art keywords
- decision
- neural network
- classification
- architecture
- parameters
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for selecting a neural network to perform a particular machine learning task while satisfying a set of constraints.
Description
Cross Reference to Related Applications
This application claims priority to U.S. patent application serial No. 62/993,573, filed on 23/3/2020, which is incorporated herein by reference in its entirety.
Background
The present specification relates to determining the architecture of a neural network.
Neural networks are machine learning models that employ one or more layers of nonlinear units to predict output for received inputs. In addition to the output layer, some neural networks include one or more hidden layers. The output of each hidden layer is used as an input to the next layer in the network, i.e., the next hidden layer or output layer. Each layer of the network generates an output from the received input in accordance with current values of the respective set of parameters.
Disclosure of Invention
This specification describes how the system is implemented on one or more computers in one or more locales as a computer program that determines the network architecture of a neural network configured to perform a particular machine learning task.
More specifically, the system determines an architecture of a neural network that performs a machine learning task within a specified set of resource constraints. That is, the resource constraints specify constraints on how much computational resources the neural network consumes in performing the task.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
Performing machine learning tasks on computing resource-limited devices, such as mobile devices, smart devices, or other edge devices, requires a neural network architecture that is both accurate and computationally efficient. For example, the same particular neural network architecture, if deployed on slower speed devices, may again require more than an order of magnitude of inference time. Furthermore, due to hardware and device driver differences, even two devices with similar overall speeds (e.g., smart phone CPUs manufactured by different manufacturers) may be biased towards very different neural network architectures. Thus, a motivation for particular embodiments described in this specification is considerations related to how to determine neural network architectures of different sizes optimized for a particular device with particular hardware resource constraints and how to implement efficient deployment of such models on such devices.
More specifically, the described techniques may be used to search for a neural network architecture of a neural network that may perform tasks while satisfying resource constraints of resource consumption, and thus identify a single architecture or a series of architectures that may be efficiently deployed on a target set of one or more devices to compute inferences having a target latency or satisfying different resource constraints.
In particular, the novel reward functions described in this specification may allow the system to efficiently identify an architecture that, when deployed on a target device, performs well on a task and has an approximate target latency, i.e., a latency (or other target resource requirement) within an acceptable range of the target latency.
Moreover, the described techniques allow a system to identify architectures with approximate target latencies (or to satisfy certain other resource constraints) while consuming significantly less computing resources than prior techniques for searching for such architectures. In particular, by using the described reward function, the typically very computationally intensive hyper-parameter adjustment requirements are greatly reduced when new latency targets are received or deployed on different device sets.
Additionally, the amount of memory consumed by the search process may be significantly reduced through the use of rematerialization (rematerialization), as described below.
Also, the quality of the resulting architecture may be improved by searching the search space more efficiently through operations hereinafter referred to as filter warm-up and op warm-up.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example neural architecture search system.
Fig. 2 is a flow diagram of an example process for selecting a neural network to be deployed for performing a machine learning task.
FIG. 3 is a flow diagram of an example process for performing an iteration of a joint update.
FIG. 4 is a flow diagram of an example process for performing a warm-up phase of an architectural search.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This specification describes a system implemented as a computer program on one or more computers in one or more locales to determine a network architecture of a neural network configured to perform a particular machine learning task.
Typically, the system determines the architecture by searching a space of candidate architectures. Each candidate architecture in the space has a different subset of the shared set of model parameters. In other words, each candidate architecture performs a set of operations that use different subsets of the shared set of model parameters. Each candidate architecture has a subset defined by a corresponding set of decision values that includes a respective decision value for each of a plurality of classification decisions. In other words, the decision value of the classification decision specifies which operations are performed by the candidate architecture and, accordingly, which model parameters in the shared set are used by the architecture.
The system determines an architecture by jointly updating (i) a set of controller policy parameters that define a respective probability distribution for each of a plurality of classification decisions, and (ii) a shared set of parameters. After the joint update, the system may use the updated controller policy parameters to select one or more architectures.
In particular, the following description describes an architecture for a system to determine a neural network that, when deployed on a particular target set of one or more computing devices, performs a machine learning task with approximately a specified target latency, e.g., a latency that falls within a specified range of the target latency.
More generally, however, the system may determine an architecture that satisfies any of a variety of resource constraints that specify how much computing resources the neural network may consume when performing a task when deployed on a target set of computing devices.
As examples, other examples of constraints include floating point operations per second (FLOPS) performed by the neural network when performing a task and memory footprint of the neural network when deployed for performing a task, in addition to or in place of runtime latency of the neural network for performing an input or inference of a batch of inputs.
In other words, if different types of computational resource consumption constraints are used, the latency terms described below may be replaced with similarly structured terms that incorporate different resource constraints in the reward function for learning controller policy parameters.
Moreover, the following description describes that the machine learning task is a computer vision task (also referred to as an "image processing task"), and the candidate architecture space is a convolutional neural network architecture space for performing the computer vision task.
In other words, in the following description, the neural network is a convolutional neural network configured to receive an input image and process the input image to generate a network output of the input image, i.e., perform some image processing task. In this specification, processing an input image refers to processing intensity values of pixels of the image using a neural network.
For example, the task may be an image classification, and the output generated by the neural network for a given image may be a score for each object classification in a set of object classifications, each score representing an estimated likelihood that the image contains an image of an object belonging to that classification.
As another example, the task may be image embedding generation, and the output generated by the neural network may be digital embedding of the input image.
As yet another example, the task may be object detection, and the output generated by the neural network may identify a locus in the input image depicting a particular type of object, such as a bounding box or other geometric region within the image.
As yet another example, the task may be image segmentation, and the output generated by the neural network may define, for each pixel of the input image, to which of a plurality of classifications the pixel belongs.
More generally, however, the described techniques may be used to determine an architecture of a neural network that may perform any of a variety of tasks, including tasks that process inputs other than images.
As an example, if the input to the neural network is an internet resource (e.g., a web page), a document or portion of a document, or a feature extracted from an internet resource, document or portion of a document, the task may be to classify the resource or document, i.e., the output generated by the neural network for a given internet resource, document or portion of a document may be a score for each topic in the set of topics, each score representing an estimated likelihood that the internet resource, document or portion of a document is relevant to that topic.
As another example, if the input to the neural network is characteristic of an impression context (impression context) of a particular advertisement, the output generated by the neural network may be a score representing an estimated likelihood that the particular advertisement will be clicked.
As another example, if the input to the neural network is a feature of a personalized recommendation for the user, such as a feature characterizing a recommendation context, such as a feature characterizing an action previously taken by the user, the output generated by the neural network may be a score for each content item in the set of content items, each score representing an estimated likelihood that the user will respond positively to being recommended the content item.
As another example, if the input to the neural network is a text sequence in one language, the output generated by the neural network may be a score for each text segment in the set of text segments in the other language, each score representing an estimated likelihood that the text segment in the other language is a correct translation of the input text to the other language.
As another example, the task may be an audio processing task. For example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may be a score for each text segment in the set of text segments, each score representing an estimated likelihood that the text segment is a correct transcription of the utterance. As another example, the task may be a keyword detection task in which if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may indicate whether a particular word or phrase ("hotword") was spoken in the utterance. As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may recognize the natural language in which the utterance was spoken.
As another example, a task may be a natural language processing or understanding task that operates on a text sequence of a particular natural language, such as an implication task, a paraphrase task, a text similarity task, an emotion task, a sentence completion task, a grammar task, and so forth.
As another example, the task may be a text-to-speech task, where the input is text in a natural language or a text feature in a natural language, and the network output is a spectrogram or audio other data defining the text being spoken in a natural language.
As another example, the task may be a health prediction task, where the input is electronic health record data for the patient and the output is a prediction related to the patient's future health, such as a predicted treatment that should be taken on the patient, the likelihood of the patient having an adverse health event, or a predicted diagnosis of the patient.
As another example, the task may be an agent control task, where the input is an observation characterizing a state of the environment, and the output defines an action to be performed by the agent in response to the observation. For example, the agent may be a real world or simulated robot, a control system of an industrial facility, or a control system controlling a different kind of agent.
As another example, the task may be a genomics task, where the input is a sequence of fragments representing a DNA sequence or other molecular sequence, and the output is an embedding of the fragments for a downstream task-e.g., by using unsupervised learning techniques on a data set of DNA sequence fragments-or an output of the downstream task. Examples of downstream tasks include promoter site prediction, methylation analysis, prediction of functional effects of non-coding variants, and the like.
In some cases, the machine learning task is a combination of a plurality of separate machine learning tasks, i.e., the neural network is configured to perform a plurality of different separate machine learning tasks, such as two or more of the above-mentioned machine learning tasks. For example, a neural network may be configured to perform multiple separate natural language understanding tasks. Alternatively, the network input may include an identifier of a separate natural language understanding task to be performed on the network input. As another example, a neural network may be configured to perform multiple separate image processing or computer vision tasks, i.e., to generate the output of multiple different separate image processing tasks in parallel by processing a single input image.
Fig. 1 illustrates an example neural architecture search system 100. The neural architecture search system 100 is an example of a system implemented as a computer program on one or more computers in one or more locales, in which the systems, components, and techniques described below may be implemented.
The neural architecture search system 100 is a system that obtains training data 102 and validation data 104 for a particular machine learning task and uses the training data 102 and validation data 104 to select a neural network 150 for performing the task.
In general, the training data 102 and the validation data 104 both comprise a set of neural network inputs (also referred to as training or validation examples), and for each network input a respective target output that should be generated by the neural network to perform a particular task. The training data 102 and the validation data 104 may comprise different sets of neural network inputs, i.e., such that the validation data 104 may be used to effectively measure how well a neural network that has been trained on the training data 102 performs on new inputs.
The system 100 may receive the training data 102 and the validation data 104 in any of a variety of ways. For example, system 100 may receive training data as an upload from a remote user of the system over a data communication network, e.g., using an Application Programming Interface (API) provided by system 100. The system 100 may then randomly divide the received training data into training data 102 and validation data 104. As another example, the system 100 may receive input from a user specifying which data the system 100 has maintained should be used to train a neural network.
The system 100 also receives constraint data 130, e.g., from a user, the constraint data 130 specifying a target latency for performing the machine learning task after training and during inference, i.e., new input for processing a particular task after the architecture has been determined.
Generally, the target latency is a target latency of the neural network when deployed on a target set of one or more computing devices.
As one example, the target set of one or more hardware devices may be a single specific edge device, such as a mobile phone, a smart speaker, or another embedded computing device or other edge device. As a particular example, the edge device may be a mobile phone or other device having a specific type of hardware accelerator or other computer chip on which the neural network is to be deployed.
As another example, the target set of one or more hardware devices may be a set of one or more hardware accelerator devices, such as ASICs, FPGAs, or Tensor Processing Units (TPUs) on real-world agents, such as vehicles, e.g., autonomous cars, or robots.
As yet another example, the target set of one or more hardware accelerator devices may be a set of hardware accelerators in a data center.
That is, when a neural network is deployed on a target set of one or more computing devices, a target latency measurement measures the time, e.g., in milliseconds, required to perform an inference on one or more instances of a batch, i.e., to process each instance in the batch using the neural network. As a particular example, latency may measure the time required to process a batch on a particular smart device having a particular hardware configuration, such as a particular processor, a particular memory architecture, and so forth.
Thus, using the techniques described below, the system 100 can efficiently select a neural network to be deployed on a specified target set of one or more devices while having an acceptable latency, e.g., a latency approximately equal to the target latency specified in the constraint data 130.
The system 100 then uses the training set 102, the validation data 104, and the constraint data 130 to determine an architecture by searching the candidate architecture space 120.
Each candidate architecture in space 120 has a different subset of the shared set 140 of model parameters. In other words, each candidate architecture performs a set of operations that use a different subset of the shared set of model parameters.
Each candidate architecture has a subset of shared model parameters 140 defined by a corresponding set of decision values that includes a respective decision value for each of a plurality of classification decisions.
In other words, the decision value of the classification decision specifies which operations are performed by the candidate architecture and, accordingly, which model parameters from the shared collection 140 are used by the architecture.
In general, the architecture of a neural network defines the number of layers in the neural network, the operations performed by each of the layers, and the connectivity between the layers in the neural network, i.e., which layers receive input from which other layers in the neural network.
Thus, the possible values of the classification decision define one or more aspects of the architecture of the neural network, any aspect not defined by the classification decision being fixed, i.e., all architectures in the candidate architecture space are the same.
Typically, the classification decision comprises a plurality of different types of classification decisions, each classification decision corresponding to a respective point in the neural network.
As one example, the classification decision may include a binary decision that determines whether a corresponding layer (or other operation) in the neural network is skipped or included in the neural network architecture.
As another example, the classification decision may include a decision that specifies which operation or operations from the set of corresponding operations to perform at a given point in the neural network. For example, a classification decision may specify whether a given layer in the architecture is a convolutional layer, an inverted bottleneck layer, or the like. As another example, the classification decision may specify which convolution of a set of different convolutions is performed, such as by specifying a spatial size of a filter of a convolutional layer in a convolutional neural network.
As yet another example, the classification decision may include a decision specifying a number of output filters of convolutional neural network layers in a convolutional neural network. Thus, the decision values of this type of classification decision correspond to different numbers of output filters ranging from a minimum number to a maximum number.
To maximize parameter sharing, the system may specify that for each particular decision in this type of classification decision (i.e., each decision defining the number of output filters for a convolutional layer), the candidate architecture defined by the set of decision values having any given decision value for the particular classification decision still includes the convolutional neural network layer having the largest number of output filters, i.e., rather than having only the number corresponding to the given decision value, some output filters are masked.
In particular, the system may mask-i.e., zero-zeroed-output filters that are equal to the number of differences between (i) the maximum number and (ii) the number corresponding to a given decision value. Thus, the decision value of the classification decision determines which output filters of the set comprising the largest number of output filters are zeroed. By implementing this sharing in this manner, at least some of the output filters are shared across all decision values for a particular decision.
Some examples of search spaces and the set of corresponding classification decisions that define these search spaces are described in table 1 below.
TABLE 1
In particular, table 1 describes three example search spaces: proxessNAS, proxessNAS-Enlarged, and MobileNet V3-like.
The proxyless nas search space was constructed to explore the change in the infrastructure MobilenetV2, which includes a stack of inverted bottleneck layers.
In general, each inverted bottleneck layer receives a tensor having k channels, applies a 1x1 convolution to the tensor to expand the number of channels by an expansion ratio, applies a deep convolution having a given depth kernel size to the expanded tensor, and then applies a 1x1 convolution to the output of the deep convolution to reduce the number of channels. The output can optionally be combined with the input tensor using residual concatenation. In some cases, some convolutions may be separated by other operations, such as one or more of an activation function, a normalization function, or a squeeze and fire module. In the ProxessNAS search space, the expansion ratio and depth kernel size of each inverted bottleneck layer are searchable by corresponding decisions, while the number of output filters is fixed to the base size c of each layer i i . Optionally, the search space may also include a decision to determine whether to skip certain of the layers.
The ProxessNAS search space is described in more detail in Han Cai, ligeng Zhu, and Song Han.Proxessnas: direct neural architecture search on target task and hardware.arxevprint arxev: 1812.00332,2018 (Han Cai, zhu Ligeng and Song Han. Proxessnas: direct neural architecture search for target tasks and hardware. Arxev preprogram arxev: 1812.00332,2018), the entire contents of which are incorporated herein by reference.
By adding a base size c for each inverted bottleneck layer relative to each layer i i The proxyless nas-Enlarged search space expands the proxyless nas search space, with a corresponding decision to select the number of output filters for the bottleneck layer.
The MobileNet V3-like search space expands the ProxessNAS-Enlarged search space in a number of ways. Unlike the previous space, the model in this space utilizes the SiLU/Swish activation function and a tight header. These are described in Andrew Howard, mark Sandler, grace Chu, liang-Chieh Chen, bo Chen, mingxing Tan, weijun Wang, yukun Zhu, ruming Pang, vijay Vasudevan, et al Searching for mobilenetv3 (Andrew Howard, mark Sandler, grace Chu, liang Chieh Chen, chen Bo, tan Mingxing, wang Weijun, zhu Yukun, peng Reming, vijay Vasudevan et al in search MobiletNet 3), the entire contents of which are incorporated herein by reference.
The search space is also larger than the previous two. First, the inversion bottleneck expansion ratio can be selected from a larger set than other search spaces. Second, a corresponding decision is added to each inverted bottleneck to determine whether a Squeeze-and-fire (Squeeze-and-Excite) module is added to the inverted bottleneck.
While table 1 shows three example search spaces, it should be understood that the described techniques may be used to search any search space defined by the possible values of the set of classification decisions. For example, different search spaces may have layers comprised of different kinds of operations, such as different kinds of residual blocks or different kinds of convolution operations, such as separable convolutions, dilated convolutions, spatial convolutions, and the like.
Additionally, as indicated above, the values of the classification decisions may define a portion of the final architecture, with the remainder being fixed. For example, the remainder may include a fixed initial set of one or more layers or a fixed set of output layers or both.
The system 100 determines the architecture by jointly updating (i) a set of controller policy parameters 150 that define, for each classification decision of the plurality of classification decisions, a respective probability distribution, and (ii) a shared set of parameters 140.
In some embodiments, prior to starting the joint update, the system first pre-trains the shared set of parameters 140 without using the controller policy parameters 150. This "warm-up" phase of the search will be described below with reference to fig. 4.
After the joint update, the system 100 selects a candidate architecture as the architecture of the neural network, the candidate architecture being defined by a respective particular decision value for each of the plurality of classification decisions using the updated controller policy parameters 150.
The system 100 may then provide data 160 specifying the selected neural network, i.e., data specifying the architecture of the selected neural network, for deployment to perform neural network tasks, i.e., perform inference, on a target set of one or more hardware devices, e.g., through an API provided by the system 100. Alternatively or additionally, the system 100 may deploy the selected neural network on a target set of one or more hardware devices and use the selected neural network to process new network inputs received by the target set of one or more devices.
When deploying the selected neural network, the system 100 may use the values of the corresponding subset of the shared set of parameters 140, i.e., the neural network may be deployed without any additional training, or the selected neural network may be trained first further, e.g., on additional training data or for a longer time.
Fig. 2 is a flow diagram of an example process 200 for selecting a neural network to be deployed for performing a machine learning task. For convenience, process 200 will be described as being performed by a system of one or more computers located in one or more locations. For example, a suitably programmed neural architecture search system (e.g., neural architecture search system 100 of fig. 1) may perform process 200.
The system receives training data and validation data for a particular machine learning task (step 202).
The system receives constraint data that specifies a target latency for performing a particular machine learning task (step 204). For example, the target latency may be a measure of the time required to process a single input or a batch of multiple inputs through a trained neural network when deployed on a target set of computing devices.
The system uses the training data and the validation data to select an architecture from a space of candidate architectures to be deployed for the neural network to perform the machine learning task.
As described above, each candidate architecture in the space has a different subset of the shared set of model parameters defined by a set of corresponding decision values that includes a respective decision value for each of the plurality of classification decisions.
More specifically, to select an architecture, the system jointly updates (i) a set of controller policy parameters that define, for each of a plurality of classification decisions, a respective probability distribution over the decision values of that classification decision, and (ii) a shared set of parameters (step 206).
In general, the system updates a set of controller policy parameters by reinforcement learning to maximize a reward function that measures estimated quality and estimated latency of candidate architectures defined by a set of decision values sampled from a probability distribution generated using the controller policy parameters.
The reward function includes (i) a quality term that measures an estimated quality of the candidate architecture, and (ii) a latency term that is based on an absolute value of the term that compares an estimated latency of the candidate architecture to a target latency. For example, the latency term may be the product of an absolute value and a fixed negative scalar value that governs the relative contribution, i.e., relative to quality, of the latency Xiang Duijiang excitation.
By utilizing the described reward functions, the system may avoid the need to compute expensive hyper-parameter searches to determine the optimal value of the scalar value, and may reuse the same scalar value to search for architectures for multiple different tasks, multiple different target latencies, or both. In particular, other existing architectural search techniques that target a specified latency may also include quality terms and latency terms. However, when a new specified target is received, an extremely computationally intensive hyper-parameter search is required to determine the value of the scalar value that governs the relative contribution of the delay term to the reward of producing the best performing architecture that also satisfies the newly specified target delay. However, by using the described reward function, the system can reuse the same scalar value for a wide range of possible target latencies.
As a particular example, the reward function may be the sum of a quality term and a time delay term.
As another particular example, the term comparing the estimated latency of the candidate architecture to the target latency may be equal to a difference between (i) a ratio between the estimated latency of the candidate architecture and the target latency and (ii) 1.
Specifically, the reward function r (α) for a given candidate architecture α may satisfy:
where Q (α) is the quality term, β is a negative scalar, T (α) is the estimated time delay, and T0 is the target time delay.
Additionally, the system updates the shared set of model parameters to optimize an objective function that measures performance of a particular machine learning task of the candidate architecture defined by a set of decision values sampled from a probability distribution generated using the controller policy parameters.
This joint update will be described in more detail below with reference to fig. 3.
Optionally, prior to the joint update, the system may perform a "warm-up" phase in which the system updates the shared set of parameters without using (or updating) the controller policy parameters.
The execution of the pre-heating phase is described in more detail below with reference to fig. 4.
After the joint update, the system selects a candidate architecture as the architecture of the neural network, the candidate architecture being defined by a respective particular decision value for each of the plurality of classification decisions (step 208).
For example, the system may select a candidate architecture by selecting, for each of the classification decisions, the decision value having the highest probability (or equivalently, the decision value having the highest corresponding parameter value) in the probability distribution of the classification decision as the particular decision value.
FIG. 3 is a flow diagram of an example process 300 for performing an iteration of a joint update. For convenience, the process 300 will be described as being performed by a system of one or more computers located in one or more locales. For example, a suitably programmed neural architecture search system (e.g., neural architecture search system 100 of fig. 1) may perform process 300.
The system may repeatedly perform iterations of process 300 to repeatedly update the shared set of controller policy parameters and model parameters.
Based on the current values of the controller policy parameters, the system generates a respective probability distribution for each of a plurality of classification decisions (step 302).
In particular, for each classification decision, the controller policy parameters may include a respective parameter for each possible decision value for that decision. The system may generate a probability distribution for a given classification decision by applying softmax to the current value of the respective parameter for each of the possible decision values for the given decision.
Using the respective probability distributions, the system selects a respective decision value for each of a plurality of classification decisions (step 304). For example, for each classification decision, the system may sample the decision value from the probability distribution of the classification decision.
Using the validation data, the system determines an estimated quality of a particular machine learning task of the neural network having a candidate architecture with a subset of the shared set of model parameters defined by the selected decision values of the classification decision (step 306).
In particular, the system determines the estimated quality from current values of a subset of the shared set of model parameters defined by the selected decision values of the classification decision.
As a particular example, the system may determine the estimated quality as a quality of the neural network with the candidate architecture over a number of validation examples from a batch of validation data. That is, the system may process each validation input in the batch using a neural network having a candidate architecture and according to current values of a corresponding subset of the shared set of model parameters to generate a predicted output, and then use the target outputs of the validation inputs to compute the accuracy or other suitable performance metric of the machine learning task for the predicted output.
Using the validation data, the system determines an estimated latency in executing a particular machine learning task of the neural network with a candidate architecture having a subset of the shared set of model parameters defined by the selected decision values of the classification decisions (step 308).
That is, the estimated latency is an estimate of the latency, i.e., time, e.g., in milliseconds, required to perform an inference on one or more instances of a batch when the neural network is deployed on a target set of one or more computing devices. As a particular example, the estimated latency may be an estimate of the time required to process the batch on a particular smart device having a particular hardware configuration (e.g., a particular processor, a particular memory architecture, etc.).
In some implementations, when a neural network with a candidate architecture is deployed on a particular set of one or more computing devices, the system determines a latency for each of a batch of verification examples. That is, the system may process each validation input in the batch using a neural network with a candidate architecture deployed on a target set of devices to generate a prediction output and then measure the latency of processing the batch.
In some other implementations, the system may simulate the target hardware device with a hardware simulator to simulate the effect of deploying a neural network on the target device to determine the estimated latency.
In other embodiments, the system may maintain data that specifies, for each possible operation that may be performed by any candidate architecture, the time required to perform the operation on the target set of devices. The system may then determine the latency by using the maintained data to determine the time required to perform all operations in the candidate architecture on the target set of devices.
Through reinforcement learning, the system determines updates to the controller strategy parameters that improve the reward function based on the estimated quality and the estimated time delay (step 310). In particular, the system may perform an update step of a policy gradient reinforcement learning algorithm, such as the REINFORCE algorithm, on the calculated reward, i.e., the output of the reward function, to obtain an estimated quality and an estimated time delay to determine an update to the controller policy parameters.
In some cases, during a joint update, the system may exponentially increase a learning rate of a reinforcement learning update to the controller policy parameters. In particular, by using the described absolute value based reward function, the following may be the case: while the average inference time of the model using controller parameter sampling is always close to the target as training progresses, the inference time of the most likely architecture selected at the end of the search may be a few milliseconds lower (hence, a higher performance but slower architecture may still meet the delay constraint). The system can mitigate this situation by adjusting the learning rate schedule of reinforcement learning updates. Rather than using a constant learning rate throughout the search, the system may exponentially increase the reinforcement learning rate over time. This allows the controller to explore the search space at the beginning of the search (learning rate is relatively low), but also ensures that the entropy of the RL controller is low at the end of the search, preventing a mismatch between the average and the most likely inferred time.
By optimizing the appropriate objective function for a particular machine learning task, the system uses the training data to determine updates to the current values of a subset of the shared set of model parameters, which is defined by the selected decisions for the set of classification decision-sharing parameters (step 312).
For example, the system may sample a batch of training examples from the training data and perform a training step on the sampled batch using a suitable deep learning algorithm (e.g., random gradient descent) to compute a gradient update, i.e., compute the gradient of the objective function relative to the model parameter subset, and then apply a gradient update, i.e., add or subtract, to the current values of the subset.
As will be described in more detail below, in some cases, the system performs re-externalization during step 312 to reduce the memory requirements of the training. In a reiteration, when updating a shared set of parameters, the system stores only a proper subset of intermediate outputs generated by any given neural network having any given candidate architecture during a forward pass through the given neural network, and re-computes intermediate outputs not in the proper subset during a backward pass through the neural network to compute a gradient of the objective function.
By repeatedly performing process 300, the system updates the controller parameters such that decision values that produce candidate architectures that are high performance while satisfying the latency constraint are assigned a higher probability than decision values that produce low performance ones of the architectures that do not satisfy the latency requirement.
FIG. 4 is a flow diagram of an example process 400 for performing a warm-up phase of an architectural search. For convenience, process 400 will be described as being performed by a system of one or more computers located in one or more locales. For example, a suitably programmed neural architecture search system (e.g., neural architecture search system 100 of fig. 1) may perform process 400.
The system may repeatedly perform process 400, for example, until a threshold number of iterations have been performed or until a certain amount of time has elapsed. Once the last iteration of the process 400 has been performed, the system may begin performing the process 300, i.e., begin performing the joint update.
The system selects a candidate architecture from the candidate architecture space (step 402).
In particular, the system selects candidate architectures without using controller parameters. For example, the system may select candidate architectures by sampling decision values from a fixed initial probability distribution (e.g., a uniform distribution) over possible decision values for each classification decision.
However, in some embodiments, the system implements op warm-up, filter warm-up, or both in selecting the candidate architecture.
As described above, some types of classification decisions select an operation from a set of operations to perform at a corresponding point in a neural network. In some embodiments, the system implements op warming to ensure that the search space that is likely to operate during the warm-up phase is fully explored. In op preheat, for a given classification decision of this type, the system includes (i) operations represented by all of the respective decision values of the classification decision in the candidate architecture with a probability p, and (ii) samples decision values from the fixed initial probability distribution of the classification decision with a probability 1-p, and includes only the operations represented by the sampled decision values in the candidate architecture. Thus, rather than sampling a single operation with a fixed initial distribution, the system activates all possible operations of the classification decision with a probability p.
For example, the system may apply op warming to all classification decisions corresponding to selecting one of the operations. As another example, in each iteration of the process 400, the system may, for example, randomly select a fixed-size subset of these classification decisions to which op warming is applied.
When op warm-up is used, the system can linearly decrease p from 1 to 0 during the update of the shared set of parameters without updating the controller policy parameters, i.e., while performing the warm-up phase.
Filter warm-up may be used to represent classification decisions for the number of output filters of convolutional neural network layers in a convolutional neural network, and may account for some filters always being trained while others are rarely trained due to parameter sharing schemes.
Specifically, when using filter warm-up, for a particular classification decision, the system (i) configures the convolutional neural network layer with a probability q to have the largest number of output filters, where no output filter is zeroed out, and (ii) samples decision values with probabilities 1-q from a fixed initial probability distribution for the particular classification decision, and configures the convolutional neural network layer to have the largest number of output filters, but a number of output filters equal to the difference between the largest number and the number corresponding to the sampled decision values is zeroed out.
When using filter warm-up, the system linearly reduces q from 1 to 0 during updating the shared set of parameters without updating the controller policy parameters, i.e., while performing the warm-up phase.
By optimizing the objective function of a particular machine learning task, the system then uses the training data to determine updates to a subset of the shared set of model parameters in the selected candidate architecture (step 404). The system may perform this update as described above with reference to fig. 3.
When op warming is used and the system determines that operations represented by all of the respective decision values for a given classification decision are included in the candidate architecture, the system performs all of the operations represented by all of the respective decision values on the inputs of the corresponding points in the neural network and then averages the outputs of these operations to determine a single output for the corresponding point in the neural network.
When op warm-up is used, it is determined that the model parameter update may use a large amount of memory. In particular, the intermediate outputs of all operations represented by all the corresponding decision values need to be stored, so that the gradients are computed during backward propagation through the neural network. To alleviate this, in some embodiments, the system uses rematerialization.
When rematerialization is used, during forward pass through the neural network for a batch of training examples, the system applies each of the operations to each input to a point in the neural network represented by a classification decision, and calculates, for each input, an average of the outputs of the operations of the input as the output of the point in the neural network represented by the classification decision. The system then stores only the inputs of the classification decisions and the outputs of the classification decisions for back-propagation through the neural network (rather than the outputs of the individual operations). During back-pass through the neural network for a batch of training examples, i.e., when calculating the gradient, the system recalculates the output of the operation by applying each of the operations again to the stored inputs of the classification decision. Thus, the system may efficiently apply op warming without a corresponding increase in the memory requirements of the training process.
This description uses the term "configured" in connection with system and computer program components. That a system of one or more computers is configured to perform a particular operation or action means that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operation or action. That one or more computer programs are configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by the data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, tangibly embodied computer software or firmware, computer hardware (including the structures disclosed in this specification and their structural equivalents), or combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be or further comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, app, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured in any particular way or at all, and it may be stored on a storage device in one or more bits. Thus, for example, an index database may include multiple data collections, each of which may be organized and accessed in different ways.
Similarly, in this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locales. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in particular by, special purpose logic circuitry, e.g., an FPGA or an ASIC, or a combination of special purpose logic circuitry and one or more programmed computers.
A computer suitable for executing a computer program may be based on a general-purpose or special-purpose microprocessor or both or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Moreover, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game player, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices (e.g., EPROM, EEPROM, and flash memory devices); magnetic disks (e.g., internal hard disks or removable disks); a magneto-optical disk; and CD-ROM disks and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having: a display device for displaying information to a user, such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor; and a keyboard and a pointing device, such as a mouse or a trackball, by which a user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may receive documents from a device used by the user by sending the documents to the device; the user is interacted with, for example, by sending a web page to a web browser on the user's device in response to a request received from the web browser. Moreover, a computer may interact with a user by sending a text message or other form of message to a personal device (e.g., a smartphone) running a messaging application and then receiving a response message from the user.
The data processing apparatus for implementing machine learning models may also include, for example, a dedicated hardware accelerator unit for processing common and computationally intensive portions of machine learning training or production (i.e., inference, workload).
The machine learning model may be implemented and deployed using a machine learning framework, such as a TensorFlow framework, a Microsoft cognitive toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described is this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the Internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the user device, e.g., to display data to a user interacting with the device and to receive user input from the user, the device acting as a client. Data generated at the user device (e.g., a result of the user interaction) may be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings and are recited in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the present subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (23)
1. A method performed by one or more computers, the method comprising:
receiving training data and verification data for a particular machine learning task;
receiving data specifying a target latency for performing the particular machine learning task; and
selecting, from a space of candidate architectures, an architecture of a neural network to be deployed for performing the machine learning task using the training data and the validation data,
wherein each candidate architecture in the space has a different subset of a shared set of model parameters defined by a corresponding set of decision values comprising a respective decision value for each of a plurality of classification decisions, and
wherein the selecting comprises:
jointly updating (i) a set of controller policy parameters that define, for each of the plurality of classification decisions, a respective probability distribution over the decision values of that classification decision, and (ii) the shared set of parameters, wherein:
updating the set of controller policy parameters comprises updating the set of controller policy parameters by reinforcement learning to maximize a reward function that measures estimated quality and estimated latency of candidate architectures defined by a set of decision values sampled from a probability distribution generated using the controller policy parameters,
the reward function comprises a quality term measuring the estimated quality of a candidate architecture and a latency term based on an absolute value of a term comparing the estimated latency of the candidate architecture with the target latency, and
updating the shared set of model parameters comprises updating the shared set of model parameters to optimize an objective function that measures performance of the particular machine learning task of the candidate architecture defined by the set of decision values sampled from the probability distribution generated using the controller policy parameters; and
after the joint update, selecting a candidate architecture defined by a respective particular decision value for each of the plurality of classification decisions as the architecture of the neural network.
2. The method of claim 1, wherein the joint update comprises repeatedly performing operations comprising:
generating a respective probability distribution for each of the plurality of classification decisions based on the current values of the controller policy parameters,
selecting a respective decision value for each of the plurality of classification decisions using the respective probability distribution,
using the validation data, determining an estimated quality of the particular machine learning task for a neural network having the following candidate architecture: the candidate architecture having a subset of the shared set of model parameters defined by decision values of the selected classification decision, wherein the quality is estimated from current values of the subset of the shared set of model parameters defined by decision values of the selected classification decision,
using the validation data, determining an estimated latency in executing the particular machine learning task of the neural network having the candidate architecture of: the candidate architecture having the subset of the shared set of model parameters defined by the decision values of the selected classification decision,
determining updates to the controller strategy parameters based on the estimated quality and the estimated time delay by reinforcement learning, an
Determining, using the training data, updates to current values of the subset of the shared set of model parameters defined by the selected ones of the set of classification decision sharing parameters by optimizing an objective function of the particular machine learning task.
3. The method of claim 2, wherein determining the updates to current values of the subset of the shared set of model parameters comprises computing gradient updates to the current values for a batch of training examples from the training data.
4. The method of any of claim 2 or claim 3, wherein determining an estimated latency using the validation data comprises determining a latency of the neural network with the candidate architecture for each validation example from a batch of validation examples of the validation data.
5. The method of claim 4, wherein the target latency is a target latency for when the neural network is deployed on a particular set of one or more computing devices, and wherein determining latency comprises determining latency for each validation example of the batch of validation examples when the neural network with the candidate architecture is deployed on the particular set of one or more computing devices.
6. The method of any of claims 2 to 5, wherein using the validation data to determine an estimated quality of the particular machine learning task of the neural network having the candidate architecture comprises determining a quality of the neural network having the candidate architecture for a batch of validation examples from the validation data.
7. A method according to any preceding claim, wherein the reward function is the sum of the quality term and the time-lapse term.
8. The method of any preceding claim, wherein the term comparing the estimated latency of the candidate architecture to the target latency is a difference between (i) a ratio between the estimated latency of the candidate architecture and the target latency, and (ii) 1.
9. A method as claimed in any preceding claim, wherein the time delay term is the product of the absolute value and a negative scalar value.
10. The method of any preceding claim, wherein selecting, after the joint update, a candidate architecture defined by a respective particular decision value for each of the plurality of classification decisions as the architecture of the neural network comprises:
for each of the classification decisions, selecting as the particular decision value the decision value having the highest probability in the probability distribution for that classification decision.
11. The method of any preceding claim, wherein the selecting further comprises:
prior to the joint update, updating the shared set of parameters without updating the controller policy parameters by repeatedly performing operations comprising:
selecting a candidate architecture from the space; and
determining, using the training data, updates to the subset of the shared set of model parameters in the selected candidate architecture by optimizing the objective function of the particular machine learning task.
12. The method of claim 11, wherein selecting the candidate architecture comprises, for each of one or more of the classification decisions:
including in the candidate architecture with a probability p an operation represented by all of the respective decision values of the classification decision, an
Decision values are sampled from the fixed initial probability distribution of the classification decision with probabilities 1-p and only the sampled decision values are included in the candidate architecture.
13. The method of claim 12, wherein updating the shared set of parameters without updating the controller policy parameters prior to the joint update comprises:
linearly decreasing p from 1 to 0 during updating of the shared set of parameters without updating the controller policy parameters.
14. The method of any of claims 12 to 13, wherein determining the update for the selected architecture when all of the operations represented by all of the respective decision values are included in the candidate architecture comprises:
training examples for a batch during forward pass through the neural network: applying each of the operations to each input to a point in the neural network represented by the classification decision, calculating for each input an average of the outputs of the operations for that input as the output of the point in the neural network represented by the classification decision, and storing only the inputs to the classification decision and the outputs of the classification decision for back-propagation through the neural network; and
during the backward pass through the neural network for the batch of training examples, recalculating the output of the operations by re-applying each of the operations to the stored inputs to the classification decision.
15. The method of any preceding claim, wherein the space of candidate architectures is a space of architectures of convolutional neural networks, wherein a particular one of the classification decisions represents a number of output filters of convolutional neural network layers in the convolutional neural network, wherein the decision values of the particular classification decision correspond to a different number of output filters ranging from a minimum number to a maximum number, and wherein a candidate architecture defined by a set of decision values having a given decision value of the particular classification decision comprises the convolutional neural network layer having the maximum number of output filters, but wherein a number of output filters equal to a difference between the maximum number and the number corresponding to the given decision value is zeroed.
16. A method according to claim 15, when also dependent on any of claims 11 to 14, wherein selecting the candidate architecture comprises, for the particular classification decision:
configuring the convolutional neural network layer with a probability q to have the maximum number of output filters, wherein none of the output filters are zeroed, an
Taking decision values from a fixed initial probability distribution of the particular classification decision with probabilities 1-q and configuring the convolutional neural network layer to have the maximum number of output filters, but with an output filter equal to the number of differences between the maximum number and the number corresponding to sampled decision values zeroed out.
17. The method of claim 16, wherein updating the shared set of parameters without updating the controller policy parameters prior to the joint update comprises:
decreasing q linearly from 1 to 0 during updating of the shared set of parameters without updating the controller policy parameters.
18. A method according to any preceding claim, wherein for each of the classification decisions, the controller policy parameters comprise a respective parameter for each decision value of that classification decision.
19. The method of claim 18, wherein, for each of the classification decisions, the probability distribution defined by the controller policy parameters is generated by applying softmax to a respective parameter of the decision values for that classification decision.
20. The method of any preceding claim, wherein the selecting comprises:
while updating the shared set of parameters: storing only a proper subset of intermediate outputs generated by any given neural network having any given candidate architecture during forward pass through the given neural network, and re-computing intermediate outputs not in the proper subset during backward pass through the neural network to compute a gradient of the objective function.
21. The method of any preceding claim, wherein the joint update comprises:
exponentially increasing a learning rate of the reinforcement learning update to the controller strategy parameters during the joint update.
22. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform operations of the respective methods of any of claims 1-21.
23. One or more computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations of the respective methods of any of claims 1-21.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062993573P | 2020-03-23 | 2020-03-23 | |
US62/993,573 | 2020-03-23 | ||
PCT/US2021/023706 WO2021195095A1 (en) | 2020-03-23 | 2021-03-23 | Neural architecture search with weight sharing |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115335830A true CN115335830A (en) | 2022-11-11 |
Family
ID=75478325
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180022733.3A Pending CN115335830A (en) | 2020-03-23 | 2021-03-23 | Neural architecture search with weight sharing |
Country Status (4)
Country | Link |
---|---|
US (2) | US11347995B2 (en) |
EP (1) | EP4094197A1 (en) |
CN (1) | CN115335830A (en) |
WO (1) | WO2021195095A1 (en) |
Families Citing this family (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11922314B1 (en) * | 2018-11-30 | 2024-03-05 | Ansys, Inc. | Systems and methods for building dynamic reduced order physical models |
US20220147680A1 (en) * | 2020-11-12 | 2022-05-12 | Samsung Electronics Co., Ltd. | Method for co-design of hardware and neural network architectures using coarse-to-fine search, two-phased block distillation and neural hardware predictor |
US20220035877A1 (en) * | 2021-10-19 | 2022-02-03 | Intel Corporation | Hardware-aware machine learning model search mechanisms |
CN114065003A (en) * | 2021-10-27 | 2022-02-18 | 华南理工大学 | Network structure searching method, system and medium oriented to super large searching space |
WO2023096637A1 (en) * | 2021-11-23 | 2023-06-01 | Visa International Service Association | Automatic model onboarding and searching-based optimization |
US11710026B2 (en) * | 2021-11-29 | 2023-07-25 | Deepx Co., Ltd. | Optimization for artificial neural network model and neural processing unit |
US20230259716A1 (en) * | 2022-02-14 | 2023-08-17 | International Business Machines Corporation | Neural architecture search of language models using knowledge distillation |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5185772A (en) * | 1990-08-29 | 1993-02-09 | Kabushiki Kaisha Toshiba | Radiation pickup device, and radiation imaging system and method for the same |
CN111819580A (en) | 2018-05-29 | 2020-10-23 | 谷歌有限责任公司 | Neural architecture search for dense image prediction tasks |
CN110889487A (en) * | 2018-09-10 | 2020-03-17 | 富士通株式会社 | Neural network architecture search apparatus and method, and computer-readable recording medium |
US11790212B2 (en) * | 2019-03-18 | 2023-10-17 | Microsoft Technology Licensing, Llc | Quantization-aware neural architecture search |
US20210073612A1 (en) * | 2019-09-10 | 2021-03-11 | Nvidia Corporation | Machine-learning-based architecture search method for a neural network |
US20210081806A1 (en) * | 2019-09-13 | 2021-03-18 | Latent AI, Inc. | Using a runtime engine to facilitate dynamic adaptation of deep neural networks for efficient processing |
US11443235B2 (en) * | 2019-11-14 | 2022-09-13 | International Business Machines Corporation | Identifying optimal weights to improve prediction accuracy in machine learning techniques |
-
2021
- 2021-03-23 CN CN202180022733.3A patent/CN115335830A/en active Pending
- 2021-03-23 US US17/210,391 patent/US11347995B2/en active Active
- 2021-03-23 WO PCT/US2021/023706 patent/WO2021195095A1/en unknown
- 2021-03-23 EP EP21718439.9A patent/EP4094197A1/en active Pending
-
2022
- 2022-05-27 US US17/827,626 patent/US11803731B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
US11347995B2 (en) | 2022-05-31 |
EP4094197A1 (en) | 2022-11-30 |
US20220292329A1 (en) | 2022-09-15 |
US20210303967A1 (en) | 2021-09-30 |
US11803731B2 (en) | 2023-10-31 |
WO2021195095A1 (en) | 2021-09-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11651259B2 (en) | Neural architecture search for convolutional neural networks | |
US11669744B2 (en) | Regularized neural network architecture search | |
US11544536B2 (en) | Hybrid neural architecture search | |
JP7157154B2 (en) | Neural Architecture Search Using Performance Prediction Neural Networks | |
CN115335830A (en) | Neural architecture search with weight sharing | |
JP6790286B2 (en) | Device placement optimization using reinforcement learning | |
US20220092416A1 (en) | Neural architecture search through a graph search space | |
EP3602419B1 (en) | Neural network optimizer search | |
US20220383119A1 (en) | Granular neural network architecture search over low-level primitives | |
US20230063686A1 (en) | Fine-grained stochastic neural architecture search | |
US20220019869A1 (en) | Hardware-optimized neural architecture search | |
WO2022072890A1 (en) | Neural architecture and hardware accelerator search |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |