CN113170207A - More accurate 2-tap interpolation filter for video compression - Google Patents
More accurate 2-tap interpolation filter for video compression Download PDFInfo
- Publication number
- CN113170207A CN113170207A CN201980079722.1A CN201980079722A CN113170207A CN 113170207 A CN113170207 A CN 113170207A CN 201980079722 A CN201980079722 A CN 201980079722A CN 113170207 A CN113170207 A CN 113170207A
- Authority
- CN
- China
- Prior art keywords
- pixel
- sub
- filter
- interpolation filter
- weight
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000006835 compression Effects 0.000 title description 9
- 238000007906 compression Methods 0.000 title description 9
- 238000000034 method Methods 0.000 claims abstract description 65
- 239000013598 vector Substances 0.000 claims description 87
- 230000002146 bilateral effect Effects 0.000 claims description 26
- 230000004044 response Effects 0.000 claims description 11
- 238000004891 communication Methods 0.000 description 89
- 230000008569 process Effects 0.000 description 31
- 238000001914 filtration Methods 0.000 description 13
- 208000037170 Delayed Emergence from Anesthesia Diseases 0.000 description 9
- 238000010586 diagram Methods 0.000 description 9
- 238000013461 design Methods 0.000 description 7
- 230000002123 temporal effect Effects 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 5
- 230000006870 function Effects 0.000 description 4
- 239000011159 matrix material Substances 0.000 description 4
- 238000013139 quantization Methods 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 238000004590 computer program Methods 0.000 description 3
- 230000001131 transforming effect Effects 0.000 description 3
- 238000007792 addition Methods 0.000 description 2
- 230000000903 blocking effect Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 229910001416 lithium ion Inorganic materials 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- QELJHCBNGDEXLD-UHFFFAOYSA-N nickel zinc Chemical compound [Ni].[Zn] QELJHCBNGDEXLD-UHFFFAOYSA-N 0.000 description 2
- HBBGRARXTFLTSG-UHFFFAOYSA-N Lithium ion Chemical compound [Li+] HBBGRARXTFLTSG-UHFFFAOYSA-N 0.000 description 1
- 241000023320 Luma <angiosperm> Species 0.000 description 1
- 230000003044 adaptive effect Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- OJIJEKBXJYRIBZ-UHFFFAOYSA-N cadmium nickel Chemical compound [Ni].[Cd] OJIJEKBXJYRIBZ-UHFFFAOYSA-N 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 230000001747 exhibiting effect Effects 0.000 description 1
- 239000000446 fuel Substances 0.000 description 1
- 229910052751 metal Inorganic materials 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 229910052987 metal hydride Inorganic materials 0.000 description 1
- OSWPMRLSEDHDFF-UHFFFAOYSA-N methyl salicylate Chemical compound COC(=O)C1=CC=CC=C1O OSWPMRLSEDHDFF-UHFFFAOYSA-N 0.000 description 1
- 229910052759 nickel Inorganic materials 0.000 description 1
- PXHVJJICTQNCMI-UHFFFAOYSA-N nickel Substances [Ni] PXHVJJICTQNCMI-UHFFFAOYSA-N 0.000 description 1
- -1 nickel metal hydride Chemical class 0.000 description 1
- 239000013307 optical fiber Substances 0.000 description 1
- 238000012545 processing Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 230000004043 responsiveness Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/157—Assigned coding mode, i.e. the coding mode being predefined or preselected to be further used for selection of another element or parameter
- H04N19/159—Prediction type, e.g. intra-frame, inter-frame or bidirectional frame prediction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/117—Filters, e.g. for pre-processing or post-processing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/172—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a picture, frame or field
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/513—Processing of motion vectors
- H04N19/517—Processing of motion vectors by encoding
- H04N19/52—Processing of motion vectors by encoding by predictive encoding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/523—Motion estimation or motion compensation with sub-pixel accuracy
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/587—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal sub-sampling or interpolation, e.g. decimation or subsequent interpolation of pictures in a video sequence
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/80—Details of filtering operations specially adapted for video compression, e.g. for pixel interpolation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/80—Details of filtering operations specially adapted for video compression, e.g. for pixel interpolation
- H04N19/82—Details of filtering operations specially adapted for video compression, e.g. for pixel interpolation involving filtering within a prediction loop
Abstract
A method for encoding a block of video using inter prediction comprising: selecting a first sub-pixel interpolation filter for sub-pixel interpolation of fractional motion during motion estimation for a block; selecting a second sub-pixel interpolation filter based on the first sub-pixel interpolation filter during motion compensation; and encoding the second sub-pixel interpolation filter in the compressed bitstream. The first sub-pixel interpolation filter is a 2-tap filter. The first sub-pixel interpolation filter includes a weight tuple. Each weight tuple comprises two weights and is used for sub-pixel interpolation at a sub-pixel position between the first pixel and the second pixel. For at least one of the weight tuples, the two weights are not based on a first distance between the sub-pixel location and the first pixel and a second distance between the sub-pixel location and the second pixel.
Description
Background
Digital video may be used, for example, for remote business conferencing via video conferencing, high definition video entertainment, video advertising, or sharing of user-generated video. Due to the large amount of data involved in video data, high performance compression is required for both transmission and storage. Various methods have been proposed to reduce the amount of data in a video stream, including compression and other encoding and decoding techniques. These techniques may involve sub-pixel interpolation for fractional motion.
Disclosure of Invention
One aspect is a method for encoding a block of video using inter-prediction. The method comprises the following steps: selecting a first sub-pixel interpolation filter for sub-pixel interpolation of fractional motion during motion estimation for a block; selecting a second sub-pixel interpolation filter based on the first sub-pixel interpolation filter during motion compensation; and encoding the second sub-pixel interpolation filter in the compressed bitstream. The first sub-pixel interpolation filter is a 2-tap filter. The first sub-pixel interpolation filter includes a weight tuple. Each weight tuple comprises two weights and is used for sub-pixel interpolation at a sub-pixel position between the first pixel and the second pixel. For at least one of the weight tuples, the two weights are not based on a first distance between the sub-pixel location and the first pixel and a second distance between the sub-pixel location and the second pixel.
Another aspect is an apparatus for encoding a block of video using inter-prediction, the apparatus comprising a processor and a memory. The memory includes instructions executable by the processor to: selecting a first sub-pixel interpolation filter for sub-pixel interpolation of fractional motion during motion estimation for a block; selecting a second sub-pixel interpolation filter based on the rate-distortion cost; and encoding the second sub-pixel interpolation filter in the compressed bitstream. The first sub-pixel interpolation filter is a 2-tap filter. The first sub-pixel interpolation filter includes a weight tuple. Each weight tuple comprises two weights and is used for sub-pixel interpolation at a sub-pixel position between the first pixel and the second pixel. For at least one of the weight tuples, the two weights are not based on a first distance between the sub-pixel location and the first pixel and a second distance between the sub-pixel location and the second pixel.
Another aspect is an apparatus for decoding a block of video using inter-prediction, the apparatus comprising a processor and a memory. The memory includes instructions executable by the processor to: decoding a sub-pixel interpolation filter from the encoded bitstream, the sub-pixel interpolation filter for sub-pixel interpolation of fractional motion; decoding a motion vector from the encoded bitstream; and decoding the block using the sub-pixel interpolation filter and the motion vector. The sub-pixel interpolation filter is a 2-tap filter. The sub-pixel interpolation filter includes a weight tuple. Each weight tuple comprises two weights and is used for sub-pixel interpolation at a sub-pixel position between the first pixel and the second pixel. For at least one of the weight tuples, the two weights are not based on a first distance between the sub-pixel location and the first pixel and a second distance between the sub-pixel location and the second pixel.
Variations of these and other aspects will be described in more detail below.
Drawings
The description herein makes reference to the accompanying drawings wherein like reference numerals refer to like parts throughout the several views.
FIG. 1 is an illustration of a computing device according to an embodiment of the present disclosure.
Fig. 2 is an illustration of a computing and communication system according to an embodiment of the present disclosure.
Fig. 3 is an illustration of a video stream for encoding and decoding according to an embodiment of the present disclosure.
Fig. 4 is a block diagram of an encoder according to an embodiment of the present disclosure.
Fig. 5 is a block diagram of a decoder according to an embodiment of the present disclosure.
Fig. 6 is a block diagram of motion vectors with sub-pixel accuracy according to an embodiment of the present disclosure.
Fig. 7 is an illustration of a bilateral filter according to an embodiment of the present disclosure.
Fig. 8 illustrates a 2-tap filter similar to a higher order kernel in accordance with an embodiment of the present disclosure.
Fig. 9 is a flowchart of an example of a process for encoding a block of video using inter prediction according to an embodiment of the present disclosure.
Fig. 10 is a flowchart of an example of a process for decoding a block of video using inter prediction according to an embodiment of the present disclosure.
Detailed Description
The video compression scheme may include: dividing each image or frame into smaller portions, such as blocks; and generating an output bitstream using a technique for limiting information included in each block in the output. The encoded bitstream can be decoded to recreate the source image from limited information. In some embodiments, the information included in each block in the output may be limited by reducing spatial redundancy, reducing temporal redundancy, or a combination thereof. For example, temporal redundancy or spatial redundancy may be reduced by: the method includes predicting a frame based on information available to both an encoder and a decoder, and including information representing a difference or residual between the predicted frame and an original frame.
Encoding using temporal similarity (e.g., redundancy) may be referred to as inter-prediction. Inter-frame prediction (e.g., motion compensated prediction) attempts to predict the pixel values of a block using one or more blocks from a temporally adjacent frame (i.e., reference frame) or frames that may be displaced. That is, temporal redundancy between video frames may be exploited using Motion Compensated Prediction (MCP). Temporally adjacent frames are frames that occur earlier or later in time in the video stream than the frames of the block being encoded.
The accuracy of MCP determines to a large extent the compression ratio. During encoding, and as described further below, video frames may be divided into blocks of various sizes. For each block, motion estimation is performed. For a current block being encoded (i.e., a current block), motion estimation performs a motion search to find a motion vector for the block. The motion vector indicates the position of the prediction block in a reference frame (i.e., a temporally adjacent frame). The prediction block is used to predict a block. In general, the motion vector may be a fraction. That is, a motion vector may refer to (e.g., point to) a sub-pixel location in a reference frame. That is, the motion vector can be given in sub-pixel units. In other words, the motion vector may be expressed in sub-pixel resolution. Different coding standards support different motion vector resolutions (i.e., sub-pixel units, precision). For example, in the HEVC standard, luma motion vectors may be given in quarter-luma sample units. For example, in the VP9 standard, the maximum motion vector accuracy supported is 1/16 pixels.
The sub-pixel values given by the motion vectors can be calculated by interpolation using a filter such as a Finite Impulse Response (FIR) filter. Different interpolation filters may have different numbers of taps that indicate how many neighboring pixels are used to interpolate the sub-pixel values. A weight (i.e., coefficient) may be associated with each tap. In most video codecs, the number of taps may range from 2 to 8. For example, in view of having eight weights W ═ W (W)0，w1，...，w7) And eight (integer or sub-integer) pixel values a ═ a (a0，A1，...，A7) May calculate the interpolated pixel value I as a weighted sum
As mentioned, MCP may involve the use of a sub-pixel interpolation filter that generates filtered sub-pixel values at defined locations between full pixels (also referred to as integer pixels) along a row, column, or both. The interpolation filter may be one of several interpolation filters available for use in MCP. Each of the interpolation filters may have a different frequency profile (e.g., frequency response).
The sub-pixel interpolation filter may be selected from a set of predefined or available sub-pixel interpolation filter types. For example, in the VP9 standard, predefined subpixel interpolation filter types include a SMOOTH interpolation filter (eggttop _ SMOOTH), a SHARP interpolation filter (eggttop _ SHARP), a normal interpolation filter (eggttop), and a bilinear interpolation filter (bilinear). AV1 includes similarly named interpolation filters. The bilinear filter is further described below with respect to fig. 7. Other codecs may have fewer, more, or other interpolation filters. A description of the semantics (e.g., frequency response, tap weights, etc.) of the interpolation filter type is not necessary for an understanding or use of the present disclosure.
The normal interpolation filter (EIGHTTAP) may be an 8-tap lagrangian interpolation filter. The SHARP interpolation filter (eggttop _ SHARP) may be a DCT-based interpolation filter, have a sharper response, and are used primarily around sharper edges. The SMOOTH interpolation filter (eggttop _ SMOOTH) may be an 8-tap filter, and the 8-tap filter may be designed using a windowed fourier series approach with a hamming window. As will be appreciated by those skilled in the art, techniques such as
A longer interpolation filter (e.g., an 8-tap filter) may provide higher prediction accuracy than a shorter filter (e.g., a 2-tap filter). For this reason, using a longer interpolation filter results in higher compression. However, while longer filters (e.g., 8-tap filters) provide higher prediction accuracy, shorter filters (e.g., 2-tap filters) have lower complexity. The complexity of a coding algorithm can be measured in terms of memory bandwidth and the number of operations required by the algorithm. Utilizing a longer tap filter (e.g., an 8 tap filter) increases the amount of data to be fetched from memory compared to a shorter filter (e.g., a 2 tap filter) in terms of memory bandwidth. Similarly, a longer tap length filter (e.g., an 8-tap filter) increases the number of arithmetic operations required to obtain the interpolated value. For example, an 8-tap filter performs a total of 15 multiply and add operations (MAC); i.e. 8 multiplications and 7 additions. On the other hand, a 2-tap filter performs only 3 MACs; i.e. 2 multiplications and 1 addition.
The interpolation operation using the interpolation filter may be performed during a motion estimation phase in the encoding process and during a motion compensation phase in both the encoding process and the decoding process. The design of the interpolation filter may affect the coding performance as well as the encoding speed and decoding speed. The design of the interpolation filter may include the design (e.g., selection) of weights (e.g., coefficients) for the interpolation operation.
The interpolation filter to be used by the decoder to generate the prediction block may be signaled in the coded bitstream in the header of the frame containing the block to be predicted. To this end, the same interpolation filter may be used to generate sub-pixel prediction blocks for all blocks of the frame. The interpolation filter may also be signaled at the coding unit level. To this end, the same interpolation filter may be used for each block of the coding unit (e.g., each prediction block), thereby generating a sub-pixel prediction block for the block of the coding unit. The coding units may be 64x64, smaller or larger blocks of pixels of a video frame, and may be further partitioned into smaller blocks. The encoder may generate a prediction block based on each of the available interpolation filters.
The encoder may select (e.g., signal the decoder in the encoded bitstream) an interpolation filter that results in, for example, an optimal rate-distortion cost. Rate-distortion cost refers to the ratio at which a balance is struck between the amount of distortion (i.e., loss of video quality) and the rate (i.e., number of bits) required for encoding.
Alternatively, the encoder (during the motion estimation process) may use a low complexity filter (such as a 2-tap filter) to identify the best sub-pixel resolution (i.e., the sub-pixel resolution corresponding to the best prediction), and then encode a longer interpolation filter in the bitstream for interpolation at the identified sub-pixel resolution during motion compensation. For example, motion estimation may be performed (e.g., implemented, etc.) using an interpolation filter that the encoder chooses when finding the motion vector. In an example, one interpolation FILTER may be a 2-tap FILTER, such as the bilateral FILTER 1 mentioned below. Subsequently, motion compensation can be performed using the found motion vectors. The motion compensation stage may include an interpolation filter search. That is, different candidate (e.g., available) interpolation filters may be tried, and one interpolation filter resulting in the best rate-distortion cost may be selected. The selected interpolation filter may be transmitted in the bitstream so that the encoder and decoder may use the same interpolation filter. As described further below, and to achieve better coding performance, the taps of the interpolation filter selected are typically equal to or greater than the taps of the interpolation filter used during motion estimation. To this end, the selected filter may be one of a 2-tap filter, an EIGHTTAP _ SHARP, or an EIGHTTAP _ SMOOTH filter, which will be further described below.
In addition, some real-time video applications (such as video conferencing, screen sharing, etc.) may sacrifice quality in order to improve the responsiveness of the application. For example, instead of using a longer interpolation filter, a 2-tap interpolation filter may be used in both motion estimation and motion compensation. An example of a 2-tap filter is a bilateral filter that uses a triangular kernel as described with respect to fig. 7.
The coding performance using the bilateral filter may not be as optimal as that achievable using a filter with a higher number of taps. For this reason, it is desirable to use a 2-tap filter that can provide coding performance that approximates that of a longer filter and is better than that of a bilateral filter.
Using interpolation filters with low complexity (e.g., interpolation filters with a smaller number of taps) but providing high prediction accuracy that approximates the prediction accuracy of longer interpolation filters may improve the overall performance of the video codec.
Embodiments according to the present disclosure may use weight redistribution triangle kernels (e.g., filters) to obtain coding performance that approximates the performance of longer interpolation filters. As is well known, the triangular filter described further with respect to fig. 7 is a 2-tap convolution filter that calculates, at a sample point (i.e., an interpolated pixel value), a weighted average of two adjacent pixels from the distance of the sample point to each of the two adjacent pixels. That is, the weight is inversely proportional to the distance.
The weight redistribution triangular filter is such that for at least some of the sample points (i.e., sample positions, sub-pixel positions, etc.), the weights are not proportional (e.g., inversely proportional) to the distance. Using the 2-tap interpolation filter described herein better preserves edges in the video frame and/or provides more accurate prediction for at least some video blocks. For a set of videos with edges, using the filter described herein (i.e., weight redistribution triangle kernel) may result in a bit rate reduction of 0.5% to 0.8%.
Fig. 1 is an illustration of a computing device 100 according to an embodiment of the present disclosure. Computing device 100 may include communication interface 110, communication unit 120, User Interface (UI)130, processor 140, memory 150, instructions 160, power source 170, or any combination thereof. As used herein, the term "computing device" (e.g., an apparatus) includes any unit or combination of units capable of performing any of the methods, processes, or portions thereof disclosed herein.
The communication interface 110 may be a wireless antenna, a wired communication port such as an ethernet port, an infrared port, a serial port, as shown, or any other wired or wireless unit capable of interfacing with a wired or wireless electronic communication medium 180.
The communication unit 120 may be configured to transmit or receive signals via a wired or wireless medium 180. For example, as shown, the communication unit 120 is operatively connected to an antenna configured to communicate via wireless signals. Although not explicitly shown in fig. 1, the communication unit 120 may be configured to transmit, receive, or both via any wired or wireless communication medium, such as Radio Frequency (RF), Ultraviolet (UV), visible light, optical fiber, metal wire, or a combination thereof. Although fig. 1 shows a single communication unit 120 and a single communication interface 110, any number of communication units and any number of communication interfaces may be used.
The UI 130 may include any unit capable of interfacing with a user, such as a virtual or physical keyboard, a touch pad, a display, a touch display, a speaker, a microphone, a video camera, a sensor, or any combination thereof. UI 130 may be operatively coupled with a processor as shown or with any other element of computing device 100, such as power source 170. Although shown as a single unit, the UI 130 may include one or more physical units. For example, the UI 130 may include an audio interface for performing audio communication with a user, and a touch display for performing visual and touch-based communication with the user. Although shown as separate units, the communication interface 110, the communication unit 120, and the UI 130, or portions thereof, may be configured as a combined unit. For example, the communication interface 110, the communication unit 120, and the UI 130 may be implemented as a communication port that can be engaged with an external touch screen device.
The instructions 160 may include instructions for performing any of the methods, processes, or any one or more portions thereof disclosed herein. The instructions 160 may be implemented in hardware, software, or any combination thereof. For example, the instructions 160 may be implemented as information stored in the memory 150, such as a computer program that may be executed by the processor 140 to perform any of the respective methods, algorithms, aspects, or combinations thereof, as described herein. The instructions 160, or portions thereof, may be implemented as a special purpose processor or circuitry that may include dedicated hardware for implementing any one of the methods, algorithms, aspects, or combinations thereof, as described herein. Portions of instructions 160 may be distributed among multiple processors on the same machine or on different machines, or distributed over a network such as a local area network, a wide area network, the internet, or a combination thereof.
The power source 170 may be any suitable device for powering the computing device 100. For example, power source 170 may include a wired power source; one or more dry cell batteries such as nickel cadmium (NiCd), nickel zinc (NiZn), nickel metal hydride (NiMH), lithium ion (Li-ion); a solar cell; a fuel cell; or any other device capable of powering computing device 100. Communication interface 110, communication unit 120, UI 130, processor 140, instructions 160, memory 150, or any combination thereof may be operatively coupled with power source 170.
Although shown as separate elements, communication interface 110, communication unit 120, UI 130, processor 140, instructions 160, power supply 170, memory 150, or any combination thereof, may be integrated in one or more electronic units, circuits, or chips.
Fig. 2 is an illustration of a computing and communication system 200 according to an embodiment of the present disclosure. The computing and communication system 200 may include one or more computing and communication devices 100A/100B/100C, one or more access points 210A/210B, one or more networks 220, or a combination thereof. For example, the computing and communication system 200 may be a multiple access system that provides communications, such as voice, data, video, messaging, broadcast, or a combination thereof, to one or more wired or wireless communication devices, such as the computing and communication devices 100A/100B/100C. Although FIG. 2 shows three computing and communication devices 100A/100B/100C, two access points 210A/210B, and one network 220 for simplicity, any number of computing and communication devices, access points, and networks may be used.
Computing and communication device 100A/100B/100C may be, for example, a computing device, such as computing device 100 shown in FIG. 1. For example, as shown, computing and communication devices 100A/100B may be user devices such as mobile computing devices, notebook computers, thin clients, or smart phones, and computing and communication device 100C may be a server such as a mainframe or cluster. Although the computing and communication devices 100A/100B are described as user devices and the computing and communication device 100C is described as a server, any computing and communication device may perform some or all of the functions of a server, some or all of the functions of a user device, or some or all of the functions of a server and a user device.
Each computing and communication device 100A/100B/100C may be configured to perform wired or wireless communication. For example, computing and communication devices 100A/100B/100C may be configured to transmit or receive wired or wireless communication signals and may include User Equipment (UE), mobile stations, fixed or mobile subscriber units, cellular phones, personal computers, tablet computers, servers, consumer electronics, or any similar device. Although each computing and communication device 100A/100B/100C is shown as a single unit, the computing and communication devices may include any number of interconnected elements.
Each access point 210A/210B may be any type of device configured to communicate with the computing and communication device 100A/100B/100C, the network 220, or both via a wired or wireless communication link 180A/180B/180C. For example, the access points 210A/210B may include base stations, Base Transceiver Stations (BTSs), node Bs, enhanced node Bs (eNode-Bs), home node Bs (HNode-Bs), wireless routers, wired routers, hubs, repeaters, switches, or any similar wired or wireless devices. Although each access point 210A/210B is shown as a single unit, the access points may include any number of interconnected elements.
The network 220 may be any type of network configured to provide services such as voice, data, applications, voice over internet protocol (VoIP), or any other communication protocol or combination of communication protocols over wired or wireless communication links. For example, the network 220 may be a Local Area Network (LAN), a Wide Area Network (WAN), a Virtual Private Network (VPN), a mobile or cellular telephone network, the Internet, or any other electronic communication means. The network may use a communication protocol such as Transmission Control Protocol (TCP), User Datagram Protocol (UDP), Internet Protocol (IP), real-time transport protocol (RTP) hypertext transport protocol (HTTP), or a combination thereof.
The computing and communication devices 100A/100B/100C may communicate with each other via the network 220 using one or more wired or wireless communication links, or via a combination of wired and wireless communication links. For example, as shown, computing and communication devices 100A/100B may communicate via wireless communication links 180A/180B, and computing and communication device 100C may communicate via wired communication link 180C. Any of the computing and communication devices 100A/100B/100C may communicate using any one or more wired or wireless communication links. For example, a first computing and communication device 100A may communicate via a first access point 210A using a first type of communication link, a second computing and communication device 100B may communicate via a second access point 210B using a second type of communication link, and a third computing and communication device 100C may communicate via a third access point (not shown) using a third type of communication link. Similarly, the access points 210A/210B may communicate with the network 220 via one or more types of wired or wireless communication links 230A/230B. Although FIG. 2 shows the computing and communication devices 100A/100B/100C communicating via the network 220, the computing and communication devices 100A/100B/100C may communicate with each other via any number of communication links, such as direct wired or wireless communication links.
Other implementations of the computing and communication system 200 are possible. For example, in an embodiment, the network 220 may be an ad hoc network, and one or more of the access points 210A/210B may be omitted. Computing and communication system 200 may include devices, units, or elements not shown in fig. 2. For example, the computing and communication system 200 may include many more communication devices, networks, and access points.
Fig. 3 is an illustration of a video stream 300 for encoding and decoding according to an embodiment of the disclosure. Video stream 300 (such as a video stream captured by a video camera or a video stream generated by a computing device) may include video sequence 310. Video sequence 310 may include a sequence of contiguous frames 320. Although three contiguous frames 320 are shown, the video sequence 310 may include any number of contiguous frames 320. Each frame 330 from the contiguous frames 320 may represent a single image from the video stream. Frame 330 may include blocks 340. Although not shown in fig. 3, a block may include pixels. For example, a block may include 16 × 16 pixel groups, 8 × 8 pixel groups, 8 × 16 pixel groups, or any other pixel groups. Unless otherwise indicated herein, the term "block" may include a super-block, a macroblock, a slice, or any other portion of a frame. A frame, block, pixel, or combination thereof may include display information, such as luminance information, chrominance information, or any other information that may be used to store, modify, convey, or display a video stream or a portion thereof.
Fig. 4 is a block diagram of an encoder 400 according to an embodiment of the present disclosure. The encoder 400 may be implemented in a device, such as the computing device 100 shown in fig. 1 or the computing and communication devices 100A/100B/100C shown in fig. 2, for example, as a computer software program stored in a data storage unit, such as the memory 150 shown in fig. 1. The computer software program may include machine instructions executable by a processor, such as processor 140 shown in fig. 1, and may cause an apparatus to encode video data as described herein. Encoder 400 may be implemented, for example, as specialized hardware included in computing device 100.
The encoder 400 may encode a video stream 402, such as the video stream 300 shown in fig. 3, to generate an encoded (compressed) bitstream 404. In some implementations, the encoder 400 may include a forward path for generating the compressed bitstream 404. The forward path may include an intra/inter prediction unit 410, a transform unit 420, a quantization unit 430, an entropy coding unit 440, or any combination thereof. In some implementations, the encoder 400 may include a reconstruction path (indicated by the broken connecting lines) to reconstruct the frame for encoding other blocks. The reconstruction path may include a dequantization unit 450, an inverse transform unit 460, a reconstruction unit 470, a loop filtering unit 480, or any combination thereof. Other structural variations of the encoder 400 may be used to encode the video stream 402.
To encode the video stream 402, each frame within the video stream 402 may be processed in units of blocks. Accordingly, a current block may be identified from among blocks in a frame, and the current block may be encoded.
At the intra/inter prediction unit 410, the current block may be encoded using intra prediction, which may be within a single frame, or inter prediction, which may be between frames. Intra-prediction may include generating a prediction block from samples in the current frame that have been previously encoded and reconstructed. Inter-prediction may include generating a prediction block from samples in one or more previously constructed reference frames. Generating a prediction block for a current block in a current frame may include performing motion estimation to generate a motion vector indicative of an appropriate reference block in a reference frame.
The intra/inter prediction unit 410 may subtract the prediction block from the current block (original block) to generate a residual block. Transform unit 420 may perform a block-based transform, which may include transforming the residual block into transform coefficients in, for example, the frequency domain. Examples of block-based transforms include the Karhunen-loeve transform (KLT), the Discrete Cosine Transform (DCT), and the singular value decomposition transform (SVD). In an example, DCT may include transforming a block into the frequency domain. DCT may include using spatial frequency based transform coefficient values with the lowest frequency (i.e., DC) coefficient in the upper left corner of the matrix and the highest frequency coefficient in the lower right corner of the matrix.
The quantization unit 430 may convert the transform coefficients into discrete quantum values, which may be referred to as quantized transform coefficients or quantization levels. The quantized transform coefficients may be entropy encoded by entropy encoding unit 440 to produce entropy encoded coefficients. Entropy encoding may include using a probability distribution metric. Entropy encoded coefficients and information for decoding the block, which may include the type of prediction used, the motion vector, and the quantizer value, may be output to the compressed bitstream 404. The compressed bit stream 404 may be formatted using various techniques such as Run Length Encoding (RLE) and zero run length coding.
The reconstruction path may be used to maintain reference frame synchronization between the encoder 400 and a corresponding decoder, such as the decoder 500 shown in fig. 5. The reconstruction path may be similar to the decoding process discussed below and may include dequantizing the quantized transform coefficients at dequantization unit 450 and inverse transforming the dequantized transform coefficients at inverse transform unit 460 to produce a derivative residual block. The reconstruction unit 470 may add the prediction block generated by the intra/inter prediction unit 410 to the derivative residual block to create a reconstructed block. The loop filtering unit 480 may be applied to the reconstructed block to reduce distortion, such as blocking artifacts.
Other variations of the encoder 400 may be used to encode the compressed bitstream 404. For example, a non-transform based encoder may quantize the residual block directly without transform unit 420. In some embodiments, the quantization unit 430 and the dequantization unit 450 may be combined into a single unit.
Fig. 5 is a block diagram of a decoder 500 according to an embodiment of the present disclosure. The decoder 500 may be implemented in a device such as the computing device 100 shown in fig. 1 or the computing and communication device 100A/100B/100C shown in fig. 2, for example, as a computer software program stored in a data storage unit such as the memory 150 shown in fig. 1. The computer software program may include machine instructions executable by a processor, such as processor 140 shown in fig. 1, and may cause an apparatus to decode video data as described herein. Decoder 500 may be implemented, for example, as specialized hardware included in computing device 100.
The decoder 500 may receive a compressed bitstream 502, such as the compressed bitstream 404 shown in fig. 4, and may decode the compressed bitstream 502 to generate an output video stream 504. The decoder 500 may include an entropy decoding unit 510, a dequantization unit 520, an inverse transform unit 530, an intra/inter prediction unit 540, a reconstruction unit 550, a loop filtering unit 560, a deblocking filtering unit 570, or any combination thereof. Other structural variations of the decoder 500 may be used to decode the compressed bitstream 502.
Other variations of the decoder 500 may be used to decode the compressed bitstream 502. For example, decoder 500 may generate output video stream 504 without deblocking filtering unit 570.
In some implementations, reducing temporal redundancy can include using similarities between frames to encode frames using a relatively small amount of data based on one or more reference frames, which can be previously encoded, decoded, and reconstructed frames of a video stream. For example, a block or pixel of the current frame may be similar to a spatially corresponding block or pixel of the reference frame. In some embodiments, a block or pixel of the current frame may be similar to a block or pixel of the reference frame at different portions, and reducing temporal redundancy may include generating motion information indicating a spatial difference or transition between a location of the block or pixel in the current frame and a corresponding location of the block or pixel in the reference frame.
In some implementations, reducing temporal redundancy can include identifying a block or pixel in a reference frame or a portion of a reference frame that corresponds to a current block or pixel of a current frame. For example, a reference frame or a portion of a reference frame, which may be stored in memory, may be searched for the best block or pixel for encoding the current block or pixel of the current frame. For example, the search may identify a block of the reference frame in which a difference in pixel values between the reference block and the current block is minimized, and may be referred to as a motion search. In some embodiments, the portion of the reference frame searched may be limited. For example, the portion of the searched reference frame that may be referred to as the search area may include a limited number of rows of the reference frame. In an example, identifying the reference block may include calculating a cost function, such as a Sum of Absolute Differences (SAD), between pixels of the block in the search area and pixels of the current block. In some embodiments, more than one reference frame may be provided. For example, three reference frames may be selected from eight candidate reference frames.
In some embodiments, the spatial difference between the position of the reference block in the reference frame and the position of the current block in the current frame may be represented as a motion vector. The difference in pixel values between the reference block and the current block may be referred to as differential data, residual data, or a residual block. In some implementations, generating the motion vector may be referred to as motion estimation, and the pixels of the current block may be indicated as f based on a position using cartesian coordinatesx,y. Similarly, a pixel of a search area of a reference frame may be indicated as r based on a position using Cartesian coordinatesx,y. The Motion Vector (MV) of the current block may be determined based on, for example, the SAD between pixels of the current frame and corresponding pixels of the reference frame.
Although described herein with reference to a matrix or cartesian representation of a frame for clarity, frames may be stored, transmitted, processed, or any combination thereof in any data structure such that pixel values may be efficiently represented for a frame or image. For example, frames may be stored, transmitted, processed, or any combination thereof in a two-dimensional data structure, such as a matrix, or a one-dimensional data structure, such as a vector array.
In some implementations, for inter-prediction, the encoder 400 may communicate information of the encoding of the prediction block at the block endpoints, including but not limited to prediction mode, prediction reference frame, motion vector (if needed), sub-pixel interpolation filter type.
Fig. 6 is a block diagram of a representation of inter prediction using a sub-pixel interpolation filter according to an embodiment of the present disclosure. In some embodiments, inter-prediction using sub-pixel interpolation filters may be implemented in a decoder, such as decoder 500 shown in fig. 5, an encoder, such as encoder 400 shown in fig. 4, or a combination thereof.
In some implementations, inter-prediction may include encoding a current frame 610 or a portion thereof, such as block 612 or block 614, with reference to a reference frame 620. For example, motion vector 632/634 may indicate the spatial location of reference block 622/624 in reference frame 620 relative to the location of corresponding block 612/614 in current frame 610, reference block 622/624 may be the portion of reference frame 620 that is identified as the most accurately matching corresponding block 612/614 in current frame 610, and reference block 622/624 may be used to generate a prediction block for encoding current frame 610.
In some implementations, the portion 622 of the reference frame 620 that most accurately matches the current block (e.g., block 614) of the current frame 610 may be off block or pixel boundaries, and inter-prediction may include the use of sub-pixel interpolation filters. For example, inter prediction may include using motion vector precision of 1/2pel, 1/4pel, or 1/8pel, and may use sub-pixel interpolation filters.
In some implementations, coding the current block can include: a prediction mode for coding a current block is identified from a plurality of candidate prediction modes. For example, the video coder may evaluate candidate prediction modes, which may include intra-prediction modes, inter-prediction modes, or both, to identify an optimal prediction mode, which may be, for example, a prediction mode that minimizes an error metric, such as a rate-distortion cost, for the current block.
In some embodiments, the inter prediction modes may include a new motion vector mode (NewMV), a zero motion vector mode (ZeroMV), a recent motion vector mode (neatest mv), and a near motion vector mode (neamv). The new motion vector mode (NewMV) may indicate that a new motion vector for the current block may be explicitly or explicitly signaled in the encoded video stream. In some embodiments, the new motion vector may be signaled differently. For example, the new motion vector may be signaled using the difference between the new motion vector and a reference motion vector, such as the motion vector used to encode the previously coded neighboring block. The zero motion vector mode (ZeroMV) may indicate that a zero motion vector (0,0) that may indicate no motion may be used to predict the current block, and an explicit or explicit motion vector used to predict the current block may be omitted from the encoded video stream. In some embodiments, a recent motion vector mode (NearestMV) may indicate that a motion vector identified as a recent motion vector used to encode a previously encoded neighboring block may be used to predict a current block, and an explicit or explicit motion vector used to predict the current block may be omitted from an encoded video stream. In some embodiments, a near motion vector mode (NearMV) may indicate that a motion vector identified as a near motion vector used to encode previously encoded neighboring blocks may be used to predict the current block, and an explicit or explicit motion vector used to predict the current block may be omitted from the encoded video stream.
In some implementations, the encoder can identify a candidate motion vector for encoding the current block. For example, the candidate motion vectors may include a zero motion vector, a new motion vector, a near motion vector, and a nearest motion vector. In some embodiments, the encoder may evaluate neighboring or immediate (proximal) previously encoded blocks to identify a near motion vector and a nearest motion vector. For example, a near motion vector may be identified from a first adjacent or immediate, previously encoded block, and a nearest motion vector may be identified from a second adjacent or immediate, previously encoded block.
In some implementations, a decoder can identify candidate motion vectors for decoding a current block. For example, the candidate motion vectors may include a near motion vector and a nearest motion vector. In some embodiments, the decoder may evaluate adjacent or immediate, previously decoded blocks to identify a near motion vector and a nearest motion vector. For example, a near motion vector may be identified from a first adjacent or immediate previously decoded block, and a nearest motion vector may be identified from a second adjacent or immediate previously decoded block.
In some implementations, coding the current block can include identifying an interpolation filter for predicting the current block. For example, an interpolation filter may be selected from the candidate interpolation filters. In some embodiments, the candidate interpolation filter may be a filter of 1/16pel accuracy, and may include a bilinear filter, an 8-tap filter (eggttop), a SHARP 8-tap filter (eggttop _ SHARP), a SMOOTH 8-tap filter (eggttop _ SMOOTH), any other filter described herein (e.g., filter 2 and filter 3 described below), or a combination thereof.
In practice, the interpolation filtering may be split into two 1-dimensional filtering paths, as understood by those skilled in the art. For example, the first pass may perform interpolation filtering in the x-direction; and the second path performs interpolation filtering in the y-direction. However, for simplicity of explanation, interpolation filtering is described herein only with respect to 1-dimensional filtering.
Fig. 7 is a diagram 700 of a bilateral filter according to an embodiment of the present disclosure. In an example, the encoder may use a bilateral filter during the motion estimation stage, such as described with respect to the intra/inter prediction unit 410 of fig. 4. In an example, and to ensure fast encoding and decoding, the intra/inter prediction unit 410 and reconstruction unit 470 of fig. 4 and the reconstruction unit 550 of fig. 5 may exclusively use a 2-tap bilinear filter. The bilateral filter is referred to herein as filter 1.
P＝P0*D(x，x1)+P1*D(x，x0) (1)
Equation (1) can be described as a weighted sum of 2 neighboring pixels. The weights (i.e., D (x, x0) and D (x, x1)) are the normalized distances between position x and positions x0 and x1, respectively. For example, if x
Intermediate x0 and x1, the weight may be D (x, x0) to D (x, x1) to 0.5. For example, if x is 1/4 of the distance between x0 and x1, and is closer to x0 than x1, the weights may be D (x, x0) 0.25 and D (x, x1) 0.75. That is, the distance (i.e., weight) indicates the amount (i.e., how much) of one pixel will be contributed to the interpolated value.
Equation (1) can be rewritten in the general form of equation 2, where F0 and F1 are the filter coefficients:
P＝(P0*F0)+(P1*F1) (2)
as mentioned above, a bilinear interpolation filter may be designed based on the corresponding pixel distance. Assuming a motion vector accuracy of 1/16 pixels, the coefficients of the bilateral filter (i.e., filter 1) may be as shown in table I.
TABLE I-Filter 1 weights
It should be noted that because it is desirable to perform integer operations during the coding operation, the weights of filter 1 are represented as integers using 7-bit precision. Other integer accuracies may also be used. To illustrate the use of the bilateral filter, assume two consecutive pixel values, P0-220 and P1-63. The sub-pixel at 3/8 pixel position may be obtained by P (3/8 pixel) > >7, where "> >" is a right shift operation. For this, P (3/8) is 161. As described with respect to table I, the weights of the bilateral filter for sub-pixel accuracy other than 1/16 can be readily determined by those skilled in the art.
The purpose of interpolation is to estimate the unknown sub-pixels from neighboring pixels. Interpolation attempts to reconstruct the original continuous image from the sampled images at integer pixel locations. Reconstruction is a convolution process using a defined kernel. For example, the above bilinear interpolation filter uses a triangular kernel. The triangular filter is shown in triangular kernel 720.
The triangle kernel 720 acts as a linear interpolation filter by taking a weighted average of two adjacent values according to the distance of the sample from the two adjacent values. The convolution operation may be implemented as shown in the following example. Centering the triangle kernel 722 directly above full pixel position x0, the peak of the triangle kernel 722 is the pixel value P0. Triangle kernel 722 shows the contribution of pixel value P0 to the final interpolated pixel value at the sub-pixel location. Similarly, centering the triangle kernel 724 directly above full pixel position x1, the peak of the triangle kernel 724 is the pixel value P1. Triangle kernel 724 shows the contribution of pixel value P1 to the final interpolated pixel value at the sub-pixel location. For example, as shown in fig. 7, to calculate the sample at position x, i.e. 3/pixel between x0 (with pixel value P0) and x1 (with pixel value P1), the contribution of P0 (i.e. P0x) and the contribution of P1 (i.e. P1x) are added together, where P0x ═ P0 × W0 and P1x ═ P1 × W1. The weights to be used are W0-0.25 and W1-0.75 (or equivalently, using 7-bit integers denoted 32 and 96). For this reason, the convolution operation results in P ═ 0.25P0+0.75P 1.
The plot 730 plots the weights of the triangular kernels 720 of the bilateral filter described above (i.e., filter 1). As shown in plot 730, the weights of the triangle kernels 720 form an isosceles triangle. To this end, the bilateral filter may be characterized as a 2-tap filter with a set of weight tuples (see table 1); and each tuple comprises two weights and is used for sub-pixel interpolation at a respective sub-pixel position between the first pixel and the second pixel. For each of the weight tuples, the two weights are based on a first distance between the sub-pixel position and the first pixel and a second distance between the sub-pixel position and the second distance. In other words, the drawing of the weight tuples of the bilateral filter form triangles (e.g., isosceles triangles). The first pixel may be at an integer pixel position or a sub-pixel position. The second pixel may be at an integer pixel position or a sub-pixel position.
As already mentioned, the kernel may provide answers to the following questions: how many pixel values at a certain position are used in the interpolated pixel values? For example, and as can be seen in graph 740, at x1, the pixel weight is 1 (indicated by the alignment of the center of kernel 744 with pixel position x 1). The weights of x0 and x2 are zero. That is, the pixel values are either one of the positions x0 and x2 that do not contribute their respective values to x 1. Similarly, at x2, the pixel weight is 1 (indicated by the alignment of the center of kernel 746 with pixel position x 2). For example, for sub-pixel values at positions between x0 and x1, the corresponding values obtained along kernels 744 and 748 may be added to obtain pixel values on contiguous portion 742.
It is desirable to use a kernel that provides a close approximation of an ideal low-pass filter in the frequency domain. An ideal low-pass filter can be described as a filter: it completely eliminates all frequencies above the cutoff frequency while passing those below the cutoff frequency unchanged. That is, an ideal low pass filter recovers only those frequencies in the original image.
Although the bilateral filter described above (i.e., filter 1) is a low complexity filter, it is at frequencyAn ideal low-pass filter cannot be approximated sufficiently in the domain. The Fourier transform of the triangular kernel is sinc2A function. Sinc2The function includes many side lobes and, therefore, introduces high frequencies in the filtered result (i.e., the interpolated value). For this reason, the triangular kernel (e.g., bilateral filter) as a 1 st order filter cannot provide as good results as the higher order kernel.
Fig. 8 illustrates a 2-tap filter kernel similar to a higher order kernel in accordance with an embodiment of the present disclosure. Filter kernel 810 and filter kernel 820, referred to as filter 2 and filter 3, respectively, are depicted. Each of the filter kernels 810 and 820 defines a 2-tap filter. Thus, each of filter kernels 810 and 820 can only use two consecutive pixels in the interpolation operation. Similar to the higher order kernel may mean that a similar response is exhibited in the frequency domain.
As described further below, each of the filter kernels 810 and 820 can be characterized as a 2-tap filter with a set of weight tuples (see tables II and III). Each tuple comprises two weights and is used for sub-pixel interpolation at a respective sub-pixel position between the first pixel and the second pixel. For at least some of the weight tuples, the two weights are not based on a first distance between the sub-pixel position and the first pixel and/or a second distance between the sub-pixel position and the second distance. In other words, the plot of the weight tuples for filter 2 (i.e., filter kernel 810) does not form a triangle (e.g., an isosceles triangle, such as described with respect to plot 730); and the plot of the weight tuples of filter 3 (i.e., filter kernel 820) does not form a triangle. That is, the sides of each of filter kernels 810 and 820 include bends and are not straight lines. The first pixel may be at an integer pixel position or a sub-pixel position. The second pixel may be at an integer pixel position or a sub-pixel position.
The weights of at least some of the tuples of filter kernels 810 and 820 are not distributed based on the distance between the corresponding sub-pixel locations and the pixel locations used in the interpolation operation.
Table II lists the weight tuples for filter kernel 810 (filter 2) at a given sub-pixel location. Table III lists the weight tuples for filter kernel 820 (filter 3) at a given sub-pixel location. Similar to table I, the weights for filter 2 and filter 3 are expressed as integers using 7-bit precision.
TABLE II-Filter 2 weights
TABLE III-Filter 3 weights
Filter 2 and filter 3 (illustrated with filter kernel 810 and filter kernel 820, respectively) are weight redistribution triangle kernels. Filter 2 and filter 3 (i.e., the triangular kernels of weight redistribution) are so called because, while the overall shape may resemble a triangle, the weights are redistributed such that for at least some of the weight tuples, a higher weight is assigned to a closer pixel than the weight assigned to that pixel by filter 1. It should be understood that with respect to tables I through III, X0 and X1 refer to the positions of the two pixel values being interpolated. The sub-pixel location is given with respect to the first pixel location (i.e., X0).
As an illustrative example of weight redistribution, and using 1/4 pixel resolution, filter 2 uses (100,28) whereas filter 1 uses weight tuples (96,32), assigning higher weights (i.e., 100 higher than 96) to closer pixels (i.e., x 0); and filter 3 uses (102,26) to assign higher weights (i.e., 102 higher than 96) to the closer pixels (i.e., x 0). As another example of weight redistribution, and using 3/4 pixel resolution, filter 2 uses (28,100) whereas filter 1 uses weight tuples (32,96), assigning higher weights (i.e., 100 higher than 96) to closer pixels (i.e., x 1); and filter 3 is used (26,102) to assign higher weights (i.e., 102 higher than 96) to closer pixels (i.e., x 1).
The frequency of the fourier transform of filter 2 is not as high as the frequency of the triangular kernel (i.e., filter 1). For this reason, better coding performance can be achieved using filter 2 than with the triangular kernel. To design a filter similar to filter 2 (e.g., a triangular filter with weight distributions), a relatively high cut-off frequency (C) may be selected. The cutoff frequency may be 0.5, 0.75, or other cutoff frequencies. The cutoff frequency C of 0.75 is used in the design of the filter 2. Filter 2 provides a "sharper" response than the bilateral filter (i.e., filter 1). For this reason, similar to the SHARP interpolation filter (eggttop _ SHARP), filter 2 may preserve edges in the video frame better than the bilateral filter. Filter 3 is a gaussian-like kernel. Since the fourier transform of the gaussian filter is gaussian, the filter 3 can be considered as a good low-pass filter. Although the weight redistribution technique is described with respect to filter 2 and filter 3, other techniques may be used to obtain a 2-tap filter with weights that, unlike filter 1, are not based on pixel distance, at least for some of the sub-pixel locations.
Fig. 9 is a flow diagram of an example of a process 900 for encoding a block of video using inter prediction according to an embodiment of the present disclosure. The block is encoded using inter prediction. In some embodiments, encoding a block of video using inter prediction may be implemented in an encoder, such as encoder 400 shown in fig. 4.
In an example, and at a high level, a reference block may be identified in a reference frame based on a motion vector, and a prediction block may be generated from the reference block using an interpolation filter. In some implementations, the residual or prediction error between the block and the prediction block may be included in a compressed bitstream, such as compressed bitstream 404 of fig. 4. In some embodiments, the motion vector used to code the block and the identifier of the interpolation filter may be included in the compressed bitstream, such as in the header of the first block. In some embodiments, the motion vector used to code the block may be associated with an interpolation filter used to code the block.
In the example, and as is well known, inter prediction in an encoder may require two separate processes: a motion estimation process and a motion compensation process. The motion estimation process may be performed by a prediction unit, such as the intra/inter prediction unit 410 of fig. 4. The motion compensation process may be performed by a reconstruction unit, such as the reconstruction unit 470 of fig. 4.
During the motion estimation process, a motion vector for the block may be determined. In some implementations, encoding the block may include identifying a motion vector used to encode the block. In some implementations, a motion vector can be identified to encode a block, and an interpolation filter can be identified to encode a block. In an example, the interpolation filter may be identified when the motion vector has sub-pixel accuracy. To this end, the sample at which the fractional bit is located (i.e., the pixel value) may be interpolated using the sample at which the integer pixel bit is located (i.e., the pixel value).
An interpolation filter is used in the interpolation process. There may be several filters available. In an example, the available interpolation filters (i.e., candidate interpolation filters) during motion estimation may include the aforementioned eggttop _ SMOOTH, eggttop _ SHARP, filter 1, filter 2, and filter 3. In another example, more, fewer, or other filters may be available.
In some implementations, interpolation filters used to encode a block may be identified based on motion vectors identified for encoding the block. In some implementations, identifying the interpolation filter may include evaluating candidate interpolation filters to identify an optimal interpolation filter. In an example, each of the available candidate interpolation filters may be evaluated to determine an optimal interpolation filter. The optimal interpolation filter may be a filter that results in a minimum residual error between the block and the prediction block.
In another example, only some of the candidate interpolation filters are evaluated. As described above, the eghttap _ SMOOTH, eghttap _ SHARP, and eghttap are 8-tap filters; whereas filter 1, filter 2 and filter 3 are 2-tap filters. For some applications (e.g., video conferencing, screen sharing, etc.), only the 2-tap filter will be evaluated. This may increase the compression speed even at the expense of a lower quality compression. More generally, shorter filters are evaluated during the motion estimation process. To this end, an identifier of the selected interpolation filter is included in the compressed bitstream.
In some encoding examples, fast search motion estimation techniques may be used. Such fast motion estimation techniques may include using shorter filters (e.g., 2-tap filters such as filter 1, filter 2, and filter 3) during the motion estimation stage, but using corresponding longer filters during motion compensation, as described further below.
Using one of filter 1, filter 2, and filter 3 may include identifying a desired sub-pixel precision of the motion vector and using a corresponding weight tuple. For example, if the desired sub-pixel accuracy is 3/8 pixels, using filter 2 as an interpolation filter includes selecting weight tuples (82,46) as shown in table II for the interpolation operation.
In an example, there may be a correspondence between shorter filters and longer filters. That is, a shorter filter may have a similar or approximate frequency response as a corresponding longer filter. To this end, when a shorter filter is selected during motion estimation, the corresponding longer filter may be encoded (e.g., included) in the compressed bitstream so that the decoder may use the longer filter for decoding. In another example, the correspondence may not be based on frequency response similarity. Encoding the interpolation filter in the compressed bitstream may mean encoding an indication, identifier, etc. of the interpolation filter in the compressed bitstream. In an example, if filter 2 is selected during the motion estimation phase, the eigttop _ SHARP filter may be encoded in the compressed bitstream. In an example, if filter 3 is selected during the motion estimation phase, and even though the correspondence between filter 3 and the eggttop _ SMOOTH may be weaker than the correspondence between filter 2 and the eggttop _ SHARP filter, the eggttop _ SMOOTH filter may be encoded in the compressed bitstream. In an example, if filter 1 is selected during the motion estimation phase, the eggttap filter may be encoded in the compressed bitstream. Other correspondences between shorter filters and longer filters are also possible.
During the motion compensation phase, the motion vectors, interpolation filters and residuals are decoded from the compressed bitstream and used to reconstruct the block.
Returning to process 900, at 910, process 900 selects a first sub-pixel interpolation filter. As described above, the first sub-pixel interpolation filter may be selected during motion estimation for the block. The first sub-pixel interpolation filter may be used for sub-pixel interpolation for fractional motion (i.e., at sub-pixel locations).
The first sub-pixel interpolation filter may be a 2-tap filter. To this end, the first sub-pixel interpolation filter may be one of filter 2 or filter 3 described above, or any other weight-redistribution triangle kernel exhibiting the characteristics described herein. As further described above, such as in tables II and III, the first sub-pixel interpolation filter includes a weight tuple. Each weight tuple comprises two weights and is used for sub-pixel interpolation at a respective sub-pixel position between the first pixel and the second pixel. In an example, the first pixel and the second pixel may be pixels at integer pixel positions. In an example, at least one of the first pixel or the second pixel may be at a sub-pixel location. That is, at least one of the first pixel or the second pixel may itself be an interpolated pixel. In an example, the interpolation may be divided into multiple stages. To do so, for example, the first interpolation operation interpolates 1/2 pixel values from full pixels adjacent to the pixel, and the second interpolation operation further interpolates other sub-pixel values using 1/2 pixel values.
As mentioned, the first sub-pixel interpolation filter may be filter 2. To this end, and using 7-bit accuracy, and as described in table II, the weight tuples include tuples of sub-pixel accuracy (128,0), (122,6), (116,12), (108,20), (100,28), (92,36), (82,46), (74,54), (64,64), (54,74), (46,82), (36,92), (28,100), (20,108), (12,116), and (6,122) for full pixels, 1/16 pixels, 1/8 pixels, 3/16 pixels, 1/4 pixels, 5/16 pixels, 3/8 pixels, 7/16 pixels, 1/2 pixels, 9/16 pixels, 5/8 pixels, 11/16 pixels, 3/4 pixels, 13/16 pixels, 7/8 pixels, and 15/16 pixels, respectively.
As mentioned, the first sub-pixel interpolation filter may be filter 3. To this end, and using 7-bit precision, and as described in table III, the weight tuples include tuples (128,0), (124,4), (118,10), (110,18), (102,26), (94,34), (84,44), (74,54), (64,64), (54,74), (44,84), (34,94), (26,102), (18,110), (10,118), and (4,124) for respective sub-pixel precisions of full, 1/16, 1/8, 3/16, 1/4, 5/16, 3/8, 7/16, 1/2, 9/16, 5/8, 11/16, 3/4, 13/16, 7/8, and 15/16 pixels.
As mentioned, and as described with respect to fig. 7, the weight tuple of the bilateral (i.e., filter 1) is such that the weight is inversely proportional to the distance between the interpolation location (i.e., sub-pixel location) and the location of the pixel used in the interpolation. In contrast, a characteristic of the first sub-pixel interpolation is that, for at least one of the weight-tuples, the two weights are not based on a first distance between the sub-pixel position and the first pixel and a second distance between the sub-pixel position and the second pixel.
In an example, and as described above, the first sub-pixel interpolation may cause the weights to be redistributed such that, for at least some of the weight tuples, a higher weight is allocated to a closer pixel than the weight assigned to the pixel by filter 1. In other words, in case the sub-pixel is located closer to the first pixel than the second pixel, for at least one weight tuple of the weight tuples, the first weight of the one weight tuple is higher than the bilateral weight based on the second distance, the first weight being for the first pixel.
In an example, selecting the first sub-pixel interpolation filter may include: motion vectors having fractional resolution are selected during motion estimation for a block, and a first sub-pixel interpolation filter is used to determine interpolated values at fractional resolution to generate a prediction block.
At 920, process 900 selects a second sub-pixel interpolation filter. The second sub-pixel interpolation filter may be based on the first sub-pixel interpolation filter. In an example, the second sub-pixel interpolation filter may be used during a motion compensation process, such as by a reconstruction unit of an encoder. In an example, the second sub-pixel interpolation filter may be the first sub-pixel interpolation filter. In an example, the second sub-pixel interpolation filter may be selected based on a rate-distortion cost. For example, the second sub-pixel interpolation filter may be one of the longer interpolation filters that results in the least rate-distortion cost. In an example, the second sub-pixel interpolation filter may be selected based on a frequency response of the first sub-pixel interpolation filter. For example, the second sub-pixel interpolation filter may be selected based on a correspondence between the first interpolation filter and the second interpolation filter.
At 930, process 900 encodes the second sub-pixel interpolation filter in the compressed bitstream. For example, process 900 may encode an identifier of the second sub-pixel interpolation filter.
Fig. 10 is a flow diagram of an example of a process 1000 for decoding a block of video using inter prediction according to an embodiment of the present disclosure. In some embodiments, decoding a block of video using inter prediction may be implemented in a decoder, such as decoder 500 shown in fig. 5. In some implementations, decoding a block of video using inter prediction may be implemented at least in part in an encoder, such as by a reconstruction unit of an encoder, such as reconstruction unit 470 of fig. 4.
Decoding the block may include decoding a motion vector used to decode the block from the compressed bitstream. In some implementations, decoding the block may include decoding, from the compressed bitstream, the motion vector and an identifier of an interpolation filter, such as a sub-pixel interpolation filter used to decode the block. In some embodiments, decoding the block may include decoding a residual or prediction error of the block from a compressed bitstream. In some embodiments, decoding the block may include: identifying a reference block in a reference frame based on the decoded motion vector; generating a predicted block from the reference block using the interpolation filter indicated by the decoded interpolation filter identifier; and generating a reconstructed block by adding the decoded residual to the predicted block. In some embodiments, the motion vector, the identifier of the interpolation filter, or a combination thereof may be decoded from the header of the block in the compressed bitstream. In some implementations, the reconstructed block may be included in an output (such as an output for rendering). The compressed bitstream may be as described with respect to compressed bitstream 502 of fig. 5. The compressed bitstream may be as described with respect to a bitstream received by a reconstruction path (such as the reconstruction path described with respect to fig. 4).
At 1010, process 1000 decodes a sub-pixel interpolation filter from the compressed bitstream. The sub-pixel interpolation filter may be used for sub-pixel interpolation for fractional motion. The sub-pixel interpolation filter may be a 2-tap filter. The sub-pixel interpolation filter includes a weight tuple. Each tuple includes two weights and is used for sub-pixel interpolation at a sub-pixel position between the first pixel and the second pixel. For at least one of the weight tuples, the two weights are not based on a first distance between the sub-pixel location and the first pixel and a second distance between the sub-pixel location and the second pixel. This is in contrast to the weight tuple of the bilateral filter (filter 1).
In an example, and as described above, sub-pixel interpolation may cause weights to be redistributed such that, for at least some of the weight tuples, a higher weight is assigned to a closer pixel than the weight assigned to the pixel by filter 1. In other words, in case the sub-pixel is located closer to the first pixel than the second pixel, for at least one of the weight tuples, the first weight of the one weight tuple is higher than the bilateral weight based on the second distance, wherein the first weight is for the first pixel.
The sub-pixel interpolation filter may be filter 2. To this end, and using 7-bit accuracy, and as described in table II, the weight tuples include tuples of sub-pixel accuracy (128,0), (122,6), (116,12), (108,20), (100,28), (92,36), (82,46), (74,54), (64,64), (54,74), (46,82), (36,92), (28,100), (20,108), (12,116), and (6,122) for full pixels, 1/16 pixels, 1/8 pixels, 3/16 pixels, 1/4 pixels, 5/16 pixels, 3/8 pixels, 7/16 pixels, 1/2 pixels, 9/16 pixels, 5/8 pixels, 11/16 pixels, 3/4 pixels, 13/16 pixels, 7/8 pixels, and 15/16 pixels, respectively.
The sub-pixel interpolation filter may be filter 3. To this end, and using 7-bit precision, and as described in table III, the weight tuples include tuples (128,0), (124,4), (118,10), (110,18), (102,26), (94,34), (84,44), (74,54), (64,64), (54,74), (44,84), (34,94), (26,102), (18,110), (10,118), and (4,124) for respective sub-pixel precisions of full, 1/16, 1/8, 3/16, 1/4, 5/16, 3/8, 7/16, 1/2, 9/16, 5/8, 11/16, 3/4, 13/16, 7/8, and 15/16 pixels.
At 1020, process 1000 decodes motion vectors from the compressed bitstream. At 1030, process 1000 decodes the block using the sub-pixel interpolation filter and the motion vector. For example, decoding the motion vector may include identifying a reference frame. Using the reference frame, the motion index, and the sub-pixel interpolation filter, a prediction block may be generated. In addition, the residual may be decoded from the compressed bitstream. The residual may be added to the prediction block to reconstruct the block.
The word "example" or "exemplary" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" or "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" or "exemplary" is intended to present concepts in a particular style. As used in this application, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless specified otherwise, or clear from context, "X includes a or B" is intended to mean either of the natural inclusive permutations. That is, if X comprises A; x comprises B; or X includes both a and B, then "X includes a or B" is satisfied under any of the above examples. In addition, the articles "a" and "an" as used in this application and the appended claims should generally be construed to mean "one or more" unless specified otherwise or clear from context to be directed to a singular form. Furthermore, unless described as such, the use of the terms "an embodiment" or "one embodiment" or "an implementation" or "one implementation" throughout is not intended to denote the same embodiment or implementation. As used herein, the terms "determine" and "identify," or any variation thereof, include selecting, ascertaining, calculating, looking up, receiving, determining, establishing, obtaining, or otherwise identifying or determining in any way using one or more of the devices illustrated in fig. 1.
Moreover, for simplicity of explanation, while the figures and descriptions herein may include a sequence or series of steps or stages, elements of a method or process disclosed herein may occur in various orders and/or concurrently. In addition, elements of the methods disclosed herein may occur with other elements not expressly shown or described herein. Moreover, not all elements of a method described herein may be required to implement a method or process in accordance with the disclosed subject matter.
Embodiments of the transmitting station (such as computing and communication device 100A) and/or the receiving station (such as computing and communication device 100B), as well as algorithms, methods, instructions, executable instructions, etc., stored thereon and/or implemented thereby, may be implemented in hardware, software, or any combination thereof. The hardware may include, for example, a computer, an Intellectual Property (IP) core, an Application Specific Integrated Circuit (ASIC), a programmable logic array, an optical processor, a programmable logic controller, microcode, a microcontroller, a server, a microprocessor, a digital signal processor, or any other suitable circuitry. In the claims, the term "processor" should be understood to encompass any of the aforementioned hardware, alone or in combination. The terms "signal" and "data" are used interchangeably. Furthermore, the parts of the transmitting station and the receiving station do not necessarily have to be implemented in the same way.
Further, in one embodiment, for example, a transmitting station or a receiving station may be implemented using a computer program that, when executed, implements any of the respective methods, algorithms, and/or instructions described herein. Additionally or alternatively, for example, a special purpose computer/processor can be utilized that can contain specialized hardware for carrying out any of the methods, algorithms, or instructions described herein.
The transmitting station and the receiving station may be implemented on a computer in a real-time video system, for example. Alternatively, the transmitting station may be implemented on a server and the receiving station may be implemented on a device separate from the server, such as a handheld communication device. In this example, the transmitting station may encode the content into an encoded video signal using encoder 400 and transmit the encoded video signal to the communication device. The communication device may then decode the encoded video signal using the decoder 500. Alternatively, the communication device may decode content stored locally on the communication device, e.g., content that is not transmitted by the transmission station. Other suitable transmitting station embodiments and receiving station embodiments are also available. For example, the receiving station may typically be a fixed personal computer rather than a portable communication device, and/or a device including the encoder 400 may also include the decoder 500.
Furthermore, all or portions of the embodiments can take the form of a computer program product accessible from, for example, a tangible computer-usable or computer-readable medium. A computer-usable or computer-readable medium may be, for example, any device that can tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The medium may be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable media may be used.
The above embodiments have been described in order to allow easy understanding of the present application, and are not restrictive. On the contrary, this application is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures as is permitted under the law.
Claims (20)
1. A method for encoding a block of video using inter-prediction, the method comprising:
selecting a first sub-pixel interpolation filter for sub-pixel interpolation of fractional motion during motion estimation for the block;
wherein the first sub-pixel interpolation filter is a 2-tap filter,
wherein the first sub-pixel interpolation filter comprises weight tuples, each weight tuple consisting of two weights and being used for sub-pixel interpolation at a sub-pixel position between the first pixel and the second pixel, an
Wherein, for at least one of the weight tuples, the two weights are not based on a first distance between the sub-pixel location and the first pixel and a second distance between the sub-pixel location and the second pixel;
during motion compensation, selecting a second sub-pixel interpolation filter based on the first sub-pixel interpolation filter; and
encoding the second sub-pixel interpolation filter in a compressed bitstream.
2. The method of claim 1, wherein, for the at least one of the weight tuples, a first weight of the one weight tuple is higher than a bilateral weight based on the second distance, where the first weight is for the first pixel, if the sub-pixel location is closer to the first pixel than the second pixel.
3. The method of claim 1 or 2, wherein selecting the first sub-pixel interpolation filter for sub-pixel interpolation of fractional motion during the motion estimation for the block comprises:
selecting a motion vector having a fractional resolution during the motion estimation for the block; and
determining an interpolated value at the fractional resolution using the first sub-pixel interpolation filter to generate a prediction block.
4. The method of any of claims 1-3, wherein the second sub-pixel interpolation filter is the first sub-pixel interpolation filter.
5. The method of any of claims 1-3, wherein the second sub-pixel interpolation filter is selected based on a frequency response of the first sub-pixel interpolation filter.
6. The method of any of claims 1-3, wherein the second sub-pixel interpolation filter is selected based on a correspondence between the first sub-pixel interpolation filter and the second sub-pixel interpolation filter.
7. The method of any of claims 1 to 6, wherein, with 7-bit precision, the weight-tuple comprises (128,0), (122,6), (116,12), (108,20), (100,28), (92,36), (82,46), (74,54), (64,64), (54,74), (46,82), (36,92), (28,100), (20,108) for respective sub-pixel precisions of full, 1/16, 1/8, 3/16, 1/4, 5/16, 3/8, 7/16, 1/2, 9/16, 5/8, 11/16, 3/4, 13/16, 7/8, and 15/16 pixels, (12,116) and (6, 122).
8. The method of any of claims 1 to 6, wherein, with 7-bit precision, the weight-tuple comprises (128,0), (124,4), (118,10), (110,18), (102,26), (94,34), (84,44), (74,54), (64,64), (54,74), (44,84), (34,94), (26,102), (18,110) for respective sub-pixel precisions of full, 1/16, 1/8, 3/16, 1/4, 5/16, 3/8, 7/16, 1/2, 9/16, 5/8, 11/16, 3/4, 13/16, 7/8, and 15/16 pixels, (10,118) and (4,124).
9. A device for encoding a block of video using inter-prediction, the device comprising:
a processor configured to:
selecting a first sub-pixel interpolation filter for sub-pixel interpolation of fractional motion during motion estimation for the block,
wherein the first sub-pixel interpolation filter is a 2-tap filter,
wherein the first sub-pixel interpolation filter comprises weight tuples, each weight tuple consisting of two weights and being used for sub-pixel interpolation at a sub-pixel position between the first pixel and the second pixel, an
Wherein, for at least one of the weight tuples, the two weights are not based on a first distance between the sub-pixel location and the first pixel and a second distance between the sub-pixel location and the second pixel;
selecting a second sub-pixel interpolation filter based on the rate-distortion cost; and
encoding the second sub-pixel interpolation filter in a compressed bitstream.
10. The apparatus of claim 9, wherein the processor is configured to:
the second sub-pixel interpolation filter is used during motion compensation.
11. The apparatus of claim 9 or 10, wherein, in case the sub-pixel location is closer to the first pixel than the second pixel, for the at least one of the weight tuples, the first weight of the one weight tuple and for the first pixel is higher than a bilateral weight based on the second distance.
12. The apparatus of any of claims 9-11, wherein selecting the first sub-pixel interpolation filter for sub-pixel interpolation of fractional motion during the motion estimation for the block comprises:
selecting a motion vector having a fractional resolution during the motion estimation for the block; and
determining an interpolated value at the fractional resolution using the first sub-pixel interpolation filter.
13. The apparatus of any of claims 9-12, wherein the second sub-pixel interpolation filter is the first sub-pixel interpolation filter.
14. The apparatus of any of claims 9-12, wherein the second sub-pixel interpolation filter is selected based on a frequency response of the first sub-pixel interpolation filter.
15. The apparatus of any of claims 9 to 14, wherein, with 7-bit precision, the weight-tuple comprises (128,0), (122,6), (116,12), (108,20), (100,28), (92,36), (82,46), (74,54), (64,64), (54,74), (46,82), (36,92), (28,100), (20,108) for respective sub-pixel precisions of full, 1/16, 1/8, 3/16, 1/4, 5/16, 3/8, 7/16, 1/2, 9/16, 5/8, 11/16, 3/4, 13/16, 7/8, and 15/16 pixels, (12,116) and (6, 122).
16. The apparatus of any of claims 9 to 14, wherein, with 7-bit precision, the weight-tuple comprises (128,0), (124,4), (118,10), (110,18), (102,26), (94,34), (84,44), (74,54), (64,64), (54,74), (44,84), (34,94), (26,102), (18,110) for respective sub-pixel precisions of full, 1/16, 1/8, 3/16, 1/4, 5/16, 3/8, 7/16, 1/2, 9/16, 5/8, 11/16, 3/4, 13/16, 7/8, and 15/16 pixels, (10,118) and (4,124).
17. A device for decoding a block of video using inter-prediction, the device comprising:
a processor configured to:
decoding sub-pixel interpolation filters from the encoded bitstream, the sub-pixel interpolation filters being used for sub-pixel interpolation of fractional motion,
wherein the sub-pixel interpolation filter is a 2-tap filter,
wherein the sub-pixel interpolation filter comprises weight tuples, each weight tuple consisting of two weights and being used for sub-pixel interpolation at a sub-pixel position between a first pixel and a second pixel, an
Wherein, for at least one of the weight tuples, the two weights are not based on a first distance between the sub-pixel location and the first pixel and a second distance between the sub-pixel location and the second pixel;
decoding a motion vector from the encoded bitstream; and
decoding the block using the sub-pixel interpolation filter and the motion vector.
18. The apparatus of claim 17, wherein, for the at least one of the weight tuples, a first weight of the one weight tuple and for the first pixel is higher than a bilateral weight based on the second distance if the sub-pixel location is closer to the first pixel than the second pixel.
19. The apparatus of claim 17 or 18, wherein, with 7-bit precision, the weight tuples include (128,0), (122,6), (116,12), (108,20), (100,28), (92,36), (82,46), (74,54), (64,64), (54,74), (46,82), (36,92), (28,100), (20,108), (12,116), and (6,122) for respective sub-pixel accuracies of full, 1/16, 1/8, 3/16, 1/4, 5/16, 3/8, 7/16, 1/2, 9/16, 5/8, 11/16, 3/4, 13/16, 7/8, and 15/16 pixels.
20. The apparatus of claim 17 or 18, wherein, with 7-bit precision, the weight tuples include (128,0), (124,4), (118,10), (110,18), (102,26), (94,34), (84,44), (74,54), (64,64), (54,74), (44,84), (34,94), (26,102), (18,110), (10,118), and (4,124) for respective sub-pixel accuracies of full, 1/16, 1/8, 3/16, 1/4, 5/16, 3/8, 7/16, 1/2, 9/16, 5/8, 11/16, 3/4, 13/16, 7/8, and 15/16 pixels.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/256,302 US11044480B2 (en) | 2019-01-24 | 2019-01-24 | More accurate 2-tap interpolation filters for video compression |
US16/256,302 | 2019-01-24 | ||
PCT/US2019/059600 WO2020154007A1 (en) | 2019-01-24 | 2019-11-04 | More accurate 2-tap interpolation filters for video compression |
Publications (1)
Publication Number | Publication Date |
---|---|
CN113170207A true CN113170207A (en) | 2021-07-23 |
Family
ID=69159938
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980079722.1A Pending CN113170207A (en) | 2019-01-24 | 2019-11-04 | More accurate 2-tap interpolation filter for video compression |
Country Status (4)
Country | Link |
---|---|
US (1) | US11044480B2 (en) |
EP (1) | EP3915266A1 (en) |
CN (1) | CN113170207A (en) |
WO (1) | WO2020154007A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
MX2021012760A (en) * | 2019-04-19 | 2022-01-25 | Huawei Tech Co Ltd | Method and apparatus for deriving interpolation filter index for current block. |
KR20210113464A (en) * | 2020-03-05 | 2021-09-16 | 삼성전자주식회사 | Imaging device and electronic device including the same |
KR20220030084A (en) * | 2020-09-02 | 2022-03-10 | 삼성전자주식회사 | Method and apparatus of processing image |
US11800092B2 (en) * | 2021-11-17 | 2023-10-24 | Tencent America LLC | Joint signaling method for motion vector difference |
WO2023168358A2 (en) * | 2022-03-04 | 2023-09-07 | Innopeak Technology, Inc. | Integerization for interpolation filter design in video coding |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040062307A1 (en) * | 2002-07-09 | 2004-04-01 | Nokia Corporation | Method and system for selecting interpolation filter type in video coding |
EP1578137A2 (en) * | 2004-03-17 | 2005-09-21 | Matsushita Electric Industrial Co., Ltd. | Moving picture coding apparatus with multistep interpolation process |
US20080089417A1 (en) * | 2006-10-13 | 2008-04-17 | Qualcomm Incorporated | Video coding with adaptive filtering for motion compensated prediction |
US20090022220A1 (en) * | 2005-04-13 | 2009-01-22 | Universitaet Hannover | Method and apparatus for enhanced video coding |
CN103096075A (en) * | 2011-11-07 | 2013-05-08 | 索尼公司 | Video Data Encoding And Decoding |
US20160191946A1 (en) * | 2014-12-31 | 2016-06-30 | Microsoft Technology Licensing, Llc | Computationally efficient motion estimation |
US20170150180A1 (en) * | 2015-11-20 | 2017-05-25 | Mediatek Inc. | Method and apparatus for video coding |
US20180077423A1 (en) * | 2016-09-15 | 2018-03-15 | Google Inc. | Dual filter type for motion compensated prediction in video coding |
CN107925772A (en) * | 2015-09-25 | 2018-04-17 | 华为技术有限公司 | The apparatus and method that video motion compensation is carried out using optional interpolation filter |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7266150B2 (en) * | 2001-07-11 | 2007-09-04 | Dolby Laboratories, Inc. | Interpolation of video compression frames |
-
2019
- 2019-01-24 US US16/256,302 patent/US11044480B2/en active Active
- 2019-11-04 EP EP19835896.2A patent/EP3915266A1/en active Pending
- 2019-11-04 WO PCT/US2019/059600 patent/WO2020154007A1/en unknown
- 2019-11-04 CN CN201980079722.1A patent/CN113170207A/en active Pending
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040062307A1 (en) * | 2002-07-09 | 2004-04-01 | Nokia Corporation | Method and system for selecting interpolation filter type in video coding |
EP1578137A2 (en) * | 2004-03-17 | 2005-09-21 | Matsushita Electric Industrial Co., Ltd. | Moving picture coding apparatus with multistep interpolation process |
US20090022220A1 (en) * | 2005-04-13 | 2009-01-22 | Universitaet Hannover | Method and apparatus for enhanced video coding |
US20080089417A1 (en) * | 2006-10-13 | 2008-04-17 | Qualcomm Incorporated | Video coding with adaptive filtering for motion compensated prediction |
CN103096075A (en) * | 2011-11-07 | 2013-05-08 | 索尼公司 | Video Data Encoding And Decoding |
US20160191946A1 (en) * | 2014-12-31 | 2016-06-30 | Microsoft Technology Licensing, Llc | Computationally efficient motion estimation |
CN107925772A (en) * | 2015-09-25 | 2018-04-17 | 华为技术有限公司 | The apparatus and method that video motion compensation is carried out using optional interpolation filter |
US20170150180A1 (en) * | 2015-11-20 | 2017-05-25 | Mediatek Inc. | Method and apparatus for video coding |
US20180077423A1 (en) * | 2016-09-15 | 2018-03-15 | Google Inc. | Dual filter type for motion compensated prediction in video coding |
Also Published As
Publication number | Publication date |
---|---|
US11044480B2 (en) | 2021-06-22 |
US20200244971A1 (en) | 2020-07-30 |
WO2020154007A1 (en) | 2020-07-30 |
EP3915266A1 (en) | 2021-12-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10165283B1 (en) | Video coding using compound prediction | |
US10555000B2 (en) | Multi-level compound prediction | |
US9374578B1 (en) | Video coding using combined inter and intra predictors | |
US10009625B2 (en) | Low-latency two-pass video coding | |
US11044480B2 (en) | More accurate 2-tap interpolation filters for video compression | |
US11457239B2 (en) | Block artefact reduction | |
USRE49615E1 (en) | Coding interpolation filter type | |
US10694205B2 (en) | Entropy coding of motion vectors using categories of transform blocks | |
US10567793B2 (en) | Adaptive overlapped block prediction in variable block size video coding | |
EP3568978A1 (en) | Compound prediction for video coding | |
CN107465925B (en) | Adaptive overlapped block prediction in variable block size video coding | |
US10419777B2 (en) | Non-causal overlapped block prediction in variable block size video coding | |
US20160094843A1 (en) | Frequency-domain denoising | |
US10455253B1 (en) | Single direction long interpolation filter | |
US11924476B2 (en) | Restoration in video coding using filtering and subspace projection | |
JP2022069546A (en) | Compound motion-compensated prediction | |
US20230291925A1 (en) | Inter-Intra Prediction With Implicit Models | |
US20240089433A1 (en) | Chroma Transform Type Determination | |
US10499078B1 (en) | Implicit motion compensation filter selection |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |