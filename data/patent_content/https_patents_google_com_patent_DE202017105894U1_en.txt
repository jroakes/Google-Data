Hintergrundbackground
Immersive Systeme für virtuelle Realität (VR), erweiterte Realität (AR) oder gemischte Realität (MR) nutzen typischerweise ein „Head-mounted Display“ (HMD), das dem Anwender stereoskopische Bilder präsentiert, um ein Gefühl der Anwesenheit in einer dreidimensionalen Szene (3D-Szene) zu geben. Ein typisches HMD ist konstruiert, um ein stereoskopisches Bild über ein Gesichtsfeld zu produzieren, das sich dem Gesichtsfeld eines menschlichen Auges, das ungefähr 180° ist, annähert oder ihm gleich ist. Beispielsweise ist das Gesichtsfeld handelsüblicher HMDs derzeit 100-110°. Mehrere Anwender können miteinander in derselben 3D-Szene, die durch ein immersives VR-, AR- oder MR-System produziert wird, interagieren. Beispielsweise können Anwender miteinander interagieren unter Verwendung von 3D-Videokonferenzen, während sie gemeinsam Filme oder YouTube-Videos in einem virtuellen Theater betrachten, eine virtuelle Wanderung durch ein Gebiet in Google Earth unternehmen oder während sie in einem virtuellen 3D-Klassenzimmer sitzen und einen Vortrag eines (realen oder virtuellen) Professors hören. Einige immersive VR-, AR- oder MR-Systeme verwenden eine Kamera, um Bilder der Anwender aufzunehmen, die dann in die virtuelle 3D-Szene eingefügt werden. In einigen Fällen, wie z. B. bei Systemen, die Avatar-basierte Darstellungen implementieren, ist die „Kamera“ eine Abstraktion, die verwendet wird, um eine Perspektive anzugeben, aus der die Szene wiedergegeben wird. In Fällen, in denen die Szene einen Anwender enthält, der ein HMD trägt, befindet sich die Kamera typischerweise außerhalb des HMD und gibt die Szene aus einer externen Perspektive relativ zu dem Anwender, der das HMD trägt, wieder oder nimmt sie daraus auf. Infolgedessen sind die Gesichter des Anwenders und insbesondere die Augen des Anwenders durch das HMD verdeckt, so dass Bilder der Anwender, die in die virtuelle 3D-Szene eingefügt werden, eine verwirrende Erscheinung mit einem „Klotz im Gesicht“ zeigen. Infolgedessen verhindern HMDs, dass die Anwender während virtueller Interaktionen Blickkontakt aufnehmen, was das Gefühl des Eintauchens und der sozialen Verbindung zwischen den Anwendern in der virtuellen 3D-Szene stören kann.Immersive virtual reality (VR), augmented reality (AR) or mixed reality (MR) systems typically use a "head-mounted display" (HMD) that presents stereoscopic images to the user to provide a sense of presence in a three-dimensional scene (FIG. 3D scene). A typical HMD is designed to produce a stereoscopic image across a field of view that approximates or equals the field of view of a human eye that is approximately 180 °. For example, the field of view of commercially available HMDs is currently 100-110 °. Multiple users can interact with each other in the same 3D scene produced by an immersive VR, AR, or MR system. For example, users can interact with each other using 3D videoconferencing while watching movies or YouTube videos together in a virtual theater, making a virtual tour of an area in Google Earth, or sitting in a virtual 3D classroom and giving a talk (real or virtual) professor. Some immersive VR, AR or MR systems use a camera to capture images of the users, which are then inserted into the virtual 3D scene. In some cases, such as For example, in systems that implement avatar-based representations, the "camera" is an abstraction that is used to indicate a perspective from which the scene is rendered. In cases where the scene contains a user wearing an HMD, the camera is typically outside of the HMD and plays back or captures the scene from an external perspective relative to the user wearing the HMD. As a result, the faces of the user, and particularly the eyes of the user, are obscured by the HMD, so that images of the users inserted in the virtual 3D scene show a confusing "face on the face" phenomenon. As a result, HMDs prevent users from making eye contact during virtual interactions, which can interfere with the sense of immersion and social connection between the users in the virtual 3D scene.
Zu schützen als Gebrauchsmuster und Gegenstand des Gebrauchsmusters sind in Übereinstimmung mit den Anforderungen des Gebrauchsmustergesetzes nur Vorrichtungen, wie sie in den beigefügten Ansprüchen definiert sind, jedoch keine Verfahren. In dem Fall, in dem sich die Beschreibung auf Verfahren bezieht, dienen diese Bezugnahmen lediglich dazu, die Vorrichtung oder Vorrichtungen zu veranschaulichen, für die Schutz mit den beigefügten Ansprüchen gesucht wird.However, as a utility model and subject matter of the utility model, only apparatuses as defined in the appended claims are not methods in accordance with the requirements of the Utility Model Law. In the case in which the description refers to methods, these references are merely illustrative of the apparatus or devices for which protection is sought with the appended claims.
Figurenlistelist of figures
Die vorliegende Offenbarung kann besser verstanden und ihre zahlreichen Merkmale und Vorteile für Fachleute besser sichtbar gemacht werden durch Bezugnahme auf die begleitenden Zeichnungen. Das Verwenden der gleichen Bezugszeichen in unterschiedlichen Zeichnungen gibt ähnliche oder gleiche Elemente an.
- 1 ist ein Blockdiagramm, das eine Seitenansicht eines Computersystems, das konfiguriert ist, eine Blick-Datenbank gemäß einigen Ausführungsformen aufzunehmen, darstellt.
- 2 ist ein Blockdiagramm, das eine Draufsicht eines Computersystems, das verwendet wird, um die Blick-Datenbank aufzunehmen, gemäß einigen Ausführungsformen darstellt.
- 3 stellt einen Bildschirm und einen Anwender während eines Prozesses zum Aufnehmen von Bilder des Anwenders zum Erzeugen einer Blick-Datenbank gemäß einigen Ausführungsformen dar.
- 4 stellt einen Prozess zum Erzeugen eines Modells eines Gesichts eines Anwenders aus einem aufgenommenen Bild gemäß einigen Ausführungsformen dar.
- 5 ist ein Blockdiagramm, das eine Blick-Datenbank enthält, die unter Verwendung ausgerichteter und gefilterter Gesichtsproben gemäß einigen Ausführungsformen produziert ist.
- 6 ist ein Ablaufdiagramm eines Verfahrens zum Aufnehmen von Bildern des Gesichts des Anwenders, die verwendet werden, um eine Blick-Datenbank zu erzeugen, gemäß einigen Ausführungsformen.
- 7 ist ein Ablaufdiagramm eines Verfahrens zum Erzeugen einer Blick-Datenbank unter Verwendung von Gesichtsproben, die aus Bildern des Gesichts eines Anwenders erfasst werden, während er in unterschiedlichen Richtungen schaut, gemäß einigen Ausführungsformen.
- 8 ist ein Diagramm, das ein Verarbeitungssystem darstellt, das konfiguriert ist, Headset-Entfernung unter Verwendung von Informationen, die in einer Blick-Datenbank gespeichert sind, gemäß einigen Ausführungsformen auszuführen.
- 9 stellt ein Anzeigesystem dar, das eine elektronische Einrichtung enthält, die konfiguriert ist, VR-, AR- oder MR-Funktionalität über eine Anzeigevorrichtung bereitzustellen, gemäß einigen Ausführungsformen.
- 10 stellt relative Positionen und Orientierungen einer Kamera, eines HMD und einer Anwenders in einem Headset-Entfernungssystem gemäß einigen Ausführungsformen dar.
- 11 stellt das Abgleichen eines 3D-Modells eines Gesichts mit einem aufgenommenen Bild eines Gesichts, das teilweise durch ein HMD verdeckt ist, gemäß einigen Ausführungsformen dar.
- 12 stellt das Abgleichen eines 3D-Modells eines HMD mit einem aufgenommenen Bild eines HMD gemäß einigen Ausführungsformen dar.
- 13 stellt Headset-Entfernung, die auf einem Bild eines Anwenders, der ein HMD trägt, das einen Teil des Gesichts des Anwenders verdeckt, ausgeführt wird, gemäß einigen Ausführungsformen dar.
- 14 ist ein Ablaufdiagramm eines Verfahrens zum Ausführen von Headset-Entfernung gemäß einigen Ausführungsformen.
- 15 ist ein Ablaufdiagramm eines Verfahrens zum Bestimmen einer Stellung eines teilweise verdeckten Gesichts in einem Bild in einem Koordinatensystem einer Kamera, die das Bild erfasst, gemäß einigen Ausführungsformen.
- 16 ist ein Blockdiagramm eines Ende-zu-Ende-Systems zum Ausführen von Headset-Entfernung in gemischter Realität (MR) gemäß einigen Ausführungsformen.
- 17 ist ein Diagramm, das eine Anordnung, die verwendet wird, um automatische Kalibrierung zwischen einer Kamera und einer Stellung eines HMD auszuführen, gemäß einigen Ausführungsformen darstellt.
- 18 ist ein Ablaufdiagramm eines Verfahrens zum Kalibrieren einer Kamera, die verwendet wird, um Bilder eines Anwenders, der ein HMD trägt, in einem Ende-zu-Ende-System für gemischte Realität aufzunehmen, gemäß einigen Ausführungsformen.
- 19 stellt Variationen gemessener Parameter, die der Bewegung eines HMD und Bildern des sich bewegenden HMD zugeordnet sind gemäß einigen Ausführungsformen dar.
- 20 stellt ein Anzeigesystem, das eine elektronische Einrichtung enthält, die konfiguriert ist, VR-, AR- oder MR-Funktionalität über eine Anzeigevorrichtung bereitzustellen, gemäß einigen Ausführungsformen dar.
- 21 ist ein Ablaufdiagramm eines Verfahrens zum Ausführen von Headset-Entfernung für einen Anwender, der ein HMD trägt, in einer Szene mit gemischter Realität gemäß einigen Ausführungsformen.
The present disclosure may be better understood and its numerous features and advantages made more apparent to those skilled in the art by reference to the accompanying drawings. Using the same reference numerals in different drawings indicates similar or similar elements. - 1 FIG. 3 is a block diagram illustrating a side view of a computer system configured to receive a gaze database according to some embodiments.
- 2 FIG. 10 is a block diagram illustrating a top view of a computer system used to capture the gaze database, in accordance with some embodiments.
- 3 illustrates a screen and a user during a process of capturing images of the user to create a gaze database, in accordance with some embodiments.
- 4 FIG. 10 illustrates a process for generating a model of a user's face from a captured image according to some embodiments. FIG.
- 5 FIG. 10 is a block diagram that includes a gaze database that is produced using aligned and filtered face samples, in accordance with some embodiments.
- 6 FIG. 10 is a flowchart of a method of capturing images of the user's face used to create a gaze database, in accordance with some embodiments.
- 7 FIG. 10 is a flowchart of a method for generating a gaze database using facial samples captured from images of a user's face while looking in different directions, according to some embodiments.
- 8th FIG. 10 is a diagram illustrating a processing system configured to perform headset removal using information stored in a lookup database, in accordance with some embodiments.
- 9 FIG. 12 illustrates a display system including an electronic device configured to provide VR, AR, or MR functionality via a display device, according to some embodiments.
- 10 illustrates relative positions and orientations of a camera, an HMD, and a user in a headset removal system, in accordance with some embodiments.
- 11 Figure 3 illustrates matching a 3D model of a face with a captured image of a face partially obscured by an HMD, according to some embodiments.
- 12 Figure 3 illustrates the trimming of a 3D model of an HMD with a captured image of an HMD according to some embodiments.
- 13 illustrates headset removal performed on an image of a user wearing an HMD that obscures a portion of the user's face, in accordance with some embodiments.
- 14 FIG. 10 is a flowchart of a method for performing headset removal in accordance with some embodiments. FIG.
- 15 FIG. 10 is a flow chart of a method for determining a position of a partially obscured face in an image in a coordinate system of a camera capturing the image, in accordance with some embodiments.
- 16 FIG. 10 is a block diagram of an end-to-end system for performing mixed-reality headset removal (MR) according to some embodiments.
- 17 FIG. 10 is a diagram illustrating an arrangement used to perform automatic calibration between a camera and a position of an HMD, according to some embodiments.
- 18 FIG. 10 is a flow chart of a method of calibrating a camera used to capture images of a user wearing an HMD in a mixed-reality end-to-end system, in accordance with some embodiments.
- 19 illustrates variations of measured parameters associated with movement of an HMD and images of the moving HMD according to some embodiments.
- 20 provides a display system including an electronic device configured to provide VR, AR, or MR functionality via a display device, in accordance with some embodiments.
- 21 FIG. 10 is a flowchart of a method for performing headset removal for a user wearing an HMD in a mixed reality scene, according to some embodiments.
Ausführliche BeschreibungDetailed description
Die soziale Verbindung zwischen Anwendern in einer virtuellen 3D-Szene wie z. B. einer Szene mit gemischter Realität kann wesentlich verbessert werden durch Ersetzen eines Abschnitts des HMD durch ein Modell eines Abschnitts des Gesichts des Anwenders, der durch das HMD verdeckt wird, in dem Bild des Anwenders, das in die virtuelle 3D-Szene eingefügt wird. Einige Ausführungsformen des Systems enthalten drei Komponenten: (1) Erzeugen einer Blick-Datenbank für ein 3D-Modell eines Gesichts eines Anwenders, die nach der Blickrichtung des Anwenders indexiert ist, (2) „Entfernen“ des HMD aus einem Bild des Anwenders durch Überschreiben eines Abschnitts des Bilds, das dem HMD entspricht, mit einem Abschnitt des 3D-Modells, das basierend auf der Stellung und Blickrichtung des Anwenders wiedergegeben wird, und (3) Erzeugen eines Bild des Anwenders für gemischte Realität, das eine lichtdurchlässige Repräsentation des HMD enthält, die basierend auf einem wiedergegebenen 3D-Modell eines Abschnitts des Gesichts des Anwenders und zeitsynchronisierten Strömen, die für die HMD-Stellungstelemetrie repräsentativ sind, der Blickrichtung des Anwenders und Bildern, die durch eine externe Kamera aufgenommen sind, erzeugt wird.The social connection between users in a virtual 3D scene such as: A mixed reality scene can be substantially improved by replacing a portion of the HMD with a model of a portion of the user's face obscured by the HMD in the image of the user being inserted into the 3D virtual scene. Some embodiments of the system include three components: ( 1 ) Generating a gaze database for a 3D model of a user's face indexed in the user's line of sight (FIG. 2 ) "Removing" the HMD from an image of the user by overwriting a portion of the image corresponding to the HMD with a portion of the 3D model rendered based on the user's position and gaze, and ( 3 ) Generating a mixed reality user image containing a translucent representation of the HMD based on a rendered 3-D model of a portion of the user's face and time-synchronized streams representative of the HMD positional telemetry, the user's line of sight, and Pictures taken by an external camera are generated.
Ein 3D-Modell des Gesichts des Anwenders wird durch Aufnehmen von mehreren Bildern des Gesichts des Anwenders, die mehreren unterschiedlichen Blickrichtungen entsprechen, erzeugt. In einigen Ausführungsformen nimmt eine Kamera die mehreren Bilder auf, während der Anwender einem sich bewegenden Zielbild auf einem Bildschirm folgt. Beispielsweise kann eine Kamera als eine RGBD-Kamera implementiert sein, die RGB-Werte von Pixeln in dem Bild und einen Tiefenwert für jedes Pixel, der einen Abstand zwischen der Kamera und dem durch das Pixel repräsentierte Objekt angibt, aufnimmt. Die Blickrichtung für jedes Bild wird dann basierend auf den relativen Positionen der Augen des Anwenders, der Kamera und des sich bewegenden Bilds auf dem Bildschirm bestimmt. Die Kamera nimmt außerdem ein Bild auf, während der Anwender blinzelt. Gesichtsproben werden für jedes Bild durch Definieren von Orten von Eckpunkten in der Gesichtsprobe unter Verwendung von Tiefenwerten für die Pixel in dem Bild berechnet, und Texturwerte werden für jeden Eckpunkt unter Verwendung der RGB-Werte des entsprechenden Pixels definiert. Die Gesichtsproben für die unterschiedlichen Bilder werden dann ausgerichtet, z. B. unter Verwendung eines iterativen „Closest Point“-Algorithmus (ICP-Algorithmus), gefiltert und kombiniert, um ein 3D-Referenzmodell des Gesichts des Anwenders zu erzeugen. Die Texturen für jede aus den Gesichtsproben werden dann erneut abgebildet, um an dem 3D-Referenzmodell ausgerichtet zu werden. Sowohl das Referenzmodell und die Gesichtsproben, die unterschiedlichen Blickrichtungen entsprechen, als auch die Gesichtsprobe für den blinzelnden Anwender werden als eine Blick-Datenbank, die nach der Blickrichtung indexiert ist, gespeichert.A 3D model of the user's face is generated by capturing multiple images of the user's face corresponding to several different viewing directions. In some embodiments, a camera captures the multiple images as the user tracks a moving target image on a screen. For example, a camera may be implemented as an RGBD camera, the RGB values of pixels in the image, and a depth value for each pixel that is a distance between the camera and the object represented by the pixel. The viewing direction for each image is then determined based on the relative positions of the user's eyes, the camera, and the moving image on the screen. The camera also takes a picture while the user blinks. Face samples are calculated for each image by defining locations of vertices in the face sample using depth values for the pixels in the image, and texture values are defined for each vertex using the RGB values of the corresponding pixel. The face samples for the different images are then aligned, e.g. Using an iterative Closest Point (ICP) algorithm, and combined to produce a 3D reference model of the user's face. The textures for each of the facial samples are then remapped to align with the 3D reference model. Both the reference model and the face samples corresponding to different line directions, as well as the face sample for the blinking user are stored as a look database indexed in the line of sight.
Ein Abschnitt eines HMD, der die Augen eines Anwenders verdeckt, wird aus einem Bild des Anwenders, das durch eine Kamera aufgenommen wird, durch Bestimmen einer 3D-Stellung, die eine Orientierung und einen Ort des Gesichts des Anwenders in Bezug auf die Kamera angibt, und Wiedergeben eines Abschnitts eines 3D-Modells des Gesichts des Anwenders in das Bild, um den Abschnitt des HMD zu ersetzen, entfernt. In einigen Ausführungsformen wird der Blick eines Anwenders durch eine Augenverfolgungseinheit, die in dem HMD implementiert ist, bestimmt, und der verfolgte Blick wird verwendet, um das geeignete 3D-Modell des Gesichts des Anwenders (oder die Textur, die verwendet wird, um einen Abschnitt des Gesichts des Anwenders wiederzugeben) aus der Datenbank, die nach der Blickrichtung indexiert ist, auszuwählen. Die 3D-Stellung des Gesichts des Anwenders relativ zu der Kamera wird basierend auf einer Transformation des HMD-Koordinatensystems in das Kamera-Koordinatensystem, der HMD-Stellung in dem HMD-Koordinatensystem und der 3D-Stellung des Gesichts des Anwenders relativ zu dem HMD bestimmt. Die Kamera-zu-HMD-Transformation wird durch Abgleichen eines Modells des HMD mit dem Bild, das durch die Kamera aufgenommen ist, bestimmt. Die 3D-Stellung des Gesichts des Anwenders relativ zu dem HMD wird durch Abgleichen eines nicht verdeckten Abschnitts des Gesichts des Anwenders (z. B. des Kinns oder der Stirn des Gesichts des Anwenders) mit dem Bild des Anwenders, das durch die Kamera aufgenommen ist, bestimmt. Beispielsweise kann die 3D-Stellung unter Verwendung von ICP-Abgleichen der nicht verdeckten Abschnitte des Gesichts des Anwenders in dem Bild mit dem 3D-Modell, das in unterschiedlichen Orientierungen, die unterschiedlichen Kandidaten-3D-Stellungen entsprechen, wiedergegeben wird, bestimmt werden. Die 3D-Stellung des Gesichts des Anwenders relativ zu der Kamera wird dann vollständig bestimmt, falls das HMD Stellungsdaten bereitstellt, z. B. stellt ein Oculus Rift oder HTC Vive Stellungsdaten mit 6 Freiheitsgraden (6DoF) bereit. Falls jedoch das HMD keine Stellungsdaten bereitstellt, z. B. stellen Google Cardboard oder Daydream View nur 3DoF-Stellungsdaten oder überhaupt keine Stellungsdaten bereit, wird die HMD-Stellung in dem HMD-Koordinatensystem durch einen Abgleichsprozess, der in Echtzeit für jedes Bild ausgeführt wird, bestimmt.A portion of an HMD that obscures a user's eyes becomes an image of the user captured by a camera by determining a 3D position indicating an orientation and a location of the user's face with respect to the camera. and playing a portion of a 3D model of the user's face into the image to replace the portion of the HMD. In some embodiments, a user's gaze is determined by an eye tracking unit implemented in the HMD, and the tracked gaze is used to determine the appropriate 3D model of the user's face (or the texture used to make a section of the user's face) from the database indexed according to the viewing direction. The 3D position of the user's face relative to the camera is determined based on a transformation of the HMD coordinate system into the camera coordinate system, the HMD position in the HMD coordinate system, and the 3D position of the user's face relative to the HMD , The camera-to-HMD transformation is determined by matching a model of the HMD with the image captured by the camera. The 3D position of the user's face relative to the HMD is made by matching an uncovered portion of the user's face (eg, the user's chin or forehead) with the user's image captured by the camera , certainly. For example, the 3D position may be determined using ICP matching of the uncovered portions of the user's face in the image with the 3D model represented in different orientations corresponding to different candidate 3D positions. The 3D position of the user's face relative to the camera is then completely determined if the HMD provides positional data, e.g. For example, an Oculus Rift or HTC Vive provides 6 degrees of freedom (6DoF) posture data. However, if the HMD does not provide position data, e.g. For example, if Google Cardboard or Daydream View only provide 3DoF pose data or no pose data at all, the HMD position in the HMD coordinate system is determined by a matching process performed in real time for each frame.
Sobald die 3D-Stellung des Gesichts des Anwenders relativ zu der Kamera bestimmt ist, wird ein Abschnitt des 3D-Modells des Gesichts des Anwenders, der den verdeckten Abschnitten des Gesichts des Anwenders entspricht, aus der Perspektive der Kamera wiedergegeben. Der Abschnitt des 3D-Modells des Gesichts des Anwenders wird unter Verwendung von Texturproben, die aus einer Blick-Datenbank ausgewählt werden, wiedergegeben. In einigen Ausführungsformen enthält das HMD eine Augenverfolgungseinheit, die eine Blickrichtung detektiert, die jedem Bild entspricht, und die Blickrichtung wird als ein Index verwendet, um Texturproben in der Blick-Datenbank, die verwendet werden, um den Abschnitt des 3D-Modells wiederzugeben, zu identifizieren. In einigen Ausführungsformen werden die Texturproben unter Verwendung einer affinen Transformation in dem RGB-Raum, die Farben der Texturproben auf Farben eines nicht verdeckten Abschnitts des Gesichts des Anwenders in dem durch die Kamera aufgenommenen Bild abbildet, farbkorrigiert. In einigen Ausführungsformen variiert eine Lichtdurchlässigkeit (a) des wiedergegebenen Abschnitts des 3D-Modells des Gesichts des Anwenders von einer Mitte zu einem Rand des wiedergegebenen Abschnitts. Beispielsweise kann die Lichtdurchlässigkeit (a) von einem Wert von 1 (z. B. das HMD ist vollständig durchsichtig) in der Mitte des HMD zu einem Wert von null (z. B. das HMD ist vollständig lichtundurchlässig) an den Rändern des HMD variieren.Once the 3D position of the user's face relative to the camera is determined, a portion of the 3D model of the user's face corresponding to the hidden portions of the user's face is displayed from the perspective of the camera. The portion of the 3D model of the user's face is rendered using texture samples selected from a gaze database. In some embodiments, the HMD includes an eye tracking unit that detects a line of sight that corresponds to each image, and the line of sight is used as an index to obtain texture samples in the look database that are used to render the portion of the 3D model identify. In some embodiments, the texture samples are color corrected using an affine transformation in the RGB space that maps colors of the texture samples to colors of an uncovered portion of the user's face in the image captured by the camera. In some embodiments, translucency (a) of the rendered portion of the 3D model of the user's face varies from a center to an edge of the rendered portion. For example, the translucency (a) may vary from a value of 1 (eg, the HMD is completely transparent) at the center of the HMD to a value of zero (eg, the HMD is completely opaque) at the edges of the HMD ,
In einem System mit gemischter Realität werden tatsächliche Bilder eines Anwenders (und anderer Objekte in einer physikalischen Szene) mit virtuellen Bildern (die Avatare der Anwender enthalten können) kombiniert, um eine Szene mit gemischter Realität zu erzeugen. Ein HMD, das durch den Anwender getragen wird, wird (wenigstens teilweise) aus einem Bild des Anwenders, das durch eine Kamera aufgenommen wird, entfernt und in einer Szene mit gemischter Realität durch Wiedergeben eines Abschnitts eines 3D-Modells des Gesichts des Anwenders basierend auf einer Blickrichtung des Anwenders und einer Stellung des HMD relativ zur Kamera präsentiert. Pixel in dem wiedergegebenen Abschnitt des 3D-Modells werden verwendet, um entsprechende Pixel, die für das HMD repräsentativ sind, in dem Bild zu überschreiben. In einigen Ausführungsformen wird die Stellung des HMD relativ zu der RGB-Kamera bestimmt durch Verfolgen von Unterscheidungsmerkmalen auf einer Oberfläche des HMD in dem durch die Kamera aufgenommenen Bild. Beispielsweise können Aruco-Markierungen auf dem HMD platziert werden, und die Stellung des HMD kann relativ zu der Kamera durch Detektieren der Aruco-Markierung in den durch die Kamera aufgenommenen Bildern verfolgt werden. Signalströme, die für die HMD-Stellungstelemetrie repräsentativ sind, und die Bilder, die durch die Kamera aufgenommen sind, werden durch Kreuzkorrelieren der Bewegung der verfolgten Merkmale in den Bildern, die durch die Kamera aufgenommen sind, mit der entsprechenden Bewegung des HMD, die durch die HMD-Stellungstelemetrie angegeben ist, synchronisiert. Ein Signalstrom, der für die Blickrichtung des Anwenders repräsentativ ist, wird mit der HMD-Stellungstelemetrie unter Verwendung eines Takts, der durch das HMD und eine Augenverfolgungseinheit gemeinsam verwendet wird, oder alternativ durch Kreuzkorrelieren der HMD-Stellungstelemetrie und der Blickrichtungen während einer vorbestimmten Bewegung des Anwenders, der das HMD trägt, synchronisiert.In a mixed reality system, actual images of a user (and other objects in a physical scene) are combined with virtual images (which may include avatars of users) to create a mixed reality scene. An HMD carried by the user is removed (at least in part) from an image of the user captured by a camera and in a mixed reality scene based on rendering a portion of a 3D model of the user's face a viewing direction of the user and a position of the HMD relative to the camera presents. Pixels in the rendered section of the 3D model are used to overwrite corresponding pixels representative of the HMD in the image. In some embodiments, the position of the HMD relative to the RGB camera is determined by tracking distinguishing features on a surface of the HMD in the image captured by the camera. For example, Aruco marks may be placed on the HMD, and the position of the HMD may be tracked relative to the camera by detecting the Aruco mark in the images taken by the camera. Signal streams representative of the HMD position telemetry and the images captured by the camera are detected by cross-correlating the motion of the tracked features in the images captured by the camera with the corresponding motion of the HMD passing through the HMD position telemetry is specified, synchronized. A signal stream representative of the line of sight of the user is detected with the HMD position telemetry using a clock shared by the HMD and an eye tracking unit, or alternatively by cross correlating the HMD position telemetry and the view directions during a predetermined movement of the User carrying the HMD synchronized.
1 ist ein Blockdiagramm, das eine Seitenansicht 100 eines Computersystems, das konfiguriert ist, eine Blick-Datenbank 105 aufzunehmen, gemäß einigen Ausführungsformen darstellt. Das Computersystem enthält einen Prozessor 110 und einen Speicher 115. Der Prozessor 110 wird verwendet, um Anweisungen auszuführen, die in dem Speicher 115 gespeichert sind, und Informationen wie z. B. die Ergebnisse der ausgeführten Anweisungen in dem Speicher zu speichern. Beispielsweise kann der Speicher 115 die Blick-Datenbank 105, die durch den Prozessor 110 erzeugt wird, speichern. Der Prozessor 110 ist mit einem Bildschirm 120 verbunden, der konfiguriert ist, Bilder für einen Anwender 125 basierend auf Informationen, die durch den Prozessor 110 für den Bildschirm 120 bereitgestellt werden, anzuzeigen. Beispielsweise kann der Prozessor 110 Werte von Pixeln erzeugen, die für ein Bild repräsentativ sind, und die Pixelwerte für den Bildschirm bereitstellen, der die Pixelwerte verwendet, um die Eigenschaften des Lichts, das durch Pixel des Bildschirms 120 emittiert wird, um das Bild zu erzeugen, zu steuern. 1 is a block diagram that is a side view 100 of a computer system that is configured, a look database 105 represents, according to some embodiments represents. The computer system includes a processor 110 and a memory 115 , The processor 110 is used to execute statements in the memory 115 are stored, and information such. For example, store the results of the executed instructions in memory. For example, the memory 115 the look database 105 that through the processor 110 is generated, store. The processor 110 is with a screen 120 connected, which is configured images for a user 125 based on information provided by the processor 110 for the screen 120 be provided. For example, the processor 110 Generate values of pixels that are representative of an image and provide the pixel values for the screen that uses the pixel values to determine the properties of light passing through pixels of the screen 120 is emitted to generate the image to control.
Eine Kamera 130 wird verwendet, um Bilder des Anwenders 125 aufzunehmen und Informationen, die für die aufgenommenen Bilder repräsentativ sind, für den Prozessor 110 bereitzustellen. Einige Ausführungsformen der Kamera 130 sind als Rot-Grün-Blau-Tiefe-Kamera (RGBD-Kamera) implementiert, die RGB-Werte für eine Menge von Kamerapixeln basierend auf Licht, das auf Licht sammelnde Elemente in der Kamera 130 wie z. B. Ladungskopplungseinrichtungen (CCDs) fällt, erzeugt. Die RGBD-Kamera 130 bestimmt außerdem Tiefenwerte für jedes der Kamerapixel. Die Tiefenwerte repräsentieren einen Abstand von der RGBD-Kamera 130 zu dem Abschnitt der Szene, die durch das entsprechende Kamerapixel repräsentiert wird. Einige Ausführungsformen der RGBD-Kamera 130 enthalten eine Infrarotquelle, um die Szene mit einem Infrarotgrautonmuster zu beleuchten, und einen Infrarotsensor, um reflektiertes Infrarotlicht aufzunehmen. Die RGBD-Kamera 130 kann bekannte Algorithmen verwenden, um die Tiefen, die jedem Kamerapixel zugeordnet sind, basierend auf dem reflektierten Infrarotlicht zu bestimmen.A camera 130 is used to take pictures of the user 125 and represent information representative of the captured images for the processor 110 provide. Some embodiments of the camera 130 are implemented as a red-green-blue-depth camera (RGBD camera) that provides RGB values for a lot of camera pixels based on light that is on light-gathering elements in the camera 130 such as B. Charge coupling devices (CCDs) falls generated. The RGBD camera 130 Also determines depth values for each of the camera pixels. The depth values represent a distance from the RGBD camera 130 to the portion of the scene represented by the corresponding camera pixel. Some embodiments of the RGBD camera 130 include an infrared source to illuminate the scene with an infrared gray tone pattern, and an infrared sensor to pick up reflected infrared light. The RGBD camera 130 may use known algorithms to determine the depths associated with each camera pixel based on the reflected infrared light.
Eine Augenverfolgungseinheit 135 wird verwendet, um Bewegungen und Positionen der Augen des Anwenders 125 durch Messen des Blickpunkts des Anwenders 125 oder Messen der Bewegung der Augen relativ zu dem Kopf des Anwenders 125 zu verfolgen. Einige Ausführungsformen der Augenverfolgungseinheit 135 implementieren ein kontaktloses optisches Verfahren zum Messen von Augenbewegung. Beispielsweise kann die Augenverfolgungseinheit 135 Infrarotlicht erzeugen, das wenigstens einen Abschnitt des Gesichts des Anwenders 125, der die Augen des Anwenders enthält, beleuchtet. Das Infrarotlicht wird aus den Augen des Anwenders reflektiert und analysiert (entweder durch die Augenverfolgungseinheit 135 oder durch den Prozessor 110), um Informationen, die die Bewegung oder Drehung der Augen angeben, basierend auf Änderungen der Eigenschaften des reflektierten Infrarotlichts zu extrahieren. Andere Typen von Augenverfolgungseinheiten können jedoch ebenfalls verwendet werden, um Bewegungen und Positionen der Augen des Anwenders 125 zu verfolgen. Beispielsweise kann die Augenbewegung unter Verwendung von Augenzubehör wie z. B. speziell konstruierte Kontaktlinsen, Elektroden, die in der Nähe der Augen platziert sind, und dergleichen detektiert werden. Obwohl das Computersystem sowohl die Kamera 130 als auch die Augenverfolgungseinheit 135 enthält, ist das nicht in allen Ausführungsformen des Computersystems erforderlich. Einige Ausführungsformen des Computersystems enthalten entweder die Kamera 130 oder die Augenverfolgungseinheit 135.An eye tracking unit 135 is used to control movements and positions of the user's eyes 125 by measuring the viewpoint of the user 125 or measuring the movement of the eyes relative to the user's head 125 to pursue. Some embodiments of the eye tracking unit 135 implement a non-contact optical method for measuring eye movement. For example, the eye tracking unit 135 Infrared light generate the at least a portion of the user's face 125 , which contains the eyes of the user, lights up. The infrared light is reflected from the user's eyes and analyzed (either by the eye tracking unit 135 or by the processor 110 ) to extract information indicating the movement or rotation of the eyes based on changes in the characteristics of the reflected infrared light. However, other types of eye tracking units may also be used to control movements and positions of the user's eyes 125 to pursue. For example, the eye movement using eye accessories such. For example, specially designed contact lenses, electrodes placed near the eyes, and the like are detected. Although the computer system both the camera 130 as well as the eye tracking unit 135 This is not required in all embodiments of the computer system. Some embodiments of the computer system either contain the camera 130 or the eye tracking unit 135 ,
Die Kamera 130 nimmt Bilder des Gesichts des Anwenders auf, die unterschiedlichen Blickrichtungen der Augen des Anwenders in den Bildern entsprechen. Beispielsweise kann der Prozessor 110 Bilder des Anwenders 125 aufzeichnen, die durch die Kamera 130 aufgenommen werden, während der Anwender 125 ein Zielbild, das auf dem Bildschirm 120 angezeigt wird, beobachtet. Das Zielbild bewegt sich zu unterschiedlichen Orten auf dem Bildschirm 120, und es wird erwartet, dass der Anwender 125 dem Zielbild mit seinen Augen folgt. Die Kamera 130 nimmt ein Bild des Anwenders 125 auf, während der Anwender 125 auf jeden aus den unterschiedlichen Orten auf dem Bildschirm 120, die durch das Zielbild angegeben sind, schaut. Somit erzeugt die Kamera 130 eine Menge von Bildern, die das Gesicht des Anwenders aufzeichnen, während der Anwender in einer entsprechenden Menge von Blickrichtungen schaut. Der Anwender 125 wird außerdem angewiesen, zu blinzeln, und die Kamera 130 nimmt ein oder mehrere Bilder des Anwenders 125 auf, während der Anwender 125 blinzelt.The camera 130 captures images of the user's face that correspond to different directions of the user's eyes in the images. For example, the processor 110 Pictures of the user 125 Record by the camera 130 be recorded while the user 125 a target image on the screen 120 is displayed, observed. The target image moves to different locations on the screen 120 , and it is expected that the user 125 follows the target image with his eyes. The camera 130 takes a picture of the user 125 on, while the user 125 on everyone the different places on the screen 120 that are indicated by the target image looks. Thus, the camera generates 130 a set of images that record the user's face while the user is looking in a corresponding set of viewing directions. The user 125 is also instructed to blink, and the camera 130 takes one or more pictures of the user 125 on, while the user 125 winks.
Der Prozessor 110 bestimmt dreidimensionale Orte (3D-Orte) der Augen des Anwenders 125 in jedem aus den gesammelten Bildern durch Anwenden eines Gesichtserkennungsalgorithmus, um eine Position des Gesichts des Anwenders in dem Bild zu detektieren. Ein Orientierungspunktalgorithmus kann dann verwendet werden, um die Positionen der Augen des Anwenders in einem zweidimensionalen RGB-Bild (2D-RGB-Bild), das durch den Rahmen der Kamera 130 definiert ist, zu lokalisieren. Die 2D-Orte der Augen des Anwenders in dem Bild werden in einen entsprechenden Ort in dem Tiefen-Kanal (D-Kanal) der Kamera 130 unter Verwendung einer vorbestimmten Kalibrierung zwischen Tiefe und RGB-Werten für die Pixel in dem Bild umgesetzt. Die 2D-Orte der Augen in dem Tiefenkanal können deshalb verwendet werden, um die entsprechenden 3D-Orte der Augen unter Verwendung der bekannten intrinsischen Kalibrierungsparameter der Kamera 130 zu berechnen. In Ausführungsformen des Computersystems, die die Augenverfolgungseinheit 135 integrieren, werden Verfolgungsinformationen, die durch die Augenverfolgungseinheit 135 gleichzeitig damit, dass die Kamera 130 Bild aufnimmt, erfasst werden, verwendet, um die Schätzungen der 3D-Orte der Augen in den Bildern zu verfeinern oder zu verbessern.The processor 110 determines three-dimensional locations (3D locations) of the user's eyes 125 in each of the collected images by applying a face recognition algorithm to detect a position of the user's face in the image. A landmark algorithm can then be used to map the positions of the user's eyes in a two-dimensional RGB image (2D RGB image) through the frame of the camera 130 is defined to locate. The 2D locations of the user's eyes in the image become a corresponding location in the depth channel (D channel) of the camera 130 using a predetermined calibration between depth and RGB values for the pixels in the image. The 2D locations of the eyes in the depth channel can therefore be used to locate the corresponding 3D locations of the eyes using the camera's known intrinsic calibration parameters 130 to calculate. In embodiments of the computer system that includes the eye tracking unit 135 Integrate, tracking information, by the eye tracking unit 135 at the same time that the camera 130 Capture image is used to refine or enhance the estimates of the 3D locations of the eyes in the images.
Die Orte des Zielbilds auf dem Bildschirm 120 werden durch 2D-Koordinaten in der Ebene des Bildschirms 120 definiert. Die Kalibrierungsinformationen werden verwendet, um Positionen und Orientierungen des Bildschirms 120 und der Kamera 130 in einem Koordinatensystem 140 der Kamera 130 zu bestimmen. In einigen Ausführungsformen werden die Kalibrierungsinformationen unter Verwendung eines Vorprozesses bestimmt, der vor dem Aufnehmen von Bildern, die verwendet werden, um die Blick-Datenbank 105 zu erzeugen, ausgeführt wird. Die Kalibrierung ist als eine Transformation repräsentiert, die die 2D-Koordinaten in der Ebene des Bildschirms 120 in 3D-Orte in dem Koordinatensystem 140, das durch die Kamera 130 definiert ist, umsetzt.The locations of the destination image on the screen 120 be through 2D coordinates in the plane of the screen 120 Are defined. The calibration information is used to position and orient the screen 120 and the camera 130 in a coordinate system 140 the camera 130 to determine. In some embodiments, the calibration information is determined using a pre-process that is prior to capturing images that are used to view the gaze database 105 to generate is executed. The calibration is represented as a transformation representing the 2D coordinates in the plane of the screen 120 in 3D locations in the coordinate system 140 , which is defined by the camera 130, converts.
Die 3D-Orte der Augen des Anwenders und die 3D-Orte des Zielbilds, die für jedes Bild, das durch die Kamera 130 aufgenommen wird, bestimmt werden, werden verwendet, um Blickvektoren, die die Blickrichtung für den Anwender 125 in jedem der Bilder angeben, zu bestimmen. Beispielsweise ist eine erste Blickrichtung 145 für das erste Bild durch die relativen Positionen des 3D-Orts der Augen des Anwenders in einem ersten Bild und des 3D-Orts des Zielbilds, während das erste Bild erfasst wurde, definiert. Als ein weiteres Beispiel ist eine zweite Blickrichtung 150 für das zweite Bild durch die relativen Positionen des 3D-Orts der Augen des Anwenders in einem zweiten Bild und des 3D-Orts des Zielbilds, während das zweite Bild erfasst wurde, definiert. Die erste Blickrichtung 145 ist als ein erster Winkel 155 relativ zu einer Mittelachse 160 repräsentiert, und die zweite Blickrichtung 150 ist als ein zweiter Winkel 165 relativ zu der Mittelachse 160 repräsentiert. In der Seitenansicht 100 sind die Blickrichtungen 145, 150 und die Winkel 155, 165 in einer vertikalen Ebene dargestellt. In einigen Ausführungsformen werden Schwenk/Neigungswinkel verwendet, um die Blickrichtungen 145, 150 in dem Koordinatensystem 140 der Kamera 130 zu repräsentieren.The 3D locations of the user's eyes and the 3D locations of the target image, for each image created by the camera 130 are determined, are used to view vectors, the line of sight for the user 125 specify in each of the pictures to determine. For example, a first gaze direction 145 for the first image is defined by the relative positions of the 3D location of the user's eyes in a first image and the 3D location of the target image while the first image was captured. As another example, a second viewing direction 150 for the second image is defined by the relative positions of the 3D location of the user's eyes in a second image and the 3D location of the target image while the second image was acquired. The first line of sight 145 is as a first angle 155 relative to a central axis 160 and the second viewing direction 150 is a second angle 165 relative to the central axis 160 represents. In the side view 100 are the directions of view 145 . 150 and the angles 155 . 165 shown in a vertical plane. In some embodiments, pan / tilt angles are used to determine the viewing directions 145 . 150 in the coordinate system 140 the camera 130 to represent.
2 ist ein Blockdiagramm, das eine Draufsicht 200 eines Computersystems das verwendet wird, um die Blick-Datenbank 105 aufzunehmen, gemäß einigen Ausführungsformen darstellt. Das Computersystem enthält einen Prozessor 110, einen Speicher 115, einen Bildschirm 120, eine Kamera 130 und (optional) eine Augenverfolgungseinheit 135. Wie hier mit Bezug auf 1 diskutiert ist, ist der Prozessor 110 konfiguriert, Blickvektoren, die die Blickrichtung für den Anwender 125 in jedem aus den Bildern , die durch die Kamera 130 aufgenommen sind, angeben, unter Verwendung der 3D-Orte der Augen des Anwenders und der 3D-Orte des Zielbilds, die für jedes durch die Kamera 130 aufgenommene Bild bestimmt werden, zu bestimmen. Beispielsweise ist die erste Blickrichtung 145 für das erste Bild durch die relativen Positionen des 3D-Orts der Augen des Anwenders in einem ersten Bild und des 3D-Orts des Zielbilds, während das erste Bild erfasst wurde, definiert. Als ein weiteres Beispiel ist die zweite Blickrichtung 150 für das zweite Bild durch die relativen Positionen des 3D-Orts der Augen des Anwenders in einem zweiten Bild und des 3D-Orts des Zielbilds, während das zweite Bild erfasst wurde, definiert. In der Draufsicht 200 ist die erste Blickrichtung 145 als ein dritter Winkel 205 relativ zu der Mittelachse 160 repräsentiert, und die zweite Blickrichtung 150 ist als ein vierter Winkel 210 relativ zu der Mittelachse 160 repräsentiert. In der Draufsicht 200 sind die Blickrichtungen 145, 150 und die Winkel 205, 210 in einer horizontalen Ebene, die senkrecht zu der vertikalen Eben in der in 1 gezeigten Seitenansicht 100 ist, dargestellt. 2 is a block diagram, which is a top view 200 a computer system used to view the gaze database 105 represents, according to some embodiments represents. The computer system includes a processor 110, a memory 115 , a screen 120 , a camera 130 and (optionally) an eye tracking unit 135 , As here with respect to 1 is discussed is the processor 110 configured, view vectors, the line of sight for the user 125 in each one of the pictures taken by the camera 130 Specify, using the 3D locations of the user's eyes and the 3D locations of the target image, for each by the camera 130 taken picture to be determined. For example, the first line of sight 145 for the first image is defined by the relative positions of the 3D location of the user's eyes in a first image and the 3D location of the target image while the first image was captured. As another example, the second line of sight 150 for the second image is defined by the relative positions of the 3D location of the user's eyes in a second image and the 3D location of the target image while the second image was acquired. In the plan view 200 is the first line of sight 145 as a third angle 205 relative to the central axis 160 represents, and the second line of sight 150 is as a fourth angle 210 relative to the central axis 160 represents. In the plan view 200 are the directions of view 145 . 150 and the angles 205 . 210 in a horizontal plane that is perpendicular to the vertical plane in the in 1 shown side view 100 is presented, layed out.
3 stellt einen Bildschirm 300 und einen Anwender 305 während eines Prozesses zum Aufnehmen von Bildern des Anwenders 305 zum Erzeugen einer Blick-Datenbank gemäß einigen Ausführungsformen dar. Eine Kamera 310 und (optional) eine Augenverfolgungseinheit 315 werden verwendet, um Bilder des Anwenders 305 zum Erzeugen einer Blick-Datenbank aufzunehmen, wie hier diskutiert ist. Der Bildschirm 300 wird verwendet, um einige Ausführungsformen der in den 1 und 2 gezeigten Bildschirms 120 zu implementieren, die Kamera 310 wird verwendet, um einige Ausführungsformen der in den 1 und 2 gezeigten Kamera 130 zu implementieren, die Augenverfolgungseinheit 315 wird verwendet, um einige Ausführungsformen der in den 1 und 2 gezeigten Augenverfolgungseinheit 135 zu implementieren, und der Anwender 305 entspricht dem in den 1 und 2 gezeigten Anwender 125. Der Bildschirm 300 und der Anwender 305 sind in zwei Zeitintervallen dargestellt, die zwei Fällen 320, 325 entsprechen, in denen die Kamera 310 und (optional) die Augenverfolgungseinheit 315 Bilder des Gesichts des Anwenders 305 aufnehmen. 3 puts a screen 300 and a user 305 during a process of taking pictures of the user 305 for generating a gaze database according to some embodiments. A camera 310 and (optionally) an eye tracking unit 315 are used to take pictures of the user 305 to create a gaze database as discussed herein. The screen 300 is used to some embodiments of the in the 1 and 2 shown screen 120 to implement the camera 310 is used to some embodiments of the in the 1 and 2 shown camera 130 to implement the eye tracking unit 315 is used to some embodiments of the in the 1 and 2 shown eye tracking unit 135 to implement, and the user 305 corresponds to that in the 1 and 2 shown user 125 , The screen 300 and the user 305 are shown in two time intervals, the two cases 320 . 325 in which the camera 310 and (optionally) the eye tracking unit 315 Pictures of the face of the user 305 take up.
Eine Transformation zwischen dem Ort des Bildschirms 300 und dem Ort der Kamera 310 wird unter Verwendung eines Kalibrierungsprozesses bestimmt. Beispielsweise kann ein Abstand zwischen einer Mitte der Kamera 310 und einer Mitte des Bildschirms 300 unter Halten der Kamera 310 und des Bildschirms 310 in festen relativen Positionen, z. B. parallel zueinander, manuell gemessen werden. Ein Versatz zwischen dem Bildschirm 300 und der Kamera 310 kann durch Halten des Bildschirms 300 nahe an der Kamera 310 minimiert werden, so dass ein Gesichtsbild mit einem frontalen (0, 0) Blickwinkel auch eine frontale Gesichtsstellung aufweist. Obwohl manuelle Kalibrierung eine Näherung ist, ist die Genauigkeit der manuellen Kalibrierung typischerweise für Anwendungsfälle, die keine extreme Präzision erfordern, ausreichend. Als ein weiteres Beispiel kann ein spiegelbasiertes automatisches Kalibrierungsverfahren eingesetzt werden. In diesem Fall wird ein bekanntes Muster auf dem Bildschirm 300 angezeigt und über einen Spiegel (in 3 nicht gezeigt), der in unterschiedlichen Orientierungen positioniert ist, auf die Kamera zurück reflektiert. Das reflektierte Bild kann mit dem auf dem Bildschirm 300 angezeigten bekannten Muster verglichen werden (z. B. räumlich korreliert), um die Transformation zwischen dem Bildschirm 300 und der Kamera 310 zu bestimmen. Eine Transformation zwischen dem Ort des Bildschirms 300 und dem Ort der Augenverfolgungseinheit 315 kann unter Verwendung der gleichen Techniken in Ausführungsformen, die die Augenverfolgungseinheit 315 enthalten, bestimmt werden.A transformation between the location of the screen 300 and the location of the camera 310 is determined using a calibration process. For example, a distance between a center of the camera 310 and a center of the screen 300 while holding the camera 310 and the screen 310 in fixed relative positions, e.g. B. parallel to each other, are measured manually. An offset between the screen 300 and the camera 310 can by holding the screen 300 close to the camera 310 be minimized so that a facial image with a frontal ( 0 . 0 ) Also has a frontal facial position. Although manual calibration is an approximation, the accuracy of manual calibration is typically sufficient for applications that do not require extreme precision. As another example, a mirror-based automatic calibration method may be used. In this case, a familiar pattern will appear on the screen 300 displayed and a mirror (in 3 not shown) positioned in different orientations, reflected back onto the camera. The reflected image can match the one on the screen 300 displayed known patterns are compared (eg spatially correlated) to the transformation between the screen 300 and the camera 310 to determine. A transformation between the location of the screen 300 and the location of the eye tracking unit 315 can be done using the same techniques in embodiments that the eye tracking unit 315 be determined.
Vor dem Aufnehmen eines Bilds des Anwenders 305 während des ersten Falls 320 wird ein Zielbild 330 zu einem Ort auf dem Bildschirm 300 bewegt. Der Ort des Zielbilds 300 wird durch ein Raster 335 von Orten bestimmt. Ein Bild des Anwenders 305 wird durch die Kamera 310 und (optional) die Augenverfolgungseinheit 315 während des Falls 320 aufgenommen, während der Anwender 305 auf das Zielbild 330 schaut. In der dargestellten Ausführungsform scheinen die Augen des Anwenders 305 nach rechts zu schauen, und der Ort des Zielbilds 330 ist auf der linken Seite des Bildschirms 300, weil der Anwender 305 zu dem Bildschirm 300 weist. Das Bild, das durch die Kamera 310 und (optional) die Augenverfolgungseinheit 315 aufgenommen wird, wird verwendet, um eine entsprechende Blickrichtung während des ersten Falls 320 unter Verwendung eines 3D-Orts des Zielbilds 330 und von 3D-Orten der Augen des Anwenders zu bestimmen, wie hier diskutiert ist.Before taking a picture of the user 305 during the first case 320 becomes a target image 330 to a place on the screen 300 emotional. The location of the target image 300 is through a grid 335 determined by locations. An image of the user 305 is through the camera 310 and (optionally) the eye tracking unit 315 during the fall 320 recorded while the user 305 on the target image 330 looks. In the illustrated embodiment, the eyes of the user appear 305 to look to the right, and the location of the target image 330 is on the left side of the screen 300 because of the user 305 to the screen 300 has. The picture taken by the camera 310 and (optionally) the eye tracking unit 315 is used, a corresponding line of sight during the first case 320 using a 3D location of the target image 330 and from 3D locations of the user's eyes, as discussed herein.
Vor dem Aufnehmen eines Bilds des Anwenders 305 während des zweiten Falls 325 wird das Zielbild 330 zu einem anderen Punkt in dem Raster 335, der einem anderen Ort auf dem Bildschirm 300 zugeordnet ist, bewegt. Der Ort des Zielbilds 330 auf dem Raster 335 kann in Reaktion auf eine Eingabe von dem Anwender 305 modifiziert werden, oder das Zielbild 330 kann automatisch zu der neuen Position auf dem Raster 335 weiterbewegt werden, während der Anwender 305 dem Zielbild 330 mit seinen Augen folgt. Ein Bild des Anwenders 305 wird durch die Kamera 310 und (optional) die Augenverfolgungseinheit 315 während des Falls 325 aufgenommen, während der Anwender 305 auf das Zielbild 330 an dem anderen Ort schaut. In der dargestellten Ausführungsform scheinen die Augen des Anwenders 305 nach links zu schauen, und der Ort des Zielbilds 330 ist auf der rechten Seite des Bildschirms 300, weil der Anwender 305 zu dem Bildschirm 300 gewandt ist. Das Bild, das durch die Kamera 310 und (optional) die Augenverfolgungseinheit 315 aufgenommen wird, wird verwendet, um eine entsprechende Blickrichtung während des zweiten Falls 325 unter Verwendung eines 3D-Orts des Zielbilds 330 und von 3D-Orten der Augen des Anwenders zu bestimmen, wie hier diskutiert ist.Before taking a picture of the user 305 during the second case 325 becomes the target image 330 to another point in the grid 335 that's another place on the screen 300 is assigned, moves. The location of the target image 330 on the grid 335 can in response to an input from the user 305 be modified, or the target image 330 can automatically move to the new position on the grid 335 while the user 305 is moving to the target image 330 with his eyes following. An image of the user 305 is through the camera 310 and (optionally) the eye tracking unit 315 during the case 325 while the user 305 on the target image 330 looks in the other place. In the illustrated embodiment, the eyes of the user appear 305 to look to the left, and the location of the target image 330 is on the right side of the screen 300 because of the user 305 to the screen 300 is turned. The picture taken by the camera 310 and (optionally) the eye tracking unit 315 is captured is used to provide a corresponding line of sight during the second case 325 using a 3D location of the target image 330 and from 3D locations of the user's eyes, as discussed herein.
Ein vollständiger Blick-Aufnahmeprozess enthält Bewegen des Zielbilds 330 zu jedem aus den Orten auf dem Raster 335 und Aufnehmen von Bildern des Anwenders 305 in jedem entsprechenden Fall. Der Blick-Aufnahmeprozess enthält außerdem Aufnehmen eines Bilds des Anwenders 305 während der Anwender 325 seine Augen geschlossen hat. Dieses Bild ist als ein „Blinzelbild“ bezeichnet. Die Menge von Bildern, die durch die Kamera 310 und (optional) die Augenverfolgungseinheit 315 aufgenommen wird, wird verwendet, um Modelle des Gesichts des Anwenders 305, die den Blickrichtungen entsprechen, die jedem aus den Bildern zugeordnet sind, zu erzeugen. Die Modelle sind hier als „Proben“ des Gesichts des Anwenders bezeichnet.A complete gaze capture process involves moving the target image 330 to each of the locations on the grid 335 and taking pictures of the user 305 in each case. The gaze recording process also includes taking a picture of the user 305 while the user 325 his eyes closed. This picture is called a "blinking picture". The amount of pictures taken by the camera 310 and (optionally) the eye tracking unit 315 is used to model the user's face 305 that generate the line of sight associated with each of the pictures. The models are referred to here as "samples" of the user's face.
4 stellt einen Prozess 400 zum Erzeugen eines Modells eines Gesichts eines Anwenders aus einem aufgenommenen Bild 405 gemäß einigen Ausführungsformen dar. Das aufgenommene Bild 405 wird durch einige Ausführungsformen der in 1 gezeigten Kamera 130 oder der in 3 gezeigten Kamera 310 erfasst. In der dargestellten Ausführungsform ist das aufgenommene Bild 405 ein Rahmen in einer Folge von Rahmen, die aufgenommen werden, um zum Konstruieren der Blick-Datenbank verwendet zu werden. Das aufgenommene Bild 405 ist durch Werte in einer Menge 410 von Pixeln repräsentiert. Im Interesse der Deutlichkeit ist der Maßstab der Pixel überhöht und nur eine Teilmenge aus der Menge 410 von Pixeln ist in 4 gezeigt. Die Werte der Pixel in der Menge 410 können als RGB-Werte und ein entsprechender Tiefenwert, der einen Abstand von der Kamera, die das Bild 405 aufgenommen hat, zu einem Abschnitt der Szene, die das Gesicht des Anwenders enthält, das durch das entsprechende Pixel repräsentiert ist, angibt, repräsentiert sein. Das aufgenommene Bild 405 wird in ein strukturiertes Gesichtsmodell durch einen Prozessor wie z. B. den in 1 gezeigten Prozessor 110 umgesetzt. 4 represents a process 400 for generating a model of a user's face from a captured image 405 according to some embodiments. The captured image 405 is characterized by some embodiments of in 1 shown camera 130 or the in 3 shown camera 310 detected. In the illustrated embodiment, the captured image is 405 a frame in a sequence of frames taken to be used to construct the look database. The picture taken 405 is by values in a crowd 410 represented by pixels. For the sake of clarity, the scale of the pixels is excessive and only a subset of the set 410 of pixels is in 4 shown. The values of the pixels in the set 410 can be used as RGB values and a corresponding depth value, which is a distance from the camera that the picture 405 has been added to a portion of the scene containing the face of the user represented by the corresponding pixel. The picture taken 405 is transformed into a structured face model by a processor such as B. the in 1 shown processor 110 implemented.
Der Prozessor lässt anfangs einen Gesichtserkennungsalgorithmus laufen, um einen Abschnitt des aufgenommenen Bilds 405, der das Gesicht des Anwenders repräsentiert, zu identifizieren, der hier als „das detektierte Gesicht 415“ bezeichnet ist. Beispielsweise kann der Gesichtserkennungsalgorithmus Informationen zurückgeben, die eine Grenze des detektierten Gesichts 415 in dem aufgenommenen Bild 405 identifizieren. Der Prozessor verwendet dann das detektierte Gesicht 415, um zusätzliche Informationen zu berechnen, die für das Gesichts des Anwenders repräsentativ sind, wie z. B. einen Begrenzungsrahmen und Orientierungspunkte, die die Augen, die Nase, den Mund und dergleichen enthalten. Falls der Gesichtserkennungsalgorithmus kein Gesicht in dem aufgenommenen Bild 405 detektiert, wird das Bild verworfen.The processor initially runs a face recognition algorithm to capture a portion of the captured image 405 identifying the user's face, referred to herein as "the detected face 415." For example, the face recognition algorithm may return information representing a boundary of the detected face 415 in the captured image 405. The processor then uses the detected face 415 to calculate additional information that is representative of the user's face, such as: A bounding box and landmarks containing the eyes, nose, mouth and the like. If the face recognition algorithm does not have a face in the captured image 405 detected, the image is discarded.
Tiefendaten für die Pixel, die das detektierte Gesicht 415 repräsentieren, werden räumlich und zeitlich gefiltert. Beispielsweise kann räumliches Filtern unter Verwendung eines Gauß-, Laplace- oder Median-Filters ausgeführt werden, das Rauschen oder Ausreißer entfernt. Zeitliches Filtern wird durch Ausrichten des Begrenzungsrahmens des detektierten Gesichts 415 an Begrenzungsrahmen des detektierten Gesichts in anderen nahegelegenen Rahmen in der Videosequenz ausgeführt. Die Ausrichtung der Begrenzungsrahmen kann unter Verwendung Ausrichtung des optischen Flusses oder von Gesichts-Orientierungspunkten gefolgt von zeitlichem Mitteln der ausgerichteten Tiefenrahmen, die den ausgerichteten Rahmen enthalten, der das detektierte Gesicht 415 enthält, ausgeführt werden.Depth data for the pixels that the detected face 415 represent are spatially and temporally filtered. For example, spatial filtering may be performed using a Gaussian, Laplacian, or median filter that removes noise or outliers. Temporal filtering is accomplished by aligning the bounding box of the detected face 415 on bounding frames of the detected face in other nearby frames in the video sequence. The alignment of the bounding boxes may be accomplished using orientation of the optical flow or facial landmarks followed by averaging the aligned depth frames containing the aligned frame that the detected face 415 contains, to be executed.
Gefilterte Daten, die für das detektierte Gesicht 415 repräsentativ sind, werden trianguliert, um ein 3D-Modell 420 des Gesichts des Anwenders zu erzeugen. Das 3D-Modell 420 enthält eine Menge von Eckpunkten 425 (im Interesse der Deutlichkeit ist nur einer durch ein Bezugszeichen angegeben), die durch entsprechende Kanten 430 (im Interesse der Deutlichkeit ist nur eine durch ein Bezugszeichen angegeben) miteinander verbunden sind. Die Eckpunkte 425 sind entsprechenden Pixeln in dem detektierten Gesicht 415 zugeordnet. Triangulation wird durch Verbinden der Eckpunkte 425, die benachbarten Pixeln in dem Tiefenrahmen zugeordnet sind, durch die Kanten 430 ausgeführt, während Pixel, die eine unbekannte Tiefe oder eine Tiefe, die von der Tiefe benachbarter Pixel ausreichend verschieden sind, um eine Tiefenunstetigkeit anzugeben, aufweisen, ignoriert werden. Eine RGB-Textur für das 3D-Modell 420 wird durch Abbilden der Eckpunkte 425 auf ihre Pixelorte in dem RGB-Bild 405 definiert. Beispielsweise kann die RGB-Textur durch einen Tiefenwert 435 und einen RGB-Wert 440 für jeden aus den Eckpunkten definiert sein. In einigen Ausführungsformen kann der Tiefenwert 435 unter Verwendung von Struktur-aus-Bewegung-Techniken (SfM-Techniken) oder maschinenlernenbasierten Tiefenvorhersage-Techniken bestimmt oder verbessert werden. Das texturabgebildete 3D-Gesichtsmodell 420 wird als eine Gesichtsprobe 445 gespeichert. Gesichtsproben für Bilder in unterschiedlichen Rahmen können ausgerichtet und gefiltert werden, um die Qualität der Modelle zu verbessern.Filtered data relevant to the detected face 415 are triangulated to produce a 3D model 420 of the user's face. The 3D model 420 contains a lot of vertices 425 (in the interest of clarity, only one is indicated by a reference numeral), which is defined by corresponding edges 430 (in the interest of clarity, only one is indicated by a reference numeral). The vertices 425 are corresponding pixels in the detected face 415 assigned. Triangulation is done by connecting the vertices 425 that are associated with adjacent pixels in the depth frame, through the edges 430 while ignoring pixels having an unknown depth or depth sufficiently different from the depths of adjacent pixels to indicate a depth discontinuity. An RGB texture for the 3D model 420 is made by mapping the vertices 425 to their pixel locations in the RGB image 405 Are defined. For example, the RGB texture may be a depth value 435 and an RGB value 440 be defined for each of the vertices. In some embodiments, the depth value 435 be determined or improved using structure-out-of-motion (SfM) techniques or machine-learning-based depth prediction techniques. The texture-mapped 3D face model 420 is considered a face sample 445 saved. Face samples for images in different frames can be aligned and filtered to improve the quality of the models.
5 ist ein Blockdiagramm, das eine Blick-Datenbank 500 enthält, die unter Verwendung ausgerichteter und gefilterter Gesichtsproben 501, 502, 503, 504 gemäß einigen Ausführungsformen produziert ist. Die Gesichtsproben 501-504 werden durch einen Prozessor erzeugt, der konfiguriert ist, texturabgebildete 3D-Gesichtsmodelle zu erzeugen, die verwendet werden, um die Gesichtsproben 501-504 zu bilden, wie mit Bezug auf 4 diskutiert ist. In der dargestellten Ausführungsform sind die Gesichtsproben 501-504 zeitliche Nachbarn, z. B. werden sie aus Bildern erzeugt, die durch eine Kamera in aufeinanderfolgenden Zeitintervallen aufgenommen werden, wenn ein Anwender seine Augen bewegt, um einem Muster auf einem Bildschirm zu folgen. Die Gesichtsproben 501-504 sind außerdem eine Teilmenge einer größeren Menge von Gesichtsproben, die für den Anwender erfasst werden. 5 is a block diagram that has a look database 500 using targeted and filtered facial samples 501 . 502 . 503 . 504 produced according to some embodiments. The facial samples 501 - 504 are generated by a processor configured to generate texture-mapped 3D facial models that are used to make the face samples 501 - 504 to form, as related to 4 is discussed. In the illustrated embodiment, the facial samples are 501 - 504 temporal neighbors, z. For example, they are generated from images taken by a camera at successive time intervals when a user moves their eyes to follow a pattern on a screen. The facial samples 501 - 504 are also a subset of a larger quantity of face samples collected for the user.
In einigen Ausführungsformen kann die Teilmenge der Gesichtsproben 501-504 aneinander ausgerichtet sein, oder die Gesichtsproben 501-504 können an einer größeren Menge von Gesichtsproben ausgerichtet sein. Beispielsweise können die Gesichtsproben 501-504 unter Verwendung eines iterativen „Closest Point“-Algorithmus (ICP-Algorithmus) ausgerichtet werden, um 3D-Ausrichtung der Gesichtsproben 501-504 auszuführen. In einigen Fällen wird der ICP-Algorithmus mit einer RGB-Angleichung unter Verwendung von Verfolgung von Merkmalen, Verfolgung von Gesichtsorientierungspunkten und dergleichen kombiniert. Sobald die Gesichtsproben 501-504 ausgerichtet worden sind, können die Gesichtsproben 501-504 gefiltert werden, um entsprechende Punkte in den Gesichtsproben 501-504 zu mitteln, während Punkte, die wegen Fehlern, nicht-starren Verformungen auf dem Gesicht und dergleichen nicht übereinstimmen, ausgeschlossen werden.In some embodiments, the subset of the facial samples 501-504 may be aligned with each other, or the facial samples 501 - 504 can be targeted at a larger amount of facial samples. For example, the facial samples 501 - 504 using an iterative "Closest Point "algorithm (ICP algorithm) aligned to 3D alignment of the facial samples 501 - 504 perform. In some cases, the ICP algorithm is combined with RGB matching using feature tracking, tracking of facial landmarks, and the like. Once the facial samples 501 - 504 face samples 501-504 may be filtered to provide corresponding points in the face samples 501 -504, while excluding points due to imperfections, non-rigid deformations on the face, and the like.
Die ausgerichteten und gefilterten Gesichtsproben 501-504 können kombiniert werden, um ein Referenzmodell 510 zu bilden, das die Geometrie des Modells des Gesichts definiert. In der dargestellten Ausführungsform ist das Gesicht des Anwenders stationär (oder als stationär angenommen), während die Kamera die Bilder aufnimmt, die verwendet werden, um die Gesichtsproben 501-504 zu produzieren. Die Gesichtsproben 501-504 (und irgendwelche weiteren verfügbaren Gesichtsproben) werden deshalb verwendet, um ein einziges Referenzmodell 510 zu bilden. In einigen Fällen ändert sich jedoch die Geometrie des Gesichts des Anwenders, weil der Anwender nicht fähig ist, während des Bildaufnahmeprozesses bewegungslos zu sein oder ausdruckslos zu bleiben. Änderungen der Position des Gesichts des Anwenders oder des Ausdrucks auf dem Gesicht des Anwenders unterbrechen die Steifheitsannahme, die verwendet wird, um das einzelne Referenzmodell 510 aus den Gesichtsproben 501-504 zu produzieren. Das kann zu unangenehmem ruckartigem Verhalten während der Wiedergabe der Bilder basierend auf dem Referenzmodell 510 und den Gesichtsproben 501-504 führen. Einige Ausführungsformen können deshalb mehrere Referenzmodelle erzeugen, die unterschiedlichen Grundpositionen oder Ausdrücken entsprechen. Änderungen der Form des Gesichts können ebenfalls in der Form von Geometrietexturen, z. B. Verlagerungs- oder normale Abbildungen, aufgezeichnet werden. Schattierer, die in Grafikverarbeitungseinheiten (GPUs) implementiert sind, können die Geometrietexturen benutzen, während Bilder basierend auf dem Referenzmodell 510 und den Gesichtsproben 501-504 wiedergegeben werden.The aligned and filtered face samples 501 - 504 can be combined to a reference model 510 which defines the geometry of the model of the face. In the illustrated embodiment, the user's face is stationary (or assumed to be stationary) while the camera captures the images used to make the face samples 501 - 504 to produce. The facial samples 501 - 504 (and any other available facial samples) are therefore used to construct a single reference model 510 to build. In some cases, however, the geometry of the user's face changes because the user is unable to be motionless during the image acquisition process or to remain blank. Changes in the position of the user's face or the expression on the user's face interrupt the stiffness assumption that is used about the single reference model 510 from the face samples 501 - 504 to produce. This can lead to uncomfortable jerky behavior during playback of the images based on the reference model 510 and the face samples 501 - 504 to lead. Some embodiments may therefore generate multiple reference models corresponding to different basic positions or expressions. Changes in the shape of the face may also be in the form of geometry textures, e.g. Displacement or normal images. Shaders implemented in graphics processing units (GPUs) can use the geometry textures while images based on the reference model 510 and the face samples 501 - 504 be reproduced.
Die Blick-Datenbank 500 wird verwendet, um das Referenzmodell 510 und die Gesichtsproben 501-504 für den Anwender zu speichern. Das Referenzmodell 510 wird mit einem vorbestimmten Wert (REF) indexiert. Die Gesichtsproben 501-504 werden nach der Blickrichtung indexiert, die aus dem Bild bestimmt wird, das verwendet wird, um die entsprechende Gesichtsprobe zu produzieren. Beispielsweise wird Probe 1 durch den Blick 1 indexiert, Probe 2 wird durch den Blick 2 indexiert, und Probe 3 wird durch den Blick 3 indexiert. Die Blick-Datenbank 500 enthält außerdem eine Blinzelprobe, die ein texturabgebildetes 3D-Modell des Gesichts des Anwenders mit geschlossenen Augen repräsentiert. Die Blinzelprobe wird durch einen vorbestimmten Wert (BLINK) indexiert. Auf das Referenzmodell 510, die Blinzelprobe oder die Gesichtsproben 501-504 kann aus der Blick-Datenbank 500 unter Verwendung des entsprechenden Index zugegriffen werden. In Ausführungsformen, die mehrere Referenzmodelle und entsprechende Gesichtsproben, die unterschiedlichen Ausdrücken oder Emotionen zugeordnet sind, enthalten, kann die Blick-Datenbank 500 außerdem nach Parametern, die die Ausdrücke oder Emotionen definieren, indexiert sein.The look database 500 is used to the reference model 510 and the facial samples 501 - 504 for the user to save. The reference model 510 is indexed with a predetermined value (REF). The facial samples 501 -504 are indexed according to the line of sight determined from the image used to produce the corresponding face sample. For example, sample becomes 1 through the look 1 indexed, sample 2 is by the look 2 indexed, and sample 3 is by the look 3 indexed. The look database 500 Also includes a Blink sample, which represents a texture-mapped 3D model of the user's face with eyes closed. The Blinzelprobe is indexed by a predetermined value (BLINK). On the reference model 510, the Blinzelprobe or the face samples 501 - 504 can from the look database 500 be accessed using the appropriate index. In embodiments containing multiple reference models and corresponding face samples associated with different expressions or emotions, the gaze database may 500 also be indexed by parameters that define the expressions or emotions.
Wie hier diskutiert werden das Referenzmodell 510 und die Gesichtsproben 501-504 verwendet, um Bilder einiger oder aller Gesichter des Anwenders wiederzugeben. Beispielsweise können das Referenzmodell 510 und die Gesichtsproben 501-504 verwendet werden, um Bilder für Headset-Entfernung und Anwendungen für gemischte Realität wiederzugeben. Da das Referenzmodell 510 nur die Geometrie des Gesichts repräsentiert, wird das Referenzmodell 510 mit dem 3D-Texturmodell, das durch eine aus den Gesichtsproben 501-504 repräsentiert wird, kombiniert, um ein Bild zu produzieren, das das Gesicht des Anwenders repräsentiert. Jede Gesichtsprobe 501-504 entspricht einer unterschiedlichen Blickrichtung. Es kann deshalb scheinen, dass der Anwender in unterschiedliche Richtungen schaut, abhängig davon, welche der Gesichtsproben 501-504 verwendet wird, um die Texturen, die auf das Referenzmodell 510 angewandt werden, zu produzieren.As discussed here, the reference model 510 and the facial samples 501-504 used to render images of some or all faces of the user. For example, the reference model 510 and Facials 501-504 are used to render images for headset removal and mixed reality applications. Because the reference model 510 only representing the geometry of the face becomes the reference model 510 with the 3D texture model, through one of the facial samples 501 - 504 is combined to produce an image representing the face of the user. Every facial sample 501 - 504 corresponds to a different line of sight. It may therefore seem that the user is looking in different directions, depending on which of the facial samples 501 - 504 is used to apply the textures to the reference model 510 be applied to produce.
6 ist ein Ablaufdiagramm eines Verfahrens 600 zum Aufnehmen von Bildern des Gesichts des Anwenders, die verwendet werden, um eine Blick-Datenbank zu erzeugen, gemäß einigen Ausführungsformen. Das Verfahren 600 ist in einigen Ausführungsformen des in den 1 und 2 gezeigten Verarbeitungssystems implementiert. 6 is a flowchart of a method 600 for capturing images of the user's face used to create a gaze database, in accordance with some embodiments. The procedure 600 is in some embodiments of the in the 1 and 2 implemented processing system implemented.
Bei Block 605 stellt ein Prozessor Signale bereit, die ein Zielbild auf einem Bildschirm positionieren. Der Anwender wird angewiesen, bewegungslos und ausdruckslos zu bleiben, während er dem Zielbild mit seinen Augen folgt. Der Anwender wird ebenfalls angewiesen, seine Augen auf das auf dem Bildschirm angezeigte Zielbild zu trainieren und dem Zielbild zu folgen, wenn es sich über den Bildschirm bewegt.At block 605 a processor provides signals that position a target image on a screen. The user is instructed to remain motionless and expressionless as he follows the target image with his eyes. The user is also instructed to train his eyes on the target image displayed on the screen and to follow the target image as it moves across the screen.
Bei Block 610 nimmt eine Kamera ein Bild des Gesichts des Anwenders auf, während der Anwender das Zielbild beobachtet. Das Bild enthält RGB-Werte von Pixeln, die das Bild (das das Gesicht des Anwenders enthält) repräsentieren, und Tiefenwerte für die Pixel. Jeder Tiefenwert gibt einen Abstand von der Kamera zu einem Abschnitt der Szene an, der durch das entsprechende Pixel repräsentiert ist. In Ausführungsformen, die eine Augenverfolgungseinheit enthalten, werden bei Block 615 Informationen, die die Position und Orientierung der Augen des Anwenders angeben, durch die Augenverfolgungseinheit aufgenommen. Beispielsweise können die Position und Orientierung der Augen des Anwenders durch die in 1 gezeigte Augenverfolgungseinheit 135 aufgenommen werden.At block 610 a camera captures an image of the user's face while the user is watching the target image. The image contains RGB values of pixels representing the image (the face of the user contains) and depth values for the pixels. Each depth value indicates a distance from the camera to a portion of the scene represented by the corresponding pixel. In embodiments that include an eye tracking unit, at block 615 Information indicating the position and orientation of the user's eyes is recorded by the eye tracking unit. For example, the position and orientation of the user's eyes may be determined by the in 1 shown eye tracking unit 135 be recorded.
Bei dem Entscheidungsblock 620 bestimmt der Prozessor, ob zusätzliche Zielpositionen vorhanden sind. Beispielsweise können die Zielpositionen durch ein Raster von Zielpositionen bestimmt sein, und der Prozessor kann bestimmen, ob das Zielbild an allen Zielpositionen, die durch das Raster angegeben sind, positioniert worden ist. Falls zusätzliche Zielpositionen vorhanden sind, die nicht verwendet worden sind, geht das Verfahren 600 zu Block 605 weiter. Falls keine zusätzlichen Zielpositionen vorhanden sind, geht das Verfahren zu Block 625 weiter, und der Anwender wird angewiesen, seine Augen zu schließen. Die Kamera nimmt ein zusätzliches Blinzelbild auf, während die Augen des Anwenders geschlossen sind.At the decision block 620 the processor determines if additional target positions exist. For example, the target positions may be determined by a grid of target positions, and the processor may determine whether the target image has been positioned at all target positions indicated by the grid. If there are additional target positions that have not been used, the procedure goes 600 to block 605 further. If there are no additional target positions, the procedure goes to block 625 continue, and the user is instructed to close his eyes. The camera takes an additional blinking picture while the user's eyes are closed.
7 ist ein Ablaufdiagramm 700 eines Verfahrens zum Erzeugen einer Blick-Datenbank unter Verwendung von Gesichtsproben, die aus Bildern des Gesichts eines Anwenders erfasst werden, während er in unterschiedlichen Richtungen schaut, gemäß einigen Ausführungsformen. Das Verfahren 700 ist in einigen Ausführungsformen des in den 1 und 2 gezeigten Prozessors 110 implementiert. 7 is a flowchart 700 a method of generating a gaze database using facial samples captured from images of a user's face while looking in different directions, according to some embodiments. The procedure 700 is in some embodiments of the in the 1 and 2 shown processor 110 implemented.
Bei Block 705 wird eine Menge von Gesichtsproben basierend auf Informationen, die in den 3D-Modellen des Gesichts des Anwenders enthalten sind, die durch die Gesichtsproben repräsentiert sind, ausgerichtet. Die Gesichtsproben können unter Verwendung von ICP-Algorithmen, RGB-Ausrichtung unter Verwendung von Merkmalsverfolgung, Verfolgung von Gesichts-Orientierungspunkten oder Kombinationen daraus ausgerichtet werden.At block 705 A set of face samples is aligned based on information contained in the 3D models of the user's face represented by the face samples. The face samples may be aligned using ICP algorithms, RGB alignment using feature tracking, tracking of facial landmarks, or combinations thereof.
Bei Block 710 werden die ausgerichteten Gesichtsproben zeitlich und räumlich gefiltert. Räumliches Filtern kann durch Anwenden eines Gauß- oder Median-Filters ausgeführt werden, um Rauschen oder Ausreißer aus den ausgerichteten Gesichtsproben zu entfernen. Zeitliches Filtern kann durch Ausrichten von Begrenzungsrahmen der Gesichtsproben unter Verwendung eines optischen Flusses oder Ausrichtung von Gesichts-Orientierungspunkten gefolgt durch zeitliches Mitteln ausgeführt werden.At block 710 The aligned face samples are temporally and spatially filtered. Spatial filtering can be performed by applying a Gaussian or median filter to remove noise or outliers from the aligned face samples. Temporal filtering can be accomplished by aligning bounding frames of the face samples using optical flow or orientation of facial landmarks followed by temporal averaging.
Bei Block 715 werden die gefilterten und ausgerichteten Gesichtsproben kombiniert, um ein Referenzmodell zu erzeugen. Wie hier diskutiert repräsentiert das Referenzmodell die Geometrie eines 3D-Modells des Gesichts in den gefilterten und ausgerichteten Gesichtsproben.At block 715 The filtered and aligned face samples are combined to produce a reference model. As discussed herein, the reference model represents the geometry of a 3D model of the face in the filtered and aligned face samples.
Bei Block 720 werden die Texturen in den gefilterten und ausgerichteten Gesichtsproben erneut abgebildet, um an dem Referenzmodell ausgerichtet zu werden. Beispielsweise werden die texturabgebildeten 3D-Modelle in den gefilterten und ausgerichteten Gesichtsproben aus dem Gesichtspunkt des Referenzmodells erneut wiedergegeben. In einigen Ausführungsformen wird die Texturausrichtung über die Gesichtsproben weiter verbessert durch Ausführen einer 2D-Bilderfassung, um verbleibende Fehlausrichtungen zwischen den Gesichtsproben zu entfernen.At block 720 For example, the textures in the filtered and aligned face samples are remapped to align with the reference model. For example, the texture-mapped 3D models in the filtered and aligned face samples are re-rendered from the point of view of the reference model. In some embodiments, texture alignment across the face samples is further enhanced by performing 2D image capture to remove residual misalignments between the face samples.
Bei Block 725 werden die neu abgebildeten, gefilterten und ausgerichteten Gesichtsproben in der Blick-Datenbank gespeichert und nach der entsprechenden Blickrichtung indexiert. Das Referenzmodell und ein Blinzelmodell (das auch neu abgebildet, gefiltert und ausgerichtet sein kann, wie hier diskutiert) werden ebenfalls in der Blick-Datenbank gespeichert.At block 725 The newly imaged, filtered, and aligned face samples are stored in the Blick database and indexed according to the viewing direction. The reference model and a blink model (which may also be remapped, filtered and aligned as discussed herein) are also stored in the Blick database.
8 ist ein Diagramm, das ein Verarbeitungssystem 800 darstellt, das konfiguriert ist, Headset-Entfernung unter Verwendung von Informationen, die in einer Blick-Datenbank 805 gespeichert sind, gemäß einigen Ausführungsformen auszuführen. Das Verarbeitungssystem 800 enthält eine Kamera 810, die verwendet wird, um Bilder einer Szene, die einen Anwender enthält, der durch den Kopf 815 des Anwenders repräsentiert ist, aufzunehmen. Die Kamera 810 kann als eine RGB-Kamera, die ein Bild erzeugt, das durch RGB-Werte von Pixeln in dem Bild repräsentiert wird, eine RGBD-Kamera, die ein Bild erzeugt, das durch die RGB-Werte der Pixel in dem Bild und Tiefenwerte, die einen Abstand zwischen der Kamera 810 und einem Abschnitt der Szene, die durch die entsprechenden Pixel repräsentiert wird, repräsentiert wird, oder andere Typen von Kameras implementiert sein. Einige Ausführungsformen der Kamera 810 sind Video-Kameras, die eine konfigurierbare Anzahl von Bildern pro Sekunde aufnehmen. Bilder, die durch eine Video-Kamera aufgenommen werden, werden typischerweise als „Rahmen“ bezeichnet, und die Geschwindigkeit der Bildaufnahme wird in Rahmen pro Sekunde (FPS) gemessen. Beispielsweise kann die Kamera 810 Bilder mit 60 FPS, 90 FPS, 120 FPS oder mit höheren oder niedrigeren Geschwindigkeiten aufnehmen. Einige Ausführungsformen der Kamera 810 sind an einer Verfolgungseinheit 812, wie z. B. einer VR-Verfolgungseinheit, die verwendet wird, um eine Position und Orientierung der Kamera 810 zu bestimmen, angebracht. 8th is a diagram showing a processing system 800 that is configured to use headset removal using information stored in a look database 805 stored in accordance with some embodiments. The processing system 800 contains a camera 810 which is used to take pictures of a scene that contains a user by the head 815 is represented by the user. The camera 810 For example, as an RGB camera that produces an image represented by RGB values of pixels in the image, an RGBD camera that generates an image by the RGB values of the pixels in the image and depth values that a distance between the camera 810 and a portion of the scene represented by the corresponding pixels, or other types of cameras. Some embodiments of the camera 810 are video cameras that record a configurable number of frames per second. Images captured by a video camera are typically referred to as "frames," and the speed of image capture is measured in frames per second (FPS). For example, the camera 810 Images with 60 FPS, 90 FPS, 120 FPS or record at higher or lower speeds. Some embodiments of the camera 810 are at a tracking unit 812 , such as A VR tracking unit used to position and orient the camera 810 to determine, appropriate.
Das Verarbeitungssystem 800 enthält außerdem einen Prozessor 810 und einen Speicher 825. Der Prozessor 820 ist konfiguriert, Anweisungen wie z. B. Anweisungen, die in dem Speicher 825 gespeichert sind, auszuführen und die Ergebnisse der Anweisungen in dem Speicher 815 zu speichern. Der Prozessor 820 ist außerdem konfiguriert, Informationen zu empfangen, die für die Bilder repräsentativ sind, die durch die Kamera 810 aufgenommen werden, wie z. B. RGB-Werte, Tiefenwerte und dergleichen für jedes aus den Pixeln in den Bildern. Der Prozessor 820 kann die empfangenen Informationen in dem Speicher 825 speichern. Der Prozessor 820 ist außerdem konfiguriert, Bilder basierend auf den Informationen, die aus der Kamera 810 empfangen werden, oder Informationen, auf die aus dem Speicher 825 zugegriffen wird, wiederzugeben. Die Bilder werden auf einer Anzeigevorrichtung 830 wiedergegeben. Obwohl die Anzeigevorrichtung 830 im Interesse der Deutlichkeit als ein Fernsehbildschirm oder ein Monitor abgebildet ist, sind einige Ausführungsformen der Anzeigevorrichtung in anderen Einrichtungen wie z. B. Mobiltelefonen, Tablet-Computern, Head-mounted Displays (HMDs) und dergleichen implementiert. Eine Kopie der Blick-Datenbank 805 wird in dem Speicher 825 gespeichert, und der Prozessor 820 ist fähig, auf Informationen in der Blick-Datenbank aus dem Speicher 825 zuzugreifen.The processing system 800 also contains a processor 810 and a memory 825 , The processor 820 is configured to use instructions such as B. instructions that are in the memory 825 are stored, execute and the results of the instructions in the memory 815 save. The processor 820 It is also configured to receive information representative of the images captured by the camera 810 be recorded, such. RGB values, depth values, and the like for each of the pixels in the images. The processor 820 may receive the received information in the memory 825 to save. The processor 820 It also configures images based on the information coming from the camera 810 be received, or information on the memory 825 is accessed, play. The pictures are displayed on a display device 830 played. Although the display device 830 For the sake of clarity, it is shown as a television screen or a monitor, some embodiments of the display device in other devices such. Mobile phones, tablet computers, head-mounted displays (HMDs), and the like. A copy of the look database 805 will be in the memory 825 saved, and the processor 820 is able to access information in the look database from the store 825 access.
Die Blick-Datenbank 805 wird unter Verwendung einiger Ausführungsformen des in 1 gezeigten Verarbeitungssystems produziert. Beispielsweise wird die Blick-Datenbank 805 unter Verwendung einiger Ausführungsformen des in 6 gezeigten Verfahrens 600 und des in 7 gezeigten Verfahrens 700 erzeugt. Einige Ausführungsformen der Blick-Datenbank 805 werden vor dem Ausführen der Headset-Entfernung unter Verwendung eines Verarbeitungssystems, das von dem Verarbeitungssystem 800 verschieden ist, erzeugt. Beispielsweise kann ein Anwender 815 einen Aufnahmeprozess ausführen, um die Blick-Datenbank 805 zu erzeugen, bevor er sich an einer Sitzung mit AR-, VR- oder gemischter Realität (MR) beteiligt, unter Verwendung des Verarbeitungssystems 800. In Fällen, in denen die Blick-Datenbank 805 unter Verwendung eines Vorprozesses erzeugt wird, kann die Blick-Datenbank 805 in einem nichttransitorischen computerlesbaren Medium gespeichert werden, das Speicherelemente enthalten kann, wie z. B. RAM, der in einem Cloud-Server implementiert ist, digitale Video-Discs (DVDs), Flash-Speicher und dergleichen. Die gespeicherte Blick-Datenbank 805 kann nachfolgend in einen Speicher in dem Verarbeitungssystem 800 übertragen oder kopiert werden. Beispielsweise kann die Blick-Datenbank 805 von dem Cloud-Server über drahtgebundene oder drahtlose Kommunikationsstrecken heruntergeladen werden, es kann auf eine DVD, die die Blick-Datenbank 805 speichert, unter Verwendung eines Plattenlaufwerks, das in dem Verarbeitungssystem 800 implementiert ist, zugegriffen werden, es kann ein Flash-Laufwerk, das die Blick-Datenbank 805 speichert, in einen USB-Anschluss in dem Verarbeitungssystem 800 eingeführt werden, und dergleichen. Alternativ kann das Verarbeitungssystem 800 konfiguriert sein, die Blick-Datenbank 805 zu erzeugen, z. B. unter Verwendung einiger Ausführungsformen des in 6 gezeigten Verfahrens 600 und des in 7 gezeigten Verfahrens 700. In Fällen, in denen die Blick-Datenbank 805 durch das Verarbeitungssystem 800 erzeugt wird, kann die Blick-Datenbank 805 direkt in dem Speicher 825 gespeichert werden.The look database 805 is made using some embodiments of the in 1 produced processing system produced. For example, the Blick database 805 using some embodiments of the in 6 shown method 600 and of in 7 shown method 700 generated. Some embodiments of the look database 805 be done before running the headset removal using a processing system, by the processing system 800 different, generated. For example, a user 815 perform a recording process to the gaze database 805 before participating in an AR, VR or mixed reality (MR) session using the processing system 800 , In cases where the look database 805 is generated using a pre-process, the look database can 805 stored in a non-transitory computer-readable medium that may contain storage elements, such as storage media. RAM implemented in a cloud server, digital video discs (DVDs), flash memories, and the like. The saved view database 805 can subsequently into a memory in the processing system 800 be transferred or copied. For example, the look database 805 downloaded from the cloud server via wired or wireless communication links, it can access a DVD containing the view database 805 stores, using a disk drive, in the processing system 800 it can be accessed, it can be a flash drive containing the view database 805 stores in a USB port in the processing system 800 be introduced, and the like. Alternatively, the processing system 800 be configured, the look database 805 to produce, for. B. using some embodiments of the in 6 shown method 600 and of in 7 shown method 700 , In cases where the look database 805 through the processing system 800 can be generated, the look database 805 directly in the store 825 get saved.
Der Anwender 815 trägt ein HMD 835, das es dem Anwender erlaubt, an VR-, AR- oder MR-Sitzungen teilzunehmen, die durch entsprechende Anwendungen unterstützt werden, die in dem Prozessor 820 oder in anderen Prozessoren wie z. B. entfernten Cloud-Servern implementiert sein können. Die VR-, AR- oder MR-Sitzung produzierte eine virtuelle 3D-Szene, die den Anwender 815 enthält und die auf der Anzeigevorrichtung 830 angezeigt werden kann. Die Kamera 810 nimmt Bilder des Anwenders 815 auf, während der Anwender 815 an der VR-, AR- oder MR-Sitzung teilnimmt. Die aufgenommenen Bilder (oder wenigstens ein Abschnitt davon) werden in die virtuelle 3D-Szene zusammengeführt und auf der Anzeigevorrichtung 830 gezeigt. Der Anwender 815 in der virtuellen 3D-Szene kann durch andere Anwender betrachtet werden, und in einigen Fällen können die anderen Anwender in der virtuellen 3D-Szene durch den Anwender 815 betrachtet werden. Beispielsweise falls der Anwender 815 an einer gemeinsamen VR-, AR- oder MR-Sitzung teilnimmt, die es anderen Anwendern (in 8 nicht gezeigt) erlaubt, einander und den Anwender 815 zu sehen, können die aufgenommenen Bilder des Anwenders 815 in die virtuelle 3D-Szene zusammengeführt werden und in den HMDs, die durch die anderen Anwender getragen werden, die an den gemeinsamen VR-, AR- oder MR-Sitzungen teilnehmen, angezeigt werden. Abschnitte des Gesichts des Anwenders 815 und insbesondere die Augen des Anwenders 815 sind jedoch durch das HMD 835 verdeckt, so dass die Bilder des Anwenders 815, die in der Anzeigevorrichtung 830 (oder anderen Anzeigevorrichtungen) gezeigt werden, eine verwirrende „Klotz-im-Gesicht“-Erscheinung aufweisen. Infolgedessen verhindern sowohl das HMD 835, das durch den Anwender 815 getragen wird, als auch andere HMDs, die durch andere Anwender getragen werden, dass die Anwender während virtueller Interaktionen Augenkontakt aufnehmen können, was das Gefühl des Eintauchens und die soziale Verbindung zwischen den Anwendern in einer virtuellen 3D-Szene stören kann.The user 815 carries an HMD 835 that allows the user to participate in VR, AR, or MR sessions supported by appropriate applications residing in the processor 820 or in other processors such. B. remote cloud servers can be implemented. The VR, AR or MR session produced a virtual 3D scene representing the user 815 contains and that on the display device 830 can be displayed. The camera 810 takes pictures of the user 815 on, while the user 815 participate in the VR, AR or MR meeting. The captured images (or at least a portion thereof) are merged into the virtual 3D scene and on the display device 830 shown. The user 815 in the virtual 3D scene can be viewed by other users, and in some cases, the other users in the virtual 3D scene by the user 815 to be viewed as. For example, if the user 815 participate in a joint VR, AR, or MR session that will be shared with other users (in 8th not shown) allows each other and the user 815 to see the captured pictures of the user 815 be merged into the virtual 3D scene and displayed in the HMDs carried by the other users participating in the common VR, AR or MR sessions. Sections of the face of the user 815 and especially the eyes of the user 815 are however through the HMD 835 obscured, so that the pictures of the user 815 that in the display device 830 (or other display devices) have a confusing "block-in-the-face" appearance. As a result, both prevent the HMD 835 by the user 815 worn, as well as other HMDs that are worn by other users that users while virtual interactions can make eye contact, which can interfere with the feeling of immersion and the social connection between the users in a virtual 3D scene.
Um das Gefühl des Eintauchens und der sozialen Verbindung zwischen dem Anwender 815 und anderen Anwendern, die ein Bild des Anwenders 815 in der virtuellen 3D-Szene betrachten, wenigstens teilweise zu verbessern, gibt der Prozessor 820 einen Abschnitt eines Modells des Gesichts des Anwenders 815 wieder, das dem Abschnitt des Gesichts entspricht, der durch das HMD 835 verdeckt ist, und überschreibt einen Abschnitt des Bilds, der dem HMD 835 entspricht, mit dem wiedergegebenen Abschnitt des Modells des Gesichts des Anwenders 815. In einigen Ausführungsformen nimmt die Kamera 810 ein Bild des Anwenders 815 auf, während der Anwender 815 das HMD 835 trägt, das einen Abschnitt des Gesichts des Anwenders 815 verdeckt. Der Prozessor 820 bestimmt eine dreidimensionale Stellung (3D-Stellung), die eine Orientierung und einen Ort des Gesichts des Kopfes 815 des Anwenders relativ zu der Kamera angibt. Wie hier verwendet bezieht sich der Begriff „Stellung“ auf Parameter, die die Translation und Rotation einer Person oder eines Objekts in einer Szene charakterisieren. Die Stellung wird relativ zu einem Koordinatensystem bestimmt. Somit wird die 3D-Stellung des Kopfs 815 des Anwenders relativ zu der Kamera 810 in einem Koordinatensystem bestimmt, das der Kamera 810 zugeordnet ist. Beispielsweise enthält die 3D-Stellung des Kopfs 815 des Anwenders relativ zu der Kamera 810 die X-, Y- und Z-Koordinaten, die die Translation des Kopfs des Anwenders 815 definieren, und die Nick-, Roll- und Gier-Werte, die die Drehung des Kopfs des Anwenders 815 relativ zu der Kamera 810 definieren.To the feeling of immersion and social connection between the user 815 and other users taking a picture of the user 815 in the virtual 3D scene, at least partially improve, gives the processor 820 a section of a model of the face of the user 815 again, that corresponds to the section of the face passing through the HMD 835 is obscured and overwrites a portion of the image belonging to the HMD 835 corresponds to the reproduced portion of the model of the user's face 815 , In some embodiments, the camera takes 810 a picture of the user 815 on, while the user 815 the HMD 835 wearing a section of the face of the user 815 covered. The processor 820 determines a three-dimensional position (3D position) indicating an orientation and location of the face of the user's head 815 relative to the camera. As used herein, the term "position" refers to parameters that characterize the translation and rotation of a person or object in a scene. The position is determined relative to a coordinate system. Thus, the 3D position of the head 815 the user relative to the camera 810 in a coordinate system that determines the camera 810 assigned. For example, the 3D position of the user's head 815 relative to the camera 810 the X, Y, and Z coordinates that translate the user's head 815 define, and the pitch, roll and yaw values, the rotation of the user's head 815 relative to the camera 810 define.
Der Prozessor 820 gibt ein 3D-Modell des verdeckten Abschnitts des Gesichts des Anwenders wieder und verwendet das wiedergegebene Bild, um einen Abschnitt des HMD 835 in der virtuellen 3D-Szene basierend auf der 3D-Stellung zu überschreiben oder zu ersetzen. Der Prozessor 820 gibt das 3D-Modell des verdeckten Abschnitts des Gesichts des Anwenders unter Verwendung von Texturproben, auf die er aus der Blick-Datenbank 805 zugreift, wieder. Beispielsweise kann eine Blickrichtung des Anwenders 815 detektiert werden und als ein Index in die Blick-Datenbank 805 verwendet werden. Basierend auf dem Index wird auf Texturproben in der Blick-Datenbank 805 zugegriffen. Beispielsweise kann der Prozessor 820 auf Texturen aus den Gesichtsproben, die dem Index zugeordnet sind, aus einer Blick-Datenbank 805 wie z. B. der in 5 gezeigten Blick-Datenbank 500 zugreifen. In einigen Ausführungsformen werden die Texturproben unter Verwendung einer affinen Transformation, die Farben der Texturproben auf Farben eines nicht verdeckten Abschnitts des Gesichts des Anwenders in dem Bild abbildet, farbkorrigiert. Das Ersetzen von Abschnitten des HMD 835 durch Abschnitte des wiedergegebenen 3D-Modells des Gesichts des Anwenders 815 in der virtuellen 3D-Szene stellt die Illusion bereit, dass das HMD 835 entfernt worden ist oder transparent ist. In einigen Ausführungsformen ist das HMD 835 als ein lichtdurchlässiges Objekt wiedergegeben. Beispielsweise kann das 3D-Modell des verdeckten Abschnitts des Gesichts des Anwenders unter Verwendung einer Lichtdurchlässigkeit, die von einer Mitte zu einem Rand des verdeckten Abschnitts des Gesichts des Anwenders variiert, wiedergegeben werden.The processor 820 reproduces a 3D model of the hidden portion of the user's face and uses the rendered image to display a portion of the HMD 835 in the virtual 3D scene based on the 3D position to overwrite or replace. The processor 820 Gives the 3D model of the hidden portion of the user's face using texture samples referenced from the Blick database 805 accesses, again. For example, a viewing direction of the user 815 be detected and as an index in the look database 805 be used. Based on the index will be on texture samples in the Blick database 805 accessed. For example, the processor 820 may look at textures from the face samples associated with the index from a lookup database 805 such as B. the in 5 shown look database 500 access. In some embodiments, the texture samples are color corrected using an affine transformation that maps colors of the texture samples to colors of an uncovered portion of the user's face in the image. Replacing sections of the HMD 835 through sections of the rendered 3D model of the user's face 815 in the virtual 3D scene, the illusion provides that the HMD 835 has been removed or is transparent. In some embodiments, the HMD is 835 represented as a translucent object. For example, the 3D model of the hidden portion of the user's face may be rendered using a translucency that varies from a center to an edge of the hidden portion of the user's face.
9 stellt ein Anzeigesystem 900, das eine elektronische Einrichtung 905 enthält, die konfiguriert ist, VR-, AR- oder MR-Funktionalität über eine Anzeigevorrichtung bereitzustellen, gemäß einigen Ausführungsformen dar. Die dargestellte Ausführungsform der elektronischen Einrichtung 905 kann ein tragbares Anwendergerät wie z. B. ein HMD, einen Tablet-Computer, ein computerfähiges Mobiltelefon (z. B. ein „Smartphone“), einen Notebook-Computer, einen persönlichen digitalen Assistenten (PDA), ein Spielkonsolensystem und dergleichen enthalten. In anderen Ausführungsformen kann die elektronische Einrichtung eine Befestigungseinrichtung enthalten, wie z. B. eine medizinische Bildaufnahmeausrüstung, ein Sicherheitsbildaufnahmesensorsystem, ein Industrieroboter-Steuersystem, ein Drohnen-Steuersystem und dergleichen. Zur Vereinfachung der Darstellung ist die elektronische Einrichtung 905 hier allgemein in dem beispielhaften Kontext eines HMD-Systems beschrieben; die elektronische Einrichtung ist jedoch nicht auf diese beispielhaften Implementierungen beschränkt. 9 provides a display system 900 that is an electronic device 905 includes configured to provide VR, AR, or MR functionality via a display device according to some embodiments. The illustrated embodiment of the electronic device 905 can a portable user device such. An HMD, a tablet computer, a computer-enabled mobile phone (eg, a "smartphone"), a notebook computer, a personal digital assistant (PDA), a game console system, and the like. In other embodiments, the electronic device may include a fastening device, such as. Medical imaging equipment, a security imaging sensor system, an industrial robot control system, a drone control system, and the like. For ease of illustration, the electronic device 905 described herein generally in the exemplary context of an HMD system; however, the electronic device is not limited to these exemplary implementations.
Die elektronische Einrichtung 905 ist in 9 so gezeigt, dass sie auf dem Kopf 910 eines Anwenders angebracht ist. Die elektronische Einrichtung 905 ist deshalb verwendet, um einige Ausführungsformen des in 8 gezeigten HMD 835 zu implementieren. Wie dargestellt enthält die elektronische Einrichtung 905 ein Gehäuse 915, das eine Anzeigevorrichtung 920 enthält, die ein Bild zur Präsentation für die Anwender erzeugt. Die Anzeigevorrichtung 920 ist verwendet, um einige Ausführungsformen der in 8 gezeigten Anzeigevorrichtung 830 zu implementieren. In der dargestellten Ausführungsform ist die Anzeigevorrichtung 920 aus einer linken Anzeigevorrichtung 921 und einer rechten Anzeigevorrichtung 922 gebildet, die verwendet werden, um stereoskopische Bilder für das entsprechende linke Auge und rechte Auge anzuzeigen. In anderen Ausführungsformen ist die Anzeigevorrichtung 920 jedoch eine einzige monolithische Anzeigevorrichtung 920., die getrennte stereoskopische Bilder zur Anzeige für das linke und das rechte Auge erzeugt. Die elektronische Einrichtung 905 enthält außerdem die Okularlinsen 925 und 930, die in entsprechenden Aperturen oder anderen Öffnungen in einer zum Anwender weisenden Oberfläche 935 des Gehäuses 915 angeordnet sind. Die Anzeigevorrichtung 920 ist entfernt von den Okularlinsen 925 und 930 innerhalb des Gehäuses 915 angeordnet. Die Okularlinse 925 ist an der Anzeigevorrichtung 921 des linken Auges ausgerichtet, und die Okularlinse 930 ist an der Anzeigevorrichtung 922 des rechten Auges ausgerichtet.The electronic device 905 is in 9 so shown to be upside down 910 a user is attached. The electronic device 905 is therefore used to some embodiments of in 8th shown HMD 835 to implement. As shown, the electronic device contains 905 a housing 915 that is a display device 920 contains, which creates an image for presentation to users. The display device 920 is used to some embodiments of in 8th shown display device 830 to implement. In the illustrated embodiment, the display device is 920 from a left display device 921 and a right display device 922 which are used to display stereoscopic images for the corresponding left eye and right eye. In other embodiments, the display device is 920 however, a single monolithic display device 920 ., which generates separate stereoscopic images for displaying the left and right eyes. The electronic device 905 also contains the eyepiece lenses 925 and 930 located in corresponding apertures or other openings in a user-facing surface 935 of the housing 915 are arranged. The display device 920 is removed from the eyepiece lenses 925 and 930 inside the case 915 arranged. The eyepiece lens 925 is on the display device 921 Aligned the left eye, and the eyepiece lens 930 is on the display device 922 aligned to the right eye.
In einer stereoskopischen Anzeigebetriebsart werden Bilder durch die Anzeigevorrichtung 921 für das linke Auge angezeigt und durch das linke Auge des Anwenders über die Okularlinse 925 betrachtet. Die Bilder werden gleichzeitig durch die Anzeigevorrichtung 922 für das rechte Auge angezeigt und durch das rechte Auge des Anwenders über die Okularlinse 925 betrachtet. Die durch das linke Auge und das rechte Auge betrachteten Bilder sind konfiguriert, eine stereoskopische Ansicht für den Anwender zu erzeugen. Einige Ausführungsformen der Anzeigevorrichtungen 920, 921, 922 sind hergestellt, so dass sie eine Einfassung (in 9 nicht gezeigt) enthalten, die einen oder mehrere Außenränder der Anzeigevorrichtungen 920. 921, 922 umschließt. In diesem Fall werden die Linsen 925, 930 oder andere optische Einrichtungen verwendet, um die Bilder, die durch die Anzeigevorrichtungen 920 921, 922 erzeugt werden, zu kombinieren, so dass Einfassungen um die Anzeigevorrichtungen 920. 921,922 durch den Anwender nicht gesehen werden. Stattdessen führen die Linsen 925, 930 die Bilder zusammen, so dass sie über Grenzen zwischen den Anzeigevorrichtungen 920, 921, 922 kontinuierlich erscheinen.In a stereoscopic display mode, images are displayed by the display device 921 displayed for the left eye and through the left eye of the user via the eyepiece lens 925 considered. The images are simultaneously viewed through the display device 922 displayed for the right eye and through the right eye of the user via the eyepiece lens 925 considered. The images viewed through the left eye and the right eye are configured to produce a stereoscopic view for the user. Some embodiments of the display devices 920 . 921 . 922 are made so that they have a mount (in 9 not shown) containing one or more outer edges of the display devices 920 , 921, 922 encloses. In this case, the lenses become 925 . 930 or other optical devices used to capture the images through the display devices 920 921 . 922 be generated, so that surrounds around the display devices 920 , 921.922 can not be seen by the user. Instead, carry the lenses 925 . 930 put the pictures together so they cross boundaries between the display devices 920 . 921 . 922 appear continuously.
Einige der oder alle elektronische Komponenten, die den Betrieb der Anzeigevorrichtung 920 steuern und unterstützen, und andere Komponenten der elektronischen Einrichtung 905 sind innerhalb des Gehäuses 915 implementiert. Einige Ausführungsformen der elektronischen Einrichtung 905 enthalten einen oder mehrere Sensoren 940, 945, die verwendet werden, um eine Position oder Orientierung der elektronischen Einrichtung 905 zu detektieren. Obwohl zwei Sensoren 940, 945 im Interesse der Deutlichkeit gezeigt sind, kann die elektronische Einrichtung 905 mehr oder weniger Sensoren enthalten. Die Sensoren 940, 945 können Beschleunigungsmesser, Magnetometer, gyroskopische Detektoren, Positionssensoren, Infrarotsensoren und dergleichen enthalten, die als mikroelektromechanische Sensoren (MEMS-Sensoren) implementiert sein können. Einige Ausführungsformen der elektronischen Einrichtung 905 enthalten Sensoren 940, 945, die fähig sind, Informationen zu erzeugen, die die Stellung der elektronischen Einrichtung 905 mit sechs Freiheitsgraden (6DoF-Stellung) angeben, was eine dreidimensionale Position der elektronischen Einrichtung 905 und eine dreidimensionale Orientierung der elektronischen Einrichtung 905 enthält. Die 6Dof-Stellung wird in einem Koordinatensystem erzeugt, das durch die elektronische Einrichtung 905 definiert ist. Einige Ausführungsformen der elektronischen Einrichtung 905 enthalten Sensoren 940, 945, die nur fähig sind, Informationen zu erzeugen, die weniger Freiheitsgrade oder überhaupt keine Stellungsinformationen angeben. Beispielsweise können die Sensoren 940, 945 nur fähig sein, eine Stellung der elektronischen Einrichtung 905 mit drei Freiheitsgraden (3DoF-Stellung) bereitzustellen.Some or all of the electronic components that control the operation of the display device 920 control and support, and other components of the electronic device 905 are inside the case 915 implemented. Some embodiments of the electronic device 905 contain one or more sensors 940 . 945 which are used to indicate a position or orientation of the electronic device 905 to detect. Although two sensors 940 . 945 For the sake of clarity, the electronic device 905 contain more or fewer sensors. The sensors 940 . 945 may include accelerometers, magnetometers, gyroscopic detectors, position sensors, infrared sensors, and the like, which may be implemented as microelectromechanical sensors (MEMS sensors). Some embodiments of the electronic device 905 contain sensors 940 . 945 able to generate information regarding the position of the electronic device 905 with six degrees of freedom (6DoF position), which is a three-dimensional position of the electronic device 905 and a three-dimensional orientation of the electronic device 905 contains. The 6Dof position is generated in a coordinate system created by the electronic device 905 is defined. Some embodiments of the electronic device 905 contain sensors 940 , 945, which are only capable of generating information indicating fewer degrees of freedom or no position information at all. For example, the sensors 940 . 945 only be capable of a position of the electronic device 905 with three degrees of freedom (3DoF position).
Einige Ausführungsformen der elektronischen Einrichtung 905 implementieren eine Augenverfolgungseinheit 905, die konfiguriert ist, Bewegungen und Positionen der Augen des Anwenders 910 durch Messen des Blickpunkts des Anwenders 910 oder Messen der Bewegung der Augen relativ zu dem Kopf des Anwenders 910 zu verfolgen. Wie hier diskutiert implementieren einige Ausführungsformen der Augenverfolgungseinheit 950 ein kontaktloses optisches Verfahren zum Messen von Augenbewegung. Andere Typen von Augenverfolgungseinheiten können jedoch ebenfalls verwendet werden, um Bewegungen und Positionen der Augen des Anwenders 910 zu verfolgen. Beispielsweise kann die Augenbewegung unter Verwendung von Augenzubehör wie z. B. speziell konstruierte Kontaktlinsen, Elektroden, die in der Nähe der Augen platziert sind, und dergleichen detektiert werden.Some embodiments of the electronic device 905 implement an eye tracking unit 905 that is configured to movements and positions of the user's eyes 910 by measuring the viewpoint of the user 910 or measuring the movement of the eyes relative to the user's head 910 to pursue. As discussed herein, some embodiments implement the eye tracking unit 950 a contactless optical method for measuring eye movement. However, other types of eye tracking units may also be used to control movements and positions of the user's eyes 910 to pursue. For example, the eye movement using eye accessories such. For example, specially designed contact lenses, electrodes placed near the eyes, and the like are detected.
10 stellt relative Positionen und Orientierungen einer Kamera 1005, eines HMD 1010 und einer Anwenders 1015 in einem Headset-Entfernungssystem 1000 gemäß einigen Ausführungsformen dar. Die Kamera, 1005, das HMD 1010 und der Anwender 1015 entsprechen der Kamera 810, dem HMD 835 und dem Anwender 815, die in 8 gezeigt sind. Die relativen Positionen und Orientierungen, die in 10 gezeigt sind, werden verwendet, um eine 3D-Stellung zu bestimmen, die eine Orientierung und einen Ort des Gesichts des Anwenders 1015 relativ zu der Kamera 1005 angibt. Die 3D-Stellung des Anwenders 1015 ist jedoch nicht notwendigerweise in einem Koordinatensystem 1020 bekannt, das der Kamera 1005 zugeordnet ist. Darüber hinaus ist es wahrscheinlich, dass sich die 3D-Stellung des Anwenders 1015 in dem Koordinatensystem 1020 in Reaktion auf die Bewegung des Anwenders 1015 ändert. Das Koordinatensystem 1020, das der Kamera 1005 zugeordnet ist, ist ebenfalls gegen Änderung empfindlich. Beispielsweise kann das Koordinatensystem 1020 durch eine VR-Verfolgungseinheit 1025 definiert sein, die an der Kamera 1005 angebracht ist und verwendet wird, um die Position und Orientierung der Kamera 1005 zu verfolgen. Änderungen der Position oder Orientierung der Kamera 1005 oder Änderungen der relativen Position und Orientierung der VR-Verfolgungseinheit 1025 und der Kamera 1005 führen zu Änderungen an dem Koordinatensystem 1020. 10 represents relative positions and orientations of a camera 1005 , an HMD 1010 and a user 1015 in a headset removal system 1000 according to some embodiments. The camera, 1005, the HMD 1010 and the user 1015 correspond to the camera 810 , the HMD 835 and the user 815 , in the 8th are shown. The relative positions and orientations in 10 are used to determine a 3D position, the orientation and location of the user's face 1015 relative to the camera 1005 indicates. The 3D position of the user 1015 however, is not necessarily in a coordinate system 1020 known, which is associated with the camera 1005. In addition, it is likely that the 3D position of the user 1015 in the coordinate system 1020 in response to the movement of the user 1015 changes. The coordinate system 1020 , the camera 1005 is also sensitive to change. For example, the coordinate system 1020 through a VR tracking unit 1025 be defined on the camera 1005 is attached and used to adjust the position and orientation of the camera 1005 to pursue. Changes in position or Orientation of the camera 1005 or changes in the relative position and orientation of the VR tracking unit 1025 and the camera 1005 lead to changes to the coordinate system 1020 ,
Eine 3D-Stellung des Anwenders 1015 in dem Koordinatensystem 1020, das der Kamera 1005 zugeordnet ist, kann unter Verwendung eines Abgleichsalgorithmus bestimmt werden, um ein 3D-Modell des Gesichts des Anwenders 1015 mit Pixeln in Bildern, die durch die Kamera 1005 erfasst werden, abzugleichen. Die 3D-Stellung des Anwenders 1015, die durch den Abgleichsalgorithmus bestimmt wird, ist als PFACE,MATCH bezeichnet. Der Abgleichsalgorithmus kann als 2D-Abgleich, falls die Kamera 1005 nur Farbwerte der Pixel bereitstellt, oder 3D-Abgleich, falls die Kamera 1005 auch Tiefeninformationen bereitstellt, implementiert sein. Bei dem 2D-Abgleich wird der Abgleich basierend auf visueller Ähnlichkeit zwischen Abschnitten des Bilds und einem wiedergegebenen 3D-Modell des Gesichts des Anwenders 1015 ausgeführt. Maße der Ähnlichkeit sind als „Bewertungen“ bezeichnet. Beispielsweise kann das 3D-Modell des Gesichts für eine Menge von Orten und Orientierungen relativ zu der Kamera 1005 wiedergegeben werden, um eine Menge von 2D-Modellbildern zu produzieren. Jedes aus der Menge von 2D-Bildern wird mit dem Bild verglichen, das durch die Kamera 1005 aufgenommen ist, und die größte Übereinstimmung (z. B. die höchste Bewertung) bestimmt den geschätzten Ort und die geschätzte Orientierung (z. B. die Stellung PFACE,MATCH) des Anwenders 1015. Alternativ könnte der Abgleichsalgorithmus die Diskrepanz von 2D-Merkmalen zwischen dem durch die Kamera 1005 aufgenommenen Bild und den 2D-Modellbildern minimieren. Bei dem 3D-Abgleich wird das Abgleichen unter Verwendung von ICP-Abgleichen ausgeführt, wie hier diskutiert. Das Modell des Anwenders 1015 kann aus einer Blick-Datenbank erfasst werden. Beispielsweise kann das Modell ein Referenzmodell sein, wie z. B. das Referenzmodell 510, das in der in 5 gezeigten Blick-Datenbank 500 gespeichert ist.A 3D position of the user 1015 in the coordinate system 1020 , the camera 1005 can be determined using a matching algorithm to create a 3D model of the user's face 1015 with pixels in pictures taken by the camera 1005 be recorded, reconciled. The 3D Position of the user 1015 , which is determined by the matching algorithm, is designated as P FACE, MATCH . The matching algorithm can be used as a 2D match if the camera 1005 only provides color values of the pixels, or 3D matching if the camera 1005 also provides depth information. In 2D matching, matching is performed based on visual similarity between portions of the image and a rendered 3D model of the user's face 1015. Measures of similarity are referred to as "ratings". For example, the 3D model of the face can be used for a lot of locations and orientations relative to the camera 1005 to produce a set of 2D model images. Each of the set of 2D images is compared to the image taken by the camera 1005 and the largest match (eg, the highest score) determines the estimated location and orientation (eg, P FACE, MATCH ) of the user 1015 , Alternatively, the matching algorithm could be the discrepancy of 2D features between that by the camera 1005 minimize the captured image and the 2D model images. In 3D matching, matching is performed using ICP matching as discussed herein. The model of the user 1015 can be captured from a look database. For example, the model may be a reference model, such. B. the reference model 510 that in the in 5 shown look database 500 is stored.
Die durch die Kamera 1005 erfassten Bilder enthalten Bilder des Anwenders 1015, in denen des Gesichts des Anwenders 1015 weitgehend durch das HMD 1010 verdeckt ist. Somit ist der Abgleichsalgorithmus, der verwendet wird, um PFACE,MATCH zu bestimmen, erforderlich, um das weitgehend verdeckte Gesicht mit einem nicht verdeckten 3D-Modell des Gesichts abzugleichen. In dem 3D-Fall wird ein Gesichtsdetektor auf das 3D-Gesichtsmodell angewandt, und Pixel nahe dem Augenbereich des Gesichts werden eliminiert, weil diese Pixel wahrscheinlich verdeckt sind und infolgedessen wahrscheinlich Rauschen in dem Abgleichsalgorithmus erzeugen. Pixel in den Bildern, die das HMD 1010 repräsentieren, werden ebenfalls aus dem Live-Tiefen-Strom, der die erfassten Bilder enthält, entfernt. In dem 2D-Fall werden Bewertungen für eine hypothetische Stellung durch Wiedergeben des 3D-Gesichtsmodells aus der Stellung erzeugt. Pixel, die wahrscheinlich verdeckt sind, werden ausgetastet durch Wiedergeben einer Maske, die das Modell des HMD 1010 repräsentiert, und Legen der Maske über das Bild, um die Pixel anzugeben, die aus dem Abgleichsprozess entfernt werden sollten. Das Abgleichen wird dann auf den verbleibenden Pixeln in dem wiedergegebenen Bild des 3D-Gesichtsmodells und den erfassten Bildern ausgeführt.The through the camera 1005 Captured images contain images of the user 1015 in which the face of the user 1015 is largely obscured by the HMD 1010. Thus, the matching algorithm used to determine P FACE, MATCH is required to match the largely obscured face to an unobscured 3D model of the face. In the 3D case, a face detector is applied to the 3D face model and pixels near the eye area of the face are eliminated because these pixels are likely to be obscured and, as a result, are likely to produce noise in the matching algorithm. Pixels in the images that the HMD 1010 are also removed from the live depth stream containing the captured images. In the 2D case, hypothesis position estimates are generated by rendering the 3D face model from the pose. Pixels that are probably obscured are blanked by playing a mask that is the model of the HMD 1010 and overlay the mask over the image to indicate the pixels that should be removed from the matching process. The matching is then performed on the remaining pixels in the rendered image of the 3D face model and the captured images.
In einigen Ausführungsformen bleiben sowohl eine Transformation 1035 zwischen dem Koordinatensystem 1020 und dem Koordinatensystem 1030 als auch ein relativer Ort und eine Orientierung des Anwenders 1015 in Bezug auf das HMD 1010, die durch den Doppelpfeil 1040 angegeben ist, über ein verlängertes Zeitintervall, das zahlreiche Rahmen oder Bilder, die durch die Kamera 1005 aufgenommen sind, enthält, konstant. Infolgedessen kann die Stellung des HMD 1010 als ein Stellvertreter für die Stellung des Anwenders 1015 verwendet werden, was insbesondere für Ausführungsformen des HMD, die 6DoF-Stellungsinformationen in Echtzeit bereitstellen, nützlich ist. In diesem Fall wird die 3D-Stellung des Anwenders 1015 in dem Koordinatensystem 1020 basierend auf der Transformation 1035 und einer Transformationsmatrix, die den relativen Ort und die Orientierung des Anwenders 1015, die durch den Doppelpfeil 1040 angegeben ist, repräsentiert, bestimmt. Beispielsweise kann die Stellung PFACE,CAMERA des Anwenders 1015 in dem Koordinatensystem 1020 in einem Rahmen (i) geschrieben werden als
wobei PHMD(i) die Stellung des HMD 1010 in dem Koordinatensystem 1030 ist, RFACE→HMD(i) eine Transformationsmatrix ist, die den relativen Ort und die Orientierung des Anwenders 1015 in Bezug auf das HMD 1010 repräsentiert, und THMD→CAMERA(i) die Transformation 1035 zwischen dem Koordinatensystem 1020 und dem Koordinatensystem 1030 ist. Wie vorstehend diskutiert sind weder THMD→CAMERA (i) noch RFACE→HMD(i) notwendigerweise a priori bekannt, und jede Größe kann sich ändern, z. B. wenn der Anwender 1015 das HMD 1010 anpasst oder wenn die VR-Verfolgungseinheit 1025 in Bezug auf die Kamera 1005 bewegt wird.In some embodiments, both remain a transformation 1035 between the coordinate system 1020 and the coordinate system 1030 as well as a relative location and an orientation of the user 1015 in terms of the HMD 1010, by the double arrow 1040 is indicated, over a prolonged time interval, the numerous frames or pictures taken by the camera 1005 are included, constant. As a result, the position of the HMD 1010 as a substitute for the position of the user 1015 which is particularly useful for embodiments of the HMD that provide 6DoF position information in real time. In this case, the 3D position of the user 1015 in the coordinate system 1020 based on the transformation 1035 and a transformation matrix representing the relative location and orientation of the user 1015 by the double arrow 1040 is given, represented, determined. For example, the position P FACE, CAMERA of the user 1015 in the coordinate system 1020 to be written in a frame (i) as where P HMD (i) is the position of the HMD 1010 in the coordinate system 1030 is, R FACE → HMD (i) is a transformation matrix representing the relative location and orientation of the user 1015 in terms of the HMD 1010 and T HMD → CAMERA (i) represents the transformation 1035 between the coordinate system 1020 and the coordinate system 1030 is. As discussed above, neither T HMD → CAMERA (i) nor R FACE → HMD (i) are necessarily known a priori, and any size may change, e.g. If the user 1015 the HMD 1010 adapts or if the VR tracking unit 1025 in relation to the camera 1005 is moved.
Die unbekannten Größen RFACE→HMD(i) und THMD→CAMERA(i) werden unter Verwendung von Abgleichsalgorithmen berechnet, die Pixel in den Bildern mit entsprechenden Modellen vergleichen. Beispielsweise kann die Transformation THMD→CAMERA(i) durch Abgleichen eines Modells des HMD 1010 mit Werten von Pixeln in den durch die Kamera 1005 aufgenommenen Bildern bestimmt werden. Als ein weiteres Beispiel kann die Transformationsmatrix RFACE→HMD(i) durch Abgleichen eines Modells nicht verdeckter Abschnitte des Gesichts des Anwenders 1015, wie z. B. eines Mund/Kinn-Bereichs oder eines Stirn-Bereichs, mit Werten von Pixeln in den durch die Kamera 1005 aufgenommenen Bildern bestimmt werden, wie hier diskutiert ist.The unknown quantities R FACE → HMD (i) and T HMD → CAMERA (i) are calculated using trimming algorithms which compare pixels in the images with corresponding models. For example can the transformation T HMD → CAMERA (i) by matching a model of the HMD 1010 with values of pixels in the camera 1005 recorded images are determined. As another example, the transformation matrix R FACE → HMD (i) may be obtained by matching a model of uncovered portions of the user's face 1015 , such as A mouth / chin area or a forehead area, with values of pixels in the camera 1005 captured images as discussed herein.
Die Abgleichsalgorithmen können als 2D-Abgleich, falls die Kamera 1005 nur Farbwerte der Pixel bereitstellt, oder 3D-Abgleich, falls die Kamera 1005 auch Tiefeninformationen bereitstellt, implementiert sein. Bei dem 2D-Abgleich wird Abgleichen basierend auf der visuellen Ähnlichkeit von Abschnitten des Bilds mit wiedergegebenen 3D-Modellen, z. B. einem wiedergegebenen 3D-Modell des HMD 1010 oder einem wiedergegebenen 3D-Modell des Gesichts des Anwenders 1015, ausgeführt. Beispielsweise kann das 3D-Modell für eine Menge von Orten und Orientierungen relativ zu der Kamera 1005 wiedergegeben werden, um eine Menge von 2D-Modellbildern zu produzieren. Jedes aus der Menge von 2D-Bildern wird mit dem Bild verglichen, das durch die Kamera 1005 aufgenommen ist, und die beste Übereinstimmung bestimmt den geschätzten Ort und die Orientierung des HMD 1010 oder des Anwenders 1015. Alternativ könnte der Abgleichsalgorithmus die Diskrepanz von 2D-Merkmalen zwischen dem durch die Kamera 1005 aufgenommenen Bild und den 2D-Modellbildern minimieren. Bei dem 3D-Abgleich wird das Abgleichen unter Verwendung von ICP-Abgleichen ausgeführt, wie hier diskutiert ist.The matching algorithms can be used as a 2D match if the camera 1005 only provides color values of the pixels, or 3D matching if the camera 1005 also provides depth information. In 2D matching, matching is done based on the visual similarity of portions of the image with rendered 3D models, e.g. A reproduced 3D model of the HMD 1010 or a rendered 3D model of the user's face 1015 , executed. For example, the 3D model can account for a lot of locations and orientations relative to the camera 1005 to produce a set of 2D model images. Each of the set of 2D images is compared to the image taken by the camera 1005 and the best match determines the estimated location and orientation of the HMD 1010 or the user 1015 , Alternatively, the matching algorithm could be the discrepancy of 2D features between that by the camera 1005 minimize the captured image and the 2D model images. In 3D matching, matching is performed using ICP matching as discussed herein.
Die unbekannten Größen RFACE→HMD(i) und THMD→CAMERA(i) werden aus den Ergebnissen der Abgleichsalgorithmen bestimmt. Die Stellung des HMD 1010, die durch den Abgleichsalgorithmus bestimmt wird, ist ausgedrückt aus:
The unknown quantities R FACE → HMD (i) and T HMD → CAMERA (i) are determined from the results of the matching algorithms. The position of the HMD 1010 that is determined by the matching algorithm is expressed as:
Die Transformation THMD→CAMERA(i) kann bestimmt werden als:
The transformation T HMD → CAMERA (i) can be determined as:
Die Transformationsmatrix RFACE→HMD(i) kann bestimmt werden als:
wobei PFACE,MATCH die Stellung des Gesichts ist, die durch Abgleichen des 3D-Modells des Gesichts des Anwenders 1015 mit dem erfassten Bild bestimmt wird, wie vorstehend diskutiert ist. Die Berechnung der Transformationsmatrix RFACE→HMD(i) kann verbessert werden, falls das HMD 1010 eine Augenverfolgungseinheit wie z. B. die in 9 gezeigte Augenverfolgungseinheit 950 enthält. Beispielsweise können die durch die Augenverfolgungseinheit bestimmten Augenpositionen als robuste 2D-Merkmale verwendet werden, um die Berechnung der Transformationsmatrix RFACE→HMD(i) zu verbessern.The transformation matrix R FACE → HMD (i) can be determined as: where P FACE, MATCH is the position of the face, by matching the 3D model of the face of the user 1015 is determined with the captured image as discussed above. The calculation of the transformation matrix R FACE → HMD (i) can be improved if the HMD 1010 an eye tracking unit such as B. the in 9 shown eye tracking unit 950 contains. For example, the eye positions determined by the eye tracking unit may be used as robust 2D features to improve the calculation of the transformation matrix R FACE → HMD (i).
Die Größen RFACE→HMD(i) und THMD→CAMERA(i) können für ein einzelnes (oder initiales) Bild (i) bestimmt werden und dann wiederverwendet werden, solange sie sich nicht geändert haben. Somit kann in Ausführungsformen, in denen das HMD 1010 Informationen bereitstellt, die die 6DoF-Stellung des HMD 1010 in einem Koordinatensystem 1030 angeben, das dem HMD 1010 zugeordnet ist, die Berechnung der Größen RFACE→HMD(i) und THMD→CAMERA(i) in Nicht-Echtzeit ausgeführt werden, z. B. unter Verwendung eines oder mehrerer Hintergrund-Threads. Einige Ausführungsformen verbessern die Robustheit oder die Reduktion des Rauschens durch Kombinieren von Ergebnissen für mehrere unterschiedliche Bilder, um Ausreißer auszusondern. Beispielsweise können die Größen RFACE→HMD(i) und THMD→CAMERA(i) über einen gleitenden Mittelwert einer vorbestimmten Anzahl (N) von Rahmen berechnet werden, in denen ein ICP-Algorithmus oder ein Algorithmus für visuellen Abgleich die höchsten Bewertungen über ein Zeitfenster produziert haben.The quantities R FACE → HMD (i) and T HMD → CAMERA (i) can be determined for a single (or initial) image (i) and then reused, as long as they have not changed. Thus, in embodiments where the HMD 1010 Information that provides the 6DoF position of the HMD 1010 in a coordinate system 1030 specify that to the HMD 1010 the calculation of the quantities R FACE → HMD (i) and T HMD → CAMERA (i) are performed in non-real time, e.g. Using one or more background threads. Some embodiments improve the robustness or reduction of noise by combining results for several different images to weed out outliers. For example, the quantities R FACE → HMD (i) and T HMD → CAMERA (i) may be calculated over a moving average of a predetermined number (N) of frames in which an ICP algorithm or algorithm for visual matching has the highest ratings over have produced a time window.
In Ausführungsformen, in denen das HMD 1010 keine Informationen bereitstellt, die die 6DoF-Stellung des HMD 1010 angeben, ist die HMD-Stellung im Weltraum, PHMD(i), nicht bekannt und kann nicht verwendet werden, um die 3D-Stellung des Anwenders 1015 zu bestimmen. Infolgedessen werden die vorstehend offenbarten Abgleichsoperationen für das HMD 1010 in Echtzeit ausgeführt, um PHMD,MATCH(i) in jedem Bild zu bestimmen. Der Echtzeit-Abgleichsalgorithmus versucht, dazwischenliegende schlechte Übereinstimmungen zu vermeiden. In einigen Ausführungsformen wird der Echtzeit-Abgleich unter Verwendung eines früheren Abgleichsergebnisses (z. B. eines früheren Ergebnisses von ICP-Abgleichen, das auf einem früheren Bild ausgeführt wird) als eine Start-Stellung für das aktuelle Bild ausgeführt, sofern die Abgleichsbewertung für das frühere Bild nicht zu niedrig ist, wobei in diesem Fall der Abgleichsalgorithmus zurückgesetzt und ab einem anderen früheren Bild, das eine höhere Abgleichs-Abwertung hatte, neu gestartet werden kann. Anstatt alle verfügbaren Punkte aus dem 3D-Modell und dem ankommenden Datenstrom von Bildern zu verwenden, werden eine Teilmenge der Punkte in dem 3D-Modell und des ankommenden Datenstroms durch den Abgleichsalgorithmus verwendet, um eine hohe Abgleichsgeschwindigkeit aufrechtzuerhalten. Zuverlässige frühere Überstimmungen (typischerweise von Vorderansichten) können verwendet werden, um ein kombiniertes Modell des HMD 1010 und des Gesichts des Anwenders 1015 zusammenzustellen. Das kombinierte Modell kann zuverlässig verfolgt werden, selbst bei schiefen Winkeln.In embodiments in which the HMD 1010 does not provide any information that indicates the 6DoF position of the HMD 1010 indicate, the HMD position in space, P HMD (i), is unknown and can not be used to determine the 3D position of the user 1015 to determine. As a result, the above-disclosed matching operations for the HMD 1010 executed in real time to determine P HMD, MATCH (i) in each image. The real-time matching algorithm tries to intervene bad ones To avoid matches. In some embodiments, real-time matching is performed using a previous match result (eg, an earlier result of ICP matching performed on an earlier picture) as a start position for the current picture, as long as the match score for the current match former image is not too low, in which case the alignment algorithm may be reset and restarted from another previous image that had a higher alignment penalty. Instead of using all available points from the 3D model and the incoming data stream of images, a subset of the points in the 3D model and the incoming data stream are used by the matching algorithm to maintain a high matching speed. Reliable earlier overshoots (typically from front views) can be used to model a combined HMD 1010 and the face of the user 1015 together. The combined model can be tracked reliably, even at oblique angles.
Sobald die 3D-Stellung des Anwenders 1015 in dem Koordinatensystem 1020 bestimmt worden ist, werden Abschnitte des 3D-Modells des Anwenders 1015 die den Abschnitten des Gesichts des Anwenders entsprechen, die durch das HMD 1010 verdeckt sind, wiedergegeben und verwendet, um die entsprechenden Pixel in den durch die Kamera 1005 erfassten Bildern zu ersetzen. In einigen Ausführungsformen versucht das System, so viele ursprüngliche Daten wie möglich zu verwenden, und synthetisiert nur verdeckte Bereiche basierend auf dem 3D-Modell des Anwenders 1015. Beispielsweise können das Kinn und der Stirnbereich des Anwenders 1015 unter Verwendung der entsprechenden Pixel in dem durch die Kamera 1005 erfassten Bild angezeigt werden, während der Augen-Nasen-Bereich basierend auf dem 3D-Modell des Anwenders 1015 synthetisiert wird.Once the 3D position of the user 1015 in the coordinate system 1020 become sections of the user's 3D model 1015 which correspond to the sections of the user's face that are obscured by the HMD 1010, reproduced and used to pass the corresponding pixels into the camera 1005 replace captured images. In some embodiments, the system attempts to use as much original data as possible and synthesizes only hidden areas based on the user's 3D model 1015 , For example, the chin and forehead area of the user 1015 using the corresponding pixels in the image captured by the camera 1005, while the eye-nose region is displayed based on the user's 3D model 1015 is synthesized.
In einigen Ausführungsformen wird den Abschnitten des Gesichts des Anwenders, die aus dem 3D-Modell wiedergegeben werden, eine Lichtdurchlässigkeit zugewiesen, bevor sie mit den ursprünglichen Bildern überlagert werden. Beispielsweise kann die Lichtdurchlässigkeit durch einen Wert 0 ≤ α ≤ 1 angegeben werden, wobei kleinere Werte von a einen höheren Grad der Transparenz des wiedergegebenen Abschnitts des Gesichts des Anwenders angeben. Höhere Grade der Transparenz führen dazu, dass mehr des ursprünglichen Bilds in dem endgültigen kombinierten Bild sichtbar ist. Scharfe Wiedergabekanten können vermieden werden durch Verringern des Werts von α von α~1 in der Mitte des HMD 101 bis α~0 am Rand des HMD 1010. Diese Herangehensweise kann außerdem kleine Abgleichsfehler kaschieren, weil Abschnitte des HMD 1010, die in den endgültigen kombinierten Bildern sichtbar bleiben, Artefakte wie z. B. Unterbrechungen an Nahtstellen in dem wiedergegebenen Abschnitt des Gesichts des Anwenders verbergen können.In some embodiments, portions of the user's face rendered from the 3D model are assigned light transmission before being superimposed with the original images. For example, the light transmission through a value 0 ≦ α ≦ 1, where smaller values of a indicate a higher degree of transparency of the reproduced portion of the user's face. Higher levels of transparency result in more of the original image being visible in the final combined image. Sharp rendering edges can be avoided by reducing the value of α from α ~ 1 in the middle of the HMD 101 to α ~ 0 at the edge of the HMD 1010 , This approach can also hide small alignment errors because of sections of the HMD 1010 that remain visible in the final combined images, artifacts such as. B. hide breaks at seams in the rendered portion of the user's face.
Der Anwender 1015, der das HMD 1010 trägt, kann in 3D präsentiert sein, z. B. in einem 3D-Modell der Szene, die auf einer VR-Einrichtung präsentiert wird. Die Headset-Entfernung kann dann entweder in einer Herangehensweise mit einem Durchlauf oder einer Herangehensweise mit zwei Durchläufen ausgeführt werden. In der Herangehensweise mit einem Durchlauf werden die Werte von a Dreiecken, die das HMD 1010 repräsentieren, basierend auf der Nähe zugewiesen. Beispielsweise kann der Ort des HMD 1010 wie vorstehend diskutiert bestimmt werden, und die Werte von a könne Dreiecken basierend auf ihrem Ort zugewiesen werden, so dass Dreiecke in der Mitte des HMD 1010 hoch transparent sind und Dreiecke nahe dem Rand des HMD 1010 nahezu lichtundurchlässig sind. Die Dreiecke, die aus dem 3D-Modell des Gesichts des Anwenders 1015 abgeleitet sind, werden dann fest zu dem Szenenraster (mit α = 1) hinzugefügt, so dass die Dreiecke, die für das Gesicht repräsentativ sind, hinter den lichtdurchlässigen Dreiecken, die für das HMD 1010 repräsentativ sind, erscheinen. In der Herangehensweise mit zwei Durchgängen wird zuerst die 3D-Repräsentation des Anwenders 1015, der das HMD 1010 trägt, wiedergegeben. Ein lichtdurchlässiges Modell des Gesichts des Anwenders 1015 (das den verdeckten Abschnitt des Gesichts des Anwenders enthält) wird nachfolgend oben auf der 3D-Repräsentation des Anwenders 1015 wiedergegeben.The user 1015 who is the HMD 1010 can be presented in 3D, eg. In a 3D model of the scene presented on a VR device. The headset removal can then be performed in either a one-pass or a two-pass approach. In the one-pass approach, the values of a triangles are the HMD 1010 represent assigned based on proximity. For example, the location of the HMD 1010 as discussed above, and the values of a can be assigned to triangles based on their location, leaving triangles in the middle of the HMD 1010 are highly transparent and triangles near the edge of the HMD 1010 are almost opaque. The triangles that come from the 3D model of the user's face 1015 are then firmly added to the scene grid (with α = 1) so that the triangles representative of the face behind the translucent triangles corresponding to the HMD 1010 are representative. In the two-pass approach, first the user's 3D representation 1015 who is the HMD 1010 bears, reproduced. A translucent model of the user's face 1015 (which contains the hidden portion of the user's face) will be at the top of the user's 3D representation below 1015 played.
11 stellt das Abgleichen eines 3D-Modells 1100 eines Gesichts mit einem aufgenommenen Bild 1105 eines Gesichts, das teilweise durch ein HMD 1110 verdeckt ist, gemäß einigen Ausführungsformen dar. Der in 11 dargestellte Abgleichsalgorithmus ist in einigen Ausführungsformen des in 8 gezeigten Prozessors 820 implementiert. 11 makes matching a 3D model 1100 of a face with a captured image 1105 of a face partially through an HMD 1110 is concealed, according to some embodiments 11 Alignment algorithm shown in FIG 8th shown processor 820 implemented.
Das 3D-Modell 1100 wird aus einer Blick-Datenbank erfasst. Beispielsweise kann das 3D-Modell 1100 das Referenzmodell 510 sein, das in der in 5 gezeigten Blick-Datenbank 500 gespeichert ist. Das 3D-Modell 1100 ist durch Position und Orientierung charakterisiert, die gemeinsam durch den Pfeil 1115 angegeben sind. Der Abgleichsalgorithmus wählt Abschnitte des 3D-Modells 1100 aus, die verwendet werden, um den Vergleich mit dem aufgenommenen Bild 1105 auszuführen. Beispielsweise kann der Abgleichsalgorithmus einen Abschnitt 1120, der einem Stirnbereich des Anwenders entspricht, und einen Abschnitt 1125, der einem Nase/Mund/Kinn-Bereich des Anwenders entspricht, auswählen. Es ist unwahrscheinlich, dass die Abschnitte 1120, 1125 durch das HMD 1110 verdeckt sind, und deshalb sind sie gute Kandidaten zum Abgleichen mit dem Bild 1105.The 3D Model 1100 is captured from a Blick database. For example, the 3D model 1100 may be the reference model 510 be that in the in 5 shown look database 500 is stored. The 3D model 1100 is characterized by position and orientation, which together are indicated by the arrow 1115 are indicated. The matching algorithm selects portions of the 3D model 1100 that are used to compare with the captured image 1105 perform. For example, the matching algorithm may have a section 1120 which corresponds to a frontal area of the user, and a section 1125 which corresponds to a nose / mouth / chin area of the user. It is unlikely that the sections 1120 . 1125 through the HMD 1110 they are good candidates for matching with the image 1105 ,
Das aufgenommene Bild 1105 des Gesichts ist durch eine Position und Orientierung charakterisiert, die gemeinsam durch den Pfeil 1130 angegeben sind. Der Abgleichsalgorithmus bestimmt eine Drehung 1135, die die relative Position oder Orientierung des 3D-Modells 110 und des aufgenommenen Bilds 1105 charakterisiert, durch Vergleichen der Abschnitte 1120, 1125 mit Pixeln in dem aufgenommenen Bild 1105, wie hier diskutiert ist. Beispielsweise verwendet der Abgleichsalgorithmus den Vergleich des 3D-Modells 1100 und des aufgenommenen Bilds 1105, um eine Drehmatrix wie z. B. die vorstehend diskutierte Transformationsmatrix RHMD→FACE(i) zu erzeugen.The picture taken 1105 of the face is characterized by a position and orientation shared by the arrow 1130 are indicated. The matching algorithm determines a rotation 1135 representing the relative position or orientation of the 3D model 110 and the captured image 1105 characterized by comparing the sections 1120 . 1125 with pixels in the captured image 1105 as discussed here. For example, the matching algorithm uses the comparison of the 3D model 1100 and the captured image 1105 to form a rotary matrix such. For example, to generate the transformation matrix R HMD → FACE (i) discussed above.
12 stellt das Abgleichen eines 3D-Modells 1200 eines HMD mit einem aufgenommenen Bild 1205 eines HMD 1210 gemäß einigen Ausführungsformen dar. Der in 12 dargestellte Abgleichsalgorithmus ist in einigen Ausführungsformen des in 8 gezeigten Prozessors 820 implementiert. 12 provides matching of a 3D model 1200 of an HMD with a captured image 1205 an HMD 1210 according to some embodiments 12 Alignment algorithm shown in FIG 8th shown processor 820 implemented.
Das 3D-Modell 1200 ist durch Position und Orientierung charakterisiert, die gemeinsam durch den Pfeil 1212 angegeben sind. Der Abgleichsalgorithmus verwendet Merkmale in dem 3D-Modell 1299, um den Vergleich mit dem aufgenommenen Bild 1205 auszuführen. Beispielsweise enthält das in 12 gezeigte 3D-Modell 1200 einen Aruco-Markierer 1215. Wie hier verwendet wird der Begriff „Aruco-Markierer“ verwendet, um eine Anordnung von Quadraten zu bezeichnen, die eine vorbestimmte Anzahl von Zeilen oder Spalten aufweist. Werte (z. B. schwarz oder weiß) der Quadrate werden basierend auf Hamming-Codes bestimmt, und die Werte werden so gewählt, dass der Aruco-Code eine identifizierbare Orientierung aufweist. Somit kann eine detektierte Orientierung des Aruco-Markierers 1215 verwendet werden, um die Orientierung 1212 des 3D-Modells 1200 zu bestimmen. In einigen Ausführungsformen werden andere Merkmale des HMD wie z. B. Ränder, Ecken, Logos und dergleichen (in Kombination mit dem oder anstelle des) Aruco-Markierer(s) 1215 verwendet.The 3D model 1200 is characterized by position and orientation, shared by the arrow 1212 are indicated. The matching algorithm uses features in the 3D Model 1299 to compare with the captured image 1205 perform. For example, this contains in 12 3D model 1200 shown an Aruco marker 1215 , As used herein, the term "arco marker" is used to refer to an array of squares having a predetermined number of rows or columns. Values (eg, black or white) of the squares are determined based on Hamming codes, and the values are chosen so that the Aruco code has an identifiable orientation. Thus, a detected orientation of the Aruco marker 1215 used for orientation 1212 of the 3D model 1200. In some embodiments, other features of the HMD, such as: Edges, corners, logos and the like (in combination with or in place of the Aruco marker (s) 1215).
Das aufgenommene Bild 1205 des HMD ist durch eine Position und Orientierung, die gemeinsam durch den Pfeil 1220 angegeben sind, charakterisiert. Das HMD weist einen Aruco-Markierer 1225 auf, der auf einem Abschnitt des HMD angezeigt wird, der in dem aufgenommenen Bild 1205 sichtbar ist. Der Aruco-Markierer 1225 weist das gleiche Muster auf wie der Aruco-Markierer 1215 in dem 3D-Modell 1200. Der Abgleichsalgorithmus bestimmt eine Drehung 1230, die die relative Position oder Orientierung des 3D-Modells 1200 und des aufgenommenen Bilds 1205 charakterisiert. Die Drehung 1230 wird durch Vergleichen der Orientierung des Aruco-Markierers 1215 in dem 3D-Modell 1200 mit Pixeln in dem aufgenommenen Bild 1205 des HMD bestimmt, wie hier diskutiert ist. Der Abgleichsalgorithmus verwendet den Vergleich des 3D-Modells 1200 und des aufgenommenen Bilds 1205, um eine Transformation zwischen dem Kamera-Koordinatensystem und dem HMD-Koordinatensystem zu bestimmen. Beispielsweise kann der Vergleich verwendet werden, um die vorstehend diskutierte Transformation THMD→CAMERA(i) zu bestimmen.The picture taken 1205 The HMD is defined by a position and orientation that is shared by the arrow 1220 are characterized. The HMD has an Aruco marker 1225 which is displayed on a section of the HMD included in the captured image 1205 is visible. The Aruco marker 1225 has the same pattern as the Aruco marker 1215 in the 3D model 1200. The matching algorithm determines a rotation 1230 characterizing the relative position or orientation of the 3D model 1200 and the captured image 1205. The rotation 1230 is determined by comparing the orientation of the Aruco marker 1215 in the 3D model 1200 with pixels in the captured image 1205 of the HMD as discussed herein. The matching algorithm uses the comparison of the 3D model 1200 and the captured image 1205 to determine a transformation between the camera coordinate system and the HMD coordinate system. For example, the comparison may be used to determine the transformation T HMD → CAMERA (i) discussed above.
13 stellt Headset-Entfernung, die auf einem Bild 1300 eines Anwenders 1305, der ein HMD 1310 trägt, das einen Teil des Gesichts des Anwenders verdeckt, ausgeführt wird, gemäß einigen Ausführungsformen dar. Das Bild 1300 ist durch einige Ausführungsformen der in 8 gezeigten Kamera 810 aufgenommen, und die Headset-Entfernung wird durch einige Ausführungsformen des in 8 gezeigten Prozessors 820 ausgeführt. 13 Represents Headset removal on an image 1300 a user 1305 who is an HMD 1310 carrying a portion of the user's face obscured, according to some embodiments. The image 1300 is through some embodiments of in 8th shown camera 810 recorded, and the headset removal is supported by some embodiments of the 8th shown processor 820 executed.
Der Prozessor gibt ein Bild 1315 eines Abschnitts des Gesichts des Anwenders, der durch das HMD 1310 verdeckt ist, wieder. Beispielsweise kann eine Blickrichtung für den Anwender 1305 während eines Zeitintervalls, das dem Bild 1300 entspricht, unter Verwendung einer in dem HMD 1310 implementierten Augenverfolgungseinheit bestimmt werden. Die Blickrichtung wird als ein Index in eine Blick-Datenbank verwendet, wie z. B. die in 5 gezeigte Blick-Datenbank 500. Der Prozessor ist deshalb fähig, auf eine Gesichtsprobe zuzugreifen, die der Blickrichtung für den Anwender 1305 in dem Bild 1300 entspricht. Es kann auf ein Blinzelmodell aus der Blick-Datenbank zugegriffen werden, falls die Augen des Anwenders geschlossen sind. Der Prozessor ist außerdem fähig, auf ein Referenzmodell des Gesichts des Anwenders aus der Blick-Datenbank zuzugreifen. Texturen aus der Gesichtsprobe (oder Blinzelprobe) werden auf die Geometrie des Referenzmodells abgebildet, um das Bild 1315 des verdeckten Abschnitts des Gesichts des Anwenders zu erzeugen. Das Wiedergeben des Bilds 1315 aus der Gesichtsprobe (oder der Blinzelprobe) und dem Referenzmodell wird basierend auf der Position und Orientierung des Anwenders 1305 in dem Koordinatensystem der Kamera, die das Bild 1300 erfasst, ausgeführt. Die Position und Orientierung des Anwenders 1305 wird gemäß Ausführungsformen der hier offenbarten Techniken bestimmt.The processor gives a picture 1315 of a section of the face of the user, through the HMD 1310 is covered, again. For example, a viewing direction for the user 1305 during a time interval that is the picture 1300 corresponds, using one in the HMD 1310 implemented eye tracking unit to be determined. The viewing direction is used as an index in a look database, such as B. the in 5 gaze database shown 500 , The processor is therefore able to access a face sample, the viewing direction for the user 1305 in the picture 1300 equivalent. A blinking model can be accessed from the Blick database if the user's eyes are closed. The processor is also able to access a reference model of the user's face from the Blick database. Textures from the face sample (or Blinzelprobe) are mapped to the geometry of the reference model to the image 1315 of the hidden portion of the user's face. Playing the picture 1315 From the facial sample (or Blinzelprobe) and the reference model is based on the position and orientation of the user 1305 in the coordinate system of the camera that captures the image 1300. The position and orientation of the user 1305 is determined in accordance with embodiments of the techniques disclosed herein.
Das aufgenommene Bild 1300 und das wiedergegebene Bild 1315 werden kombiniert, um ein Bild zur Präsentation auf einer Anzeigevorrichtung 1320 zu produzieren. In einigen Ausführungsformen ist dem wiedergegebenen Bild 1315 eine Lichtdurchlässigkeit zugeordnet, die abhängig von der Position eines Abschnitts des wiedergegebenen Bilds 1315 relativ zu dem HMD 1310 variieren kann. Beispielsweise kann die Lichtdurchlässigkeit des wiedergegebenen Bilds 1315 nahe der Mitte des HMD 1310 niedrig sein, so dass das kombinierte Bild vorwiegend durch Pixelwerte in dem wiedergegebenen Bild bestimmt wird, und die Lichtdurchlässigkeit des wiedergegebenen Bilds kann nahe den Rändern des HMD 1310 hoch sein, so dass das kombinierte Bild vorwiegend durch Pixelwerte in dem aufgenommenen Bild 1300 des HMD 1310 nahe den Rändern des HMD 1310 bestimmt wird. The picture taken 1300 and the rendered image 1315 are combined to form an image for presentation on a display device 1320 to produce. In some embodiments, the rendered image is 1315 assigned a light transmission, which depends on the position of a portion of the reproduced image 1315 relative to the HMD 1310 can vary. For example, the light transmittance of the reproduced image 1315 near the middle of the HMD 1310 may be low, so that the combined image is predominantly determined by pixel values in the reproduced image, and the light transmittance of the reproduced image may be near the edges of the HMD 1310 be high so that the combined image is predominantly pixel values in the captured image 1300 of the HMD 1310 near the edges of the HMD 1310 is determined.
14 ist ein Ablaufdiagramm eines Verfahrens 1400 zum Ausführen von Headset-Entfernung gemäß einigen Ausführungsformen. Das Verfahren 1400 ist in einigen Ausführungsformen des in 1 gezeigten Prozessors 820 implementiert. In der dargestellten Ausführungsform wird Headset-Entfernung für ein HMD, das Abschnitte eines Gesichts des Anwenders verdeckt, in Bildern einer Szene, die durch eine Kamera erfasst wird, ausgeführt. Das HMD ist konfiguriert, 6DoF-Stellungsinformationen in Echtzeit bereitzustellen. Die 6DoF-Stellung des HMD ist deshalb als ein Stellvertreter für die Stellung des Anwenders in dem Koordinatensystem der Kamera verwendet, die Bilder erfasst, die den Anwender enthalten, z. B. wie hier mit Bezug auf 10 diskutiert ist. Das Verfahren 1400 kann außerdem modifiziert sein, um Headset-Entfernung für ein HMD auszuführen, das nicht konfiguriert ist, 6DoF-Stellungsinformationen bereitzustellen, z. B. wie hier mit Bezug auf 10 diskutiert ist. 14 is a flowchart of a method 1400 for performing headset removal, in accordance with some embodiments. The procedure 1400 is in some embodiments of the in 1 shown processor 820 implemented. In the illustrated embodiment, headset removal for an HMD obscuring portions of a user's face is performed in images of a scene captured by a camera. The HMD is configured to provide 6DoF position information in real time. The 6DoF position of the HMD is therefore used as a proxy for the user's position in the camera's coordinate system, which captures images containing the user, e.g. B. as here with reference to 10 is discussed. The procedure 1400 can also be modified to perform headset removal for an HMD that is not configured to provide 6DoF pose information, e.g. B. as here with reference to 10 is discussed.
Bei Block 1405 bestimmt der Prozessor eine Stellung des HMD. Beispielsweise kann die Stellung des HMD basierend auf den 6DoF-Stellungsinformationen, die durch das HMD erzeugt und in Echtzeit zu dem Prozessor übertragen werden, bestimmt werden. Die Stellung des HMD wird in einem Koordinatensystem bestimmt, das dem HMD zugeordnet ist, wie z. B. Koordinaten im Weltraum. Das Koordinatensystem, das dem HMD zugeordnet ist, ist typischerweise anders als ein Koordinatensystem, das einer Kamera, die verwendet wird, um Bilder der Szene, die den Anwender enthalten, aufzunehmen, zugeordnet ist.At block 1405 the processor determines a position of the HMD. For example, the position of the HMD may be determined based on the 6DoF position information generated by the HMD and transmitted to the processor in real time. The position of the HMD is determined in a coordinate system that is associated with the HMD, such. B. Coordinates in space. The coordinate system associated with the HMD is typically different than a coordinate system associated with a camera used to capture images of the scene containing the user.
Bei Block 1410 bestimmt der Prozessor eine Orientierung einer Stellung des Gesichts des Anwenders relativ zu der Stellung des HMD. Die Stellung des Gesichts des Anwenders kann auf einer Stellung des Gesichts des Anwenders basieren, die in den Koordinaten der Kamera bestimmt wird, z. B. durch Anwenden eines Abgleichsalgorithmus auf das aufgenommene Bild und ein 3D-Modell des Gesichts des Anwenders, wie hier mit Bezug auf die 10 und 11 diskutiert ist. Die Orientierung der Stellung des Gesichts des Anwenders relativ zu der Stellung des HMD kann als eine Drehmatrix repräsentiert werden, wie hier diskutiert ist. Einige Ausführungsformen des HMD enthalten Sensoren, die verwendet werden können, um die Stellung des Gesichts des Anwenders relativ zu der Stellung des HMD zu bestimmen (oder deren Bestimmung zu verbessern).At block 1410 the processor determines an orientation of a position of the user's face relative to the position of the HMD. The position of the user's face may be based on a position of the user's face determined in the coordinates of the camera, e.g. By applying a matching algorithm to the captured image and a 3D model of the user's face as described herein with respect to FIGS 10 and 11 is discussed. The orientation of the position of the user's face relative to the position of the HMD may be represented as a rotational matrix, as discussed herein. Some embodiments of the HMD include sensors that may be used to determine (or improve the location of) the user's face relative to the position of the HMD.
Bei Block 1415 bestimmt der Prozessor eine Transformation zwischen den Koordinaten, die dem HMD zugeordnet sind, und den Koordinaten in dem Referenzrahmen der Kamera. Der Prozessor kann die Transformation basierend auf den Ergebnissen der Anwendung des Abgleichsalgorithmus auf das aufgenommene Bild und ein 3D-Modell des HMD bestimmen. Beispielsweise kann die Transformation durch Vergleichen von Orientierungen eines Aruco-Markierers in dem 3D-Modell des HMD und des gleichen Aruco-Markierers, der auf dem HMD eingesetzt und in dem aufgenommenen Bild sichtbar ist, bestimmt werden.At block 1415 the processor determines a transformation between the coordinates associated with the HMD and the coordinates in the reference frame of the camera. The processor may determine the transformation based on the results of applying the matching algorithm to the captured image and a 3D model of the HMD. For example, the transformation may be determined by comparing orientations of an Aruco marker in the 3D model of the HMD and the same Aruco marker used on the HMD and visible in the captured image.
Bei Block 1420 bestimmt der Prozessor eine Orientierung des Gesichts des Anwenders zu der Kamera in den Kamera-Koordinaten. Beispielsweise kann der Prozessor die Orientierung des Gesichts des Anwenders in den Kamera-Koordinaten durch Anwenden der Drehmatrix, die die relative Orientierung des Gesichts des Anwenders und des HMD bestimmt, in Kombination mit der Transformation zwischen den Koordinaten, die dem HMD zugeordnet sind, und den Koordinaten in dem Referenzrahmen der Kamera bestimmen.At block 1420 the processor determines an orientation of the user's face to the camera in the camera coordinates. For example, the processor may determine the orientation of the user's face in the camera coordinates by applying the rotation matrix, which determines the relative orientation of the user's face and the HMD, in combination with the transformation between the coordinates associated with the HMD and the Determine coordinates in the reference frame of the camera.
Bei Block 1425 synthetisiert der Prozessor einen verdeckten Abschnitts des Gesichts des Anwenders basierend auf einem Modell, das aus einer Blick-Datenbank wie z. B. der in 5 gezeigten Blick-Datenbank 500 abgerufen wird. Beispielsweise kann der Prozessor eine Blickrichtung für den Anwender in dem aufgenommenen Bild unter Verwendung von Signalen, die durch eine Augenverfolgungseinheit bereitgestellt sind, die in dem HMD eingesetzt ist, bestimmen. Der Prozessor kann dann auf eine Gesichtsprobe unter Verwendung der Blickrichtung als einen Index in die Blick-Datenbank zugreifen. Die Gesichtsprobe und in einigen Fällen ein Referenzmodell werden verwendet, um ein Bild, das den verdeckten Abschnitt des Gesichts des Anwenders repräsentiert, wiederzugeben.At block 1425 The processor synthesizes a hidden portion of the user's face based on a model derived from a look-up database such as B. the in 5 shown look database 500 is retrieved. For example, the processor may determine a viewing direction for the user in the captured image using signals provided by an eye tracking unit deployed in the HMD. The processor can then access a face sample using the line of sight as an index into the look database. The face sample, and in some cases a reference model, are used to render an image representing the hidden portion of the user's face.
Bei Block 1430 kombiniert der Prozessor das synthetisierte Bild, das für den verdeckten Abschnitt des Gesichts des Anwenders repräsentativ ist, und das aufgenommene Bild, das für den nicht verdeckten Abschnitt des Gesichts des Anwenders repräsentativ ist, um ein endgültiges Bild zu bilden. Beispielsweise kann das synthetisierte Bild auf das aufgenommene Bild angewandt werden, ihm überlagert werden oder verwendet werden, um Pixel, die dem verdeckten Abschnitts des Gesichts des Anwenders entsprechen, darin zu ersetzen. Eine Lichtdurchlässigkeit kann auf das synthetisierte Bild angewandt werden, bevor das synthetisierte Bild mit dem aufgenommenen Bild kombiniert wird, wie hier diskutiert ist. At block 1430 For example, the processor combines the synthesized image representative of the hidden portion of the user's face and the captured image representative of the unobstructed portion of the user's face to form a final image. For example, the synthesized image may be applied to the captured image, superimposed on it, or used to replace pixels therein corresponding to the hidden portion of the user's face. Light transmission may be applied to the synthesized image before the synthesized image is combined with the captured image, as discussed herein.
15 ist ein Ablaufdiagramm eines Verfahrens 1500 zum Bestimmen einer Stellung eines teilweise verdeckten Gesichts in einem Bild in einem Koordinatensystem einer Kamera, die das Bild erfasst, gemäß einigen Ausführungsformen. Das Verfahren 1500 ist in einigen Ausführungsformen des in 1 gezeigten Prozessors 820 implementiert. Das Verfahren 1500 kann in Nicht-Echtzeit (z. B. als Hintergrund-Thread oder Vorprozess) ausgeführt werden in Fällen, in denen das verdeckende HMD konfiguriert ist, 6DoF-Stellungsinformationen in Echtzeit bereitzustellen und deshalb als ein Stellvertreter für die Position und Orientierung des Gesichts des Anwenders verwendet werden kann. Das Verfahren 1500 kann außerdem in Echtzeit ausgeführt werden in Fällen, in denen das verdeckende HMD nicht konfiguriert ist, 6DoF-Stellungsinformationen bereitzustellen, und die Stellung des Gesichts des Anwenders für jedes durch die Kamera aufgenommene Bild bestimmt werden muss. 15 is a flowchart of a method 1500 for determining a position of a partially obscured face in an image in a coordinate system of a camera capturing the image, in accordance with some embodiments. The procedure 1500 is in some embodiments of the in 1 shown processor 820 implemented. The procedure 1500 can be performed in non-real time (eg, as a background thread or pre-process) in cases where the obscuring HMD is configured to provide 6DoF position information in real time and therefore as a proxy for the position and orientation of the user's face can be used. The procedure 1500 can also be performed in real time in cases where the obscuring HMD is not configured to provide 6DoF position information, and the position of the user's face must be determined for each image taken by the camera.
Bei Block 1505 nimmt die Kamera ein Bild einer Szene auf, das das Gesicht des Anwenders enthält. In der Szene trägt der Anwender ein HMD, das einen Abschnitt des Gesichts des Anwenders verdeckt.At block 1505 the camera captures an image of a scene containing the user's face. In the scene, the user wears an HMD that obscures a portion of the user's face.
Bei Block 1510 greift der Prozessor auf ein Referenzmodell des Gesichts des Anwenders zu. Beispielsweise kann der Prozessor das Referenzmodell aus einer Blick-Datenbank wie z. B. der in 5 gezeigten Blick-Datenbank 500 abrufen.At block 1510 the processor accesses a reference model of the user's face. For example, the processor may use the reference model from a lookup database such as B. the in 5 shown look database 500 recall.
Bei Block 1515 wählt der Prozessor eine Kandidatenstellung (z. B. Position und Orientierung) des Referenzmodells in dem der Kamera zugeordneten Koordinatensystem aus. Die Kandidatenstellung des Referenzmodells entspricht einer Kandidatenstellung des Gesichts des Anwenders.At block 1515 For example, the processor selects a candidate position (eg, position and orientation) of the reference model in the coordinate system associated with the camera. The candidate position of the reference model corresponds to a candidate position of the face of the user.
Bei dem Entscheidungsblock 1520 bestimmt der Prozessor, ob nicht verdeckte Abschnitte des Referenzmodells (z. B. die Stirn des Anwenders und der Mund/Kinn-Bereich des Anwenders) mit entsprechenden Abschnitten in dem aufgenommenen Bild übereinstimmen. In einem 2D-Vergleich wird das Referenzmodell aus der Perspektive der Kamera wiedergegeben, um Pixel zu erzeugen, die für das 3D-Modell des Gesichts des Anwenders in der Kandidatenorientierung repräsentativ sind. Das wiedergegebene Bild wird dann mit dem aufgenommenen Bild verglichen, um eine Abgleichsbewertung zu erzeugen. In einem 3D-Vergleich wird ein ICP-Algorithmus verwendet, um das 3D-Referenzmodell mit dem aufgenommenen Bild, das Tiefeninformationen für jedes Pixel enthält, zu vergleichen und eine Abgleichsbewertung zu erzeugen. Ein relativ hoher Wert der Abgleichsbewertung wie z. B. eine Abgleichsbewertung oberhalb eines Schwellenwerts gibt eine Übereinstimmung an. Falls der Prozessor eine Übereinstimmung detektiert, fährt das Verfahren 1500 zu Block 1525 fort. Falls der Prozessor keine Übereinstimmung detektiert, fährt das Verfahren 1500 zu Block 1530 fort.At the decision block 1520 the processor determines whether non-obscured portions of the reference model (eg, the user's forehead and the user's mouth / chin area) match corresponding portions in the captured image. In a 2D comparison, the reference model is rendered from the perspective of the camera to produce pixels representative of the 3D model of the user's face in the candidate orientation. The rendered image is then compared to the captured image to produce a match score. In a 3D comparison, an ICP algorithm is used to compare the 3D reference model with the captured image containing depth information for each pixel and generate a match score. A relatively high value of the matching score such. For example, a match score above a threshold indicates a match. If the processor detects a match, the method continues 1500 to block 1525 continued. If the processor does not detect a match, the method continues 1500 to block 1530 continued.
Bei Block 1525 bestimmt der Prozessor die Stellung des Gesichts des Anwenders basierend auf der Stellung des Referenzmodells, das den hohen Wert der Abgleichsbewertung produziert hat.At block 1525 the processor determines the position of the user's face based on the position of the reference model that produced the high value of the match score.
Bei Block 1530 modifiziert der Prozessor die Orientierung des Referenzmodells in den Kamera-Koordinaten, um eine neue Kandidatenorientierung zu erzeugen. Das Verfahren 1500 kehrt dann zu dem Entscheidungsblock 1520 zurück, um zu bestimmen, ob die modifizierte Orientierung eine Übereinstimmung produziert. Obwohl die Blöcke 1515, 1520, 1525 als eine Schleife abgebildet sind, in der eine potentielle Übereinstimmung für jede Kandidatenorientierung vor dem Erzeugen einer neuen Kandidatenorientierung ermittelt wird, bestimmen einige Ausführungsformen des Verfahrens 1500 Abgleichsbewertungen für jede Kandidatenorientierung vor dem Bestimmen, welche aus den Kandidatenorientierungen mit dem aufgenommenen Bild am besten übereinstimmt. Beispielsweise erzeugt das Verfahren 1500 Abgleichsbewertungen für eine Menge von Kandidatenorientierungen und wählt dann eine höchste Abgleichsbewertung unter der Menge von Kandidatenorientierungen aus, um die Stellung des Gesichts des Anwenders bei Block 1530 zu bestimmen.At block 1530 The processor modifies the orientation of the reference model in the camera coordinates to create a new candidate orientation. The procedure 1500 then return to the decision block 1520 back to determine if the modified orientation produces a match. Although the blocks 1515 . 1520 . 1525 as a loop in which a potential match for each candidate orientation is determined prior to creating a new candidate orientation, some embodiments of the method determine 1500 Matching scores for each candidate orientation prior to determining which of the candidate orientations best matches the captured image. For example, the method generates 1500 Match scores for a set of candidate orientations and then select a highest match score among the set of candidate orientations to match the position of the user's face at block 1530 to determine.
16 ist ein Blockdiagramm eines Ende-zu-Ende-Systems 1600 zum Ausführen von Headset-Entfernung in gemischter Realität (MR) gemäß einigen Ausführungsformen. In einer gemischten Realität werden ein Bild oder ein Avatar eines Anwenders 1610 und ein virtueller Kontext des Anwenders 1610 in einem flachen Videoformat gemeinsam verwendet, das durch ein Publikum oder einen anderen Teilnehmer an der Szene mit gemischter Realität betrachtet werden kann, z. B. unter Verwendung eines Anzeigevorrichtung oder eines Bildschirms 1612. Das Ende-zu-Ende-System 1600 enthält eine Kamera 1605, die verwendet wird, um Bilder einer Szene, die den Anwender 1610 enthalten, aufzunehmen. Die Kamera 1605 ist als eine RGB-Kamera, die ein Bild erzeugt, das durch RGB-Werte von Pixeln in dem Bild repräsentiert wird, eine RGBD-Kamera, die ein Bild erzeugt, das durch die RGB-Werte der Pixel in dem Bild und Tiefenwerte, die einen Abstand zwischen der Kamera 1605 und einem Abschnitt der Szene, die durch die entsprechenden Pixel repräsentiert wird, repräsentiert wird, oder andere Typen von Kameras implementiert. Einige Ausführungsformen der Kamera 1605 sind Video-Kameras, die eine konfigurierbare Anzahl von Bildern pro Sekunde aufnehmen, z. B. kann die Kamera 1605 Bilder mit 60 FPS, 90 FPS, 120 FPS oder mit höheren oder niedrigeren Geschwindigkeiten aufnehmen. Einige Ausführungsformen der Kamera 1605 sind an einer Verfolgungseinheit 1615, wie z. B. einer 6DoF-VR-Verfolgungseinheit, die verwendet wird, um eine Position und Orientierung der Kamera 1605 zu bestimmen, angebracht. 16 is a block diagram of an end-to-end system 1600 for performing mixed-reality headset removal (MR) according to some embodiments. In a mixed reality become a picture or an avatar of a user 1610 and a virtual context of the user 1610 Used in a flat video format shared by an audience or other participant in the scene with mixed reality, e.g. B. using a display device or a screen 1612 , The end-to-end system 1600 contains a camera 1605 which is used to take pictures of a scene representing the user 1610 included, record. The camera 1605 is an RGB camera that produces an image represented by RGB values of pixels in the image, an RGBD camera that produces an image by the RGB values of the pixels in the image, and depth values a distance between the camera 1605 and a portion of the scene represented by the corresponding pixels, or other types of cameras. Some embodiments of the camera 1605 are video cameras that record a configurable number of frames per second, e.g. For example, the camera can shoot 1605 pictures at 60 FPS, 90 FPS, 120 FPS, or at higher or lower speeds. Some embodiments of the camera 1605 are at a tracking unit 1615 , such as A 6DoF VR tracking unit used to position and orient the camera 1605 to determine, appropriate.
Das Ende-zu-Ende-System 1600 enthält außerdem einen Prozessor 1620 und einen Speicher 1625. Der Prozessor 1620 ist konfiguriert, Anweisungen wie z. B. Anweisungen, die in dem Speicher 1625 gespeichert sind, auszuführen und die Ergebnisse der Anweisungen in dem Speicher 1625 zu speichern. Der Prozessor 1620 kann als ein individueller Prozessor oder als eine verteile Gruppe von Prozessoren implementiert sein. Der Prozessor 1620 ist konfiguriert, einen Bild-Strom zu empfangen, der Informationen enthält, die für die Bilder, die durch die Kamera 1605 aufgenommen werden, repräsentativ sind, wie z. B. RGB-Werte, Tiefenwerte und dergleichen für jedes aus den Pixeln in den Bildern. Der Prozessor 1620 kann die empfangenen Informationen in dem Speicher 1625 speichern. Der Prozessor 1620 ist außerdem konfiguriert, Bilder für gemischte Realität basierend auf den Informationen, die aus der Kamera 1605 empfangen werden, oder Informationen, auf die aus dem Speicher 1625 zugegriffen wird, zu erzeugen.The end-to-end system 1600 also contains a processor 1620 and a memory 1625 , The processor 1620 is configured to use instructions such as B. instructions that are in the memory 1625 are stored, execute and the results of the instructions in the memory 1625 save. The processor 1620 may be implemented as an individual processor or as a distributed set of processors. The processor 1620 is configured to receive a picture stream that contains information relevant to the pictures taken by the camera 1605 are representative, such. RGB values, depth values, and the like for each of the pixels in the images. The processor 1620 can the received information in the memory 1625 to save. The processor 1620 It also configures images for mixed reality based on the information coming from the camera 1605 be received, or information on the memory 1625 is accessed to generate.
Eine Blick-Datenbank 1630 wird in dem Speicher 1625 gespeichert, und der Prozessor 1620 ist fähig, auf Informationen in der Blick-Datenbank 1630 aus dem Speicher 1625 zuzugreifen. Die Blick-Datenbank 1630 wird unter Verwendung einiger Ausführungsformen des in 1 gezeigten Verarbeitungssystems produziert. A look database 1630 will be in the memory 1625 saved, and the processor 1620 is able to access information in the look database 1630 from the store 1625 access. The look database 1630 is made using some embodiments of the in 1 produced processing system produced.
Beispielsweise wird die Blick-Datenbank 1630 unter Verwendung einiger Ausführungsformen des in 6 gezeigten Verfahrens 600 und des in 7 gezeigten Verfahrens 700 erzeugt. Einige Ausführungsformen der Blick-Datenbank 1630 werden vor dem Ausführen der Headset-Entfernung unter Verwendung eines Verarbeitungssystems, das von dem Ende-zu-Ende-System 1600 verschieden ist, erzeugt. Beispielsweise kann der Anwender 1610 einen Aufnahmeprozess ausführen, um die Blick-Datenbank 1630 zu erzeugen, bevor er an der MR-Sitzung unter Verwendung des Ende-zu-Ende-Systems 1600 teilnimmt. In Fällen, in denen die Blick-Datenbank 1630 unter Verwendung eines Vorprozesses erzeugt wird, kann die Blick-Datenbank 1630 in einem nichttransitorischen computerlesbaren Medium gespeichert werden, das Speicherelemente enthalten kann, wie z. B. RAM, der in einer Cloud implementiert ist, digitale Video-Discs (DVDs), Flash-Speicher und dergleichen. Die gespeicherte Blick-Datenbank 1630 kann nachfolgend in den Speicher 1625 übertragen oder kopiert werden. Beispielsweise kann die Blick-Datenbank 1630 aus der Cloud über drahtgebundene oder drahtlose Kommunikationsstrecken heruntergeladen werden, es kann auf eine DVD, die die Blick-Datenbank 1630 speichert, unter Verwendung eines Plattenlaufwerks, das in dem Ende-zu-Ende-System 1600 implementiert ist, zugegriffen werden, es kann ein Flash-Speicher, der die Blick-Datenbank 1630 enthält, in einen USB-Anschluss in dem Ende-zu-Ende-System 1600 eingeführt werden, und dergleichen. Alternativ kann das Ende-zu-Ende-System 1600 konfiguriert sein, die Blick-Datenbank 1630 zu erzeugen, z. B. unter Verwendung einiger Ausführungsformen des in 6 gezeigten Verfahrens 600 und des in 7 gezeigten Verfahrens 700. In Fällen, in denen die Blick-Datenbank 1630 durch das Ende-zu-Ende-System 1600 erzeugt wird, wird die Blick-Datenbank 1630 direkt in dem Speicher 1625 gespeichert.For example, the Blick database 1630 using some embodiments of the in 6 shown method 600 and of in 7 shown method 700 generated. Some embodiments of the look database 1630 are prior to performing the headset removal using a processing system that is from the end-to-end system 1600 different, generated. For example, the user 1610 perform a recording process to the gaze database 1630 before generating at the MR session using the end-to-end system 1600 participates. In cases where the look database 1630 is generated using a pre-process, the look database can 1630 stored in a non-transitory computer-readable medium that may contain storage elements, such as storage media. RAM implemented in a cloud, digital video discs (DVDs), flash memories, and the like. The saved view database 1630 can subsequently be transferred to the memory 1625 or copied. For example, the gaze database 1630 may be downloaded from the cloud via wired or wireless communication links, it may be on a DVD containing the gaze database 1630 stores, using a disk drive that is implemented in the end-to-end system 1600, it can be a flash memory containing the view database 1630 contains into a USB port in the end-to-end system 1600 be introduced, and the like. Alternatively, the end-to-end system 1600 be configured, the look database 1630 to produce, for. B. using some embodiments of the in 6 shown method 600 and of in 7 shown method 700 , In cases where the Blick database 1630 is through the end-to-end system 1600 is generated, the look database 1630 is directly in the memory 1625 saved.
Der Anwender 1610 trägt ein HMD 1635, das ein oder mehrere Unterscheidungsmerkmale wie z. B. einen Aruco-Markierer 1640 enthält. Das HMD 1635 ist mit dem Prozessor 1620 verbunden, so dass der Prozessor 1620 und das HMD 1635 Signale über die Verbindung austauschen. Die Verbindung kann eine drahtgebundene Verbindung (z. B. ein Haltegurt) oder eine drahtlose Verbindung sein. The user 1610 carries an HMD 1635 having one or more distinguishing features, such as An arco marker 1640 contains. The HMD 1635 is with the processor 1620 connected, so the processor 1620 and the HMD 1635 will exchange signals over the link. The connection may be a wired connection (eg, a tether) or a wireless connection.
In einigen Ausführungsformen stellt das HMD 1635 einen Telemetrie-Strom, der Informationen wie z. B. 6DoF-Stellungsinformationen enthält, für den Prozessor 1620 über die Verbindung bereit. Der Prozessor 1620 kann den Telemetrie-Strom verwenden, um die Stellung des HMD 1635 in Echtzeit zu bestimmen. Das HMD 1635 enthält außerdem eine Augenverfolgungseinheit zum Verfolgen der Blickrichtungen der Augen des Anwenders. Informationen, die die Blickrichtungen angeben, werden über die Verbindung von dem HMD 1635 zu dem Prozessor 1620 in einem Blick-Strom übertragen. Der Prozessor 1620 kann den Blick-Strom der Blickrichtungen verwenden, um die Blickrichtungen für die Augen des Anwenders in Echtzeit zu bestimmen.In some embodiments, the HMD 1635 a telemetry stream containing information such as B. 6DoF position information for the processor 1620 ready for the connection. The processor 1620 can use the telemetry stream to adjust the position of the HMD 1635 in real time. The HMD 1635 Also includes an eye tracking unit for tracking the viewing directions of the user's eyes. Information indicating the line of sight is sent over the connection from the HMD 1635 to the processor 1620 transmitted in a glance stream. The processor 1620 can use the gaze stream of gaze directions to determine the gaze directions for the user's eyes in real time.
In einigen Ausführungsformen hält der Anwender 1610 eine oder mehrere VR-Steuereinheiten wie z. B. die 6DoF-Steuereinheiten 1645, 1650, die in 16 gezeigt sind. Das Ende-zu-Ende-System 1600 kann deshalb auch eine oder mehrere VR-Verfolgungseinheiten 1655, 1660 enthalten, die verwendet werden, um die Positionen und Orientierungen der VR-Steuereinheiten 1645, 1650 zu verfolgen. Die VR-Verfolgungseinheiten 1655, 1660 können außerdem verwendet werden, um die VR-Verfolgungseinheit 1615, die an der Kamera 1605 angebracht ist, zu verfolgen. Die Verfolgungsinformationen für die VR-Verfolgungseinheit 1615 können verwendet werden, um die Kalibrierung der relativen Positionen und Orientierungen der Kamera 1605 und des HMD 1635 zu verbessern, wie hier diskutiert ist. Der Anwender 1610 in der dargestellten Ausführungsform ist vor einer grünen Wand 1665 positioniert, um Zusammenstellen von Bildern mit farbbasierter Bildfreistellung des Anwenders 1610 mit anderen virtuellen Bildern zu unterstützen, um die Erfahrung gemischter Realität zu produzieren. In einigen Ausführungsformen wird die grüne Wand 1665 nicht verwendet. Stattdessen kann ein RGBD-Sensor, der für die Kamera 1605 kalibriert ist, verwendet werden, um genaue Segmentierung des Anwenders 1610 aus Hintergrund-Pixeln auszuführen, z. B. basierend auf der relativen Tiefe der Vordergrund- und Hintergrund-Pixel. Alternativ können Maschinenlerntechniken verwendet werden, um Segmentierung des Anwenders 1610 aus dem Hintergrund unter Verwendung von nur RGB-Informationen auszuführen. In some embodiments, the user stops 1610 one or more VR control units such. For example, the 6DoF controllers 1645, 1650 included in 16 are shown. The end-to-end system 1600 may therefore also have one or more VR tracking units 1655 . 1660 included, which are used to determine the positions and orientations of the VR control units 1645 . 1650 to pursue. The VR tracking units 1655 . 1660 can also be used to control the VR tracking unit 1615 standing at the camera 1605 appropriate to pursue. The tracking information for the VR tracking unit 1615 can be used to calibrate the relative positions and orientations of the camera 1605 and the HMD 1635 to improve, as discussed here. The user 1610 in the illustrated embodiment, is positioned in front of a green wall 1665 to compose images with color-based image relief of the user 1610 to assist with other virtual images to produce the experience of mixed reality. In some embodiments, the green wall becomes 1665 not used. Instead, an RGBD sensor can be used for the camera 1605 calibrated, used to accurately segment the user 1610 from background pixels, e.g. Based on the relative depths of the foreground and background pixels. Alternatively, machine learning techniques may be used to segment the user 1610 from the background using only RGB information.
Wie hier diskutiert ist, kann die „Klotz-im-Gesicht“-Erscheinung des Anwenders 1610, der das HMD 1635 trägt, das Gefühl des Eintauchens für den Anwender 1610 oder andere Anwender, die an der Erfahrung der gemischten Realität, die durch das Ende-zu-Ende-System 1600 bereitgestellt ist, teilnehmen, stören. Um das Gefühl des Eintauchens für alle Teilnehmer zu verbessern, ist das Ende-zu-Ende-System 1600 konfiguriert, Headset-Entfernung auf dem HMD 1635, das durch den Anwender 1610 getragen wird, in den durch die Kamera 1605 aufgenommenen Bildern auszuführen. Die Bilder nach der Headset-Entfernung werden dann mit entsprechenden Bildern der virtuellen Realität und in einigen Fällen mit Bildern anderer Anwender nach der Headset-Entfernung zusammengeführt, um Bilder mit gemischter Realität zu bilden. Einige Ausführungsformen des Prozessors 1620 sind deshalb konfiguriert, auf einen ersten Strom von Informationen, der die Telemetrie einer Stellung des HMD 1635 repräsentiert, einen zweiten Strom von Informationen, der Blickrichtungen des Anwenders 1610 repräsentiert, und einen dritten Strom von Informationen, der mehrere Bilder der durch die Kamera 1605 aufgenommenen Szene repräsentiert, zuzugreifen. Der Prozessor 1620 ist konfiguriert, 3D-Modelle eines ersten Abschnitts des Gesichts des Anwenders, der in den aufgenommenen Bildern durch das HMD 1635 verdeckt ist, basierend auf dem ersten, dem zweiten und dem dritten Strom wiederzugeben. Der Prozessor 1620 ist ferner konfiguriert, Bilder mit gemischter Realität durch Kombinieren von Bildern mit virtueller Realität, eines zweiten Abschnitts des Gesichts des Anwenders, der in den aufgenommenen Bildern nicht durch das HMD verdeckt ist, und der wiedergegebenen 3D-Modelle des verdeckten Abschnitts des Gesichts des Anwenders zu erzeugen.As discussed here, the user's "block-in-the-face" appearance may be 1610 who is the HMD 1635 contributes, the feeling of immersion for the user 1610 or other users who are interested in the experience of mixed reality by the end-to-end system 1600 is provided, participate, disturb. To improve the feeling of immersion for all participants is the end-to-end system 1600 configured, headset removal on the HMD 1635 by the user 1610 is borne in by the camera 1605 to take pictures taken. The images after the headset removal are then merged with corresponding virtual reality images and, in some cases, other users' images after the headset removal to form mixed reality images. Some embodiments of the processor 1620 are therefore configured on a first stream of information, which is the telemetry of a position of the HMD 1635 represents a second stream of information, the viewing directions of the user 1610 represents, and a third stream of information, the multiple images of the camera 1605 represented scene, access. The processor 1620 is configured 3D models of a first section of the user's face, in the captured images through the HMD 1635 is hidden, based on the first, the second and the third stream to play. The processor 1620 It is further configured to render mixed reality images by combining virtual reality images, a second portion of the user's face that is not obscured by the HMD in the captured images, and the rendered 3D models of the hidden portion of the user's face produce.
In einigen Ausführungsformen ist der Anwender 1610 durch eine Live-3D-Repräsentation repräsentiert, die unter Verwendung einer strukturierten Punktwolke, einem strukturierten Raster und dergleichen berechnet werden kann. Die 3D-Repräsentation kann unter Verwendung eines RGBD-Sensors, eines Stereo-Kamerapaars oder Anwenden von Maschinenlernen, um Modelle von Menschen zu lernen, berechnet werden, die dann verwendet werden können, um die 3D-Repräsentation zu erzeugen. Die 3D-Repräsentation des Anwenders 1610 kann dann in die Szene mit gemischter Realität zusammengesetzt werden. Das Verwenden der 3D-Repräsentation kann zu einem realistischeren Zusammenführen der realen und der virtuellen Welt führen. Beispielsweise stellt die 3D-Repräsentation eine genaue Tiefe für jedes Pixel bereit, das dem Anwender 1610 zugeordnet ist. Als ein weiteres Beispiel ermöglicht es die 3D-Repräsentation, dass das Ende-zu-Ende-System 1600 die korrekten Beleuchtungs-Interaktionen zwischen der virtuellen Welt und dem Anwender 1610 produziert.In some embodiments, the user is 1610 through a live 3D Representation that can be calculated using a structured point cloud, a structured grid and the like. The 3D Presentation can be calculated using an RGBD sensor, a stereo camera pair, or applying machine learning to learn models of humans, which can then be used to generate the 3D representation. The 3D representation of the user 1610 can then be put into the mixed reality scene. Using the 3D representation can lead to a more realistic merging of the real world and the virtual world. For example, the 3D representation provides an accurate depth for each pixel to the user 1610 assigned. As another example, the 3D representation allows the end-to-end system 1600 to provide the correct illumination interactions between the virtual world and the user 1610 produced.
17 ist ein Diagramm, das eine Anordnung 1700, die verwendet wird, um automatische Kalibrierung zwischen einer Kamera 1705 und einer Stellung eines HMD auszuführen, gemäß einigen Ausführungsformen darstellt. Die Kalibrierung kann durch einen Prozessor 1708 ausgeführt werden, der unter Verwendung einiger Ausführungsformen des in 16 gezeigten Prozessors 1620 implementiert sein kann. In der dargestellten Ausführungsform ist die Kamera 1705 mit einer VR-Verfolgungseinheit 1730 verbunden, so dass die Position und Orientierung der Kamera 1705 durch Verfolgen der Position und Orientierung der VR-Verfolgungseinheit 1730 bestimmt werden können. Die VR-Verfolgungseinheit 1730 ist jedoch optional und nicht erforderlich, um die automatische Kalibrierung auszuführen. 17 is a diagram showing an arrangement 1700 which is used to perform automatic calibration between a camera 1705 and performing a position of an HMD, in accordance with some embodiments. The calibration can be done by a processor 1708 carried out using some embodiments of the in 16 shown processor 1620 can be implemented. In the illustrated embodiment, the camera is 1705 with a VR tracking unit 1730 connected, so that the position and orientation of the camera 1705 by tracking the position and orientation of the VR tracking unit 1730 can be determined. The VR tracking unit 1730 however, it is optional and not required to perform the automatic calibration.
Die Anordnung 1700 bildet das HMD in drei unterschiedlichen Orientierungen 1710, 1715, 1720 ab. In der ersten Orientierung 1710 befindet sich das HMD in einer Position und Orientierung, die durch den Pfeil 1725 angegeben ist, was angibt, dass die erste Orientierung 1710 mit dem Gesicht zur Kamera 1705 ist. In der zweiten Orientierung 1715 befindet sich das HMD in einer Position und Orientierung, die durch den Pfeil 1730 angegeben ist, was angibt, dass die zweite Orientierung 1715 in Bezug auf die erste Orientierung 1710 um einen Winkel 1736 um eine Achse, die senkrecht zu der Zeichenebene ist, gedreht ist. In der dritten Orientierung 1720 ist das HMD um eine Achse, die durch den Pfeil 1725 angegeben ist z. B. eine Achse, die in der Zeichenebene ist, wie durch den Pfeil 1740 angegeben ist, gedreht. In einigen Ausführungsformen sind zusätzliche Positionen oder Orientierungen enthalten, um die Kalibrierung zu verbessern.The order 1700 The HMD forms in three different orientations 1710 . 1715 . 1720 from. In the first orientation 1710 The HMD is in a position and orientation indicated by the arrow 1725 is indicated, indicating that the first orientation 1710 with the face to the camera 1705 is. In the second orientation 1715 The HMD is in a position and orientation that is determined by the arrow 1730 is indicated, indicating that the second orientation 1715 in relation to the first orientation 1710 at an angle 1736 is rotated about an axis that is perpendicular to the plane of the drawing. In the third orientation 1720 is the HMD around an axis, indicated by the arrow 1725 is specified z. An axis that is in the drawing plane, such as the arrow 1740 is specified, rotated. In some embodiments, additional positions or orientations are included to enhance calibration.
Die Kalibrierung wird auf der Basis der Unterscheidungsmerkmale des HMD in den unterschiedlichen Orientierungen 1710, 1715 1720 ausgeführt. Beispielsweise kann die Kalibrierung auf der Basis eines Aruco-Markierers 1745, der an einer Oberfläche des HMD befestigt ist, ausgeführt werden. Als ein weiteres Beispiel kann die Kalibrierung auf der Basis eines Logo oder anderer spezifischer geometrischer Merkmale des HMD wie z. B. Kanten, Ecken, Kabelverbindungselemente und dergleichen ausgeführt werden. Ein Versatz des Aruco-Markierers 1745 oder anderer geometrischer Merkmale relativ zu einer Mitte (oder einem anderen Referenzpunkt) auf dem HMD kann manuell gemessen und für nachfolgendes Zugreifen durch den Prozessor 1708 gespeichert werden.The calibration is based on the distinguishing features of the HMD in the different orientations 1710 . 1715 1720 executed. For example, the calibration may be based on an Aruco marker 1745 which is attached to a surface of the HMD. As another example, the calibration may be based on a logo or other specific geometric features of the HMD, such as a. As edges, corners, cable connectors and the like can be performed. An offset of the Aruco marker 1745 or other geometric features relative to a center (or other reference point) on the HMD can be measured manually and for subsequent access by the processor 1708 get saved.
Die Kamera 1705 nimmt die Bilder 1750, 1755, 1760 des HMD in den entsprechenden Orientierungen 1710, 1715, 1720 auf. Für jedes Merkmal, das in den aufgenommenen Bildern 1750, 1755, 1760 detektiert wird, ist der Prozessor 1708 konfiguriert, einen 3D-Ort des Merkmals in dem HMD-Koordinatensystem unter Verwendung einer Stellung des HMD für die entsprechenden Orientierungen 1710, 1715, 1720 und des gemessenen Versatzes zu bestimmen. Wie hier diskutiert ist, ist der Prozessor 1708 fähig, die Stellungsinformationen für das HMD in einem Telemetrie-Strom, der von dem HMD empfangen wird, zu erfassen. Der Prozessor 1708 ist außerdem fähig, Pixelorte der Merkmale in den aufgenommenen Bildern 1750, 1755, 1760 zu identifizieren. Der 3D-Ort und die Pixelorte des Merkmals bilden ein 2D/3D-Punktepaar. Der Prozessor 1708 ist deshalb fähig, eine Menge von 2D/3D-Punktepaaren aus den aufgenommenen Bildern 1750, 1755, 1760 und den entsprechenden Stellungsinformationen zu erzeugen. Die Menge von 2D/3D-Punktepaaren wird dann verwendet, um eine Projektionsmatrix zu bestimmen, die die 3D-Merkmale auf die 2D-Bildebene der Kamera 1705 projiziert. In einigen Ausführungsformen enthält die Menge der 2D/3D-Punktepaare Bilder, die aus unterschiedlichen Positionen durch eine bewegliche Kamera 1705 aufgenommen sind, und die Orte der Kamera, als die Bilder erfasst wurden. Die Projektionsmatrix kann nachfolgend durch die kalibrierte Kamera 1705 verwendet werden, um einen 3D-Ort und die Orientierung des HMD basierend auf einem 2D-Bild des HMD, das die in der Projektionsmatrix repräsentierten Merkmale enthält, zu bestimmen.The camera 1705 take the pictures 1750 . 1755 . 1760 of the HMD in the corresponding orientations 1710 . 1715 . 1720 on. For each feature, that in the captured images 1750 . 1755 . 1760 is detected is the processor 1708 configured a 3D location of the feature in the HMD coordinate system using a position of the HMD for the respective orientations 1710 , 1715, 1720 and the measured offset. As discussed here, the processor is 1708 capable of detecting the position information for the HMD in a telemetry stream received from the HMD. The processor 1708 is also capable of identifying pixel locations of the features in the captured images 1750, 1755, 1760. The 3D location and pixel locations of the feature form a 2D / 3D point pair. The processor 1708 is therefore capable of generating a set of 2D / 3D point pairs from the captured images 1750 . 1755 . 1760 and generate the corresponding position information. The set of 2D / 3D point pairs is then used to determine a projection matrix that maps the 3D features to the 2D image plane of the camera 1705 projected. In some embodiments, the set of 2D / 3D point pairs includes images taken from different positions by a moving camera 1705 and the locations of the camera when the images were captured. The projection matrix can subsequently pass through the calibrated camera 1705 be used to determine a 3D location and the orientation of the HMD based on a 2D image of the HMD containing the features represented in the projection matrix.
18 ist ein Ablaufdiagramm eines Verfahrens 1800 zum Kalibrieren einer Kamera, die verwendet wird, um Bilder eines Anwenders, der ein HMD trägt, in einem Ende-zu-Ende-System für gemischte Realität aufzunehmen, gemäß einigen Ausführungsformen. Das Verfahren 1800 ist in einem Prozessor wie z. B. einigen Ausführungsformen des Prozessors 1620 in dem in 16 gezeigten Ende-zu-Ende-Systems 1600 für den in 17 gezeigten Prozessor 1708 implementiert. Das Verfahren 1800 wird auf Bilder von Merkmalen des HMD in unterschiedlichen Orientierungen angewandt, z. B. wie in der in 17 gezeigten Anordnung 1700 abgebildet. 18 is a flowchart of a method 1800 for calibrating a camera used to capture images of a user wearing an HMD in a mixed-reality end-to-end system, in accordance with some embodiments. The procedure 1800 is in a processor such. B. some embodiments of the processor 1620 in the 16 shown end-to-end system 1600 for the in 17 shown processor 1708 implemented. The procedure 1800 is applied to images of features of the HMD in different orientations, e.g. As in the in 17 shown arrangement 1700 displayed.
Bei Block 1805 identifiziert der Prozessor 2D-Pixelorte eines oder mehrerer HMD-Merkmale in einem aufgenommenen Bild, das das HMD an einem/einer ersten Ort und Orientierung enthält. Beispielsweise kann der Prozessor 2D-Pixelorte von HMD-Merkmalen in einem Bild eines Aruco-Markierers wie z. B. des in 17 gezeigten Aruco-Markierers 1745 identifizieren. Der Prozessor kann außerdem 2D-Pixelorte anderer HMD-Merkmale identifizieren, entweder zusätzlich zum oder anstelle des Identifizierens der 2D-Pixelorte eines Aruco-Markierers.At block 1805 The processor identifies 2D pixel locations of one or more HMD features in a captured image that includes the HMD at a first location and orientation. For example, the processor may include 2D pixel locations of HMD features in an image of an Aruco marker, such as an Arco marker. B. of in 17 shown Aruco marker 1745 identify. The processor may also identify 2D pixel locations of other HMD features, either in addition to or instead of identifying the 2D pixel locations of an Aruco marker.
Bei Block 1810 bestimmt der Prozessor 3D-Orte der HMD-Merkmale in dem aufgenommenen Bild in HMD-Koordinaten. Die 3D-Orte der HMD-Merkmale können durch Berechnen eines Mittelpunkts (oder eines anderen Referenzpunkts) des HMD unter Verwendung von Stellungsinformationen in einem Telemetrie-Strom, der durch das HMD für den Prozessor bereitgestellt wird, bestimmt werden. Früher gemessener Versatz der HMD-Merkmale in Bezug auf den Mittelpunkt (oder den anderen Referenzpunkt) wird dann mit dem 3D-Ort des Mittelpunkts (oder des anderen Referenzpunkts) kombiniert, um die 3D-Orte der HMD-Merkmale zu bestimmen.At block 1810 the processor determines 3D locations of the HMD features in the captured image in HMD coordinates. The 3D locations of the HMD features may be determined by computing a midpoint (or other reference point) of the HMD using position information in a telemetry stream provided by the HMD for the processor. Previously measured offset of the HMD features with respect to the midpoint (or other reference point) is then combined with the 3D location of the midpoint (or other reference point) to determine the 3D locations of the HMD features.
Bei Block 1815 speichert der Prozessor ein 2D/3D-Punktepaar, das Informationen enthält, die den 2D-Ort jedes Pixels, das dem HMD-Merkmal zugeordnet ist, und einen entsprechenden 3D-Ort des Abschnitts des HMD-Merkmals, das durch das Pixel repräsentiert ist, angeben.At block 1815 the processor stores a 2D / 3D point pair containing information representing the 2D location of each pixel associated with the HMD feature and a corresponding 3D location of the portion of the HMD feature represented by the pixel, specify.
Bei dem Entscheidungsblock 1820 bestimmt der Prozessor, ob zusätzliche Bilder, die einem neuen HMD-Ort zugeordnet sind, zum Verarbeiten vorhanden sind. Der neue HMD-Ort kann eine neue Position oder Orientierung des HMD und in Fällen, in denen die Kamera beweglich ist, eine neue Positionsorientierung der Kamera angeben. Falls zusätzliche Bilder zum Verarbeiten vorhanden sind, kehrt das Verfahren 1800 zu Block 1805 zurück. Falls keine zusätzlichen Bilder zum Verarbeiten vorhanden sind, fährt das Verfahren mit Block 1825 fort. At the decision block 1820 the processor determines whether additional images associated with a new HMD location are available for processing. The new HMD location may indicate a new position or orientation of the HMD and, in cases where the camera is movable, a new positional orientation of the camera. If there are additional images to process, the process returns 1800 to block 1805 back. If there are no additional images to process, the process moves to block 1825 continued.
Bei Block 1825 bestimmt der Prozessor eine Kamerakalibrierung, die eine Beziehung zwischen den HMD-Merkmalen, die in durch die Kamera aufgenommenen Bildern detektiert werden, und der HMD Stellung, die dem aufgenommenen Bild entspricht, herstellt. Der Prozessor bestimmt die Kamerakalibrierung basierend auf der Menge von 2D/3D-Punktepaaren. Beispielsweise kann der Prozessor eine Projektionsmatrix unter Verwendung einer Standardtechnik zum Minimieren eines Rückprojektionsfehlers bestimmen, wie z. B. eine Summe quadrierter Differenzen zwischen der Projektion des 3D-Punkts, der einem Merkmal zugeordnet ist, und dem Pixel, in dem das Merkmal detektiert wurde.At block 1825 For example, the processor determines a camera calibration that establishes a relationship between the HMD features detected in images captured by the camera and the HMD position corresponding to the captured image. The processor determines the camera calibration based on the amount of 2D / 3D point pairs. For example, the processor may determine a projection matrix using a standard technique for minimizing a backprojection error, such as: A sum of squared differences between the projection of the 3D point associated with a feature and the pixel in which the feature was detected.
Die Kamerakalibrierung, die durch das Verfahren 1800 erzeugt wird, kann dann verwendet werden, um Bilder mit gemischter Realität, die einen Anwender enthalten, der das HMD trägt, durch Zusammenführen eines oder mehrerer VR-Bilder, die durch die Kamera aufgenommen sind, und einer synthetisierten Repräsentation eines Abschnitts des Gesichts des Anwenders, der durch das HMD verdeckt ist, zu erzeugen. Ströme, die Informationen enthalten, die die VR-Bilder, die aufgenommenen Bilder und die synthetisierten Repräsentationen repräsentieren, können synchronisiert werden. Beispielsweise kann eine Zeitreferenz, die durch den Prozessor verwendet wird, mit einer Zeitreferenz, die durch das HMD verwendet wird, synchronisiert werden, und kann auf der Basis einer Kreuzkorrelation der zugehörigen Ereignisse, die in dem HMD auftreten und durch den Prozessor in den aufgenommenen Bildern des HMD detektiert werden, bestimmt werden.The camera calibration, by the method 1800 can then be used to produce mixed reality images that include a user wearing the HMD by merging one or more VR images captured by the camera and a synthesized representation of a portion of the user's face that is obscured by the HMD. Streams containing information representing the VR images, the captured images, and the synthesized representations can be synchronized. For example, a time reference used by the processor may be synchronized with a time reference used by the HMD, and may be based on cross-correlation of the related events occurring in the HMD and by the processor in the captured images of the HMD are detected.
19 stellt Variationen gemessener Parameter, die der Bewegung eines HMD und Bildern des sich bewegenden HMD zugeordnet sind, gemäß einigen Ausführungsformen dar. In der dargestellten Ausführungsform wird das HMD vor einer RGB-Kamera parallel zu der RGB-Bildebene bewegt. Die 3D-HMD-Position ist in einem Telemetrie-Strom angegeben, der durch einen Prozessor von dem HMD empfangen wird. Die 3D-HMD-Position wird auf die RGB-Kamera-Bildebene auf der Basis einer Projektionsmatrix, die durch einen Kamerakalibrierungsprozess wie z. B. das in 18 gezeigte Verfahren 1800 bestimmt wird, projiziert. Der Telemetrie-Strom ist zeitgestempelt, und die projizierte HMD-Position 1905 ist als eine Funktion der HMD-Zeitreferenz auf der Basis der Zeitstempel in dem Telemetrie-Strom aufgetragen. 19 illustrates variations of measured parameters associated with movement of an HMD and images of the moving HMD according to some embodiments. In the illustrated embodiment, the HMD is moved in front of an RGB camera parallel to the RGB image plane. The 3D HMD position is indicated in a telemetry stream received by a processor from the HMD. The 3D HMD position is applied to the RGB camera image plane based on a projection matrix generated by a camera calibration process, such as a camera calibration process. B. in 18 shown method 1800 is determined, projected. The telemetry stream is time stamped and the projected HMD position 1905 is plotted as a function of the HMD time reference based on the timestamps in the telemetry stream.
Ein oder mehrere Markierer (wie z. B. ein Aruco-Markierer) auf dem HMD werden in der RGB-Kamera-Bildebene visuell verfolgt, wenn das HMD vor der RGB-Kamera bewegt wird. Die durch die RGB-Kamera aufgenommenen Bilder werden unter Verwendung einer RGB-Kamera-Zeitreferenz zeitgestempelt. Die Markierer-Position 1910 in der RGB-Kamera-Bildebene ist als eine Funktion der RGB-Kamera-Zeitreferenz aufgetragen. Die projizierte HMD-Position in 1905 und die Markierer-Position 1910 sind kreuzkorreliert, und eine Spitze in der Kreuzkorrelationsfunktion gibt eine Verzögerung 1915 zwischen den zwei Signalen an. Die HMD-Zeitreferenz und die Kamera-Zeitreferenz werden durch Anwenden eines Zeitversatzes gleich der Verzögerung 1015, die unter Verwendung der Kreuzkorrelationsfunktion detektiert wird, synchronisiert. Die HMD-Stellungsinformationen, die in dem Telemetrie-Strom enthalten sind, können deshalb mit durch die Kamera aufgenommenen Bildern synchronisiert werden.One or more markers (such as an Aruco marker) on the HMD are visually tracked in the RGB camera image plane as the HMD is moved in front of the RGB camera. The images captured by the RGB camera are time-stamped using an RGB camera time reference. The marker position 1910 in the RGB camera image plane is plotted as a function of the RGB camera time reference. The projected HMD position in 1905 and the marker position 1910 are cross-correlated, and a spike in the cross-correlation function gives a delay 1915 between the two signals. The HMD time reference and the camera time reference become equal to the delay by applying a time offset 1015 , which is detected using the cross-correlation function, synchronized. The HMD position information contained in the telemetry stream can therefore be synchronized with images taken by the camera.
In einigen Ausführungsformen kann die Synchronisation der HMD-Zeitreferenz und der Kamera-Zeitreferenz ohne Bezug auf Merkmale oder Markierer auf dem HMD ausgeführt werden. Beispielsweise können feste Punkte auf dem Anwender oder einer VR-Steuereinheit verfolgt werden, um den Zeitversatz zu bestimmen. Als weiteres Beispiel kann, falls das HMD eine nach außen weisende Kamera aufweist, die einen Einrichtungstakt mit dem HMD gemeinsam verwendet, dann der HMD-Kamera-Strom (und im weiteren Sinne die HMD-Zeitreferenz) mit der Kamera-Zeitreferenz durch Verfolgung von Merkmalen in der physikalischen Umgebung, die durch die nach außen weisende Kamera des HMD gesehen werden, synchronisiert werden.In some embodiments, the synchronization of the HMD time reference and the camera time reference may be performed without reference to features or markers on the HMD. For example, fixed points on the user or a VR control unit can be tracked to determine the time offset. As another example, if the HMD has an outboard camera that shares a device clock with the HMD, then the HMD camera stream (and, in the broader sense, the HMD time reference) with the camera time reference can be tracked by tracing features in the physical environment seen by the HMD's out-facing camera.
20 stellt ein Anzeigesystem 2000, das eine elektronische Einrichtung 2005 enthält, die konfiguriert ist, VR-, AR- oder MR-Funktionalität über eine Anzeigevorrichtung bereitzustellen, gemäß einigen Ausführungsformen dar. Die dargestellte Ausführungsform der elektronischen Einrichtung 2005 ist ähnlich der in 9 gezeigten Ausführungsform der elektronischen Einrichtung 905. Die Funktionselemente der elektronischen Einrichtung 2005 sind deshalb gleich oder ähnlich den Funktionselementen der elektronischen Einrichtung 905, die durch die gleichen Bezugszeichen angegeben sind. Die elektronische Einrichtung 2005 enthält außerdem einen Taktgeber 2010, der eine interne Zeitreferenz zum Betrieb der Funktionselemente der elektronischen Einrichtung 2005 bereitstellt. 20 provides a display system 2000 , which includes an electronic device 2005 configured to provide VR, AR, or MR functionality via a display device, in accordance with some embodiments. The illustrated embodiment of the electronic device 2005 is similar to the one in 9 shown embodiment of the electronic device 905 , The functional elements of the electronic device 2005 are therefore the same or similar to the functional elements of the electronic device 905 , which are indicated by the same reference numerals. The electronic device 2005 contains as well as a clock 2010 , which is an internal time reference for operating the functional elements of the electronic device 2005 provides.
Die elektronische Einrichtung 2005 implementiert eine Augenverfolgungseinheit 950, die konfiguriert ist, Bewegungen und Positionen der Augen des Anwenders 910 durch Messen des Blickpunkts des Anwenders 910 oder Messen der Bewegung der Augen relativ zu dem Kopf des Anwenders 910 zu verfolgen. Die elektronische Einrichtung 2005 ist deshalb fähig, einen Blick-Strom zu erzeugen, der zeitgestempelte Informationen enthält, die für die Blickrichtungen der Augen des Anwenders 910 repräsentativ sind. Der Blick-Strom kann in Echtzeit für andere Einrichtungen wie z. B. den in 16 gezeigten Prozessor 1620 bereitgestellt werden.The electronic device 2005 implements an eye tracking unit 950 that is configured to movements and positions of the user's eyes 910 by measuring the viewpoint of the user 910 or measuring the movement of the eyes relative to the user's head 910 to pursue. The electronic device 2005 is therefore able to generate a gaze stream containing time-stamped information relevant to the gaze directions of the user's eyes 910 are representative. The Blick stream can be viewed in real time for other facilities such as B. the in 16 shown processor 1620 to be provided.
Die elektronische Einrichtung 2005 enthält die Sensoren 940, 945, die verwendet werden, um eine Positionsorientierung der elektronischen Einrichtung 905 zu detektieren. Obwohl zwei Sensoren 940, 945 im Interesse der Deutlichkeit gezeigt sind, kann die elektronische Einrichtung 905 mehr oder weniger Sensoren enthalten. Die Sensoren 940, 945 können Beschleunigungsmesser, Magnetometer, gyroskopische Detektoren, Positionssensoren, Infrarotsensoren und dergleichen enthalten, die als mikroelektromechanische Sensoren (MEMS-Sensoren) implementiert sein können. Die Sensoren 940, 945 in der elektronischen Einrichtung 2005 sind fähig, Informationen zu erzeugen, die die Stellung mit sechs Freiheitsgraden (6DoF-Stellung) der elektronischen Einrichtung 2005 angeben, was eine dreidimensionale Position der elektronischen Einrichtung 2005 und eine dreidimensionale Orientierung der elektronischen Einrichtung 2005 enthält. Die 6DoF-Stellung wird in einem Koordinatensystem erzeugt, das durch die elektronische Einrichtung 2005 definiert ist. Die elektronische Einrichtung 2005 ist deshalb fähig, einen Telemetrie-Strom zu erzeugen, der zeitgestempelte Informationen enthält, die für die 6DoF-Stellung der elektronischen Einrichtung 2005 repräsentativ sind. Der Telemetrie-Strom kann in Echtzeit für andere Einrichtungen wie z. B. den in 16 gezeigten Prozessor 1620 bereitgestellt werden.The electronic device 2005 contains the sensors 940 . 945 which are used to position the electronic device 905 to detect. Although two sensors 940 . 945 For the sake of clarity, the electronic device 905 contain more or fewer sensors. The sensors 940 . 945 may include accelerometers, magnetometers, gyroscopic detectors, position sensors, infrared sensors, and the like, which may be implemented as microelectromechanical sensors (MEMS sensors). The sensors 940 . 945 in the electronic device 2005 are able to generate information representing the six degrees of freedom position (6DoF position) of the electronic device 2005 indicate what a three-dimensional position of the electronic device 2005 and a three-dimensional orientation of the electronic device 2005 contains. The 6DoF position is generated in a coordinate system created by the electronic device 2005 is defined. The electronic device 2005 is therefore able to generate a telemetry stream containing time-stamped information relevant to the 6DoF position of the electronic device 2005 are representative. The telemetry stream can be used in real time for other facilities such. B. the in 16 shown processor 1620 to be provided.
Der Blick-Strom und der Telemetrie-Strom können unter Verwendung des gemeinsam verwendeten Taktes 2010 zeitlich synchronisiert werden. Einige Ausführungsformen der elektronischen Einrichtung 2005 implementieren jedoch keinen Takt, der durch die Augenverfolgungseinheit 950 und die Sensoren 940, 945 gemeinsam verwendet wird. In diesem Fall kann eine Kreuzkorrelation verwendet werden, um einen Versatz zwischen dem Blick-Strom und dem Telemetrie-Strom zu bestimmen. Beispielsweise kann der Anwender 910 seinen Kopf schnell hin und her bewegen, während er die elektronische Einrichtung 2005 trägt und während er seine Augen auf einen Punkt in dem virtuellen 3D-Raum fokussiert. Die Blickrichtung, die durch die Augenverfolgungseinheit 950 detektiert wird, ist dann (zeitlich) negativ korreliert mit der Orientierung des HMD, die durch die 6DoF-Stellung der elektronischen Einrichtung 2005, die durch die Sensoren 940, 945 detektiert wird, angegeben ist. Unter der Annahme, dass die Blickrichtung an einem Ursprung des HMD-Koordinatensystems einen Wert von null aufweist und dass Werte der Blickrichtung zu einer Seite des Ursprungs negativ sind und Werte der Blickrichtung zu der anderen Seite des Ursprungs positiv sind, kann der Zeitversatz zwischen dem Blick-Strom und dem Telemetrie-Strom durch Multiplizieren der Blickrichtung mit -1 berechnet werden, um eine „negative Blickrichtung“ zu bestimmen. Ein Zeitversatz, der eine maximale Kreuzkorrelation zwischen der negativen Blickrichtung und der Orientierung des HMD produziert, wird als der Zeitversatz zwischen dem Blick-Strom und dem Telemetrie-Strom verwendet.The look stream and the telemetry stream can be made using the shared clock 2010 be synchronized in time. Some embodiments of the electronic device 2005 however, do not implement a clock that passes through the eye tracking unit 950 and the sensors 940 . 945 is shared. In this case, a cross-correlation can be used to determine an offset between the gaze current and the telemetry current. For example, the user 910 move his head quickly back and forth while listening to the electronic device 2005 and focusing his eyes on a point in the virtual 3D space. The line of sight taken by the eye tracking unit 950 is then negatively correlated (time-wise) with the orientation of the HMD due to the 6DoF position of the electronic device 2005 passing through the sensors 940 . 945 is detected, is indicated. Assuming that the line of sight at a source of the HMD coordinate system has a value of zero, and that values of the line of sight to one side of the origin are negative and values of the line of sight to the other side of the origin are positive, the time lag between the view Current and the telemetry current by multiplying the line of sight by -1 to determine a "negative line of sight". A time offset that produces a maximum cross-correlation between the negative line of sight and the orientation of the HMD is used as the time offset between the gaze stream and the telemetry stream.
21 ist ein Ablaufdiagramm eines Verfahrens 2100 zum Ausführen von Headset-Entfernung für einen Anwender, der ein HMD trägt, in einer Szene mit gemischter Realität gemäß einigen Ausführungsformen. Das Verfahren 2100 ist in einem Prozessor wie z. B. in einigen Ausführungsformen des in 16 gezeigten Prozessors 1620 implementiert. 21 is a flowchart of a method 2100 for performing headset removal for a user wearing an HMD in a mixed reality scene, according to some embodiments. The procedure 2100 is in a processor such. B. in some embodiments of in 16 shown processor 1620 implemented.
Bei Block 2105 bestimmt der Prozessor eine Kalibrierung zwischen einer Kamera und einer HMD-Stellung. Beispielsweise kann der Prozessor eine Projektionsmatrix, die 3D-Merkmale des HMD auf die 2D-Bildebene der Kamera projiziert, bestimmen. Die Projektionsmatrix kann durch die kalibrierte Kamera verwendet werden, um einen 3D-Ort und eine Orientierung des HMD basierend auf einem 2D-Bild des HMD zu bestimmen.At block 2105 the processor determines a calibration between a camera and an HMD position. For example, the processor may determine a projection matrix that projects 3D features of the HMD onto the 2D image plane of the camera. The projection matrix may be used by the calibrated camera to determine a 3D location and orientation of the HMD based on a 2D image of the HMD.
Bei Block 2110 führt der Prozessor eine zeitliche Synchronisation des HMD-Stellungstelemetrie-Stroms, des Blick-Stroms und des Kamera-Stroms aus, der Bilder enthalten kann, die durch den Prozessor produziert werden oder in dem Prozessor aus anderen Quellen erhalten werden. Beispielsweise können die HMD-Stellungstelemetrie und der Blick-Strom auf der Basis einer gemeinsamen Zeitreferenz wie z. B. einem Takt, der in dem HMD implementiert ist, synchronisiert werden. Als ein weiteres Beispiel können die HMD-Stellungstelemetrie und der Kamera-Strom durch Verwenden einer Kreuzkorrelationsprozedur, um einen Zeitversatz zwischen einer HMD-Zeitreferenz und einer Kamera-Zeitreferenz zu bestimmen, synchronisiert werden. Der Zeitversatz kann dann verwendet werden, um die HMD- und die Kamera-Zeitreferenz und folglich den HMD-Stellungstelemetrie-Strom und den Kamera-Strom zu synchronisieren.At block 2110 For example, the processor performs a timing synchronization of the HMD position telemetry stream, gaze stream, and camera stream, which may include images produced by the processor or received in the processor from other sources. For example, the HMD position telemetry and gaze current may be determined based on a common time reference, such as a time reference. A clock implemented in the HMD. As another example, the HMD position telemetry and the camera current may be synchronized by using a cross-correlation procedure to determine a time offset between an HMD time reference and a camera time reference become. The skew can then be used to synchronize the HMD and camera time reference, and thus the HMD position telemetry current and camera current.
Bei Block 2115 erzeugt der Prozessor eine Repräsentation eines nicht verdeckten Abschnitts des Gesichts des Anwenders basierend auf Informationen, die in dem Blick-Strom enthalten sind. Beispielsweise kann der Prozessor eine Blickrichtung für den Anwender in jedem Bild unter Verwendung des zeitsynchronisierten Blick-Datenstroms bestimmen. Die Blickrichtung wird dann als ein Index in eine Blick-Datenbank, wie z. B. die in 16 gezeigte Blick-Datenbank 1630, verwendet. Eine Gesichtsprobe, die der Blickrichtung zugeordnet ist, wird aus der Blick-Datenbank abgerufen. Eine Blinzelprobe kann ebenfalls aus der Blick-Datenbank abgerufen werden, falls der Blick-Datenstrom angibt, dass der Anwender in dem entsprechenden Rahmen blinzelt. Der Prozessor ruft außerdem ein Referenzmodell ab, das die Geometrie des Gesichts des Anwenders angibt. Texturen aus den Gesichtsproben werden mit der Geometrie des Referenzmodells kombiniert, um eine Repräsentation des verdeckten Abschnitts des Gesichts des Anwenders zu synthetisieren.At block 2115 For example, the processor generates a representation of a non-obscured portion of the user's face based on information contained in the gaze stream. For example, the processor may determine a viewing direction for the user in each image using the time-synchronized gaze data stream. The viewing direction is then displayed as an index in a lookup database, such as B. the in 16 gaze database shown 1630 , used. A face sample associated with the gaze direction is retrieved from the gaze database. A blink sample may also be retrieved from the gaze database if the gaze stream indicates that the user is blinking in the corresponding frame. The processor also retrieves a reference model that specifies the geometry of the user's face. Textures from the face samples are combined with the geometry of the reference model to synthesize a representation of the hidden portion of the user's face.
Bei Block 2120 produziert der Prozessor jedes Bild in der Szene mit gemischter Realität durch Zusammenführen eines VR-Bilds eines virtuellen Abschnitts der Szene mit gemischter Realität, eines Bilds einer Szene, die den Anwender enthält, der das HMD trägt, das durch die Kamera aufgenommen ist, und die Repräsentation des verdeckten Abschnitts des Gesichts des Anwenders. Einige Ausführungsformen des Prozessors führen das VR-Bild, das aufgenommene Bild und die Repräsentation des verdeckten Abschnitts des Gesichts des Anwenders durch Überschreiben von Pixeln in dem VR-Bild oder dem aufgenommenen Bild mit Pixeln, die den verdeckten Abschnitts des Gesichts des Anwenders repräsentieren, zusammen. Wie hier diskutiert kann das Zusammenführen des VR-Bilds, des aufgenommenen Bilds und der Repräsentation des verdeckten Abschnitts des Gesichts des Anwenders außerdem Anwenden einer Lichtdurchlässigkeit für die Pixel, die den verdeckten Abschnitt des Gesichts des Anwenders repräsentieren, enthalten. Die Lichtdurchlässigkeit eines Pixels in der Repräsentation kann abhängig von dem relativen Ort des Pixels und dem HMD in dem aufgenommenen Bild variieren.At block 2120 For example, the processor produces each image in the mixed reality scene by merging a VR image of a virtual portion of the mixed reality scene, an image of a scene containing the user wearing the HMD captured by the camera, and Representation of the hidden portion of the user's face. Some embodiments of the processor combine the VR image, the captured image, and the representation of the hidden portion of the user's face by overwriting pixels in the VR image or the captured image with pixels that represent the hidden portion of the user's face , As discussed herein, merging the VR image, the captured image, and the representation of the hidden portion of the user's face may also include applying translucency to the pixels representing the hidden portion of the user's face. The translucency of a pixel in the representation may vary depending on the relative location of the pixel and the HMD in the captured image.
Wie hier diskutiert kann die Kalibrierung der Kamera gemäß dem in 18 gezeigten Verfahren 1800 verwendet werden, um einen 3D-Ort des HMD in nachfolgend aufgenommenen Bildern genau vorherzusagen. In einigen Ausführungsformen können kleine jedoch erkennbare Fehler in dem vorhergesagten 3D-Ort des HMD andauern und das Gefühl des Eintauchens beeinträchtigen, wenn die Headset-Entfernung auf der Basis der Kamerakalibrierung ausgeführt wird. Beispielsweise können Anwender eine relative Verschiebung von einigen wenigen Millimetern zwischen dem synthetisierten Augenbereich und den anderen Abschnitten des Gesichts, die durch das aufgenommene Bild repräsentiert werden, erkennen. Die Fehler können zeitabhängig (z. B. aufgrund ungenau berichteten Stellungen des HMD oder der VR-Verfolgungseinheit, die der Kamera zugeordnet ist, oder ungenauen Zeitversatz) oder zeitunabhängig (z. B. aufgrund von Fehlern der Kalibrierung oder radialer Verzerrung) sein.As discussed herein, the calibration of the camera may be performed in accordance with the method described in U.S. Patent Nos. 5,466,074 18 shown method 1800 can be used to accurately predict a 3D location of the HMD in subsequently captured images. In some embodiments, small but detectable errors in the predicted 3D location of the HMD may persist and affect the sense of immersion when the headset removal is performed based on the camera calibration. For example, users may recognize a relative displacement of a few millimeters between the synthesized eye area and the other portions of the face represented by the captured image. The errors may be time-dependent (eg, due to inaccurately reported positions of the HMD or the VR tracking unit associated with the camera or inaccurate time offset) or time-independent (eg, due to errors in calibration or radial distortion).
Einige Ausführungsformen können diese Fehler durch initiales Wiedergeben des HMD und der Repräsentation des verdeckten Abschnitts des Gesichts des Anwenders auf der Basis der Kamerakalibrierung, die bei Block 2015 bestimmt wird, und der HMD-Stellungsinformationen in dem synchronisierten HMD-Stellungstelemetrie-Strom kompensieren. Der Prozessor kann dann irgendwelche verbleibenden Fehler unter Verwendung eines Markierers (wie z. B. eines Aruco-Markierers) und eines 3D-Modells des HMD kompensieren. Einige Ausführungsformen des Prozessors kompensieren die HMD-Stellung basierend auf einer Differenz zwischen einem Bild, das aus dem 3D-Modell des HMD wiedergegeben wird, und einem entsprechenden Bild des HMD, das durch die Kamera aufgenommenen ist. Beispielsweise bestimmt der Prozessor für jedes Bild oder jeden Rahmen in der MR-Szene einen Pixelversatz zwischen einer vorhergesagten HMD-Position (in dem wiedergegebenen Bild) und einer tatsächlichen Position des HMD in dem aufgenommenen Bild. Der Pixelversatz kann durch Vergleichen einer Position eines Markierers (wie z. B. eines Aruco-Markierers) in dem wiedergegebenen Bild mit der Position des Markierers in dem aufgenommenen Bild bestimmt werden. Der Pixelversatz kann außerdem durch Vergleichen eines erwarteten Umrisses des HMD in dem wiedergegebenen Bild mit dem tatsächlichen MHD-Umriss in dem aufgenommenen Bild bestimmt werden. Der Prozessor berechnet dann eine 3D-Stellungskorrektur für die HMD-Stellung basierend auf dem Pixelversatz.Some embodiments may mitigate these errors by initially rendering the HMD and the representation of the hidden portion of the user's face based on the camera calibration shown in block 2015 and compensate for the HMD position information in the synchronized HMD position telemetry stream. The processor can then compensate for any remaining errors using a marker (such as an Aruco marker) and a 3D model of the HMD. Some embodiments of the processor compensate for the HMD position based on a difference between an image rendered from the 3D model of the HMD and a corresponding image of the HMD captured by the camera. For example, for each frame or frame in the MR scene, the processor determines a pixel offset between a predicted HMD position (in the rendered image) and an actual position of the HMD in the captured image. The pixel offset may be determined by comparing a position of a marker (such as an Aruco marker) in the rendered image with the position of the marker in the captured image. The pixel offset may also be determined by comparing an expected outline of the HMD in the rendered image with the actual MHD outline in the captured image. The processor then calculates a 3D position correction for the HMD position based on the pixel offset.
In einigen Ausführungsformen gibt der Prozessor das 3D-HMD-Modell zusammen mit dem Markierer aus und wendet ein Kantenfilter auf die wiedergegebene Ansicht und das entsprechende Kamerabild an. Kanten in dem wiedergegebenen Bild und dem Kamerabild werden dann abgeglichen, um einen 2D-Bildversatz zu bestimmen, der die beste Ausrichtung zwischen den Kanten in dem wiedergegebenen Bild und dem Kamerabild bereitstellt. Kanten in dem Kamerabild und dem wiedergegebenen Bild können unter Verwendung eines Gauß-Filters vorgefiltert werden, um die Robustheit des Umrissabgleichs zu erhöhen. Das Ausführen des Abgleichens auf dem Markierer und dem Umriss des MHD auf diese Weise weist zwei Vorteile auf: (1) Abgleichen der Kantenbilder anstatt der Bilder selbst ist weniger von der Beleuchtung abhängig, während eine HMD-Oberfläche typischerweise glänzend ist und irgendwo zwischen dunkel und hell erscheinen kann, abhängig von dem Winkel und der lokalen Beleuchtung, und (2) die Beiträge zu der Korrektur von dem Markierer und von dem Umriss sind komplementär, weil die Markierer-Kanten von der Vorderansicht am besten unterscheidbar sind, während der Umrissabgleich am besten funktioniert, wenn er aus einer Seitenansicht betrachtet wird. Der Prozessor kann außerdem einen 3D-Positionsversatz unter Verwendung der Brennweite der Kamera berechnen. Beispielsweise kann der Prozessor den 3D-Positionsversatz als eine Weltraum-Translation senkrecht zu der Betrachtungsrichtung der Kamera in dem Abstand des HMD berechnen. Zusätzlich kann der Prozessor eine Menge von Algorithmen anwenden, um die 3D-Korrektur einzublenden und auszublenden, in dem Fall, wenn der Abgleich von Markierer/Umriss temporär fehlschlägt oder wiederaufgenommen wird.In some embodiments, the processor outputs the 3D HMD model along with the marker and applies an edge filter to the rendered view and the corresponding camera image. Edges in the rendered image and the camera image are then adjusted to determine a 2D image offset that provides the best alignment between the edges in the rendered image and the camera image. Edges in the camera image and the rendered image may be prefiltered using a Gaussian filter to increase the robustness of the outline trim. Running the Balancing on the marker and the outline of the MHD in this way has two advantages: 1 ) Aligning the edge images rather than the images themselves is less dependent on illumination, while an HMD surface is typically glossy and may appear somewhere between dark and light, depending on the angle and local illumination, and ( 2 The contributions to the correction from the marker and from the outline are complementary because the marker edges are best distinguished from the front view, while the outline matching works best when viewed from a side view. The processor may also calculate a 3D position offset using the focal length of the camera. For example, the processor may calculate the 3D position offset as a space translation perpendicular to the viewing direction of the camera at the distance of the HMD. In addition, the processor may employ a set of algorithms to fade in and fade out the 3-D correction, in case the marker / outline match temporarily fails or resumes.
Hoch sichtbare Markierer wie z. B. Aruco-Markierer können die hier beschriebenen Merkmalsdetektionstechniken wesentlich verbessern. Diese Markierer können jedoch auch die visuelle Erscheinung des Anwenders oder des HMD in der Szene mit gemischter Realität stören. Einige Ausführungsformen des Prozessors sind deshalb konfiguriert, die Markierer aus der Szene mit gemischter Realität unter Verwendung von Markierer-Übermalen virtuell zu entfernen. Der Prozessor kann konfiguriert sein, Algorithmen zu implementieren, um Markierer-Umrisse zu detektieren und die entsprechenden Pixel mit Pixeln zu übermalen, die Pixelwerte aufweisen, die Farben des HMD im Bereich des Markierers entsprechen. In Ausführungsformen, die Aruco-Markierer verwenden, sagen die Markierer-Detektionsalgorithmen Orte der vier Ränder des Aruco-Markierers basierend auf der korrigierten HMD-Stellung voraus. Der Prozessor kann außerdem die Randorte verfeinern, so dass sie mit Rändern zusammenfallen, die in entsprechenden aufgenommenen Bildern detektiert werden. Das Verfeinern der Randorte ermöglicht es, dass der Prozessor Orte der Ränder detektiert, selbst bei Vorhandensein von Bewegungsunschärfe, die einen quadratischen Markierer in eine rechteckige Form oder sogar Polygonform transformieren kann. Die sichtbaren Oberflächen des HMD sind normalerweise von einheitlicher Farbe, so dass der Ort des Aruco-Markierers mit einer festen Farbe übermalt werden kann, die als ein Mittelwert der Nicht-Markierer-Pixel, die die Markierer-Fläche auf der Oberfläche des HMD umgeben, berechnet ist.High visibility markers such. Aruco markers can significantly improve the feature detection techniques described herein. However, these markers can also interfere with the visual appearance of the user or the HMD in the mixed reality scene. Some embodiments of the processor are therefore configured to virtually remove the markers from the mixed reality scene using marker over-painting. The processor may be configured to implement algorithms to detect marker outlines and paint over the corresponding pixels with pixels having pixel values corresponding to colors of the HMD in the region of the marker. In embodiments using Aruco markers, the marker detection algorithms predict locations of the four edges of the Aruco marker based on the corrected HMD position. The processor may also refine the edge locations to coincide with edges detected in corresponding captured images. The refinement of the edge locations allows the processor to detect locations of the edges, even in the presence of motion blur, which can transform a square marker into a rectangular shape or even polygon shape. The visible surfaces of the HMD are usually of uniform color so that the location of the Aruco marker can be overpainted with a solid color, which is an average of the non-marker pixels surrounding the marker area on the surface of the HMD. is calculated.
22 ist ein Blockdiagramm eines Verarbeitungssystems 2200 zum Erzeugen von Bildern einer MR-Szene, die einen Anwender enthalten, der ein HMD trägt, gemäß einigen Ausführungsformen. Das Verarbeitungssystem 2200 ist in einigen Ausführungsformen des in 16 gezeigten Ende-zu-Ende-Systems 1600 implementiert. Das Verarbeitungssystem 2200 enthält einen MR-Server 2205, der zeitsynchronisierte Ströme empfängt, die einen Telemetrie-Strom 2210 von HMD-Stellungsinformationen, einen Blick-Strom 2215 von Informationen, die eine Blickrichtung für den Anwender angeben, und einen Bild-Strom 2220, der Informationen enthält, die durch eine Kamera aufgenommene Bilder repräsentieren, enthalten. Der MR-Server 2205 kann als eine einzelne Entität (wie in 22 gezeigt) oder als ein verteiltes System wie z. B. mehrere Server, die verwendet werden, um Cloud-Dienste bereitzustellen, implementiert sein. Das Verarbeitungssystem 2200 enthält außerdem eine Anzeigevorrichtung 2225, die verwendet wird, um die MR-Bilder, die durch den MR-Server 2205 produziert werden, anzuzeigen. Obwohl die Anzeigevorrichtung 2225 in 22 als ein Fernsehgerät oder ein Computer-Monitor abgebildet ist, kann die Anzeigevorrichtung 2225 auch in anderen Formaten wie z. B. einem HMD, einem Mobiltelefon und dergleichen implementiert sein. 22 is a block diagram of a processing system 2200 for generating images of an MR scene containing a user wearing an HMD, according to some embodiments. The processing system 2200 is in some embodiments of the in 16 shown end-to-end system 1600 implemented. The processing system 2200 contains an MR server 2205 that receives time-synchronized streams that have a telemetry stream 2210 from HMD position information, a look stream 2215 information indicating a viewing direction for the user, and a picture stream 2220 containing information representing images taken by a camera. The MR server 2205 can be considered a single entity (as in 22 shown) or as a distributed system such. For example, multiple servers that are used to provide cloud services may be implemented. The processing system 2200 also includes a display device 2225 which is used to scan the MR images through the MR server 2205 produced to display. Although the display device 2225 in 22 is displayed as a TV or a computer monitor, the display device 2225 in other formats such. An HMD, a mobile phone, and the like.
Der MR-Server 2205 enthält einen Sender/Empfänger 2230 zum Senden und Empfangen von Signalen. Beispielsweise ist der Sender/Empfänger 2230 konfiguriert, die zeitsynchronisierten Ströme 2210, 2215, 2220 zu empfangen. Der Sender/Empfänger 2230 kann als eine einzelne integrierte Schaltung (z. B. unter Verwendung einer einzigen ASIC oder FPGA) oder als ein Ein-Chip-System (SOC), das unterschiedliche Module zum Implementieren der Funktionalität des Sender/Empfängers 2230 enthält, implementiert sein. Der MR-Server 2205 enthält außerdem einen Prozessor 2235 und einen Speicher 2240. Der Prozessor 2235 kann verwendet werden, um Anweisungen auszuführen, die in dem Speicher 2240 gespeichert sind, und Informationen wie z. B. die Ergebnisse der ausgeführten Anweisungen in dem Speicher zu speichern. Beispielsweise kann der Prozessor 2235 konfiguriert sein, einige Ausführungsformen des in 18 gezeigten Verfahrens 1800 oder des in 21 gezeigten Verfahrens 2100 zu implementieren.The MR server 2205 contains a transmitter / receiver 2230 for sending and receiving signals. For example, the sender / receiver 2230 configured, the time-synchronized streams 2210 . 2215 . 2220 to recieve. The transmitter / receiver 2230 can be used as a single integrated circuit (eg, using a single ASIC or FPGA) or as a single-chip system (SOC) that has different modules to implement the functionality of the transceiver 2230 contains, implements. The MR server 2205 also contains a processor 2235 and a memory 2240 , The processor 2235 can be used to execute statements in the memory 2240 are stored, and information such. For example, store the results of the executed instructions in memory. For example, the processor 2235 be configured to some embodiments of in 18 shown method 1800 or in 21 shown method 2100 to implement.
In einigen Ausführungsformen können spezielle Aspekte der vorstehend beschriebenen Techniken durch einen oder mehrere Prozessoren eines Verarbeitungssystems, die Software ausführen, implementiert sein. Die Software umfasst eine oder mehrere Gruppen ausführbarer Anweisungen, die in einem nichttransitorischen computerlesbaren Speichermedium gespeichert oder auf andere Weise materiell verwirklicht sind. Die Software kann die Anweisungen und spezielle Daten enthalten, die dann, wenn sie durch den einen oder die mehreren Prozessoren ausgeführt werden, den einen oder die mehreren Prozessoren manipulieren, um einen oder mehrere Aspekte der vorstehend beschriebenen Techniken auszuführen. Das nichttransitorische computerlesbare Speichermedium kann beispielsweise eine magnetische oder optische Plattenspeichervorrichtung, Festkörperspeichervorrichtungen wie z. B. Flash-Speicher, einen Cache, Direktzugriffsspeicher (RAM) oder andere nichtflüchtige Speichervorrichtung oder -vorrichtungen und dergleichen enthalten. Die ausführbaren Anweisungen, die auf dem nichttransitorischen computerlesbaren Speichermedium gespeichert sind, können in Quellcode, Assemblersprachen-Code, Objektcode oder einem anderen Anweisungsformat sein, das interpretiert wird oder auf andere Weise durch einen oder mehrere Prozessoren ausführbar ist.In some embodiments, specific aspects of the techniques described above may be implemented by one or more processors of a processing system executing software. The software includes one or more sets of executable instructions stored in or otherwise materially embodied in a non-transitory computer-readable storage medium. The software may include the instructions and specific data that, when executed by the one or more processors, manipulates the one or more processors to perform one or more aspects of the techniques described above. The non-transitory computer-readable storage medium may include, for example, a magnetic or optical disk storage device, solid-state storage devices such. Flash memory, a cache, Random Access Memory (RAM) or other non-volatile memory device or devices, and the like. The executable instructions stored on the non-transitory computer-readable storage medium may be in source code, assembly language code, object code, or other instruction format that is interpreted or otherwise executable by one or more processors.
Ein computerlesbares Speichermedium kann irgendein Speichermedium oder eine Kombination aus Speichermedien enthalten, die durch ein Computersystem während des Gebrauchs zugreifbar sind, um Anweisungen und/oder Daten für das Computersystem bereitzustellen. Solche Speichermedien können optische Medien (z. B. Compact Disc (CD), Digital Versatile Disc (DVD), Blu-Ray-Disc), magnetische Medien (z. B. Floppy-Disc, Magnetband oder magnetische Festplatte), flüchtigen Speicher (z. B. Direktzugriffsspeicher (RAM) oder Cache), nichtflüchtigen Speicher (z. B. Festwertspeicher (ROM) oder Flash-Speicher) oder auf mikroelektromechanischen Systemen basierende (MEMS-basierende) Speichermedien enthalten, sind jedoch nicht darauf beschränkt. Das computerlesbare Speichermedium kann in das Computersystem eingebettet sein (z. B. System-RAM oder ROM), fest an dem Computersystem angebracht sein (z. B. magnetisches Festplattenlaufwerk), abnehmbar an dem Computersystem angebracht sein (z. B. eine optische Platte oder ein auf dem universellen seriellen Bus (USB) basierter Flash-Speicher) oder mit dem Computersystem über ein drahtgebundenes oder drahtloses Netz verbunden sein (z. B. über ein Netz zugreifbarer Speicher (NAS)).A computer-readable storage medium may include any storage medium or combination of storage media that is accessible by a computer system during use to provide instructions and / or data to the computer system. Such storage media may include optical media (eg, Compact Disc (CD), Digital Versatile Disc (DVD), Blu-ray Disc), magnetic media (eg, floppy disk, magnetic tape, or magnetic hard disk), volatile memory ( random access memory (RAM) or cache), non-volatile memory (e.g., read-only memory (ROM) or flash memory), or microelectromechanical system based (MEMS-based) memory media, but are not limited thereto. The computer readable storage medium may be embedded in the computer system (eg, system RAM or ROM), fixedly attached to the computer system (eg, magnetic hard disk drive), removably attached to the computer system (eg, an optical disk or a universal serial bus (USB) based flash memory) or connected to the computer system via a wired or wireless network (eg via a network accessible storage (NAS)).
Gemäß einer beispielhaften Ausführungsform nimmt eine Kamera ein Bild eines Anwenders auf, der ein Head-mounted Device (HMD) trägt, das einen Abschnitt des Gesichts des Anwenders verdeckt. Eine 3D-Stellung, die eine Orientierung und einen Ort des Gesichts des Anwenders in einem Kamera-Koordinatensystem angibt, wird bestimmt. Eine Repräsentation des verdeckten Abschnitts des Gesichts des Anwenders wird basierend auf einem 3D-Modell des Gesichts des Anwenders bestimmt. Die Repräsentation ersetzt einen Abschnitt des HMD in dem Bild basierend auf der 3D-Stellung des Gesichts des Anwenders in dem Kamera-Koordinatensystem. Bilder mit gemischter Realität können durch Kombinieren von Bildern der virtuellen Realität, nicht verdeckten Abschnitten des Gesichts des Anwenders und Repräsentationen eines verdeckten Abschnitts des Gesichts des Anwenders erzeugt werden.According to an exemplary embodiment, a camera captures an image of a user wearing a head-mounted device (HMD) that obscures a portion of the user's face. A 3D position indicating an orientation and a location of the user's face in a camera coordinate system is determined. A representation of the hidden portion of the user's face is determined based on a 3D model of the user's face. The representation replaces a portion of the HMD in the image based on the 3D position of the user's face in the camera coordinate system. Mixed reality images can be created by combining virtual reality images, unobscured portions of the user's face, and representations of a hidden portion of the user's face.
Es wird darauf hingewiesen, dass nicht alle Aktivitäten oder Elemente, die vorstehend in der allgemeinen Beschreibung beschrieben sind, erforderlich sind, und dass ein Abschnitt einer spezifischen Aktivität oder Einrichtung nicht erforderlich sein kann und dass eine oder mehrere weitere Aktivitäten ausgeführt werden können oder Elemente enthalten sein können, zusätzlich zu den beschriebenen. Des Weiteren ist die Reihenfolge, in denen Aktivitäten aufgezählt sind, nicht notwendigerweise die Reihenfolge, in der sie ausgeführt werden. Außerdem sind die Konzepte mit Bezug auf spezifische Ausführungsformen beschrieben worden. Ein normaler Fachmann erkennt jedoch, dass verschiedene Modifikationen und Änderungen vorgenommen werden können, ohne vom Schutzbereich der vorliegenden Offenbarung, wie er in den nachstehenden Ansprüchen dargelegt ist, abzuweichen. Dementsprechend sind die Spezifikation und die Figuren in einem erläuternden anstatt in einem einschränkenden Sinn zu betrachten, und alle solche Modifikationen sollen in dem Schutzbereich der vorliegenden Offenbarung enthalten sein.It should be understood that not all of the activities or elements described above in the general description are required, and that a portion of a specific activity or device may not be required and that one or more other activities may be performed or include elements can be, in addition to those described. Furthermore, the order in which activities are enumerated is not necessarily the order in which they are performed. In addition, the concepts have been described with reference to specific embodiments. However, one of ordinary skill in the art appreciates that various modifications and changes can be made without departing from the scope of the present disclosure as set forth in the claims below. Accordingly, the specification and the figures are to be considered in an illustrative rather than a limiting sense, and all such modifications are intended to be included within the scope of the present disclosure.
Nutzen, andere Vorteile und Lösungen für Probleme sind vorstehend mit Bezug auf spezifische Ausführungsformen beschrieben worden. Der Nutzen, die Vorteile, Lösungen für Probleme und irgendwelche Merkmale, die bewirken können, dass irgendein Nutzen, Vorteil oder Lösung auftritt oder deutlicher werden, sollen nicht als kritisches, erforderliches oder wesentliches Merkmal irgendeines der oder aller Ansprüche gedeutet werden. Außerdem sind die speziellen vorstehend offenbarten Ausführungsformen nur erläuternd, da der offenbarte Gegenstand modifiziert und auf unterschiedliche jedoch äquivalente Weisen praktiziert werden kann, die für Fachleute, die den Nutzen aus den hier gezeigten Lehren haben, offensichtlich ist. Keine anderen als Einschränkungen für die Einzelheiten der Konstruktion oder des Designs, die hier gezeigt sind, als die in den nachstehenden Ansprüchen beschriebenen sind beabsichtigt. Es ist deshalb einleuchtend, dass die vorstehend offenbarten speziellen Ausführungsformen verändert oder modifiziert werden können und alle solche Variationen als innerhalb des Schutzbereichs des offenbarten Gegenstands betrachtet werden. Dementsprechend ist hier der gesuchte Schutz wie in den nachstehenden Ansprüchen dargelegt.Benefits, other advantages, and solutions to problems have been described above with respect to specific embodiments. The benefits, advantages, solutions to problems, and any features that may cause any benefit, benefit, or solution to occur or to become more apparent should not be construed as a critical, required, or essential feature of any or all claims. In addition, the particular embodiments disclosed above are illustrative only, as the disclosed subject matter may be modified and practiced in various, but equivalent, manners which will be apparent to those skilled in the art having the benefit of the teachings herein. Nothing more than limitations on the details of construction or design shown herein are intended to be those described in the claims below. It is therefore to be understood that the specific embodiments disclosed above may be altered or modified and all such variations are considered to be within the scope of the disclosed subject matter. Accordingly, the protection sought is as set forth in the following claims.