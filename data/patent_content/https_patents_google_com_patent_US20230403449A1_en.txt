US20230403449A1 - Closed loop photography - Google Patents
Closed loop photography Download PDFInfo
- Publication number
- US20230403449A1 US20230403449A1 US18/334,585 US202318334585A US2023403449A1 US 20230403449 A1 US20230403449 A1 US 20230403449A1 US 202318334585 A US202318334585 A US 202318334585A US 2023403449 A1 US2023403449 A1 US 2023403449A1
- Authority
- US
- United States
- Prior art keywords
- shutter
- scene
- image capture
- electromagnetic radiation
- capture device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 30
- 238000001514 detection method Methods 0.000 claims description 56
- 238000012545 processing Methods 0.000 claims description 49
- 230000005670 electromagnetic radiation Effects 0.000 claims description 32
- 230000003287 optical effect Effects 0.000 claims description 27
- 238000013527 convolutional neural network Methods 0.000 claims description 19
- 230000004044 response Effects 0.000 claims description 17
- 238000012706 support-vector machine Methods 0.000 claims description 16
- 230000002123 temporal effect Effects 0.000 claims description 16
- 239000004984 smart glass Substances 0.000 claims description 15
- 238000004590 computer program Methods 0.000 claims description 8
- 230000001902 propagating effect Effects 0.000 claims 5
- 230000005236 sound signal Effects 0.000 claims 2
- 230000033001 locomotion Effects 0.000 description 6
- 238000010586 diagram Methods 0.000 description 5
- 230000008569 process Effects 0.000 description 4
- 230000005855 radiation Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 239000011521 glass Substances 0.000 description 3
- 238000004891 communication Methods 0.000 description 2
- 238000004519 manufacturing process Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000012549 training Methods 0.000 description 2
- 208000003251 Pruritus Diseases 0.000 description 1
- 230000009471 action Effects 0.000 description 1
- 230000001154 acute effect Effects 0.000 description 1
- 230000000712 assembly Effects 0.000 description 1
- 238000000429 assembly Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000004397 blinking Effects 0.000 description 1
- 230000000881 depressing effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 230000008570 general process Effects 0.000 description 1
- 238000005286 illumination Methods 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 230000011514 reflex Effects 0.000 description 1
- 229910052709 silver Inorganic materials 0.000 description 1
- 239000004332 silver Substances 0.000 description 1
- -1 silver halide Chemical class 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000006467 substitution reaction Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/50—Constructional details
- H04N23/52—Elements optimising image sensor operation, e.g. for electromagnetic interference [EMI] protection or temperature control by heat transfer or cooling elements
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B27/0172—Head mounted characterised by optical features
-
- G—PHYSICS
- G03—PHOTOGRAPHY; CINEMATOGRAPHY; ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ELECTROGRAPHY; HOLOGRAPHY
- G03B—APPARATUS OR ARRANGEMENTS FOR TAKING PHOTOGRAPHS OR FOR PROJECTING OR VIEWING THEM; APPARATUS OR ARRANGEMENTS EMPLOYING ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ACCESSORIES THEREFOR
- G03B9/00—Exposure-making shutters; Diaphragms
- G03B9/58—Means for varying duration of "open" period of shutter
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/0002—Inspection of images, e.g. flaw detection
- G06T7/0012—Biomedical image inspection
- G06T7/0014—Biomedical image inspection using an image reference approach
- G06T7/0016—Biomedical image inspection using an image reference approach involving temporal comparison
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/61—Control of cameras or camera modules based on recognised objects
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/617—Upgrading or updating of programs or applications for camera control
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/665—Control of cameras or camera modules involving internal camera communication with the image sensor, e.g. synchronising or multiplexing SSIS control signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6812—Motion detection based on additional sensors, e.g. acceleration sensors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/70—Circuitry for compensating brightness variation in the scene
- H04N23/71—Circuitry for evaluating the brightness variation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/80—Camera processing pipelines; Components thereof
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B2027/0192—Supplementary details
- G02B2027/0196—Supplementary details having transparent supporting structure for display mounting, e.g. to a window or a windshield
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
Definitions
- This description relates in general to image capture and determining whether a scene in an image to be captured is being occluded by an object.
- Some cameras are used by people to take photographs of scenes.
- a person looks at a scene through a viewfinder, electronic or otherwise, and depresses a button which triggers the opening of a shutter for a set amount of time based on desired exposure level, focus setting, and other factors related to the camera itself.
- an occlusion detection device that (i) detects a moving finger, (ii) determines whether the finger is about to depress the shutter trigger, (iii) if such a determination is made, determines whether the finger will occlude the image about to be captured, and (iv) if such a determination is made, issue an alert to the user.
- the detection of the moving finger may be made by a motion detector that reflects electromagnetic radiation off the finger.
- the occlusion detection device then inputs the reflected radiation into a support vector machine (SVM); the SVM outputs an indicator of whether the finger is about to touch the shutter trigger. If the indicator indicates that the finger is about to touch the shutter trigger, the occlusion detection device then causes the camera to take 2-4 images of the finger and input those images into a convolutional neural network (CNN).
- the CNN then outputs an occlusion score based on the input images of the finger, which if greater than a threshold causes the occlusion detection device to issue an alert to the user before the image is captured. In this way, finger-occluded images of scenes are avoided.
- an apparatus may include an image capture device configured to capture images of a scene.
- the image capture device may include a light detector and a shutter trigger configured to cause a shutter to open to allow light reflected from the scene toward the image capture device to be incident on the light detector.
- the apparatus may also include an occlusion detection device configured to determine whether an object will occlude the scene while the shutter is open.
- the occlusion detection device may include a proximity detection device configured to determine whether the object is moving toward the shutter trigger; and a notification device configured to, in response to the proximity detection device determining that the object is moving toward the shutter trigger, transmit a notification that the object is occluding the scene.
- a method may include receiving electromagnetic radiation reflected by an object. The method may also include determining whether the object is occluding a scene based on the received electromagnetic radiation reflected by the object. The method may further include, in response to the proximity detection device determining that the object is occluding the scene, transmit a notification that the object is occluding the scene.
- a computer program product may comprise a non-transitory storage medium.
- the computer program product may include code that, when executed by processing circuitry, causes the processing circuitry to perform a method.
- the method may include receiving electromagnetic radiation reflected by an object.
- the method may also include determining whether the object is occluding a scene based on the received electromagnetic radiation reflected by the object.
- the method may further include, in response to the proximity detection device determining that the object is occluding the scene, transmit a notification that the object is occluding the scene.
- FIG. 1 A is a diagram that illustrates an example system, in accordance with implementations described herein.
- FIG. 1 B is a front view
- FIG. 1 C is a rear view
- FIG. 1 D is a perspective view, of the example head mounted wearable device shown in FIG. 1 A , in accordance with implementations described herein.
- FIG. 2 is a diagram that illustrates a side view of a camera mounted on a smartglasses frame such that the camera has a shutter trigger with an IR proximity sensor embedded in the shutter trigger.
- FIG. 3 is a diagram that illustrates a side view of a camera mounted on a smartglasses frame such that the camera has a shutter trigger with an IR proximity sensor next to the shutter trigger.
- FIG. 4 is a flow chart that illustrates a process flow for occlusion detection.
- FIG. 5 is a diagram that illustrates an electronic environment for performing occlusion detection for a camera.
- FIG. 6 is a flow chart that illustrates a method of performing occlusion detection for a camera.
- This disclosure relates to addressing a technical problem of occlusion of a scene to be captured with a camera (or other image capture device) by a finger used to press a shutter trigger of the camera. It is commonplace to have videos or photos affected by an unwanted occlusion by the finger used to press the shutter trigger. While this problem exists in any camera form factor (e.g., single-lens reflex, cell phone, tablet, watch), the problem is especially acute when the camera is mounted on a smartglasses frame or other head mounted wearable devices (e.g., goggles, a headset, etc.) because there is no way to tell that such an occlusion took place until after the picture has been taken.
- a camera form factor e.g., single-lens reflex, cell phone, tablet, watch
- head mounted wearable devices e.g., goggles, a headset, etc.
- a conventional solution to the above-described technical problem involves disposing the shutter trigger on a portion of the smartglasses frame opposite to the portion on which the camera is disposed. In this way, there is far less chance that the finger (or generally, an object) used to depress or activate the shutter trigger may occlude the scene as viewed by the camera.
- This conventional solution is not as desirable as a first impression would suggest.
- the camera-trigger frame coexistence sets a proper expectation of the camera field-of-view to the user.
- a better way to mitigate occlusion due to finger placement near the shutter trigger is to indicate to the user about a potential occlusion before the user activates the shutter trigger (e.g., depresses a button that opens and closes the shutter of the camera).
- a technical solution to the above-described technical problem includes the use of “soft” indicators such as haptics, audio, heads-up display, and other modalities to warn the user of a potential occlusion before the user activates the shutter trigger.
- a technical advantage of this technical solution is that the user may be guided by a soft indicator to position their finger (or object used to activate the shutter trigger) in such a way as to avoid occluding the scene and accordingly produce a higher-quality photograph.
- this technical solution avoids any parallax effect from seeing different perspectives of the scene.
- the above-described technical solution is denoted “closed-loop photography,” and involves hardware and software that is not normally included in image capture devices.
- Elements of the closed-loop photography solution an occlusion detection device configured to determine whether an object (e.g., finger) will occlude the scene while the shutter is open.
- the occlusion detection device includes a proximity detection device configured to determine whether the object is moving toward the shutter trigger, and a notification device configured to, in response to the proximity detection device determining that the object is moving toward the shutter trigger, transmit a notification that the object is occluding the scene.
- FIG. 1 A illustrates a user wearing an example head mounted wearable device 100 .
- the example head mounted wearable device 100 is in the form of example smartglasses including display capability and computing/processing capability, for purposes of discussion and illustration. The principles to be described herein may be applied to other types of eyewear, both with and without display capability and/or computing/processing capability.
- FIG. 1 B is a front view
- FIG. 1 C is a rear view
- FIG. 1 D is a perspective view, of the example head mounted wearable device 100 shown in FIG. 1 A .
- the example head mounted wearable device 100 may take the form of a pair of smartglasses, or augmented reality glasses.
- the example head mounted wearable device 100 includes a frame 102 .
- the frame 102 includes a front frame portion defined by rim portions 103 surrounding respective optical portions in the form of lenses 107 , with a bridge portion 109 connecting the rim portions 103 .
- Arm portions 105 are coupled, for example, pivotably or rotatably coupled, to the front frame by hinge portions 110 at the respective rim portion 103 .
- the lenses 107 may be corrective/prescription lenses.
- the lenses 107 may be an optical material including glass and/or plastic portions that do not necessarily incorporate corrective/prescription parameters.
- a display device 104 may be coupled in a portion of the frame 102 .
- the display device 104 is coupled in the arm portion 105 of the frame 102 .
- an eye box 140 extends toward the lens(es) 107 , for output of content at an output coupler 144 at which content output by the display device 104 may be visible to the user.
- the output coupler 144 may be substantially coincident with the lens(es) 107 .
- the head mounted wearable device 100 can also include an audio output device 106 (such as, for example, one or more speakers), an illumination device 108 , a sensing system 111 , a control system 112 , at least one processor 114 , and an image capture device 116 , or camera 116 .
- the camera (or image capture device) 116 may capture images via a shutter trigger, or button; the shutter trigger is configured to cause a shutter to open to allow light reflected or scattered from a scene toward the image capture device 116 to be incident on a light detector (e.g., a charged-coupled device (CCD) array, a photomultiplier, silver halide photographic film, and the like).
- a light detector e.g., a charged-coupled device (CCD) array, a photomultiplier, silver halide photographic film, and the like.
- the display device 104 may include a see-through near-eye display.
- the display device 104 may be configured to project light from a display source onto a portion of teleprompter glass functioning as a beamsplitter seated at an angle (e.g., 30-45 degrees).
- the beamsplitter may allow for reflection and transmission values that allow the light from the display source to be partially reflected while the remaining light is transmitted through.
- Such an optic design may allow a user to see both physical items in the world, for example, through the lenses 107 , next to content (for example, digital images, user interface elements, virtual content, and the like) generated by the display device 104 .
- waveguide optics may be used to depict content on the display device 104 .
- the head mounted wearable device 100 may include a gaze tracking device 120 including, for example, one or more sensors 125 , to detect and track eye gaze direction and movement. Data captured by the sensor(s) 125 may be processed to detect and track gaze direction and movement as a user input.
- the sensing system 111 may include various sensing devices and the control system 112 may include various control system devices including, for example, one or more processors 114 operably coupled to the components of the control system 112 .
- the control system 112 may include a communication module providing for communication and exchange of information between the head-mounted wearable device 100 and other external devices.
- the camera 116 has a shutter button in its vicinity, e.g., next to the camera 116 .
- the one or more processors 114 are configured to determine whether (i) a finger moving near the shutter button is about to press the shutter button (or, alternatively, touch another portion of the frame 102 ) and (ii) if the one or more processors 114 determines that the finger is about the press the shutter button, whether the finger is going to obscure the resulting image.
- there needs to be some motion detection capability to detect the motion of the finger This is illustrated in FIGS. 2 and 3 .
- FIG. 2 illustrates a side view 200 of a camera 215 mounted on a smartglasses frame 210 such that the camera 215 has a shutter trigger 220 with a proximity detection device 225 that includes an infrared (IR) proximity sensor.
- the proximity detection device 225 is embedded in the shutter trigger 220 , which takes the form of a mechanical button.
- the IR proximity sensor may operate in a wavelength band centered at 850 nm.
- the IR proximity sensor includes a light-emitting diode (LED)-photodiode (PD) pair.
- LED light-emitting diode
- PD photodiode
- the IR proximity sensor may be placed in a vicinity of the shutter trigger as shown in FIG. 3 .
- FIG. 3 illustrates a side view 300 of a camera 315 mounted on a smartglasses frame 310 such that the camera 315 is adjacent to a shutter trigger 320 with a proximity detection device 325 .
- the proximity detection device 325 may include an IR proximity sensor.
- the IR proximity sensor may use wavelength bands other than infrared, e.g., millimeter wave (mmWave) or an optical band.
- mmWave millimeter wave
- the IR proximity sensor is configured to detect a “ready to capture” moment just as the potentially occluding object (e.g., finger, pointer) is about to engage the shutter trigger. Nevertheless, it may not be absolutely clear to the IR proximity sensor that an object in proximity to the shutter trigger is necessarily going to engage the shutter trigger (e.g., 220 , 320 ). For example, when a camera (e.g., 215 , 315 ) is disposed on a smartglasses frame, the IR proximity sensor may or may not have a capability to distinguish between a movement to engage the shutter trigger from that, for example, to scratch an itch on the user's face in a vicinity of the smartglasses frame.
- a camera e.g., 215 , 315
- the proximity detection device may also include processing circuitry configured to determine whether the object is moving to cause the shutter to be opened based on a temporal pattern of the electromagnetic radiation reflected from the object.
- the IR proximity sensor transmits IR radiation that propagates away from the camera (e.g., 215 , 315 ) and receives the IR electromagnetic radiation that is reflected by the object in the vicinity of the shutter trigger (e.g., 220 , 320 ).
- the reflected radiation is processed as a reflected signal with a time-varying amplitude (e.g., brightness, intensity, irradiance) because the object is moving near the shutter trigger.
- the proximity detection device is configured to determine whether the object is about to engage the shutter trigger based on a temporal pattern (e.g., signature) of the amplitude.
- FIG. 4 illustrates a general process flow for occlusion detection that involves a proximity detection device as described with respect to FIGS. 2 and 3 .
- the user moves their index finger toward the shutter button for possible image capture.
- the proximity detection device uses processing circuitry configured to run an optical touch classifier that determines whether an object is about to engage the shutter trigger based on the temporal pattern of amplitude of a signal (i.e., IR radiation) reflected from the object. That is, the processing circuitry maps the temporal pattern of amplitude of the reflected signal to a binary value: whether or not the object is about to engage the shutter trigger.
- a signal i.e., IR radiation
- the optical touch classifier mapping between the temporal pattern of amplitude of the reflected signal to the determination of whether or not the object is about to engage the shutter trigger may be defined by a machine learning algorithm. Nevertheless, because the optical touch classifier may be of minimal complexity for a real-time determination, in some implementations the optical touch classifier includes a linear support vector machine (SVM).
- SVM linear support vector machine
- the optical touch classifier uses a (proximity, signal amplitude) ordered pair measured by the IR proximity sensor.
- the proximity coordinate may be represented by relatively large values when e.g., a finger is directly occluding within a distance of less than 1 cm from the shutter trigger); the received reflected amplitude may also be high as reflected signal amplitude falls off in squared fashion over distance and the object is near to the IR proximity sensor.
- the optical touch classifier as shown in FIG. 4 uses a simple linear SVM trained to classify the valid standby data points (i.e., high proximity, high amplitude) from other false-positive sources. To accomplish this, the linear SVM outputs a classification score as follows.
- classification score ⁇ 1 *proximity+ ⁇ 2 *amplitude+ ⁇ 3 .
- the object may be classified as being about to engage the shutter trigger if the classification score is sufficiently large (i.e, larger than a threshold score).
- the optical touch classifier takes no further action other than continuing classification of reflected signals.
- the classification score is sufficiently large, e.g., greater than a threshold score, then the occlusion detection device begins the standby mode to determine whether the object is occluding the scene sufficiently to warrant a notification.
- the standby mode includes three stages: 1) taking few-frame shots at 415 , 2) inputting the few-frame shots into an occlusion scoring model at 420 to produce an occlusion score, and 3) modulating a playback of a notification at 430 if the occlusion score is sufficiently large at 425 .
- These stages may be performed by additional processing circuitry located in the camera (image capture device) or within a network connected to the camera.
- the few-frame shots at 415 are a small number of sequential frames that are caused to be taken by the additional processing circuitry.
- the number of frames taken may be four or five, spaced apart in time by, e.g., 2-4 microseconds or a rate of 1-2 frames per microsecond.
- a benefit of using the small number of sequential frames rather than a single frame is that the sequential frames provide a more consistent occlusion score that has less chance of false positives. Accordingly, an optical flow measure may be computed as a measure of the consistency of the occlusion.
- the frames may be input into the occlusion scoring model at 420 .
- the optical flow measure is also input into the occlusion scoring model.
- the occlusion scoring model includes a convolutional neural network (CNN) that takes the frames and/or optical flow measure and outputs an occlusion score.
- the CNN has between three and five hidden layers, with each layer having, e.g., 100 ⁇ 100 coefficients.
- the occlusion strength (e.g., score) is a number between zero and one; in such an implementation, output nodes of the CNN may have softmax layers that restrict the score to values between zero (no occlusion) and one (totally occluded).
- the CNN is trained offline with a calibrated and controlled training dataset that includes of images with an object (e.g., finger, hand) occluding the field-of-view of the camera.
- the additional processing circuitry In the final stage, at 430 the additional processing circuitry generates a notification over a specified modality (e.g., audio, heads-up display, tactile) when the occlusion score is sufficiently large (e.g., greater than a threshold) at 425 .
- the occlusion score serves as an indicator of the notification intensity (e.g., audio volume, display brightness or color, vibration amplitude).
- the notifications may continue until the object no longer occludes the scene, or has a sufficiently small occlusion score (e.g., less than a threshold.)
- FIG. 5 is a diagram that illustrates an example electronic environment including processing circuitry 520 for performing closed-loop photography as described with regard to FIG. 4 .
- the processing circuitry 520 includes a network interface 522 , one or more processing units 524 , and non-transitory memory (storage medium) 526 .
- one or more of the components of the processing circuitry 520 can be, or can include processors (e.g., processing units 524 ) configured to process instructions stored in the memory 526 as a computer program product. Examples of such instructions as depicted in FIG. 5 include proximity detection manager 530 , occlusion score manager 540 , and alert manager 550 . Further, as illustrated in FIG. 5 , the memory 526 is configured to store various data, which is described with respect to the respective services and managers that use such data.
- the proximity detection manager 530 is configured to detect a user's finger in the vicinity of a shutter button and determine if the user intends to press the shutter button with the finger.
- the proximity detection manager 530 is configured to use a (proximity, signal amplitude) ordered pair measured by the IR proximity sensor as proximity detection data 532 .
- the proximity detection manager 530 may represent the proximity coordinate by relatively large values when e.g., a finger is directly occluding within a distance of less than 1 cm from the shutter trigger); the received reflected amplitude may also be high as reflected signal amplitude falls off in squared fashion over distance and the object is near to the IR proximity sensor.
- the proximity detection manager 530 is configured to use a simple linear SVM trained to classify the valid standby data points (i.e., high proximity, high amplitude) from other false-positive sources. To accomplish this, the linear SVM outputs a classification score as follows.
- classification score ⁇ 1 *proximity+ ⁇ 2 *amplitude+ ⁇ 3 .
- the coefficients ⁇ 1 , ⁇ 2 , and ⁇ 3 are stored as SVM data 533 .
- the SVM data 533 also includes the intermediate values of the SVM that give rise to the output of the classification score.
- the proximity detection manager 530 is also configured to determine whether to engage the occlusion score manager 540 if the proximity detection manager 530 determines, using the SVM output, whether the finger is about to press the shutter button.
- the occlusion score manager 540 is configured to determine whether the user's finger will occlude an image of a scene as it depresses the shutter button. To this effect, the occlusion score manager 540 is configured to take few-frame shots and input the few-frame shots into an occlusion scoring model to produce an occlusion score, represented by occlusion score data 542 . The decision whether to issue an alert is based on the occlusion score.
- the occlusion score data 542 includes optical flow data 543 and convolutional neural network (CNN) data 544 .
- the optical flow data 543 results from the few-frame shots (e.g., about 2-4 frames taken in quick succession).
- the occlusion score manager 540 computes an optical flow measure as optical flow data as a measure of the consistency of the occlusion.
- the occlusion score manager 540 then inputs the optical flow data 543 into a CNN, whose layer data, hyperparameters, etc. are represented by CNN data 544 .
- the output of the CNN is the occlusion score, represented by the occlusion score data 542 .
- the alert manager 550 is configured to issue an alert to the user if the occlusion score is greater than a threshold.
- the alert is visual, e.g., a flashing or blinking light, a textual message on the display, etc.
- the alert includes audio, e.g., a beep or a buzzer.
- the alert includes tactile responses such as a vibration on the frame.
- the components (e.g., modules, processing units 524 ) of processing circuitry 520 can be configured to operate based on one or more platforms (e.g., one or more similar or different platforms) that can include one or more types of hardware, software, firmware, operating systems, runtime libraries, and/or so forth.
- the components of the processing circuitry 520 can be configured to operate within a cluster of devices (e.g., a server farm). In such an implementation, the functionality and processing of the components of the processing circuitry 520 can be distributed to several devices of the cluster of devices.
- the components of the processing circuitry 520 can be, or can include, any type of hardware and/or software configured to process private data from a wearable device in a split-compute architecture.
- one or more portions of the components shown in the components of the processing circuitry 520 in FIG. 5 can be, or can include, a hardware-based module (e.g., a digital signal processor (DSP), a field programmable gate array (FPGA), a memory), a firmware module, and/or a software-based module (e.g., a module of computer code, a set of computer-readable instructions that can be executed at a computer).
- a hardware-based module e.g., a digital signal processor (DSP), a field programmable gate array (FPGA), a memory
- firmware module e.g., a firmware module
- a software-based module e.g., a module of computer code, a set of computer-readable instructions that can be executed at a computer.
- one or more portions of the components of the processing circuitry 520 can be, or can include, a software module configured for execution by at least one processor (not shown).
- the functionality of the components can be included in different modules and/or different components than those shown in FIG. 5 , including combining functionality illustrated as two components into a single component.
- the network interface 522 includes, for example, wireless adaptors, and the like, for converting electronic and/or optical signals received from the network to electronic form for use by the processing circuitry 520 .
- the set of processing units 524 include one or more processing chips and/or assemblies.
- the memory 526 includes both volatile memory (e.g., RAM) and non-volatile memory, such as one or more ROMs, disk drives, solid state drives, and the like.
- the set of processing units 524 and the memory 526 together form processing circuitry, which is configured and arranged to carry out various methods and functions as described herein.
- the components of the processing circuitry 520 can be configured to operate within, for example, a data center (e.g., a cloud computing environment), a computer system, one or more server/host devices, and/or so forth.
- the components of the processing circuitry 520 can be configured to operate within a network.
- the components of the processing circuitry 520 can be configured to function within various types of network environments that can include one or more devices and/or one or more server devices.
- the network can be, or can include, a local area network (LAN), a wide area network (WAN), and/or so forth.
- the network can be, or can include, a wireless network and/or wireless network implemented using, for example, gateway devices, bridges, switches, and/or so forth.
- the network can include one or more segments and/or can have portions based on various protocols such as Internet Protocol (IP) and/or a proprietary protocol.
- IP Internet Protocol
- the network can include at least a portion of the Internet.
- one or more of the components of the processing circuitry 520 can be, or can include, processors configured to process instructions stored in a memory.
- processors configured to process instructions stored in a memory.
- proximity detection manager 530 and/or a portion thereof
- occlusion score manager 540 and/or a portion thereof
- alert manager 550 are examples of such instructions.
- the memory 526 can be any type of memory such as a random-access memory, a disk drive memory, flash memory, and/or so forth. In some implementations, the memory 526 can be implemented as more than one memory component (e.g., more than one RAM component or disk drive memory) associated with the components of the processing circuitry 520 . In some implementations, the memory 526 can be a database memory. In some implementations, the memory 526 can be, or can include, a non-local memory. For example, the memory 526 can be, or can include, a memory shared by multiple devices (not shown). In some implementations, the memory 526 can be associated with a server device (not shown) within a network and configured to serve the components of the processing circuitry 520 . As illustrated in FIG. 5 , the memory 526 is configured to store various data, including proximity detection data 532 and occlusion score data 542 .
- FIG. 6 is a flow chart that illustrates a method 600 of performing occlusion detection for a camera.
- the method 600 may be performed using the processing circuitry 520 of FIG. 5 .
- the proximity detection manager 530 receives electromagnetic radiation (e.g., light) reflected by an object (e.g., a user's finger).
- electromagnetic radiation e.g., light
- the proximity detection manager 530 and the occlusion score manager 540 determine whether the object is occluding a scene based on the electromagnetic radiation reflected by the object. That is, the proximity detection manager 530 determines, based on the received electromagnetic radiation, that the object is going to depress a shutter trigger. The occlusion score manager then determines, from images of the object, whether the finger will occlude an image of scene taken upon the depressing of the shutter trigger.
- the alert manager 550 in response to a proximity detection device determining that the object is occluding the scene, transmits a notification that the object is occluding the scene.
- spatially relative terms such as “beneath,” “below,” “lower,” “above,” “upper,” and the like, may be used herein for ease of description to describe one element or feature in relationship to another element(s) or feature(s) as illustrated in the figures. It will be understood that the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if the device in the figures is turned over, elements described as “below” or “beneath” other elements or features would then be oriented “above” the other elements or features. Thus, the term “below” can encompass both an orientation of above and below. The device may be otherwise oriented (rotated 70 degrees or at other orientations) and the spatially relative descriptors used herein may be interpreted accordingly.
- Example embodiments of the concepts are described herein with reference to cross-sectional illustrations that are schematic illustrations of idealized embodiments (and intermediate structures) of example embodiments. As such, variations from the shapes of the illustrations as a result, for example, of manufacturing techniques and/or tolerances, are to be expected. Thus, example embodiments of the described concepts should not be construed as limited to the particular shapes of regions illustrated herein but are to include deviations in shapes that result, for example, from manufacturing. Accordingly, the regions illustrated in the figures are schematic in nature and their shapes are not intended to illustrate the actual shape of a region of a device and are not intended to limit the scope of example embodiments.
Abstract
Improved techniques of generating photographs using a camera or other image capture device includes the use of “soft” indicators such as haptics, audio, heads-up display, and other modalities to warn the user of a potential occlusion before a user activates a shutter trigger.
Description
- This application claims the benefit of U.S. Provisional Application No. 63/366,375, filed Jun. 14, 2022, the disclosure of which is enclosed herewith by reference in its entirety.
- This description relates in general to image capture and determining whether a scene in an image to be captured is being occluded by an object.
- Some cameras are used by people to take photographs of scenes. To effect the taking of a photograph, a person looks at a scene through a viewfinder, electronic or otherwise, and depresses a button which triggers the opening of a shutter for a set amount of time based on desired exposure level, focus setting, and other factors related to the camera itself.
- In some cameras, occlusion of an image of a scene by a finger that depresses a shutter trigger is a common occurrence. Moreover, some form factors such as smartglasses devices do not permit previewing an image before capture. In such form factors, it is important to find a way to warn a user when the finger is about to occlude an image. In this description, an occlusion detection device is presented that (i) detects a moving finger, (ii) determines whether the finger is about to depress the shutter trigger, (iii) if such a determination is made, determines whether the finger will occlude the image about to be captured, and (iv) if such a determination is made, issue an alert to the user. The detection of the moving finger may be made by a motion detector that reflects electromagnetic radiation off the finger. The occlusion detection device then inputs the reflected radiation into a support vector machine (SVM); the SVM outputs an indicator of whether the finger is about to touch the shutter trigger. If the indicator indicates that the finger is about to touch the shutter trigger, the occlusion detection device then causes the camera to take 2-4 images of the finger and input those images into a convolutional neural network (CNN). The CNN then outputs an occlusion score based on the input images of the finger, which if greater than a threshold causes the occlusion detection device to issue an alert to the user before the image is captured. In this way, finger-occluded images of scenes are avoided.
- In one general aspect, an apparatus may include an image capture device configured to capture images of a scene. The image capture device may include a light detector and a shutter trigger configured to cause a shutter to open to allow light reflected from the scene toward the image capture device to be incident on the light detector. The apparatus may also include an occlusion detection device configured to determine whether an object will occlude the scene while the shutter is open. The occlusion detection device may include a proximity detection device configured to determine whether the object is moving toward the shutter trigger; and a notification device configured to, in response to the proximity detection device determining that the object is moving toward the shutter trigger, transmit a notification that the object is occluding the scene.
- In another general aspect, a method may include receiving electromagnetic radiation reflected by an object. The method may also include determining whether the object is occluding a scene based on the received electromagnetic radiation reflected by the object. The method may further include, in response to the proximity detection device determining that the object is occluding the scene, transmit a notification that the object is occluding the scene.
- In another general aspect, a computer program product may comprise a non-transitory storage medium. The computer program product may include code that, when executed by processing circuitry, causes the processing circuitry to perform a method. The method may include receiving electromagnetic radiation reflected by an object. The method may also include determining whether the object is occluding a scene based on the received electromagnetic radiation reflected by the object. The method may further include, in response to the proximity detection device determining that the object is occluding the scene, transmit a notification that the object is occluding the scene.
- The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
-
FIG. 1A is a diagram that illustrates an example system, in accordance with implementations described herein. -
FIG. 1B is a front view,FIG. 1C is a rear view, andFIG. 1D is a perspective view, of the example head mounted wearable device shown inFIG. 1A , in accordance with implementations described herein. -
FIG. 2 is a diagram that illustrates a side view of a camera mounted on a smartglasses frame such that the camera has a shutter trigger with an IR proximity sensor embedded in the shutter trigger. -
FIG. 3 is a diagram that illustrates a side view of a camera mounted on a smartglasses frame such that the camera has a shutter trigger with an IR proximity sensor next to the shutter trigger. -
FIG. 4 is a flow chart that illustrates a process flow for occlusion detection. -
FIG. 5 is a diagram that illustrates an electronic environment for performing occlusion detection for a camera. -
FIG. 6 is a flow chart that illustrates a method of performing occlusion detection for a camera. - This disclosure relates to addressing a technical problem of occlusion of a scene to be captured with a camera (or other image capture device) by a finger used to press a shutter trigger of the camera. It is commonplace to have videos or photos affected by an unwanted occlusion by the finger used to press the shutter trigger. While this problem exists in any camera form factor (e.g., single-lens reflex, cell phone, tablet, watch), the problem is especially acute when the camera is mounted on a smartglasses frame or other head mounted wearable devices (e.g., goggles, a headset, etc.) because there is no way to tell that such an occlusion took place until after the picture has been taken.
- A conventional solution to the above-described technical problem involves disposing the shutter trigger on a portion of the smartglasses frame opposite to the portion on which the camera is disposed. In this way, there is far less chance that the finger (or generally, an object) used to depress or activate the shutter trigger may occlude the scene as viewed by the camera. This conventional solution, however, is not as desirable as a first impression would suggest. For example, the camera-trigger frame coexistence sets a proper expectation of the camera field-of-view to the user.
- A better way to mitigate occlusion due to finger placement near the shutter trigger is to indicate to the user about a potential occlusion before the user activates the shutter trigger (e.g., depresses a button that opens and closes the shutter of the camera). Accordingly, a technical solution to the above-described technical problem includes the use of “soft” indicators such as haptics, audio, heads-up display, and other modalities to warn the user of a potential occlusion before the user activates the shutter trigger.
- A technical advantage of this technical solution is that the user may be guided by a soft indicator to position their finger (or object used to activate the shutter trigger) in such a way as to avoid occluding the scene and accordingly produce a higher-quality photograph. In addition, this technical solution avoids any parallax effect from seeing different perspectives of the scene.
- The above-described technical solution is denoted “closed-loop photography,” and involves hardware and software that is not normally included in image capture devices. Elements of the closed-loop photography solution an occlusion detection device configured to determine whether an object (e.g., finger) will occlude the scene while the shutter is open. The occlusion detection device includes a proximity detection device configured to determine whether the object is moving toward the shutter trigger, and a notification device configured to, in response to the proximity detection device determining that the object is moving toward the shutter trigger, transmit a notification that the object is occluding the scene.
-
FIG. 1A illustrates a user wearing an example head mountedwearable device 100. In this example, the example head mountedwearable device 100 is in the form of example smartglasses including display capability and computing/processing capability, for purposes of discussion and illustration. The principles to be described herein may be applied to other types of eyewear, both with and without display capability and/or computing/processing capability.FIG. 1B is a front view,FIG. 1C is a rear view, andFIG. 1D is a perspective view, of the example head mountedwearable device 100 shown inFIG. 1A . As noted above, in some examples, the example head mountedwearable device 100 may take the form of a pair of smartglasses, or augmented reality glasses. - As shown in
FIGS. 1B-1D , the example head mountedwearable device 100 includes aframe 102. Theframe 102 includes a front frame portion defined byrim portions 103 surrounding respective optical portions in the form oflenses 107, with abridge portion 109 connecting therim portions 103.Arm portions 105 are coupled, for example, pivotably or rotatably coupled, to the front frame byhinge portions 110 at therespective rim portion 103. In some examples, thelenses 107 may be corrective/prescription lenses. In some examples, thelenses 107 may be an optical material including glass and/or plastic portions that do not necessarily incorporate corrective/prescription parameters. - A
display device 104 may be coupled in a portion of theframe 102. In the example shown inFIGS. 1B and 1C , thedisplay device 104 is coupled in thearm portion 105 of theframe 102. With thedisplay device 104 coupled in thearm portion 105, an eye box 140 extends toward the lens(es) 107, for output of content at anoutput coupler 144 at which content output by thedisplay device 104 may be visible to the user. In some examples, theoutput coupler 144 may be substantially coincident with the lens(es) 107. In some examples, the head mountedwearable device 100 can also include an audio output device 106 (such as, for example, one or more speakers), anillumination device 108, asensing system 111, acontrol system 112, at least oneprocessor 114, and animage capture device 116, orcamera 116. The camera (or image capture device) 116 may capture images via a shutter trigger, or button; the shutter trigger is configured to cause a shutter to open to allow light reflected or scattered from a scene toward theimage capture device 116 to be incident on a light detector (e.g., a charged-coupled device (CCD) array, a photomultiplier, silver halide photographic film, and the like). - In some examples, the
display device 104 may include a see-through near-eye display. For example, thedisplay device 104 may be configured to project light from a display source onto a portion of teleprompter glass functioning as a beamsplitter seated at an angle (e.g., 30-45 degrees). The beamsplitter may allow for reflection and transmission values that allow the light from the display source to be partially reflected while the remaining light is transmitted through. Such an optic design may allow a user to see both physical items in the world, for example, through thelenses 107, next to content (for example, digital images, user interface elements, virtual content, and the like) generated by thedisplay device 104. In some implementations, waveguide optics may be used to depict content on thedisplay device 104. - In some examples, the head mounted
wearable device 100 may include agaze tracking device 120 including, for example, one ormore sensors 125, to detect and track eye gaze direction and movement. Data captured by the sensor(s) 125 may be processed to detect and track gaze direction and movement as a user input. In some examples, thesensing system 111 may include various sensing devices and thecontrol system 112 may include various control system devices including, for example, one ormore processors 114 operably coupled to the components of thecontrol system 112. In some examples, thecontrol system 112 may include a communication module providing for communication and exchange of information between the head-mountedwearable device 100 and other external devices. - In some implementations, the
camera 116 has a shutter button in its vicinity, e.g., next to thecamera 116. In such implementations, the one ormore processors 114 are configured to determine whether (i) a finger moving near the shutter button is about to press the shutter button (or, alternatively, touch another portion of the frame 102) and (ii) if the one ormore processors 114 determines that the finger is about the press the shutter button, whether the finger is going to obscure the resulting image. In order to determine whether a finger moving near the shutter button is about to press the shutter button, there needs to be some motion detection capability to detect the motion of the finger. This is illustrated inFIGS. 2 and 3 . -
FIG. 2 illustrates aside view 200 of acamera 215 mounted on asmartglasses frame 210 such that thecamera 215 has ashutter trigger 220 with aproximity detection device 225 that includes an infrared (IR) proximity sensor. As shown inFIG. 2 , theproximity detection device 225 is embedded in theshutter trigger 220, which takes the form of a mechanical button. As an example, the IR proximity sensor may operate in a wavelength band centered at 850 nm. In some implementations, the IR proximity sensor includes a light-emitting diode (LED)-photodiode (PD) pair. - The arrangement of the
proximity detection device 225 and theshutter trigger 220 shown inFIG. 2 , while better for standby detection capability, is only an example illustrating the principal of usage and is not meant to be limiting. For example, in some implementations, the IR proximity sensor may be placed in a vicinity of the shutter trigger as shown inFIG. 3 . -
FIG. 3 illustrates aside view 300 of acamera 315 mounted on asmartglasses frame 310 such that thecamera 315 is adjacent to ashutter trigger 320 with aproximity detection device 325. In the case illustrated inFIG. 3 , theproximity detection device 325 may include an IR proximity sensor. In some implementations, the IR proximity sensor may use wavelength bands other than infrared, e.g., millimeter wave (mmWave) or an optical band. - The IR proximity sensor is configured to detect a “ready to capture” moment just as the potentially occluding object (e.g., finger, pointer) is about to engage the shutter trigger. Nevertheless, it may not be absolutely clear to the IR proximity sensor that an object in proximity to the shutter trigger is necessarily going to engage the shutter trigger (e.g., 220, 320). For example, when a camera (e.g., 215, 315) is disposed on a smartglasses frame, the IR proximity sensor may or may not have a capability to distinguish between a movement to engage the shutter trigger from that, for example, to scratch an itch on the user's face in a vicinity of the smartglasses frame.
- Accordingly, the proximity detection device (e.g., 225, 325) may also include processing circuitry configured to determine whether the object is moving to cause the shutter to be opened based on a temporal pattern of the electromagnetic radiation reflected from the object. For example, the IR proximity sensor transmits IR radiation that propagates away from the camera (e.g., 215, 315) and receives the IR electromagnetic radiation that is reflected by the object in the vicinity of the shutter trigger (e.g., 220, 320). The reflected radiation is processed as a reflected signal with a time-varying amplitude (e.g., brightness, intensity, irradiance) because the object is moving near the shutter trigger. Accordingly, the proximity detection device is configured to determine whether the object is about to engage the shutter trigger based on a temporal pattern (e.g., signature) of the amplitude.
-
FIG. 4 illustrates a general process flow for occlusion detection that involves a proximity detection device as described with respect toFIGS. 2 and 3 . As shown inFIG. 4 , at 405 the user moves their index finger toward the shutter button for possible image capture. At 410 the proximity detection device uses processing circuitry configured to run an optical touch classifier that determines whether an object is about to engage the shutter trigger based on the temporal pattern of amplitude of a signal (i.e., IR radiation) reflected from the object. That is, the processing circuitry maps the temporal pattern of amplitude of the reflected signal to a binary value: whether or not the object is about to engage the shutter trigger. - As shown in
FIG. 4 , the optical touch classifier mapping between the temporal pattern of amplitude of the reflected signal to the determination of whether or not the object is about to engage the shutter trigger may be defined by a machine learning algorithm. Nevertheless, because the optical touch classifier may be of minimal complexity for a real-time determination, in some implementations the optical touch classifier includes a linear support vector machine (SVM). - In some implementations, and as shown in
FIG. 4 , at 410 the optical touch classifier uses a (proximity, signal amplitude) ordered pair measured by the IR proximity sensor. In some implementations, during a standby mode the proximity coordinate may be represented by relatively large values when e.g., a finger is directly occluding within a distance of less than 1 cm from the shutter trigger); the received reflected amplitude may also be high as reflected signal amplitude falls off in squared fashion over distance and the object is near to the IR proximity sensor. - The optical touch classifier as shown in
FIG. 4 uses a simple linear SVM trained to classify the valid standby data points (i.e., high proximity, high amplitude) from other false-positive sources. To accomplish this, the linear SVM outputs a classification score as follows. -
classification score=α1*proximity+α2*amplitude+α3. - The coefficients α1, α2, and α3 may be trained offline using optical touch data. For example, these coefficients may be determined through an offline training to take values α1=0.8, α2=3, and α3=2. The object may be classified as being about to engage the shutter trigger if the classification score is sufficiently large (i.e, larger than a threshold score).
- If the classification score indicates that the object is not about to engage the shutter trigger at a moment in time, then the optical touch classifier takes no further action other than continuing classification of reflected signals. In contrast, if the classification score is sufficiently large, e.g., greater than a threshold score, then the occlusion detection device begins the standby mode to determine whether the object is occluding the scene sufficiently to warrant a notification.
- As shown in
FIG. 4 , the standby mode includes three stages: 1) taking few-frame shots at 415, 2) inputting the few-frame shots into an occlusion scoring model at 420 to produce an occlusion score, and 3) modulating a playback of a notification at 430 if the occlusion score is sufficiently large at 425. These stages may be performed by additional processing circuitry located in the camera (image capture device) or within a network connected to the camera. - The few-frame shots at 415 are a small number of sequential frames that are caused to be taken by the additional processing circuitry. In some implementations, the number of frames taken may be four or five, spaced apart in time by, e.g., 2-4 microseconds or a rate of 1-2 frames per microsecond. A benefit of using the small number of sequential frames rather than a single frame is that the sequential frames provide a more consistent occlusion score that has less chance of false positives. Accordingly, an optical flow measure may be computed as a measure of the consistency of the occlusion.
- If the optical flow measure indicates sufficient consistency of occlusion, then the frames may be input into the occlusion scoring model at 420. In some implementations, the optical flow measure is also input into the occlusion scoring model. In some implementations, the occlusion scoring model includes a convolutional neural network (CNN) that takes the frames and/or optical flow measure and outputs an occlusion score. In some implementations, the CNN has between three and five hidden layers, with each layer having, e.g., 100×100 coefficients. In some implementations, the occlusion strength (e.g., score) is a number between zero and one; in such an implementation, output nodes of the CNN may have softmax layers that restrict the score to values between zero (no occlusion) and one (totally occluded). In some implementations, the CNN is trained offline with a calibrated and controlled training dataset that includes of images with an object (e.g., finger, hand) occluding the field-of-view of the camera.
- In the final stage, at 430 the additional processing circuitry generates a notification over a specified modality (e.g., audio, heads-up display, tactile) when the occlusion score is sufficiently large (e.g., greater than a threshold) at 425. In some implementations, the occlusion score serves as an indicator of the notification intensity (e.g., audio volume, display brightness or color, vibration amplitude). The notifications may continue until the object no longer occludes the scene, or has a sufficiently small occlusion score (e.g., less than a threshold.)
-
FIG. 5 is a diagram that illustrates an example electronic environment includingprocessing circuitry 520 for performing closed-loop photography as described with regard toFIG. 4 . Theprocessing circuitry 520 includes anetwork interface 522, one ormore processing units 524, and non-transitory memory (storage medium) 526. - In some implementations, one or more of the components of the
processing circuitry 520 can be, or can include processors (e.g., processing units 524) configured to process instructions stored in thememory 526 as a computer program product. Examples of such instructions as depicted inFIG. 5 includeproximity detection manager 530, occlusion score manager 540, andalert manager 550. Further, as illustrated inFIG. 5 , thememory 526 is configured to store various data, which is described with respect to the respective services and managers that use such data. - The
proximity detection manager 530 is configured to detect a user's finger in the vicinity of a shutter button and determine if the user intends to press the shutter button with the finger. Theproximity detection manager 530 is configured to use a (proximity, signal amplitude) ordered pair measured by the IR proximity sensor asproximity detection data 532. In some implementations, during a standby mode theproximity detection manager 530 may represent the proximity coordinate by relatively large values when e.g., a finger is directly occluding within a distance of less than 1 cm from the shutter trigger); the received reflected amplitude may also be high as reflected signal amplitude falls off in squared fashion over distance and the object is near to the IR proximity sensor. - The
proximity detection manager 530 is configured to use a simple linear SVM trained to classify the valid standby data points (i.e., high proximity, high amplitude) from other false-positive sources. To accomplish this, the linear SVM outputs a classification score as follows. -
classification score=α1*proximity+α2*amplitude+α3. - The coefficients α1, α2, and α3 are stored as
SVM data 533. TheSVM data 533 also includes the intermediate values of the SVM that give rise to the output of the classification score. - The
proximity detection manager 530 is also configured to determine whether to engage the occlusion score manager 540 if theproximity detection manager 530 determines, using the SVM output, whether the finger is about to press the shutter button. - The occlusion score manager 540 is configured to determine whether the user's finger will occlude an image of a scene as it depresses the shutter button. To this effect, the occlusion score manager 540 is configured to take few-frame shots and input the few-frame shots into an occlusion scoring model to produce an occlusion score, represented by occlusion score data 542. The decision whether to issue an alert is based on the occlusion score.
- As shown in
FIG. 5 , the occlusion score data 542 includesoptical flow data 543 and convolutional neural network (CNN)data 544. Theoptical flow data 543 results from the few-frame shots (e.g., about 2-4 frames taken in quick succession). For the few-frame shots, the occlusion score manager 540 computes an optical flow measure as optical flow data as a measure of the consistency of the occlusion. The occlusion score manager 540 then inputs theoptical flow data 543 into a CNN, whose layer data, hyperparameters, etc. are represented byCNN data 544. The output of the CNN is the occlusion score, represented by the occlusion score data 542. - The
alert manager 550 is configured to issue an alert to the user if the occlusion score is greater than a threshold. In some implementations, the alert is visual, e.g., a flashing or blinking light, a textual message on the display, etc. In some implementations, the alert includes audio, e.g., a beep or a buzzer. In some implementations, the alert includes tactile responses such as a vibration on the frame. - The components (e.g., modules, processing units 524) of
processing circuitry 520 can be configured to operate based on one or more platforms (e.g., one or more similar or different platforms) that can include one or more types of hardware, software, firmware, operating systems, runtime libraries, and/or so forth. In some implementations, the components of theprocessing circuitry 520 can be configured to operate within a cluster of devices (e.g., a server farm). In such an implementation, the functionality and processing of the components of theprocessing circuitry 520 can be distributed to several devices of the cluster of devices. - The components of the
processing circuitry 520 can be, or can include, any type of hardware and/or software configured to process private data from a wearable device in a split-compute architecture. In some implementations, one or more portions of the components shown in the components of theprocessing circuitry 520 inFIG. 5 can be, or can include, a hardware-based module (e.g., a digital signal processor (DSP), a field programmable gate array (FPGA), a memory), a firmware module, and/or a software-based module (e.g., a module of computer code, a set of computer-readable instructions that can be executed at a computer). For example, in some implementations, one or more portions of the components of theprocessing circuitry 520 can be, or can include, a software module configured for execution by at least one processor (not shown). In some implementations, the functionality of the components can be included in different modules and/or different components than those shown inFIG. 5 , including combining functionality illustrated as two components into a single component. - The
network interface 522 includes, for example, wireless adaptors, and the like, for converting electronic and/or optical signals received from the network to electronic form for use by theprocessing circuitry 520. The set of processingunits 524 include one or more processing chips and/or assemblies. Thememory 526 includes both volatile memory (e.g., RAM) and non-volatile memory, such as one or more ROMs, disk drives, solid state drives, and the like. The set of processingunits 524 and thememory 526 together form processing circuitry, which is configured and arranged to carry out various methods and functions as described herein. - Although not shown, in some implementations, the components of the processing circuitry 520 (or portions thereof) can be configured to operate within, for example, a data center (e.g., a cloud computing environment), a computer system, one or more server/host devices, and/or so forth. In some implementations, the components of the processing circuitry 520 (or portions thereof) can be configured to operate within a network. Thus, the components of the processing circuitry 520 (or portions thereof) can be configured to function within various types of network environments that can include one or more devices and/or one or more server devices. For example, the network can be, or can include, a local area network (LAN), a wide area network (WAN), and/or so forth. The network can be, or can include, a wireless network and/or wireless network implemented using, for example, gateway devices, bridges, switches, and/or so forth. The network can include one or more segments and/or can have portions based on various protocols such as Internet Protocol (IP) and/or a proprietary protocol. The network can include at least a portion of the Internet.
- In some implementations, one or more of the components of the
processing circuitry 520 can be, or can include, processors configured to process instructions stored in a memory. For example, proximity detection manager 530 (and/or a portion thereof), occlusion score manager 540 (and/or a portion thereof), and alert manager 550 (and/or a portion thereof) are examples of such instructions. - In some implementations, the
memory 526 can be any type of memory such as a random-access memory, a disk drive memory, flash memory, and/or so forth. In some implementations, thememory 526 can be implemented as more than one memory component (e.g., more than one RAM component or disk drive memory) associated with the components of theprocessing circuitry 520. In some implementations, thememory 526 can be a database memory. In some implementations, thememory 526 can be, or can include, a non-local memory. For example, thememory 526 can be, or can include, a memory shared by multiple devices (not shown). In some implementations, thememory 526 can be associated with a server device (not shown) within a network and configured to serve the components of theprocessing circuitry 520. As illustrated inFIG. 5 , thememory 526 is configured to store various data, includingproximity detection data 532 and occlusion score data 542. -
FIG. 6 is a flow chart that illustrates amethod 600 of performing occlusion detection for a camera. Themethod 600 may be performed using theprocessing circuitry 520 ofFIG. 5 . - At 602, the
proximity detection manager 530 receives electromagnetic radiation (e.g., light) reflected by an object (e.g., a user's finger). - At 604, the
proximity detection manager 530 and the occlusion score manager 540 determine whether the object is occluding a scene based on the electromagnetic radiation reflected by the object. That is, theproximity detection manager 530 determines, based on the received electromagnetic radiation, that the object is going to depress a shutter trigger. The occlusion score manager then determines, from images of the object, whether the finger will occlude an image of scene taken upon the depressing of the shutter trigger. - At 606, the
alert manager 550, in response to a proximity detection device determining that the object is occluding the scene, transmits a notification that the object is occluding the scene. - Specific structural and functional details disclosed herein are merely representative for purposes of describing example embodiments. Example embodiments, however, may be embodied in many alternate forms and should not be construed as limited to only the embodiments set forth herein.
- The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the embodiments. As used herein, the singular forms “a,” “an,” and “the” are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms “comprises,” “comprising,” “includes,” and/or “including,” when used in this specification, specify the presence of the stated features, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, steps, operations, elements, components, and/or groups thereof.
- It will be understood that when an element is referred to as being “coupled,” “connected,” or “responsive” to, or “on,” another element, it can be directly coupled, connected, or responsive to, or on, the other element, or intervening elements may also be present. In contrast, when an element is referred to as being “directly coupled,” “directly connected,” or “directly responsive” to, or “directly on,” another element, there are no intervening elements present. As used herein the term “and/or” includes any and all combinations of one or more of the associated listed items.
- Spatially relative terms, such as “beneath,” “below,” “lower,” “above,” “upper,” and the like, may be used herein for ease of description to describe one element or feature in relationship to another element(s) or feature(s) as illustrated in the figures. It will be understood that the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if the device in the figures is turned over, elements described as “below” or “beneath” other elements or features would then be oriented “above” the other elements or features. Thus, the term “below” can encompass both an orientation of above and below. The device may be otherwise oriented (rotated 70 degrees or at other orientations) and the spatially relative descriptors used herein may be interpreted accordingly.
- Example embodiments of the concepts are described herein with reference to cross-sectional illustrations that are schematic illustrations of idealized embodiments (and intermediate structures) of example embodiments. As such, variations from the shapes of the illustrations as a result, for example, of manufacturing techniques and/or tolerances, are to be expected. Thus, example embodiments of the described concepts should not be construed as limited to the particular shapes of regions illustrated herein but are to include deviations in shapes that result, for example, from manufacturing. Accordingly, the regions illustrated in the figures are schematic in nature and their shapes are not intended to illustrate the actual shape of a region of a device and are not intended to limit the scope of example embodiments.
- It will be understood that although the terms “first,” “second,” etc. may be used herein to describe various elements, these elements should not be limited by these terms. These terms are only used to distinguish one element from another. Thus, a “first” element could be termed a “second” element without departing from the teachings of the present embodiments.
- Unless otherwise defined, the terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which these concepts belong. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and/or the present specification and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.
- While certain features of the described implementations have been illustrated as described herein, many modifications, substitutions, changes, and equivalents will now occur to those skilled in the art. It is, therefore, to be understood that the appended claims are intended to cover such modifications and changes as fall within the scope of the implementations. It should be understood that they have been presented by way of example only, not limitation, and various changes in form and details may be made. Any portion of the apparatus and/or methods described herein may be combined in any combination, except mutually exclusive combinations. The implementations described herein can include various combinations and/or sub-combinations of the functions, components, and/or features of the different implementations described.
Claims (23)
1. An apparatus, comprising:
an image capture device configured to capture images of a scene, the image capture device including:
a light detector; and
a shutter trigger configured to cause a shutter to open to allow light reflected from the scene toward the image capture device to be incident on the light detector; and
an occlusion detection device configured to determine whether an object will occlude the scene while the shutter is open, the occlusion detection device including:
a proximity detection device configured to determine whether the object is moving toward the shutter trigger; and
a notification device configured to, in response to the proximity detection device determining that the object is moving toward the shutter trigger, transmit a notification that the object is occluding the scene.
2. The apparatus as in claim 1 , wherein the proximity detection device includes:
a source of electromagnetic radiation configured to transmit electromagnetic radiation propagating away from the image capture device; and
first processing circuitry configured to determine whether the object is moving to cause the shutter to be opened based on a temporal pattern of the electromagnetic radiation reflected from the object.
3. The apparatus as in claim 2 , wherein the first processing circuitry configured to determine whether the object is moving to cause the shutter to be opened is further configured to:
perform a classification operation on the temporal pattern of electromagnetic radiation reflected toward the image capture device to produce a classification of whether or not the object is moving to cause the shutter to be opened, the classification operation being based on a support vector machine trained to perform the classification operation based on a proximity of the object to the image capture device and an amplitude of the temporal pattern of electromagnetic radiation reflected toward the image capture device.
4. The apparatus as in claim 3 , wherein the classification of whether or not the object is moving to cause the shutter to be opened is based on whether a classification score is less than or greater than a threshold, the classification score including a linear combination of a proximity of the object to the image capture device and an amplitude of the temporal pattern of electromagnetic radiation propagating toward the image capture device, coefficients of the proximity and the amplitude being based on a device upon which the image capture device is disposed.
5. The apparatus as in claim 2 , wherein the proximity detection device further includes:
second processing circuitry configured to:
determine, in response to the first processing circuitry determining that the object is moving to cause the shutter to be opened, a degree to which the object is occluding the scene; and
in response to the degree being larger than a threshold, transmit notification data indicating that the scene is occluded by the object.
6. The apparatus as in claim 5 , wherein the second processing circuitry is further configured to, in response to the first processing circuitry determining that the object is moving to cause the shutter to be opened:
open and close the shutter a specified number of times at a specified rate to produce a plurality of image frames; and
wherein the second processing circuitry configured to determine the degree to which the object is occluding the scene is further configured to:
input the plurality of image frames into a convolutional neural network configured to produce an occlusion strength indicating the degree to which the object is occluding the scene.
7. The apparatus as in claim 6 , wherein the second processing circuitry configured to input the plurality of image frames into the convolutional neural network is further configured to:
generate an optical flow measure based on the plurality of image frames, the optical flow measure indicating a measure of consistency of occlusion between each pair of sequential frames of the plurality of image frames; and
input the optical flow measure into the convolutional neural network.
8. The apparatus as in claim 6 , wherein the second processing circuitry configured to transmit notification data indicating that the scene is occluded by the object is further configured to:
transmit an audio signal having a volume based on the occlusion strength.
9. The apparatus as in claim 5 , wherein the image capture device is disposed on a frame of a smartglasses system, and
wherein the second processing circuitry configured to transmit notification data indicating that the scene is occluded by the object is further configured to:
transmit a video signal configured to be displayed on a display of the smartglasses system.
10. The apparatus as in claim 2 , wherein the source of electromagnetic radiation is disposed within the shutter trigger.
11. A method, comprising:
receiving electromagnetic radiation reflected by an object;
determining whether the object is occluding a scene based on the electromagnetic radiation reflected by the object; and
in response to a proximity detection device determining that the object is occluding the scene, transmitting a notification that the object is occluding the scene.
12. The method as in claim 11 , wherein the proximity detection device includes:
a source of electromagnetic radiation configured to transmit electromagnetic radiation propagating away from an image capture device; and
first processing circuitry configured to determine whether the object is moving to cause a shutter to be opened based on a temporal pattern of the electromagnetic radiation reflected from the object.
13. The method as in claim 12 , wherein determining whether the object is moving to cause the shutter to be opened includes:
performing a classification operation on the temporal pattern of electromagnetic radiation reflected toward the image capture device to produce a classification of whether or not the object is moving to cause the shutter to be opened, the classification operation being based on a support vector machine trained to perform the classification operation based on a proximity of the object to the image capture device and an amplitude of the temporal pattern of electromagnetic radiation reflected toward the image capture device.
14. The method as in claim 13 , wherein the classification of whether or not the object is moving to cause the shutter to be opened is based on whether a classification score is less than or greater than a threshold, the classification score including a linear combination of a proximity of the object to the image capture device and an amplitude of the temporal pattern of electromagnetic radiation propagating toward the image capture device, coefficients of the proximity and the amplitude being based on a device upon which the image capture device is disposed.
15. The method as in claim 12 , further comprising:
determining, in response to determining that the object is moving to cause the shutter to be opened, a degree to which the object is occluding the scene; and
in response to the degree being larger than a threshold, transmitting notification data indicating that the scene is occluded by the object.
16. The method as in claim 15 , further comprising, in response to determining that the object is moving to cause the shutter to be opened:
opening and closing the shutter a specified number of times at a specified rate to produce a plurality of image frames; and
wherein determining the degree to which the object is occluding the scene includes:
inputting the plurality of image frames into a convolutional neural network configured to produce an occlusion strength indicating the degree to which the object is occluding the scene.
17. The method as in claim 16 , wherein inputting the plurality of image frames into the convolutional neural network includes:
generating an optical flow measure based on the plurality of image frames, the optical flow measure indicating a measure of consistency of occlusion between each pair of sequential frames of the plurality of image frames; and
inputting the optical flow measure into the convolutional neural network.
18. The method as in claim 16 , wherein transmitting the notification data indicating that the scene is occluded by the object includes:
transmitting an audio signal having a volume based on the occlusion strength.
19. The method as in claim 15 , wherein the image capture device is disposed on a frame of a smartglasses system, and
wherein transmitting the notification data indicating that the scene is occluded by the object includes:
transmitting a video signal configured to be displayed on a display of the smartglasses system.
20. A computer program product comprising a non-transitory storage medium, the computer program product including code that, when executed by processing circuitry, causes the processing circuitry to perform a method, the method comprising:
receiving electromagnetic radiation reflected by an object;
determining whether the object is occluding a scene based on the electromagnetic radiation reflected by the object; and
in response to a proximity detection device determining that the object is occluding the scene, transmitting a notification that the object is occluding the scene.
21. The computer program product as in claim 20 , wherein the proximity detection device includes:
a source of electromagnetic radiation configured to transmit electromagnetic radiation propagating away from an image capture device; and
first processing circuitry configured to determine whether the object is moving to cause a shutter to be opened based on a temporal pattern of the electromagnetic radiation reflected from the object.
22. The computer program product as in claim 21 , wherein determining whether the object is moving to cause the shutter to be opened includes:
performing a classification operation on the temporal pattern of electromagnetic radiation reflected toward the image capture device to produce a classification of whether or not the object is moving to cause the shutter to be opened, the classification operation being based on a support vector machine trained to perform the classification operation based on a proximity of the object to the image capture device and an amplitude of the temporal pattern of electromagnetic radiation reflected toward the image capture device.
23. The computer program product as in claim 21 , wherein the method further comprises:
determining, in response to determining that the object is moving to cause the shutter to be opened, a degree to which the object is occluding the scene; and
in response to the degree being larger than a threshold, transmitting notification data indicating that the scene is occluded by the object.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/334,585 US20230403449A1 (en) | 2022-06-14 | 2023-06-14 | Closed loop photography |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202263366375P | 2022-06-14 | 2022-06-14 | |
US18/334,585 US20230403449A1 (en) | 2022-06-14 | 2023-06-14 | Closed loop photography |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230403449A1 true US20230403449A1 (en) | 2023-12-14 |
Family
ID=89077008
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US18/334,585 Pending US20230403449A1 (en) | 2022-06-14 | 2023-06-14 | Closed loop photography |
Country Status (1)
Country | Link |
---|---|
US (1) | US20230403449A1 (en) |
-
2023
- 2023-06-14 US US18/334,585 patent/US20230403449A1/en active Pending
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP5060622B2 (en) | Interaction device for interaction between the screen and the indicator | |
US20200387757A1 (en) | Neural network training for three dimensional (3d) gaze prediction with calibration parameters | |
US11330200B2 (en) | Parallax correction using cameras of different modalities | |
US10002293B2 (en) | Image collection with increased accuracy | |
US10073201B2 (en) | See through near-eye display | |
US10278576B2 (en) | Behind-eye monitoring using natural reflection of lenses | |
US10805543B2 (en) | Display method, system and computer-readable recording medium thereof | |
CN108205374B (en) | Eyeball tracking module and method of video glasses and video glasses | |
US20150193658A1 (en) | Enhanced Photo And Video Taking Using Gaze Tracking | |
US20210012161A1 (en) | Training of a neural network for three dimensional (3d) gaze prediction | |
WO2015035745A1 (en) | Information observation method and information observation device | |
WO2022134957A1 (en) | Camera occlusion detection method and system, electronic device, and storage medium | |
US20230403449A1 (en) | Closed loop photography | |
CN114079726B (en) | Shooting method and equipment | |
KR20220111144A (en) | Image processing method of head mounted display and head mounted display implementing the same | |
WO2024045446A1 (en) | Iris image capture method based on head-mounted display, and related product | |
CN114830621A (en) | Recording indicator light | |
US11822714B2 (en) | Electronic device and control method for capturing an image of an eye | |
US11796801B2 (en) | Reducing light leakage via external gaze detection | |
US20230115678A1 (en) | Apparatus and Method of Focusing Light | |
TW202004669A (en) | Method for multi-spectrum high-precision identification of objects capable of being widely used in security monitoring, industrial monitoring, face recognition, vehicle image recognition and door opening | |
EP3139586B1 (en) | Image shooting processing method and device | |
WO2022066816A2 (en) | Pose optimization in biometric authentication systems | |
CN108881722B (en) | Intelligent shooting method, intelligent glasses, electronic equipment and storage medium | |
US20230281846A1 (en) | Specular surface mapping |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SHIN, DONGEEK;REEL/FRAME:064136/0449Effective date: 20230614 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |