CN115552394A - Converting data from streaming media - Google Patents
Converting data from streaming media Download PDFInfo
- Publication number
- CN115552394A CN115552394A CN202180034022.8A CN202180034022A CN115552394A CN 115552394 A CN115552394 A CN 115552394A CN 202180034022 A CN202180034022 A CN 202180034022A CN 115552394 A CN115552394 A CN 115552394A
- Authority
- CN
- China
- Prior art keywords
- audio
- content
- words
- textual
- spoken
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/68—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/683—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/685—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using automatically derived transcript of audio data, e.g. lyrics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/61—Indexing; Data structures therefor; Storage structures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
- G10L25/54—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination for retrieval
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, are described for creating an enhanced content stream by converting non-textual content into a form that enables a text-based matching system to select the non-textual content. In some aspects, a method includes obtaining a first audio, storing a textual transcription of the first audio in a searchable database. Media content including second audio is obtained. The second audio is converted to textual content. A determination is made that the textual content of the second audio matches the textual transcription of the first audio based on a search of a searchable database. In response to a determination that the textual content of the second audio matches the textual transcription of the first audio, the first audio is inserted into the media content to create an enhanced content stream.
Description
Cross Reference to Related Applications
This application claims priority to U.S. provisional application No. 63/107,943, filed on 30/10/2020. The disclosures of the aforementioned applications are incorporated herein by reference in their entirety.
Background
This description relates to data processing and converting data from streaming media to detect characteristics of the streaming media.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in a method that includes obtaining first audio, storing textual transcriptions of the first audio in a searchable database, obtaining media content that includes second audio, converting the second audio to textual content, determining that the textual content of the second audio matches the textual transcriptions of the first audio based on a search of the searchable database, and inserting the first audio into the media content to create an enhanced content stream in response to determining that the textual content of the second audio matches the textual transcriptions of the first audio.
In some implementations, the method includes determining a first context of the first audio based on a textual transcription of the first audio and determining a second context of the second audio based on textual content of the second audio. Determining that the textual content of the second audio matches the textual transcription of the first audio may include determining that the first context matches the second context.
In some implementations, converting the second content to textual content includes: the method includes detecting spoken words in a second audio, analyzing one or more audio characteristics of the second audio, adjusting importance of one or more words among the spoken words based on the analysis of the one or more audio characteristics, generating textual content representing the spoken words, and assigning the adjusted importance of the one or more words to the textual content representing the one or more words. In some implementations, analyzing the one or more audio characteristics of the second audio includes detecting an audible indication of a point of emphasis of the one or more words. Adjusting the importance of the one or more words may include increasing the importance of the one or more words based on an audible indication of an emphasized point. Determining the context of the second audio can include determining a topic of the first audio based on the adjusted importance assigned to the one or more words of the textual content representing the one or more words.
In some implementations, analyzing the one or more audio characteristics can include distinguishing a first portion of a spoken word spoken by a host voice in the second audio and a second portion of the spoken word spoken by a guest voice in the second audio. Adjusting the importance of the one or more words may include increasing the importance of a first portion of the spoken words relative to the importance of a second portion of the spoken words.
In some implementations, the method includes identifying an exclusion zone of the second audio, omitting words spoken during the exclusion zone from textual content of the second audio. The method may further include detecting a product name in the second audio and omitting words from the textual content of the second audio that were spoken within a specified amount of time of a position of the product name in the second audio.
Other embodiments of this aspect include corresponding systems, apparatus, and computer programs configured to perform the actions of the methods encoded on computer storage devices.
Media content requested and consumed by users is typically provided in text format, and existing systems are customized to use text-based characterizing data to analyze and select content. Current methods of matching and selecting additional media content for non-text-based media depend on contextual data (e.g., in the form of textual metadata) that is typically provided manually by a media content generator. However, text-based systems do not utilize the actual audio/video media content itself to match and/or select media content. This can lead to resources being wasted, for example, when the context data assigned to the content does not accurately describe the content or fails to provide a sufficient description of the content, as this can lead to the distribution of useless content in the context of the media content being streamed, which can lead to additional media content needing to be streamed and additional resources being consumed, such as network bandwidth, processing power and memory usage, and battery usage (e.g., of the mobile device).
The following description discusses various techniques and systems for improving a mechanism for controlling whether content is transmitted over a network by converting non-text streaming media content into a form that enables selection of content for presentation with the streaming media content. Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. For example, the system allows for the use of non-textual content in a text matching mechanism to enable selection of audio content to be presented to a user with a streaming media content item based on a determination that the non-textual content corresponds to audio.
The techniques described in this document may enhance the conversion of streaming media content (e.g., audio content such as podcasts) based on various audio characteristics other than words spoken within the streaming media content. For example, the techniques discussed herein may distinguish a host of a streaming media program (e.g., a podcast) from a guest of the podcast. Using this distinction of the person speaking, the techniques described in this document can filter out words spoken by guests for the purpose of using only the topic discussed by the host for selecting additional content for presentation with the streaming media program. In some cases, rather than filtering out words spoken by guests, words spoken by the host may be weighted higher than words spoken by guests, placing more emphasis on words spoken by the host. In any of these cases, the system is able to emphasize the host-discussed topics relative to those discussed by the guests, which enhances the system's ability to identify additional content related to the topic being discussed by the host and prevents semantic drift that may occur without distinction between speakers.
The techniques discussed herein also provide improvements over conventional systems by emphasizing the conversion of spoken words in streaming media content (e.g., podcasts) based on audio characteristics other than the recognition of the speaker. For example, the volume of some words relative to other words may be used to embed (or assign) emphasis points (or other indicators of importance) to detected words in the streaming media content, thereby enhancing the information used to select additional content (e.g., digital components) for presentation with the streaming media content. In some cases, the emphasized point assigned to a word may be proportional (e.g., directly or inversely) to the volume of the word, pitch changes relative to other spoken words, or the detected quiet volume surrounding the word. This provides richer information about the audio of the streaming media content that can be input to the text matching mechanism to provide a more accurate match between the subject matter of the streaming media content and the additional content selected for presentation.
The prior art techniques that require manual selection of additional non-textual content by its creator and integration thereof into a particular media content item are time consuming and limited to this additional content to which the creator has access or knowledge. By converting non-textual content into a format that can be processed and matched by a text-based system, the described techniques allow access to a more diverse and comprehensive selection of additional content, as well as improved and more customized selection of content. The described techniques reduce the resources required to train and improve the content matching and selection process because existing infrastructure and systems can be used, and allow non-textual content systems to leverage the cumulative knowledge available to existing systems used for text-based content matching and selection. Furthermore, the described techniques allow for more closely tailored content by analyzing the entire content, as compared to user-defined context data that might otherwise only use manual input provided by the content creator.
Further, the use of context to identify and select additional audio content may prevent the distribution of inappropriate content, thereby reducing wasted resources. The described techniques reduce the amount of resources spent distributing content that is inappropriate and should not be distributed, and more efficiently provide content across a network-the described techniques prevent distribution of content to entities that do not actually consume (listen to and/or view) the content. In other words, computing resources such as network bandwidth, processor cycles, and/or allocated memory are not wasted by using these resources to distribute content that should not be distributed.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example environment in which a digital component distribution system creates an enhanced content stream.
Fig. 2 depicts a data flow for a content transformation method for improving content matching and selection.
FIG. 3 is a flow diagram of an example process for content matching and selection using content transformation.
FIG. 4 is a block diagram of an example computer system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This document describes techniques and systems to convert non-textual content into a form that enables a text-based matching system to select the non-textual content for presentation. For example, as discussed in more detail below, a podcast (e.g., an audio program with a host and frequent guests) may be analyzed using the techniques discussed herein, and a conversion of the audio aspect of the podcast may be created to enable selection of a digital component or other content presented with the podcast.
In some implementations, the digital component is in audio form and audio of the digital component is obtained. Text transcriptions of the audio of the digital component may be stored in a text searchable database. Media content, such as podcasts, including audio, is obtained and converted to textual content. There are a number of ways in which the transformation of the textual content may be encoded (or emphasized) to reflect various audio characteristics of the media content, not just the spoken word, as described in more detail below. The conversion of the media content is used to search a searchable database for a match between the media content and one or more digital components available for delivery with the media content. Digital components are inserted into media content to create an enhanced content stream and delivered to users who have requested the media content. The techniques described herein may be performed when a user requests media content, such that additional information (e.g., user-specific information, current event information, or other information) may be used to enhance the search of the searchable database as the media content is being presented, thereby providing dynamic media items that may be modified and/or based on information that changes over time for each individual user.
Throughout this document, a user (such as an end user, a content generator or content provider, among other types of users) may be provided with controls to allow the user to select about: whether and when the systems, programs, or features described herein may enable the collection of user information (e.g., information about a user's social network, social actions or activities, profession, user preferences, or user's current location), and whether the user sends content or communications from a server. In addition, certain data may be processed in one or more ways before it is stored or used, thereby removing personally identifiable information. For example, the user's identity may be processed such that no personal identity information of the user can be determined, or the user's geographic location may be summarized (such as to a city, zip code, or state level) with location information obtained such that a particular location of the user cannot be determined. Thus, the user may have control over which information about the user is collected, how that information is used, and which information is provided to the user.
FIG. 1 is a block diagram of an example environment 100 in which a digital component distribution system 110 creates an enhanced content stream. The example environment 100 includes a network 102, such as a Local Area Network (LAN), a Wide Area Network (WAN), the Internet, or a combination thereof. The network 102 connects an electronic document server 104 ("electronic document server"), user devices 106, and a digital component distribution system 110 (also referred to as a DCDS 110). The example environment 100 may include many different electronic document servers 104 and user devices 106.
The user device 106 is an electronic device capable of requesting and receiving resources (e.g., electronic documents) over the network 102. Example user devices 106 include personal computers, wearable devices, smart speakers, tablet devices, mobile communication devices (e.g., smart phones), smart appliances, and other devices that can send and receive data over the network 102. In some implementations, the user device can include a speaker that outputs audible information to the user and a microphone that accepts audible input from the user (e.g., input of a spoken word). The user device may also include a digital assistant that provides an interactive voice interface for submitting input and/or receiving provided output in response to the input. The user device 106 may also include a display for presenting visual information (e.g., text, images, and/or video). The user device 106 typically includes a user application, such as a Web browser, to facilitate the transmission and reception of data over the network 102, but a native application executed by the user device 106 may also facilitate the transmission and reception of data over the network 102.
The user device 106 includes software such as a browser or operating system. In some implementations, the software allows a user to access information over a network, such as network 102, retrieve the information from a server, and display the information on a display of user device 106. In some embodiments, the software manages the hardware and software resources of the user device 106 and provides common services for other programs on the user device 106. The software may act as an intermediary between the program and the hardware of the user device.
An electronic document is data that presents a set of content, for example, at a user device 106. Examples of electronic documents include web pages, word processing documents, portable Document Format (PDF) documents, images, videos, search result pages, and feed sources. Local applications (e.g., "applications (apps)"), such as applications installed on mobile, tablet, or desktop computing devices, are also examples of electronic documents. Electronic document 105 ("Electronic documents") may be provided by Electronic document server 104 to user device 106. For example, the electronic document server 104 may include a server hosting a publisher's website. In this example, the user device 106 may initiate a request for a given publisher web page, and the electronic document server 104 hosting the given publisher web page may respond to the request by sending machine hypertext markup language (HTML) code that initiates presentation of the given web page at the user device 106.
The electronic document may include a variety of content. For example, the electronic document 105 may include static content (e.g., text or other specified content) that is within the electronic document itself and/or that does not change over time. The electronic document may also include dynamic content that may change over time or upon each request. For example, a publisher of a given electronic document may maintain a data source that is used to populate portions of the electronic document. In this example, the given electronic document may include a tag or script that causes the user device 106 to request content from a data source when the given electronic document is processed (e.g., rendered or executed) by the user device 106. The user device 106 integrates content obtained from the data source into the presentation of the given electronic document to create a composite electronic document that includes the content obtained from the data source. The media content referred to herein is a digital content.
In some cases, a given electronic document may include a digital content tag or digital content script that references the DCDS 110. In these cases, the digital content tag or digital content script is executed by the user device 106 when a given electronic document is processed by the user device 106. Execution of the digital content tag or digital content script configures the user device 106 to generate a request 108 for digital content that is transmitted over the network 102 to the DCDS 110. For example, a digital content tag or digital content script may enable user device 106 to generate a packetized data request including a header and payload data. The request 108 may include data such as from the name (or network location) of the server for which the digital content is being requested, the name (or network location) of the requesting device (e.g., user device 106), and/or information that the DCDS110 may use to select the digital content to be provided in response to the request. The request 108 is transmitted by the user equipment 106 to a server of the DCDS110 over the network 102 (e.g., a telecommunications network).
The request 108 may include data specifying characteristics of the electronic document and the location where the digital content may be presented. For example, data specifying a reference (e.g., a Universal Resource Locator (URL)) to an electronic document (e.g., a web page) in which digital content is to be rendered, available locations of the electronic document available for rendering the digital content (e.g., digital content slots), sizes of the available locations, locations of the available locations within the rendering of the electronic document, and/or media types eligible for rendering in those locations may be provided to the DCDS 110. Similarly, data specifying keywords designated for selection of an electronic document ("document keywords") or an entity referenced by the electronic document (e.g., a person, place, or thing) may also be included in the request 108 (e.g., as payload data) and provided to the DCDS110 to facilitate identification of digital content items that are eligible for presentation with the electronic document.
The request 108 may also include data related to other information, such as information that the user has provided, geographic information indicating the state or region from which the request was submitted, or other information that provides context for the environment in which the digital content is to be displayed (e.g., the type of device that will display the digital content, such as a mobile device or tablet device). The user-provided information may include demographic data of the user device 106. For example, demographic information may include geographic location, occupation, hobbies, social media data, and whether a user owns a particular project, among other characteristics.
Data specifying characteristics of the user device 106 may also be provided in the request 108, such as information identifying a model of the user device 106, a configuration of the user device 106, or a size (e.g., a physical size or resolution) of an electronic display (e.g., a touchscreen or desktop monitor) on which the electronic document is presented. The request 108 may be transmitted, for example, over a packetized network, and the request 108 itself may be formatted as packetized data with a header and payload data. The header may specify the destination of the packet and the payload data may include any of the information discussed above.
The DCDS110 selects digital content to be presented with a given electronic document in response to receiving the request 108 and/or using information included in the request 108. In some implementations, the DCDS110 is implemented in a distributed computing system (or environment) that includes, for example, a server and a set of multiple computing devices that are interconnected and that identify and distribute digital content in response to the request 108. The set of multiple computing devices operate together to identify a set of digital content that is eligible to be presented in an electronic document from among a corpus of millions or more of available digital content. Millions or more of available digital content may be indexed, for example, in the digital component database 112. Each digital content index entry may reference corresponding digital content and/or include a distribution parameter (e.g., selection criteria) that conditions a distribution of the corresponding digital content.
The identification of eligible digital content may be segmented into a plurality of tasks, which are then distributed among the computing devices within the set of multiple computing devices. For example, different computing devices may each analyze different portions of the digital component database 112 to identify various digital content having distribution parameters that match the information included in the request 108.
The DCDS110 aggregates the results received from the set of multiple computing devices and uses information associated with the aggregated results to select one or more instances of digital content to be provided in response to the request 108. In turn, the DCDS110 may generate and transmit reply data 114 (e.g., digital data representing a reply) over the network 102, the reply data 114 enabling the user device 106 to integrate the selected set of digital content into a given electronic document such that the selected set of digital content is presented at a display of the user device 106 along with the content of the electronic document.
The text-to-speech system 120 is a content conversion system that can convert text content to audio content and audio content to text content. In the context of this specification, audio content is not limited to pure audio content. For example, in some embodiments, audio content may include video content, which may be referred to as multimedia content, and still be considered audio content for purposes of this document. The text-to-speech system 120 may perform content conversion using techniques for text-to-speech and speech-to-text conversion. For example, the text-to-speech system 120 may transcribe a pure audio file into pure text. The text-to-speech system 120 may also convert the plain text file to an audio file to include a corresponding image file. The output of the text-to-speech system 120 may be a pure audio file, a video file with audio and video data, a pure image file, or a pure text file, among other formats. The text-to-speech system 120 detects words within the audio file and outputs the words, transcribing the content of the audio file.
The matching and selection system 130 performs a content matching and selection process of the DCDS 110. For example, the matching and selection system 130 may perform an analysis of the output of text to the speech system 120 or any received content to determine specific characteristics of the content itself, such as the subject matter or category of the content, the entity to which the content refers or suggests, and/or the frequency of the topic or entity referred, among other characteristics. The matching and selection system 130 may also perform analysis to determine characteristics of the audio data, such as volume, emphasized point, pitch, and other characteristics. For example, the matching and selection system 130 may determine a subject or intent of the media content.
The matching and selection system 130 can use the converted media content as input to be used in the content or digital component selection process. For example, the matching and selection system 130 may perform a content selection auction in accordance with existing techniques and using the converted media content as input.
The matching and selection system 130 can perform content matching by matching the content to characteristics of the document itself, for example, using artificial intelligence and machine learning techniques.
The matching and selection system 130 may use statistical and/or machine learning models that accept user-provided information and media content as input. The machine learning model may use any of a variety of models, such as decision trees, models based on generating confrontation networks, deep learning models, linear regression models, logistic regression models, neural networks, classifiers, support vector machines, inductive logic programming, ensemble models (e.g., using techniques such as guided aggregation (bagging), boosting, random forests, etc.), genetic algorithms, bayesian networks, etc., and may be trained using a variety of methods, such as deep learning, association rules, inductive logic, clustering, maximum entropy classification, learning classification, etc. In some examples, the machine learning model may use supervised learning. In some examples, the machine learning model uses unsupervised learning.
In some implementations, the content delivery system 140 can provide additional audio content within the media content stream within the defined time slots, thereby creating an enhanced content stream. For example, in some implementations, the content delivery system 140 may insert a pause that serves as a defined time slot within the audio file at the marker where the additional audio content may be integrated.
In some implementations, the content delivery system 140 can pause the delivery of the media content stream, deliver the additional audio content separately, and then resume the delivery of the media content when the marker is detected.
Fig. 2 depicts a data flow 200 for a content transformation method for improving content matching and selection. The operations of data flow 200 are performed by various components of system 100. For example, the operations of the data flow 200 may be performed by the DCDS110 in communication with the user equipment 106.
The flow begins at step a, where non-textual media content is generated and uploaded to a content provider location. For example, a content creator, a podcast host, poddington Casterly, may generate and upload pure audio media content 202 and upload the content 202 to his own website, cars R Cool N Fast, where he periodically uploads his episodes of a car fan podcast. In some implementations, the entire media content is uploaded at once. In some implementations, the media content is uploaded in real-time (e.g., as the content is to be distributed and/or presented).
In this particular example, the non-textual media content 202 is a podcast episode named "Guest Leuy Hamster talk about others cute bulldog" that includes only audio data. In other examples, the non-textual media content may include video files with audio and image data or video files with only image data, as well as other types of non-textual media content. For example, the non-textual media may be a television program or a video game.
The flow continues to step B where the audio content is provided to the digital component distribution system. In this example, the audio content 202 may be provided to the DCDS110 by Poddington Casterly. In some implementations, the audio content 202 can be uploaded to or retrieved by the DCDS110, as well as other manners obtained by the DCDS 110.
The described system is particularly efficient because audio content can be provided to the DCDS110, the DCDS110 having access to a unique wealth of information and models with the refining techniques used to perform portions of the described techniques.
The flow continues to step C in which the text-to-speech system processes the audio content received by the DCDS110 and outputs text data. For example, the text-to-speech system 120 may process audio data received by the DCDS110 to generate output text data 204. The original form of the media content and the text of the media content may be stored or cached for future requests for the media content. For example, the original form and text of the media content may be stored in a database, such as the digital component database 112, or a different remote database. The database in which the original form and/or text of the media content is stored may be indexed and/or easily searchable. The media content includes non-textual media content from a content creator and media content provided as additional audio content integrated with the non-textual media content.
In some implementations, the media content is uploaded in real-time and steps A, B and C are performed in real-time. For example, the uploading may occur while the media content is being recorded. In some implementations, the non-text media content is fully uploaded and steps A, B and C may be performed once the content is received. In some implementations, the media content is fully uploaded, and steps A, B and C may be performed when requesting to provide the media content to the user device.
The text-to-speech system 120 performs content conversion by analyzing the characteristics of the content itself and the stream. The text-to-speech system 120 need not transcribe each word in order to determine the intent, category, subject, volume, etc. of the media content, although other text-to-speech or speech-to-text systems may be required to transcribe each word. Thus, the system described herein is particularly efficient because it only requires that a portion of the media content be transcribed. In addition, the system can exclude low confidence transcripts, relying only on high confidence transcripts to perform content matching and selection. In some implementations, the text-to-speech system 120 can automatically exclude any transcriptions below a confidence threshold. In some implementations, the text-to-speech system 120 can provide the entire transcription to the matching and selection system 130, even if not enough to perform content matching and selection. In some implementations, the text-to-speech system 120 may instead provide a message to the matching and selection system 130 that the content may be converted to be insufficient.
In some implementations, the additional audio content is received as text-based content and may be converted by the text-to-speech system 120 into the same type of media content as the media content being presented to the user device 106. For example, additional audio content may be received as lines of text and may be converted to pure audio content to match the format of the podcast episode of Poddington Casterly "Guest leiy Hamster talk about other cut belldogs (Guest leiy Hamster talks about his bulldog)".
In some implementations, the additional audio content is received as non-textual content and may be transcribed into text by the text-to-speech system 120. The text of the additional audio content may then be stored in a database, such as the digital component database 112. The original form of the additional audio content may also be stored so that matching can be performed with the text of the additional audio content and the additional audio content itself can be provided upon request.
In some implementations, the audio content can include multiple speakers. For example, the podcast 202 may include two speakers: poddington Casterly and his guest Leuy Hamster. The text-to-speech system 120 can distinguish between the sound of the moderator Poddington Casterly and the guest Leuy Hamster. In some implementations, the text-to-speech system 120 can indicate within its text output 204 which speaker uttered what text. For example, the text-to-speech system 120 may tag each portion of text with its speaker. The text-to-speech system 120 may distinguish the voices of different speakers based on one or more characteristics of the audio. For example, the text-to-speech system 120 may compare the differences between two voices, match characteristics of a particular voice to a particular voice profile indicating unique combinations of frequency and/or pitch characteristics, and/or learn and develop a voice profile for each speaker, among other techniques.
The flow proceeds to step D where the DCDS110 receives a request for content including various information, such as information about the user of the user device, from the user device. For example, the DCDS110 may receive a request 108 from a user of the user device 106 that includes profile information for the user.
The flow continues to step E where the matching and selection system 130 uses the text data 204 from the text-to-speech system 120 to perform matching and selection of digital content to be presented with the media content. The text data 204 may include text data of the media content being streamed to the user device 106 as well as text data of the digital content available for presentation with the media content. In some implementations, the matching and selection system 130 can access the text data 204 from a searchable database.
The matching and selection system 130 performs the matching and selection process according to the technology used with the textual content. For example, the matching and selection system 130 may match characteristics such as a subject or entity of the media content with other digital content items, e.g., with transcript text of the content item. Certain characteristics of the media content may be given more weight than other characteristics when determining other characteristics. For example, the matching and selection system 130 may give more weight to the title of a podcast episode than to words only within the episode content. In some implementations, the matching and selection system 130 determines and assigns a theme or some other information to the additional audio content.
The matching and selection system 130 can assign more weight to words spoken at the beginning of the time of speaking by different speakers, words spoken at a particular pitch, phrases containing a particular entity or topic. In some implementations, if there are multiple speakers, the matching and selection system 130 assigns a particular weight to the word spoken by the moderator of the content. For example, in a podcast or talk show, the host may invite guests to join them, but the guests may have a different view from the host. The matching and selection system 130 may default to matching content only to that spoken by the moderator. In some implementations, the matching and selection system 130 can provide options to content creators and/or publishers to select whether they would like to allow content matching and what portions of their content can be used for content matching. The matching and selection system 130 can also provide users (content creators, publishers, end users) with the option of not allowing certain digital content to be provided with the media content being streamed.
In addition, the matching and selection system 130 may assign more weight based on other characteristics, including the quality of the additional audio content to be selected, the quality of the media content being streamed to the user device, whether there is background music, and/or the type of music being played, among other characteristics.
The text-to-speech system 120 may capture characteristics of the non-text content and provide these characteristics to the matching and selection system 130. For example, the text-to-speech system 120 may determine that the creator of the content has placed emphasized points or other special meanings for a particular word or phrase and draw parallel lines between the non-text emphasized points and the text counterparts. In one example, the text-to-speech system 120 may determine that if a person speaking in the audio content says a particular word or phrase aloud, the word or phrase may be considered emphasized in the same manner as the visual form of bold text, underlined text, larger fonts, and other emphasized points. Other forms of audio emphasis include the pitch and speed of the word or phrase being spoken. For example, if the speaker uses a very low pitch and speaks a phrase quickly, text-to-speech system 120 may determine that the phrase is not important, or that the speaker does not agree with the spoken phrase.
For example, the podcast 202 is shown as including audio data indicating a voice indication, in which case the words "so excited" and "Leuy Hamster" correspond to a larger amplitude than the other words, and the text-to-speech system 120 may treat the words to be emphasized as if they had been bolded, and the matching and selection system 130 may assign weights to the words as if they were bolded, or otherwise add metadata indicating increased emphasis points, such as bolding or underlining. In addition to indications provided within sources such as metadata, the matching and selection system 130 can also use audible indications of words spoken in the audio content.
The matching and selection system 130 can continually improve its matching and selection algorithms and processes. For example, the matching and selection system 130 may determine that some members of the audience of Poddington Casterly will stream each of his podcast episode, but not necessarily listen to the entire podcast. The matching and selection system 130 may then assign a matching probability of the content that increases over time to the particular content that the user may eventually hear. As described above with respect to fig. 1, the matching and selection system 130 may use machine learning models and probabilistic models.
In some implementations, when the matching and selection system 130 determines or receives a message that there is insufficient converted content to perform the matching and selection, the matching and selection system 130 can select a default or generic type of digital content to provide the media content to the user device.
In some implementations, the matching and selection system 130 can access profile information of the user device 106 and/or the content creator or publisher to augment and/or improve the matching and selection process. For example, the matching and selection system 130 may access the user's profile information to determine that the user is not interested in seeing advertisements related to cat food, but is interested in advertisements related to dog food, even though the user generally likes animals.
In some embodiments, the matching and selection system 130 may give more weight to the information matching the user profile. For example, the matching and selection system 130 may personalize the additional audio content based on the user's profile, the content within the media content being streamed, and the creator and/or publisher of the media content.
When selecting digital content, the matching and selection system 130 can use the weights to determine which digital content more closely matches the media content 202. For example, the matching and selection system 130 may aggregate, e.g., determine a sum or average of the weights of each matching item of the digital content that matches the media content. The matching and selection system 130 may then select the digital content with the highest aggregation weight.
In some implementations, step E also includes running the digital content in the selected text format through the text-to-speech system 120 to produce audio content to be presented with the media content 202.
The flow continues to step F where the content delivery system 140 identifies a specified time or time slot during which the selected digital content is to be presented and provides the selected digital content to the user device 106.
As previously described, the DCDS110 may detect and/or determine the time slots in which the additional audio content should be placed.
In some implementations, the exclusion zone or the portion of the content where additional audio content should not be inserted or where words spoken during the zone should be omitted from the analysis may be defined by a user, such as a content creator or publisher. It is stated that the publisher may specify these in their account settings-each episode may be different. It may also be indicated by a particular inaudible tone-i.e., tone 1 at the beginning of the exclusion zone and tone 2 at the end of the exclusion zone.
In some implementations, the content delivery system 140 of the DCDS110 can automatically determine and indicate the exclusion zone. For example, the content delivery system 140 may determine when a natural pause occurs and insert a content slot while creating a forbidden zone so that the content does not break in the middle of a sentence or segment.
In some implementations, the media content being streamed can include additional content, such as sponsored content. The content delivery system 140 may create a exclusion zone for the duration of the sponsored content so as not to confuse the audience or viewers. For example, poddington Casterly may say several sentences about Mercedes during "guest Leuy Hamster tells us his beagle dog" because the episode is sponsored by Mercedes. The content delivery system 140 can detect these several sentences and create a forbidden zone so that no additional audio content is inserted into the Poddington Casterly promotional interlocusing on Mercedes sponsorship.
The content delivery system 140 may also be capable of detecting additional content within the media content being streamed based on the subject matter of the content. For Example, poddington caterpillar might market an Example Brand Phone (Example Brand Phone) in his car fan podcast as a cool Phone with a very rod camera application. The content delivery system 140 may determine that the podcast of Poddington Casterly is about an automobile and that his promotion of an example brand phone identifies a particular brand and is about a smart phone, thus relating to a topic completely unrelated to automobiles. Content delivery system 140 may then determine his pairing promotions of an exemplary branded phone should be in the exclusion zone.
The content delivery system 140 may specify the exclusion zone based on entities mentioned within the media content. For example, the content delivery system 140 may detect a product name within the media content and omit from the textual content of the media content any words spoken within a specified amount of time of the product name location in the second audio. In one example, content delivery system 140 may detect Poddington Casterly mentioning that BRAND Y butter is a very good bar of butter, all cool drivers eat with their toast slices, and may determine any words spoken within 10 seconds of when "BRAND Y" was spoken. The amount of time before and after speaking a particular word need not be the same, and in some implementations, the content delivery system 140 can detect whether an entity name is spoken at the beginning, middle, or end of a promotional piece to adjust the exclusion zone range.
The content delivery system 140 also performs delivery of the selected digital content 206 to the user device 106 in response to the request 108. Content delivery system 140 may provide selected digital content 206 in reply 114.
In some implementations, the DCDS110 can act as an intermediary for delivering media content being streamed to the user device 106. For example, the DCDS110 may receive media content, insert additional, selected digital content into the media content, and stream the media content to the user device 106 along with the selected digital content such that the user device 106 receives a continuous stream of content.
In some implementations, the system 100 as described with respect to fig. 1-2 can pause the streaming of the media content, stream selected digital content from a different source during designated content slots, and then continue the streaming of the media content to the user device 106.
The flow ends with step G, where the DCDS110 provides a reply to the user equipment. For example, the DCDS110 provides the reply 114 to the user device 106 as described above with respect to fig. 1.
FIG. 3 is a flow diagram of an example process 300 for content matching and selection using content transformation. In some implementations, the process 300 may be performed by one or more systems. For example, the process 300 may be implemented by the DCDS110 and/or the user equipment 106 of fig. 1-2. In some implementations, the process 300 may be implemented as instructions stored on a computer-readable medium, which may be non-transitory, and when executed by one or more servers, the instructions may cause the one or more servers to perform the operations of the process 300.
The process 300 begins by obtaining first audio (302). For example, the system 100 may obtain digital content for presentation as additional content to be presented with the media content. The digital content may be, for example, an audio clip from a pet store that promotes a dog toy for stubborn chewers.
The process 300 continues with storing the textual transcription of the first audio in a searchable database (304). For example, the text-to-speech system 120 may store text transcripts of audio clips promoting dog toys in a searchable database, such as the digital component database 112.
The process 300 continues with obtaining media content including second audio (306). For example, the system 100 can receive media content including a podcast 202 from Poddington caterpillar. The media content may be pre-recorded or completed upon receipt or live so that more content is being obtained continuously. In some implementations, the media content can be provided to the user device 106 directly from the content creator and/or publisher. In other implementations, the media content may be provided to the DCDS 112, with the DCDS 112 acting as an intermediary and providing the media content to the user device 106.
In some implementations, the media content can be delivered separately from the additional selected digital content, and the media content can be paused while the additional selected digital content is provided to the user device 106 and restarted when the additional selected digital content ends.
The process 300 continues with converting the second audio to textual content (308). For example, the text-to-speech system 120 can convert a podcast 202 to be streamed to the user device 106 into textual content 204. In some implementations, converting the second audio includes detecting spoken words in the second audio, analyzing one or more audio characteristics of the second audio, adjusting importance of one or more words from among the spoken words based on the analysis of the one or more audio characteristics, generating textual content representing the spoken words, and assigning the adjusted importance of the one or more words to the textual content representing the one or more words. For example, the text-to-speech system 120 may detect a spoken word in the podcast 202, analyze one or more audio characteristics of the podcast 202, such as sound emphasis points, generate a textual representation of the spoken word, such as the textual data 204, and assign and/or adjust weights for one or more words within the textual data 204.
In some implementations, analyzing the one or more audio characteristics of the second audio includes detecting an audible indication of a point of emphasis of the one or more words. For example, the audible indication of the emphasized point may include a repetition of the one or more words, or a volume or tone of a speaker of the word. In some implementations, analyzing the one or more audio characteristics includes distinguishing a first portion of a spoken word spoken by a host voice in the second audio and a second portion of the spoken word spoken by a guest voice in the second audio. For example, the text-to-speech system 120 may specify portions of spoken words spoken by the moderator Poddington Casterly and portions of spoken words spoken by the guest Leuy Hamster.
In some implementations, adjusting the importance of the one or more words includes increasing the importance of the one or more words based on an audible indication of a point of emphasis. For example, the text-to-speech system 120 may increase the weight of words based on an audible indication of emphasized points within the model used to match and select the digital content. In some implementations, adjusting the importance of the one or more words includes increasing the importance of a first portion of the spoken word relative to the importance of a second portion of the spoken word. For example, the text-to-speech system 120 may increase the weight of words spoken by the moderator Poddington Casterly relative to the weight of words spoken by the guest Leuy Hamster.
In some implementations, determining the context of the second audio includes determining a topic of the first audio based on the adjusted importance assigned to the one or more words of the textual content representing the one or more words. For example, the matching and selection system 130 can determine the topic of the audio clip that promotes the dog toy based on the weights of the words associated with the topic of the audio clip.
In some implementations, the importance or weight of a particular word can change based on the emphasized point, the location within a paragraph of detected media content, the frequency of items, and other characteristics. In some implementations, the weight of the words may be increased due to a match between the words in the textual description of the media content and the detected audio. In some implementations, the weight of the words can be adjusted based on whether the host or guest uttered the words.
The process 300 continues with determining that the textual content of the second audio matches the textual transcription of the first audio based on the search of the searchable database (310). For example, the matching and selection system 130 of the DCDS110 can determine that the content of the podcast 202 matches the text transcription of the audio clip of the promotional dog toy based on searching the digital component database 112.
The process 300 continues with inserting the first audio into the media content to create an enhanced content stream in response to determining that the textual content of the second audio matches the textual transcription of the first audio (312). For example, the content delivery system 140 may insert an audio clip into the podcast 202 to create an enhanced content stream after determining that the content of the podcast 202, "guest Leuy Hamster talks about his lovely bulldog," matches the textual transcription of the audio clip that promotes the dog toy for a stubborn chewer, such as a bulldog.
In some implementations, the process includes determining a first context of the first audio based on the text transcription of the first audio and determining a second context of the second audio based on the text content of the second audio, wherein determining that the text content of the second audio matches the text transcription of the first audio includes determining that the first context matches the second context. This context may be a topic, or some other information that may be determined from the text obtained by the podcast. For example, the content delivery system 140 may insert an audio clip into the podcast 202 to create an enhanced content stream after determining that the topic of the podcast 202, "guest Leuy Hamster talks about his lovely bulldog," matches the textual transcription of the audio clip that promotes the dog toy.
In some implementations, the process includes identifying an exclusion zone of the second audio and omitting words spoken during the exclusion zone from textual content of the second audio. For example, the content delivery system 140 can identify an exclusion zone of the podcast 202 and omit words spoken on the podcast 202 during the exclusion zone that are analyzed as part of the podcast 202 content.
FIG. 4 is a block diagram of an example computer system 400 that may be used to perform the operations described above. System 400 includes processor 410, memory 420, storage 430, and input/output device 440. Each of the components 410, 420, 430, and 440 may be interconnected, for example, using a system bus 450. The processor 410 is capable of processing instructions for execution within the system 400. In one implementation, the processor 410 is a single-threaded processor. In another implementation, the processor 410 is a multi-threaded processor. The processor 410 is capable of processing instructions stored on the memory 420 or the storage device 430.
The storage device 430 is capable of providing mass storage for the system 400. In one implementation, the storage device 430 is a computer-readable medium. In various different implementations, the storage device 430 may include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices over a network (e.g., a cloud storage device), or some other mass storage device.
The input/output device 440 provides input/output operations for the system 400. In one embodiment, the input/output devices 440 may include one or more of a network interface device, such as an Ethernet card, a serial communication device, such as a USB and RS-232 port, and/or a wireless interface device, such as a USB and 802.11 card. In another embodiment, the input/output devices may include driver devices configured to receive input data and transmit output data to other input/output devices, such as a keyboard, a printer, and a display device 460. However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and so forth.
Although an example processing system has been described in fig. 5, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
The media does not necessarily correspond to a file. The media may be stored in a portion of a file that contains other documents, in a single file dedicated to the document in question, or in multiple coordinated files.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage medium for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium may be or be embodied in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Furthermore, although a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be contained in one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification can be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all types of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or a plurality or combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as Web services, distributed computing and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that contains other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform operations on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with the instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Moreover, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a Web page to a Web browser on a user's client device in response to a request received from the Web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described is this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the client device (e.g., for displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) may be received at the server from the client device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, but rather it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the present subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (21)
1. A method, comprising:
obtaining a first audio;
storing a textual transcription of the first audio in a searchable database;
obtaining media content comprising second audio;
converting the second audio into text content;
determining, based on a search of the searchable database, that the textual content of the second audio matches the textual transcription of the first audio; and
in response to determining that the textual content of the second audio matches the textual transcription of the first audio, inserting the first audio into the media content to create an enhanced content stream.
2. The method of claim 1, further comprising:
determining a first context of the first audio based on the textual transcription of the first audio; and
determining a second context of the second audio based on the textual content of the second audio, wherein,
determining that the textual content of the second audio matches the textual transcription of the first audio comprises: determining that the first context matches the second context.
3. The method of claim 1 or 2, wherein converting the second content into textual content comprises:
detecting a spoken word in the second audio;
analyzing one or more audio characteristics of the second audio;
adjusting importance of one or more words from among the spoken words based on the analysis of the one or more audio characteristics;
generating the textual content representing the spoken word; and
assigning the adjusted importance of the one or more words to the textual content representing the one or more words.
4. The method of claim 3, wherein:
analyzing the one or more audio characteristics of the second audio comprises: detecting an audible indication of a highlight point for the one or more words;
adjusting the importance of the one or more words includes: increasing the importance of the one or more words based on the audible indication of the emphasized point; and
determining the context of the second audio comprises: determining a topic of the first audio based on the adjusted importance assigned to the one or more words of the textual content representing the one or more words.
5. The method of claim 4, wherein:
analyzing the one or more audio characteristics includes: distinguishing between a first portion of a spoken word spoken by a host voice in the second audio and a second portion of the spoken word spoken by a guest voice in the second audio; and
adjusting the importance of the one or more words includes: increasing the importance of the first portion of the spoken word relative to the importance of the second portion of the spoken word.
6. The method of any of the preceding claims, further comprising:
identifying an exclusion zone for the second audio; and
omitting words spoken during the exclusion zone from the text content of the second audio.
7. The method of claim 6, further comprising:
detecting a product name in the second audio; and
omitting words from the textual content of the second audio that were spoken within a specified amount of time of the position of the product name in the second audio.
8. A system, comprising:
one or more processors; and
one or more memory elements comprising instructions that when executed cause the one or more processors to perform operations comprising:
obtaining a first audio;
storing a textual transcription of the first audio in a searchable database;
obtaining media content comprising second audio;
converting the second audio into text content;
determining, based on a search of the searchable database, that the textual content of the second audio matches the textual transcription of the first audio; and
in response to determining that the textual content of the second audio matches the textual transcription of the first audio, inserting the first audio into the media content to create an enhanced content stream.
9. The system of claim 8, the operations further comprising:
determining a first context of the first audio based on the textual transcription of the first audio; and
determining a second context of the second audio based on the textual content of the second audio, wherein,
determining that the textual content of the second audio matches the textual transcription of the first audio comprises: determining that the first context matches the second context.
10. The system of claim 8 or 9, wherein converting the second content into textual content comprises:
detecting a spoken word in the second audio;
analyzing one or more audio characteristics of the second audio;
adjusting importance of one or more words from among the spoken words based on the analysis of the one or more audio characteristics;
generating the textual content representing the spoken word; and
assigning the adjusted importance of the one or more words to the textual content representing the one or more words.
11. The system of claim 10, wherein:
analyzing the one or more audio characteristics of the second audio comprises: detecting an audible indication of a highlight point for the one or more words;
adjusting the importance of the one or more words includes: increasing the importance of the one or more words based on the audible indication of the emphasized point; and
determining the context of the second audio comprises: determining a topic of the first audio based on the adjusted importance assigned to the one or more words of the textual content representing the one or more words.
12. The system of claim 11, wherein:
analyzing the one or more audio characteristics includes: distinguishing between a first portion of a spoken word spoken by a host voice in the second audio and a second portion of the spoken word spoken by a guest voice in the second audio; and
adjusting the importance of the one or more words includes: increasing the importance of the first portion of the spoken word relative to the importance of the second portion of the spoken word.
13. The system of any of claims 8 to 12, the operations further comprising:
identifying an exclusion zone for the second audio; and
omitting words spoken during the forbidden zone from the text content of the second audio.
14. The system of claim 13, the operations further comprising:
detecting a product name in the second audio; and
omitting words from the textual content of the second audio that were spoken within a specified amount of time of the position of the product name in the second audio.
15. A computer storage medium encoded with instructions that, when executed by a distributed computing system, cause the distributed computing system to perform operations comprising:
obtaining a first audio;
storing a textual transcription of the first audio in a searchable database;
obtaining media content comprising second audio;
converting the second audio into text content;
determining, based on a search of the searchable database, that the textual content of the second audio matches the textual transcription of the first audio; and
in response to determining that the textual content of the second audio matches the textual transcription of the first audio, inserting the first audio into the media content to create an enhanced content stream.
16. The computer storage medium of claim 15, the operations further comprising:
determining a first context of the first audio based on the textual transcription of the first audio; and
determining a second context of the second audio based on the textual content of the second audio, wherein,
determining that the textual content of the second audio matches the textual transcription of the first audio comprises: determining that the first context matches the second context.
17. The computer storage medium of claim 15 or 16, wherein converting the second content into textual content comprises:
detecting a spoken word in the second audio;
analyzing one or more audio characteristics of the second audio;
adjusting importance of one or more words from among the spoken words based on the analysis of the one or more audio characteristics;
generating the textual content representing the spoken word; and
assigning the adjusted importance of the one or more words to the textual content representing the one or more words.
18. The computer storage medium of claim 17, wherein:
analyzing the one or more audio characteristics of the second audio comprises: detecting an audible indication of a highlight point for the one or more words;
adjusting the importance of the one or more words includes: increasing the importance of the one or more words based on the audible indication of the emphasized point; and
determining the context of the second audio comprises: determining a topic of the first audio based on the adjusted importance assigned to the one or more words of the textual content representing the one or more words.
19. The computer storage medium of claim 18, wherein:
analyzing the one or more audio characteristics includes: distinguishing between a first portion of a spoken word spoken by a host voice in the second audio and a second portion of the spoken word spoken by a guest voice in the second audio; and
adjusting the importance of the one or more words includes: increasing the importance of the first portion of the spoken word relative to the importance of the second portion of the spoken word.
20. The computer storage medium of any of claims 15 to 19, the operations further comprising:
identifying an exclusion zone for the second audio; and
omitting words spoken during the forbidden zone from the text content of the second audio.
21. The computer storage medium of claim 20, the operations further comprising:
detecting a product name in the second audio; and
omitting words from the text content of the second audio that are spoken within a specified amount of time of a position of the product name in the second audio.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063107943P | 2020-10-30 | 2020-10-30 | |
US63/107,943 | 2020-10-30 | ||
PCT/US2021/051864 WO2022093453A1 (en) | 2020-10-30 | 2021-09-24 | Transforming data from streaming media |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115552394A true CN115552394A (en) | 2022-12-30 |
Family
ID=78414741
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180034022.8A Pending CN115552394A (en) | 2020-10-30 | 2021-09-24 | Converting data from streaming media |
Country Status (7)
Country | Link |
---|---|
US (1) | US20230244716A1 (en) |
EP (1) | EP4133387A1 (en) |
JP (1) | JP2023533902A (en) |
KR (1) | KR20220157505A (en) |
CN (1) | CN115552394A (en) |
CA (1) | CA3178823A1 (en) |
WO (1) | WO2022093453A1 (en) |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060212897A1 (en) * | 2005-03-18 | 2006-09-21 | Microsoft Corporation | System and method for utilizing the content of audio/video files to select advertising content for display |
US20080201361A1 (en) * | 2007-02-16 | 2008-08-21 | Alexander Castro | Targeted insertion of an audio - video advertising into a multimedia object |
US9123335B2 (en) * | 2013-02-20 | 2015-09-01 | Jinni Media Limited | System apparatus circuit method and associated computer executable code for natural language understanding and semantic content discovery |
CN106792003B (en) * | 2016-12-27 | 2020-04-14 | 西安石油大学 | Intelligent advertisement insertion method and device and server |
US11183195B2 (en) * | 2018-09-27 | 2021-11-23 | Snackable Inc. | Audio content processing systems and methods |
-
2021
- 2021-09-24 WO PCT/US2021/051864 patent/WO2022093453A1/en unknown
- 2021-09-24 EP EP21799399.7A patent/EP4133387A1/en active Pending
- 2021-09-24 US US17/918,974 patent/US20230244716A1/en active Pending
- 2021-09-24 CA CA3178823A patent/CA3178823A1/en active Pending
- 2021-09-24 KR KR1020227038447A patent/KR20220157505A/en unknown
- 2021-09-24 JP JP2022568898A patent/JP2023533902A/en active Pending
- 2021-09-24 CN CN202180034022.8A patent/CN115552394A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230244716A1 (en) | 2023-08-03 |
WO2022093453A1 (en) | 2022-05-05 |
JP2023533902A (en) | 2023-08-07 |
EP4133387A1 (en) | 2023-02-15 |
CA3178823A1 (en) | 2022-05-05 |
KR20220157505A (en) | 2022-11-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11798528B2 (en) | Systems and methods for providing notifications within a media asset without breaking immersion | |
JP7335062B2 (en) | Voice service providing method and apparatus | |
US10346455B2 (en) | Method and system for generating a summary of the digital content | |
JP6967059B2 (en) | Methods, devices, servers, computer-readable storage media and computer programs for producing video | |
US11233756B2 (en) | Voice forwarding in automated chatting | |
US11003720B1 (en) | Relevance-ordered message search | |
CN111800671B (en) | Method and apparatus for aligning paragraphs and video | |
JP7171911B2 (en) | Generate interactive audio tracks from visual content | |
EP3596615A1 (en) | Adaptive interface in a voice-activated network | |
US20230033396A1 (en) | Automatic adjustment of muted response setting | |
US20130332170A1 (en) | Method and system for processing content | |
US20220398276A1 (en) | Automatically enhancing streaming media using content transformation | |
CN111490929B (en) | Video clip pushing method and device, electronic equipment and storage medium | |
US20230244716A1 (en) | Transforming data from streaming media | |
CN113823282A (en) | Voice processing method, system and device | |
JP2014109998A (en) | Interactive apparatus and computer interactive method | |
EP3854037B1 (en) | Dynamic insertion of supplemental audio content into audio recordings at request time | |
WO2023003537A1 (en) | Bit vector-based content matching for third-party digital assistant actions | |
US20170372393A1 (en) | Method, Apparatus and Computer-Readable Medium for Question Answer Advertising |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |