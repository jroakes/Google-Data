CN113557566B - Dynamically adapting assistant responses - Google Patents
Dynamically adapting assistant responses Download PDFInfo
- Publication number
- CN113557566B CN113557566B CN201980092024.5A CN201980092024A CN113557566B CN 113557566 B CN113557566 B CN 113557566B CN 201980092024 A CN201980092024 A CN 201980092024A CN 113557566 B CN113557566 B CN 113557566B
- Authority
- CN
- China
- Prior art keywords
- intent
- response
- user
- familiarity
- automated assistant
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000004044 response Effects 0.000 title claims abstract description 231
- 238000000034 method Methods 0.000 claims abstract description 84
- 238000010801 machine learning Methods 0.000 claims abstract description 47
- 238000012545 processing Methods 0.000 claims abstract description 20
- 230000003993 interaction Effects 0.000 claims description 70
- 230000008569 process Effects 0.000 abstract description 23
- 230000006978 adaptation Effects 0.000 abstract description 2
- 238000012549 training Methods 0.000 description 20
- 230000009471 action Effects 0.000 description 15
- 230000001755 vocal effect Effects 0.000 description 15
- 230000008859 change Effects 0.000 description 10
- 230000004048 modification Effects 0.000 description 6
- 238000012986 modification Methods 0.000 description 6
- 238000006243 chemical reaction Methods 0.000 description 5
- 238000004891 communication Methods 0.000 description 5
- 230000015654 memory Effects 0.000 description 5
- 230000000007 visual effect Effects 0.000 description 5
- 230000007423 decrease Effects 0.000 description 4
- 238000003062 neural network model Methods 0.000 description 4
- 230000002093 peripheral effect Effects 0.000 description 4
- 238000009877 rendering Methods 0.000 description 4
- 238000010586 diagram Methods 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 3
- 230000009118 appropriate response Effects 0.000 description 2
- 238000013528 artificial neural network Methods 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000026676 system process Effects 0.000 description 2
- 230000004913 activation Effects 0.000 description 1
- 230000008649 adaptation response Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000005352 clarification Methods 0.000 description 1
- 238000001816 cooling Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000010438 heat treatment Methods 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 239000003607 modifier Substances 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000010255 response to auditory stimulus Effects 0.000 description 1
- 230000035807 sensation Effects 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/216—Parsing using statistical methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/253—Grammatical analysis; Style critique
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/02—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail using automatic reactions or user delegation, e.g. automatic replies or chatbot-generated messages
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
Abstract
Techniques are disclosed to enable dynamic adaptation of automated assistant responses using dynamic familiarity metrics. Various implementations process received user input to determine at least one intent, and generate a familiarity measure by processing intent-specific parameters and intent-agnostic parameters using a machine learning model. An automated assistant response based on the intent and based on the familiarity measure is then determined. The assistant is responsive to user input and is adapted to the familiarity measure. For example, as familiarity measures become more indicative of familiarity, assistant responses may be more abbreviated and/or more resource efficient.
Description
Background
A user may interact with an automated assistant (also referred to as a "personal assistant," "mobile assistant," etc.) via various client devices such as a smart phone, tablet computer, wearable device, automotive system, standalone personal assistant device, etc. The automated assistant receives input from the user that includes spoken natural language input (i.e., utterances) and may respond by performing an action, by controlling another device, and/or providing responsive content (e.g., visual and/or audible natural language output). An automated assistant interacting via a client device may be implemented via the client device itself and/or via one or more remote computing devices in network communication with the client device (e.g., a computing device in the cloud).
Disclosure of Invention
Implementations described herein relate to dynamically adapting assistant responses based on a user's dynamic familiarity measure. As described herein, the familiarity measure of the user may be generated based on various parameters that change over time, such as parameters that quantify one or more aspects of the user's historical interactions with the automated assistant. Thus, the familiarity measure of the user will dynamically change (e.g., increase and/or decrease) over time as the parameters used in determining the familiarity measure change. Further, in various implementations, familiarity metrics generated in determining whether and/or how to adapt an automated assistant response are customized to one or more intents (and/or optionally intended values/arguments) associated with the user input to which the automated assistant response is responsive. For example, one or more parameters used in generating the response may be specialized to include the intent in the request, and optionally, to the relevant intent. Thus, the familiarity measure generated for a given user may vary depending on the intent associated with the user input from the user.
Implementations make assistant response adaptation shorter and/or resource efficient as familiarity measures become more indicative of familiarity. For example, assume that an automated assistant interface of a client device is provided with the verbal command "change my thermostat schedule for Mondays (change monday schedule of my thermostat)". When the familiarity measure of the user indicates no/little familiarity, the response may be relatively robust to provide additional guidance to the user when completing the task. For example, the response may be a computer generated voice "sure, please state the times and desired thermostat temperature at each time (good, please state the time and the thermostat temperature desired each time)". When the user's familiarity indicates more familiarity, the response may be a simplified computer-generated voice such as "sure, times and desired temperatures please (good, time and desired temperature)". When the familiarity measure of the user indicates even more familiarity, the response (e.g., "times and temperatures") can be even further simplified. As described in greater detail herein, various techniques may be utilized to generate a response that meets the determined familiarity measure. For example, some implementations may select an appropriate response from a plurality of candidate responses each associated with a corresponding familiarity metric. Further, for example, some implementations may obtain an initial/standard response and then thumbnail the initial response according to a familiarity metric. For example, the text of the initial response may be summarized, the terms of the initial response may be removed, the terms of the initial response may be replaced with corresponding pronouns, and/or the initial response may be otherwise manipulated to generate a more abbreviated version.
Further, when the user (in response to a request for a desired time and temperature) provides further verbal input having the desired time and temperature, further responses from the automated assistant may be dynamically adapted based on the familiarity measure. For example: when the indication is unfamiliar/nearly unfamiliar, it may be a computer generated voice, "your schedule for Mondays has been adapted to [ restatement of times and temperatures ] (your Monday schedule has been adapted to [ re-describe time and temperature ])"; when the indication is more familiar, it may be a computer generated voice "your schedule is adapted (your schedule is adapted)"; and when the indication is more familiar it may simply be an confirmatory "ding" (or other non-voice sound) which may optionally be pre-stored on the client device.
Thus, various implementations set forth techniques for directing human/automated assistant interactions when performing technical tasks, and do so in a manner that enables interactions to be more efficient as a user's familiarity metric becomes more indicative of familiarity. For example, as a user's familiarity measure becomes more indicative of familiarity, a more abbreviated response is provided. The more abbreviated response may include fewer bytes than the abbreviated response, thereby utilizing less network resources when generated at the automated assistant server and transmitted to the client device. Further, the abbreviated response may be more efficiently rendered by the client device when rendered as an audible output at the client device. Further, the abbreviated response may be rendered faster, thereby reducing the duration of the human/automated assistant interaction and reducing the duration of the communication session between the client device and the remote automated assistant component. Furthermore, while providing fewer abbreviated (or non-abbreviated) responses when the user's familiarity measure is less indicative of familiarity, human/automated assistant interactions are still directed in an efficient manner. For example, fewer abbreviated responses may provide more guidance to less familiar users, alleviating the need for users to provide resource-intensive follow-up prompts or other interactions to proactively request additional guidance. As described herein, a user's familiarity measure with intents may indicate less familiarity of the user with initial inputs of those intents, as a result of the user not providing any (or less) inputs for a recent period of time, and/or as a result of a change in additional or alternative dynamic parameters utilized in determining the familiarity measure.
In various implementations, in addition to adapting the assistant response based on the dynamic familiarity metric, commands issued in response to user input may also be adapted based on the familiarity metric and/or historical user input. For example, the user may provide an input of "turn up the thermostat" (open thermostat) "and when the user's familiarity measure indicates low familiarity, the assistant may provide a response" please specify how many degrees, or a default of one degree will be used (please specify how many degrees, otherwise a default of 1 degree will be used) ". Then, a command will be provided to control the thermostat based on the user's response (if any). For example, if the user does not provide a response, the command will increment the thermostat one degree. On the other hand, if a response is provided, the command will cause the thermostat to increment in accordance with the user's response. When the familiarity measure of the user indicates high familiarity, the response to the input of "turn up the thermostat (open thermostat)" may simply be an "OK" or "ding" sound. In some of these implementations, historical user input of the user in response to past prompts of "please specify how many degrees, or a default of one degree will be used (please specify how many degrees, otherwise a default of 1 degree will be used)" will be used to automatically determine the increment specified by the command provided for controlling the thermostat. For example, if the user never specified a certain number or specified only one degree, that one degree may be utilized. Further, for example, if the user always designates three degrees, three degrees may be utilized.
Other implementations may include a non-transitory computer-readable storage medium storing instructions executable by one or more processors (e.g., a Central Processing Unit (CPU), a Graphics Processing Unit (GPU), and/or a Tensor Processing Unit (TPU)) to perform a method such as one or more of the methods described above and/or elsewhere herein. Other implementations may include a system of one or more computers and/or one or more robots including one or more processors operable to execute stored instructions to perform methods such as one or more of the methods described above and/or elsewhere herein.
It should be understood that all combinations of the foregoing concepts and additional concepts described in more detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
FIG. 1 is a block diagram illustrating an example environment in which various implementations may be implemented.
Fig. 2 is a flow chart illustrating an example process according to an implementation disclosed herein.
Fig. 3 is another flow chart illustrating another example process according to implementations disclosed herein.
FIG. 4 is a block diagram illustrating an example architecture of a computing device.
Detailed Description
In various implementations, user interface input (e.g., natural language spoken input, text input, gesture input, etc.) provided by a user of an automated assistant of a client device can be processed to determine one or more intents associated with the user interface input. The intent may include at least one corresponding intent of the user interface input, and optionally one or more related intents. For example, the user may provide a verbal input of "Assistant, turn off the kitchen light (Assistant, turn off kitchen lights)" to an automated Assistant, with the corresponding intent being to turn off the connected smart lights. One or more values may be associated with intent, such as in this case, a value indicating "kitchen light" is an identifier of the connected smart light to be turned off.
The automated assistant typically provides a response that is rendered in response to the spoken input. For example, in addition to transmitting an appropriate command to cause the kitchen light to turn off, the automated assistant may also provide a verbal response, such as a verbal response of "Turning off the kitchen light (turn off kitchen light)", via the speaker of the client device. Implementations described herein may be used to selectively generate abbreviated responses (i.e., shortened responses) to be provided in lieu of initial automated assistant responses. For example, where "Turning off the kitchen light (turn off kitchen light)" is an initial/standard response, the response may be provided when the user's familiarity metric indicates a low familiarity with the "turn on/off light" intent, the "control intelligent light" intent, and/or other intent associated with the user interface input. However, in response to the familiarity measure indicating a greater familiarity with intent, the automated assistant may provide a abbreviated verbal response to the user, such as "turning off the light (turn off light)" (which omits the modifier "kitchen"), "turn it off" (replace "kitchenlight" with the pronoun "it"), or just sound without providing any verbal response. Additional and/or alternative abbreviated responses may be provided. Further, as described herein, as the user's familiarity measure with intent dynamically changes, different abbreviated responses may be provided over time. For example, "turning off the light (turn off light)" may be replaced with "turn it off" which may then be replaced with just sound which may then be replaced with "turn it off" (e.g., return to "turn it off" based on familiarity measures becoming indicative of less familiarity due to lack of automated assistant interactions by the user).
As described herein, an initial response is a response provided to a user by an automated assistant when providing a response to a particular intent, and detailed information about the corresponding intent is frequently provided. For example, "Ok, turning off [ total number of smart lights ] lights (good, turn off [ total number of intelligent lights ]) (where the total number of lights is the total number of connected intelligent lights being controlled)" is an initial response associated with a natural language user input of "Assistant, turn off all lights (Assistant, turn off all lights)". Similarly, the response "OK, turning off [ total number of smart lights ] lights (good, total number of intelligent lights off)" is an initial response associated with the natural language input of "Assistant, turn off all kitchen lights (Assistant, all kitchen lights off)". In other words, two natural language spoken inputs provided by the user with the same intent correspond to the same initial response.
In many implementations, the initial response may be truncated to a abbreviated response based on a value of the familiarity metric, where the familiarity metric is determined based on one or more parameters indicative of a user's historical interactions with the automated assistant. In various implementations, the familiarity measure may be determined by processing parameters using a machine learning model, such as a feed forward neural network model, a convolutional neural network model, a recurrent neural network model, and/or an additional neural network model.
In various implementations, parameters utilized in determining the familiarity measure may include an assistant type parameter, an intent-specific parameter, a value-specific parameter, a device type parameter, and/or additional parameters. The assistant type parameters may include parameters that are each based on historical interactions of the user with the automated assistant. For example, the assistant type parameters may include a total number of interactions that the user has made with the automated assistant on a particular client device, a total number of interactions that the user has made with the automated assistant on any client device, a length of time that the user has used the automated assistant, a length of time since the user last used the automated assistant, a total number of interactions that the user has made with the automated assistant within a time frame (e.g., last week, last month), a frequency of interactions with the automated assistant (relative to a unit of time), and/or additional assistant type parameters. In various implementations, one or more (e.g., all) of the assistant-type parameters are intent-agnostic and/or value-agnostic. In other words, those parameters are based on the number of interactions, regardless of whether those interactions involve intent and/or values specified by the user input that is determining the familiarity measure. However, such intent and/or value agnostic parameters may still be processed when generating the familiarity measure and may have a meaningful impact on the generated familiarity measure.
The intent-specific parameters may include historical interactions of the user with the automated assistant for one or more specific intents. For example, the intent-specific parameters may include a total number of interactions of the user with the automated assistant for the intent at the particular client device, a total number of interactions of the user with the automated assistant for the intent at any client device, a length of time since the user last interacted with the automated assistant for the intent, a total number of interactions of the user with the automated assistant for the intent and related intent (e.g., "turn off the kitchen lights (turn off kitchen light)", "dim the kitchen lights (turn on kitchen light)", and "turn on the kitchen lights (turn on kitchen light)" are related intents), a length of time since the user interacted with the automated assistant for the intent and related intent, a total number of interactions of the user with the automated assistant for the intent over a time range (e.g., last week, last month), a frequency of interactions with the automated assistant for the intent (relative to a unit of time), and/or additional intent-related parameters. In some implementations, the intent-specific parameters may additionally or alternatively include one or more intent-specific parameters that are specific to the intent specified by the user input and specific to one or more values/arguments of the intent. For example, for a first user input "dim office light", a value identifying "office light" may be specified for dimming intent. For the user input "dim living room lights (dimming living room light)", a value of the flag "living room lights (living room light)" may be specified for dimming intention. The parameters for the first input may include one or more of an amount based on user interaction for a "dimming" intent and for an "office light" value. The parameters for the second input may include one or more parameters that are different from the parameters for the first input, as the parameters for the second input may include parameters based on the amount of interaction of the user for the "dimming" intent and for the "living room lights (living room light)" value. Thus, in various implementations, one or more parameters may be intent-specific and value-specific.
The value-specific parameters may include a user's historical interactions with the automated assistant for one or more values/arguments specified by the user input (optionally, without regard to intent specified by the user input). For example, the value-specific parameters may include a total number of interactions of the user with the automated assistant for a value at a particular client device, a total number of interactions of the user with the automated assistant for a value at any client device, a length of time since the user last interacted with the automated assistant for a value, a total number of interactions the user has had for the value within a time range (e.g., last week, last month), a frequency of interaction with the automated assistant for a value (relative to a certain time unit), and/or additional value-related parameters. For example, for user input of "turn off the thermostat (close thermostat)", the value-specific parameters may include one or more based on an amount of interaction by a user associated with the thermostat, including interactions for "close" intent of the thermostat and/or for additional intent such as "increase/decrease temperature" intent, "change mode" (e.g., cooling mode, heating mode, leaving mode, economy mode, etc.), and/or other intent. Thus, the value-specific parameter may provide an indication of historical interactions of the user with respect to one or more values, optionally without regard to intent. However, as noted above, in some implementations, the parameters may additionally or alternatively include one or more of both value-specific and intent-specific.
The device type parameters may include information related to the client device, such as the client device type, current client device modality, all available client device modalities, smart devices connected with the client device, and/or additional device parameters.
Additionally or alternatively, the parameters may include one or more parameters related to the current dialog session. For example, the parameters may include a total number of dialogue sessions that the user interacted with the automated assistant for intent, a total number of interactions the user with the automated assistant for intent within the current session, a total number of interactions the user with the automated assistant for intent and related intent within the current session, and/or additional session parameters. As used herein, a "conversation session" may include a logically self-contained exchange of one or more messages between a user and an automated assistant (and, in some cases, other human participants). The automated assistant may distinguish between multiple conversations with the user based on various signals such as passage of time, changes in user context between conversations (e.g., location, before/during/after a scheduled meeting, etc.), detection of one or more intermediate interactions between the user and the client device instead of conversations between the user and the automated assistant (e.g., user switching applications for a period of time, user leaving and then returning to a separate voice-activated product), locking/sleeping of the client device between conversations, changes in the client device for interfacing with one or more instances of the automated assistant, etc.
Once the familiarity score has been determined, the techniques described herein can be utilized to generate a abbreviated version of the automated assistant response. In some implementations, the verbal portion of the initial response may be removed. In some such implementations, the visual, audible, and/or other portions of the response remain after the verbal portion is removed. For example, a abbreviated response to sound can be generated from an initial response of "OK, your reservation at Hypothetical Caf e has been made for e 3pm of sound (good, imaginary coffee house that has been reserved for you for 3 pm)". In some implementations, the nouns (and/or phrases that include nouns) in the initial response can be replaced with pronouns. For example, by replacing the phrase (kitchenlights) with the pronoun "they", an abbreviated response of "OK, turn off they" may be generated from the initial response of "OK, turning the kitchen lights off (good, turn off kitchen lights)". In some implementations, the spoken initial response can be truncated to generate a abbreviated response. For example, an initial response of "OK, your reservation at Hypothetical Caf e has been made for Tuesday at 7pm (good, already reserved for a hypothetical coffee house of 7PM on Tuesday)" may be truncated to generate a abbreviated response of "OK, your reservation at Hypothetical Caf e has ben map (good, already reserved for a hypothetical coffee house of you)".
In many implementations, the abbreviated version of the response may change over time (e.g., as the familiarity of the user with the intent increases, as the familiarity of the user with the relevant intent increases, as the familiarity of the user with the intent decreases, etc.). For example, a first thumbnail response may be generated from the initial response and then a second thumbnail response may be generated from the initial response, wherein the second thumbnail response is shorter than the first thumbnail response. For example, the first abbreviated response of "ok, turn-off can be determined from the initial response of" ok, turn 22 lights off*ding (good, turn off 22 lights) ". As the familiarity measure dynamically changes over time, an "ok" second abbreviated response may be determined from the initial response. The length of the thumbnail response may continue to decrease and/or increase over time in response to changing values of the dynamic familiarity measure. Thus, various implementations describe techniques that seek to adapt human/automated assistant interactions in a manner that provides an assistant response that is efficient for delivery and/or rendering-while also ensuring that the assistant response is adapted to the current familiarity metric (thereby slowing the opportunity for the user to request further clarification resulting in further consumption of resources).
Turning now to FIG. 1, an example environment is illustrated in which dynamic familiarity measures can be used to dynamically adapt automated assistant responses. Fig. 1 includes a client computing device 102 that executes an instance of an automated assistant client 104. The one or more cloud-based automation assistant components 114 can be implemented on one or more computing systems (collectively, "cloud" computing systems) communicatively coupled to the client device 102 via one or more local and/or wide area networks (e.g., the internet) indicated generally at 112.
An instance of the automated assistant client 104 through its interaction with one or more cloud-based automated assistant components 114 may be a logical instance of the automated assistant 100 that appears to a user to be a human-machine conversation with which the user may conduct. An example of such an automated assistant 100 is depicted in fig. 1. It should therefore be appreciated that in some implementations, the automated assistant 100 may respond to requests from any user, regardless of whether the user is actually "served" by a particular instance of the automated assistant 100.
The client computing device 102 may be, for example: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a stand-alone interactive speaker, a smart appliance such as a smart television, and/or a wearable apparatus of a user including a computing device (e.g., a watch of a user with a computing device, glasses of a user with a computing device, a virtual or augmented reality computing device). Additional and/or alternative client computing devices may be provided. In various implementations, the client computing device 102 may optionally operate one or more other applications in addition to the automated assistant client 104, such as a message exchange client (e.g., SMS, MMS, online chat), browser, and so forth. In some of those various implementations, one or more of the other applications may optionally interface with the automation assistant 107 (e.g., via an application programming interface), or contain an instance of its own automation assistant application (which may also interface with the cloud-based automation assistant component 114).
The automated assistant 100 participates in a human-machine conversation session with a user via user interface input and output devices of the client device 102. In order to preserve user privacy and/or conserve resources, in many cases, the user must often explicitly invoke the automated assistant 100 before the automated assistant will fully process the spoken utterance. Explicit invocation of the automated assistant 100 may occur in response to certain user interface inputs received at the client device 102. For example, user interface inputs that may invoke the automated assistant 100 via the client device 102 may optionally include actuation of hardware and/or virtual buttons of the client device 102. Further, the automated assistant client may include one or more local engines 110, such as a call engine operable to detect the presence of one or more spoken call phrases. The invocation engine may invoke the automated assistant 100 in response to detecting one of the spoken invocation phrases. For example, the invocation engine may invoke the automated Assistant 100 in response to detecting a spoken invocation phrase such as "Hey Assistant", "OK Assistant", and/or "Assistant". The invocation engine may continuously process (e.g., if not in "inactive" mode) the audio data stream based on output from one or more microphones of the client device 102 to monitor for the occurrence of spoken invocation phrases. Upon monitoring for the occurrence of a spoken phrase, the call engine discards (e.g., after temporary storage in a buffer) any audio data frames that do not include the spoken phrase. However, when the call engine detects the presence of a spoken call phrase in the processed audio data frame, the call engine may call the automated assistant 100. As used herein, a "call" of the automated assistant 100 may include a function that causes activation of one or more prior inactivity of the automated assistant 100. For example, invoking the automated assistant 100 may include causing one or more local engines 110 and/or cloud-based automated assistant components 114 to further process the audio data frames, detect the invocation phrase based on the audio data frames, and/or one or more subsequent audio data frames (without further processing of the audio data frames occurring prior to invocation).
The one or more local engines 110 of the automated assistant 100 are optional and may include, for example, the call engine described above, a local speech-to-text ("STT") engine (that converts captured audio data into text), a local text-to-speech ("TTS") engine (that converts text into speech), a local natural language processor (that determines semantic meaning of audio and/or text converted from audio), and/or other local components. Because the client device 102 is relatively constrained in terms of computing resources (e.g., processor cycles, memory, battery, etc.), the local engine 110 may have limited functionality relative to any counterpart included in the cloud-based automation assistant component 114.
The cloud-based automation assistant component 114 utilizes virtual infinite resources of the cloud to perform more robust and/or accurate processing of audio data and/or other user interface inputs relative to any counterpart of the local engine 110. Again, in various implementations, the client device 102 may provide audio data and/or other data to the cloud-based automation assistant component 114 in response to the invocation engine detecting a spoken invocation phrase or detecting some other explicit invocation of the automation assistant 100.
The illustrated cloud-based automation assistant component 114 includes a cloud-based TTS module 116, a cloud-based STT module 118, a natural language processor 120, a dialog state tracker 122, and a dialog manager 124. The illustrated cloud-based automation assistant component 114 also includes a familiarity engine 106 that utilizes the model 108 to dynamically generate user familiarity measures based on user parameters. In some implementations, one or more of the engines and/or modules of the automated assistant 100 can be omitted, combined, and/or implemented in a separate component from the automated assistant 100. For example, in some implementations, familiarity engine 106 and/or model 108 can be implemented in whole or in part on client device 102. Furthermore, in some implementations, the automated assistant 100 may include additional and/or alternative engines and/or modules.
Familiarity engine 106 can dynamically determine familiarity measures by processing parameters using machine learning model 108. Additionally or alternatively, the familiarity engine 106 can be used to determine parameters corresponding to one or more intents and determine a response to the intent based on a familiarity metric.
The cloud-based STT module 118 may convert the audio data into text, which may then be provided to the natural language processor 120. Additionally or alternatively, the cloud-based TTS module 116 can convert text data (e.g., natural language responses formulated by the automated assistant 100) into computer-generated speech output. In some implementations, TTS module 116 can provide computer-generated speech output to client device 102 for direct output, for example, using one or more speakers. In other implementations, text data (e.g., natural language response) generated by the automated assistant 100 may be provided to one of the local engines 110, which may then convert the text data into locally output computer-generated speech.
The natural language processor 120 of the automated assistant 100 processes the free-form natural language input and generates annotation output for use by one or more other components of the automated assistant 100 based on the natural language input. For example, the natural language processor 120 may process natural language free-form input that is text input that is a conversion by the STT module 118 of audio data provided by the user via the client device 102. The generated annotation output may include one or more annotations of the natural language input, and optionally one or more (e.g., all) items of the natural language input.
In some implementations, the natural language processor 120 is configured to identify and annotate various types of grammar information in the natural language input. For example, the natural language processor 120 may include a voice labeler (not shown) configured to annotate a term with its grammatical role. Further, for example, in some implementations, the natural language processor 120 may additionally and/or alternatively include a dependency parser (not shown) configured to determine syntactic relationships between terms in the natural language input.
In some implementations, the natural language processor 120 may additionally and/or alternatively include an entity marker (not depicted) configured to annotate entity references in one or more segments, such as references to people (including, for example, literature roles, celebrities, public personas, etc.), organizations, locations (real and imaginary), and the like. The entity markers of the natural language processor 120 may annotate references to entities at a high level of granularity (e.g., to enable identification of all references to entity classes such as people) and/or at a low level of granularity (e.g., to enable identification of all references to particular entities such as particular people). The entity markers may rely on the content of the natural language input to parse a particular entity and/or may optionally communicate with a knowledge graph or other entity database to parse a particular entity.
In some implementations, the natural language processor 120 may additionally and/or alternatively include a co-fingering parser (not depicted) configured to group or "cluster" references to the same entity based on one or more contextual cues. For example, the co-index parser may be used to parse the term "ther" in the natural language input "I liked Hypothetical Caf eastmast time we ate ther" (i like the Hypothetical coffee house where we last took a meal) ", into" hypo-therapeutic cafe ".
In some implementations, one or more components of the natural language processor 120 may rely on annotations from one or more other components of the natural language processor 120. For example, in some implementations, the named entity tagger may rely on annotations from the co-fingering resolver and/or the dependency resolver when all of the annotations are annotated to a particular entity. Further, for example, in some implementations, the co-fingering resolver may rely on annotations from the dependency resolver when clustering references to the same entity. In some implementations, one or more components of the natural language processor 120 may determine one or more annotations when processing a particular natural language input using related prior inputs and/or other related data other than the particular natural language input.
In some implementations, the dialog state tracker 122 may be configured to track "dialog states," which include belief states of targets (or "intents") of one or more users, for example, during a human-machine dialog session and/or across multiple dialog sessions. In determining dialog states, some dialog state trackers may seek to determine the most likely value of a slot for instantiation in a dialog based on user and system utterances in the dialog session. Some techniques utilize a fixed body that defines a set of slots and a set of values associated with the slots. Some techniques may additionally or alternatively be tailored to individual slots and/or domains. For example, some techniques may require training a model of each slot type in each domain.
The dialog manager 122 may be configured to map the current dialog state, for example, provided by the dialog state tracker 122, to one or more "response actions" of the plurality of candidate response actions that are subsequently performed by the automated assistant 100. The responsive action may occur in various forms depending on the current dialog state. For example, the initial and midstream dialog states of the respective rounds corresponding to dialog sessions that occur prior to the last round (e.g., when performing the end user desired task) may be mapped to various responsive actions that include the automated assistant 100 outputting additional natural language dialogs, non-speech sounds (e.g., "ding"), and/or graphical indications. The response dialog may include, for example, a request by the user to provide parameters for certain actions the dialog state tracker 122 deems the user to intend to perform (i.e., fill the slot). In some implementations, the responsive action may include actions such as "request" (e.g., find parameters for slot fill), "provide" (e.g., suggest actions or course of actions for the user), "select", "notify" (e.g., provide requested information to the user), "not match" (e.g., notify the user that the last input was not understood), command to the peripheral device (e.g., turn off the light bulb), etc.
As described herein, when the responsive action includes content to be rendered in response to user input, such content may be generated based on a dynamically determined familiarity metric. For example, content may be selected from a plurality of candidate content items based on mapping the selected content to a dynamically determined familiarity measure, where each candidate content item maps to a corresponding familiarity measure. Further, for example, the content may be generated by modifying the initial/standard content to a degree determined based on the determined familiarity measure and/or using a technique selected based on the determined familiarity measure. Further, when the responsive action includes a command to the peripheral device, such a command may optionally be generated based on historical actions of the user if the familiarity measure of the user also meets one or more thresholds.
Fig. 2 is a flow chart illustrating an example process 200 according to an implementation disclosed herein. For convenience, the operations of the flowcharts are described with reference to systems performing the operations. Such a system may include various components of various computer systems, such as one or more components of a computing system implementing the automated assistant 100. Further, although the operations of process 200 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 202, the system receives user input provided by a user at an automated assistant interface of a client device. The user input may include various types of input, including verbal input, gesture input, text input, physical input, combinations of input types, and/or additional types of user input. For example, the user input may include natural language speech spoken by a user and captured by at least one microphone of the client device.
At block 204, the system processes the user input to determine at least one intent associated with the user input. In many implementations, the user input may include a single intent. For example, a user may provide a gesture to flip over a light switch, which may be processed to determine an intent to turn on the light. In addition, for example, the user may provide a verbal input of "turn off the smart plug (close smart plug)". The spoken input may be converted to text and the text processed to determine the intent to turn off the smart plug. Additionally or alternatively, the user input may include a plurality of intents. For example, the user may provide a verbal input of "turn on all lights and play 'phantom song'", which includes an intent to turn on the lights and play the song. The intent may be determined in a number of ways including one or more engines of an automated assistant, such as dialog manager 124 as described with reference to fig. 1.
In some implementations, at block 204, only the intent specified by the user input is determined. For example, for user input of "dim the lights", only the intent of "dim" and/or "dim light" may be determined. In some implementations, at block 204, one or more related intents are additionally or alternatively determined. In implementations where the relevant intent is determined, the relevant intent may be used to determine intent-specific parameters (used in determining intent-specific familiarity metrics), while the intent specified in the user input is used in determining the response (which may be customized to the familiarity metrics). If the intent is a generic intent specifying an intent, the intent may be related to the intent specified in the user input. For example, "controlling a smart light" is a generic intent to "turn on a smart light", "dim a smart light", and "change the color of a smart light". Further, for example, "weather forecast" is a generic intention of "forecast today", "forecast of 3 days", and "forecast of 5 days". If the intent relates to control of the same device and/or device type, the intent may additionally or alternatively be related to the intent specified in the user input. For example, "turning on a smart light" may be associated with "dimming a smart light" because both intentions are associated with the control of the smart light. Additional and/or alternative techniques may be utilized to determine an intent that is related to the intent specified in the user input. In various implementations, a stored intent classification (taxonomy) may be accessed to identify an intent related to an intent specified in the user input. The stored classifications may identify, for example, generic intents and the kinds of those generic intents, and/or may identify intents related to control of the same device and/or device type.
At block 206, the system optionally determines a user profile associated with the user input. For example, the spoken input can be processed (e.g., by an additional machine learning model) to generate an output associated with the spoken input. The user profile associated with the user input may be selected by comparing the output to one or more user profiles associated with the client device. For example, the output may be an embedding of spoken input, may be compared to a speaker embedding stored with a given user profile, and the given user profile is selected if the output and stored speaker embedding meet a proximity threshold. As another example, the user profile may be selected based on other biometric techniques (e.g., facial recognition), based on a pin or other verification provided by the user, and/or based on other user authentication techniques, if permitted by the user. Once the user profile is determined, the system may determine parameters associated with the user profile. Parameters associated with the user profile may include, for example, a total number of interactions the user profile has with the automated assistant across all client devices, a frequency with which the user profile invokes various intents, and so forth.
At block 208, the system determines parameters including one or more intent-specific parameters based on historical interactions of the user with the automated assistant for the intent determined at block 204. The parameters may include various intent-specific parameters and various intent-agnostic parameters. For example, the intent-specific parameters may include a measure of how often the intent was invoked, how long since the user last invoked the intent, and so on. The intent agnostic parameters may include a measure of a length of time since the user profile invoked the automated assistant, a frequency with which the user invoked the automated assistant, a type of client device, and so forth. For example, the metric indicating the length of time since the intent of the last call is an intent-related parameter, while the metric indicating the length of time since the automated assistant was called (for any intent) is an intent-agnostic parameter.
In implementations in which intent-specific parameters are determined at block 208, the familiarity measure generated at block 210 will be specific to the intent. For example, the machine learning model may accept as input a plurality of parameters including one or more parameters that are intent-specific parameters. For example, the machine learning model may accept intent-specific parameters including: days since the intent specified by the user input was last utilized by the user; a total number of times the specified intent has been utilized by the user input; the total number of times the intent specified by the user input has been utilized in the last 30 days divided by 30; and the total number of times the user has used any relevant intent. If the user input at a given time specifies an intent of "dimming lights", a first intent-specific parameter will be determined. On the other hand, if the user input at a given time specifies the intent "riding a ride-sharing ride", a different second intent-specific parameter will be determined. Thus, even though the non-intent-specific parameters processed using the machine learning model at a given time will be the same between two intents, different intent-specific parameters will be utilized depending on the intent of the user input. Thus, different familiarity measures can be generated for a given time according to the intent specified by the user input.
At block 210, the system processes the parameters using a machine learning model to generate a familiarity measure. In various implementations, the system may process at least two intent-related parameters and no intent-agnostic parameters. For example, the system may process a parameter set that includes three intent-related parameters (and no intent-agnostic parameters). In various implementations, the system may process at least one intent-related parameter and at least one intent-agnostic parameter. For example, the system may process a parameter set that includes five intent-related parameters and seven intent-agnostic parameters.
In many implementations, the machine learning model may be a neural network model, such as a feed forward neural network, a convolutional neural network, and/or additional types of neural networks. In many implementations, the machine learning model may be stored locally at the client device. In many implementations, the machine learning model may be stored remotely from the client device. In some implementations, a single machine learning may be used to process familiarity metrics for all intents, but different familiarity metrics may be generated for different graphs at a given time based at least in part on intent-specific parameters of the variation of the different intents. In some implementations, multiple machine learning models may be trained, each model trained to generate familiarity metrics for one or more corresponding specific intents. In many implementations, a single machine learning model may be used to determine a familiarity measure for each user in a group of users (e.g., all users). In many implementations, each user profile may be associated with an individual machine learning model trained to generate familiarity metrics specific to that user profile. For example, a "basic" machine learning model may be adapted to a particular user based on feedback provided by the particular user during use of the machine learning model when generating an adaptive response for the particular user.
At block 212, the system determines a response of the automated assistant to the user input based on the familiarity measure and based on the at least one intent. In various implementations, an initial response may be determined for user input, and a familiarity metric may be used to determine a abbreviated version of the response. In various implementations, the initial response may be determined at a system remote from the client device, such as one or more of the cloud-based automation assistant components 114 of fig. 1, in some of those implementations, the remote system may determine a thumbnail version of the response using the familiarity metric, and transmit only the thumbnail version of the response to the client device for rendering to the user. In some other implementations, the remote system may transmit the entire initial response to the client device, and one or more engines of the client device may determine a abbreviated version of the initial response.
In implementations that modify an initial response to generate a abbreviated version of the response, one or more techniques can be utilized to automatically modify the initial response. For example, pronunciations may be utilized to replace nouns in the text of the response with pronouns that may have fewer characters than nouns. Further, for example, text summaries may additionally or alternatively be used to convert text into a abbreviated version of the text. Such abbreviated and/or pronounciated text may be provided as a response and/or computer-generated speech based on the text may be provided as a response. As another example, all text and/or computer generated speech from the initial response may be removed, and pre-existing (in the initial response) non-speech sounds may be only rendered responses, or non-speech sounds (not pre-existing in the initial response) may be selected for the response. As yet another example, in some situations where the familiarity measure indicates high familiarity, no response can be provided. In some implementations, the technique employed and/or the application of such techniques may be based on the size of the familiarity measure. For example, assume that the familiarity measure is a number from zero to one, and that a larger number indicates a greater familiarity. In such examples, familiarity metrics from 0.00-0.20 may result in no modification of the initial response, metrics from 0.21-0.35 may result in modification with pronouns only, metrics from 0.36-0.50 may result in modification with generalization to a first degree, metrics from 0.50-0.75 may result in modification with generalization to a second degree that is more aggressive than the first degree (i.e., results in fewer words than the first degree), metrics from 0.76-0.90 may result in modification by omitting all text and including only pre-existing or selected non-speech sounds, and metrics from 0.90-1.00 may result in modification by not providing any response.
In many implementations, the initial response may be provided by a third party other than the party controlling the automated assistant. For example, in response to a user input of "How long will it take third-party ride-sharing company to get a car to my house (how long a third party co-ride will take to get a car to my home)", the automated assistant may interface with the third party co-ride's system to receive an initial response of "A standard car can be at your house in approximately minutes (standard car may be at your home in about 10 minutes)" from the third party system. For example, the automated assistant may send a structured intent request to a third party system using an Application Programming Interface (API), and receive an initial response in response to the structured intent request. The automated assistant may then optionally thumbnail the initial response (e.g., generate a thumbnail response of "about 10 minutes") according to the familiarity metric. In some implementations, a familiarity score (and/or parameters for determining the familiarity score) may be provided to a third party and the third party may provide a abbreviated version of the response in place of the initial response. For example, the automated assistant may send a structured intent request to the third party system along with the familiarity measure to enable the third party system to generate the abbreviated response if appropriate for the familiarity measure.
Instead of generating the thumbnail response based on modifying the initial response, other implementations may generate a response to the familiarity measure based on the response mapped to the familiarity measure. For example, a user input of "turn off the [ light(s) ] off [ light ]" may be mapped to the following response options: (a) "Ok, turn off [ total number of smart lights ] lights good,", (b) "Ok, turn off them/it", (c) "turn" are non-voice only. Further, response option (a) may be mapped to a first familiarity metric range (e.g., 0.0-0.40, where familiarity metrics are numbers from 0 to 1, where higher numbers indicate greater familiarity), response option (b) may be mapped to a second familiarity metric range (e.g., 0.41-0.70), and response option (c) may be mapped to a third familiarity metric range (e.g., 0.71-1.0). Thus, in such examples, the response option may be selected based on the current familiarity metric mapped to the user.
At block 214, the system causes the client device to render the determined response. In some implementations, the client device directly renders the response. In some implementations, the client device renders the output based on the response. For example, a client device using a text-to-speech engine may convert a text response to a spoken output. Further, for example, the server may provide the response as audio data (including computer-generated speech and/or non-speech sounds) and provide the audio data to the client device for rendering. By selecting an appropriate response based on the familiarity measure, the amount of data, such as audio data transmitted to the client device, may be adapted to better utilize network resources. Furthermore, in addition to preventing unnecessary detailed audio feedback from being transmitted in the event that it is not needed, limiting the transmission may reduce the time required to establish a communication channel between the client device and the server. This may improve the use of both network and server resources.
As described herein, in various implementations, the system also performs one or more additional responsive actions in response to user input. For example, where the intent of the user input is to control the smart device, the system may provide commands to cause the smart device to be controlled. As further described herein, in some of those various implementations, if the familiarity measure meets a threshold, the provided command may be based on past responses of the user to fewer abbreviated responses to the same intent (and optionally to the same value of intent). For example, more thumbnail responses may omit a request by the user specifying a particular degree to which the state of the smart device is to be altered, and as a result, the user's past input may instead be used to infer the degree and incorporate the degree into the provided command. For example, a previously provided response to a user input of "dim light" (for a previous familiarity measure) may have prompted the user to specify a percentage of the dim light. More abbreviated versions may omit such hints and may utilize the percentage specified by the user in previous responses.
In some implementations, one or more weights of a machine learning model used in generating the familiarity measure (block 210) may be updated based on user satisfaction and/or dissatisfaction with the rendered response, as indicated by analysis of additional user input and/or sensor data after rendering at least a portion of the response. The system may generate one or more feedback values indicative of the user's reaction to the given response and update the machine learning model using the feedback values. The user reaction may occur in various forms. In some implementations, the user reaction may take the form of a user providing additional user input. In some of these implementations, the characteristics of the additional user input may be used to infer an actual familiarity level of the user, which is compared to the generated familiarity level (used to generate the response), and the comparison is used to generate an error for updating the weights of the machine learning model. For example, if the inferred familiarity level of the user is the same as the actual familiarity measure, the model may not be updated. However, if the inferred familiarity level of the user is different from the actual familiarity level, the model may be updated with an error, optionally based on the degree of difference between the inferred familiarity metric and the actual familiarity metric.
As one example, if a thumbnail response is provided and additional user input indicates disagreement with the graph and is provided after the thumbnail response is fully rendered, this may be interpreted as positive feedback regarding the thumbnail response. For example, the intent of the additional user input "increase the temperature by five degrees (increase temperature by five degrees)" is different from the intent of the user input "turn off the light (turn off the light)". Such a change in intent between the user input and the additional user input may provide feedback that the user understands the response to the intent to turn off the light (e.g., the response is appropriate given the familiarity of the user). In contrast, an additional user input of "I said increase the temperature by five degrees (i say increase temperature by five degrees)" following the original user input of "increase the temperature by five degrees (increase temperature by five degrees)" may provide feedback that the user does not understand the thumbnail response to the original user input (e.g., actual familiarity of the given user, response too thumbnail). As a result, this can be interpreted as negative feedback and corresponding error for updating the machine learning model. For example, the error may be based on the generated familiarity measure as compared to an inferred familiarity measure that indicates familiarity less than the generated familiarity measure. As another example, if a response (abbreviated or non-abbreviated) is provided and additional user input indicates a different intent and is provided before the response is fully rendered, this may indicate that the response is not sufficiently abbreviated (because the intent of the additional input is different, but the user begins to provide additional input before the response is fully rendered—indicating that the user is already familiar with the response). As a result, this can also be interpreted as negative feedback and corresponding errors for updating the machine learning model. For example, the error may be based on the generated familiarity measure as compared to an inferred familiarity measure that indicates familiarity more than the generated familiarity measure.
Additional or alternative user reactions may be utilized to generate the feedback value. For example, in some implementations, the user may be prompted to provide explicit feedback regarding the appropriateness of the response to the familiarity level, and the explicit feedback is used to generate the feedback value. For example, a prompt "was the level of detail in this response appropriate? (is the level of detail in the response appropriate. The prompt may be visual and/or audible, and the feedback may be, for example, verbal input and/or selection of a corresponding graphical element. For example, a "yes" verbal response or a selective response may be used as a positive feedback signal (e.g., without updating the model), while a "no" response may be used as a negative feedback signal (e.g., using the weights of the "standard" error update model). As another example, the pitch of the user in the subsequent speech may be used to generate feedback. For example, the user's immediate subsequent speech may be processed to determine if it indicates disappointment, and if so, a feedback value may be generated to indicate that the familiarity level is inadequate. In other words, the weights of the model may be updated in response to the existence of a frustration sensation and/or other characteristics in the verbal input following (e.g., immediately following) the rendering response.
Fig. 3 is a flow chart illustrating an example process 300 according to an implementation disclosed herein. For convenience, the operations of the flowcharts are described with reference to systems performing the operations. Such a system may include various components of various computer systems, such as one or more components of a computing system implemented in the automated assistant 100. Furthermore, although the operations of process 300 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 302, the system selects a familiarity training instance that includes a parameter portion and a familiarity metric portion. In many implementations, the parameter portion includes at least one intent-related parameter and additional intent-related and/or intent-agnostic parameters. In addition, the training examples include a familiarity measure portion that indicates a resulting familiarity measure corresponding to parameters included in a parameter portion of the training examples. In many implementations, the familiarity metric portion of the training example may be marked based on user interface input provided by a person (or persons). For example, a first person may determine a first familiarity measure from a given parameter portion of the training example, a second person may determine a second familiarity measure from a given parameter portion of the training example, and so on, and the familiarity measure portion of the training example may be determined as an average of the first familiarity measure, the second familiarity measure, and so on.
At block 304, the system applies the parameter portion of the familiarity training example as an input to a machine learning model to generate a predicted familiarity metric. In many implementations, the machine learning model may be initialized with manually provided hand-tone parameters (e.g., as before any training is initiated).
At block 306, the system determines a difference between the predicted familiarity measure and the familiarity measure portion of the training example.
At block 308, the system updates the machine learning model based on the determined differences. For example, the system may update one or more weights of the machine learning model based on the determined differences. For example, the system may update the weights of the machine learning model by back propagation of errors based on the determined differences.
At block 310, the system determines whether additional training examples are selected. If so, the system returns to block 302 and selects a different familiarity training example and proceeds to blocks 304, 306, and 308. In some implementations, the system may determine to select additional training instances if one or more additional unprocessed training instances are present and/or if other criterion/criteria have not been met. Other criterion/criteria may include, for example, whether a threshold number of epochs have occurred and/or whether a threshold training duration has occurred. Although process 300 is described with respect to non-batch learning techniques, batch learning may additionally and/or alternatively be utilized. If at the iteration of block 310, the system determines that additional training instances are not selected, process 300 ends.
The process 300 of FIG. 3 describes training a machine learning model with a supervised training example, where a reviewer may assign labels that indicate ground truth familiarity metrics. However, in various implementations, as described above, feedback from the user during deployment of the machine learning model may additionally or alternatively be used to train the machine learning model. For example, even though some (or all) of the user-provided feedback may not explicitly indicate a "ground truth" familiarity measure that should have been determined, the feedback may still be used to determine whether the corresponding generated familiarity measure is appropriate, and if not, update the weights of the model. In other words, the user feedback may be used as a reward signal and the reward signal applied to a reward function that may be used to update the model in a semi-supervised learning approach. By multiple iterations of training based on such reward signals, the machine learning model may converge to provide an accurate and robust familiarity measure. In some implementations, the machine learning model may be initially trained using the process 300 of fig. 3, and then further trained using feedback during deployment. In some other implementations, the machine learning model may be trained with feedback without first being trained with the process 300 of fig. 3.
FIG. 4 is a block diagram of an example computing device 410 that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing devices and/or other components may include one or more components of the example computing device 410.
Computing device 410 typically includes at least one processor 414 that communicates with a number of peripheral devices via a bus subsystem 412. These peripheral devices may include a storage subsystem 424 that includes, for example, a memory subsystem 425 and a file storage subsystem 426, a user interface output device 420, a user interface input device 422, and a network interface subsystem 416. Input and output devices allow a user to interact with computing device 410. The network interface subsystem 416 provides an interface to external networks, and is coupled to corresponding interface devices among other computing devices.
User interface input devices 422 may include a keyboard, a pointing device such as a mouse, trackball, touch pad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways of inputting information into computing device 410 or onto a communication network.
The user interface output device 420 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a cathode ray tube ("CRT"), a flat panel device such as a liquid crystal display ("LCD"), a projection device, or some other mechanism for creating a viewable image. The display subsystem may also provide for non-visual display, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 510 to a user or another machine or computing device.
Storage subsystem 424 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, storage subsystem 424 may include logic to perform selected aspects of one or more of the processes of fig. 2-3, as well as to implement the various components shown in fig. 1.
These software modules are typically executed by processor 414 alone or in combination with other processors. Memory 425 used in storage subsystem 424 may include a number of memories, including a main random access memory ("RAM") 430 for storing instructions and data during program execution and a read only memory ("ROM") 432 in which fixed instructions are stored. File storage subsystem 426 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive and associated removable media, a CD-ROM drive, an optical disk drive, or a removable media cartridge. Modules implementing the functionality of certain implementations may be stored by file storage subsystem 426 in storage subsystem 424, or in other machines accessible by processor 414.
Bus subsystem 412 provides a mechanism for allowing the various components and subsystems of computing device 410 to communicate with each other as desired. Although bus subsystem 412 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses.
Computing device 410 may be of various types including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 410 depicted in FIG. 4 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computing device 410 are possible with more or fewer components than the computing device depicted in fig. 4.
Where the system described herein gathers personal information or available personal information about a user (or as often referred to herein as a "participant"), the user may be provided with an opportunity to control whether programs or features gather user information (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the user's current geographic location) or whether and/or how to receive content from a content server that may be more relevant to the user. Moreover, certain data may be processed in one or more ways to remove personally identifiable information before being stored or used. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be summarized if geographic location information is obtained (such as to a city, zip code, or state level) such that a particular geographic location of the user cannot be determined. Thus, the user may control how information about the user is collected and/or used.
In some implementations, a method implemented by one or more processors is provided and includes receiving user input. The user input is provided by the user at an automated assistant interface of the client device, and the automated assistant interface is an interface for interacting with an automated assistant executing on the client device and/or one or more remote computing devices. The method also includes processing the user input to determine at least one intent associated with the user input, and generating a familiarity measure for the at least one intent. Generating the familiarity measure of the at least one intent includes processing the plurality of parameters using a machine learning model to generate the familiarity measure. The plurality of parameters processed using the machine learning model to generate the familiarity measure include one or more intent-specific parameters based on historical interactions of the user with the automated assistant for at least one intent. The method further includes determining a response of the automated assistant to the user input based on the familiarity measure and based on the at least one intent. The method also includes causing the client device to render the determined response.
These and other implementations of the technology disclosed herein can include one or more of the following features.
In some implementations, determining the response based on the familiarity measure and based on the determined at least one intent includes determining whether the familiarity measure meets a threshold. In some of those implementations, when the familiarity measure does not meet the threshold, the method further includes including in the response: computer-generated speech responsive to the intent, or text that is converted to computer-generated speech when the client device renders the determined response. In some of those implementations, when the familiarity measure meets a threshold, the method further includes: any computer-generated speech and any text are omitted from the response. Omitting any computer-generated speech and any text may optionally include removing any computer-generated speech and/or any text from the initial response.
In some implementations, determining the response based on the familiarity measure and based on the determined at least one intent includes: determining an initial response comprising a first number of bytes based on the determined intent; and in response to determining that the familiarity measure meets a threshold: the initial response is modified to generate a thumbnail response including a second number of bytes less than the first number of bytes. In some of these implementations, modifying the initial response to generate the abbreviated response includes: removing any computer-generated speech from the initial response or any text that is converted to computer-generated speech when the client device renders the determined response; replacing nouns in the text with pronouns having fewer characters than nouns, and/or performing text summarization to convert the text into a shortened version of the text.
In some implementations, determining that the response occurs at one or more servers of the automated assistant that are remote from the client device, and causing the client device to render the response includes transmitting the response to the client device by one or more of the servers over one or more networks.
In some implementations, the user input is a spoken user input, and the method further includes: processing the spoken user input or the spoken call prior to the spoken user input to determine that the spoken user input corresponds to a user profile accessible to the automated assistant; and responsive to determining that the spoken user input corresponds to the user profile: one or more (all) of the plurality of parameters are determined based on data stored in association with the user profile.
In some implementations, the method further includes determining one or more intent-agnostic parameters each determined without consideration of the determined intent, and the parameters processed using the machine learning model in generating the familiarity measure further include the one or more intent-agnostic parameters. In some of those implementations, determining the one or more intent-agnostic parameters includes determining at least one of the intent-agnostic parameters based on additional historical interactions of the user with the automated assistant, the additional historical interactions including historical interactions that do not correspond to the determined at least one intent. In some versions of those implementations, determining the one or more intent-agnostic parameters includes determining at least one of the intent-agnostic parameters based on a number of additional historical interactions between the user and the automated assistant. In some of those versions, the one or more intent-agnostic parameters determined based on the number of historical interactions between the user and the automated assistant include one or more intent-agnostic parameters based on a total number of additional historical interactions and/or a frequency of additional historical interactions.
In some implementations, determining the one or more intent-specific parameters includes determining at least one of the intent-specific parameters based on a number of historical interactions between the user and the automated assistant for the at least one intent and/or a length of time since a last interaction between the user and the automated assistant for the at least one intent. In some of those implementations, the one or more intent-specific parameters include a plurality of intent-specific parameters including at least one intent-specific parameter based on a number of historical interactions between the user and the automated assistant for intent, and optionally including at least one additional intent-specific parameter based on a length of time since a last interaction between the user and the automated assistant for intent.
In some implementations, the at least one intent includes only one or more intents referenced by the user input.
In some implementations, the at least one intent includes one or more intents referenced by the user input and one or more additional intents defined in the stored intent classification as related to the intent.
In some implementations, the at least one intent includes a given intent specified in the user input to control the at least one smart device. In some versions of those implementations, determining the response based on the familiarity measure and based on the at least one intent includes: in response to determining that the familiarity measure meets a threshold: any hint requesting a value for controlling the at least one smart device is omitted from the response. In some versions of these implementations, the method further includes: automatically generating a value for controlling the at least one smart device based on one or more previous user inputs of a user to a previous response responsive to the given intent and including the prompt; one or more commands based on the automatically generated values are transmitted. The one or more commands are transmitted such that the at least one smart device is controlled based on the automatically generated values.
In some implementations, the plurality of parameters processed using the machine learning model to generate the familiarity measure includes a current modality of the client device.
In some implementations, a method implemented by one or more processors is provided and includes receiving user input. The user input is provided by the user at an automated assistant interface of the client device, and the automated assistant interface is an interface for interacting with an automated assistant executing on the client device and/or one or more remote computing devices. The method also includes processing the user input to determine at least one intent specified by the user input and at least one value specified by the user input for the intent. The method also includes generating a familiarity measure. Generating the familiarity measure includes processing a plurality of parameters using a machine learning model to generate the familiarity measure. The plurality of parameters processed using the machine learning model to generate the familiarity measure include one or more value-specific parameters based on historical interactions of the user with the automated assistant for at least one intent. The method further includes determining a response of the automated assistant to the user input based on the familiarity measure and based on the at least one intent. The method also includes causing the client device to render the determined response.
These and other implementations of the technology disclosed herein can include one or more of the following features.
In some implementations, the one or more value-specific parameters include a given value-specific parameter that is based on historical interactions of the user with the automated assistant for at least one value and for a plurality of intents including at least one additional intent in addition to the at least one intent specified by the user input.
In some implementations, the one or more value-specific parameters include a given value-specific parameter based on historical interactions of the user with the automated assistant for at least one value and for at least one intent specified by the user input.
In some implementations, the plurality of parameters processed using the machine learning model to generate the familiarity measure includes one or more intent-specific parameters based on historical interactions of the user with the automated assistant for at least one intent and for a plurality of values, the plurality of values including at least one additional value other than the at least one value specified by the user input.
In some implementations, a method implemented by one or more processors is provided and includes receiving user input. The user input is provided by the user at an automated assistant interface of the client device, and the automated assistant interface is an interface for interacting with an automated assistant executing on the client device and/or one or more remote computing devices. The method also includes processing the user input to determine at least one attribute specified by the user input. The method also includes generating a familiarity measure. Generating the familiarity measure includes processing a plurality of parameters using a machine learning model to generate the familiarity measure. The plurality of parameters processed using the machine learning model to generate the familiarity measure include one or more particular parameters based on historical interactions of the user with the automated assistant for at least one attribute. The plurality of parameters processed using the machine learning model to generate the familiarity measure include one or more agnostic parameters based on a user's historical interactions with the automated assistant, the historical interactions including historical interactions of attributes other than the at least one attribute. The method also includes determining a response of the automated assistant to the user input based on the familiarity measure and based on the at least one attribute. The method also includes causing the client device to render the determined response.
These and other implementations of the technology disclosed herein can include one or more of the following features.
In some implementations, the at least one attribute includes at least one intent and/or at least one value for the at least one intent.
Claims (18)
1. A method implemented by one or more processors, the method comprising:
receiving user input, wherein the user input is provided by a user at an automated assistant interface of a client device, and wherein the automated assistant interface is an interface for interacting with an automated assistant executing on the client device and/or one or more remote computing devices;
processing the user input to determine at least one intent associated with the user input;
generating a familiarity measure for the at least one intent, wherein generating the familiarity measure for the at least one intent comprises:
processing a plurality of parameters using a machine learning model to generate the familiarity measure,
wherein the plurality of parameters processed using the machine learning model to generate the familiarity measure include one or more intent-specific parameters based on historical interactions of the user with the automated assistant for the at least one intent;
Determining a response of the automated assistant to the user input based on the familiarity measure and based on the at least one intent, wherein determining the response of the automated assistant to the user input based on the familiarity measure and based on the at least one intent comprises:
determining whether the familiarity measure meets a threshold;
when the familiarity measure meets a threshold:
determining a thumbnail response of the automated assistant to the user input based on the familiarity measure and based on the at least one intent; and
causing the client device to render the determined response.
2. The method of claim 1, wherein determining the response based on the familiarity measure and based on the determined at least one intent comprises:
when the familiarity measure does not meet the threshold:
the response includes:
computer-generated speech responsive to the intent, or
Text converted to computer-generated speech when the client device renders the determined response;
when the familiarity measure meets the threshold value:
any computer generated speech and any text are omitted from the response.
3. The method of claim 1, wherein determining the response based on the familiarity measure and based on the determined at least one intent comprises:
determining an initial response based on the determined intent, wherein the initial response includes a first number of bytes;
in response to determining that the familiarity measure meets a threshold:
the initial response is modified to generate the thumbnail response, wherein the thumbnail response includes a second number of bytes that is less than the first number of bytes.
4. The method of claim 3, wherein modifying the initial response to generate the thumbnail response comprises:
any computer-generated speech or any text that was converted to computer-generated speech when the client device rendered the determined response is removed from the initial response,
replacing nouns in the text with pronouns having fewer characters than the nouns, and/or
Text summarization is performed to convert the text into a shortened version of the text.
5. The method of claim 1, wherein determining that the response occurs at one or more servers of the automated assistant that are remote from the client device, and wherein causing the client device to render the response comprises transmitting the response to the client device by one or more of the servers over one or more networks.
6. The method of claim 1, wherein the user input is a spoken user input, and the method further comprises:
processing the spoken user input or a spoken call prior to the spoken user input to determine that the spoken user input corresponds to a user profile accessible by the automated assistant; and
responsive to determining that the spoken user input corresponds to the user profile:
one or more parameters of the plurality of parameters are determined based on data stored in association with the user profile.
7. The method of claim 1, further comprising:
one or more intent-agnostic parameters are determined each determined without consideration of the determined intent,
wherein the parameters processed using the machine learning model in generating the familiarity measure further include the one or more intent-agnostic parameters.
8. The method of claim 7, wherein determining the one or more intent-agnostic parameters comprises:
at least one of the intent-agnostic parameters is determined based on additional historical interactions of the user with the automated assistant, the additional historical interactions including historical interactions that do not correspond to the determined at least one intent.
9. The method of claim 8, wherein determining the one or more intent-agnostic parameters includes determining at least one of the intent-agnostic parameters based on an amount of the additional historical interaction between the user and the automated assistant.
10. The method of claim 9, wherein the one or more intent-agnostic parameters determined based on the amount of the additional historical interactions between the user and the automated assistant include one or more intent-agnostic parameters based on a total number of the additional historical interactions and/or a frequency of the additional historical interactions.
11. The method of claim 1, wherein determining the one or more intent-specific parameters includes determining at least one of the intent-specific parameters based on an amount of the historical interactions between the user and the automated assistant for the at least one intent and/or a length of time since a last interaction between the user and the automated assistant for the at least one intent.
12. The method of claim 11, wherein the one or more intent-specific parameters include a plurality of intent-specific parameters including at least one intent-specific parameter based on an amount of the historical interaction between the user and the automated assistant for the intent, and optionally including at least one additional intent-specific parameter based on the length of time since the most recent interaction between the user and the automated assistant for the intent.
13. The method of claim 1, wherein the at least one intent comprises only one or more intents referenced by the user input.
14. The method of claim 1, wherein the at least one intent comprises one or more intents referenced by the user input and one or more additional intents defined in the stored intent classification as being related to the intent.
15. The method of claim 1, wherein the at least one intent comprises a given intent specified in the user input to control at least one smart device, and wherein determining the response based on the familiarity measure and based on the at least one intent comprises:
in response to determining that the familiarity measure meets a threshold:
omitting from the response any hint requesting a value for controlling the at least one smart device; and wherein the method further comprises:
automatically generating a value for controlling the at least one smart device based on one or more previous user inputs of the user to a previous response responsive to the given intent and including the prompt; and
Transmitting one or more commands based on the automatically generated values, wherein transmitting the one or more commands causes the at least one smart device to be controlled based on the automatically generated values.
16. The method of any of claims 1-15, wherein the plurality of parameters processed using the machine learning model to generate the familiarity measure includes a current modality of the client device.
17. A system comprising one or more processors to perform the method of any of claims 1-16.
18. A computer-readable storage medium comprising instructions that, when executed by one or more processors, cause the one or more processors to perform the method of any of claims 1-16.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/020394 WO2020180286A1 (en) | 2019-03-01 | 2019-03-01 | Dynamically adapting assistant responses |
Publications (2)
Publication Number | Publication Date |
---|---|
CN113557566A CN113557566A (en) | 2021-10-26 |
CN113557566B true CN113557566B (en) | 2024-04-12 |
Family
ID=65763906
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980092024.5A Active CN113557566B (en) | 2019-03-01 | 2019-03-01 | Dynamically adapting assistant responses |
Country Status (7)
Country | Link |
---|---|
US (2) | US11875790B2 (en) |
EP (1) | EP3724874A1 (en) |
JP (1) | JP7386878B2 (en) |
KR (1) | KR20210134741A (en) |
CN (1) | CN113557566B (en) |
AU (1) | AU2019432912B2 (en) |
WO (1) | WO2020180286A1 (en) |
Families Citing this family (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
US10978069B1 (en) * | 2019-03-18 | 2021-04-13 | Amazon Technologies, Inc. | Word selection for natural language interface |
US10614800B1 (en) | 2019-08-19 | 2020-04-07 | Voicify, LLC | Development of voice and other interaction applications |
US11508365B2 (en) * | 2019-08-19 | 2022-11-22 | Voicify, LLC | Development of voice and other interaction applications |
US10762890B1 (en) | 2019-08-19 | 2020-09-01 | Voicify, LLC | Development of voice and other interaction applications |
CN111243587A (en) * | 2020-01-08 | 2020-06-05 | 北京松果电子有限公司 | Voice interaction method, device, equipment and storage medium |
US11270080B2 (en) * | 2020-01-15 | 2022-03-08 | International Business Machines Corporation | Unintended bias detection in conversational agent platforms with machine learning model |
US11550605B2 (en) * | 2020-06-30 | 2023-01-10 | Kasisto, Inc. | Building and managing cohesive interaction for virtual assistants |
US11531821B2 (en) * | 2020-08-13 | 2022-12-20 | Salesforce, Inc. | Intent resolution for chatbot conversations with negation and coreferences |
US11935527B2 (en) | 2020-10-23 | 2024-03-19 | Google Llc | Adapting automated assistant functionality based on generated proficiency measure(s) |
US11776542B1 (en) | 2021-03-30 | 2023-10-03 | Amazon Technologies, Inc. | Selecting dialog acts using controlled randomness and offline optimization |
US11978445B1 (en) * | 2021-03-30 | 2024-05-07 | Amazon Technologies, Inc. | Confidence scoring for selecting tones and text of voice browsing conversations |
US20230056003A1 (en) * | 2021-08-23 | 2023-02-23 | International Business Machines Corporation | Automated document adaptation based on topic classification |
US11804215B1 (en) * | 2022-04-29 | 2023-10-31 | Apple Inc. | Sonic responses |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2016071050A (en) * | 2014-09-29 | 2016-05-09 | シャープ株式会社 | Voice interactive device, voice interactive system, terminal, voice interactive method, program for letting computer function as voice interactive device |
KR20160147303A (en) * | 2015-06-15 | 2016-12-23 | 포항공과대학교 산학협력단 | Method for dialog management based on multi-user using memory capacity and apparatus for performing the method |
CN107003797A (en) * | 2015-09-08 | 2017-08-01 | 苹果公司 | Intelligent automation assistant in media environment |
CN108369808A (en) * | 2015-11-10 | 2018-08-03 | 三星电子株式会社 | Electronic equipment and method for controlling the electronic equipment |
CN109074292A (en) * | 2016-04-18 | 2018-12-21 | 谷歌有限责任公司 | The automation assistant of agency appropriate calls |
Family Cites Families (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7920682B2 (en) * | 2001-08-21 | 2011-04-05 | Byrne William J | Dynamic interactive voice interface |
JP2010107614A (en) * | 2008-10-29 | 2010-05-13 | Mitsubishi Motors Corp | Voice guidance and response method |
CN103761083B (en) * | 2013-12-31 | 2017-04-05 | 华为技术有限公司 | A kind of data sharing method, device and terminal |
JP6257368B2 (en) * | 2014-02-18 | 2018-01-10 | シャープ株式会社 | Information processing device |
US20160092160A1 (en) * | 2014-09-26 | 2016-03-31 | Intel Corporation | User adaptive interfaces |
US10614799B2 (en) * | 2014-11-26 | 2020-04-07 | Voicebox Technologies Corporation | System and method of providing intent predictions for an utterance prior to a system detection of an end of the utterance |
KR101583181B1 (en) | 2015-01-19 | 2016-01-06 | 주식회사 엔씨소프트 | Method and computer program of recommending responsive sticker |
US10613213B2 (en) * | 2016-05-13 | 2020-04-07 | Google Llc | Systems, methods, and devices for utilizing radar with smart devices |
US10049663B2 (en) * | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
JP6468258B2 (en) | 2016-08-01 | 2019-02-13 | トヨタ自動車株式会社 | Voice dialogue apparatus and voice dialogue method |
US20180054523A1 (en) * | 2016-08-16 | 2018-02-22 | Rulai, Inc. | Method and system for context sensitive intelligent virtual agents |
US11086593B2 (en) * | 2016-08-26 | 2021-08-10 | Bragi GmbH | Voice assistant for wireless earpieces |
US10860898B2 (en) * | 2016-10-16 | 2020-12-08 | Ebay Inc. | Image analysis and prediction based visual search |
US10467509B2 (en) * | 2017-02-14 | 2019-11-05 | Microsoft Technology Licensing, Llc | Computationally-efficient human-identifying smart assistant computer |
GB2565315B (en) * | 2017-08-09 | 2022-05-04 | Emotech Ltd | Robots, methods, computer programs, computer-readable media, arrays of microphones and controllers |
CN107978311B (en) * | 2017-11-24 | 2020-08-25 | 腾讯科技（深圳）有限公司 | Voice data processing method and device and voice interaction equipment |
WO2019128541A1 (en) * | 2017-12-31 | 2019-07-04 | Midea Group Co., Ltd. | Method and system for controlling multiple home devices |
KR20200098079A (en) * | 2019-02-11 | 2020-08-20 | 현대자동차주식회사 | Dialogue system, and dialogue processing method |
US20200279553A1 (en) * | 2019-02-28 | 2020-09-03 | Microsoft Technology Licensing, Llc | Linguistic style matching agent |
-
2019
- 2019-03-01 US US16/621,371 patent/US11875790B2/en active Active
- 2019-03-01 WO PCT/US2019/020394 patent/WO2020180286A1/en unknown
- 2019-03-01 EP EP19710974.7A patent/EP3724874A1/en active Pending
- 2019-03-01 AU AU2019432912A patent/AU2019432912B2/en active Active
- 2019-03-01 CN CN201980092024.5A patent/CN113557566B/en active Active
- 2019-03-01 JP JP2021544624A patent/JP7386878B2/en active Active
- 2019-03-01 KR KR1020217031803A patent/KR20210134741A/en active Search and Examination
-
2024
- 2024-01-12 US US18/412,004 patent/US20240153502A1/en active Pending
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2016071050A (en) * | 2014-09-29 | 2016-05-09 | シャープ株式会社 | Voice interactive device, voice interactive system, terminal, voice interactive method, program for letting computer function as voice interactive device |
KR20160147303A (en) * | 2015-06-15 | 2016-12-23 | 포항공과대학교 산학협력단 | Method for dialog management based on multi-user using memory capacity and apparatus for performing the method |
CN107003797A (en) * | 2015-09-08 | 2017-08-01 | 苹果公司 | Intelligent automation assistant in media environment |
CN108369808A (en) * | 2015-11-10 | 2018-08-03 | 三星电子株式会社 | Electronic equipment and method for controlling the electronic equipment |
CN109074292A (en) * | 2016-04-18 | 2018-12-21 | 谷歌有限责任公司 | The automation assistant of agency appropriate calls |
Also Published As
Publication number | Publication date |
---|---|
AU2019432912B2 (en) | 2022-10-06 |
US20210082420A1 (en) | 2021-03-18 |
EP3724874A1 (en) | 2020-10-21 |
JP2022522104A (en) | 2022-04-14 |
WO2020180286A1 (en) | 2020-09-10 |
CN113557566A (en) | 2021-10-26 |
US20240153502A1 (en) | 2024-05-09 |
KR20210134741A (en) | 2021-11-10 |
JP7386878B2 (en) | 2023-11-27 |
US11875790B2 (en) | 2024-01-16 |
AU2019432912A1 (en) | 2021-08-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN113557566B (en) | Dynamically adapting assistant responses | |
CN111033492B (en) | Providing command bundle suggestions for automated assistants | |
US20220157309A1 (en) | Supplementing voice inputs to an automated assistant according to selected suggestions | |
KR102599607B1 (en) | Dynamic and/or context-specific hot words to invoke automated assistant | |
US11741953B2 (en) | Using corrections, of automated assistant functions, for training of on-device machine learning models | |
US11217247B2 (en) | Determining whether to automatically resume first automated assistant session upon cessation of interrupting second session | |
US11763813B2 (en) | Methods and systems for reducing latency in automated assistant interactions | |
CN115552515A (en) | User intermediary for hotword/keyword detection | |
KR20240007261A (en) | Use large-scale language models to generate automated assistant response(s) | |
JP2022539674A (en) | Speaker Recognition Using Speaker-Specific Speech Models | |
US20230237312A1 (en) | Reinforcement learning techniques for selecting a software policy network and autonomously controlling a corresponding software client based on selected policy network | |
CN114981772A (en) | Selectively invoking automatic assistant based on detected environmental conditions without requiring voice-based invocation of automatic assistant | |
US20230343324A1 (en) | Dynamically adapting given assistant output based on a given persona assigned to an automated assistant | |
US11948580B2 (en) | Collaborative ranking of interpretations of spoken utterances | |
KR20230153450A (en) | Device arbitration for local implementation of automatic speech recognition | |
CN117121102A (en) | Collaborative ranking of interpretations of spoken utterances |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |