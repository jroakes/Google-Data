DE202016107442U1 - Vector arithmetic unit in a neural network processor - Google Patents
Vector arithmetic unit in a neural network processor Download PDFInfo
- Publication number
- DE202016107442U1 DE202016107442U1 DE202016107442.8U DE202016107442U DE202016107442U1 DE 202016107442 U1 DE202016107442 U1 DE 202016107442U1 DE 202016107442 U DE202016107442 U DE 202016107442U DE 202016107442 U1 DE202016107442 U1 DE 202016107442U1
- Authority
- DE
- Germany
- Prior art keywords
- circuit
- value
- values
- normalization
- activation
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 239000013598 vector Substances 0.000 title claims abstract description 66
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 52
- 238000010606 normalization Methods 0.000 claims abstract description 124
- 230000004913 activation Effects 0.000 claims abstract description 108
- 230000015654 memory Effects 0.000 claims description 36
- 230000001186 cumulative effect Effects 0.000 claims description 29
- 230000006870 function Effects 0.000 claims description 12
- 230000009172 bursting Effects 0.000 claims description 6
- 239000011159 matrix material Substances 0.000 description 20
- 238000000034 method Methods 0.000 description 15
- 238000004364 calculation method Methods 0.000 description 14
- 230000002776 aggregation Effects 0.000 description 10
- 238000004220 aggregation Methods 0.000 description 10
- 238000004590 computer program Methods 0.000 description 9
- 230000008569 process Effects 0.000 description 9
- 230000000694 effects Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 5
- 230000035508 accumulation Effects 0.000 description 4
- 238000009825 accumulation Methods 0.000 description 4
- 230000009286 beneficial effect Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000012886 linear function Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 230000005714 functional activity Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 208000016261 weight loss Diseases 0.000 description 1
- 230000004580 weight loss Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/16—Matrix or vector computation, e.g. matrix-matrix or matrix-vector multiplication, matrix factorization
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F5/00—Methods or arrangements for data conversion without changing the order or content of the data handled
- G06F5/06—Methods or arrangements for data conversion without changing the order or content of the data handled for changing the speed of data flow, i.e. speed regularising or timing, e.g. delay lines, FIFO buffers; over- or underrun control therefor
- G06F5/08—Methods or arrangements for data conversion without changing the order or content of the data handled for changing the speed of data flow, i.e. speed regularising or timing, e.g. delay lines, FIFO buffers; over- or underrun control therefor having a sequence of storage locations, the intermediate ones not being accessible for either enqueue or dequeue operations, e.g. using a shift register
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F7/00—Methods or arrangements for processing data by operating upon the order or content of the data handled
- G06F7/38—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation
- G06F7/48—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation using non-contact-making devices, e.g. tube, solid state device; using unspecified devices
- G06F7/544—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation using non-contact-making devices, e.g. tube, solid state device; using unspecified devices for evaluating functions by calculation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/10—Interfaces, programming languages or software development kits, e.g. for simulating neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
Abstract
Eine Schaltung zum Durchführen neuronaler Netzwerkberechnungen für ein neuronales Netzwerk mit einer Vielzahl von Schichten, wobei die Schaltung umfasst: eine Aktivierungsschaltung, die so konfiguriert ist, um einen Vektor von kumulierten Werten zu empfangen und konfiguriert ist, eine Funktion auf jeden kumulierten Wert anzuwenden, um einen Vektor von aktivierten Werten zu generieren; und eine Normierungsschaltung, die mit der Aktivierungsschaltung gekoppelt ist und konfiguriert ist, einen jeweiligen normierten Wert für jeden Aktivierungswert zu generieren.A circuit for performing neural network computations on a neural network having a plurality of layers, the circuit comprising: an activation circuit configured to receive a vector of accumulated values and configured to apply a function to each accumulated value generate a vector of activated values; and a normalization circuit coupled to the activation circuit and configured to generate a respective normalized value for each activation value.
Description
HINTERGRUNDBACKGROUND
Diese Patentschrift bezieht sich auf die Datenverarbeitung neuronaler Netzwerk-Inferenzen in Hardware.This patent relates to data processing of neural network inferences in hardware.
Neuronale Netzwerke sind Maschinenlernprogramme, bestehend aus einer oder mehreren Schichten, zum Erzeugen einer Ausgabe, z. B. eine Klassifikation für eine empfangene Eingabe. Einige neuronale Netzwerke umfassen zusätzlich zu einer Außenschicht eine oder mehrere ausgeblendete Schichten. Der Ausgang jeder ausgeblendeten Schicht wird als Eingang zur nächsten Schicht im Netzwerk verwendet, z. B. die nächste ausgeblendete Schicht oder die Ausgangsschicht des Netzwerks. Jede Schicht des Netzwerks erzeugt in Übereinstimmung mit aktuellen Werten eines jeweiligen Satzes von Parametern einen Ausgang von einem empfangenen Eingang.Neural networks are machine learning programs consisting of one or more layers for generating an output, e.g. For example, a classification for a received input. Some neural networks include one or more hidden layers in addition to an outer layer. The output of each hidden layer is used as input to the next layer in the network, e.g. The next hidden layer or the output layer of the network. Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
KURZDARSTELLUNGSUMMARY
Im Allgemeinen beschreibt diese Spezifikation eine spezielle Hardware-Schaltung, die neuronale Netzwerkinferenzen berechnet.In general, this specification describes a special hardware circuit that calculates neural network inferences.
Im Allgemeinen kann ein innovativer Aspekt des Gegenstandes, wie in dieser Spezifikation beschrieben, in einer physischen Schaltung zum Durchführen neuronaler Netzwerkberechnungen für ein neuronales Netzwerk eine Vielzahl von Schichten aufweisen, wobei die Schaltung umfasst: eine Aktivierungsschaltung, die so konfiguriert ist, um einen Vektor von kumulierten Werten zu empfangen und so konfiguriert, um eine Funktion auf jeden kumulierten Wert anzuwenden, um einen Vektor von Aktivierungswerten zu erzeugen; und eine Normierungsschaltung, die mit der Aktivierungsschaltung gekoppelt ist und konfiguriert ist, um einen entsprechenden normierten Wert für jeden Aktivierungswert zu erzeugen.In general, an innovative aspect of the article as described in this specification may include a plurality of layers in a physical circuit for performing neural network calculations for a neural network, the circuit comprising: an activation circuit configured to generate a vector of receive cumulative values and configured to apply a function to each cumulative value to generate a vector of activation values; and a normalization circuit coupled to the activation circuit and configured to generate a corresponding normalized value for each activation value.
Implementierungen können eines oder mehrere der folgenden Merkmale beinhalten. Die Aktivierungsschaltung erhält den Vektor der kumulierten Werte von einem systolischen Array in der Schaltung. Die Normierungsschaltung umfasst eine Vielzahl von Normierungsregisterspalten, wobei jede Normierungsregisterspalte eine zusammenhängende Reihe bildet, wobei jede Normierungsregisterspalte so konfiguriert ist, um einen bestimmten Aktivierungswert zu empfangen, wobei eine entsprechende Normierungseinheit in der Normierungsregisterspalte konfiguriert ist, um den entsprechenden normierten Wert zu berechnen. Jede Normierungseinheit ist so konfiguriert, um den eindeutigen Aktivierungswert an eine benachbarte Normierungseinheit zu übergeben. Jede Normierungseinheit ist so konfiguriert, um: einen entsprechenden Aktivierungswert zu empfangen; aus dem entsprechenden Aktivierungswert einen entsprechenden Zwischenwert zu erzeugen; und den entsprechenden normierten Zwischenwert an eine oder mehrere benachbarte Normierungseinheiten zu übermitteln das Generieren des entsprechenden zwischengeschalteten normierten Wertes umfasst das Generieren einer Quadratzahl des entsprechenden Aktivierungswertes. Jede Normierungseinheit ist ferner so konfiguriert, um: von einer oder mehreren Normierungseinheiten einen oder mehrere normierte Zwischenwerte zu empfangen, die aus Aktivierungswerten erzeugt werden; jeden normierten Zwischenwert zu summieren, um einen Index zu erzeugen; einen Index zu verwenden, um auf einen oder mehrere Werte aus einer Nachschlagetabelle zuzugreifen; einen Skalierungsfaktor aus einem oder mehreren Werten und dem Index zu erzeugen; und um den entsprechenden normierten Wert aus dem Skalierungsfaktor und dem entsprechenden Aktivierungswert zu erzeugen. Eine Bündelschaltung ist so konfiguriert, um die normierten Werte zu empfangen und konfiguriert, um die normierten Werte zu bündeln und einen Bündelwert zu erzeugen. Die Bündelschaltung ist so konfiguriert, um die Vielzahl von normierten Werten in einer Vielzahl von Registern und einer Vielzahl von Speichereinheiten zu speichern, wobei die Vielzahl von Registern und die Vielzahl von Speicher in Serie miteinander verbunden sind, wobei jedes Register einen normierten Wert speichert und jede Speichereinheit eine Vielzahl von normierten Werten, wobei die Bündelschaltung so konfiguriert ist, dass sie nach jedem Taktzyklus einen gegebenen normierten Wert zu einem nachfolgenden Register oder einer Speichereinheit verschiebt, und wobei die Anordnung der Bündelschaltung so konfiguriert ist, um den Bündelwert aus den normierten Werten zu erzeugen. Eine Bündelschaltung ist so konfiguriert, um die normierten Werte zu empfangen und konfiguriert, um die normierten Werte zu bündeln und einen Bündelwert zu erzeugen. Die Bündelschaltung ist so konfiguriert, um die Vielzahl von Aktivierungswerten in einer Vielzahl von Registern und einer Vielzahl von Speichereinheiten zu speichern, wobei die Vielzahl von Registern und die Vielzahl von Speicher in Serie miteinander verbunden sind, wobei jedes Register einen normierten Wert speichert und jede Speichereinheit eine Vielzahl von Aktivierungswerten, wobei die Bündelschaltung so konfiguriert ist, dass sie nach jedem Taktzyklus einen gegebenen Aktivierungswert zu einem nachfolgenden Register oder einer Speichereinheit verschiebt, und wobei die Anordnung der Bündelschaltung so konfiguriert ist, um den Bündelwert aus den Aktivierungswerten zu erzeugen.Implementations may include one or more of the following features. The activation circuit receives the vector of the cumulative values from a systolic array in the circuit. The normalization circuit includes a plurality of normalization register columns, each normalization register column forming a contiguous series, each normalization register column configured to receive a particular activation value, wherein a corresponding normalization unit in the normalization register column is configured to calculate the corresponding normalized value. Each normalization unit is configured to pass the unique activation value to an adjacent normalization unit. Each normalization unit is configured to: receive a corresponding activation value; to generate a corresponding intermediate value from the corresponding activation value; and transmitting the corresponding normalized intermediate value to one or more adjacent normalization units generating the corresponding intermediate normalized value comprises generating a square number of the corresponding activation value. Each normalization unit is further configured to: receive from one or more normalization units one or more normalized intermediate values generated from activation values; to sum each normalized intermediate value to produce an index; use an index to access one or more values from a lookup table; create a scaling factor of one or more values and the index; and to generate the corresponding normalized value from the scaling factor and the corresponding activation value. A burst circuit is configured to receive the normalized values and configured to collimate the normalized values and generate a burst value. The burst circuit is configured to store the plurality of normalized values in a plurality of registers and a plurality of memory units, wherein the plurality of registers and the plurality of memories are connected in series with each register storing a normalized value and each Storage unit having a plurality of normalized values, wherein the burst circuitry is configured to shift a given normalized value to a subsequent register or memory unit after each clock cycle, and wherein the array of the burst circuitry is configured to receive the burst value from the normalized values produce. A burst circuit is configured to receive the normalized values and configured to collimate the normalized values and generate a burst value. The burst circuit is configured to store the plurality of activation values in a plurality of registers and a plurality of memory units, wherein the plurality of registers and the plurality of memories are connected in series with each register storing a normalized value and each memory unit a plurality of activation values, wherein the burst circuit is configured to shift a given activation value to a subsequent register or memory unit after each clock cycle, and wherein the arrangement of the trunk circuit is configured to generate the trunk value from the activation values.
Bestimmte Ausführungsformen des in dieser Spezifikation beschriebenen Gegenstands können so implementiert werden, dass sie einen oder mehrere der folgenden Vorteile verwirklichen. Während einem gegebenen Taktzyklus können mehrere Aktivierungswerte für jede neuronale Netzwerkschicht eines neuronalen Netzwerkes errechnet werden. Während einem anderen Taktzyklus kann der Prozessor optional mehrere normierte Werte erzeugen. Der Prozessor kann auch optional gebündelte Werte aus den normierten Werten oder den Aktivierungswerten erzeugen. Der Prozessor ist in der Lage, in jedem Taktzyklus eine kumulierte Summe zu empfangen und in jedem Taktzyklus ein aktiviertes, normiertes und gebündeltes Ergebnis zu erzeugen und somit die Berechnungen parallel durchgeführt werden.Certain embodiments of the subject matter described in this specification may be implemented to achieve one or more of the following advantages. While For a given clock cycle, multiple activation values can be calculated for each neural network layer of a neural network. During another clock cycle, the processor may optionally generate a plurality of normalized values. The processor may also optionally generate clustered values from the normalized values or the activation values. The processor is capable of receiving a cumulative sum every clock cycle and generating an activated, normalized and clustered result in each clock cycle, thus performing the calculations in parallel.
Die Erfindung umfasst auch entsprechende Verfahren zum Betreiben eines neuronalen Netzes.The invention also includes corresponding methods for operating a neural network.
Die Details einer oder mehrerer Ausführungen eines Gegenstands dieser Spezifikation sind in den begleiteten Bildern und der Beschreibung unten veranschaulicht. Andere Merkmale, Aspekte und Vorteile des Gegenstands werden aus der Beschreibung, den Zeichnungen und den Ansprüchen deutlich.The details of one or more embodiments of an article of this specification are illustrated in the accompanying drawings and the description below. Other features, aspects and advantages of the subject matter will be apparent from the description, drawings and claims.
KURZBESCHREIBUNG DER ZEICHNUNGENBRIEF DESCRIPTION OF THE DRAWINGS
Entsprechende Referenznummern und Kennzeichnungen in den verschiedenen Zeichnungen zeigen entsprechende Elemente an.Corresponding reference numbers and markings in the various drawings indicate corresponding elements.
DETAILLIERTE BESCHREIBUNGDETAILED DESCRIPTION
Ein neuronales Netzwerk mit mehreren Schichten kann verwendet werden, um Inferenzen zu berechnen. So kann das neuronale Netzwerk beispielsweise bei einer Eingabe, eine Inferenz für die Eingabe berechnen. Insbesondere können die Schichten des neuronalen Netzwerks sind in einer Reihe mit jeweils einem Gewichtssatz in einer Sequenz angeordnet sein. Insbesondere die Schichten des neuronalen Netzes sind mit jeweils einem entsprechenden Satz von Gewichten in einer Sequenz angeordnet. Jede Schicht empfängt eine Eingabe und verarbeitet die Eingabe entsprechend den Gewichtssätzen für die Schicht, um eine Ausgabe zu erzeugen.A multi-layered neural network can be used to calculate inferences. For example, in an input, the neural network may calculate an inference for the input. In particular, the layers of the neural network may be arranged in a row each having a weight set in a sequence. In particular, the layers of the neural network are each arranged with a corresponding set of weights in a sequence. Each layer receives an input and processes the input according to the weight sets for the layer to produce an output.
Daher empfängt das neuronale Netzwerk die Eingabe und verarbeitet diese zum Erzeugen der Inferenz in der Sequenz durch jede der neuronalen Netzwerkschichten, wobei der Ausgang einer Netzwerkschicht als Eingang für die nächste neuronale Netzwerkschicht vorgesehen ist. Dateneingaben zu einer neuronalen Netzwerkschicht, z. B. können entweder als Eingabe für das neuronale Netzwerk oder als Ausgang der Schicht der darunterliegenden Schicht in der Sequenz zu einer neuronalen Netzwerkschicht als Aktivierungseingaben für die Schicht bezeichnet werden.Therefore, the neural network receives the input and processes it to generate the inference in the sequence through each of the neural network layers, with the output of one network layer being provided as input to the next neural network layer. Data input to a neural network layer, e.g. For example, may be referred to as either an input to the neural network or an output of the layer of the underlying layer in the sequence to a neural network layer as activation inputs for the layer.
Bei einigen Implementierungen sind die Schichten des neuronalen Netzwerks in einem gerichteten Graph angeordnet. Das heißt, jede spezielle Schicht kann mehrere Eingänge, mehrere Ausgänge oder beide empfangen. Die Schichten des neuronalen Netzes können auch so angeordnet sein, dass ein Ausgang einer Schicht als Eingang in eine vorhergehende Schicht zurückgesendet werden kann.In some implementations, the layers of the neural network are arranged in a directional graph. That is, each particular layer may receive multiple inputs, multiple outputs, or both. The layers of the neural network may also be arranged such that an output of a layer can be returned as an input to a previous layer.
Einige neuronale Netzwerke normieren Ausgänge von einer oder mehreren neuronalen Netzwerkschichten, um normierte Werte zu erzeugen, die als Eingaben für nachfolgende neuronale Netzwerkschichten verwendet werden. Eine Normierung der Ausgänge kann dazu beitragen, dass die normierten Werte für die Eingänge der nachfolgenden neuronalen Netzwerkschichten, innerhalb der erwarteten Domain bleiben. Dies kann Fehler in der Inferenzberechnung reduzieren.Some neural networks normalize outputs from one or more neural network layers to produce normalized values that are used as inputs to subsequent neural network layers. Normalization of the outputs may help to keep the normalized values for the inputs of the subsequent neural network layers within the expected domain. This can reduce errors in the inference calculation.
Einige neuronale Netzwerke bündeln Ausgänge von einer oder mehreren neuronalen Netzwerkschichten, um gebündelte Werte zu erzeugen, die als Eingaben für nachfolgende neuronale Netzwerkschichten verwendet werden. Bei einigen Implementierungen bündelt das neuronale Netzwerk eine Gruppe von Ausgaben durch bestimmen einer maximalen oder durchschnittlichen Ausgabe der Gruppe und das Maximum oder der Durchschnitt der gebündelten Ausgabe für die Gruppe verwendet wird. Das Bündeln der Ausgänge kann eine gewisse räumliche Invarianz beibehalten, so dass die in verschiedenen Konfigurationen angeordneten Ausgänge verarbeitet werden können, um über dieselbe Inferenz zu verfügen. Das Bündeln der Ausgänge der Ausgänge kann auch die Dimensionalität der an den nachfolgenden neuronalen Netzwerkschichten empfangenen Eingaben verringern, während die gewünschten Merkmale vor dem Bündeln beibehalten werden, was, ohne die Qualität der durch die neuronalen Netzwerke erzeugten Inferenzen signifikant zu beeinträchtigen, die Effizienz verbessern kann.Some neural networks bundle outputs from one or more neural network layers to produce clustered values that are used as inputs to subsequent neural network layers. In some implementations, the neural network bundles a group of outputs by determining a maximum or average output of the group and using the maximum or average of the bundled output for the group. The Bundling the outputs can maintain some spatial invariance so that the outputs arranged in different configurations can be processed to have the same inference. Bundling the outputs of the outputs can also reduce the dimensionality of the inputs received at the subsequent neural network layers while maintaining the desired features before bundling, which can improve efficiency without significantly affecting the quality of the inferences generated by the neural networks ,
Diese Spezifikation beschreibt spezielle Hardwareschaltungen, die optional Normierung, Bündelung oder beides auf Ausgängen einer oder mehrerer neuronaler Netzwerkschichten durchführen.This specification describes specific hardware circuits that optionally perform normalization, collimation, or both on outputs of one or more neural network layers.
Das System empfängt für die gegebene Schicht Gewichtseingangssätze (Schritt
Das System erzeugt aus den Gewichtseingaben und den Aktivierungseingaben mithilfe einer Matrixmultiplikationseinheit der speziellen Hardwareschaltung (Schritt
Das System kann unter Verwendung einer Vektor-Recheneinheit der speziellen Hardware-Schaltung eine Schichtausgabe von den kumulierten Werten (Schritt
Die Host-Schnittstelle
Die Host-Schnittstelle
Die einheitliche Puffer
Der dynamische Speicher
Die Matrix-Recheneinheit
In dem dargestellten Beispiel senden die Wertlader
Die Wertlader
Die Gewichtsabrufschnittstelle
Bei einigen Implementierungen kann eine Hostschnittstelle, z. B. die Hostschnittstelle
Auf jedem Taktzyklus kann jede Zelle eine gegebene Gewichtseingabe, eine gegebene Aktivierungseingabe und eine kumulierte Ausgabe von einer benachbarten Zelle verarbeiten, um eine kumulierte Ausgabe zu erzeugen. Die kumulierte Ausgabe kann auch an die benachbarte Zelle entlang der gleichen Dimension wie die gegebene Gewichtseingabe weitergegeben werden. Nachfolgend weiter unten wird unter Bezugnahme auf
Die kumulierte Ausgabe kann entlang der gleichen Spalte wie die Gewichtseingabe, z. B. in Richtung des Bodens der Spalte in dem Array
Die Zelle kann ein Aktivierungsregister
Die Summierungsschaltung kann das Produkt und den kumulierten Wert aus der Summe im Register
Die Zelle kann auch die Gewichtseingabe und die Aktivierungseingabe zur Verarbeitung zu benachbarten Zellen verschieben. So kann beispielsweise das Gewichtsregister
Bei einigen Implementierungen enthält die Zelle auch ein Steuerregister. Das Steuerregister kann ein Steuersignal speichern, das bestimmt, ob die Zelle entweder die Gewichtseingabe oder die Aktivierungseingabe zu benachbarten Zellen verschieben soll. Bei einigen Implementierungen erfordert das Verschieben der Gewichtseingabe oder des Aktivierungseingangs einen oder mehrere Taktzyklen. Das Steuersignal kann weiterhin bestimmen, ob die Aktivierungseingabe oder Gewichtseingaben an die Multiplikationsschaltung
Bei einigen Implementierungen werden die Gewichte in ein Gewichtspfadregister
Die Vektor-Recheneinheit
Optional kann die Vektor-Recheneinheit
Optional kann die Vektor-Recheneinheit
Steuersignale
Die Vektor-Recheneinheit
Bei einigen Implementierungen empfängt die Bündeleinheit
Bei einigen Implementierungen umfasst der Vektor der aktivierten Werte aktivierte Werte, die durch Anwenden einer Aktivierungsfunktion auf kumulierte Werte erzeugt werden, die von Aktivierungseingängen auf der Grundlage eines Satzes von Gewichtseingaben erzeugt werden.In some implementations, the activated value vector includes enabled values that are generated by applying an activation function to accumulated values generated from activation inputs based on a set of weight inputs.
Bei einigen anderen Implementierungen werden die aktivierten Werte für den Satz von Gewichtseingaben über mehrere aktivierte Vektorwerte, aufgrund von Verzögerungen gestaffelt, die verursacht werden, wenn Aktivierungs- und Gewichtseingaben verschoben werden. So kann beispielsweise eine Matrix-Recheneinheit kumulierte Werte A0–An aus einem Satz von Aktivierungseingaben und einem Satz von Gewichtseingaben aus Kernel A, kumulierte Werte B0–Bn aus einem Satz von Aktivierungseingaben und einen Satz von Gewichtseingaben aus Kernel B erzeugen, und kumulierte Werte C0–Cn aus einem Satz von Aktivierungseingaben und einem Satz von Gewichtseingaben von Kernel C. Die kumulierten Werte A0–An und B0–Bn können über darauffolgende Taktzyklen erzeugt werden, da Gewichtseingaben und Aktivierungseingaben über die Matrix-Recheneinheit vor der Berechnung der entsprechenden kumulierten Werte verschoben werden, wie vorstehend unter Bezugnahme auf
Als Ergebnis davon können die Mehrfachvektoren von kumulierten Werte zu mehreren Vektoren von aktivierten Werten werden, z. B. nach der Verarbeitung durch die Aktivierungsschaltung
Bei einigen Implementierungen sendet die Schaltung die Vektoren an die Quadrierungseinheit, bevor die Vektoren der aktivierten Werte an die Normierungsregisterspalte geliefert werden. Wie nachfolgend beschrieben wird, kann die Quadrierungseinheit für die Verwendung bei der Berechnung der normierten Werte, eine Quadratzahl jedes aktivierten Wertes errechnen. Die Quadrierungseinheit kann Vektoren von quadrierten aktivierten Werten erzeugen, d. h. einen für jeden Vektor von aktivierten Wertes, und die Vektoren der quadrierten aktivierten Werte an die Normierungsregisterspalten senden Bei einigen Implementierungen sendet die Quadrierungseinheit sowohl die Vektoren der aktivierten Werte als auch die Vektoren der quadrierten aktivierten Werte an die Normierungsregisterspalten.In some implementations, the circuit sends the vectors to the squaring unit before delivering the vectors of the activated values to the normalization register column. As will be described below, the squaring unit for use in calculating the normalized values may calculate a squared number of each activated value. The squaring unit may generate vectors of squared activated values, i. H. send one for each vector of the activated value, and the vectors of the squared activated values to the normalization register columns In some implementations, the squaring unit sends both the vectors of the activated values and the vectors of the squared activated values to the normalization register columns.
Bei einigen Implementierungen bildet die Normierungsschaltung auf der Grundlage eines Radiusparameters einer Normierung gestaffelte Gruppen, z. B. gestaffelte Gruppen
Bei einigen Implementierungen verwenden Normierungseinheiten, z. B. Normierungseinheiten
Die Normierungsschaltung kann einen normierten Wert für einen aktivierten Wert, basierend auf den gestaffelten Gruppen erzeugen. So kann beispielsweise der Normierungswert für einen in dem Normierungsregister
Die Normierungsschaltung kann aus den erzeugten normierten Werten einen Vektor von normierten Werten bilden, z. B. die in den Normierungseinheiten gespeichert werden können, und kann den Vektor von normierten Werten zu einer Bündelschaltung senden, wenn diese durch einen neuronalen Netzwerkparameter oder einen einheitlichen Zwischenspeicher bestimmt wird.The normalization circuit may form from the generated normalized values a vector of normalized values, e.g. Which may be stored in the normalization units, and may send the vector from normalized values to a burst circuit when determined by a neural network parameter or a unique cache.
Die Normierungsregister
Wie in der Figur veranschaulicht, werden aktivierte Werte gestaffelt geladen. Auf dem Taktzyklus 0 können zum Beispiel die aktivierten Werte A0,0, A1,0 und A2,0 berechnet werden, aber die Normierungsschaltung lädt die drei aktivierten Werte über drei Taktzyklen. Bei einigen Implementierungen werden die aktivierten Werte nicht gestaffelt geladen. Das heißt, A0,0, A1,0 und A2,0 können in einem Taktzyklus geladen werden. As illustrated in the figure, activated values are loaded staggered. For example, on the
N0 kann ein normierter Wert für A0,1 sein, der im Normierungsregister
Die Normierungsschaltung kann mithilfe eines Radius von 1 normierte Werte für jeden aktivierten Wert berechnen. Weitere Radien sind möglich. Wenn die Normierungsschaltung die für eine Normierungsberechnung benötigten aktivierten Werte noch nicht geladen hat, kann die Normierungsschaltung den aktivierten Wert in ein darauffolgendes Normierungsregister verschieben, bis die erforderlichen aktivierten Werte geladen sind. Um beispielsweise einen normierten Wert für den aktivierten Wert A0,2, der in dem Normierungsregister
Bei einigen Implementierungen wird der aktivierte Wert an die Quadrierschaltung
Wie vorstehend unter Bezugnahme auf
Die Normierungseinheit kann auch aktivierte Quadratwerte
Die Summe kann zu einer Speichereinheit
Bei einigen Implementierungen ist die Quadratsumme ein 12-Bit-Wert. Die Normierungseinheit kann die oberen 4 Bits der Quadratsumme als Index für die Nachschlagetabelle verwenden. Die oberen 4 Bits können verwendet werden, um auf Koeffizienten zuzugreifen, z. B. die von einem Benutzer aus der Nachschlagetabelle festgelegt sind. Bei einigen Implementierungen greifen die oberen 4 Bits auf 2 12-Bit-Koeffizienten zu: A & B. Die unteren acht Bits können ein Delta sein, das in einer Gleichung zur Berechnung des Normierungs-Skalierungsfaktors verwendet wird. Durch den Skalierungsfaktor = Minimum (1048575, [A·delta + B·256 + 2^7]) >> 8 ist ein Beispiel der Gleichung gegeben, wobei mindestens zwei Argumente verarbeitet werden und das Argument mit dem Minimalwert zurückgegeben wird.In some implementations, the sum of squares is a 12-bit value. The normalization unit may use the upper 4 bits of the sum of squares as the index for the lookup table. The upper 4 bits can be used to access coefficients, e.g. As determined by a user from the lookup table. In some implementations, the upper 4 bits access 2 12-bit coefficients: A & B. The lower eight bits may be a delta used in an equation to calculate the scale scaling factor. The scaling factor = minimum (1048575, [A · delta + B · 256 + 2 ^ 7]) >> 8 gives an example of the equation where at least two arguments are processed and the argument is returned with the minimum value.
Um einen normierten Wert zu generieren, kann die Normierungseinheit mithilfe der Multiplikationseinheit
Die Bündelschaltung kann eine Abfolge von Elementen von dem Vektor von normierten Werten empfangen, z. B. aus der Normierungsschaltung
Die Bündelschaltung kann eine Reihe von Registern und Speichereinheiten umfassen. Jedes Register kann einen Ausgang an die Aggregationsschaltung
Ein erster normierter Wert kann an das Register
Nach vier Taktzyklen werden innerhalb der ersten vier Register
Die Aggregationsschaltung
Die Bündelschaltung kann aus den aufgerufenen normierten Werten mithilfe der Aggregationsschaltung
Nach dem Generieren des ersten gebündelten Wertes kann die Bündelschaltung weiterhin gebündelte Werte erzeugen, indem die normierten Werte durch jedes Register verschoben werden, sodass neue normierte Werte in den Registern gespeichert werden und von der Aggregationsschaltung
Die Aggregationsschaltung
Wie vorstehend unter Bezugnahme auf
Ausführungsformen des Gegenstands und die in dieser Spezifikation beschriebenen funktionalen Tätigkeiten können in digitalen elektronischen Schaltungen oder in einer konkret darin verkörperten Computer-Software, Firmware in die Hardware implementiert werden, einschließlich der in dieser Spezifikation offenbarten Strukturen und ihrer strukturellen Entsprechungen oder in Kombinationen von einer oder mehrerer von ihnen. Ausführungsformen des in dieser Spezifikation beschriebenen Gegenstands können als ein oder mehrere Computerprogramme, d. h. als ein oder mehrere Module von Computerprogrammanweisungen implementiert werden, die auf einem Computer-Speichermedium für die Durchführung durch oder die Steuerung der Operation des datenverarbeitenden Apparats kodiert werden. Alternativ oder zusätzlich können die Programmanweisungen auf einem erzeugten propagierten Signal, z. B. einem maschinell erzeugten elektrischen, optischen oder elektromagnetischen Signal künstlich kodiert werden, das erzeugt wird, um Informationen für die Übertragung auf einen geeigneten Empfängerapparat für die Durchführung durch einen datenverarbeitenden Apparat zu kodieren. Bei einem Computer-Speichermedium kann es sich um ein maschinell lesbares Speichergerät, einen maschinell lesbaren Speicherträger, ein zufälliges oder serielles Speicher-Array oder Speichergerät oder um eine Kombination aus einem oder mehreren dieser Geräte handeln oder in ihnen enthalten sein.Embodiments of the subject matter and the functional activities described in this specification may be implemented in hardware in digital electronic circuits or in computer software embodied therein, including but not limited to structures disclosed in this specification and their structural equivalents, or combinations of one or more embodiments several of them. Embodiments of the subject matter described in this specification may be implemented as one or more computer programs, ie, one or more modules of computer program instructions encoded on a computer storage medium for performance by or control of the operation of the data processing apparatus. Alternatively or additionally, the program instructions may be based on a generated propagated signal, e.g. A machine-generated electrical, optical or electromagnetic signal which is generated to encode information for transmission to a suitable receiver apparatus for execution by a data processing apparatus. A computer storage medium may be or may be included in a machine-readable storage device, a machine-readable storage medium, a random or serial storage array or storage device, or a combination of one or more of these devices.
Der Begriff „Datenverarbeitungsvorrichtung” umfasst alle Arten von Vorrichtungen, Geräten und Maschinen zum Verarbeiten von Daten, einschließlich beispielsweise eines programmierbaren Prozessors, eines Rechners oder mehrerer Prozessoren oder Rechner. Die Vorrichtung kann spezielle Logikschaltungen umfassen, z. B. ein FPGA (Field Programmable Gate Array-programmierbare Hardware-Logik) oder ein ASIC (anwendungsspezifische integrierte Schaltung). Der Apparat kann neben der Hardware auch einen Code einschließen, der eine Durchführungsumgebung für das betreffende Computerprogramm in der Frage erstellt, z. B. einen Code, der Prozessor-Firmware, einen Protokollstapel, ein Datenbankverwaltungssystem, ein Betriebssystem, oder eine Kombination einer oder mehrerer der genannten.The term "data processing device" encompasses all types of devices, devices and machines for processing data including, for example, a programmable processor, a computer or multiple processors or computers. The device may include special logic circuits, e.g. As an FPGA (Field Programmable Gate Array programmable hardware logic) or an ASIC (application-specific integrated circuit). The apparatus may include, in addition to the hardware, a code that provides an execution environment for the particular computer program in question, e.g. A code, the processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of the cited.
Ein Computerprogramm (auch bezeichnet als Programm, Software, Softwareanwendung, Script oder Code) kann in einer beliebigen Form von Programmiersprache geschrieben sein, einschließlich kompilierter oder interpretierter Sprachen, deklarativer oder verfahrensorientierter Sprachen, und das Programm kann in jeder beliebigen Form eingesetzt sein, darunter als unabhängiges Programm oder als ein Modul, eine Komponente, eine Subroutine, ein Objekt oder eine andere Einheit, die zur Benutzung in einer Rechenumgebung geeignet ist. Ein Computerprogramm kann, muss aber nicht, einer Datei in einem Dateisystem entsprechen. Ein Programm kann in einem Teil einer Datei gespeichert sein, die andere Programme oder Daten enthält z. B. ein oder mehrere Scripts, die in einem Dokument in Markup-Sprache gespeichert sind), in einer einzelnen Datei speziell für das betreffende Programm oder in mehreren koordinierten Dateien z. B. Dateien, die ein oder mehrere Module, Unterprogramme oder Teile von Code speichern. Ein Computerprogramm kann auf einem Computer oder mehreren Computern eingerichtet sein oder ausgeführt werden, die an einem Standort angeordnet sind oder über mehrere Standorte verteilt sind und über ein Kommunikationsnetz verbunden sind.A computer program (also referred to as program, software, software application, script or code) may be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and the program may be in any form, including as independent program or as a module, component, subroutine, object or other entity suitable for use in a computing environment. A computer program may or may not be equivalent to a file in a file system. A program may be stored in a portion of a file containing other programs or data, e.g. One or more scripts that are stored in a markup language document), in a single file specific to that program, or in several coordinated files, e.g. For example, files that store one or more modules, subprograms, or pieces of code. A computer program may be configured or executed on one or more computers located at one site or distributed over multiple sites and connected via a communications network.
Die in dieser Beschreibung dargestellten Prozesse und Logik-Abläufe können durch einen oder mehrere programmierbare Prozessoren durchgeführt werden, die ein oder mehrere Computerprogramme ausführen, um Funktionen durch das Arbeiten mit Eingabedaten und das Erzeugen von Ausgaben auszuführen. Die Prozesse und die logischen Abläufe können auch durch logische Sonderzweckschaltungen durchgeführt werden, und der Apparat kann als Sonderzweckschaltungen implementiert werden, z. B. ein FPGA (Field Programmable Gate Array) oder eine ASIC (anwendungsspezifische integrierte Schaltung).The processes and logic operations depicted in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating outputs. The processes and logical operations may also be performed by special purpose logic circuits, and the apparatus may be implemented as special purpose circuits, e.g. As an FPGA (Field Programmable Gate Array) or an ASIC (application-specific integrated circuit).
Computer, die zur Ausführung eines Datenverarbeitungsprogramms geeignet sind, können beispielsweise allgemeine oder spezielle Mikroprozessoren oder beides oder jede andere Art einer Zentraleinheit beinhalten. Ganz allgemein nimmt eine zentrale Recheneinheit Anweisungen und Daten von einem Festwertspeicher oder einem Arbeitsspeicher oder von beiden entgegen. Die wesentlichen Elemente eines Computers sind eine zentrale Recheneinheit für das Durchführen von Tätigkeiten gemäß Anweisungen und ein oder mehr Speichergeräte für das Speichern von Anweisungen und Daten. Im Allgemeinen beinhaltet ein Computer eine oder mehrere Massenspeichergeräte für das Speichern von Daten, z. B. Magnet-, magnetooptische oder optische Disketten, um Daten zu empfangen und/oder zu übertragen. Jedoch muss ein Computer solche Geräte nicht haben. Ferner kann ein Computer in einem anderen Gerät eingebettet sein, z. B. in einem Mobiltelefon, einem Organizer (PDA), einem mobilen Audio- oder Videoplayer, einer Spielkonsole, einem Funknavigationsempfänger (GPS) oder einem tragbaren Speichergerät, z. B. in einem USB-Stick, um nur einige zu nennen.For example, computers suitable for executing a data processing program may include general purpose or specialized microprocessors, or both or any other type of central processing unit. More generally, a central processing unit receives instructions and data from read-only memory or memory, or both. The essential elements of a computer are a central processing unit for performing activities in accordance with instructions and one or more storage devices for storing instructions and data. In general, a computer includes one or more mass storage devices for storing data, e.g. As magnetic, magneto-optical or optical disks to receive and / or transmit data. However, a computer does not have to have such devices. Furthermore, a computer may be embedded in another device, e.g. In a mobile phone, an organizer (PDA), a mobile audio or video player, a game console, a radio navigation receiver (GPS) or a portable storage device, e.g. In a USB stick, just to name a few.
Computerlesbare Medien, die für das Speichern von Computerprogrammanweisungen und -daten geeignet sind, schließen alle Formen von Permanentspeichern, Medien- und Speichergeräten ein, einschließlich beispielsweise Halbleiter-Speichergeräte, z. B. EPROM, EEPROM und USB-Flash-Speicher; Magnetdisketten, z. B. interne Festplatten oder herausnehmbare Disketten; magnetooptische Disketten; und CD-ROMs und DVD-ROMs. Der Prozessor und der Speicher können durch logische Sonderzweckschaltungen ergänzt werden oder darin eingebaut sein.Computer readable media suitable for storing computer program instructions and data includes all forms of non-volatile memory, media and storage devices including, for example, semiconductor memory devices, e.g. EPROM, EEPROM and USB flash memory; Magnetic disks, z. Internal hard drives or removable floppy disks; magneto-optical diskettes; and CD-ROMs and DVD-ROMs. The processor and memory may be supplemented or incorporated by special purpose logic circuits.
Um die Interaktion mit einem Benutzer zu ermöglichen, können in dieser Spezifikation beschriebene Ausführungsformen des Gegenstands auf einem Computer mit einem Anzeigegerät implementiert werden, z. B. einem CRT-(Kathodenstrahlröhre) oder LCD-(Flüssigkristallanzeige)Monitor, mit welchem dem Benutzer Informationen angezeigt werden, sowie einer Tastatur und einem Anzeigegerät, z. B. einer Maus oder einem Trackball, mit denen der Benutzer Eingaben in den Computer vornehmen kann. Es können auch andere Arten von Einrichtungen verwendet werden, um für eine Interaktion mit einem Nutzer zu sorgen; beispielsweise kann eine dem Benutzer gelieferte Rückkopplung beliebiger Form von sensorischer Rückkopplung vorliegen, z. B. eine visuelle Rückkopplung, auditive Rückkopplung oder taktile Rückkopplung; und die Eingabe von dem Nutzer kann in beliebiger Form empfangen werden, einschließlich akustischer, Sprach- oder taktiler Eingabe. Darüber hinaus kann ein Computer über das Senden von Dokumenten an und das Empfangen von Dokumenten von einer Einrichtung, die vom Benutzer verwendet wird, mit einem Benutzer interagieren; beispielsweise über das Senden von Webpages an einen Webbrowser auf dem Clientgerät des Benutzers als Antwort auf die vom Webbrowser empfangenen Aufforderungen.To facilitate interaction with a user, embodiments of the subject matter described in this specification may be implemented on a computer with a display device, e.g. As a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, with which the user information is displayed, and a keyboard and a display device, eg. As a mouse or a trackball, with which the user can make inputs to the computer. Other types of devices may be used to provide for interaction with a user; For example, a user-supplied feedback of any form of sensory feedback may be present, e.g. A visual feedback, auditory feedback or tactile feedback; and the input from the user may be received in any form, including acoustic, voice or tactile input. In addition, a computer may interact with a user by sending documents to and receiving documents from a device used by the user; for example, by sending webpages to a web browser on the user's client device in response to prompts received from the web browser.
Ausführungsformen der in dieser Spezifikation betrachteten Gegenstands können in ein Computersystem implementiert werden, das eine Backend-Komponente (z. B. einen Datenserver), oder eine Middleware-Komponente (z. B. einen Anwendungsserver), oder eine Frontend-Komponente (z. B. einen Client-Computer mit graphischem Benutzerinterface oder Webbrowser) umfasst, worüber der Benutzer mit einer Implementierung der in dieser Spezifikation betrachteten Gegenstands interagieren kann, oder eine beliebige Kombination aus solchen Backend, Middleware- oder Frontend-Komponenten. Die Komponenten des Systems können durch eine beliebige Form oder ein beliebiges Medium digitaler Datenkommunikation miteinander verbunden sein, z. B. ein Kommunikationsnetz. So beinhalten beispielsweise Kommunikationsnetzwerke ein lokales Netzwerk („LAN”), ein Fernnetz („WAN”), z. B. das Internet.Embodiments of the subject matter contemplated in this specification may be implemented in a computer system that includes a back-end component (eg, a data server), or a middleware component (eg, an application server), or a front-end component (e.g. A client computer with graphical user interface or web browser), which allows the user to interact with an implementation of the subject matter considered in this specification, or any combination of such backend, middleware or frontend components. The components of the system may be interconnected by any form or medium of digital data communication, e.g. B. a communication network. For example, communication networks include a local area network ("LAN"), a long distance network ("WAN"), e.g. For example, the Internet.
Das Rechensystem kann Client und Server beinhalten. Ein Client und Server befinden sich im Allgemeinen ortsfern voneinander und interagieren typischerweise über ein Kommunikationsnetz. Die Beziehung zwischen Client und Server entsteht aufgrund von Computerprogrammen, die auf den jeweiligen Computern laufen und die eine Client-Server-Beziehung zueinander haben.The computing system may include client and server. A client and server are generally remote from each other and typically interact over a communications network. The relationship between client and server arises due to computer programs running on the respective computers and having a client-server relationship with each other.
Zwar enthält diese Spezifikation viele spezifische Implementierungsdetails, jedoch sollten diese nicht als Beschränkungen des Umfangs oder des Anspruchs ausgelegt werden, sondern vielmehr als Beschreibungen spezifischer Merkmale bestimmter Ausführungsformen bestimmter Erfindungen. Bestimmte Merkmale, die in dieser Spezifikation im Kontext der unterschiedlichen Ausführungsformen beschrieben werden, können auch in Kombination in einer einzelnen Ausführungsform implementiert werden. Andererseits können verschiedene Merkmale, die im Kontext einer einzelnen Ausführungsform beschrieben werden, in mehreren Ausführungsformen oder in jeder geeigneten Unterkombination implementiert werden. Außerdem können ein oder mehrere Merkmale einer beanspruchten Kombination in einigen Fällen aus der Kombination herausgelöst werden, auch wenn die Merkmale vorstehend als in gewissen Kombinationen funktionierend beschrieben oder gar als eine Kombination beansprucht werden, und die beanspruchte Kombination kann an eine Unterkombination oder eine Variation einer Unterkombination verwiesen werden.While this specification contains many specific implementation details, these should not be construed as limitations on the scope or on the claims, but rather as descriptions of specific features of particular embodiments of particular inventions. Certain features described in this specification in the context of the various embodiments may also be implemented in combination in a single embodiment. On the other hand, various features described in the context of a single embodiment may be implemented in multiple embodiments or in any suitable subcombination. In addition, one or more features of a claimed combination may in some instances be released from the combination, even if the features are described above as functioning in some combinations or even claimed as a combination, and the claimed combination may be attached to a subcombination or variation of a subcombination to get expelled.
Ebenso werden Tätigkeiten in den Zeichnungen zwar in einer bestimmten Reihenfolge dargestellt, aber dies sollte nicht als Anfordernis verstanden werden, dass solche Tätigkeiten in der bestimmten gezeigten Reihenfolge oder in einer aufeinanderfolgenden Reihenfolge ausgeführt werden müssen oder dass alle dargestellten Tätigkeiten ausgeführt werden müssen, um erwünschte Ergebnisse zu erzielen. Unter bestimmten Umständen können Multitasking und eine Parallelbearbeitung vorteilhaft sein. Darüber hinaus sollte die Trennung verschiedener Systemkomponenten in den vorstehend beschriebenen Ausführungsformen nicht in allen Ausführungsformen erforderlich aufgefasst werden, und es versteht sich, dass die beschriebenen Programmkomponenten und Systeme im Allgemeinen zusammen in ein einziges Softwareprodukt integriert oder zu mehreren Softwareprodukten verkapselt werden können.Also, although activities in the drawings are presented in a particular order, this should not be construed as requiring that such activities be performed in the particular order shown or in a sequential order, or that all activities depicted must be performed to produce desired results to achieve. Under certain circumstances, multitasking and parallel processing can be beneficial. Moreover, the separation of various system components in the embodiments described above should not be construed as required in all embodiments, and it should be understood that the described program components and systems generally can be integrated together into a single software product or encapsulated into multiple software products.
Folglich wurden bestimmte Ausführungsformen des Gegenstands beschrieben. Weitere Ausführungsformen gehören zum Umfang der folgenden Ansprüche. Die in den Ansprüchen ausgeführten Vorgänge können beispielsweise in einer anderen Reihenfolge ausgeführt werden und dennoch gewünschte Ergebnisse erzielen. Die in den beigefügten Figuren dargestellten Verfahren erfordern beispielsweise nicht notwendigerweise die gezeigte Reihenfolge oder sequentielle Reihenfolge, um erwünschte Ergebnisse zu erzielen. Bei bestimmten Implementierungen können Multitasking und eine Parallelbearbeitung vorteilhaft sein.Thus, certain embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the operations set forth in the claims may be performed in a different order and still achieve desired results. For example, the methods illustrated in the attached figures do not necessarily require the order shown or sequential order to achieve desired results. In certain implementations, multitasking and parallel processing may be beneficial.
Claims (12)
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562165022P | 2015-05-21 | 2015-05-21 | |
US62/165,022 | 2015-05-21 | ||
US14/845,117 US10192162B2 (en) | 2015-05-21 | 2015-09-03 | Vector computation unit in a neural network processor |
US14/845,117 | 2015-09-03 |
Publications (1)
Publication Number | Publication Date |
---|---|
DE202016107442U1 true DE202016107442U1 (en) | 2017-01-18 |
Family
ID=56069218
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE112016002296.4T Pending DE112016002296T5 (en) | 2015-05-21 | 2016-04-29 | VECTOR CONTROL UNIT IN A NEURONAL NETWORK PROCESSOR |
DE202016107442.8U Active DE202016107442U1 (en) | 2015-05-21 | 2016-04-29 | Vector arithmetic unit in a neural network processor |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE112016002296.4T Pending DE112016002296T5 (en) | 2015-05-21 | 2016-04-29 | VECTOR CONTROL UNIT IN A NEURONAL NETWORK PROCESSOR |
Country Status (12)
Country | Link |
---|---|
US (4) | US10192162B2 (en) |
EP (2) | EP4276690A1 (en) |
JP (4) | JP6615902B2 (en) |
KR (4) | KR102516092B1 (en) |
CN (2) | CN113392964A (en) |
DE (2) | DE112016002296T5 (en) |
DK (1) | DK3298545T3 (en) |
FI (1) | FI3298545T3 (en) |
GB (2) | GB2600290A (en) |
HK (1) | HK1245954A1 (en) |
TW (2) | TWI591490B (en) |
WO (1) | WO2016186813A1 (en) |
Families Citing this family (82)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10192162B2 (en) | 2015-05-21 | 2019-01-29 | Google Llc | Vector computation unit in a neural network processor |
GB201607713D0 (en) * | 2016-05-03 | 2016-06-15 | Imagination Tech Ltd | Convolutional neural network |
KR20180034853A (en) * | 2016-09-28 | 2018-04-05 | 에스케이하이닉스 주식회사 | Apparatus and method test operating of convolutional neural network |
US10037490B2 (en) * | 2016-12-13 | 2018-07-31 | Google Llc | Performing average pooling in hardware |
US10032110B2 (en) * | 2016-12-13 | 2018-07-24 | Google Llc | Performing average pooling in hardware |
US10521488B1 (en) | 2016-12-30 | 2019-12-31 | X Development Llc | Dynamic partitioning |
JP6740920B2 (en) * | 2017-02-01 | 2020-08-19 | 株式会社デンソー | Processor |
US10699189B2 (en) * | 2017-02-23 | 2020-06-30 | Cerebras Systems Inc. | Accelerated deep learning |
US10896367B2 (en) | 2017-03-07 | 2021-01-19 | Google Llc | Depth concatenation using a matrix computation unit |
US10909447B2 (en) | 2017-03-09 | 2021-02-02 | Google Llc | Transposing neural network matrices in hardware |
US10108581B1 (en) | 2017-04-03 | 2018-10-23 | Google Llc | Vector reduction processor |
WO2018193370A1 (en) | 2017-04-17 | 2018-10-25 | Cerebras Systems Inc. | Task activating for accelerated deep learning |
US10614357B2 (en) | 2017-04-17 | 2020-04-07 | Cerebras Systems Inc. | Dataflow triggered tasks for accelerated deep learning |
US11488004B2 (en) | 2017-04-17 | 2022-11-01 | Cerebras Systems Inc. | Neuron smearing for accelerated deep learning |
US10338919B2 (en) * | 2017-05-08 | 2019-07-02 | Nvidia Corporation | Generalized acceleration of matrix multiply accumulate operations |
DE102018110607A1 (en) | 2017-05-08 | 2018-11-08 | Nvidia Corporation | Generalized acceleration of matrix multiplication and accumulation operations |
CN109937416B (en) * | 2017-05-17 | 2023-04-04 | 谷歌有限责任公司 | Low delay matrix multiplication component |
TWI769810B (en) | 2017-05-17 | 2022-07-01 | 美商谷歌有限責任公司 | Special purpose neural network training chip |
US10621269B2 (en) * | 2017-05-17 | 2020-04-14 | Google Llc | Performing matrix multiplication in hardware |
CN107146616B (en) * | 2017-06-13 | 2020-05-08 | Oppo广东移动通信有限公司 | Equipment control method and related product |
CN109284821B (en) * | 2017-07-19 | 2022-04-12 | 华为技术有限公司 | Neural network arithmetic device |
US11409692B2 (en) | 2017-07-24 | 2022-08-09 | Tesla, Inc. | Vector computational unit |
US11893393B2 (en) | 2017-07-24 | 2024-02-06 | Tesla, Inc. | Computational array microprocessor system with hardware arbiter managing memory requests |
US10671349B2 (en) | 2017-07-24 | 2020-06-02 | Tesla, Inc. | Accelerated mathematical engine |
US11157287B2 (en) | 2017-07-24 | 2021-10-26 | Tesla, Inc. | Computational array microprocessor system with variable latency memory access |
US11157441B2 (en) | 2017-07-24 | 2021-10-26 | Tesla, Inc. | Computational array microprocessor system using non-consecutive data formatting |
TWI653584B (en) | 2017-09-15 | 2019-03-11 | 中原大學 | Method of judging neural network with non-volatile memory cells |
GB2568230B (en) * | 2017-10-20 | 2020-06-03 | Graphcore Ltd | Processing in neural networks |
KR102586173B1 (en) | 2017-10-31 | 2023-10-10 | 삼성전자주식회사 | Processor and control methods thererof |
KR102424962B1 (en) | 2017-11-15 | 2022-07-25 | 삼성전자주식회사 | Memory Device performing parallel arithmetic process and Memory Module having the same |
US10599975B2 (en) | 2017-12-15 | 2020-03-24 | Uber Technologies, Inc. | Scalable parameter encoding of artificial neural networks obtained via an evolutionary process |
US11360930B2 (en) * | 2017-12-19 | 2022-06-14 | Samsung Electronics Co., Ltd. | Neural processing accelerator |
US20190205738A1 (en) * | 2018-01-04 | 2019-07-04 | Tesla, Inc. | Systems and methods for hardware-based pooling |
KR102637735B1 (en) * | 2018-01-09 | 2024-02-19 | 삼성전자주식회사 | Neural network processing unit including approximate multiplier and system on chip including the same |
CN108182471B (en) * | 2018-01-24 | 2022-02-15 | 上海岳芯电子科技有限公司 | Convolutional neural network reasoning accelerator and method |
US11561791B2 (en) | 2018-02-01 | 2023-01-24 | Tesla, Inc. | Vector computational unit receiving data elements in parallel from a last row of a computational array |
US10796198B2 (en) * | 2018-02-08 | 2020-10-06 | Western Digital Technologies, Inc. | Adjusting enhancement coefficients for neural network engine |
US11164072B2 (en) | 2018-02-08 | 2021-11-02 | Western Digital Technologies, Inc. | Convolution engines for systolic neural network processor |
US11907834B2 (en) | 2018-02-14 | 2024-02-20 | Deepmentor Inc | Method for establishing data-recognition model |
TWI659324B (en) * | 2018-02-14 | 2019-05-11 | 倍加科技股份有限公司 | Method and system for generating circuit planning results |
CN111742331A (en) * | 2018-02-16 | 2020-10-02 | 多伦多大学管理委员会 | Neural network accelerator |
CN110415157B (en) * | 2018-04-26 | 2024-01-30 | 华为技术有限公司 | Matrix multiplication calculation method and device |
US11487846B2 (en) | 2018-05-04 | 2022-11-01 | Apple Inc. | Performing multiply and accumulate operations in neural network processor |
US11537838B2 (en) * | 2018-05-04 | 2022-12-27 | Apple Inc. | Scalable neural network processing engine |
US10440341B1 (en) * | 2018-06-07 | 2019-10-08 | Micron Technology, Inc. | Image processor formed in an array of memory cells |
US11501140B2 (en) * | 2018-06-19 | 2022-11-15 | International Business Machines Corporation | Runtime reconfigurable neural network processor core |
DE102018115902A1 (en) | 2018-07-01 | 2020-01-02 | Oliver Bartels | SIMD processor with CAM for operand selection after pattern recognition |
TWI667576B (en) * | 2018-07-09 | 2019-08-01 | 國立中央大學 | Machine learning method and machine learning device |
CN109273035B (en) * | 2018-08-02 | 2020-03-17 | 北京知存科技有限公司 | Control method and terminal of flash memory chip |
US11636319B2 (en) | 2018-08-22 | 2023-04-25 | Intel Corporation | Iterative normalization for machine learning applications |
WO2020044152A1 (en) | 2018-08-28 | 2020-03-05 | Cerebras Systems Inc. | Scaled compute fabric for accelerated deep learning |
WO2020044208A1 (en) | 2018-08-29 | 2020-03-05 | Cerebras Systems Inc. | Isa enhancements for accelerated deep learning |
US11328208B2 (en) | 2018-08-29 | 2022-05-10 | Cerebras Systems Inc. | Processor element redundancy for accelerated deep learning |
KR102637733B1 (en) | 2018-10-31 | 2024-02-19 | 삼성전자주식회사 | Neural network processor and convolution operation method thereof |
JP7315317B2 (en) | 2018-11-09 | 2023-07-26 | 株式会社Preferred Networks | Processors and how they transfer data |
CN111445020B (en) * | 2019-01-16 | 2023-05-23 | 阿里巴巴集团控股有限公司 | Graph-based convolutional network training method, device and system |
US11188085B2 (en) * | 2019-03-22 | 2021-11-30 | Ford Global Technologies, Llc | Vehicle capsule networks |
US11783176B2 (en) | 2019-03-25 | 2023-10-10 | Western Digital Technologies, Inc. | Enhanced storage device memory architecture for machine learning |
US10929058B2 (en) | 2019-03-25 | 2021-02-23 | Western Digital Technologies, Inc. | Enhanced memory device architecture for machine learning |
US10733016B1 (en) | 2019-04-26 | 2020-08-04 | Google Llc | Optimizing hardware FIFO instructions |
US11853890B2 (en) * | 2019-05-02 | 2023-12-26 | Macronix International Co., Ltd. | Memory device and operation method thereof |
TWI698810B (en) * | 2019-06-14 | 2020-07-11 | 旺宏電子股份有限公司 | Neuromorphic computing device |
US11514300B2 (en) | 2019-06-14 | 2022-11-29 | Macronix International Co., Ltd. | Resistor circuit, artificial intelligence chip and method for manufacturing the same |
US11233049B2 (en) | 2019-06-14 | 2022-01-25 | Macronix International Co., Ltd. | Neuromorphic computing device |
KR20210014902A (en) | 2019-07-31 | 2021-02-10 | 삼성전자주식회사 | Processor and control method thereof |
CN110610235B (en) * | 2019-08-22 | 2022-05-13 | 北京时代民芯科技有限公司 | Neural network activation function calculation circuit |
US11836624B2 (en) | 2019-08-26 | 2023-12-05 | D5Ai Llc | Deep learning with judgment |
US11829729B2 (en) | 2019-09-05 | 2023-11-28 | Micron Technology, Inc. | Spatiotemporal fused-multiply-add, and related systems, methods and devices |
US11934824B2 (en) | 2019-09-05 | 2024-03-19 | Micron Technology, Inc. | Methods for performing processing-in-memory operations, and related memory devices and systems |
US11693657B2 (en) | 2019-09-05 | 2023-07-04 | Micron Technology, Inc. | Methods for performing fused-multiply-add operations on serially allocated data within a processing-in-memory capable memory device, and related memory devices and systems |
KR20210050243A (en) * | 2019-10-28 | 2021-05-07 | 삼성전자주식회사 | Neuromorphic package devices and neuromorphic computing systems |
KR102139229B1 (en) * | 2019-10-30 | 2020-07-29 | 주식회사 뉴로메카 | Collision Detection Method and System of Robot Manipulator Using Artificial Neural Network |
KR102357168B1 (en) * | 2019-10-30 | 2022-02-07 | 주식회사 뉴로메카 | Collision Detection Method and System of Robot Manipulator Using Artificial Neural Network |
KR20210105053A (en) * | 2020-02-18 | 2021-08-26 | 에스케이하이닉스 주식회사 | Calculation circuit and deep learning system including the same |
JP7475080B2 (en) | 2020-04-01 | 2024-04-26 | 義憲 岡島 | Fuzzy search circuit |
US11537861B2 (en) | 2020-06-23 | 2022-12-27 | Micron Technology, Inc. | Methods of performing processing-in-memory operations, and related devices and systems |
CN114654884B (en) * | 2020-12-22 | 2023-06-06 | 精工爱普生株式会社 | Printing condition setting method and printing condition setting system |
US20220277190A1 (en) * | 2021-02-28 | 2022-09-01 | Anaflash Inc. | Neural network engine with associated memory array |
US11544213B2 (en) | 2021-03-04 | 2023-01-03 | Samsung Electronics Co., Ltd. | Neural processor |
CN112992248A (en) * | 2021-03-12 | 2021-06-18 | 西安交通大学深圳研究院 | PE (provider edge) calculation unit structure of FIFO (first in first out) -based variable-length cyclic shift register |
US11714556B2 (en) * | 2021-09-14 | 2023-08-01 | quadric.io, Inc. | Systems and methods for accelerating memory transfers and computation efficiency using a computation-informed partitioning of an on-chip data buffer and implementing computation-aware data transfer operations to the on-chip data buffer |
WO2023080291A1 (en) * | 2021-11-08 | 2023-05-11 | 한국전자기술연구원 | Pooling device for deep learning accelerator |
Family Cites Families (54)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US3761876A (en) * | 1971-07-28 | 1973-09-25 | Recognition Equipment Inc | Recognition unit for optical character reading system |
US3777132A (en) * | 1972-02-23 | 1973-12-04 | Burroughs Corp | Method and apparatus for obtaining the reciprocal of a number and the quotient of two numbers |
FR2595891B1 (en) * | 1986-03-11 | 1988-06-10 | Labo Electronique Physique | METHOD FOR STRENGTHENING THE CONTOURS OF DIGITAL SIGNALS AND PROCESSING DEVICE FOR CARRYING OUT SAID METHOD |
JPS63206828A (en) | 1987-02-23 | 1988-08-26 | Mitsubishi Electric Corp | Stretch circuit for maximum value |
US5014235A (en) | 1987-12-15 | 1991-05-07 | Steven G. Morton | Convolution memory |
US5136717A (en) | 1988-11-23 | 1992-08-04 | Flavors Technology Inc. | Realtime systolic, multiple-instruction, single-data parallel computer system |
EP0411341A3 (en) * | 1989-07-10 | 1992-05-13 | Yozan Inc. | Neural network |
US5138695A (en) | 1989-10-10 | 1992-08-11 | Hnc, Inc. | Systolic array image processing system |
JP2756170B2 (en) | 1990-03-05 | 1998-05-25 | 日本電信電話株式会社 | Neural network learning circuit |
US5337395A (en) | 1991-04-08 | 1994-08-09 | International Business Machines Corporation | SPIN: a sequential pipeline neurocomputer |
US5146543A (en) | 1990-05-22 | 1992-09-08 | International Business Machines Corp. | Scalable neural array processor |
JPH04290155A (en) | 1991-03-19 | 1992-10-14 | Fujitsu Ltd | Parallel data processing system |
US5519811A (en) * | 1991-10-17 | 1996-05-21 | Kawasaki Steel Corporation | Neural network, processor, and pattern recognition apparatus |
US5903454A (en) | 1991-12-23 | 1999-05-11 | Hoffberg; Linda Irene | Human-factored interface corporating adaptive pattern recognition based controller apparatus |
US5799134A (en) | 1995-03-13 | 1998-08-25 | Industrial Technology Research Institute | One dimensional systolic array architecture for neural network |
US5812993A (en) | 1996-03-07 | 1998-09-22 | Technion Research And Development Foundation Ltd. | Digital hardware architecture for realizing neural network |
US6038337A (en) | 1996-03-29 | 2000-03-14 | Nec Research Institute, Inc. | Method and apparatus for object recognition |
JPH11177399A (en) | 1997-12-15 | 1999-07-02 | Mitsubishi Electric Corp | Clock delay circuit, oscillation circuit using the same, phase locked loop and clock generation circuit |
GB9902115D0 (en) | 1999-02-01 | 1999-03-24 | Axeon Limited | Neural networks |
US7054850B2 (en) * | 2000-06-16 | 2006-05-30 | Canon Kabushiki Kaisha | Apparatus and method for detecting or recognizing pattern by employing a plurality of feature detecting elements |
JP3613466B2 (en) * | 2001-04-06 | 2005-01-26 | 旭化成株式会社 | Data arithmetic processing apparatus and data arithmetic processing program |
US7245767B2 (en) | 2003-08-21 | 2007-07-17 | Hewlett-Packard Development Company, L.P. | Method and apparatus for object identification, classification or verification |
US7693585B2 (en) | 2004-09-30 | 2010-04-06 | Rockwell Automation Technologies, Inc. | Enabling object oriented capabilities in automation systems |
US7634137B2 (en) | 2005-10-14 | 2009-12-15 | Microsoft Corporation | Unfolded convolution for fast feature extraction |
WO2008067676A1 (en) | 2006-12-08 | 2008-06-12 | Medhat Moussa | Architecture, system and method for artificial neural network implementation |
US8184696B1 (en) | 2007-09-11 | 2012-05-22 | Xilinx, Inc. | Method and apparatus for an adaptive systolic array structure |
JP5376920B2 (en) | 2008-12-04 | 2013-12-25 | キヤノン株式会社 | Convolution operation circuit, hierarchical convolution operation circuit, and object recognition device |
EP2259214B1 (en) * | 2009-06-04 | 2013-02-27 | Honda Research Institute Europe GmbH | Implementing a neural associative memory based on non-linear learning of discrete synapses |
US8442927B2 (en) | 2009-07-30 | 2013-05-14 | Nec Laboratories America, Inc. | Dynamically configurable, multi-ported co-processor for convolutional neural networks |
TWI525558B (en) | 2011-01-17 | 2016-03-11 | Univ Nat Taipei Technology | Resilient high - speed hardware reverse transfer and feedback type neural network system |
US8924455B1 (en) | 2011-02-25 | 2014-12-30 | Xilinx, Inc. | Multiplication of matrices using systolic arrays |
TW201331855A (en) | 2012-01-19 | 2013-08-01 | Univ Nat Taipei Technology | High-speed hardware-based back-propagation feedback type artificial neural network with free feedback nodes |
KR20130090147A (en) * | 2012-02-03 | 2013-08-13 | 안병익 | Neural network computing apparatus and system, and method thereof |
JP5834997B2 (en) | 2012-02-23 | 2015-12-24 | 株式会社ソシオネクスト | Vector processor, vector processor processing method |
CN102665049B (en) * | 2012-03-29 | 2014-09-17 | 中国科学院半导体研究所 | Programmable visual chip-based visual image processing system |
US9081608B2 (en) | 2012-05-19 | 2015-07-14 | Digital System Research Inc. | Residue number arithmetic logic unit |
JP6096896B2 (en) * | 2012-07-12 | 2017-03-15 | ノキア テクノロジーズ オーユー | Vector quantization |
US9477925B2 (en) | 2012-11-20 | 2016-10-25 | Microsoft Technology Licensing, Llc | Deep neural networks training for speech and pattern recognition |
US9811775B2 (en) * | 2012-12-24 | 2017-11-07 | Google Inc. | Parallelizing neural networks during training |
US20140280989A1 (en) | 2013-03-14 | 2014-09-18 | Thomas J. Borkowski | System and method for establishing peer to peer connections through symmetric nats |
US9190053B2 (en) | 2013-03-25 | 2015-11-17 | The Governing Council Of The Univeristy Of Toronto | System and method for applying a convolutional neural network to speech recognition |
KR20150016089A (en) * | 2013-08-02 | 2015-02-11 | 안병익 | Neural network computing apparatus and system, and method thereof |
JP6107531B2 (en) | 2013-08-15 | 2017-04-05 | 富士ゼロックス株式会社 | Feature extraction program and information processing apparatus |
US9978014B2 (en) * | 2013-12-18 | 2018-05-22 | Intel Corporation | Reconfigurable processing unit |
JP6314628B2 (en) * | 2014-04-28 | 2018-04-25 | 株式会社デンソー | Arithmetic processing unit |
CN104035751B (en) | 2014-06-20 | 2016-10-12 | 深圳市腾讯计算机系统有限公司 | Data parallel processing method based on multi-graphics processor and device |
US9886948B1 (en) * | 2015-01-05 | 2018-02-06 | Amazon Technologies, Inc. | Neural network processing of multiple feature streams using max pooling and restricted connectivity |
EP3064130A1 (en) | 2015-03-02 | 2016-09-07 | MindMaze SA | Brain activity measurement and feedback system |
US20160267111A1 (en) | 2015-03-11 | 2016-09-15 | Microsoft Technology Licensing, Llc | Two-stage vector reduction using two-dimensional and one-dimensional systolic arrays |
US10102481B2 (en) * | 2015-03-16 | 2018-10-16 | Conduent Business Services, Llc | Hybrid active learning for non-stationary streaming data with asynchronous labeling |
US9552510B2 (en) * | 2015-03-18 | 2017-01-24 | Adobe Systems Incorporated | Facial expression capture for character animation |
WO2016154781A1 (en) * | 2015-03-27 | 2016-10-06 | Intel Corporation | Low-cost face recognition using gaussian receptive field features |
US10192162B2 (en) | 2015-05-21 | 2019-01-29 | Google Llc | Vector computation unit in a neural network processor |
GB2558271B (en) * | 2016-12-23 | 2021-09-08 | Imagination Tech Ltd | Median determination |
-
2015
- 2015-09-03 US US14/845,117 patent/US10192162B2/en active Active
-
2016
- 2016-04-29 KR KR1020227009700A patent/KR102516092B1/en active IP Right Grant
- 2016-04-29 DE DE112016002296.4T patent/DE112016002296T5/en active Pending
- 2016-04-29 JP JP2017550887A patent/JP6615902B2/en active Active
- 2016-04-29 CN CN202110718015.1A patent/CN113392964A/en active Pending
- 2016-04-29 EP EP23197533.5A patent/EP4276690A1/en active Pending
- 2016-04-29 FI FIEP16724517.4T patent/FI3298545T3/en active
- 2016-04-29 GB GB2200642.3A patent/GB2600290A/en not_active Withdrawn
- 2016-04-29 KR KR1020177028169A patent/KR102127524B1/en active IP Right Grant
- 2016-04-29 KR KR1020207018024A patent/KR102379700B1/en active IP Right Grant
- 2016-04-29 DK DK16724517.4T patent/DK3298545T3/en active
- 2016-04-29 GB GB1715525.0A patent/GB2553055B/en active Active
- 2016-04-29 WO PCT/US2016/029986 patent/WO2016186813A1/en active Application Filing
- 2016-04-29 DE DE202016107442.8U patent/DE202016107442U1/en active Active
- 2016-04-29 KR KR1020237010250A patent/KR20230048449A/en not_active Application Discontinuation
- 2016-04-29 EP EP16724517.4A patent/EP3298545B1/en active Active
- 2016-04-29 CN CN201680019810.9A patent/CN107533667B/en active Active
- 2016-05-20 TW TW105115868A patent/TWI591490B/en active
- 2016-05-20 TW TW106116630A patent/TWI638272B/en active
- 2016-12-22 US US15/389,288 patent/US10074051B2/en active Active
-
2018
- 2018-04-27 HK HK18105509.3A patent/HK1245954A1/en unknown
-
2019
- 2019-01-11 US US16/245,406 patent/US11620508B2/en active Active
- 2019-08-02 JP JP2019142868A patent/JP7000387B2/en active Active
-
2021
- 2021-09-10 JP JP2021148010A patent/JP7346510B2/en active Active
-
2023
- 2023-03-01 US US18/176,640 patent/US20230206070A1/en active Pending
- 2023-09-06 JP JP2023144224A patent/JP2023169224A/en active Pending
Also Published As
Similar Documents
Publication | Publication Date | Title |
---|---|---|
DE202016107442U1 (en) | Vector arithmetic unit in a neural network processor | |
DE202016107436U1 (en) | Neural network processor | |
DE112016002292T5 (en) | STACK PROCESSING IN A NEURONAL NETWORK PROCESSOR | |
DE102017121887A1 (en) | Perform core traversal in hardware | |
DE102018105457A1 (en) | Transpose matrices of neural networks in hardware | |
DE202016107439U1 (en) | Prefetching weights for use in a neural network processor | |
DE202016107443U1 (en) | Compute convolutions using a neural network processor | |
DE202017105528U1 (en) | Perform mean pooling in hardware | |
DE202016107446U1 (en) | Rotation of data for calculations in neural networks | |
DE102017125049A1 (en) | Transpose in a matrix vector processor | |
DE112019000336T5 (en) | MASSIVELY PARALLEL NEURAL INFERENCE DATA PROCESSING ELEMENTS | |
DE112018006047T5 (en) | DEFENSE OF FUNCTIONAL FUNCTIONS IN QUANTUM APPROXIMATION OPTIMIZATION | |
DE102018105198A1 (en) | Depth concatenation using a matrix computation unit | |
DE202017105403U1 (en) | Instruction set architecture for neural networks | |
DE102020101187A1 (en) | WINOGRAD TRANSFORM FOLDING OPERATION FOR NEURONAL NETWORKS | |
DE102017113232A1 (en) | TENSOR PROCESSING USING A FORMAT WITH LOW ACCURACY | |
DE102018103598A1 (en) | Permute in a matrix vector processor | |
DE102018100239A1 (en) | Loop and library fusion | |
DE202016008658U1 (en) | Bildklassifikationsneuronalnetze | |
DE112019002981T5 (en) | PARALLEL COMPUTATION ARCHITECTURE WITH RECONFIGURABLE CORE LEVEL AND VECTOR LEVEL PARALLELITY | |
DE102017115519A1 (en) | Superpixel method for folding neural networks | |
DE102019122818A1 (en) | Neural network device for a neural network operation, method of operating a neural network device and application processor incorporating the neural network device | |
DE112012005014T5 (en) | Perform arithmetic operations using both large and small floating-point values | |
DE112019000676T5 (en) | CENTRAL SCHEDULER AND INSTRUCTION ASSIGNMENT FOR A NEURAL INFERENCE PROCESSOR | |
DE112020001774T5 (en) | DATA SET-DEPENDENT LOW-RANKING DETACHMENT OF NEURAL NETWORKS |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
R207 | Utility model specification | ||
R081 | Change of applicant/patentee |
Owner name: GOOGLE LLC (N.D.GES.D. STAATES DELAWARE), MOUN, USFree format text: FORMER OWNER: GOOGLE INC., MOUNTAIN VIEW, CALIF., US |
|
R082 | Change of representative |
Representative=s name: MAIKOWSKI & NINNEMANN PATENTANWAELTE PARTNERSC, DE |
|
R150 | Utility model maintained after payment of first maintenance fee after three years | ||
R151 | Utility model maintained after payment of second maintenance fee after six years | ||
R152 | Utility model maintained after payment of third maintenance fee after eight years |