EP2466489A1 - Combining model-based aligners - Google Patents
Combining model-based aligners Download PDFInfo
- Publication number
- EP2466489A1 EP2466489A1 EP11193828A EP11193828A EP2466489A1 EP 2466489 A1 EP2466489 A1 EP 2466489A1 EP 11193828 A EP11193828 A EP 11193828A EP 11193828 A EP11193828 A EP 11193828A EP 2466489 A1 EP2466489 A1 EP 2466489A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- alignment
- computers
- model
- models
- bidirectional
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Withdrawn
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/44—Statistical methods, e.g. probability models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/45—Example-based machine translation; Alignment
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/01—Probabilistic graphical models, e.g. probabilistic networks
Definitions
- the invention relates to the field of machine translations, and specifically to methods, systems, and apparatus, including computer programs encoded on computer storage media, for word alignment for use in machine translation.
- This specification relates to word alignment for statistical machine translation.
- Word alignment is a central machine learning task in statistical machine translation (MT) that identifies corresponding words in sentence pairs.
- MT statistical machine translation
- the vast majority of MT systems employ a directional Markov alignment model that aligns the words of a sentence f to those of its translation e .
- Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e .
- a similar model generating e from f will make different alignment predictions.
- This specification describes the construction and use of a graphical model that explicitly combines two directional aligners into a single joint model. Inference can be performed through dual decomposition, which reuses the efficient inference algorithms of the directional models. The combined model enforces a one-to-one phrase constraint and improves alignment quality.
- This specification describes a model-based alternative to aligner combination that resolves the conflicting predictions of two directional alignment models by embedding them in a larger graphical model (the "bidirectional model").
- the latent variables in the bidirectional model are a proper superset of the latent variables in two directional Markov alignment models.
- the model structure and potentials allow the two directional models to disagree, but reward agreement.
- the bidirectional model enforces a one-to-one phrase alignment structure that yields the same structural benefits shown in phrase alignment models, synchronous ITG (Inversion Transduction Grammar) models, and state-of-the-art supervised models.
- Inference in the bidirectional model is not tractable because of numerous edge cycles in the model graph.
- dual decomposition as an approximate inference technique.
- the bidirectional model is a graphical model defined by a vertex set V and an edge set D that is constructed conditioned on the length of a sentence e and its translation f .
- Each vertex corresponds to a model variable V i and each undirected edge corresponds to a pair of variables ( V i , V j ).
- Each vertex has an associated vertex potential function v i (v j ) that assigns a real-valued potential to each possible value v i of V i .
- each edge has an associated potential function ⁇ ij ( v i , v j ) that scores pairs of values.
- the probability under the model of any full assignment v to the model variables, indexed by V factors over vertex and edge potentials.
- the bidirectional model contains two directional hidden Markov alignment models, along with an additional structure that resolves the predictions of these embedded models into a single symmetric word alignment.
- the following paragraphs describe the directional model and then describe the additional structure that combines two directional models into the joint bidirectional model.
- the emission model M is a learned multinomial distribution over word types.
- the transition model D is a multinomial over transition distances, which treats null alignments as a special case.
- D ⁇ a j 0
- D ⁇ a j i ⁇ ⁇ 0
- the parameters of the conditional multinomial M, the transition model c , and the null transition parameter p o can all be learned from a sentence aligned corpus via the expectation maximization algorithm.
- the highest probability word alignment vector under the model for a given sentence pair ( e, f ) can be computed exactly using the standard Viterbi algorithm for hidden Markov models in O (
- a set A constructed in this way will always be many-to-one; many positions j can align to the same i , but each j appears at most once in the set.
- transition and emission distributions of the two models are distinguished by subscripts that indicate the generative direction of the model, f ⁇ e or e ⁇ f P e , b
- the vector b can be interpreted as a set of alignment links that is one-to-many: each value i appears at most once in the set.
- aligners can be combined to create a bidirectional model by embedding the aligners in a graphical model that includes all of the random variables of two directional aligners and additional structure that promotes agreement and resolves their discrepancies.
- the bidirectional model includes observed word sequences e and f , along with the two vectors of alignment variables a and b defined above.
- FIG. 1 illustrates the graph structure of a bidirectional graphical model for a simple sentence pair in English and Chinese.
- the variables a , b , and c (which is described below) are shown as labels on the figure.
- edge potentials between a and b encode the transition model in Equation 2.
- b i - 1 j
- a random bit matrix c encodes the output of the combined aligners: c ⁇ 0 ⁇ 1 e ⁇ f
- Each random variable c ij ⁇ ⁇ 0,1 ⁇ is connected to a j and b i .
- These coherence edges connect the alignment variables of the directional models to the Boolean variables of the combined space. These edges allow the model to ensure that the three sets of variables, a , b , and c , together encode a coherent alignment analysis of the sentence pair.
- Figure 1 depicts the graph structure of the model.
- This pattern of effects can be encoded in a potential function ⁇ (c) for each edge.
- Each of these edge potential functions takes an integer value i for some variable a j and a binary value k for some c i'j .
- the matrix c is interpreted as the final alignment produced by the bidirectional model, ignoring a and b . In this way, the one-to-many constraints of the directional models are relaxed. However, all of the information about how words align is expressed by the vertex and edge potentials on a and b . The coherence edges and the link matrix c only serve to resolve conflicts between the directional models and communicate information between them.
- c For any assignment to ( a , b , c ) with non-zero probability, c must encode a one-to-one phrase alignment with a maximum phrase length of 3. That is, any word in either sentence can align to at most three words in the opposite sentence, and those words must be contiguous. This restriction is directly enforced by the edge potential in Equation 7.
- inference in the bidirectional model is an instance of the general phrase alignment problem, which is known to be NP-hard.
- One subgraph G a includes all of the vertices corresponding to variables a and c .
- the other subgraph G b includes vertices for variables b and c . Every edge in the graph belongs to exactly one of these two subgraphs.
- the dual decomposition inference approach allows this subgraph structure to be exploited (see, for example, Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola, On dual decomposition and linear programming relaxations for natural language processing, in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2010 ).
- the technique of dual decomposition has recently been shown to yield state-of-the-art performance in dependency parsing (see, for example, Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag, Dual decomposition for parsing with non-projective head automata, in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2010 ).
- the inference problem under the bidirectional graphical model is first restated in terms of the two overlapping subgraphs that admit tractable inference.
- I be the index set of all ( i, j ) for c .
- L( a , b , c (a) , c (b) , u ) f a ⁇ c a + g b ⁇ c b + ⁇ i ⁇ j ⁇ I u i ⁇ j c i , j a - c i , j b .
- FIG. 2 illustrates how the bidirectional model decomposes into two acyclic models.
- the two models each contain a copy of c .
- the variables are shown as labels on the figure.
- Equation 9 For fixed u requires only the Viterbi algorithm for linear chain graphical models. That is, one can employ the same algorithm that one would use to find the highest likelihood alignment in a standard HMM (Hidden Markov Model) aligner.
- HMM Hidden Markov Model
- Equation 9 which includes variables a and c (a) .
- the vertex potentials correspond to bilexical probabilities P ( f
- Equation 9 For fixed u , one can define the full dual decomposition algorithm for the bidirectional model, which searches for a u that optimizes Equation 9.
- Algorithm 1 The full dual decomposition optimization procedure is set forth below as Algorithm 1.
- the dual decomposition algorithm provides an inference method that is exact upon convergence. (This certificate of optimality is not provided by other approximate inference algorithms, such as belief propagation, sampling, or simulated annealing.)
- Algorithm 1 does not converge, the output of the algorithm can still be interpreted as an alignment. Given the value of u produced by the algorithm, one can find the optimal values of c (a) and c (b) from Equations 10 and 11 respectively. While these alignments may differ, they will likely be more similar than the alignments of completely independent aligners. These alignments will still need to be combined procedurally (e.g., taking their union), but because they are more similar, the importance of the combination procedure is reduced.
- u is specific to a sentence pair. Therefore, this approach does not require any additional communication overhead relative to the independent directional models in a distributed aligner implementation. Memory requirements are virtually identical to the baseline: only u must be stored for each sentence pair as it is being processed, but can then be immediately discarded once alignments are inferred.
- FIG. 4 illustrates the place of the bidirectional model in a machine translation system.
- a machine translation system involves components that operate at training time and components that operate at translation time.
- the training time components include a parallel corpus 402 of pairs of sentences in a pair of languages that are taken as having been correctly translated.
- Another training time component is the alignment model component 404, which receives pairs of sentences from the parallel corpus 402 and generates from them an aligned parallel corpus, which is received by a phrase extractor component 406.
- the bidirectional model is part of the alignment model component 404 and used to generate alignments between words in pairs of sentences, as described above.
- the phrase extractor produces a phrase table 408, i.e., a set of data that contains snippets of translated phrases and corresponding scores.
- the translation time components include a translation model 422, which is generated from the data in the phrase table 408.
- the translation time components also include a language model 420 and a machine translation component 424, e.g., a statistical machine translation engine (a system of computers, data and software) that uses the language model 420 and the translation model 422 to generate translated output text 428 from input text 426.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible program carrier for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on a propagated signal that is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (which may also be referred to as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- special purpose logic circuitry e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
Abstract
Method, computer storage medium encoded with a computer program and system for aligning words in parallel translation sentences for use in machine translation. Data characterising two directional alignment models for a pair of sentences is used to derive a combined bi-directional alignment model. Dual decomposition may be employed as an approximate inference technique.
Description
- The invention relates to the field of machine translations, and specifically to methods, systems, and apparatus, including computer programs encoded on computer storage media, for word alignment for use in machine translation.
- This specification relates to word alignment for statistical machine translation.
- Word alignment is a central machine learning task in statistical machine translation (MT) that identifies corresponding words in sentence pairs. The vast majority of MT systems employ a directional Markov alignment model that aligns the words of a sentence f to those of its translation e.
- Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions.
- Systems typically combine the predictions of two directional models, one which aligns f to e and the other e to f. Statistical machine translation systems combine the predictions of two directional models. Combination can reduce errors and relax the one-to-many structural restrictions of directional models. The most common combination methods are simply to form a union or intersection of alignments, or to apply a heuristic procedure like grow-diag-final (described in, for example, Franz Josef Och, Christopher Tillman, and Hermann Ney, Improved alignment models for statistical machine translation, in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1999).
- This specification describes the construction and use of a graphical model that explicitly combines two directional aligners into a single joint model. Inference can be performed through dual decomposition, which reuses the efficient inference algorithms of the directional models. The combined model enforces a one-to-one phrase constraint and improves alignment quality.
- The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
-
FIG. 1 illustrates the graph structure of a bidirectional graphical model for a simple sentence pair in English and Chinese. -
FIG. 2 illustrates how the bidirectional model decomposes into two acyclic models. -
FIG. 3 illustrates how the tree-structured subgraph Ga can be mapped to an equivalent chain-structured model by optimizing. -
FIG. 4 illustrates the place of the bidirectional model in a machine translation system. - Like reference symbols in the various drawings indicate like elements.
- This specification describes a model-based alternative to aligner combination that resolves the conflicting predictions of two directional alignment models by embedding them in a larger graphical model (the "bidirectional model").
- The latent variables in the bidirectional model are a proper superset of the latent variables in two directional Markov alignment models. The model structure and potentials allow the two directional models to disagree, but reward agreement. Moreover, the bidirectional model enforces a one-to-one phrase alignment structure that yields the same structural benefits shown in phrase alignment models, synchronous ITG (Inversion Transduction Grammar) models, and state-of-the-art supervised models.
- Inference in the bidirectional model is not tractable because of numerous edge cycles in the model graph. However, one can employ dual decomposition as an approximate inference technique. One can iteratively apply the same efficient sequence algorithms for the underlying Markov alignment models to search the combined model space. In cases where this approximation converges, one has a certificate of optimality under the full model.
- This model-based approach to aligner combination yields improvements in alignment quality and phrase extraction quality.
- The bidirectional model is a graphical model defined by a vertex set V and an edge set D that is constructed conditioned on the length of a sentence e and its translation f. Each vertex corresponds to a model variable Vi and each undirected edge corresponds to a pair of variables (Vi , Vj ). Each vertex has an associated vertex potential function vi(vj) that assigns a real-valued potential to each possible value vi of Vi . Likewise, each edge has an associated potential function µ ij (vi , vj ) that scores pairs of values. The probability under the model of any full assignment v to the model variables, indexed by V, factors over vertex and edge potentials.
- The bidirectional model contains two directional hidden Markov alignment models, along with an additional structure that resolves the predictions of these embedded models into a single symmetric word alignment. The following paragraphs describe the directional model and then describe the additional structure that combines two directional models into the joint bidirectional model.
- This section describes the classic hidden Markov alignment model, which is described, for example, in Stephan Vogel, Hermann Ney, and Christoph Tillmann, HMM-Based Word, Alignment in Statistical Translation, in Proceedings of the 16th Conference on Computational Linguistics, 1996. The model generates a sequence of words f conditioned on a word sequence e. One conventionally indexes the words of e by i and f by j. P(f|e) is defined in terms of a latent alignment vector a, where aj = i indicates that word position i of e aligns to word position j of f.
- In Equation 2 above, the emission model M is a learned multinomial distribution over word types. The transition model D is a multinomial over transition distances, which treats null alignments as a special case.
where c(i'- i) is a learned distribution over signed distances, normalized over the possible transitions from i. - The parameters of the conditional multinomial M, the transition model c, and the null transition parameter p o can all be learned from a sentence aligned corpus via the expectation maximization algorithm.
- The highest probability word alignment vector under the model for a given sentence pair (e, f) can be computed exactly using the standard Viterbi algorithm for hidden Markov models in O(|e|2· |f|) time.
-
- A set A constructed in this way will always be many-to-one; many positions j can align to the same i, but each j appears at most once in the set.
- The foregoing description has defined a directional model that generates f from e. An identically structured model can be defined that generates e from f. Let b be a vector of alignments where bi = j indicates that word position j of f aligns to word position i of e. Then, P(e, b|f) is defined similarly to Equation 2, but with e and f swapped. The transition and emission distributions of the two models are distinguished by subscripts that indicate the generative direction of the model, f→e or e→f
-
- As will be described, one can combine aligners to create a bidirectional model by embedding the aligners in a graphical model that includes all of the random variables of two directional aligners and additional structure that promotes agreement and resolves their discrepancies.
- The bidirectional model includes observed word sequences e and f, along with the two vectors of alignment variables a and b defined above.
- Because the word types and lengths of e and f are always fixed by the observed sentence pair, one can define an identical model with only a and b variables, where the edge potentials between any aj, fj, and e are compiled into a vertex potential on aj, defined in terms of f and e, and likewise for any bi .
-
FIG. 1 illustrates the graph structure of a bidirectional graphical model for a simple sentence pair in English and Chinese. The variables a, b, and c (which is described below) are shown as labels on the figure. -
-
- Each random variable cij ∈ {0,1} is connected to aj and bi . These coherence edges connect the alignment variables of the directional models to the Boolean variables of the combined space. These edges allow the model to ensure that the three sets of variables, a, b, and c, together encode a coherent alignment analysis of the sentence pair.
Figure 1 depicts the graph structure of the model. - The potentials on coherence edges are not learned and do not express any patterns in the dataset. Instead, they are fixed functions that promote consistency between the integer-valued directional variables a and b and the Boolean-valued combination variables c.
- Consider the variable assignment aj = i, where i = 0 indicates that fj is null-aligned and i > 0 indicates that fj aligns to ei . The coherence potential ensures the following relationship between the variable assignment aj = i and the variables ci'j , for any i': 0 < i' ≤ |e|.
- If i = 0 (null-aligned), then all ci'j = 0.
- If i > 0, then cij = 1
- cij > 0 only if i' ∈ {i - 1, i, i + 1}
- Assigning ci'j = 1 for i'≠ i incurs a cost e-α, where α is a learned constant, e.g., 0.3.
-
-
- The matrix c is interpreted as the final alignment produced by the bidirectional model, ignoring a and b. In this way, the one-to-many constraints of the directional models are relaxed. However, all of the information about how words align is expressed by the vertex and edge potentials on a and b. The coherence edges and the link matrix c only serve to resolve conflicts between the directional models and communicate information between them.
- Because directional alignments are preserved intact as components of the bidirectional model, extensions or refinements to the underlying directional Markov alignment model can be integrated cleanly into the bidirectional model as well, including lexicalized transition models (described in, for example, Xiaodong He, Using word-dependent transition models in HMM based word alignment for statistical machine, in ACL Workshop on Statistical Machine Translation, 2007), extended conditioning contexts (described in, for example, Jamie Brunning, Adria de Gispert, and William Byrne, Context-dependent alignment models for statistical machine translation, in Proceedings of the North American Chapter of the Association for Computational Linguistics, 2009), and external information (described in, for example, Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata, Word alignment with synonym regularization, in Proceedings of the Association for Computational Linguistics, 2010).
- For any assignment to (a, b, c) with non-zero probability, c must encode a one-to-one phrase alignment with a maximum phrase length of 3. That is, any word in either sentence can align to at most three words in the opposite sentence, and those words must be contiguous. This restriction is directly enforced by the edge potential in Equation 7.
- In general, graphical models admit efficient, exact inference algorithms if they do not contain cycles. Unfortunately, the bidirectional model contains numerous cycles. For every pair of indices (i, j) and (i',j'), the following cycle exists in the graph:
cij → bi → cij' → aj' → ci'j' → bi' → ci'j → aj → cij
- Additional cycles also exist in the graph through the edges between aj-1 and aj and between bi-1 and bi.
- Because of the edge potential function that has been selected, which restricts the space of non-zero probability assignments to phrase alignments, inference in the bidirectional model is an instance of the general phrase alignment problem, which is known to be NP-hard.
- While the entire graphical model has loops, there are two overlapping subgraphs that are cycle-free. One subgraph Ga includes all of the vertices corresponding to variables a and c. The other subgraph Gb includes vertices for variables b and c. Every edge in the graph belongs to exactly one of these two subgraphs.
- The dual decomposition inference approach allows this subgraph structure to be exploited (see, for example, Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola, On dual decomposition and linear programming relaxations for natural language processing, in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2010). In particular, one can iteratively apply exact inference to the subgraph problems, adjusting potentials of the subgraph problems to reflect the constraints of the full problem. The technique of dual decomposition has recently been shown to yield state-of-the-art performance in dependency parsing (see, for example, Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag, Dual decomposition for parsing with non-projective head automata, in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2010).
- To describe a dual decomposition inference procedure for the bidirectional model, the inference problem under the bidirectional graphical model is first restated in terms of the two overlapping subgraphs that admit tractable inference. Let c (a) be a copy of c associated with Ga, and c (b) with Gb. Also, let f (a, c (a)) be the log-likelihood of an assignment to Ga and let g(b, c (b)) be the log-likelihood of an assignment to Gb. Finally, let I be the index set of all (i, j) for c. Then, the maximum likelihood assignment to the bidirectional model can be found by optimizing
such that: -
-
-
FIG. 2 illustrates how the bidirectional model decomposes into two acyclic models. The two models each contain a copy of c. The variables are shown as labels on the figure. - As in previous work, one solves for u by repeatedly performing inference in the two decoupled maximization problems.
- Evaluating Equation 9 for fixed u requires only the Viterbi algorithm for linear chain graphical models. That is, one can employ the same algorithm that one would use to find the highest likelihood alignment in a standard HMM (Hidden Markov Model) aligner.
-
- In standard HMM aligner inference, the vertex potentials correspond to bilexical probabilities P(f|e). Those terms are included in f (a, c (a)).
- The additional terms of the objective can also be factored into the vertex potentials of a linear chain model. If aj = i, then cij = 1 according to the edge potential defined in Equation 7. Hence, setting aj = i adds the corresponding vertex potential
-
FIG. 3 illustrates how the tree-structured subgraph Ga can be mapped to an equivalent chain-structured model by optimizing over ci'j for aj = 1. - Defining this potential allows one to collapse the source-side sub-graph inference problem defined by Equation 10 into a simple linear chain model that only includes potential functions Vj and µ(a). Hence, one can use a highly optimized linear chain inference implementation rather than a solver for general tree-structured graphical models.
Figure 3 depicts this transformation. -
- Having the ability to efficiently evaluate Equation 9 for fixed u, one can define the full dual decomposition algorithm for the bidirectional model, which searches for a u that optimizes Equation 9. One can, for example, iteratively search for such a u by sub-gradient descent. One can use a learning rate that decays with the number of iterations. Setting the initial learning rate to α works well in practice. The full dual decomposition optimization procedure is set forth below as Algorithm 1.
- If Algorithm 1 converges, then it has found a u such that the value of c (a) that optimizes Equation 10 is identical to the value of c (b) that optimizes Equation 11. Hence, it is also a solution to the original optimization problem, namely Equation 8. Since the dual problem is an upper bound on the original problem, this solution must be optimal for Equation 8.
- The dual decomposition algorithm provides an inference method that is exact upon convergence. (This certificate of optimality is not provided by other approximate inference algorithms, such as belief propagation, sampling, or simulated annealing.) When Algorithm 1 does not converge, the output of the algorithm can still be interpreted as an alignment. Given the value of u produced by the algorithm, one can find the optimal values of c (a) and c (b) from Equations 10 and 11 respectively. While these alignments may differ, they will likely be more similar than the alignments of completely independent aligners. These alignments will still need to be combined procedurally (e.g., taking their union), but because they are more similar, the importance of the combination procedure is reduced.
- Because a maximum number of iterations n was set in the dual decomposition algorithm, and each iteration only involves optimization in a sequence model, the entire inference procedure is only a constant multiple more computationally expensive than evaluating the original directional aligners.
- Moreover, the value of u is specific to a sentence pair. Therefore, this approach does not require any additional communication overhead relative to the independent directional models in a distributed aligner implementation. Memory requirements are virtually identical to the baseline: only u must be stored for each sentence pair as it is being processed, but can then be immediately discarded once alignments are inferred.
- Other approaches to generating one-to-one phrase alignments are generally more expensive. In particular, an ITG model requires O(|e|3 · |f|3) time, whereas Algorithm 1 requires only O(n · (|f| |e|2 + |e| |f|2)).
-
FIG. 4 illustrates the place of the bidirectional model in a machine translation system. - A machine translation system involves components that operate at training time and components that operate at translation time.
- The training time components include a
parallel corpus 402 of pairs of sentences in a pair of languages that are taken as having been correctly translated. Another training time component is thealignment model component 404, which receives pairs of sentences from theparallel corpus 402 and generates from them an aligned parallel corpus, which is received by aphrase extractor component 406. The bidirectional model is part of thealignment model component 404 and used to generate alignments between words in pairs of sentences, as described above. The phrase extractor produces a phrase table 408, i.e., a set of data that contains snippets of translated phrases and corresponding scores. - The translation time components include a
translation model 422, which is generated from the data in the phrase table 408. The translation time components also include alanguage model 420 and amachine translation component 424, e.g., a statistical machine translation engine (a system of computers, data and software) that uses thelanguage model 420 and thetranslation model 422 to generate translatedoutput text 428 frominput text 426. - Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible program carrier for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions can be encoded on a propagated signal that is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- The term "data processing apparatus" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program (which may also be referred to as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous.
Claims (15)
- A method in one or more computers comprising:receiving, in one or more computers, data characterizing two directional alignment models for a pair of sentences, wherein one sentence of the pair is in a first language and the other sentence of the pair is in a different second language;deriving, in said one or more computers, a combined bidirectional alignment model from the two directional alignment models; andevaluating, in said one or more computers, the bidirectional alignment model and deriving an alignment for the pair of sentences from the evaluation of the bidirectional alignment model.
- The method of claim 1, wherein:the bidirectional model embeds the two directional alignment models and an additional structure that resolves the predictions of the embedded models into a single symmetric word alignment.
- The method of any one of claim 1 and claim 2, wherein:evaluating the bidirectional alignment model generates an alignment solution.
- The method of any one of claim 1 and claim 2, wherein:evaluating the bidirectional alignment model generates two alignment solutions, wherein the first solution is an alignment model in a first direction from the first language to the second language and the second solution is an alignment model in a second direction from the second language to the first language; andderiving the alignment for the pair of sentences comprises combining the first alignment model and the second alignment model.
- The method of any one of claim 1 and claim 4 as dependent of claim 2, wherein:each of the two directional alignment models are hidden Markov alignment models.
- A computer storage medium encoded with a computer program, the program comprising instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:receiving data characterizing two directional alignment models for a pair of sentences, one sentence in a first language and the other sentence in a different second language;deriving a combined bidirectional alignment model from the two directional alignment models; andevaluating the bidirectional alignment model and deriving an alignment for the pair of sentences from the evaluation of the bidirectional alignment model.
- The computer storage medium of claim 6, wherein the computer program further comprises instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:the bidirectional model embeds the two directional alignment models and an additional structure that resolves the predictions of the embedded models into a single symmetric word alignment.
- The computer storage medium of any one of claim 6 and claim 7, wherein the computer program further comprises instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:evaluating the bidirectional alignment model generates an alignment solution.
- The computer storage medium of any one of claim 6 and claim 7, wherein the computer program further comprises instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:evaluating the bidirectional alignment model generates two alignment solutions,
wherein the first solution is an alignment model in a first direction from the first language to the second language and the second solution is an alignment model in a second direction from the second language to the first language; andderiving the alignment for the pair of sentences comprises combining the first alignment model and the second alignment model. - The computer storage medium of any one of claim 6 and claim 9 as dependent of claim 7, wherein the computer program further comprises instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:each of the two directional alignment models are hidden Markov alignment models.
- A system comprising:one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:receiving data characterizing two directional alignment models for a pair of sentences, one sentence in a first language and the other sentence in a different second language;deriving a combined bidirectional alignment model from the two directional alignment models; andevaluating the bidirectional alignment model and deriving an alignment for the pair of sentences from the evaluation of the bidirectional alignment model.
- The system of claim 11, wherein the one or more computers and one or more storage devices further stores instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:the bidirectional model embeds the two directional alignment models and an additional structure that resolves the predictions of the embedded models into a single symmetric word alignment.
- The system of any one of claim 11 and 12, wherein the one or more computers and one or more storage devices further stores instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:evaluating the bidirectional alignment model generates an alignment solution.
- The system of any one of claim 11 and 12, wherein the one or more computers and one or more storage devices further stores instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:evaluating the bidirectional alignment model generates two alignment solutions,
wherein the first solution is an alignment model in a first direction from the first language to the second language and the second solution is an alignment model in a second direction from the second language to the first language; andderiving the alignment for the pair of sentences comprises combining the first alignment model and the second alignment model. - The system of any one of claim 11 and claim 14 as dependent of claim 12, wherein the one or more computers and one or more storage devices further stores instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:each of the two directional alignment models are hidden Markov alignment models.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201061424608P | 2010-12-17 | 2010-12-17 | |
US13/090,244 US20120158398A1 (en) | 2010-12-17 | 2011-04-19 | Combining Model-Based Aligner Using Dual Decomposition |
Publications (1)
Publication Number | Publication Date |
---|---|
EP2466489A1 true EP2466489A1 (en) | 2012-06-20 |
Family
ID=45495634
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP11193828A Withdrawn EP2466489A1 (en) | 2010-12-17 | 2011-12-15 | Combining model-based aligners |
Country Status (5)
Country | Link |
---|---|
US (1) | US20120158398A1 (en) |
EP (1) | EP2466489A1 (en) |
JP (1) | JP2012138085A (en) |
KR (1) | KR20120089793A (en) |
CN (1) | CN102681984A (en) |
Families Citing this family (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104750676B (en) * | 2013-12-31 | 2017-10-24 | 橙译中科信息技术（北京）有限公司 | Machine translation processing method and processing device |
US9898458B2 (en) | 2015-05-08 | 2018-02-20 | International Business Machines Corporation | Generating distributed word embeddings using structured information |
KR102449614B1 (en) * | 2015-11-06 | 2022-09-29 | 삼성전자주식회사 | Apparatus and method for evaluating machine translation quality using distributed representation, machine translation apparatus, and apparatus for constructing distributed representation model |
US10565511B1 (en) * | 2018-10-01 | 2020-02-18 | Microsoft Technology Licensing, Llc | Reverse debugging of software failures |
CN109887484B (en) * | 2019-02-22 | 2023-08-04 | 平安科技（深圳）有限公司 | Dual learning-based voice recognition and voice synthesis method and device |
US11334827B1 (en) * | 2019-06-03 | 2022-05-17 | Blue Yonder Group, Inc. | Image-based decomposition for fast iterative solve of complex linear problems |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090112573A1 (en) * | 2007-10-30 | 2009-04-30 | Microsoft Corporation | Word-dependent transition models in HMM based word alignment for statistical machine translation |
Family Cites Families (50)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5523946A (en) * | 1992-02-11 | 1996-06-04 | Xerox Corporation | Compact encoding of multi-lingual translation dictionaries |
US5493606A (en) * | 1994-05-31 | 1996-02-20 | Unisys Corporation | Multi-lingual prompt management system for a network applications platform |
US7734459B2 (en) * | 2001-06-01 | 2010-06-08 | Microsoft Corporation | Automatic extraction of transfer mappings from bilingual corpora |
WO2003005166A2 (en) * | 2001-07-03 | 2003-01-16 | University Of Southern California | A syntax-based statistical translation model |
AU2003220606A1 (en) * | 2002-03-27 | 2003-10-13 | Universiity Of Southern California | Phrase- based joint probability model for statistical machine translation |
US7353165B2 (en) * | 2002-06-28 | 2008-04-01 | Microsoft Corporation | Example based machine translation system |
US7349839B2 (en) * | 2002-08-27 | 2008-03-25 | Microsoft Corporation | Method and apparatus for aligning bilingual corpora |
US7249012B2 (en) * | 2002-11-20 | 2007-07-24 | Microsoft Corporation | Statistical method and apparatus for learning translation relationships among phrases |
US20050033567A1 (en) * | 2002-11-28 | 2005-02-10 | Tatsuya Sukehiro | Alignment system and aligning method for multilingual documents |
JP3973549B2 (en) * | 2002-12-19 | 2007-09-12 | 沖電気工業株式会社 | Bilingual dependency structure associating apparatus, method and program, and recording medium recording parallel translation dependency structure associating program |
US7356457B2 (en) * | 2003-02-28 | 2008-04-08 | Microsoft Corporation | Machine translation using learned word associations without referring to a multi-lingual human authored dictionary of content words |
US7698125B2 (en) * | 2004-03-15 | 2010-04-13 | Language Weaver, Inc. | Training tree transducers for probabilistic operations |
US8296127B2 (en) * | 2004-03-23 | 2012-10-23 | University Of Southern California | Discovery of parallel text portions in comparable collections of corpora and training using comparable texts |
JP2006024114A (en) * | 2004-07-09 | 2006-01-26 | Advanced Telecommunication Research Institute International | Mechanical translation device and mechanical translation computer program |
US7546235B2 (en) * | 2004-11-15 | 2009-06-09 | Microsoft Corporation | Unsupervised learning of paraphrase/translation alternations and selective application thereof |
US7672830B2 (en) * | 2005-02-22 | 2010-03-02 | Xerox Corporation | Apparatus and methods for aligning words in bilingual sentences |
US7680647B2 (en) * | 2005-06-21 | 2010-03-16 | Microsoft Corporation | Association-based bilingual word alignment |
US20070083357A1 (en) * | 2005-10-03 | 2007-04-12 | Moore Robert C | Weighted linear model |
US7957953B2 (en) * | 2005-10-03 | 2011-06-07 | Microsoft Corporation | Weighted linear bilingual word alignment model |
CN101030197A (en) * | 2006-02-28 | 2007-09-05 | 株式会社东芝 | Method and apparatus for bilingual word alignment, method and apparatus for training bilingual word alignment model |
CN101030196B (en) * | 2006-02-28 | 2010-05-12 | 株式会社东芝 | Method and apparatus for training bilingual word alignment model, method and apparatus for bilingual word alignment |
US7542893B2 (en) * | 2006-05-10 | 2009-06-02 | Xerox Corporation | Machine translation using elastic chunks |
US7725306B2 (en) * | 2006-06-28 | 2010-05-25 | Microsoft Corporation | Efficient phrase pair extraction from bilingual word alignments |
US20080154577A1 (en) * | 2006-12-26 | 2008-06-26 | Sehda,Inc. | Chunk-based statistical machine translation system |
US8788258B1 (en) * | 2007-03-15 | 2014-07-22 | At&T Intellectual Property Ii, L.P. | Machine translation using global lexical selection and sentence reconstruction |
US8185375B1 (en) * | 2007-03-26 | 2012-05-22 | Google Inc. | Word alignment with bridge languages |
US8326598B1 (en) * | 2007-03-26 | 2012-12-04 | Google Inc. | Consensus translations from multiple machine translation systems |
US7983898B2 (en) * | 2007-06-08 | 2011-07-19 | Microsoft Corporation | Generating a phrase translation model by iteratively estimating phrase translation probabilities |
US8548791B2 (en) * | 2007-08-29 | 2013-10-01 | Microsoft Corporation | Validation of the consistency of automatic terminology translation |
US8046211B2 (en) * | 2007-10-23 | 2011-10-25 | Microsoft Corporation | Technologies for statistical machine translation based on generated reordering knowledge |
CN101464856A (en) * | 2007-12-20 | 2009-06-24 | 株式会社东芝 | Alignment method and apparatus for parallel spoken language materials |
US8229728B2 (en) * | 2008-01-04 | 2012-07-24 | Fluential, Llc | Methods for using manual phrase alignment data to generate translation models for statistical machine translation |
US8060358B2 (en) * | 2008-03-24 | 2011-11-15 | Microsoft Corporation | HMM alignment for combining translation systems |
US8229729B2 (en) * | 2008-03-25 | 2012-07-24 | International Business Machines Corporation | Machine translation in continuous space |
US8594992B2 (en) * | 2008-06-09 | 2013-11-26 | National Research Council Of Canada | Method and system for using alignment means in matching translation |
US20110295857A1 (en) * | 2008-06-20 | 2011-12-01 | Ai Ti Aw | System and method for aligning and indexing multilingual documents |
US8150677B2 (en) * | 2008-06-26 | 2012-04-03 | Microsoft Corporation | Machine translation using language order templates |
US20090326916A1 (en) * | 2008-06-27 | 2009-12-31 | Microsoft Corporation | Unsupervised chinese word segmentation for statistical machine translation |
CN101685441A (en) * | 2008-09-24 | 2010-03-31 | 中国科学院自动化研究所 | Generalized reordering statistic translation method and device based on non-continuous phrase |
US9176952B2 (en) * | 2008-09-25 | 2015-11-03 | Microsoft Technology Licensing, Llc | Computerized statistical machine translation with phrasal decoder |
KR20100037813A (en) * | 2008-10-02 | 2010-04-12 | 삼성전자주식회사 | Statistical machine translation apparatus and method |
CN101996631B (en) * | 2009-08-28 | 2014-12-03 | 国际商业机器公司 | Method and device for aligning texts |
US8930176B2 (en) * | 2010-04-01 | 2015-01-06 | Microsoft Corporation | Interactive multilingual word-alignment techniques |
US9201871B2 (en) * | 2010-06-11 | 2015-12-01 | Microsoft Technology Licensing, Llc | Joint optimization for machine translation system combination |
US9092483B2 (en) * | 2010-10-19 | 2015-07-28 | Microsoft Technology Licensing, Llc | User query reformulation using random walks |
US8775155B2 (en) * | 2010-10-25 | 2014-07-08 | Xerox Corporation | Machine translation using overlapping biphrase alignments and sampling |
US8612204B1 (en) * | 2011-03-30 | 2013-12-17 | Google Inc. | Techniques for reordering words of sentences for improved translation between languages |
WO2012170817A1 (en) * | 2011-06-10 | 2012-12-13 | Google Inc. | Augmenting statistical machine translation with linguistic knowledge |
US8935151B1 (en) * | 2011-12-07 | 2015-01-13 | Google Inc. | Multi-source transfer of delexicalized dependency parsers |
US9311293B2 (en) * | 2012-04-13 | 2016-04-12 | Google Inc. | Techniques for generating translation clusters |
-
2011
- 2011-04-19 US US13/090,244 patent/US20120158398A1/en not_active Abandoned
- 2011-12-13 KR KR1020110133765A patent/KR20120089793A/en not_active Application Discontinuation
- 2011-12-15 JP JP2011274598A patent/JP2012138085A/en active Pending
- 2011-12-15 EP EP11193828A patent/EP2466489A1/en not_active Withdrawn
- 2011-12-19 CN CN2011104274717A patent/CN102681984A/en active Pending
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090112573A1 (en) * | 2007-10-30 | 2009-04-30 | Microsoft Corporation | Word-dependent transition models in HMM based word alignment for statistical machine translation |
Non-Patent Citations (15)
Title |
---|
ALEXANDER M. RUSH; DAVID SONTAG; MICHAEL COLLINS; TOMMI JAAKKOLA: "On dual decomposition and linear programming relaxations for natural language processing", PROCEEDINGS OF THE CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, 2010 |
ALEXANDER M. RUSH; DAVID SONTAG; MICHAEL COLLINS; TOMMI JAAKKOLA: "On dual decomposition and linear programming relaxations for natural language processing", PROCEEDINGS OF THE CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, 9 October 2010 (2010-10-09), XP055024305 * |
FRANZ JOSEF OCH; CHRISTOPHER TILLMAN; HERMANN NEY: "Improved alignment models for statistical machine translation", PROCEEDINGS OF THE CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, 1999 |
HIROYUKI SHINDO; AKINORI FUJINO; MASAAKI NAGATA: "Word alignment with synonym regularization", PROCEEDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, 2010 |
JAMIE BRUNNING; ADRIA DE GISPERT; WILLIAM BYRNE: "Context- dependent alignment models for statistical machine translation", PROCEEDINGS OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, 2009 |
JOAO GRAÇA ET AL: "Expectation maximization posterior constraints", NEURAL INFORMATION PROCESSING SYSTEMS CONFERENCE (NIPS), 4 December 2007 (2007-12-04), Vancouver, BC, Canada, XP055024313, Retrieved from the Internet <URL:http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.4545&rep=rep1&type=pdf> [retrieved on 20120412] * |
JOHN DENERO ET AL: "Model-based aligner combination using dual decomposition", HLT '11 PROCEEDINGS OF THE 49TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, 19 June 2011 (2011-06-19), Portland, Oregon, USA, pages 420 - 429, XP055024146 * |
KUZMAN GANCHEV ET AL: "Better Alignments = Better Translations?", REPRINTED FROM: BETTER ALIGNMENTS = BETTER TRANSLATIONS? KUZMAN GANCHEV, JOAO GRACA AND BEN TASKAR. IN PROCEEDINGS OF THE 46TH ANNUAL MEETING OF THE ASSOCIATION OF COMPUTATIONAL LINGUISTICS, 16 June 2008 (2008-06-16), Columbus, Ohio, USA, XP055024295, Retrieved from the Internet <URL:http://repository.upenn.edu/cgi/viewcontent.cgi?article=1042&context=grasp_papers> [retrieved on 20120412] * |
OCH F J ET AL: "Improved Statistical Alignment Models", PROCEEDINGS OF THE ANNUAL MEETING OF THE ACL, XX, XX, 2 October 2000 (2000-10-02), pages 440 - 447, XP002279144 * |
PERCY LIANG ET AL: "Alignment by agreement", PROCEEDING HLT-NAACL '06 PROCEEDINGS OF THE MAIN CONFERENCE ON HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION OF COMPUTATIONAL LINGUISTICS, 4 June 2006 (2006-06-04), Stroudsburg, PA, USA, XP055024271, Retrieved from the Internet <URL:http://www.cs.berkeley.edu/~pliang/papers/alignment-naacl2006.pdf> [retrieved on 20120411] * |
STEPHAN VOGEL; HERMANN NEY; CHRISTOPH TILLMANN: "HMM-Based Word Alignment in Statistical Translation", PROCEEDINGS OF THE 16TH CONFERENCE ON COMPUTATIONAL LINGUISTICS, 1996, pages 836 - 841, XP002673705 * |
STEPHAN VOGEL; HERMANN NEY; CHRISTOPH TILLMANN: "HMM-Based Word, Alignment in Statistical Translation", PROCEEDINGS OF THE 16TH CONFERENCE ON COMPUTATIONAL LINGUISTICS, 1996 |
TERRY KOO; ALEXANDER M. RUSH; MICHAEL COLLINS; TOMMI JAAKKOLA; DAVID SONTAG: "Dual decomposition for parsing with non-projective head automata", PROCEEDINGS OF THE CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, 2010 |
XIAODONG HE: "Using word- dependent transition models in HMM based word alignment for statistical machine", ACL WORKSHOP ON STATISTICAL MACHINE TRANSLATION, 2007 |
YONGGANG DENG ET AL: "Optimizing word alignment combination for phrase table training", PROCEEDINGS OF THE ACL-IJCNLP 2009 CONFERENCE SHORT PAPERS, 2 August 2009 (2009-08-02), Stroudsburg, PA, USA, pages 229 - 232, XP055024324 * |
Also Published As
Publication number | Publication date |
---|---|
CN102681984A (en) | 2012-09-19 |
US20120158398A1 (en) | 2012-06-21 |
KR20120089793A (en) | 2012-08-13 |
JP2012138085A (en) | 2012-07-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Alvarez-Melis et al. | A causal framework for explaining the predictions of black-box sequence-to-sequence models | |
Kim et al. | Unsupervised recurrent neural network grammars | |
EP4007951B1 (en) | Multi-lingual line-of-code completion system | |
US11972365B2 (en) | Question responding apparatus, question responding method and program | |
Xie | Neural text generation: A practical guide | |
Cohen et al. | Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction | |
Täckström et al. | Efficient inference and structured learning for semantic role labeling | |
Dalvi et al. | Understanding and improving morphological learning in the neural machine translation decoder | |
US10025778B2 (en) | Training markov random field-based translation models using gradient ascent | |
EP2466489A1 (en) | Combining model-based aligners | |
Nguyen et al. | A deep neural network language model with contexts for source code | |
US8504354B2 (en) | Parallel fragment extraction from noisy parallel corpora | |
KR102195223B1 (en) | Globally normalized neural networks | |
Nguyen et al. | T2api: Synthesizing api code usage templates from english texts with statistical translation | |
CN110140133A (en) | The implicit bridge joint of machine learning task | |
US20170068665A1 (en) | Word alignment score computing apparatus, word alignment apparatus, and computer program | |
US10657296B2 (en) | Techniques for using controlled natural language to capture design intent for computer-aided design | |
DeNero et al. | Model-based aligner combination using dual decomposition | |
CA2779349A1 (en) | Predictive analysis by example | |
WO2023109436A1 (en) | Part of speech perception-based nested named entity recognition method and system, device and storage medium | |
Venugopal et al. | Preference grammars: Softening syntactic constraints to improve statistical machine translation | |
Anwar et al. | A natural language processing (nlp) framework for embedded systems to automatically extract verification aspects from textual design requirements | |
Domingo et al. | Spelling normalization of historical documents by using a machine translation approach | |
Zhou | Statistical machine translation for speech: A perspective on structures, learning, and decoding | |
Hu et al. | Improved beam search with constrained softmax for nmt |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE APPLICATION IS DEEMED TO BE WITHDRAWN |
|
18D | Application deemed to be withdrawn |
Effective date: 20121221 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230519 |