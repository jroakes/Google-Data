CN108351805A - Calculate the accelerator processing based on stream of figure - Google Patents
Calculate the accelerator processing based on stream of figure Download PDFInfo
- Publication number
- CN108351805A CN108351805A CN201680063365.6A CN201680063365A CN108351805A CN 108351805 A CN108351805 A CN 108351805A CN 201680063365 A CN201680063365 A CN 201680063365A CN 108351805 A CN108351805 A CN 108351805A
- Authority
- CN
- China
- Prior art keywords
- node
- subgraph
- equipment
- specific
- stream
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000012545 processing Methods 0.000 title claims abstract description 32
- 238000004590 computer program Methods 0.000 claims abstract description 26
- 230000004044 response Effects 0.000 claims abstract description 19
- 230000015654 memory Effects 0.000 claims description 90
- 238000000034 method Methods 0.000 claims description 45
- 238000004364 calculation method Methods 0.000 claims description 15
- 238000012549 training Methods 0.000 claims description 11
- 230000006399 behavior Effects 0.000 claims description 8
- 210000004218 nerve net Anatomy 0.000 claims 1
- 238000013528 artificial neural network Methods 0.000 description 30
- 230000009471 action Effects 0.000 description 11
- 239000011159 matrix material Substances 0.000 description 11
- 230000001537 neural effect Effects 0.000 description 8
- 230000008569 process Effects 0.000 description 8
- 238000004891 communication Methods 0.000 description 5
- 238000010586 diagram Methods 0.000 description 5
- 238000010801 machine learning Methods 0.000 description 5
- 230000008859 change Effects 0.000 description 4
- 238000000926 separation method Methods 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000000712 assembly Effects 0.000 description 2
- 238000000429 assembly Methods 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 2
- 230000005611 electricity Effects 0.000 description 2
- 230000011218 segmentation Effects 0.000 description 2
- 230000009118 appropriate response Effects 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000018109 developmental process Effects 0.000 description 1
- 239000004744 fabric Substances 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 210000003127 knee Anatomy 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000009897 systematic effect Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5005—Allocation of resources, e.g. of the central processing unit [CPU] to service a request
- G06F9/5027—Allocation of resources, e.g. of the central processing unit [CPU] to service a request the resource being a machine, e.g. CPUs, Servers, Terminals
- G06F9/5038—Allocation of resources, e.g. of the central processing unit [CPU] to service a request the resource being a machine, e.g. CPUs, Servers, Terminals considering the execution order of a plurality of tasks, e.g. taking priority or time dependency constraints into consideration
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5061—Partitioning or combining of resources
- G06F9/5066—Algorithms for mapping a plurality of inter-dependent sub-tasks onto a plurality of physical CPUs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Abstract
Mthods, systems and devices, including the computer program that is coded on computer storage media, for receiving the request that processing calculates figure by calculating drawing system；Obtain the data for indicating the subgraph for calculating figure, the calculating figure includes multiple nodes and directed edge, wherein each node indicates corresponding operation, corresponding first node is connected to corresponding second node by wherein each directed edge, and the subgraph is assigned to the first equipment by the distributor in the calculating drawing system；Determine that first equipment includes the hardware accelerator for having multiple streams；Instruction is generated in response to determining, described instruction makes first equipment when being executed by first equipment：The operation indicated by each node in the subgraph is assigned to respective streams；And the operation indicated by the node in the subgraph is executed according to the appointment.
Description
Background technology
This specification is related to by the way that subgraph to be assigned to the accelerator facility with multiple streams (for example, graphics processing unit
(GPU)) it indicates the calculating figure of neural network to handle and/or is related to this processed calculating figure for handling model
Input.
Neural network is to generate output (for example, one using one or more layers of model to be directed to the input received
Or multiple classification) machine learning model.Some neural networks further include one or more hide other than including output layer
Layer.The output of each hidden layer is used as the defeated of next layer (that is, next hidden layer or output layer of network) in network
Enter.Each layer of network generates output according to the current value of the corresponding parameter sets for layer from the input received.
In existing system, calculating the operation of figure can be handled by individual plants.In some embodiments, equipment is
GPU.Equipment, which can have, to be executed operation (for example, generating output from input at one layer) and is stored in the output from operation
Processor in memory.The big figure and size of the commonly required operation wanted are exported due to being generated in calculating figure, one sets
It is standby to take a significant amount of time to handle the operation of figure.
Invention content
In general, this specification describe it is a kind of based on being handled using the accelerator facility (for example, GPU) based on stream
The system or method of the subgraph of nomogram.
In general, the side of the available action for including the following terms of a novel aspects of theme described in this specification
Method is implemented：Receive the request that processing calculates figure；Obtain the data for indicating the subgraph for calculating figure, the calculating figure
Including multiple nodes and directed edge, wherein each node indicates corresponding operation, wherein each directed edge is by corresponding first segment
Point is connected to corresponding second node, and corresponding second node indicates to receive the behaviour indicated by corresponding first node
The output operation as input of work, the subgraph are assigned to the first equipment by the distributor in the calculating drawing system；It determines
First equipment includes the hardware accelerator for having multiple streams；Include that there are multiple streams in response to determination first equipment
Graphics processing unit and generate instruction, described instruction makes first equipment when being executed by first equipment：It will be by described
The operation that each node in subgraph indicates is assigned to the respective streams in multiple streams of the graphics processing unit；And root
The operation indicated by the node in the subgraph is executed according to the appointment；And by described instruction and the data
It is supplied to first equipment.Method in terms of this can be computer implemented method.Method in terms of this can be by
One or more computing devices execute, such as are executed by one or more computing devices including calculating drawing system.
Embodiment may include one or more of following characteristics.The specified identification of request is in the subgraph
One or more specific outputs of one or more respective nodes, further comprise：It is received from first equipment one
Or multiple specific outputs；And one or more of specific outputs are supplied to the client.Described instruction further makes
One or more of specific outputs are stored in the memory of first equipment by first equipment.For the subgraph
The operation include for neural network part reasoning or training calculate.The subgraph is analyzed to identify institute in chain structure
State the node group in subgraph；Wherein, described instruction makes first equipment that the node group is assigned to a stream.The appointment packet
It includes：The subgraph is analyzed to identify that the first node in the subgraph has multiple directed edges as output；The wherein described finger
Order makes first equipment that the node that the directed edge is directed toward are assigned to institute for each directed edge in the directed edge
State unique stream of graphics processing unit.Described instruction makes first equipment for each node come based on to described in the node
Directed edge determines that the corresponding memory in the graphics processing unit consumed by the operation indicated by the node provides
It measures in source, wherein the appointment is at least based on corresponding amount of memory resources.Described instruction make first equipment determine by
The specific operation that node indicates terminates at specific stream；Terminate in response to the determination specific operation：Determination will be released
The first memory amount consumed by the specific operation；For each unassigned node in unassigned node group come really
The fixed corresponding estimation amount of memory consumed by the unassigned node；It determines to have from the unassigned node group and makes described the
First unassigned node of the maximized estimation amount of memory of the use of one amount of memory；And will not it be referred to by described first
The operation that node indicates is sent to be assigned to the specific stream.
In one embodiment the method further includes：Receive mode input；And by the hardware accelerator
The mode input is handled according to the operation indicated by the node in the subgraph.
In another aspect, theme described in this specification can be added with the method for the action that may include the following terms
With specific implementation：Machine learning mould corresponding with the processed calculating figure that the method by the first aspect obtains is provided
Type；And handle mode input using the machine learning model.
In another aspect, theme described in this specification can be added with the method for the action that may include the following terms
With specific implementation：The subgraph of the processed calculating figure obtained by the method for the first aspect is executed by hardware accelerator.
Calculating figure described in these areas can be the expression of such as machine learning model of neural network.
Another novel aspects include the action of the following terms：It is received by the graphics processing unit with multiple streams described in indicating
The data of the subgraph of figure are calculated, the calculating figure includes multiple nodes and directed edge, wherein each node indicates corresponding operation,
Corresponding first node is connected to corresponding second node by wherein each directed edge, and corresponding second node indicates to receive
By the output operation as input for the operation that corresponding first node indicates, the subgraph is by the cloth in calculating drawing system
It puts device and is assigned to graphics processing unit；The operation indicated by each node in the subgraph is assigned to graphics process list
Respective streams in multiple streams of member；And the behaviour indicated by the node in the subgraph is executed according to the appointment
Make.
Embodiment may include one or more of following characteristics.Receive one or more of the identification from the subgraph
The request of one or more specific outputs of respective nodes；And one or more of specific outputs are supplied to the client
End.Receive the data that the node group in the subgraph is identified in chain structure；And the node group is assigned to a stream.It is described
Appointment includes：Receive data of the identification with multiple directed edges as the first node of output in the subgraph；And it is directed to
The node that the directed edge is directed toward is assigned to the unique of the graphics processing unit by each directed edge in the directed edge
Stream.It is consumed by the operation indicated by the node based on the directed edge determination to the node for each node
Corresponding amount of memory resources in the graphics processing unit, wherein the appointment is at least based on corresponding memory
Stock number.Determine that the specific operation indicated by node terminates in specific stream；Terminate in response to the determination specific operation,
Determine the first memory amount consumed by the specific operation that will be released；Not for each in unassigned node group
Node is assigned to determine the corresponding estimation amount of memory consumed by the unassigned node；It is determined from the unassigned node group
With the first unassigned node for making the maximized estimation amount of memory of the use of the first memory amount；And it will be by
The operation that the first unassigned node indicates is assigned to the specific stream.
The other embodiment of these and other aspects includes being configured to execute to be coded in computer memory device (its
May or may not be non-transitory storage device) on method action correspondence system, device and computer program.
The specific embodiment of theme described in this specification can be implemented to realize one or more in following advantages
It is a.The operation (for example, operation for generating reasoning from input) of neural network may be expressed as the calculating of node and directed edge
Figure.System handles this calculation chart and shows to be effectively carried out operation.System realizes this efficiency, because calculating figure with multiple
Stream.It is reordered or is performed simultaneously using the permissible operation being logically independent of multiple streams.It is reduced entirely when system has
When the target of the end-to-end delay of calculating, example system can resequence the operation being logically independent.It is realized when system has
When the target of high throughput, example system can simultaneously execute operation.It, can be for parallel work-flow more compared with routinely indicating
Easily separation calculation figure.As explanation, the subgraph for calculating figure can be assigned to Unique Device, each is in corresponding son
Operation is executed in figure, to reduce the operation required total time for executing neural network.
The equipment that subgraph is assigned to can be GPU.Can by dividing sub-picture at multiple streams of GPU more efficiently to execute
The operation of subgraph.The one or more embodiments of the detail of the theme of this specification are elaborated in following attached drawing and description.
Other feature, aspect and the advantage of theme will become apparent from specification, drawings and the claims.It is to be understood that
Various aspects and embodiment can be combined, and can be realized in other aspects or in the context of embodiment in one aspect or real
Apply the feature described in the context of mode.
Description of the drawings
Example calculations drawing system of Fig. 1 diagrams for the neural network distributed operation to be expressed as calculating figure.
Fig. 2 be for use GPU handle calculating figure subgraph instantiation procedure flow chart.
The example subgraph for calculating figure that Fig. 3 diagrams are handled by GPU.
Fig. 4 is the flow chart of the instantiation procedure for node to be assigned to stream.
Similar reference numeral and title indicate similar element in various figures.
Specific implementation mode
This specification is generally described the calculating drawing system for executing the operation shown by calculation chart in a distributed way.
Calculating figure includes the node connected by directed edge.It calculates each node in figure and indicates an operation.To node
Incoming side indicate to the input in node stream, that is, by node indicate operation input.Outflow side from node indicates
The stream of the output of the operation indicated by node is to be used as the input of the operation indicated by another node.Therefore, by figure
One node be connected to the second node in figure directed edge instruction by by first node indicate operation generate output by with
Make the input of the operation indicated by second node.
Usually, it is tensor along outputting and inputting for directed edge flowing in calculating figure.Tensor is numerical value or other values
The Multidimensional numerical of (for example, character string), the numerical value or other values have certain order corresponding with the dimension of array.Example
Such as, scalar value is 0 rank tensor, and the vector of numerical value is 1 rank tensor, and matrix is 2 rank tensors.
In some embodiments, the operation indicated in calculating figure is that neural network operates or be directed to different types of machine
The operation of learning model.Neural network is to be directed to the input prediction received output using one or more non-linear unit layers
Machine learning model.Some neural networks are other than including output layer further include one or more hidden layers depth god
Through network.The output of each hidden layer is used as the defeated of another layer (that is, another hidden layer, output layer or both) in network
Enter.Some layers of network generate output according to the current value of corresponding parameter set from the input received, however network is other
Layer can not have parameter.
For example, can be that neural computing reasoning is located by the layer of neural network by the operation that calculation chart shows
Reason input thinks that the input generates operation necessary to neural network output.As another example, the behaviour shown by calculation chart
Make adjust the value of the parameter of neural network (for example, with according to the first of parameter by executing neural metwork training program
Initial value determines the trained values of parameter) it is operated necessary to neural network to train.In some cases, for example, in neural network
Training during, by the operation that calculation chart shows may include by neural network multiple copies execute operation.
As explanation, received from preceding layer input neural net layer parameter matrix can be used execute parameter matrix with it is defeated
Matrix multiplication between entering.In some cases, the multiple nodes that this matrix multiplication can be expressed as in calculating figure.Example
Such as, matrix multiplication is divided into multiple multiplication and add operation, and each operation can by the different nodes in calculating figure Lai
It indicates.Corresponding output is produced by the operation that each node indicates, the output is flowed on directed edge to subsequent node.By
After the result for the operation generator matrix multiplication that finish node indicates, which flows to another node on directed edge.The result
It is equivalent to the output for the neural net layer for executing matrix multiplication.
In some other cases, matrix multiplication is represented as a node in figure.The operation indicated by node can connect
The weight tensor (for example, parameter matrix) on input the second directed edge of tensor sum on the first directed edge is received as input.One
In a little embodiments, weight tensor is associated with the shared permanent state of model.Node, which can be handled, for example executes input and weight
The matrix multiplication of tensor, to export the output tensor for the output for being equivalent to neural net layer on third directed edge.
The other neural networks that can be indicated by the node in calculating figure are operated including other mathematical operations, for example, subtraction,
Division and gradient calculate；Array operation, for example, cascade (concatenate), splicing (splice), segmentation (split) or sequence
(rank)；And the operation of neural network configuration block, for example, SoftMax, Sigmoid, rectification linear unit (ReLU) or convolution.
By Neural Networks Representation for calculating figure provide it is a kind of flexibly and granular mode efficiently realizes neural network, especially
It is if the operation of neural network is across the multiple equipment distribution with different hardware profiles.
Example calculations drawing system 100 of Fig. 1 diagrams for the neural network distributed operation to be expressed as calculating figure.System
100 be the example that the system on the computer of one or more of one or more positions is realized as computer program,
In may be implemented in systems described below, component and technology.
The user of client 102 can ask to indicating that the calculating figure of neural network executes action.For example, client can be to meeting
Manager registration figure is talked about, enters data into and is fed in figure, or assess one or more of the output of figure.Client 102
It can be the application run on computers.
As a part for request, client 102 to system 100 provide identification calculate figure data and it is specified will be to meter
The type for the action that nomogram executes.
It indicates the calculating figure for the reasoning of specific neural network for example, request is recognizable and can recognize that and should hold on it
The input of row reasoning.
As another example, the recognizable expression of request is directed to the calculating figure of the training process of specific neural network and can know
Trained input, such as training data should not be executed on it.In this example, training program is indicated when receiving to handle
When calculating the request of figure, system 100 can be for example used for using classical back propagation or other neural metwork training technologies to determine
Calculate the modified values of the parameter on one or more sides of figure.The parameter changed can be stored in the memory of equipment by system 100
In, and actuator 106 can retrieve and store the address for the weight changed at system 100.Based on from client 102
For the further request of the reasoning for the weight for needing to have changed, training or other operations, address can be used to access in system 100
The weight changed.
In some cases, request can specify the response that should respond to transmit in request.For example, for neural network
Train request, client 102 can ask requested neural metwork training to operate completed instruction, and optionally, request
The instruction for the memory location that the trained values or trained values of the parameter of neural network can be accessed by client 102.Show as another
Example, asks ANN Reasoning, and client 102 can ask to indicate pushing away for one or more specific nodes from calculating figure
Manage the output valve of operation.
System 100 executes operation with by dividing the operation next life shown by calculation chart across multiple equipment 116-122
At specific output.System 100 is divided by data communication network 114 (for example, LAN (LAN) or wide area network (WAN)) to more
The operation of a equipment 116-122.Equipment 116-122 executes operation, and if applicable, then will export or indicate accordingly to return
To system 100, requested output or instruction can be returned to client 102 by the system 100.
Any equipment (for example, equipment 116-122) for executing neural network operation may include for storing instruction and data
Memory (for example, random access memory (RAM)) and for execute storage instruction processor.Usually, each
Equipment is independently of the hardware resource that miscellaneous equipment executes operation.For example, each equipment can be with the processing unit of own.If
Standby can be graphics processing unit (GPU), central processing unit (CPU) or other accelerators.As explanation, a machine can be held in the palm
Pipe one or more equipment, for example, multiple CPU and GPU.
Each equipment can also have corresponding computing capability.That is, equipment can have different amount of memory, processing
Speed or other architected features.Therefore, some equipment can perform the inexecutable operation of miscellaneous equipment.For example, some operations need
Will certain amount of memory or some equipment that only particular device has be configured to only execute certain types of operation, example
Such as, reasoning operates.
Session manager 104 in system 100 receives the session that the startup from client 102 executes the operation for calculating figure
Request.The set of the equipment of the executable operation for calculating figure of the management of session manager 104, for example, equipment 116-122, and
The set for the equipment that can be used for executing operation can be provided to distributor 108.
For each operation to be executed in calculation chart, distributor 108 determines the corresponding target for executing the operation
Equipment, for example, equipment 116, and in some embodiments, determine that corresponding target device executes the time of operation.It lays
Device 108 by know in the case of the size of given input data operation will be spent in each available devices how long come
Best equipment is executed to assign.Distributor 108 obtains the estimation of processing time using measurement or estimated performance model.It can be concurrently
Some operations are executed, however other operations need to complete formerly operation in calculating figure, for example, other operation processings formerly operate
Output as input.
After equipment executes the operation distributed by distributor 108 to generate output, actuator 106 can search and output.It holds
Row device 106 produces the appropriate response to request, for example, handling completed output or instruction.Then, actuator 106 can incite somebody to action
Response returns to client 102.Although Fig. 1 illustrates an actuator 106, in one embodiment, have one per equipment
A actuator.When operation becomes that (i.e. its all input has been calculated) can be run, this actuator sends out operation to equipment.This
Embodiment also has figure manager, and the figure manager is by calling distributor 108 come segmentation figure to be transported in multiple equipment
It goes and creates necessary actuator.
Session manager 104 also provides the operational set to be executed in calculating figure to actuator 106.Actuator 106 from
It is executed in related equipment 116-122 with the figure of operation and periodically retrieves run time statistics amount.When actuator 106 will be run
Between statistic be supplied to distributor 108, the distributor 108 can re-optimization further operating laying and dispatching.
Fig. 2 be for use GPU handle calculating figure subgraph instantiation procedure 200 flow chart.For convenience, process
200 will be described as being executed by being located at one or more of one or more positions system for computer.For example, suitably compiling
The calculating drawing system (for example, calculating drawing system 100 of Fig. 1) of journey can perform process 200.
System receives the request (step 202) that the processing from client calculates figure.For example, request can be to specified
Input executes the request of the ANN Reasoning shown by calculation chart, executes specified training data set by calculating figure
The request of the neural metwork training operation of expression or the request for executing the other neural networks operation shown by calculation chart, as above
Face is with reference to described in figure 1.
In some cases, calculating figure is sent together with from the request of client.In other cases, request identification
Calculating figure and system retrieve the data for indicating identified figure from memory.
Calculating figure can be divided into multiple subgraphs by system.In some embodiments, subgraph is by sending the client asked
It is specified, and system is according to specification separation calculation figure.In some other embodiments, System Partition calculates figure so that each
Subgraph needs similar stock number for executing operation compared with other subgraphs.
Each subgraph for example can be assigned to available devices by system using the distributor of Fig. 1 108.
System obtains the data (step 204) for the specific subgraph for indicating calculating figure from the calculating figure divided.It can be from system
Database or memory obtain data.As explanation, the operation of specific subgraph indicates that part reasoning or training calculate.
System determines that the equipment that subgraph is assigned to is the graphics processing unit with multiple streams or other hardware accelerators
Equipment (step 206).As explanation, system can be set by be assigned to the explorer request of the equipment of calculating figure from management
Standby type comes whether assessment equipment is the GPU with multiple streams.Each stream is the separate hardware team for handling its operation in order
Row.
System generates the instruction (step 208) for making the equipment execute specific operation when being executed by equipment.Particularly, it instructs
Make equipment that the operation indicated by each node in subgraph to be assigned to the respective streams of equipment.
The calculating of some hardware accelerators can be assigned to stream (for example, if an operation by example system in a specific way
It is executed on stream A, then relevant operation later must also execute on stream A).For example, first operation can be it is stateful and
It is executed on stream A.By executing, the first operation can change hardware in a manner of it must occur before the second operation executes
Internal state.After the first operation is completed, then the second operation can execute on stream A.
In some embodiments, two internal hardware resources cannot be used simultaneously and therefore be needed serial
Change.
Usually, equipment will not depend on mutual operation and be assigned to different stream.By will not depend on mutual behaviour
Be assigned to different stream, hardware require no knowledge about operation will spend how long and can be chosen from many available actions with
Execute the first operation for being ready to execute in the case of not expensive host intervention.
Instruction also makes equipment according to the operation for assigning execution to be indicated by the node in subgraph.When operation is assigned to specific stream
When, these operations are queued.Equipment can execute operation in a manner of first in first out (FIFO).Therefore, if there are one equipment only has
Stream, the then operation for being assigned to equipment are serially executed.If equipment has multiple streams, the not operation in cocurrent flow can be parallel
Ground is executed and is resequenced relative to each other, while the operation in given stream is serially executed.It is executed using multiple streams
Operation reduces the total time for the operation for executing subgraph.Further this is described below with reference to Fig. 3 and Fig. 4.
Instruction and data is supplied to equipment (step 210) by system.In some embodiments, system is opened to equipment transmission
The request of dynamic operation.Equipment, which receives, asks and in response, executes the instruction received from system.For example, equipment can connect
Mode input is received, and the mode input is handled according to the operation indicated by the node in subgraph.
The example child Figure 31 6 for the calculating figure that Fig. 3 diagrams are handled by accelerator 302.Subgraph 316 has node 308-314,
In the operation to be executed by accelerator 302 of each expression.Drawing system (for example, system 100 of Fig. 1) is calculated by subgraph 316
It is assigned to accelerator 302.
There are two streams 304 and 306 for the tool of accelerator 302.These streams share the utilization of accelerator 302.In GPU, stream can be with
It is symmetrical, it is meant that all operations can be executed to any stream.This symmetry may not be suitable for all accelerator facilities.Example
Such as, the behaviour of the replicate data between host and device memory must be executed using certain streams in particular-accelerator equipment
Make.
Subgraph 316 can be analyzed to determine how subgraph 316 is assigned to multiple streams 304 and 306 by calculating drawing system.At some
In embodiment, system assigns son in a manner of generating the number minimum for making accelerator 302 to make directed edge be connected to not cocurrent flow
The instruction of the node of Figure 31 6.Performance cost can be had by implementing dependence between streams.Ordering instruction has certain expense cost.
Each sequence dependence reduces the number that can be sorted by the possible execution of equipment utilization, to reduce dispatching flexibility.Whenever
When directed edge from first-class is connected to second, second wait for flowed to from first second directed edge operation with
Completion is handled.Waiting can make second keep idle, this makes GPU utilize with being deactivated rate.
In some embodiments, system generates the characteristic for making accelerator 302 based on accelerator 302 and assigns subgraph 316
The instruction of node.For example, accelerator 302 has the stream of fixed number, that is, flow 304 and 306.System can assign node, so often
Accelerated device 302 is similarly used a stream.For the accelerator as GPU, all shared single big thread pools of stream.
Some streams also execute other specific operations for flowing and not executing.For example, 306 executable direct memory access of stream
(DMA) it operates, however flows 304 and do not execute dma operation.Therefore, system can analyze each node to determine the behaviour indicated by node
The type of work, and the node can be assigned to the stream for being able to carry out the type operation by system.In GPU, main congestion resource
It is the DMA engine of the replicate data between host and device memory.DMA engine can be used by any stream.If a stream is just
Dma operation is being executed, then the stream cannot simultaneously execute calculating.Example system is thereby, it is ensured that at least one other stream has some meter
Calculating work will be performed simultaneously.System can analyze subgraph to identify and therefore generate the software module for making management assign operation or driving
Device assigns the instruction of node by following two general rules.First, system tries to refer to the node being arranged in chain structure
Task phase cocurrent flow.Node in chain structure is by following from a directed edge of node-to-node by node connected with each other.
Therefore, operation of the node in chain at the preceding node in having to wait for chain before calculating the operation of oneself is completed to calculate.Refer to
The chain of node is sent to be not always possible to, because branch and merging occur in figure, such as from shared input variable or public son
Expression formula.
Secondly, system, which may be selected to generate, makes accelerator 302 will be respectively since multiple nodes that a node receives input are assigned
To the instruction uniquely flowed.That is, if first node has multiple outputs to multiple and different nodes, system is by different sections
Each in point is assigned to unique stream.Each in different nodes is to any of other different nodes with data
Dependence, and therefore, efficiency is improved when being operated to non-intersecting stream.
As explanation, accelerator 302 receives subgraph 316.Make accelerator 302 by start node by the instruction that system receives
308 are assigned to first-class 306.The tool of start node 308 is there are two a directed edge of output-to node 310 and arrives node 314
A directed edge.Therefore, using Second Rule, instruction makes accelerator 302 that node 310 and 314 is assigned to not cocurrent flow.Node
The 312 also only output conduct inputs of receiving node 310.Therefore, using the first rule, system is as node 310 by node 312
It is assigned to phase cocurrent flow, that is, flows 304.
As described above, stream is the hardware queue for executing operation in order.Therefore, node is assigned to stream by accelerator 302
Order is important.Node is assigned to stream by accelerator 302 according to the order in the direction of the data flow in subgraph.That is,
Accelerator 302 identifies one or more start nodes of subgraph and assigns one or more of start nodes.Then, accelerate
Device 302 follows the directed edge as the output of one or more start nodes to identify subsequent node, and accelerator 302 by this
A little subsequent nodes are assigned to respective streams.Accelerator 302 continues the appointment of node until each node in subgraph is assigned.
As described above, as according to this order assign node as a result, will also be executed in given stream according to the order that be assigned of operation
Operation.When generating the input of operation A on different streams, it is necessary to ensure that be performed before them in operation A and all counted
It calculates.Execution on the stream that operation A is assigned to should be stopped, until all inputs of operation A have been calculated as stopping.Accurately
Stopping mechanism be that equipment is specific.For GPU equipment, it can be each one event of establishment in inlet flow and can will refer to
Order is added to each stream to signal the event.For each input, instruction can be also added to and assign the stream of A to allow
Operation waits for dependent event to execute.One or more in calculating the input for operating A on stream identical with operation A
In the case of a, Data flow dependency instruction can be safely deleted, so as to cause better performance.In given stream, by assigning
The operation indicated to the node of given stream will be calculated or be scheduling to execute by one or more of the other in accelerator 302
It is calculated when the operation that node indicates, the operation is generated by one or more of the other node expression by being assigned to given stream
It is operated as the output of input.
The explanation continued the above, because data flow to node 312 from node 310, stream 304 is assigned node 310
And then assign node 312.When executing operation in stream, the behaviour indicated by node 310 is first carried out in accelerator 302
Make, then executes the operation indicated by node 312.
After finish node (that is, node 312 and 314) executes operation, accelerator 302 is by the output of node or operates
The instruction of completion returns to system.In the example system, exist and back copy to result of calculation from the memory of accelerator 302
Special " transmission " node in mainframe memory, in the mainframe memory it can be received node give distinct device or
Client is returned in remote procedure call (RPC) response.When necessary, then output or instruction can be returned to client by system
End.
Another embodiment that node is assigned to stream will be further described below with reference to Fig. 4.
Fig. 4 is the flow chart of the instantiation procedure 400 for subgraph to be assigned to equipment.For convenience, process 400 will be retouched
It states to be executed by system (for example, GPU).For example, GPU can receive by calculating drawing system (for example, calculating drawing system 100 of Fig. 1)
The instruction of generation, described instruction upon being performed, make GPU implementation procedures 400.
System can based on by node or by prior assignments node consume amount of memory resources specific node is assigned to
Stream.For example, system can calculate the size with the tensor on each directed edge of each node from subgraph.The size of tensor
It indicates that the size for operating consumed memory will be executed by equipment.System may need all sizes for calculating tensor with determination
The size.Then the specific node of the tensor of memory with consumption particular size can be assigned to specific big with this by system
The equipment of small memory.
Particularly, when equipment execute operate when, software driver or actuator distribution memory come store it is any input with
And any output calculated as the result of operation.Because the amount of memory in equipment is limited, works as and do not use
When memory, equipment will discharge memory.
As explanation, system determines whether the operation indicated by node has terminated (step 402) in specific stream.For example,
System can be periodically polled whether stream has been terminated with the operation in the specific stream of determination.Stream can support that allowing host to determine executes
The list of the operation in stream progress action how far passed through.In some embodiments, event or label can signal
How far execution has been in progress.When the event occurs, which can be added to the special hardware operation queue in stream.Host can poll
This queue has occurred which is operated to determine.Other stream embodiments can determine the operation of all queuings only to allow host
When complete.Alternatively or additionally, hardware can provide interruption or readjustment when stream reaches some point.
After operation, system can determine that the memory of the input for operation can be released to be used in other operations
In.System does not discharge the memory of the output for operation, because output can be used in subsequent node.
Therefore, system is determined the amount of memory (step 404) for the consumption being released.System to software driver or can be held
Row device sends request to identify the size for the memory that will be released.
In some embodiments, example system allows to access (RDMA) network interface using Remote Direct Memory, far
The RDMA network interfaces can be used to put the memory that data are transferred directly to hardware accelerator at any time in journey machine
In.Any other operation that this memory must not be run on any stream uses.Example system may not be needed accurately
Know how far the operation on each stream has been in progress.However, system should track the known memory not used by any stream.This
Then free storage can be used for RDMA.
System for each unassigned node in unassigned node group come determine consumed by unassigned node it is corresponding
Estimate amount of memory (step 406).Unassigned node may include operating the node that completed node receives input from it.Do not refer to
It sends node may also comprise and is completed that however, there remains the nodes of the node handled by accelerator independently of its operation.Institute as above
It states, can determine estimation amount of memory to the size of unassigned node by assessing corresponding tensor.
System determines the first unassigned node for indicating operation from the group of unassigned node, and the operation is when by accelerator
When being executed on stream, the use for the amount of memory that will be released is made to maximize (step 408).If indicated by unassigned node
Operation is needed than executing the more memories of the amount of memory being released, then unassigned node will not be assigned to stream.
If the first operation and the second operation need to be less than or equal to by the corresponding estimation amount of memory for the amount of memory being released,
Then Systematic selection makes the use for the amount of memory that will be released maximumlly operate.In other words, in this case, system will
Indicate that the node of selected operation is determined as the first unassigned node.Example system does not make operation join the team on stream, until it
Can determine which region of accelerator memory by for keeping operation temporary working space and output until.It is dilute in memory
In the case of lacking, example system can select the operation for the amount of memory for keeping needs less to join the team or preferentially make to consume big
The operation of input tensor is joined the team, to allow them to be deallocated.
The operation indicated by the first unassigned node is assigned to specific stream (step 410) by system.Then system can make spy
Constant current executes operation, and system can continue to operate as above with reference to described in Fig. 2-3.
Theme described in this specification and the embodiment available digital electronic circuit of feature operation, with visibly specific real
Existing computer software or firmware, with computer hardware (including structure disclosed in this specification and its equivalent structures) or
It is realized with the combination of one or more of which.The embodiment of theme described in this specification can be used as one or more
A computer program on tangible non-transitory program carrier (that is, encode for being executed by data processing equipment or being controlled
One or more modules of the computer program instructions of the operation of data processing equipment) it is implemented.Alternatively or in addition, program
Instruction can be coded on manually generated transmitting signal, for example, electricity, optics or electromagnetic signal that machine generates, the signal
It is generated to encode information to be transferred to suitable acceptor device for being executed by data executive device.Computer
Storage medium can be machine readable storage device, machine readable storage substrate, random or serial access storage device or they
One or more of combination.However, computer storage media is not transmitting signal.
Term " data processing equipment " includes device, equipment and the machine of all kinds for handling data, as showing
Example includes programmable processor, computer or multiple processors or computer.The device may include dedicated logic circuit, for example,
FPGA (field programmable gate array) or ASIC (application-specific integrated circuit).The device may also include other than including hardware for institute
The code that computer program creates performing environment is stated, for example, constituting processor firmware, protocol stack, data base management system, operation
The code of the combination of system or one or more of which.
Computer program (its can also be referred to as or be described as program, software, software application, module, software module,
Script or code) it can be write with any type of programming language, including compiling or interpretative code or declaratively or procedural language, and
And it can be disposed in any form, including as stand-alone program or as module, component, subroutine or be suitable for calculating
The other units used in environment.Computer program can with but do not need to correspond to the file in file system.It can keep other
In a part for the file of program or data (for example, being stored in one or more of marking language document script), special
(for example, the one or more modules of storage, subprogram or code in the single file of described program or in multiple coordination files
Part file) in store program.Computer program can be deployed on a computer or positioned at a website
It is executed on place or the distribution of leap multiple websites and the multiple computers for passing through interconnection of telecommunication network.
As used in this description, " engine " or " software engine " refers to the software for providing the output different from input
The input/output of realization.Engine can be functional encoding block, such as library, platform, Software Development Kit
(" SDK ") or object.Each engine implementation can be fitted including one or more processors and any of computer-readable medium
On the computing device of type, for example, server, mobile phone, tablet computer, notebook computer, music player, electricity
Philosophical works reader, on knee or desktop computer, PDA, smart phone or other fixations or portable device.It additionally, can be with
By two or more realizations in these engines on identical computing device or on different computing devices.
Process and logic flow described in this specification can by one or more programmable calculators execute one or
Multiple computer programs by output is operated and generated to input data to execute function to be performed.Process and logic
Flow can also be executed by dedicated logic circuit, and device also can be used as dedicated logic circuit and be implemented, the special logic example
Such as it is FPGA (field programmable gate array) or ASIC (application-specific integrated circuit).
As an example, general purpose microprocessor or special microprocessor can be based on by being adapted for carrying out the computer of computer program
Or both or any other kind of central processing unit.Usually, central processing unit will be deposited from read-only memory or at random
Access to memory or both receives instruction and data.The necessary component of computer is performed for or carries out the central processing list of instruction
Member and for storing instruction with one or more storage devices of data.Usually, computer also will include or operate
Upper coupling from one or more mass memory units (for example, disk, magneto-optic disk or CD) for storing data to receive number
According to data are either transferred to the mass memory unit or are had both at the same time.However, computer need not have such equipment.This
Outside, computer can be embedded in another equipment, another equipment such as mobile phone, personal digital assistant (PDA), movement
Audio or video player, game machine, global positioning system (GPS) receiver or portable memory apparatus are (for example, general serial
Bus (USB) flash drive) etc..
It is suitable for storing computer program instructions and the computer-readable medium of data including the non-volatile of form of ownership
Memory, medium and memory devices include semiconductor memory devices as example, for example, EPROM, EEPROM and flash
Memory devices；Disk, for example, internal hard drive or removable disk；Magneto-optic disk；And CD-ROM and DVD-ROM disks.Processor and
Memory can be by supplemented, or is incorporated to dedicated logic circuit.
In order to provide the interaction with user, the embodiment of theme described in this specification, institute can be realized on computers
Stating computer has for the display equipment to user's display information, such as CRT (cathode-ray tube) monitor, LCD (liquid crystals
Show device) monitor or OLED display, and the input equipment for providing input to the computer, for example, keyboard, mouse or
There are sensitive display or other surfaces.The equipment of other types can also be used for providing the interaction with user；For example, being supplied to use
The feedback at family can be any type of sense feedback, such as visual feedback, audio feedback or touch feedback；And it can be any
Form receives input from the user, including sound, voice or sense of touch.In addition, computer can be by being used by user
Equipment sends resource and receives resource from the equipment used by user to be interacted with user；For example, by response to clear from web
Request that device of looking at receives and the web browser on the client device of user sends web page.
It can realize that the embodiment of theme described in this specification, the computing system include rear end group in computing systems
Part (for example, as data server), either including middleware component (for example, application server) or including front end assemblies
(for example, having, user can be used to the graphic user interface interacted with the embodiment of theme described in this specification or Web is clear
Look at the client computer of device), or any combinations including one or more such rear ends, middleware or front end assemblies.System
The component of system can be interconnected by any form or the digital data communications (for example, communication network) of medium.Communication network shows
Example includes LAN (" LAN ") and wide area network (" WAN "), for example, internet.
Computing system may include client and server.Client and server is generally remote from each other and usually passes through
Communication network interaction.Relationship between client and server by means of running and having client each other on corresponding computer
The computer program of end-relationship server and occur.
Although this specification includes many specific implementation mode details, these are not construed as to any invention
Or what range can be claimed be construed as limiting, but opposite be interpreted may be specific to the specific reality of specific invention
Apply the description of the feature of example.Also it can be realized in combination in the present specification in the field of individual embodiment in single embodiment
Certain features described in border.On the contrary, also can be in various embodiments individually or real according to any suitable sub-portfolio
Various features described in the context of present single embodiment.In addition, although these features can be described as above according to
Certain combinations are worked and therefore even initially claimed protection, however from the one or more features of claimed combination
It can be left out from combination in some cases, and combination claimed can be directed to the change of sub-portfolio or sub-portfolio
Change.
Similarly, although in the accompanying drawings according to certain order describe operate, this be not construed as require according to
Shown certain order or in sequence order execute this generic operation, or require the operation of all diagrams of execution desired to realize
Result.In some cases, multitasking and parallel processing can be advantageous.In addition, each in the above embodiments
The separation of kind system module and component is not construed as requiring this separation in all embodiments, and should be appreciated that
It is that described program assembly and system usually can be integrated together in single software product or be encapsulated into multiple softwares
In product.
The specific embodiment of theme is described.Other embodiments are in the range of following claims.Example
Such as, the action described in claim can be realized perform in different order and still desirable result.As an example,
The process described in attached drawing not necessarily requires shown certain order or sequential order, to realize desirable result.In certain realities
It applies in mode, multitasking and parallel processing can be advantageous.
Claims (41)
1. a method of computer implementation, the method includes：
Receive the request that processing calculates figure；
The data for indicating the subgraph for calculating figure are obtained, the calculating figure includes multiple nodes and directed edge, wherein each section
Point indicates corresponding operation, wherein corresponding first node is connected to corresponding second node by each directed edge, it is described corresponding
Second node indicate to receive the output operation as input of operation indicated by corresponding first node, the subgraph
It is assigned to the first equipment；
Determine that first equipment includes the hardware accelerator for having multiple streams；
Include that there is the hardware accelerator of multiple streams and generate instruction in response to determination first equipment, described instruction is when by institute
Stating when the first equipment executes makes first equipment：
The operation indicated by each node in the subgraph is assigned in the multiple stream of the hardware accelerator
Respective streams；And
The operation indicated by the node in the subgraph is executed according to the appointment；And
Described instruction and the data are supplied to first equipment.
2. according to the method described in claim 1, wherein, the specified identification of request is from one or more of described subgraph
One or more specific outputs of respective nodes, the method further includes：
One or more of specific outputs are received from first equipment；And
One or more of specific outputs are supplied to the client.
3. method according to claim 1 or 2, wherein described instruction further makes first equipment will be one
Or multiple specific outputs are stored in the memory of first equipment.
4. method according to claim 1,2 or 3, wherein the operation for the subgraph includes being directed to nerve net
The part reasoning or training of network calculate.
5. method according to any preceding claims, further comprises：
The subgraph is analyzed to identify the node group in the subgraph in chain structure；
Wherein, described instruction makes first equipment that the node group is assigned to a stream.
6. method according to any preceding claims, wherein the appointment includes：
The subgraph is analyzed to identify the first node in the subgraph with multiple directed edges as output；
Wherein, described instruction makes first equipment for each directed edge in the directed edge by the directed edge institute
The node of direction is assigned to the non-intersecting stream of the hardware accelerator.
7. method according to any preceding claims, wherein described instruction makes first equipment be directed to each node,
Based on determining the hardware accelerator consumed by the operation that is indicated by the node to the directed edge of the node
In corresponding amount of memory resources, wherein it is described appointment at least be based on corresponding amount of memory resources.
8. method according to any preceding claims, wherein described instruction makes first equipment determine by node table
The specific operation shown terminates at specific stream；
Terminate in response to the determination specific operation：
Determine the first memory amount consumed by the specific operation that will be released；
For each unassigned node in unassigned node group, the operation institute by being indicated by the unassigned node is determined
The corresponding estimation amount of memory of consumption；
Determine that the first unassigned node, the first unassigned node expression add in the hardware from the unassigned node group
The operation executed on the stream of fast device, the operation, which is utilized, makes the maximized estimation of the use of the first memory amount deposit
Reservoir amount；And
The operation indicated by the described first unassigned node is assigned to the specific stream.
9. method according to any preceding claims, wherein described instruction makes first equipment determine by node table
The specific operation shown terminates at specific stream：
Terminate in response to the determination specific operation：
Determine the output at least one subsequent operation as input using the specific operation；And
After at least one subsequent operation executed, reuses the output for the specific operation and distribute
Memory.
10. according to the method described in claim 9, wherein it is determined that the output using the specific operation is as input
At least one subsequent operation includes：
Determine at least two subsequent operations, it is first-class in the first operation and second in the second operation, using described specific
The output of operation is as input；
In first-class the first label of middle placement, when the first label instruction first operation has used the specific operation
As input；
The second label is placed in second, when the second label instruction second operation has used the specific operation
As input；
According to the instruction from first and second label, determine that two operations have used the specific operation.
11. method according to any preceding claims, further comprises：Receive mode input；And by the hardware
Accelerator handles the mode input according to the operation indicated by the node in the subgraph.
12. a kind of system, the system comprises：
One or more computers；And
Computer-readable medium, the computer-readable medium, which is coupled to one or more of computers and has, to be stored in
Instruction thereon, described instruction make one or more of computers execute when being executed by one or more of computers
Operation, the operation include：
The request that processing calculates figure is received by calculating drawing system；
The data for indicating the subgraph for calculating figure are obtained, the calculating figure includes multiple nodes and directed edge, wherein each section
Point indicates corresponding operation, wherein corresponding first node is connected to corresponding second node by each directed edge, it is described corresponding
Second node indicate to receive the output operation as input of operation indicated by corresponding first node, the subgraph
It is assigned to the first equipment；
Determine that first equipment includes the hardware accelerator for having multiple streams；
Include that there is the hardware accelerator of multiple streams and generate instruction in response to determination first equipment, described instruction is when by institute
Stating when the first equipment executes makes first equipment：
The operation indicated by each node in the subgraph is assigned in the multiple stream of the hardware accelerator
Respective streams；And
The operation indicated by the node in the subgraph is executed according to the appointment；And
Described instruction and the data are supplied to first equipment.
13. system according to claim 12, wherein the request is specified to identify one or more in the subgraph
One or more specific outputs of a respective nodes, the operation further comprise：
One or more of specific outputs are received from first equipment；And
One or more of specific outputs are supplied to the client.
14. system according to claim 12 or 13, the operation further comprises：
The subgraph is analyzed to identify the node group in the subgraph in chain structure；
Wherein, described instruction makes first equipment that the node group is assigned to a stream.
15. according to the system described in claim 12,13 or 14, wherein the appointment includes：
The subgraph is analyzed to identify the first node in the subgraph with multiple directed edges as output；
Wherein, described instruction makes first equipment for each directed edge in the directed edge by the directed edge institute
The node of direction is assigned to unique stream of the hardware accelerator.
16. the system according to any one of claim 12 to 15, wherein described instruction makes first equipment be directed to
Each node, based on to the directed edge of the node come determine consumed by the operation that is indicated by the node it is described hard
Corresponding amount of memory resources in part accelerator, wherein the appointment is at least based on corresponding amount of memory resources.
17. the system according to any one of claim 12 to 16, wherein described instruction makes first equipment determine
The specific operation indicated by node has terminated in specific stream；
Terminate in response to the determination specific operation：
Determine the first memory amount consumed by the specific operation that will be released；
For each unassigned node in unassigned node group, determine by the corresponding estimation of the unassigned node consumption
Amount of memory；
The first unassigned node is determined from the unassigned node group, the first unassigned node, which has, makes described first to deposit
Amount of memory is maximumlly estimated in the use of reservoir amount；
And
The operation indicated by the described first unassigned node is assigned to the specific stream.
18. a kind of computer program product being coded on one or more computer storage medias, the computer program
Product includes instruction, and described instruction makes one or more of computers execute behaviour when being executed by one or more computers
Make, the operation includes：
The request that processing calculates figure is received by calculating drawing system；
The data for indicating the subgraph for calculating figure are obtained, the calculating figure includes multiple nodes and directed edge, wherein each section
Point indicates corresponding operation, wherein corresponding first node is connected to corresponding second node by each directed edge, it is described corresponding
Second node indicate to receive the output operation as input of operation indicated by corresponding first node, the subgraph
It is assigned to the first equipment；
Determine that first equipment includes the hardware accelerator for having multiple streams；
Include that there is the hardware accelerator of multiple streams and generate instruction in response to determination first equipment, described instruction is when by institute
Stating when the first equipment executes makes first equipment：
The operation indicated by each node in the subgraph is assigned in the multiple stream of the hardware accelerator
Respective streams；And
The operation indicated by the node in the subgraph is executed according to the appointment；And
Described instruction and the data are supplied to first equipment.
19. computer program product according to claim 18, wherein the specified identification of request is in the subgraph
One or more respective nodes one or more specific outputs, the operation further comprises：
One or more of specific outputs are received from first equipment；And
One or more of specific outputs are supplied to the client.
20. the computer program product according to claim 18 or 19, the operation further comprises：
The subgraph is analyzed to identify the node group in the subgraph in chain structure；
Wherein, described instruction makes first equipment that the node group is assigned to a stream.
21. according to the computer program product described in claim 18,19 or 20, wherein the appointment includes：
The subgraph is analyzed to identify the first node in the subgraph with multiple directed edges as output；
Wherein, described instruction makes first equipment for each directed edge in the directed edge by the directed edge institute
The node of direction is assigned to unique stream of the hardware accelerator.
22. the computer program product according to any one of claim 18 to 21, wherein described instruction makes described
One equipment is directed to each node, is determined based on the directed edge to the node and is consumed by the operation indicated by the node
The hardware accelerator in corresponding amount of memory resources, wherein it is described appointment at least be based on corresponding memory
Stock number.
23. the computer program product according to any one of claim 18 to 22, wherein described instruction makes described
One equipment determines that the specific operation indicated by node terminates in specific stream；
Terminate in response to the determination specific operation：
Determine the first memory amount consumed by the specific operation that will be released；
For each unassigned node in unassigned node group, determine by the corresponding estimation of the unassigned node consumption
Amount of memory；
The first unassigned node is determined from the unassigned node group, the first unassigned node, which has, makes described first to deposit
Amount of memory is maximumlly estimated in the use of reservoir amount；And
The operation indicated by the described first unassigned node is assigned to the specific stream.
24. a kind of method, the method includes：
The data for the subgraph for indicating to calculate figure are received by the hardware accelerator with multiple streams, the calculating figure includes multiple nodes
And directed edge, wherein each node indicates corresponding operation, wherein corresponding first node is connected to accordingly by each directed edge
Second node, corresponding second node indicates to receive the output conduct of the operation indicated by corresponding first node
The operation of input, the subgraph are assigned to hardware accelerator by the distributor in calculating drawing system；
It will be assigned to by the operation of each node expression in the subgraph by the hardware accelerator described hardware-accelerated
Respective streams in the multiple stream of device；And
The operation indicated by the node in the subgraph is executed according to the appointment by the hardware accelerator.
25. according to the method for claim 24, further comprising：
Receive the request of one or more specific outputs of the identification from one or more of subgraph respective nodes；And
One or more of specific outputs are supplied to the client.
26. the method according to claim 24 or 25, further comprises：
Receive the data that the node group in the subgraph is identified in chain structure；And
The node group is assigned to a stream.
27. according to the method described in claim 24,25 or 26, wherein the appointment includes：
Receive data of the identification with multiple directed edges as the first node of output in the subgraph；And
For each directed edge in the directed edge, the node pointed by the directed edge is assigned to described hardware-accelerated
Unique stream of device.
28. the method according to any one of claim 24 to 27, further comprises：For each node, based on arriving
The directed edge of the node determines corresponding in the hardware accelerator consumed by the operation indicated by the node
Amount of memory resources, wherein it is described appointment at least be based on corresponding amount of memory resources.
29. the method according to any one of claim 24 to 28, further comprises：It determines and indicates specific by node
Operation terminates at specific stream；
Terminate in response to the determination specific operation, has determined the first storage consumed by the specific operation that will be released
Tolerance；
For each unassigned node in unassigned node group, determine by the corresponding estimation of the unassigned node consumption
Amount of memory；
The first unassigned node is determined from the unassigned node group, the first unassigned node, which has, makes described first to deposit
Amount of memory is maximumlly estimated in the use of reservoir amount；And
The operation indicated by the described first unassigned node is assigned to the specific stream.
30. a kind of computer program product being coded on one or more non-transitory computer storage mediums, the meter
Calculation machine program product includes instruction, and described instruction makes when by including that the computer of the hardware accelerator with multiple streams executes
The computer executes operation, and the operation includes：
The data for the subgraph for indicating to calculate figure are received, the calculating figure includes multiple nodes and directed edge, wherein each node table
Show corresponding operation, wherein corresponding first node is connected to corresponding second node by each directed edge, corresponding
Two nodes indicate the output operation as input for the operation that reception is indicated by corresponding first node, and the subgraph is by counting
Distributor in Nomograph system is assigned to hardware accelerator；
The operation indicated by each node in the subgraph is assigned in the multiple stream of the hardware accelerator
Respective streams；And
The operation indicated by the node in the subgraph is executed according to the appointment.
31. computer program product according to claim 30, further comprises：
Receive the request of one or more specific outputs of the identification from one or more of subgraph respective nodes；And
One or more of specific outputs are supplied to the client.
32. the computer program product according to claim 30 or 31, further comprises：
Receive the data that the node group in the subgraph is identified in chain structure；And
The node group is assigned to a stream.
33. according to the computer program product described in claim 30,31 or 32, wherein the appointment includes：
Receive data of the identification with multiple directed edges as the first node of output in the subgraph；And
For each directed edge in the directed edge, the node pointed by the directed edge is assigned to described hardware-accelerated
Unique stream of device.
34. the computer program product according to any one of claim 30 to 33, further comprises：For each section
Point, based on the directed edge to the node, determination is consumed described hardware-accelerated by the operation indicated by the node
Corresponding amount of memory resources in device, wherein the appointment is at least based on corresponding amount of memory resources.
35. the computer program product according to any one of claim 30 to 34, further comprises：It determines by node
The specific operation of expression terminates at specific stream；
Terminate in response to the determination specific operation, has determined the first storage consumed by the specific operation that will be released
Tolerance；
For each unassigned node in unassigned node group, determine by the corresponding estimation of the unassigned node consumption
Amount of memory；
The first unassigned node is determined from the unassigned node group, the first unassigned node, which has, makes described first to deposit
Amount of memory is maximumlly estimated in the use of reservoir amount；And
The operation indicated by the described first unassigned node is assigned to the specific stream.
36. a kind of system including hardware accelerator, wherein the hardware accelerator includes multiple streams, and the wherein described system
System is configured to execute operation, and the operation includes：
The data for the subgraph for indicating to calculate figure are received, the calculating figure includes multiple nodes and directed edge, wherein each node table
Show corresponding operation, wherein corresponding first node is connected to corresponding second node by each directed edge, corresponding
Two nodes indicate the output operation as input for the operation that reception is indicated by corresponding first node, and the subgraph is by counting
Distributor in Nomograph system is assigned to hardware accelerator；
The operation indicated by each node in the subgraph is assigned in the multiple stream of the hardware accelerator
Respective streams；And
The operation indicated by the node in the subgraph is executed according to the appointment.
37. system according to claim 36, further comprises：
Receive the request of one or more specific outputs of the identification from one or more of subgraph respective nodes；And
One or more of specific outputs are supplied to the client.
38. the system according to claim 36 or 37, further comprises：
Receive the data that the node group in the subgraph is identified in chain structure；And
The node group is assigned to a stream.
39. according to the system described in claim 36,37 or 38, wherein the appointment includes：
Receive data of the identification with multiple directed edges as the first node of output in the subgraph；And
For each directed edge in the directed edge, the node pointed by the directed edge is assigned to described hardware-accelerated
Unique stream of device.
40. the system according to any one of claim 36 to 39, further comprises：For each node, based on arriving
The directed edge of the node determines corresponding in the hardware accelerator consumed by the operation indicated by the node
Amount of memory resources, wherein it is described appointment at least be based on corresponding amount of memory resources.
41. according to the system described in claim 36 to 40, further comprise：Determine the specific operation indicated by node in spy
Terminate at constant current；
Terminate in response to the determination specific operation, has determined the first storage consumed by the specific operation that will be released
Tolerance；
For each unassigned node in unassigned node group, determine by the corresponding estimation of the unassigned node consumption
Amount of memory；
The first unassigned node is determined from the unassigned node group, the first unassigned node, which has, makes described first to deposit
Amount of memory is maximumlly estimated in the use of reservoir amount；And
The operation indicated by the described first unassigned node is assigned to the specific stream.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202211570829.6A CN115840643A (en) | 2015-10-28 | 2016-10-28 | Flow-based accelerator processing of computational graphs |
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562247703P | 2015-10-28 | 2015-10-28 | |
US62/247,703 | 2015-10-28 | ||
US201562253046P | 2015-11-09 | 2015-11-09 | |
US62/253,046 | 2015-11-09 | ||
PCT/US2016/059334 WO2017075360A1 (en) | 2015-10-28 | 2016-10-28 | Stream-based accelerator processing of computational graphs |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202211570829.6A Division CN115840643A (en) | 2015-10-28 | 2016-10-28 | Flow-based accelerator processing of computational graphs |
Publications (2)
Publication Number | Publication Date |
---|---|
CN108351805A true CN108351805A (en) | 2018-07-31 |
CN108351805B CN108351805B (en) | 2022-12-23 |
Family
ID=57354431
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680063365.6A Active CN108351805B (en) | 2015-10-28 | 2016-10-28 | Flow-based accelerator processing of computational graphs |
CN202211570829.6A Pending CN115840643A (en) | 2015-10-28 | 2016-10-28 | Flow-based accelerator processing of computational graphs |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202211570829.6A Pending CN115840643A (en) | 2015-10-28 | 2016-10-28 | Flow-based accelerator processing of computational graphs |
Country Status (6)
Country | Link |
---|---|
US (3) | US11151446B2 (en) |
EP (1) | EP3353655B1 (en) |
JP (1) | JP6672456B2 (en) |
KR (2) | KR102081952B1 (en) |
CN (2) | CN108351805B (en) |
WO (1) | WO2017075360A1 (en) |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109669772A (en) * | 2018-12-28 | 2019-04-23 | 第四范式（北京）技术有限公司 | Calculate the parallel execution method and apparatus of figure |
CN110210614A (en) * | 2019-05-31 | 2019-09-06 | 北京中科寒武纪科技有限公司 | Operation method, device and Related product |
CN111611087A (en) * | 2020-06-30 | 2020-09-01 | 中国人民解放军国防科技大学 | Resource scheduling method, device and system |
CN111915002A (en) * | 2019-05-09 | 2020-11-10 | 中科寒武纪科技股份有限公司 | Operation method, device and related product |
CN112016681A (en) * | 2019-05-31 | 2020-12-01 | 苹果公司 | Decomposition of machine learning operations |
WO2021012609A1 (en) * | 2019-07-24 | 2021-01-28 | 华为技术有限公司 | Neural network segmentation method, prediction method, and related apparatus |
CN113767364A (en) * | 2019-05-03 | 2021-12-07 | 谷歌有限责任公司 | Reshaping and broadcast optimization to avoid unnecessary data movement |
US11836635B2 (en) | 2019-05-31 | 2023-12-05 | Apple Inc. | Mutable parameters for machine learning models during runtime |
Families Citing this family (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180032568A1 (en) * | 2016-07-29 | 2018-02-01 | Sysomos L.P. | Computing System with Multi-Processor Platform for Accelerating Influence Maximization Computation and Related Methods |
US10241956B2 (en) * | 2016-09-12 | 2019-03-26 | International Business Machines Corporation | Virtualizing coherent hardware accelerators |
WO2018175972A1 (en) | 2017-03-24 | 2018-09-27 | Google Llc | Device placement optimization with reinforcement learning |
US11138494B2 (en) * | 2017-05-02 | 2021-10-05 | International Business Machines Corporation | Storage controller acceleration for neural network training and inference |
US10585703B2 (en) * | 2017-06-03 | 2020-03-10 | Apple Inc. | Dynamic operation allocation for neural networks |
US11138516B2 (en) * | 2017-06-30 | 2021-10-05 | Visa International Service Association | GPU enhanced graph model build and scoring engine |
CN107480080B (en) * | 2017-07-03 | 2021-03-23 | 深圳致星科技有限公司 | Zero-copy data stream based on RDMA |
US10599482B2 (en) * | 2017-08-24 | 2020-03-24 | Google Llc | Method for intra-subgraph optimization in tuple graph programs |
US10642582B2 (en) | 2017-08-24 | 2020-05-05 | Google Llc | System of type inference for tuple graph programs method of executing a tuple graph program across a network |
US10887235B2 (en) | 2017-08-24 | 2021-01-05 | Google Llc | Method of executing a tuple graph program across a network |
US11568218B2 (en) * | 2017-10-17 | 2023-01-31 | Xilinx, Inc. | Neural network processing system having host controlled kernel acclerators |
US11373088B2 (en) * | 2017-12-30 | 2022-06-28 | Intel Corporation | Machine learning accelerator mechanism |
FI130232B (en) * | 2018-04-18 | 2023-05-03 | Meeshkan Oy | Method for distributed information processing and distributed information processing system |
US11948073B2 (en) * | 2018-04-20 | 2024-04-02 | Advanced Micro Devices, Inc. | Machine learning inference engine scalability |
US20190333506A1 (en) | 2018-04-30 | 2019-10-31 | Avnera Corporation | Personal interactive speaker device having audio recognition neural net processor architecture |
US11615289B2 (en) * | 2018-06-28 | 2023-03-28 | Oracle International Corporation | Configuration price quote with enhanced approval control |
CN111079916B (en) * | 2018-10-19 | 2021-01-15 | 安徽寒武纪信息科技有限公司 | Operation method, system and related product |
CN111079907B (en) * | 2018-10-19 | 2021-01-26 | 安徽寒武纪信息科技有限公司 | Operation method, device and related product |
US11769041B2 (en) | 2018-10-31 | 2023-09-26 | Advanced Micro Devices, Inc. | Low latency long short-term memory inference with sequence interleaving |
CN111353575A (en) | 2018-12-20 | 2020-06-30 | 超威半导体公司 | Tiled format for convolutional neural networks |
US11645358B2 (en) * | 2019-01-29 | 2023-05-09 | Hewlett Packard Enterprise Development Lp | Generation of executable files corresponding to neural network models |
US11687795B2 (en) * | 2019-02-19 | 2023-06-27 | International Business Machines Corporation | Machine learning engineering through hybrid knowledge representation |
US11521042B2 (en) * | 2019-05-21 | 2022-12-06 | Anil Ravindranath | System and method to dynamically and automatically sharing resources of coprocessor AI accelerators |
US11494237B2 (en) | 2019-06-26 | 2022-11-08 | Microsoft Technology Licensing, Llc | Managing workloads of a deep neural network processor |
US10884755B1 (en) * | 2019-07-31 | 2021-01-05 | International Business Machines Corporation | Graph rewriting for large model support using categorized topological sort |
US11521062B2 (en) * | 2019-12-05 | 2022-12-06 | International Business Machines Corporation | Neural network training using a data flow graph and dynamic memory management |
WO2021183135A1 (en) * | 2020-03-13 | 2021-09-16 | Hewlett-Packard Development Company, L.P. | Transmitting node instructions |
US20220051085A1 (en) * | 2020-08-11 | 2022-02-17 | Mediatek Inc. | Runtime hyper-heterogeneous optimization for processing circuits executing inference model |
CN114169491A (en) * | 2020-09-10 | 2022-03-11 | 阿里巴巴集团控股有限公司 | Model processing method, device, equipment and computer readable storage medium |
CN114565102A (en) * | 2020-11-27 | 2022-05-31 | 伊姆西Ip控股有限责任公司 | Method, electronic device and computer program product for deploying machine learning model |
CN112734011B (en) * | 2021-01-04 | 2021-12-28 | 北京大学 | Deep neural network accelerator collaborative design method based on incremental synthesis |
CN115934306A (en) * | 2021-08-08 | 2023-04-07 | 联发科技股份有限公司 | Electronic equipment, method for generating output data and machine-readable storage medium |
CN114004347A (en) | 2021-08-30 | 2022-02-01 | 平头哥(上海)半导体技术有限公司 | Hardware accelerator, system and method for accelerating graph neural network attribute access |
US20240104341A1 (en) * | 2022-09-27 | 2024-03-28 | Zhejiang Lab | Memory optimization method and apparatus for neural network compilation |
CN115759233B (en) * | 2022-11-24 | 2023-10-20 | 北京百度网讯科技有限公司 | Model training method, graph data processing device and electronic equipment |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060095722A1 (en) * | 2004-10-20 | 2006-05-04 | Arm Limited | Program subgraph identification |
CN101443733A (en) * | 2006-05-16 | 2009-05-27 | 起元软件有限公司 | Managing computing resources in graph-based computations |
CN101501678A (en) * | 2006-08-10 | 2009-08-05 | 起元软件有限公司 | Distributing services in graph-based computations |
CN102089752A (en) * | 2008-07-10 | 2011-06-08 | 洛克泰克科技有限公司 | Efficient parallel computation of dependency problems |
US20150007182A1 (en) * | 2013-06-27 | 2015-01-01 | Microsoft Corporation | Iteration support in a heterogeneous dataflow engine |
CN104615488A (en) * | 2015-01-16 | 2015-05-13 | 华为技术有限公司 | Task scheduling method and device on heterogeneous multi-core reconfigurable computing platform |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5317734A (en) | 1989-08-29 | 1994-05-31 | North American Philips Corporation | Method of synchronizing parallel processors employing channels and compiling method minimizing cross-processor data dependencies |
US7350055B2 (en) | 2004-10-20 | 2008-03-25 | Arm Limited | Tightly coupled accelerator |
US8881141B2 (en) * | 2010-12-08 | 2014-11-04 | Intenational Business Machines Corporation | Virtualization of hardware queues in self-virtualizing input/output devices |
WO2014009031A2 (en) | 2012-07-09 | 2014-01-16 | Toyota Motor Europe Nv/Sa | Artificial memory system and method for use with a computational machine for interacting with dynamic behaviours |
CN102609298B (en) * | 2012-01-11 | 2016-01-13 | 中国科学技术大学苏州研究院 | Based on network interface card virtualization system and the method thereof of hardware queue expansion |
CN102790777B (en) * | 2012-08-07 | 2016-06-15 | 华为技术有限公司 | Network interface adapter register method and driving equipment, server |
CN103970604B (en) | 2013-01-31 | 2017-05-03 | 国际商业机器公司 | Method and device for realizing image processing based on MapReduce framework |
US11061539B2 (en) | 2013-03-15 | 2021-07-13 | The Mathworks, Inc. | Reference nodes in a computational graph |
US9645575B2 (en) | 2013-11-27 | 2017-05-09 | Adept Ai Systems Inc. | Method and apparatus for artificially intelligent model-based control of dynamic processes using probabilistic agents |
-
2016
- 2016-10-27 US US15/336,673 patent/US11151446B2/en active Active
- 2016-10-28 WO PCT/US2016/059334 patent/WO2017075360A1/en active Application Filing
- 2016-10-28 CN CN201680063365.6A patent/CN108351805B/en active Active
- 2016-10-28 KR KR1020187015068A patent/KR102081952B1/en active IP Right Grant
- 2016-10-28 EP EP16798590.2A patent/EP3353655B1/en active Active
- 2016-10-28 JP JP2018522024A patent/JP6672456B2/en active Active
- 2016-10-28 CN CN202211570829.6A patent/CN115840643A/en active Pending
- 2016-10-28 KR KR1020207004981A patent/KR20200021104A/en active Application Filing
-
2018
- 2018-04-27 US US15/965,670 patent/US10373053B2/en active Active
-
2021
- 2021-10-12 US US17/499,330 patent/US20220027202A1/en active Pending
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060095722A1 (en) * | 2004-10-20 | 2006-05-04 | Arm Limited | Program subgraph identification |
CN101443733A (en) * | 2006-05-16 | 2009-05-27 | 起元软件有限公司 | Managing computing resources in graph-based computations |
CN101501678A (en) * | 2006-08-10 | 2009-08-05 | 起元软件有限公司 | Distributing services in graph-based computations |
CN102089752A (en) * | 2008-07-10 | 2011-06-08 | 洛克泰克科技有限公司 | Efficient parallel computation of dependency problems |
US20150007182A1 (en) * | 2013-06-27 | 2015-01-01 | Microsoft Corporation | Iteration support in a heterogeneous dataflow engine |
CN104615488A (en) * | 2015-01-16 | 2015-05-13 | 华为技术有限公司 | Task scheduling method and device on heterogeneous multi-core reconfigurable computing platform |
Cited By (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111522640A (en) * | 2018-12-28 | 2020-08-11 | 第四范式（北京）技术有限公司 | Parallel execution method and equipment of computational graph |
CN109669772A (en) * | 2018-12-28 | 2019-04-23 | 第四范式（北京）技术有限公司 | Calculate the parallel execution method and apparatus of figure |
CN113767364A (en) * | 2019-05-03 | 2021-12-07 | 谷歌有限责任公司 | Reshaping and broadcast optimization to avoid unnecessary data movement |
CN111915002A (en) * | 2019-05-09 | 2020-11-10 | 中科寒武纪科技股份有限公司 | Operation method, device and related product |
CN111915002B (en) * | 2019-05-09 | 2023-12-19 | 中科寒武纪科技股份有限公司 | Operation method, device and related product |
CN110210614B (en) * | 2019-05-31 | 2020-08-25 | 中科寒武纪科技股份有限公司 | Operation method, device and related product |
CN112016681A (en) * | 2019-05-31 | 2020-12-01 | 苹果公司 | Decomposition of machine learning operations |
US11836635B2 (en) | 2019-05-31 | 2023-12-05 | Apple Inc. | Mutable parameters for machine learning models during runtime |
CN110210614A (en) * | 2019-05-31 | 2019-09-06 | 北京中科寒武纪科技有限公司 | Operation method, device and Related product |
CN112016681B (en) * | 2019-05-31 | 2024-04-30 | 苹果公司 | Decomposition of machine learning operations |
WO2021012609A1 (en) * | 2019-07-24 | 2021-01-28 | 华为技术有限公司 | Neural network segmentation method, prediction method, and related apparatus |
CN112543918A (en) * | 2019-07-24 | 2021-03-23 | 华为技术有限公司 | Neural network segmentation method, prediction method and related device |
CN111611087A (en) * | 2020-06-30 | 2020-09-01 | 中国人民解放军国防科技大学 | Resource scheduling method, device and system |
Also Published As
Publication number | Publication date |
---|---|
KR20180073669A (en) | 2018-07-02 |
US20180247196A1 (en) | 2018-08-30 |
US11151446B2 (en) | 2021-10-19 |
EP3353655B1 (en) | 2023-01-11 |
CN115840643A (en) | 2023-03-24 |
KR102081952B1 (en) | 2020-04-23 |
JP6672456B2 (en) | 2020-03-25 |
EP3353655A1 (en) | 2018-08-01 |
KR20200021104A (en) | 2020-02-27 |
WO2017075360A1 (en) | 2017-05-04 |
JP2018533795A (en) | 2018-11-15 |
US20170124451A1 (en) | 2017-05-04 |
US10373053B2 (en) | 2019-08-06 |
US20220027202A1 (en) | 2022-01-27 |
CN108351805B (en) | 2022-12-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108351805A (en) | Calculate the accelerator processing based on stream of figure | |
US11769061B2 (en) | Processing computational graphs | |
US20210295161A1 (en) | Training neural networks represented as computational graphs | |
JP7094262B2 (en) | Correction of calculation graph | |
US10789544B2 (en) | Batching inputs to a machine learning model | |
US11763146B1 (en) | Processing loops in computational graphs | |
CN108460458A (en) | It is executed in graphics processing unit and calculates figure | |
Genez et al. | Time-discretization for speeding-up scheduling of deadline-constrained workflows in clouds | |
Papazachos et al. | Scheduling of frequently communicating tasks | |
He et al. | Dynamic scalable stochastic petri net: A novel model for designing and analysis of resource scheduling in cloud computing | |
US20240160948A1 (en) | Processing computational graphs | |
Cao et al. | Online Learning-Based Co-task Dispatching with Function Configuration in Edge Computing |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |