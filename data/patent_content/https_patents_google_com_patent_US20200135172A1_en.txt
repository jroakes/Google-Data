US20200135172A1 - Sample-efficient adaptive text-to-speech - Google Patents
Sample-efficient adaptive text-to-speech Download PDFInfo
- Publication number
- US20200135172A1 US20200135172A1 US16/666,043 US201916666043A US2020135172A1 US 20200135172 A1 US20200135172 A1 US 20200135172A1 US 201916666043 A US201916666043 A US 201916666043A US 2020135172 A1 US2020135172 A1 US 2020135172A1
- Authority
- US
- United States
- Prior art keywords
- audio
- new
- speaker
- generation model
- text
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000003044 adaptive effect Effects 0.000 title claims abstract description 36
- 238000000034 method Methods 0.000 claims abstract description 59
- 239000013598 vector Substances 0.000 claims abstract description 48
- 238000012549 training Methods 0.000 claims abstract description 42
- 230000006978 adaptation Effects 0.000 claims abstract description 35
- 238000013528 artificial neural network Methods 0.000 claims abstract description 17
- 238000004590 computer program Methods 0.000 claims abstract description 14
- 238000003860 storage Methods 0.000 claims abstract description 10
- 238000013519 translation Methods 0.000 claims description 9
- 238000012795 verification Methods 0.000 claims description 2
- 230000008569 process Effects 0.000 description 34
- 238000012545 processing Methods 0.000 description 14
- 238000010586 diagram Methods 0.000 description 8
- 238000004891 communication Methods 0.000 description 5
- 230000003750 conditioning effect Effects 0.000 description 5
- 238000010801 machine learning Methods 0.000 description 5
- 230000014616 translation Effects 0.000 description 5
- 230000008901 benefit Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000013459 approach Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000001537 neural effect Effects 0.000 description 3
- 230000015572 biosynthetic process Effects 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000003786 synthesis reaction Methods 0.000 description 2
- 241000009334 Singa Species 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000003672 processing method Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/04—Details of speech synthesis systems, e.g. synthesiser structure or memory management
- G10L13/047—Architecture of speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/033—Voice editing, e.g. manipulating the voice of the synthesiser
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
-
- G10L13/043—
Definitions
- This specification relates to signal-generation neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input.
- Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer.
- Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- One example of a neural network is an audio generation neural network.
- Audio generation neural networks take text as input and generate as output a raw audio waveform of a speaker speaking the text. Realistic audio generation typically requires multiple thousands of samples to be generated per second, e.g., 24,000 samples per second.
- An audio generation neural network is a WaveNet. WaveNets were initially described in van den Oord et al., WaveNet: A Generative Model for Raw Audio , in arXiv preprint arXiv:1609.03499 (2016), available at arxiv.org.
- a WaveNet is a deep neural network that models the conditional probability of an audio sample having a particular value given a particular number of previously occurring sample values.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that can generate a sample-efficient, adaptive audio-generation model.
- being sample-efficient and adaptive means that the model can be customized to emulate the speech of a new speaker with far less training data than was used to train the adaptive model. For example, while training the adaptive model may require hours of audio recordings for each individual speaker, adapting the model for a new speaker may require only a few minutes of audio recordings of the new speaker.
- a training system can train the audio-generation model using a plurality of embedding vectors for respective individual speakers and an audio-generation neural network. Because of the computationally intensive nature of the training process, the training can be performed by a distributed computing system, e.g., a datacenter, having hundreds or thousands of computers.
- the output of the training process is an adaptive audio-generation model that can be efficiently adapted to a new speaker.
- Adapting the model generally involves learning a new embedding vector for the new speaker, and may optionally involve fine-tuning the parameters of the neural network for the new speaker.
- the adaptation data can be only a few seconds or a few minutes of audio recordings of the new speaker.
- the adaptation process is therefore much less computationally intensive than the original training process.
- the adaptation process can be performed on much less powerful hardware, e.g., a mobile phone or another wearable device, a desktop or laptop computer, or another internet-enabled device installed in a user's home, to name just a few examples.
- Adaptive audio-generation model can be used to rapidly adapt to a new speaker using orders of magnitude less data than was used to train the model. This enables the adaptation process to be performed by consumer hardware of end users rather than being performed in a datacenter.
- the adaptation technology enables a variety of technological use cases that were previously not possible. For example, realistic voice translations of video or audio content can be generated that emulate the characteristics of a particular speaker without requiring hours of audio recordings of the speaker. As another example, real-time translated video conferencing or phone calls can be generated in which the translation realistically emulates the speaker who does not actually speak the translated language.
- an adaptive audio generation model can be used to provide realistic speech capabilities for medical patients suffering voice-impairing medical conditions without requiring such patients to have hours of audio recordings of themselves speaking.
- FIG. 1A is a diagram that illustrates an example architecture for training a sample-efficient, adaptive audio-generation model.
- FIG. 1B is a diagram that illustrates an example architecture for adapting a sample-efficient, adaptive audio-generation model to a new individual speaker.
- FIG. 1C is a diagram that illustrates an example architecture 100 c for performing inference using an adapted sample-efficient, adaptive audio-generation model.
- FIG. 2 is a flowchart of an example process for generating and using a sample-efficient, adaptive audio-generation model.
- FIG. 3 is a diagram that illustrates an example architecture 300 for an embedding encoder for predicting speaker embeddings.
- FIGS. 1A, 1B, and 1C respectively illustrate example architectures for training, adapting, and performing inference using a sample-efficient, adaptive audio-generation model.
- the examples in this specification generally discuss using a WaveNet architecture to implement the audio-generation model.
- the same techniques can also be applied using any other appropriate neural audio-generation model, e.g., the WaveRNN model described in Kalchbrenner et al., Efficient Neural Audio Synthesis , in arXiv preprint arXiv:1802.08435 (2016), available at arxiv.org, and the Tacotron 2 model described in Shen et al., Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions , in arXiv preprint arXiv:1712.05884 (2017), available at arxiv.org, to name just a few examples.
- FIG. 1A is a diagram that illustrates an example architecture 100 a for training a sample-efficient, adaptive audio-generation model.
- the architecture 100 a includes a WaveNet Stack 110 a that is trained using an embedding table 120 that stores embedding vectors for multiple different respective individual speakers, each individual speaker having a speaker index 105 .
- the components illustrated in FIG. 1A can be implemented by a distributed computing system comprising a plurality of computers that coordinate to train the WaveNet Stack 110 a . As described above, the training process can use many different individual speakers using hours of audio waveform data.
- x t is the t-th timestep sample
- h and w are respectively the conditioning inputs and parameters of the model.
- the conditioning inputs h consist of the speaker identity s, the linguistic features l, and the logarithmic fundamental frequency f0 values.
- the variable l encodes the sequence of phonemes derived from the input text, and f0 controls the dynamics of the pitch in the generated utterance.
- the model Given the speaker index s for each utterance in the dataset, the model can be expressed as:
- the linguistic features l 145 and the fundamental frequency values f 0 135 can each be a respective time-series with a lower sampling frequency than the waveform 125 .
- the linguistic features 145 and fundamental frequency values 135 can be upsampled by a transposed convolutional network.
- l 145 and f 0 135 can be extracted by signal processing methods from pairs of training utterance and transcript. During testing, those values can be predicted from text by existing models.
- FIG. 1B is a diagram that illustrates an example architecture 100 b for adapting a sample-efficient, adaptive audio-generation model to a new individual speaker.
- the parameters learned during training are adjusted so that they are adapted to a particular individual's voice characteristics.
- the purpose of the training process illustrated in FIG. 1A is to learn a prior. During adaptation, this prior is combined with new data to rapidly adapt to a new speakers' voice characteristics.
- the architecture 100 b includes a WaveNet Stack 110 b whose parameters have been adjusted according to a new embedding vector 122 that represents the characteristics of a new speaker.
- the new embedding vector 122 can be generated in a variety of ways, which are discussed in more detail below with reference to FIG. 2 .
- FIG. 1C is a diagram that illustrates an example architecture 100 c for performing inference using an adapted sample-efficient, adaptive audio-generation model.
- new text 155 and the embedding vector 122 generated for the new speaker are used as input to generate an output waveform 165 having characteristics of the new speaker.
- a predictor 150 that uses existing models to generate linguistic features and fundamental frequencies of the new speaker 150 from the new text 155 .
- FIG. 2 is a flowchart of an example process for generating and using a sample-efficient, adaptive audio-generation model.
- the process includes three stages: training, adaptation, and inference.
- the training stage is performed on a distributed computing system having multiple computers.
- the other two stages can be performed on much less computationally expensive hardware, e.g., a desktop computer, laptop computer, or mobile computing device.
- the example process will be described as being performed by a system of one or more computers.
- the system generates an adaptive audio-generation model using data representing audio spoken by a plurality of different individual speakers ( 210 ).
- the system can generate different embedding vectors for a plurality of individual speakers.
- the system can then train parameter values of a neural audio-generation model, e.g., a WaveNet stack, a WaveRNN model, or a Tacotron 2 model, using training data that includes text and audio data representing a plurality of different individual speakers speaking portions of the text.
- Each of the embedding vectors generally represents respective voice characteristics of the plurality of different individual speakers.
- the system adapts the adaptive audio-generation model for a new individual speaker using data representing audio spoken by the new individual speaker ( 220 ). As described above with reference to FIG. 1B , the adaptation process uses audio data representing the new individual speaker speaking portions of text.
- the data used for the adaptation process can be orders of magnitude smaller than data used for the training process.
- the training data comprises multiple hours of audio recordings for each individual speaker of the plurality of different individual speakers, while the adaptation process can use less than ten minutes of audio recordings of the new individual speaker.
- the adaptation process is generally much less computationally intensive than the training process.
- the training process is performed in a datacenter having tens or hundreds or thousands of computers, while the adaptation process is performed on a mobile device or a single, Internet-enabled device.
- the system can generate a new embedding vector using the audio data to be used for adapting the model to the new individual speaker.
- the new embedding vector can be different from any of the embedding vectors used during the training process.
- the adaptation process can be performed in multiple ways.
- the system can use a non-parametric technique or a parametric technique.
- the non-parametric technique involves adapting the new speaker embeddings, the model parameters, or both, using held-aside demonstration data.
- the system can fine-tune the model parameters by retraining with respect to held-aside adaptation data.
- the system can jointly optimize both the set of speaker parameters ⁇ e s ⁇ and the shared WaveNet core parameters w.
- the system can then extend the model to a new speaker by extracting the l and f 0 features from their adaptation data and randomly initializing a new embedding vector e.
- the new embedding vector e can then be optimized such that the demonstration waveforms, ⁇ x(1) demo , . . . x(n) demo ⁇ paired with features ⁇ l (1) demo , f 0 (1) demo ), . . . (l (n) demo , f (n) demo ) ⁇ that are most likely, e.g., by satisfying a likelihood threshold under the model with w fixed (SEA-EMB):
- e demo arg ⁇ ⁇ max e ⁇ ⁇ i ⁇ ⁇ log ⁇ ⁇ p ⁇ ( x demo ( i ) ⁇ l demo ( i ) , f 0 , demo ( i ) ; e , w ) .
- model parameters may be additionally fine-tuned (SEA-ALL):
- the system can use a parametric technique that involves training an auxiliary network to predict the embedding vector of a new speaker using the demonstration data.
- the system can train an auxiliary encoder network to predict an embedding vector for a new speaker given their demonstration data.
- the system can model:
- the parametric approach exhibits the advantage of being trained in a transcript-independent setting given only the input waveform, e(x demo ), and requires negligible computation at adaptation time.
- the learned encoder can also introduce bias when fitting an embedding due to its limited network capacity.
- the system performs a text-to-speech inference process to convert text into an output waveform having characteristics of the new individual speaker ( 230 ).
- the system uses the audio-generation model adapted for the new individual speaker, which includes using as input the new embedding vector for the individual speaker and features of a new portion of text.
- This process can also include automatically generating audio of a translation of video or audio content in which the audio of the translation is adapted to have the characteristics of the new speaker.
- this process can be performed during a video conference or a phone call such that audio of the translation is adapted to the characteristics of the new speaker.
- FIG. 3 is a diagram that illustrates an example architecture 300 for an embedding encoder for predicting speaker embeddings.
- the embedding encoder takes as input a demo waveform 310 and generates a predicted speaker embedding 320 .
- the embedding encoder predicts an embedding vector for a speaker when given adaptation data for the speaker.
- the embedding encoder network is illustrated as the summation of the output of two sub-networks 330 and 340 .
- the first subnetwork 330 is a pre-trained speaker verification model (TI-SV) comprising 3 LSTM layers and a single linear layer.
- the first subnetwork 330 generally maps a waveform sequence of arbitrary length to a fixed dimensional d-vector with a sliding window.
- the first subnetwork 330 can be trained from utterances of speakers extracted from anonymized voice search logs.
- the first subnetwork 330 also includes a shallow MLP to project the output d-vector to the speaker embedding space.
- the second subnetwork 340 includes 16 1-D convolutional layers.
- the second subnetwork 340 essentially reduces the temporal resolution, e.g., to 256 ms per frame for 16 kHz audio), and then averages across time and projects into the speaker embedding space.
- the purpose of the second subnetwork 340 is to extract residual speaker information present in the demonstration waveforms but not captured by the pre-trained TI-SV model.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations.
- the index database can include multiple collections of data, each of which may be organized and accessed differently.
- engine is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions.
- an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- the central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser.
- a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- a machine learning framework e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received at the server from the device.
Abstract
Description
- This application claims the benefit under 35 U.S.C. § 119(a) of the filing date of Greek Patent Application No. 20180100486, filed in the Greek Patent Office on Oct. 26, 2018. The disclosure of the foregoing application is herein incorporated by reference in its entirety.
- This specification relates to signal-generation neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer. Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- One example of a neural network is an audio generation neural network. Audio generation neural networks take text as input and generate as output a raw audio waveform of a speaker speaking the text. Realistic audio generation typically requires multiple thousands of samples to be generated per second, e.g., 24,000 samples per second. One example of an audio generation neural network is a WaveNet. WaveNets were initially described in van den Oord et al., WaveNet: A Generative Model for Raw Audio, in arXiv preprint arXiv:1609.03499 (2016), available at arxiv.org. A WaveNet is a deep neural network that models the conditional probability of an audio sample having a particular value given a particular number of previously occurring sample values.
- One of the fundamental limitations of realistic audio generation neural networks is that they require large training datasets. Typically, a training system needs hours of audio recordings for each individual speaker represented in the training data. This amount of data is expensive and cumbersome to obtain and curate in general, and there are many use cases in which this volume of training data is impractical or impossible to obtain. For example, one application of text-to-speech models is providing realistic speech capabilities for medical patients who suffer voice-impairing medical conditions. Such patients typically do not have access to hours of audio recordings of themselves speaking.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that can generate a sample-efficient, adaptive audio-generation model. In this context, being sample-efficient and adaptive means that the model can be customized to emulate the speech of a new speaker with far less training data than was used to train the adaptive model. For example, while training the adaptive model may require hours of audio recordings for each individual speaker, adapting the model for a new speaker may require only a few minutes of audio recordings of the new speaker.
- A training system can train the audio-generation model using a plurality of embedding vectors for respective individual speakers and an audio-generation neural network. Because of the computationally intensive nature of the training process, the training can be performed by a distributed computing system, e.g., a datacenter, having hundreds or thousands of computers.
- The output of the training process is an adaptive audio-generation model that can be efficiently adapted to a new speaker. Adapting the model generally involves learning a new embedding vector for the new speaker, and may optionally involve fine-tuning the parameters of the neural network for the new speaker. The adaptation data can be only a few seconds or a few minutes of audio recordings of the new speaker. The adaptation process is therefore much less computationally intensive than the original training process. Thus, the adaptation process can be performed on much less powerful hardware, e.g., a mobile phone or another wearable device, a desktop or laptop computer, or another internet-enabled device installed in a user's home, to name just a few examples.
- The subject matter described in this specification can be implemented in particular embodiments so as to realize one or more of the following advantages. Adaptive audio-generation model can be used to rapidly adapt to a new speaker using orders of magnitude less data than was used to train the model. This enables the adaptation process to be performed by consumer hardware of end users rather than being performed in a datacenter.
- The adaptation technology enables a variety of technological use cases that were previously not possible. For example, realistic voice translations of video or audio content can be generated that emulate the characteristics of a particular speaker without requiring hours of audio recordings of the speaker. As another example, real-time translated video conferencing or phone calls can be generated in which the translation realistically emulates the speaker who does not actually speak the translated language. In addition, an adaptive audio generation model can be used to provide realistic speech capabilities for medical patients suffering voice-impairing medical conditions without requiring such patients to have hours of audio recordings of themselves speaking.
- The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1A is a diagram that illustrates an example architecture for training a sample-efficient, adaptive audio-generation model. -
FIG. 1B is a diagram that illustrates an example architecture for adapting a sample-efficient, adaptive audio-generation model to a new individual speaker. -
FIG. 1C is a diagram that illustrates anexample architecture 100 c for performing inference using an adapted sample-efficient, adaptive audio-generation model. -
FIG. 2 is a flowchart of an example process for generating and using a sample-efficient, adaptive audio-generation model. -
FIG. 3 is a diagram that illustrates anexample architecture 300 for an embedding encoder for predicting speaker embeddings. - Like reference numbers and designations in the various drawings indicate like elements.
-
FIGS. 1A, 1B, and 1C respectively illustrate example architectures for training, adapting, and performing inference using a sample-efficient, adaptive audio-generation model. The examples in this specification generally discuss using a WaveNet architecture to implement the audio-generation model. However, the same techniques can also be applied using any other appropriate neural audio-generation model, e.g., the WaveRNN model described in Kalchbrenner et al., Efficient Neural Audio Synthesis, in arXiv preprint arXiv:1802.08435 (2016), available at arxiv.org, and the Tacotron 2 model described in Shen et al., Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions, in arXiv preprint arXiv:1712.05884 (2017), available at arxiv.org, to name just a few examples. -
FIG. 1A is a diagram that illustrates anexample architecture 100 a for training a sample-efficient, adaptive audio-generation model. Thearchitecture 100 a includes a WaveNet Stack 110 a that is trained using an embedding table 120 that stores embedding vectors for multiple different respective individual speakers, each individual speaker having aspeaker index 105. The components illustrated inFIG. 1A can be implemented by a distributed computing system comprising a plurality of computers that coordinate to train the WaveNet Stack 110 a. As described above, the training process can use many different individual speakers using hours of audio waveform data. - During each iteration of training,
text 115 and awaveform 125 of audio spoken by an individual corresponding to a value of the speaker index is used as input to the WaveNet Stack 110 a. The computing system performing the training process can then iterate over each individual in the embedding table 120 and optionally over multiple different segments of text for each of the individuals. - The WaveNet Stack 110 a can be implemented as is an autoregressive model that factorizes the joint probability distribution of a waveform, x={x1, . . . , xT}, into a product of conditional distributions using the probabilistic chain rule:
-
- where xt is the t-th timestep sample, and h and w are respectively the conditioning inputs and parameters of the model.
- To train a multi-speaker WaveNet, the conditioning inputs h consist of the speaker identity s, the linguistic features l, and the logarithmic fundamental frequency f0 values. The variable l encodes the sequence of phonemes derived from the input text, and f0 controls the dynamics of the pitch in the generated utterance. Given the speaker index s for each utterance in the dataset, the model can be expressed as:
-
- where the embedding table 120 of speaker embedding vectors es is learned alongside the standard WaveNet parameters. These vectors capture salient voice characteristics across individual speakers and provide a convenient mechanism for generalizing WaveNet to the sample-efficient adaptation techniques described in this specification.
- The
linguistic features l 145 and the fundamentalfrequency values f 0 135 can each be a respective time-series with a lower sampling frequency than thewaveform 125. Thus, to be used as local conditioning variables, thelinguistic features 145 andfundamental frequency values 135 can be upsampled by a transposed convolutional network. During training,l 145 andf 0 135 can be extracted by signal processing methods from pairs of training utterance and transcript. During testing, those values can be predicted from text by existing models. -
FIG. 1B is a diagram that illustrates anexample architecture 100 b for adapting a sample-efficient, adaptive audio-generation model to a new individual speaker. During the adaptation process, the parameters learned during training are adjusted so that they are adapted to a particular individual's voice characteristics. In other words, the purpose of the training process illustrated inFIG. 1A is to learn a prior. During adaptation, this prior is combined with new data to rapidly adapt to a new speakers' voice characteristics. - The
architecture 100 b includes aWaveNet Stack 110 b whose parameters have been adjusted according to a new embeddingvector 122 that represents the characteristics of a new speaker. The new embeddingvector 122 can be generated in a variety of ways, which are discussed in more detail below with reference toFIG. 2 . -
FIG. 1C is a diagram that illustrates anexample architecture 100 c for performing inference using an adapted sample-efficient, adaptive audio-generation model. During the inference process,new text 155 and the embeddingvector 122 generated for the new speaker are used as input to generate anoutput waveform 165 having characteristics of the new speaker. As part of this process, apredictor 150 that uses existing models to generate linguistic features and fundamental frequencies of thenew speaker 150 from thenew text 155. -
FIG. 2 is a flowchart of an example process for generating and using a sample-efficient, adaptive audio-generation model. As described above, the process includes three stages: training, adaptation, and inference. Typically the training stage is performed on a distributed computing system having multiple computers. And as described above, the other two stages can be performed on much less computationally expensive hardware, e.g., a desktop computer, laptop computer, or mobile computing device. For convenience, the example process will be described as being performed by a system of one or more computers. - The system generates an adaptive audio-generation model using data representing audio spoken by a plurality of different individual speakers (210). As described above with reference to
FIG. 1A , the system can generate different embedding vectors for a plurality of individual speakers. The system can then train parameter values of a neural audio-generation model, e.g., a WaveNet stack, a WaveRNN model, or aTacotron 2 model, using training data that includes text and audio data representing a plurality of different individual speakers speaking portions of the text. Each of the embedding vectors generally represents respective voice characteristics of the plurality of different individual speakers. - The system adapts the adaptive audio-generation model for a new individual speaker using data representing audio spoken by the new individual speaker (220). As described above with reference to
FIG. 1B , the adaptation process uses audio data representing the new individual speaker speaking portions of text. - Generally the data used for the adaptation process can be orders of magnitude smaller than data used for the training process. In some implementations, the training data comprises multiple hours of audio recordings for each individual speaker of the plurality of different individual speakers, while the adaptation process can use less than ten minutes of audio recordings of the new individual speaker.
- In addition, the adaptation process is generally much less computationally intensive than the training process. Thus, in some implementations the training process is performed in a datacenter having tens or hundreds or thousands of computers, while the adaptation process is performed on a mobile device or a single, Internet-enabled device.
- The system can generate a new embedding vector using the audio data to be used for adapting the model to the new individual speaker. Generally, the new embedding vector can be different from any of the embedding vectors used during the training process.
- The adaptation process can be performed in multiple ways. In particular, the system can use a non-parametric technique or a parametric technique.
- The non-parametric technique involves adapting the new speaker embeddings, the model parameters, or both, using held-aside demonstration data. For example, the system can fine-tune the model parameters by retraining with respect to held-aside adaptation data.
- For example, when training a WaveNet model to maximize the conditional log-likelihood of the generated audio, the system can jointly optimize both the set of speaker parameters {es} and the shared WaveNet core parameters w. The system can then extend the model to a new speaker by extracting the l and f0 features from their adaptation data and randomly initializing a new embedding vector e. The new embedding vector e can then be optimized such that the demonstration waveforms, {x(1)demo, . . . x(n)demo} paired with features {l(1) demo, f0 (1) demo), . . . (l(n) demo, f(n) demo)} that are most likely, e.g., by satisfying a likelihood threshold under the model with w fixed (SEA-EMB):
-
- Alternatively, all of the model parameters may be additionally fine-tuned (SEA-ALL):
-
- Both non-parametric approaches to sample-efficient voice adaptation as the number of embedding vectors scales with the number of speakers. However, the training processes are slightly different. Because the SEA-EMB method optimizes only a low-dimensional vector, it is far less prone to overfitting, and the system is therefore able to retrain the model to convergence even with mere seconds of adaptation data. By contrast, the SEA-ALL has many more parameters that might overfit to the adaptation data. To address this issue, the system can hold out a particular fraction, e.g., 5%, 10%, or 15%, of the demonstration data for calculating a standard early termination criterion. The system can also pre-initialize e with the optimal value from the SEA-EMB method, which can significantly improve the generalization performance even with just a few seconds of adaptation data.
- Alternatively or in addition, the system can use a parametric technique that involves training an auxiliary network to predict the embedding vector of a new speaker using the demonstration data.
- In contrast to the non-parametric approach, whereby a different embedding vector is fitted for each speaker, the system can train an auxiliary encoder network to predict an embedding vector for a new speaker given their demonstration data. Specifically, the system can model:
-
- where for each training example, a randomly selected demonstration utterance from that speaker is included in addition to the regular conditioning inputs. The full WaveNet model and the encoder network e( ) can then be trained together from scratch. This technique is described in more detail below with reference to
FIG. 3 . - In general, the parametric approach (SEA-ENC) exhibits the advantage of being trained in a transcript-independent setting given only the input waveform, e(xdemo), and requires negligible computation at adaptation time. However, the learned encoder can also introduce bias when fitting an embedding due to its limited network capacity.
- The system performs a text-to-speech inference process to convert text into an output waveform having characteristics of the new individual speaker (230). In general, the system uses the audio-generation model adapted for the new individual speaker, which includes using as input the new embedding vector for the individual speaker and features of a new portion of text. This process can also include automatically generating audio of a translation of video or audio content in which the audio of the translation is adapted to have the characteristics of the new speaker. As one example, this process can be performed during a video conference or a phone call such that audio of the translation is adapted to the characteristics of the new speaker.
-
FIG. 3 is a diagram that illustrates anexample architecture 300 for an embedding encoder for predicting speaker embeddings. In general, the embedding encoder takes as input ademo waveform 310 and generates a predicted speaker embedding 320. In other words, the embedding encoder predicts an embedding vector for a speaker when given adaptation data for the speaker. The embedding encoder network is illustrated as the summation of the output of twosub-networks - The
first subnetwork 330 is a pre-trained speaker verification model (TI-SV) comprising 3 LSTM layers and a single linear layer. Thefirst subnetwork 330 generally maps a waveform sequence of arbitrary length to a fixed dimensional d-vector with a sliding window. Thefirst subnetwork 330 can be trained from utterances of speakers extracted from anonymized voice search logs. Thefirst subnetwork 330 also includes a shallow MLP to project the output d-vector to the speaker embedding space. - The
second subnetwork 340 includes 16 1-D convolutional layers. Thesecond subnetwork 340 essentially reduces the temporal resolution, e.g., to 256 ms per frame for 16 kHz audio), and then averages across time and projects into the speaker embedding space. The purpose of thesecond subnetwork 340 is to extract residual speaker information present in the demonstration waveforms but not captured by the pre-trained TI-SV model. - Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program, which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- This specification uses the term “configured” in connection with systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- In this specification, the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations. Thus, for example, the index database can include multiple collections of data, each of which may be organized and accessed differently.
- Similarly, in this specification the term “engine” is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. Generally, an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser. Also, a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received at the server from the device.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially be claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings and recited in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/061,437 US11355097B2 (en) | 2018-10-26 | 2020-10-01 | Sample-efficient adaptive text-to-speech |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
GR20180100486 | 2018-10-26 | ||
GR20180100486 | 2018-10-26 |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/061,437 Continuation US11355097B2 (en) | 2018-10-26 | 2020-10-01 | Sample-efficient adaptive text-to-speech |
Publications (2)
Publication Number | Publication Date |
---|---|
US20200135172A1 true US20200135172A1 (en) | 2020-04-30 |
US10810993B2 US10810993B2 (en) | 2020-10-20 |
Family
ID=70328405
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/666,043 Active US10810993B2 (en) | 2018-10-26 | 2019-10-28 | Sample-efficient adaptive text-to-speech |
US17/061,437 Active 2039-11-17 US11355097B2 (en) | 2018-10-26 | 2020-10-01 | Sample-efficient adaptive text-to-speech |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/061,437 Active 2039-11-17 US11355097B2 (en) | 2018-10-26 | 2020-10-01 | Sample-efficient adaptive text-to-speech |
Country Status (1)
Country | Link |
---|---|
US (2) | US10810993B2 (en) |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10810993B2 (en) * | 2018-10-26 | 2020-10-20 | Deepmind Technologies Limited | Sample-efficient adaptive text-to-speech |
CN112133278A (en) * | 2020-11-20 | 2020-12-25 | 成都启英泰伦科技有限公司 | Network training and personalized speech synthesis method for personalized speech synthesis model |
CN112634856A (en) * | 2020-12-10 | 2021-04-09 | 苏州思必驰信息科技有限公司 | Speech synthesis model training method and speech synthesis method |
CN113408208A (en) * | 2021-06-25 | 2021-09-17 | 成都欧珀通信科技有限公司 | Model training method, information extraction method, related device and storage medium |
WO2022045486A1 (en) * | 2020-08-25 | 2022-03-03 | 주식회사 딥브레인에이아이 | Method and apparatus for generating speech video |
US11335324B2 (en) * | 2020-08-31 | 2022-05-17 | Google Llc | Synthesized data augmentation using voice conversion and speech recognition models |
WO2022156479A1 (en) * | 2021-01-20 | 2022-07-28 | 北京沃东天骏信息技术有限公司 | Custom tone and vocal synthesis method and apparatus, electronic device, and storage medium |
US11763799B2 (en) * | 2020-11-12 | 2023-09-19 | Samsung Electronics Co., Ltd. | Electronic apparatus and controlling method thereof |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060230140A1 (en) * | 2005-04-05 | 2006-10-12 | Kazumi Aoyama | Information processing apparatus, information processing method, and program |
US20110087488A1 (en) * | 2009-03-25 | 2011-04-14 | Kabushiki Kaisha Toshiba | Speech synthesis apparatus and method |
US20130262119A1 (en) * | 2012-03-30 | 2013-10-03 | Kabushiki Kaisha Toshiba | Text to speech system |
US20140379348A1 (en) * | 2013-06-21 | 2014-12-25 | Snu R&Db Foundation | Method and apparatus for improving disordered voice |
US20170069327A1 (en) * | 2015-09-04 | 2017-03-09 | Google Inc. | Neural Networks For Speaker Verification |
US20170076713A1 (en) * | 2015-09-14 | 2017-03-16 | International Business Machines Corporation | Cognitive computing enabled smarter conferencing |
US20180032871A1 (en) * | 2016-07-29 | 2018-02-01 | Google Inc. | Systems and Methods to Perform Machine Learning with Feedback Consistency |
US20180082679A1 (en) * | 2016-09-18 | 2018-03-22 | Newvoicemedia, Ltd. | Optimal human-machine conversations using emotion-enhanced natural speech using hierarchical neural networks and reinforcement learning |
US20200005763A1 (en) * | 2019-07-25 | 2020-01-02 | Lg Electronics Inc. | Artificial intelligence (ai)-based voice sampling apparatus and method for providing speech style |
US20200005764A1 (en) * | 2019-07-31 | 2020-01-02 | Lg Electronics Inc. | Artificial intelligence (ai)-based voice sampling apparatus and method for providing speech style in heterogeneous label |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9721561B2 (en) * | 2013-12-05 | 2017-08-01 | Nuance Communications, Inc. | Method and apparatus for speech recognition using neural networks with speaker adaptation |
US10360901B2 (en) * | 2013-12-06 | 2019-07-23 | Nuance Communications, Inc. | Learning front-end speech recognition parameters within neural network training |
US10810993B2 (en) * | 2018-10-26 | 2020-10-20 | Deepmind Technologies Limited | Sample-efficient adaptive text-to-speech |
-
2019
- 2019-10-28 US US16/666,043 patent/US10810993B2/en active Active
-
2020
- 2020-10-01 US US17/061,437 patent/US11355097B2/en active Active
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060230140A1 (en) * | 2005-04-05 | 2006-10-12 | Kazumi Aoyama | Information processing apparatus, information processing method, and program |
US20110087488A1 (en) * | 2009-03-25 | 2011-04-14 | Kabushiki Kaisha Toshiba | Speech synthesis apparatus and method |
US20130262119A1 (en) * | 2012-03-30 | 2013-10-03 | Kabushiki Kaisha Toshiba | Text to speech system |
US20140379348A1 (en) * | 2013-06-21 | 2014-12-25 | Snu R&Db Foundation | Method and apparatus for improving disordered voice |
US20170069327A1 (en) * | 2015-09-04 | 2017-03-09 | Google Inc. | Neural Networks For Speaker Verification |
US20170076713A1 (en) * | 2015-09-14 | 2017-03-16 | International Business Machines Corporation | Cognitive computing enabled smarter conferencing |
US20180032871A1 (en) * | 2016-07-29 | 2018-02-01 | Google Inc. | Systems and Methods to Perform Machine Learning with Feedback Consistency |
US20180082679A1 (en) * | 2016-09-18 | 2018-03-22 | Newvoicemedia, Ltd. | Optimal human-machine conversations using emotion-enhanced natural speech using hierarchical neural networks and reinforcement learning |
US20200005763A1 (en) * | 2019-07-25 | 2020-01-02 | Lg Electronics Inc. | Artificial intelligence (ai)-based voice sampling apparatus and method for providing speech style |
US20200005764A1 (en) * | 2019-07-31 | 2020-01-02 | Lg Electronics Inc. | Artificial intelligence (ai)-based voice sampling apparatus and method for providing speech style in heterogeneous label |
Non-Patent Citations (1)
Title |
---|
Taigman Non-patent literature VoiceLoop Voice Fitting and Synthesis Via A Phonological Loop, listed in IDS dated 12/13/2019 * |
Cited By (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10810993B2 (en) * | 2018-10-26 | 2020-10-20 | Deepmind Technologies Limited | Sample-efficient adaptive text-to-speech |
US11355097B2 (en) * | 2018-10-26 | 2022-06-07 | Deepmind Technologies Limited | Sample-efficient adaptive text-to-speech |
WO2022045486A1 (en) * | 2020-08-25 | 2022-03-03 | 주식회사 딥브레인에이아이 | Method and apparatus for generating speech video |
US11335324B2 (en) * | 2020-08-31 | 2022-05-17 | Google Llc | Synthesized data augmentation using voice conversion and speech recognition models |
US11763799B2 (en) * | 2020-11-12 | 2023-09-19 | Samsung Electronics Co., Ltd. | Electronic apparatus and controlling method thereof |
CN112133278A (en) * | 2020-11-20 | 2020-12-25 | 成都启英泰伦科技有限公司 | Network training and personalized speech synthesis method for personalized speech synthesis model |
CN112634856A (en) * | 2020-12-10 | 2021-04-09 | 苏州思必驰信息科技有限公司 | Speech synthesis model training method and speech synthesis method |
WO2022156479A1 (en) * | 2021-01-20 | 2022-07-28 | 北京沃东天骏信息技术有限公司 | Custom tone and vocal synthesis method and apparatus, electronic device, and storage medium |
CN113408208A (en) * | 2021-06-25 | 2021-09-17 | 成都欧珀通信科技有限公司 | Model training method, information extraction method, related device and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US10810993B2 (en) | 2020-10-20 |
US11355097B2 (en) | 2022-06-07 |
US20210020160A1 (en) | 2021-01-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11355097B2 (en) | Sample-efficient adaptive text-to-speech | |
US11869530B2 (en) | Generating audio using neural networks | |
US11948066B2 (en) | Processing sequences using convolutional neural networks | |
US10573293B2 (en) | End-to-end text-to-speech conversion | |
US20210295858A1 (en) | Synthesizing speech from text using neural networks | |
EP3926623A1 (en) | Speech recognition method and apparatus, and neural network training method and apparatus | |
US20210089909A1 (en) | High fidelity speech synthesis with adversarial networks | |
JP7335460B2 (en) | clear text echo | |
KR102663654B1 (en) | Adaptive visual speech recognition | |
WO2024055752A1 (en) | Speech synthesis model training method, speech synthesis method, and related apparatuses | |
US20240135955A1 (en) | Generating audio using neural networks | |
US20240153485A1 (en) | Systems and methods for machine-learning based multi-lingual pronunciation generation | |
CA3214170A1 (en) | Adaptive visual speech recognition | |
CN111522917A (en) | Dialogue emotion detection method and device, storage medium and electronic equipment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: DEEPMIND TECHNOLOGIES LIMITED, UNITED KINGDOMFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GOOGLE LLC;REEL/FRAME:051268/0307Effective date: 20191029 |
|
AS | Assignment |
Owner name: DEEPMIND TECHNOLOGIES LIMITED, UNITED KINGDOMFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CHEN, YUTIAN;REED, SCOTT ELLISON;VAN DEN OORD, AARON GERARD ANTONIUS;AND OTHERS;SIGNING DATES FROM 20200414 TO 20200429;REEL/FRAME:052523/0189 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |