US11854165B2 - Debanding using a novel banding metric - Google Patents
Debanding using a novel banding metric Download PDFInfo
- Publication number
- US11854165B2 US11854165B2 US17/922,531 US202017922531A US11854165B2 US 11854165 B2 US11854165 B2 US 11854165B2 US 202017922531 A US202017922531 A US 202017922531A US 11854165 B2 US11854165 B2 US 11854165B2
- Authority
- US
- United States
- Prior art keywords
- image
- training
- banding
- map
- model
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 238000012549 training Methods 0.000 claims abstract description 326
- 238000000034 method Methods 0.000 claims abstract description 88
- 230000006870 function Effects 0.000 claims description 39
- 230000015654 memory Effects 0.000 claims description 27
- 238000012545 processing Methods 0.000 claims description 19
- 238000010801 machine learning Methods 0.000 description 30
- 238000013527 convolutional neural network Methods 0.000 description 21
- 238000010586 diagram Methods 0.000 description 16
- 238000001914 filtration Methods 0.000 description 16
- 238000004422 calculation algorithm Methods 0.000 description 15
- 230000006835 compression Effects 0.000 description 12
- 238000007906 compression Methods 0.000 description 12
- 238000004891 communication Methods 0.000 description 11
- 238000000605 extraction Methods 0.000 description 9
- 238000013139 quantization Methods 0.000 description 7
- 238000012360 testing method Methods 0.000 description 7
- 230000004913 activation Effects 0.000 description 6
- 238000013528 artificial neural network Methods 0.000 description 6
- 239000003086 colorant Substances 0.000 description 6
- 239000011159 matrix material Substances 0.000 description 6
- 238000009826 distribution Methods 0.000 description 5
- 238000011156 evaluation Methods 0.000 description 5
- 238000007781 pre-processing Methods 0.000 description 5
- 208000037170 Delayed Emergence from Anesthesia Diseases 0.000 description 4
- 238000004590 computer program Methods 0.000 description 4
- 230000000694 effects Effects 0.000 description 4
- 238000012805 post-processing Methods 0.000 description 4
- 230000000903 blocking effect Effects 0.000 description 3
- 238000010606 normalization Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000011176 pooling Methods 0.000 description 3
- 230000001131 transforming effect Effects 0.000 description 3
- 230000007704 transition Effects 0.000 description 3
- 239000013598 vector Substances 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 2
- 230000000052 comparative effect Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 239000000284 extract Substances 0.000 description 2
- 238000012886 linear function Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 230000008569 process Effects 0.000 description 2
- 238000009877 rendering Methods 0.000 description 2
- 230000002123 temporal effect Effects 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 241000233805 Phoenix Species 0.000 description 1
- 238000012152 algorithmic method Methods 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000015556 catabolic process Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000008867 communication pathway Effects 0.000 description 1
- 230000006837 decompression Effects 0.000 description 1
- 238000006731 degradation reaction Methods 0.000 description 1
- 235000019580 granularity Nutrition 0.000 description 1
- 210000004209 hair Anatomy 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 238000007670 refining Methods 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000010408 sweeping Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Images
Classifications
-
- G06T5/70—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T5/00—Image enhancement or restoration
- G06T5/001—Image restoration
- G06T5/002—Denoising; Smoothing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T5/00—Image enhancement or restoration
- G06T5/20—Image enhancement or restoration by the use of local operators
-
- G06T5/60—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/10—Segmentation; Edge detection
- G06T7/13—Edge detection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/56—Extraction of image or video features relating to colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/74—Image or video pattern matching; Proximity measures in feature spaces
- G06V10/761—Proximity, similarity or dissimilarity measures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
Definitions
- Image content (e.g., of still images or frames of video) represents a significant amount of online content.
- a web page may include multiple images, and a large portion of the time and resources spent rendering the web page are dedicated to rendering those images for display.
- the amount of time and resources required to receive and render an image for display depends in part on the manner in which the image is compressed. As such, an image can be rendered faster by reducing the total data size of the image using lossy compression and decompression techniques.
- Lossy compression techniques seek to represent image content using fewer bits than the number of bits in the original image. Lossy compression techniques can introduce visual artefacts, such as ringing artefacts and banding artefacts, into the decompressed image. Higher compression levels can result in more observable artefacts. It is desirable to detect (e.g., measure, etc.) such artefacts (e.g., banding artefacts) so that they can be eliminated (or at least reduced).
- a first aspect is a method of removing banding artefacts from an image.
- the method includes training a first model to measure the banding artefacts; training a second model to deband the image; and generating a debanded image for the image using the second model.
- Training the first model includes selecting a first set of first training images to train the first model; for a first training image of the first set of the first training images, generating a banding edge map, where the banding edge map includes weights that emphasize banding edges and de-emphasize true edges in the first training image; and using the banding edge map and a luminance plane of the first training image as input to the first model.
- Training the second model includes selecting a second set of second training images to train the second model; for a second training image of the second set of the second training images, generating a debanded training image; generating, using the first model, a banding score for the debanded training image; and using the banding score in a loss function that is used in training the second model.
- a second aspect is an apparatus for debanding an image.
- the apparatus includes a memory and a processor.
- the processor is configured to execute a second model that receives the image as an input and outputs a debanded image for the image.
- the second model is trained by steps to select a second set of second training images; for a second training image of the second training images, generate a training debanded image; generate a banding score for the training debanded image; generate an image difference between the second training image and the training debanded image; and use a weighted combination of the image difference and the banding score in a loss function that is used to train the second model.
- a third aspect is an apparatus for measuring a banding score of an image.
- the apparatus includes a first model that is configured to receive the image as an input; and output the banding score for the image.
- the first model is trained by steps to select a first set of first training images to train the first model; for a first training image of the first set of the first training images, generate a banding edge map, where the banding edge map includes weights that emphasize banding edges and de-emphasize true edges in the first training image; and use the banding edge map and a luminance plane of the first training image as input to the first model.
- a fourth aspect is a method of training a model to deband an image, comprising: for a training image of a second set of second training images: receiving a training debanded image, the training debanded image comprising image data obtained by removing banding artefacts from the training image; generating a banding score for the training debanded image; generating an image difference between the second training image and the training debanded image; and using a weighted combination of the image difference and the banding score in a loss function that is used to train the second model.
- the second set of second training images may comprise a plurality of training images and the method may comprise repeating the processing for each second training image of the second set.
- the method may further comprise: receiving an image comprising banding artefacts; and processing the image using the model to remove the banding artefacts.
- Generating the banding score for the training debanded image may comprise processing the training debanded image using a first model to generate the banding score, wherein the first model is trained by, for a first training image of a first set of first training images: generating a banding edge map from the first training image, wherein the banding edge map indicating edges caused by banding artefacts in the first training image; and using the banding edge map and a luminance plane of the first training image as input to the first model.
- the banding edge map may comprise weights for each of a plurality of locations of the first training image, the weights emphasizing banding edges and de-emphasizing true edges in the first training image.
- Generating the banding edge map may comprise: computing a gradient map for a luminance channel of the first training image; computing a weight map for the first training image and obtaining the banding edge map by multiplying the gradient map by the weight map.
- the weight map may comprise respective weights for pixels of the first training image.
- the weight map may be computed using the gradient map.
- Computing the weight map for the first training image may comprise: generating a map using the gradient map, wherein the map excludes high contrast pixels; and convolving, to obtain the weight map, the map with a kernel having a predefined size.
- the predefined size may be equal to 7 ⁇ 7.
- the kernel may consist of 1 values.
- a fifth aspect is a method of training a model to remove banding artefacts from an image, comprising: training a first model to measure the banding artefacts, wherein training the first model comprises: for a training image of a first set of first training images: generating a banding edge map from the training image, wherein the banding edge map indicates edges caused by banding artefacts in the training image; and using the banding edge map and a luminance plane of the training image as training input to the first model; training a second model to deband the image, wherein training the second model comprises, for a second training image of a second set of second training images: receiving a debanded training image, the debanded training image comprising image data obtained by removing banding artefacts from the training image; generating, using the first model, a banding score for the debanded training image; using the banding score in a loss function that is used in training the second model; and outputting the trained second model.
- the first set of first training images may comprise a plurality of training images and the method may comprise repeating the processing for each first training image of the first set.
- the method may further comprise: receiving an image comprising banding artefacts; and processing the image using the trained second model to remove the banding artefacts.
- Training the second model to deband the image may comprise: obtaining an image difference between the debanded training image and the second training image; and using the image difference in the loss function that is used in training the second model.
- the loss function may be a weighted sum of the image difference and the banding score.
- Generating the banding edge map may comprise: computing a gradient map for a luminance channel of the first training image; computing a weight map for the first training image; and obtaining the banding edge map by multiplying the gradient map by the weight map.
- the weight map may comprise respective weights for pixels of the first training image. The weight map may be computed using the gradient map.
- Computing the weight map for the first training image may comprise: generating a map using the gradient map, wherein the map excludes high contrast pixels; and convolving, to obtain the weight map, the map with a kernel having a predefined size.
- the predefined size may be equal to 7 ⁇ 7.
- the kernel may consist of 1 values.
- a sixth aspect is a method of training a first model to measure a banding score of an image, comprising: for a first training image of a first set of first training images: generating a banding edge map, wherein the banding edge map indicates edges caused by banding artefacts in the training image; and using the banding edge map and a luminance plane of the first training image as training input to the first model.
- the first set of first training images may comprise a plurality of training images and the method may comprise repeating the processing for each first training image of the first set.
- the method may further comprise: receiving an image as an input; and processing the image using the first model to generate a banding score for the image.
- Generating the banding edge map may comprise: computing a gradient map for the luminance plane of the first training image; computing a weight map for the first training image; and obtaining the banding edge map by multiplying the gradient map by the weight map.
- the weight map may comprise respective weights for pixels of the first training image.
- the weight map may be computed using the gradient map.
- Computing the weight map for the first training image may comprise: generating a map using the gradient map to exclude high contrast pixels; and convolving, to obtain the weight map, the map with a kernel having a predefined size.
- the predefined size may be equal to 7 ⁇ 7.
- the kernel may consist of 1 values.
- the first model may be used to train a second model, wherein the second model receives a banded image as an input and outputs a debanded image.
- the second model may be trained by steps to: select a second set of training images; for a second training image of the second training images, generate a training debanded image; generate, using the first model, a training banding score for the training debanded image; generate an image difference between the second training image and the debanded image; and use a weighted combination of the image difference and the training banding score in a loss function that is used to train the second model.
- the loss function may be a sum of a second norm of the image difference plus a weight multiplied by a second norm of the training banding score.
- the first model may be a scoring model for generating a banding score for an image.
- the first training images may be scoring images for training the scoring model to generate a banding score.
- the second model may be a banding artefact removal model.
- the second training images may be banding artefact training images for training the banding artefact removal model to remove banding artefacts from images.
- the first model is sometimes referred to herein as a BandingNet and the second model is sometimes referred to herein as a DeBandingNet.
- aspects can be implemented in any convenient form.
- aspects may be implemented by appropriate computer programs that may be carried on appropriate carrier media, which may be tangible carrier media (e.g., disks) or intangible carrier media (e.g., communications signals).
- appropriate carrier media may be tangible carrier media (e.g., disks) or intangible carrier media (e.g., communications signals).
- aspects may also be implemented using suitable apparatus which may take the form of programmable computers running computer programs arranged to implement the methods. Aspects can be combined such that features described in the context of one aspect may be implemented in another aspect.
- FIG. 1 is a diagram of a computing device in accordance with implementations of this disclosure.
- FIG. 2 is a diagram of a computing and communications system in accordance with implementations of this disclosure.
- FIG. 3 is a diagram of a video stream for use in encoding and decoding in accordance with implementations of this disclosure.
- FIG. 4 is a block diagram of an encoder in accordance with implementations of this disclosure.
- FIG. 5 is a block diagram of a decoder in accordance with implementations of this disclosure.
- FIG. 6 is an illustration of an example of staircasing.
- FIG. 7 A is an example of using a trained debanding model in a pre-processing scenario according to implementations of this disclosure.
- FIG. 7 B is an example of using a trained debanding model in a post-processing scenario according to implementations of this disclosure.
- FIG. 8 is a flowchart of an example of a technique for training a BandingNet according to implementations of this disclosure.
- FIG. 9 illustrates banding score distributions of the training images of the first set of first training images.
- FIG. 10 is an example of a luminance image and a banding edge map according to implementations of this disclosure.
- FIGS. 11 A- 11 B are comparative examples of training a BandingNet without and with using a banding edge map.
- FIG. 12 is a flowchart of an example of a technique for training a DeBandingNet according to implementations of this disclosure.
- FIG. 13 is a flowchart of an example of a technique for training a DeBandingNet using a BandingNet according to implementations of this disclosure.
- FIG. 14 is a diagram that illustrates at least some aspects of the technique of FIG. 13 .
- FIG. 15 is a block diagram of an example of a typical convolutional neural network (CNN).
- CNN convolutional neural network
- FIG. 16 is an example of a technique for training and using a machine-learning model according to implementations of this disclosure.
- Video compression schemes may include breaking each image (a single image or a frame of a video), into smaller portions, such as blocks, and generating an output bitstream using techniques to limit the information included for each block in the output.
- An encoded bitstream can be decoded to re-create the blocks and the source images from the limited information.
- the information included for each block in the output may be limited by reducing spatial redundancy, reducing temporal redundancy (in the case of video), or a combination thereof. For example, temporal (in the case of video frames) or spatial redundancies may be reduced by predicting a frame based on information available to both the encoder and decoder, and including information representing a difference, or residual, between the predicted frame and the original frame.
- the residual information may be further compressed by transforming the residual information into transform coefficients.
- Transforming the residual information into transform coefficients can include a quantization step, which introduces loss—hence the name or term “lossy compression.”
- Lossy compression can be used to code visual information of an image.
- a lossy compression technique can be applied to a source image to produce a compressed image.
- the inverse of the lossy technique can be applied to the compressed image to produce a decompressed image.
- the lossy aspect of a lossy compression technique can be attributed, at least partially, to the quantizing of frequency domain information (as further described below).
- the amount of loss is dictated by a quantization step, which uses a quantization parameter (QP).
- QP quantization parameter
- Palette-based coding can be used to code screen content, such as computer-generated video with substantial amount of text and graphics.
- Palette-based coding can result in decoded images that include artefacts, such as staircasing (or banding) artefacts.
- artefacts such as staircasing (or banding) artefacts.
- source images may include local gradients that result from the point-spread-function of a capturing device, from anti-aliasing effects of the imaged structures, or from naturally and slowly developing image structures.
- palette-based coding may result in image artefacts, such as artificial staircasing, in the decoded (i.e., reconstructed) images.
- Such staircasing artefacts may also be referred to as banding artefacts.
- palette-based coding artefacts An example of palette-based coding artefacts is now presented. Assume that an image is that of a glossy whiteboard upon which light sources are shined at different locations. As such, the image of the whiteboard may include small gradients and transition areas from white to light grey. Palette-based encoding the image may create banding effects, as described above. That is, for example, instead of gradual color changes, some bands (e.g., 16 bands) of grey may show in the decoded image where hard, artificial borders may form in the decoded image.
- some bands e.g., 16 bands
- FIG. 6 is an illustration of an example 600 of staircasing (banding).
- the banding can be due to palette-based coding.
- the example 600 includes a source image 601 and a portion 603 , which is shown as a zoomed region in a source image portion 602 .
- the source image 601 and the portion 603 include many colors. However, for clarity of the image, the source image 601 and the portion 603 are shown in black and white. As such, the source image portion 602 can include several colors; however, the source image portion 602 is shown here in monochrome colors for reproduction reasons and clarity.
- the source image portion 602 illustrates smooth and gradual transitions between different shades of grey.
- a palette-based decoded representation 604 and a palette-based decoded representation 606 of the source image portion 602 illustrate staircasing effects.
- the staircasing effects can be more prominent, as shown in the palette-based decoded representation 606 , when fewer colors are used in the palette used to encode and decode the source image portion 602 .
- the source image portion 602 includes smooth color transitions, color bands (such as color bands 608 , 610 , 612 ) are formed in the palette-based decoded representation 606 .
- the source image 601 may be a frame of a user-generated content (UGC) video.
- the video may be encoded and uploaded to (or uploaded and then encoded by) a content delivery service (e.g., YouTube).
- Image portion 620 may be part of the encoded and uploaded UGC video.
- the image portion 620 can be an encoded version of the source image portion 602 .
- the content delivery service may transcode a UGC video to accommodate different resolution, bitrate, aspect ratio, quality, network bandwidth, and/or like criteria. Further encoding an already encoded image that includes banding artefacts can further exacerbate the banding artefacts in at least some of the transcoded versions. For example, an image portion 622 is a transcoded and re-encoded version of the image portion 620 .
- debanding an image can be accomplished in two relatively independent steps.
- the first step detects where the banding artefacts are in an image; and the second step removes those artefacts.
- Complicated image processing techniques can be used to detect the banding artefacts.
- some traditional techniques may blindly assign white noise across the entire image whether or not some regions of the image include banding artefacts.
- Such traditional debanding techniques are undifferentiable. More specifically, the calculation of the debanding score is undifferentiable. For example, whereas a banding score of a debanded image (obtained using a traditional technique) may be obtained as a performance measure of the debanding step, the debanding score computation cannot be embedded in a neural network to compute the loss.
- the debanding techniques described herein can use banding information (e.g., a banding score) in the debanded image to more intelligently (e.g., such as by refining the debanding capability of the machine-learning model) assign noise to deband the image.
- banding information e.g., a banding score
- Techniques according to this disclosure use a machine-learning (ML) model (i.e., referred to below as a BandingNet and the first model) that is trained to approximate the output of non-differential traditional debanding techniques.
- ML machine-learning
- BandingNet BandingNet
- the debanding technique described herein i.e., using what is referred below as a DeBandingNet and the second model
- the debanding technique described herein is differentiable.
- the ML models can be convolutional neural networks (CNNs).
- CNNs convolutional neural networks
- a typical structure of a CNN is described with respect to FIG. 15 .
- Each of the two learning models can be as described with respect to FIG. 15 .
- BandingNet and the first model
- DeBandingNet and the second model.
- the BandingNet computes (e.g., infers, outputs, etc.) a banding score for an input image, which may include banding artefacts.
- the BandingNet can output a classification of the input image, which can be mapped to a banding score.
- the BandingNet is simply said to output a banding score.
- the DeBandingNet outputs a debanded image given an input image that may include banding artefacts.
- the BandingNet can be trained to output a banding score. After the BandingNet is trained, the BandingNet can be used in the training phase of the DeBandingNet. Banding scores output from the BandingNet can be used in a loss function that is used in the training of the DeBandingNet. After the DeBandingNet is trained, it can be used to generate debanded images of input images. For example, after training, the DeBandingNet can sufficiently distinguish between frames that include and those that don't include banding artefacts and adaptively apply debanding operations (e.g., dithering) based on the content of an input frame.
- debanding operations e.g., dithering
- the trained BandingNet and DeBandingNet can be used together. For example, given an image, a banding score (or, equivalently, a banding classification) can be used to determine whether the image should be debanded via the DeBandingNet. For example, if the banding score of the image is greater than a threshold value, then the image can be input to the DeBandingNet for debanding; otherwise, the image is not input to the DeBandingNet.
- a banding score (or, equivalently, a banding classification) can be used to determine whether the image should be debanded via the DeBandingNet. For example, if the banding score of the image is greater than a threshold value, then the image can be input to the DeBandingNet for debanding; otherwise, the image is not input to the DeBandingNet.
- the UGC can be debanded prior to transcoding the UGC.
- all images (e.g., frames) of the UGC can simply be input to the DeBandingNet to generate respective debanded images.
- a frame is input to the DeBandingNet only if the banding score of the frame, which can be obtained using the BandingNet, exceeds a threshold (e.g., a threshold value or a threshold classification).
- a threshold e.g., a threshold value or a threshold classification
- the UGC can be transcoded into one or more versions.
- a user device that receives one of the transcoded versions can use the DeBandingNet to obtain debanded images of the UGC.
- an image can be input to the DeBandingNet only if the banding score of the image, which can be obtained via the BandingNet, exceeds a threshold, prior to displaying (or saving) the one version at the user device.
- This scenario is referred to below as debanding via post-processing.
- the post-processing scenario is described below with respect to FIG. 7 B .
- FIG. 1 is a schematic of a video encoding and decoding system 100 .
- a transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2 .
- the processing of the transmitting station 102 can be distributed among multiple devices.
- a network 104 can connect the transmitting station 102 and a receiving station 106 for encoding and decoding of the video stream.
- the video stream can be encoded in the transmitting station 102
- the encoded video stream can be decoded in the receiving station 106 .
- the network 104 can be, for example, the Internet.
- the network 104 can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmitting station 102 to, in this example, the receiving station 106 .
- LAN local area network
- WAN wide area network
- VPN virtual private network
- the receiving station 106 can be a computer having an internal configuration of hardware, such as that described with respect to FIG. 2 .
- the processing of the receiving station 106 can be distributed among multiple devices.
- an implementation can omit the network 104 .
- a video stream can be encoded and then stored for transmission at a later time to the receiving station 106 or any other device having memory.
- the receiving station 106 receives (e.g., via the network 104 , a computer bus, and/or some communication pathway) the encoded video stream and stores the video stream for later decoding.
- a real-time transport protocol RTP
- a transport protocol other than RTP e.g., a Hyper-Text Transfer protocol (HTTP)-based video streaming protocol, may be used.
- HTTP Hyper-Text Transfer protocol
- the transmitting station 102 and/or the receiving station 106 may include the ability to both encode and decode a video stream as described below.
- the receiving station 106 could be a video conference participant who receives an encoded video bitstream from a video conference server (e.g., the transmitting station 102 ) to decode and view and further encodes and transmits its own video bitstream to the video conference server for decoding and viewing by other participants.
- FIG. 2 is a block diagram of an example of a computing device 200 that can implement a transmitting station or a receiving station.
- the computing device 200 can implement one or both of the transmitting station 102 and the receiving station 106 of FIG. 1 .
- the computing device 200 can be in the form of a computing system including multiple computing devices, or in the form of a single computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like.
- a CPU 202 in the computing device 200 can be a central processing unit.
- the CPU 202 can be any other type of device, or multiple devices, now-existing or hereafter developed, capable of manipulating or processing information.
- the disclosed implementations can be practiced with a single processor as shown (e.g., the CPU 202 ), advantages in speed and efficiency can be achieved by using more than one processor.
- a memory 204 in the computing device 200 can be a read-only memory (ROM) device or a random-access memory (RAM) device. Any other suitable type of storage device can be used as the memory 204 .
- the memory 204 can include code and data 206 that is accessed by the CPU 202 using a bus 212 .
- the memory 204 can further include an operating system 208 and application programs 210 , the application programs 210 including at least one program that permits the CPU 202 to perform at least some of the techniques described herein.
- the application programs 210 can include applications 1 through N, which further include one or more applications that perform at least some of the techniques described herein.
- the computing device 200 can also include a secondary storage 214 , which can, for example, be a memory card used with a computing device 200 that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- a secondary storage 214 can, for example, be a memory card used with a computing device 200 that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- the computing device 200 can also include one or more output devices, such as a display 218 .
- the display 218 may be, in one example, a touch-sensitive display that combines a display with a touch-sensitive element that is operable to sense touch inputs.
- the display 218 can be coupled to the CPU 202 via the bus 212 .
- Other output devices that permit a user to program or otherwise use the computing device 200 can be provided in addition to or as an alternative to the display 218 .
- the output device is or includes a display
- the display can be implemented in various ways, including as a liquid crystal display (LCD); a cathode-ray tube (CRT) display; or a light-emitting diode (LED) display, such as an organic LED (OLED) display.
- LCD liquid crystal display
- CRT cathode-ray tube
- LED light-emitting diode
- OLED organic LED
- the computing device 200 can also include or be in communication with an image-sensing device 220 , for example, a camera, or any other image-sensing device, now existing or hereafter developed, that can sense an image, such as the image of a user operating the computing device 200 .
- the image-sensing device 220 can be positioned such that it is directed toward the user operating the computing device 200 .
- the position and optical axis of the image-sensing device 220 can be configured such that the field of vision includes an area that is directly adjacent to the display 218 and from which the display 218 is visible.
- the computing device 200 can also include or be in communication with a sound-sensing device 222 , for example, a microphone, or any other sound-sensing device, now existing or hereafter developed, that can sense sounds near the computing device 200 .
- the sound-sensing device 222 can be positioned such that it is directed toward the user operating the computing device 200 and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates the computing device 200 .
- FIG. 2 depicts the CPU 202 and the memory 204 of the computing device 200 as being integrated into a single unit, other configurations can be utilized.
- the operations of the CPU 202 can be distributed across multiple machines (each machine having one or more processors) that can be coupled directly or across a local area or other network.
- the memory 204 can be distributed across multiple machines, such as a network-based memory or memory in multiple machines performing the operations of the computing device 200 .
- the bus 212 of the computing device 200 can be composed of multiple buses.
- the secondary storage 214 can be directly coupled to the other components of the computing device 200 or can be accessed via a network and can comprise a single integrated unit, such as a memory card, or multiple units, such as multiple memory cards.
- the computing device 200 can thus be implemented in a wide variety of configurations.
- FIG. 3 is a diagram of an example of a video stream 300 to be encoded and subsequently decoded.
- the video stream 300 includes a video sequence 302 .
- the video sequence 302 includes a number of adjacent frames 304 . While three frames are depicted as the adjacent frames 304 , the video sequence 302 can include any number of adjacent frames 304 .
- the adjacent frames 304 can then be further subdivided into individual frames, for example, a frame 306 .
- the frame 306 can be divided into a series of segments 308 or planes.
- the segments 308 can be subsets of frames that permit parallel processing, for example.
- the segments 308 can also be subsets of frames that can separate the video data into separate colors.
- the frame 306 of color video data can include a luminance plane and two chrominance planes.
- the segments 308 may be sampled at different resolutions.
- the frame 306 may be further subdivided into blocks 310 , which can contain data corresponding to, for example, 16 ⁇ 16 pixels in the frame 306 .
- the blocks 310 can also be arranged to include data from one or more segments 308 of pixel data.
- the blocks 310 can also be of any other suitable size, such as 4 ⁇ 4 pixels, 8 ⁇ 8 pixels, 16 ⁇ 8 pixels, 8 ⁇ 16 pixels, 16 ⁇ 16 pixels, or larger.
- FIG. 4 is a block diagram of an encoder 400 in accordance with implementations of this disclosure.
- the encoder 400 can be implemented, as described above, in the transmitting station 102 , such as by providing a computer software program stored in memory, for example, the memory 204 .
- the computer software program can include machine instructions that, when executed by a processor, such as the CPU 202 , cause the transmitting station 102 to encode video data in manners described herein.
- the encoder 400 can also be implemented as specialized hardware included in, for example, the transmitting station 102 .
- the encoder 400 has the following stages to perform the various functions in a forward path (shown by the solid connection lines) to produce an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter-prediction stage 402 , a transform stage 404 , a quantization stage 406 , and an entropy encoding stage 408 .
- the encoder 400 may also include a reconstruction path (shown by the dotted connection lines) to reconstruct a frame for encoding of future blocks.
- the encoder 400 has the following stages to perform the various functions in the reconstruction path: a dequantization stage 410 , an inverse transform stage 412 , a reconstruction stage 414 , and a loop filtering stage 416 .
- Other structural variations of the encoder 400 can be used to encode the video stream 300 .
- a block can be encoded using intra-frame prediction (also called intra-prediction) or inter-frame prediction (also called inter-prediction), or a combination of both.
- intra-frame prediction also called intra-prediction
- inter-frame prediction also called inter-prediction
- a prediction block can be formed.
- intra-prediction all or part of a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed.
- inter-prediction all or part of a prediction block may be formed from samples in one or more previously constructed reference frames determined using motion vectors.
- the prediction block can be subtracted from the current block at the intra/inter-prediction stage 402 to produce a residual block (also called a residual).
- the transform stage 404 transforms the residual into transform coefficients in, for example, the frequency domain using block-based transforms.
- block-based transforms i.e., transform types
- DCT Discrete Cosine Transform
- ADST Asymmetric Discrete Sine Transform
- Other block-based transforms are possible.
- combinations of different transforms may be applied to a single residual.
- the DCT transforms the residual block into the frequency domain where the transform coefficient values are based on spatial frequency.
- the lowest frequency (DC) coefficient is at the top-left of the matrix, and the highest frequency coefficient is at the bottom-right of the matrix. It is worth noting that the size of a prediction block, and hence the resulting residual block, may be different from the size of the transform block. For example, the prediction block may be split into smaller blocks to which separate transforms are applied.
- the quantization stage 406 converts the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients, using a quantizer value or a quantization level. For example, the transform coefficients may be divided by the quantizer value and truncated.
- the quantized transform coefficients are then entropy encoded by the entropy encoding stage 408 . Entropy coding may be performed using any number of techniques, including token and binary trees.
- the entropy-encoded coefficients, together with other information used to decode the block (which may include, for example, the type of prediction used, transform type, motion vectors, and quantizer value), are then output to the compressed bitstream 420 .
- the information to decode the block may be entropy coded into block, frame, slice, and/or section headers within the compressed bitstream 420 .
- the compressed bitstream 420 can also be referred to as an encoded video stream or encoded video bitstream; these terms will be used interchangeably herein.
- the reconstruction path in FIG. 4 can be used to ensure that both the encoder 400 and a decoder 500 (described below) use the same reference frames and blocks to decode the compressed bitstream 420 .
- the reconstruction path performs functions that are similar to functions that take place during the decoding process and that are discussed in more detail below, including dequantizing the quantized transform coefficients at the dequantization stage 410 and inverse transforming the dequantized transform coefficients at the inverse transform stage 412 to produce a derivative residual block (also called a derivative residual).
- the prediction block that was predicted at the intra/inter-prediction stage 402 can be added to the derivative residual to create a reconstructed block.
- the loop filtering stage 416 can be applied to the reconstructed block to reduce distortion, such as blocking artefacts.
- encoder 400 can be used to encode the compressed bitstream 420 .
- a non-transform based encoder 400 can quantize the residual signal directly without the transform stage 404 for certain blocks or frames.
- an encoder 400 can have the quantization stage 406 and the dequantization stage 410 combined into a single stage.
- FIG. 5 is a block diagram of a decoder 500 in accordance with implementations of this disclosure.
- the decoder 500 can be implemented in the receiving station 106 , for example, by providing a computer software program stored in the memory 204 .
- the computer software program can include machine instructions that, when executed by a processor, such as the CPU 202 , cause the receiving station 106 to decode video data in the manners described below.
- the decoder 500 can also be implemented in hardware included in, for example, the transmitting station 102 or the receiving station 106 .
- the decoder 500 similar to the reconstruction path of the encoder 400 discussed above, includes in one example the following stages to perform various functions to produce an output video stream 516 from the compressed bitstream 420 : an entropy decoding stage 502 , a dequantization stage 504 , an inverse transform stage 506 , an intra/inter-prediction stage 508 , a reconstruction stage 510 , a loop filtering stage 512 , and a post filtering stage 514 .
- Other structural variations of the decoder 500 can be used to decode the compressed bitstream 420 .
- the data elements within the compressed bitstream 420 can be decoded by the entropy decoding stage 502 to produce a set of quantized transform coefficients.
- the dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage 506 inverse transforms the dequantized transform coefficients using the selected transform type to produce a derivative residual that can be identical to that created by the inverse transform stage 412 in the encoder 400 .
- the decoder 500 can use the intra/inter-prediction stage 508 to create the same prediction block as was created in the encoder 400 , for example, at the intra/inter-prediction stage 402 .
- the prediction block can be added to the derivative residual to create a reconstructed block.
- the loop filtering stage 512 can be applied to the reconstructed block to reduce blocking artefacts. Other filtering can be applied to the reconstructed block.
- the post filtering stage 514 is applied to the reconstructed block to reduce blocking distortion, and the result is output as an output video stream 516 .
- the output video stream 516 can also be referred to as a decoded video stream; these terms will be used interchangeably herein.
- the decoder 500 can be used to decode the compressed bitstream 420 .
- the decoder 500 can produce the output video stream 516 without the post filtering stage 514 .
- the post filtering stage 514 is applied after the loop filtering stage 512 .
- the loop filtering stage 512 can include an optional deblocking filtering stage.
- the encoder 400 includes an optional deblocking filtering stage in the loop filtering stage 416 .
- FIG. 7 A is an example 700 of using a trained debanding model in a pre-processing scenario according to implementations of this disclosure.
- a DeBandingNet is used prior to transcoding a user-generated content (UGC).
- UGC user-generated content
- an image or a frame of a UGC can be input through the DeBandingNet to produce a debanded UGC, which is then transcoded or re-encoded.
- a UGC 702 may be received at a content delivery system.
- the content delivery system can receive UGC, transcode the UGC according to different parameters, and generate respective versions of the UGC.
- the UGC 702 can be output by an encoder, such as the encoder 400 of FIG. 4 .
- the UGC 702 can be the compressed bitstream 420 of FIG. 4 .
- a BandingNet i.e., a banding_net 704
- the banding_net 704 determines a banding index for a decoded version of the UGC 702 .
- the UGC 702 can be decoded prior to being input to the banding_net 704 .
- the UGC 702 can be decoded by a decoder, such as the decoder 500 of FIG. 5 .
- the input to the banding_net 704 can be the output video stream 516 of FIG. 5 .
- UGC 702 can be input to a de_banding_net 706 (i.e., a trained DeBandingNet) to deband the image.
- the threshold banding score can be empirically selected such that below which banding artefacts in the image are not perceived by a human viewer.
- a to_be_transcoded 708 i.e., a data item to be transcoded corresponding to the output of processing the UGC with the de_banding_net 706 ) can be obtained from the de_banding_net 706 .
- the banding score is not greater than the threshold banding score, then the UGC 702 is not input to the de_banding_net 706 .
- the to_be_transcoded 708 is the UGC 702 itself.
- the to_be_transcoded 708 can then be input to a transcoder 710 , which outputs one or more versions of the to_be_transcoded 708 , including a debanded and encoded bitstream 712 .
- the debanded and encoded bitstream 712 can be received at a device, such as the receiving station 106 of FIG. 1 or a user that can be as described with respect to the computing device 200 of FIG. 2 .
- the debanded and encoded bitstream 712 can be decoded at the device to obtain a decoded bitstream 714 , which can be as described with respect to the output video stream 516 of FIG. 5 .
- the output video stream 516 can then be displayed at a display 716 of the device.
- the debanded and encoded bitstream 712 can be delivered to the device using a streaming protocol, such as HTTP live streaming (HLS), or some other streaming protocol.
- HLS HTTP live streaming
- the UGC 702 can be input directly to the de_banding_net 706 . That is, in such an implementation, the example 700 may not include the banding_net 704 and the test 705 . To reiterate, one implementation of the example 700 can include a path illustrated by the dotted arrows and another implementation of the example 700 can include a path illustrated by the dashed line.
- FIG. 7 B is an example 750 of using trained debanding model in a post-processing scenario according to implementations of this disclosure.
- some first implementations of the example 750 can include a first path that is illustrated by a dashed line 766 ; and other second implementation of the example 750 can include a second path that is illustrated by dotted lines 768 , 770 .
- a DeBandingNet is used at a client device.
- both a BandingNet and a DeBandingNet are used at the client device.
- a de_banding_net 762 can be used, such as at the client device, to deband a received compressed bitstream (i.e., at least some of the frames therein).
- the compressed bitstream can be streamed to the device using a streaming protocol (e.g., HLS).
- the de_banding_net 762 can perform debanding on one or more frames of the received compressed bitstream.
- the de_banding_net 762 can be, or can be part of, a post filtering stage, such as the post filtering stage 514 of FIG. 5 .
- a UGC 752 can be received by a transcoder 754 .
- the UGC 752 can be as described with respect to the UGC 702 of FIG. 7 A .
- the transcoder 754 outputs one or more versions of the UGC 752 , including a first stream 756 .
- the first stream 756 can be received at the device, such as the receiving station 106 of FIG. 1 or a client device that can be as described with respect to the computing device 200 of FIG. 2 .
- the first stream 756 can be the compressed bitstream 420 of FIG. 5 .
- the first stream 756 is decoded to produce a decoded stream 758 .
- the de_banding_net 762 can generate a debanded frame for a frame of the decoded stream 758 .
- the debanded frames can then be displayed at a display 764 of the device, such as the display 218 of FIG. 2
- a banding_net 760 (i.e., a trained BandingNet) can determine respective banding scores for at least some of the decoded frames of the decoded stream 758 . While not specifically shown in FIG. 7 B , a test, such as the test 705 of FIG. 7 A , can determine whether the banding score of a decoded frame exceeds a banding threshold. If so, the decoded frame can be input to the de_banding_net 762 to produce the debanded frame to be displayed at the display 764 ; if not, then the frame is not input to the de_banding_net 762 .
- FIG. 8 is a flowchart of an example of a technique 800 for training a BandingNet according to implementations of this disclosure.
- the BandingNet can be a convolutional neural network, as described with respect to FIG. 15 .
- the BandingNet can be a RESidual neural NETwork (ResNet).
- the BandingNet i.e., a first model
- the parameters e.g., the weights of the second model
- a general technique for training and using a neural network, such as the BandingNet is also described with respect to FIG. 16 .
- the technique 800 can be implemented, for example, as a software program that may be executed by computing devices such as the computing device 200 of FIG. 2 .
- the software program can include machine-readable instructions that may be stored in a memory such as the memory 204 or the secondary storage 214 , and that, when executed by a processor, such as CPU 202 , may cause the computing device to perform the technique 800 .
- the technique 800 can be implemented using specialized hardware or firmware. Multiple processors, memories, or both, may be used.
- the technique 800 selects a first set of first training images to train the first model (i.e., the BandingNet).
- the first training images can be such that the distribution of the ground truth banding scores of the training images of the first training images are balanced.
- the CelebFaces Attributes Dataset (CelebA), or a subset thereof, can be used.
- CelebA is a large-scale face attributes dataset with more than 200,000 celebrity images.
- other first training images can be used.
- images of the dataset can be rescaled to the same size (e.g., 256 ⁇ 256 pixels) with the same aspect ratio, and padded with zeros for the missing pixels.
- the first set of first training images can be obtained from the rescaled CelebA images by compressing the images using a lossy compression technique, e.g., Graphic Interchange Format (GIF), etc., with palette sizes of 16, 64, 128, and 256 colors.
- GIF Graphic Interchange Format
- the ground truth banding scores for each training image of the first set of first training images can be computed using a baseline (e.g., traditional) technique.
- the banding scores can be calculated in increments (e.g., intervals) of 0.01.
- FIG. 9 illustrates banding score distributions of the training images of the first set of first training images.
- the X-axis of the graphs of FIG. 9 represents the raw banding Degradation Mean Opinion Score (DMOS), and the Y-axis represents the number of images.
- a graph 902 of FIG. 9 shows the entire banding score distribution for the first set of first training images.
- a subset (e.g., 400 ) of the lossy compressed images can be used as the first set of first training images.
- a graph 904 shows the banding score distribution of the subset after random sampling that is capped at 5000 for all 0.01 intervals.
- the technique 800 For a first training image of the first set of the first training images, the technique 800 generates a banding edge map.
- a respective banding edge map can be generated for each training image of the first set of the first training images. Banding artefacts tend to appear on the boundary between two smooth regions.
- the banding edge map extracts such potential banding edges and forms an input map for use in the training the BandingNet.
- the banding edge map includes weights that emphasize banding edges and de-emphasize true edges in the first training image.
- Generating the banding edge map can include, as further described with respect to Algorithm I below, computing (e.g., calculating) a gradient map for the luminance plane of the first training image; computing (e.g., calculating) a weight map for the first training image; and obtaining the banding edge map by multiplying the gradient map by the weight map.
- the banding edge map can be generated using an algorithm such as Algorithm I.
- Algorithm I may receive a RGB image. If so, then at line 1, Algorithm I converts the RGB data to the YUV color space, which includes a luminance (Y) channel and two chrominance (U and V) channels.
- Algorithm I calculates the horizontal G x and the vertical G y gradient components at each pixel location of the luminance channel (i.e., luminance image, luminance component), denoted I.
- the gradient measures the amount of change in pixel values at a pixel location.
- Gradient information can be used to identify the boundaries between areas of the image, which are characterized by significant color changes.
- the horizontal G x and the vertical G y gradient components can be calculated using formulae (1a) and (2a), respectively. However, other ways of calculating the gradients are possible.
- G y ( x, y ) I(x, y+1) ⁇ I(x, y ⁇ 1) /2 (1a)
- G x ( x, y ) I(x+1, y) ⁇ I(x ⁇ 1, y) /2 (2a)
- G y (x, y) is the y gradient value for the current pixel having a coordinate of (x, y), and I(x, y) represents the luminance pixel value at a coordinate of (x, y).
- I(x, y) represents the luminance pixel value at a coordinate of (x, y).
- the luminance pixel values for the pixel above and the pixel below the current pixel are used.
- G x (x, y) is the x gradient value for the current pixel having a coordinate of (x, y), and I(x, y) represents the luminance pixel value at a coordinate of (x, y).
- I(x, y) represents the luminance pixel value at a coordinate of (x, y).
- formulae (1b) and (2a) can be used to calculate the horizontal G x and the vertical G y gradient components.
- G y ( x, y ) I ( x, y +1) ⁇ I( x, y ) (2a)
- G x ( x, y ) I ( x +1 , y ) ⁇ I ( x, y ) (2b)
- Algorithm I calculates the Gradient map, G.
- the gradient map G includes a gradient value for each pixel of the luminance image.
- the Sobel operator can be used to obtain the gradient map.
- the Sobel operator performs a 2-D spatial gradient measurement on the image.
- the Sobel operator can emphasize regions of high spatial frequency corresponding to edges with the corresponding directions in the image block.
- Algorithm I calculates a weight map, W.
- the weight map W includes a weight value for each pixel of the luminance image.
- the weight map includes respective weights for pixels of the first training image.
- the function ReLu(a) is the rectified linear unit that returns a if a ⁇ 0; otherwise, it returns 0.
- ReLu(1 ⁇ G(x, y)) is 0 when G(x, y) ⁇ 1; and (1 ⁇ G(x, y)) when G(x, y) ⁇ 1.
- the purpose of the Re Lu operation is to remove (e.g., attempt to remove) high contrast pixels (i.e., true edges) so that the remaining edges are those that are mainly caused by compression. That is, the remaining edges are edges that are likely to be artefacts (e.g., banding artefacts).
- the weight map, W is computed using the gradient map, G.
- the weight map can be obtained by generating a map (e.g., (1 ⁇ ReLu(1 ⁇ G)) using the gradient map, where the map excludes high contrast pixels; and convolving, to obtain the weight map, the map with a kernel (e.g., ones(N ⁇ N)) having a predefined size (e.g., N ⁇ N).
- the predefined size can be 7 ⁇ 7.
- the kernel ones(N ⁇ N) is a 2-dimensional matrix of size N ⁇ N where each value of the matrix is 1.
- the gradient map can include gradient values that are either zero or positive values.
- the gradient value for a pixel is zero if the pixel is, for example, within a uniform region. For each such pixel (i.e., in a uniform region) at a location (x, y), (1 ⁇ ReLu(1 ⁇ G(x, y))) is 0.
- the gradient value for a pixel is not zero (i.e., G(x, y)>0) if the pixel is at or very close to an edge. For each such pixel at a location (x, y), (1 ⁇ ReLu(1 ⁇ G(x, y))) is 1.
- the banding edge map is calculated as the product of the weight map W with the gradient map G.
- the technique 800 uses the banding edge Map, E, and the luminance plane Y of the first training image as input (i.e., [E, Y]) to the first model to train the first model.
- the first model is trained to output a banding score.
- the difference between the ground truth banding score and the banding score output during training can be used as a loss function, or as part of a loss function, that is used to optimize the parameter values of the BandingNet.
- FIG. 10 is an example of a luminance image and a banding edge map according to implementations of this disclosure.
- a luminance image 1002 can be the first training image.
- Algorithm I generates a banding edge map that is visually depicted using banding edge map 1004 .
- banding edge map 1004 potential banding edges (e.g., banding on the background) are highlighted in the banding edge map E with greater weights than true edges (such as eyes and hairs), which are set with relatively low weights in the banding edge map E.
- the BandingNet (i.e., the first model) may be trained using images of size N ⁇ N (e.g., 256 ⁇ 256). After training, the BandingNet may be used to output a banding score for images that are bigger or smaller than N ⁇ N. In the case of an image that is smaller than N ⁇ N, the image can be padded, using any padding technique, so that an image of size N ⁇ N is input to the BandingNet. More accurately, and as mentioned above, the luminance channel of the image and a banding edge map calculated according to Algorithm I are input to the BandingNet.
- N ⁇ N e.g., 256 ⁇ 256
- the image can be split into N ⁇ N patches (e.g., 16 patches), and a respective banding score can be output for each patch.
- the respective banding scores of the patches can be combined to obtain a banding score for the image. Any number of ways of combining the respective banding scores are possible.
- the maximum of the respective banding scores can be taken as the banding score of the image.
- the mean of the respective banding scores can be used.
- a percentile (e.g., 95% percentile) of the respective banding scores can be used.
- the BandingNet (e.g., the first model) can be used in the training of a DeBandingNet (e.g., a second model).
- the banding score from the BandingNet can be used as part of a loss function in the training of the DeBandingNet.
- the second model receives a banded image (i.e., an image that may include banding artefacts) and outputs a debanded image (i.e., an image from which the banding artefacts are removed, or at least significantly reduced).
- the second model (i.e., the DeBandingNet) can be trained by steps that include selecting a second set of training images (which may be the same as the first set of the training images); for a second training image of the second training images, generating a training debanded image; generating, using the first model, a training banding score for the training debanded image; generating an image difference between the second training image and the debanded image; and using a weighted combination of the image difference and the training banding score in a loss function that is used to train the second model.
- a second set of training images which may be the same as the first set of the training images
- generating a training debanded image generating, using the first model, a training banding score for the training debanded image
- generating an image difference between the second training image and the debanded image and using a weighted combination of the image difference and the training banding score in a loss function that is used to train the second model.
- the loss function can be a sum of a second norm (i.e., ⁇ image diff ⁇ ) of the image difference plus a weight (i.e., w) multiplied the training banding score (i.e., ⁇ banding score ⁇ ).
- the absolute value of the banding score i.e.,
- FIGS. 11 A- 11 B are comparative examples of training the BandingNet (i.e., the first model) without and with using a banding edge map.
- FIG. 11 A illustrates results 1100 of training a convolution neural network using RGB CelebA images as training data.
- Mean Square Error (MSE) between a predicted banding score (i.e., the output of the convolution neural network) and a ground truth banding score is used as the loss variable in the optimization function.
- FIG. 11 A illustrates that the training is unstable. The training in this case took a long time to converge (i.e., 20,000 epochs with a batch size of 200).
- a graph 1102 illustrates the mean absolute difference (MAD) between the ground truth banding values and the prediction banding values at different steps of the training.
- the MAD is approximately 13.4 with a range of [0, 100].
- the graph 1102 includes a first line 1104 that plots the MAD during training and a second line 1106 that plots the MAD during evaluation (e.g., testing).
- a graph 1108 illustrates the Pearson correlation for the training data (i.e., a line 1110 ) and the evaluation data (i.e., a line 1112 ).
- the Pearson correlation is 0.72.
- the Pearson correlation measures the correlation between the ground truth banding score (i.e., as calculated by an undifferentiable banding metric) and the predicted banding score (i.e., the banding score output by the BandingNet).
- FIG. 11 B illustrates results 1150 of training the BandingNet using the banding edge map described herein.
- a graph 1152 illustrates the mean absolute difference (MAD) between the ground truth banding values and the prediction banding values at different steps of the training.
- the graph 1152 includes a first line 1154 that plots the MAD during training and a second line 1156 that plots the MAD during evaluation (e.g., testing).
- a graph 1158 illustrates the Pearson correlation for the training data (i.e., a line 1160 ) and the evaluation data (i.e., a line 1162 ).
- the Person correlation is 0.83.
- FIG. 12 is a flowchart of an example of a technique 1200 for training a DeBandingNet (i.e., a second model) according to implementations of this disclosure.
- the DeBandingNet receives a banded image as input and outputs a debanded image.
- the DeBandingNet can be a convolutional neural network, as described with respect to FIG. 15 .
- the parameters e.g., the weights of the second model
- a general technique for training and using a neural network, such as the DeBandingNet, is also described with respect to FIG. 16 .
- the technique 1200 selects a set of training images (i.e., a second set of second training images). Selecting the set of training images can mean using, accessing, receiving, or selecting in any way possible.
- the technique 1200 generates a training debanded image for a second training image of the second training images. That is, the second model receives, as input, the second training image and outputs the training debanded image. Respective training debanded images can be output for the training images.
- the technique 1200 generates (e.g., calculates, obtains, etc.) a banding score for the training debanded image.
- a banding score can be computed using the techniques described in Y. Wang, S. Kum, C. Chen and A. Kokaram, “A perceptual visibility metric for banding artifacts,” 2016 IEEE International Conference on Image Processing (ICIP), Phoenix, Arizona, 2016, pp. 2067-2071.
- Respective banding scores can be generated for the training debanded images.
- the technique 1200 generates (e.g., calculates, obtains, etc.) an image difference between the second training image and the training debanded image.
- the image difference can be calculated in any number of ways.
- the image difference can be the mean square error between respective (i.e., co-located) pixel values of between the second training image and the training debanded image.
- the image difference can be a sum of absolute differences error between respective pixel values of the second training image and the training debanded image.
- the image difference can be the second (i.e., Euclidean) norm difference.
- Respective image difference can be generated between the second training images and the corresponding training debanded images.
- the technique 1200 uses a weighted combination of the image difference and the banding score in a loss function that is used to train the second model. Respective weighted combinations can be obtained for the input images.
- generating the banding score for the training debanded image can include using a first model to generate the banding score.
- the first model can be the BandingNet described above.
- the first model can be trained to output the banding score of the training debanded image as described with respect to FIG. 8 .
- training the first model can include, selecting a first set of first training images to train the first model; generating a banding edge map for a first training image of the first set of the first training images; and using the banding edge map and a luminance plane of the first training image as input to the first model.
- Generating the banding edge map can include, as described above with respect to FIG. 8 , computing a gradient map for a luminance channel of the first training image; computing a weight map for the first training image; and obtaining the banding edge map by multiplying the gradient map by the weight map.
- the weight map can include respective weights for pixels of the first training image.
- the weight map can be computed using the gradient map.
- Computing the weight map for the first training image can include, as described above with respect to FIG. 8 , generating a map using the gradient map that excludes high contrast pixels; and convolving, to obtain the weight map, the map with a kernel having a predefined size.
- the predefined size can be equal to 7 ⁇ 7.
- the kernel can consist of 1 values.
- FIG. 13 is a flowchart of an example of a technique 1300 for training a DeBandingNet (i.e., a second model) using a BandingNet (i.e., a first model) according to implementations of this disclosure.
- the DeBandingNet receives a banded image (e.g., an image that may include banding artefacts) as input and outputs a debanded image.
- a banded image e.g., an image that may include banding artefacts
- the technique 1300 trains the first model.
- Training the first model can be as described with respect to FIG. 8 .
- training the first model can include selecting a first set of first training images to train the first model; for a first training image of the first set of the first training images, generating a banding edge map; and using the banding edge map and a luminance plane of the first training image as input to the first model.
- the banding edge map includes weights that emphasize banding edges and de-emphasize true edges in the first training image.
- generating the banding edge map can include computing a gradient map for a luminance channel of the first training image; computing a weight map for the first training image; and obtaining the banding edge map by multiplying the gradient map by the weight map.
- the weight map can include respective weights for pixels of the first training image.
- the weight map can be computed using the gradient map.
- Computing the weight map for the first training image can include generating a map using the gradient map, where the map excludes high contrast pixels; and convolving, to obtain the weight map, the map with a kernel having a predefined size.
- the predefined size can be 7 ⁇ 7.
- the kernel can consist of 1 values.
- the technique 1300 trains the second model to deband the image.
- Training the second model can be as described with respect to FIG. 12 .
- training the second model can include selecting a second set of second training images to train the second model; for a second training image of the second set of the second training images, generating a debanded training image; generating, using the first model, a banding score for the debanded training image; and using the banding score in a loss function that is used in training the second model.
- training the second model to deband the image can include obtaining an image difference between the debanded training image and the second training image, and using the image difference in the loss function that is used in training the second model.
- the loss function can be a weighted sum of the image difference and the banding score.
- the technique 1300 After the second model is trained, the technique 1300 generates a debanded image for the image using the second model. That is, the image can be input to the second model and the debanded image is output by the second model.
- FIG. 14 is a diagram 1400 that illustrates at least some aspects of the technique of FIG. 13 .
- the diagram 1400 illustrates training the DeBandingNet using the BandingNet.
- the diagram 1400 includes a DeBandingNet 1404 , which is to be trained, and a BandingNet 1408 , which is already trained to output a banding score for an input image.
- a training input image 1402 is input to the DeBandingNet 1404 .
- the DeBandingNet 1404 outputs a training debanded image 1406 .
- the luminance plane of the training debanded image 1406 A and a banding edge map that is calculated as described with respect to Algorithm I are input to the BandingNet 1408 , which outputs a banding score 1410 for the training debanded image 1406 .
- An image difference 1414 is also obtained, using an operator 1412 , as the difference between the training input image 1402 and the training debanded image 1406 .
- a total loss value 1416 which combines the banding score 1410 and the image difference 1414 can be used to refine the parameters of the DeBandingNet 1404 during the training.
- FIG. 15 is a block diagram of an example 1500 of a typical convolutional neural network (CNN).
- the example 1500 illustrates a high-level block diagram of an example 1500 of a typical CNN network, or simply a CNN.
- a CNN is an example of a machine-learning model.
- a feature extraction portion typically includes a set of convolutional operations, which is typically a series of filters that are used to filter an input image based on a filter (typically a square of size k, without loss of generality). For example, and in the context of machine vision, these filters can be used to find features in an input image.
- the features can include, for example, edges, corners, endpoints, and so on. As the number of stacked convolutional operations increases, later convolutional operations can find higher-level features.
- a classification portion is typically a set of fully connected (FC) layers, which may also be referred to as dense operations.
- FC fully connected
- the fully connected layers can be thought of as looking at all the input features of an image in order to generate a high-level classifier.
- stages e.g., a series
- the classification output can be one or more values that can indicate a banding score of an input image.
- a typical CNN network is composed of a number of convolutional operations (e.g., the feature-extraction portion) followed by a number of fully connected layers.
- the number of operations of each type and their respective sizes is typically determined during the training phase of the machine learning.
- additional layers and/or operations can be included in each portion. For example, combinations of Pooling, MaxPooling, Dropout, Activation, Normalization, BatchNormalization, and other operations can be grouped with convolution operations (i.e., in the features-extraction portion) and/or the fully connected operation (i.e., in the classification portion).
- the fully connected layers may be referred to as Dense operations.
- a convolution operation can use a SeparableConvolution2D or Convolution2D operation.
- the CNN can include skip connections or shortcuts to jump over some layers from one layer to another.
- a convolution layer can be a group of operations starting with a Convolution2D or SeparableConvolution2D operation followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof), until another convolutional layer, a Dense operation, or the output of the CNN is reached.
- operations e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof
- a Dense layer can be a group of operations or layers starting with a Dense operation (i.e., a fully connected layer) followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof) until another convolution layer, another Dense layer, or the output of the network is reached.
- a Dense operation i.e., a fully connected layer
- zero or more operations e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof
- the boundary between feature extraction based on convolutional networks and a feature classification using Dense operations can be marked by a Flatten operation, which flattens the multidimensional matrix from the feature extraction into a vector.
- each of the convolution layers may consist of a set of filters. While a filter is applied to a subset of the input data at a time, the filter is applied across the full input, such as by sweeping over the input image.
- the operations performed by this layer are typically linear/matrix multiplications.
- the output of the convolution filter may be further filtered using an activation function.
- the activation function may be a linear function or non-linear function (e.g., a sigmoid function, an arcTan function, a tanH function, a Rectified Linear unit (ReLu) function, or the like).
- Each of the fully connected operations is a linear operation in which every input is connected to every output by a weight.
- a fully connected layer with N number of inputs and M outputs can have a total of N ⁇ M weights.
- a Dense operation may be generally followed by a non-linear activation function to generate an output of that layer.
- Some CNN network architectures used to perform analysis of frames and superblocks may include several feature extraction portions that extract features at different granularities (e.g., at different sub-block sizes of a superblock) and a flattening layer (which may be referred to as a concatenation layer) that receives the output(s) of the last convolution layer of each of the extraction portions.
- the flattening layer aggregates all the features extracted by the different feature extraction portions into one input set.
- the output of the flattening layer may be fed into (i.e., used as input to) the fully connected layers of the classification portion.
- the number of parameters of the entire network may be dominated (e.g., defined, set, etc.) by the number of parameters at the interface between the feature extraction portion (i.e., the convolution layers) and the classification portion (i.e., the fully connected layers). That is, the number of parameters of the network is dominated by the parameters of the flattening layer.
- FIG. 16 is an example of a technique for training and using a machine-learning model according to implementations of this disclosure.
- the machine-learning (ML) model can be a CNN, such as the CNN described with respect to FIG. 15 .
- the process or technique 1600 trains, using input data, the ML model to perform a task.
- the task to be performed is that of generating a debanded image from an input image that may include banding artefacts.
- the ML model is the BandingNet
- the task to be performed is that of generating a banding score for an input image. Outputting a banding score can mean outputting a class for the input image, which can be mapped to banding score.
- the technique 1600 then uses the trained machine-learning model to perform the task.
- the technique 1600 trains the ML model.
- the ML model can be trained using training data 1612 .
- a ground truth is compared to the output of the ML model and the difference between the ground truth and the output can be used to refine the parameters of the ML-model, such as through back propagation.
- an input datum includes a training image and the ground truth banding score, which may be obtained using a traditional (e.g., a brute force) method.
- the traditional method can be an algorithmic method.
- the traditional method can be human evaluation. That is, one or more human users can be presented with each input image and the one or more human users can assign banding scores to each input image.
- the banding scores from multiple users can be averaged to obtain the banding score for the image.
- the brute force banding score is then compared to the banding score of the BandingNet.
- the DeBandingNet there is no ground truth associated with a training banded image, per se. Rather, the combination of the banding score, as generated by the BandingNet and the image loss can be used as the loss to be used to refine the parameters of the DeBandingNet.
- parameters of the ML model are generated such that, for at least some of the training data, the ML model can infer, for a training datum, a corresponding output.
- the ML model can then be used by the technique 1600 during an inference (e.g., operation, etc.) phase.
- the inference phase includes the operations 1604 and 1606 .
- a separation 1610 indicates that the training phase and the inference phase can be separated in time.
- the inferencing phase can be performed at one or more first computing devices and the training data 1612 can be performed at one or more second computing devices, which can be different from the one or more first computing devices.
- inputs are presented to the ML module.
- the inputs can be presented to a module that incorporates, includes, executes, implements, and the like the ML model.
- the ML module can be a hardware-implemented module.
- the ML module can be stored in a memory as executable instructions, which can be executed by a processor.
- the input can be an image to be debanded, as described with respect to one of FIGS. 7 A- 7 B .
- the image can be as described with respect to FIGS. 7 A- 7 B ; and in a second example, the input can be a debanded image, such as the training debanded image 1406 of FIG. 14 .
- the technique 1600 obtains an output from the ML model.
- the output can be a banding score.
- the output can be a debanded image.
- FIGS. 8 , 12 , 13 , and 16 are depicted and described as series of steps or operations. However, the steps or operations in accordance with this disclosure can occur in various orders and/or concurrently. Additionally, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a method in accordance with the disclosed subject matter.
- example or “exemplary” are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” or “exemplary” is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the words “example” or “exemplary” is intended to present concepts in a concrete fashion.
- the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise or clear from context, “X includes A or B” is intended to mean any of the natural inclusive permutations thereof.
- the terms “determine” and “identify,” or any variations thereof, include selecting, ascertaining, computing, looking up, receiving, determining, establishing, obtaining, or otherwise identifying or determining in any manner whatsoever using one or more of the devices shown in FIG. 1 or FIG. 2 .
- the implementations of the transmitting station 102 , the receiving station 106 (and the algorithms, techniques, instructions, etc., stored thereon and/or executed thereby), the BandingNet, and/or the DeBandingNet can be realized in hardware, software, or any combination thereof.
- the hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit.
- IP intellectual property
- ASICs application-specific integrated circuits
- programmable logic arrays optical processors
- programmable logic controllers programmable logic controllers
- microcode microcontrollers
- servers microprocessors, digital signal processors, or any other suitable circuit.
- signal processors should be understood as encompassing any of the foregoing hardware, either singly or in combination.
- signals and “data” are used interchangeably. Further, portions of transmitting station
- transmitting station 102 and the receiving station 106 can be implemented using a computer program that, when executed, carries out at least some of the respective techniques, algorithms, and/or instructions described herein.
- a special-purpose computer/processor which can contain specialized hardware for carrying out any of the techniques, algorithms, or instructions described herein, can be utilized.
- the transmitting station 102 and the receiving station 106 can, for example, be implemented on computers in a real-time video system.
- the transmitting station 102 can be implemented on a server, and the receiving station 106 can be implemented on a device separate from the server, such as a hand-held communications device.
- the transmitting station 102 can encode content using an encoder 400 into an encoded video signal and transmit the encoded video signal to the communications device.
- the communications device can then decode the encoded video signal using a decoder 500 .
- the communications device can decode content stored locally on the communications device, for example, content that was not transmitted by the transmitting station 102 .
- Other suitable transmitting station 102 and a receiving station 106 implementation schemes are available.
- the receiving station 106 can be a generally stationary personal computer rather than a portable communications device, and/or a device including an encoder 400 may also include a decoder 500 .
- implementations can take the form of a computer program product accessible from, for example, a tangible computer-usable or computer-readable medium.
- a computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor.
- the medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.
Abstract
Description
Algorithm I |
1 | Convert red-green-blue (RGB) data to luma-chroma (YUV) |
||
2 | Compute Gradient Gx and Gy for the |
||
3 | radient map G = {square root over (Gx 2 + Gy 3)} | ||
4 | Calculate Weight map W = ( (1 − ReLu(1 − G)) * ones(N × N))2 | ||
5 | Calculate Banding Edge Map E = W · G | ||
G y(x, y)=I(x, y+1)−I(x, y−1)/2 (1a)
G x(x, y)=I(x+1, y)−I(x−1, y)/2 (2a)
G y(x, y)=I(x, y+1)−I(x, y) (2a)
G x(x, y)=I(x+1, y)−I(x, y) (2b)
Claims (20)
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/033545 WO2021236061A1 (en) | 2020-05-19 | 2020-05-19 | Debanding using a novel banding metric |
Publications (2)
Publication Number | Publication Date |
---|---|
US20230131228A1 US20230131228A1 (en) | 2023-04-27 |
US11854165B2 true US11854165B2 (en) | 2023-12-26 |
Family
ID=71069993
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/922,531 Active 2040-08-08 US11854165B2 (en) | 2020-05-19 | 2020-05-19 | Debanding using a novel banding metric |
Country Status (3)
Country | Link |
---|---|
US (1) | US11854165B2 (en) |
EP (1) | EP3938998A1 (en) |
WO (1) | WO2021236061A1 (en) |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110116726A1 (en) * | 2008-09-24 | 2011-05-19 | Sony Corporation | Electronic apparatus, blur image sorting method, and program |
US20140146881A1 (en) * | 2012-11-23 | 2014-05-29 | Korea Advanced Institute Of Science And Technology | Method and apparatus for estimating a quantization table for a video image |
US20160098822A1 (en) * | 2014-10-07 | 2016-04-07 | Stmicroelectronics (Grenoble 2) Sas | Detection and correction of artefacts in images or video |
US20170053176A1 (en) * | 2015-08-20 | 2017-02-23 | Novatek Microelectronics Corp. | Image processing apparatus and image processing method |
US20170347126A1 (en) | 2016-05-27 | 2017-11-30 | Qualcomm Incorporated | Video debanding using adaptive filter sizes and gradient based banding detection |
JP2018045401A (en) | 2016-09-14 | 2018-03-22 | 株式会社朋栄 | Color debanding processing and optimum parameter learning method thereof |
US20190285713A1 (en) * | 2018-03-13 | 2019-09-19 | Siemens Healthcare Gmbh | Noise suppression for wave-caipi |
US20220038749A1 (en) * | 2019-10-16 | 2022-02-03 | Tencent Technology (Shenzhen) Company Limited | Artifact removal method and apparatus based on machine learning, and method and apparatus for training artifact removal model based on machine learning |
US11259040B1 (en) * | 2019-04-25 | 2022-02-22 | Amazon Technologies, Inc. | Adaptive multi-pass risk-based video encoding |
US20220148146A1 (en) * | 2020-11-11 | 2022-05-12 | Dish Network L.L.C. | Systems and methods for compression artifact detection and remediation |
-
2020
- 2020-05-19 US US17/922,531 patent/US11854165B2/en active Active
- 2020-05-19 EP EP20731686.0A patent/EP3938998A1/en active Pending
- 2020-05-19 WO PCT/US2020/033545 patent/WO2021236061A1/en unknown
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110116726A1 (en) * | 2008-09-24 | 2011-05-19 | Sony Corporation | Electronic apparatus, blur image sorting method, and program |
US20140146881A1 (en) * | 2012-11-23 | 2014-05-29 | Korea Advanced Institute Of Science And Technology | Method and apparatus for estimating a quantization table for a video image |
US20160098822A1 (en) * | 2014-10-07 | 2016-04-07 | Stmicroelectronics (Grenoble 2) Sas | Detection and correction of artefacts in images or video |
US20170053176A1 (en) * | 2015-08-20 | 2017-02-23 | Novatek Microelectronics Corp. | Image processing apparatus and image processing method |
US20170347126A1 (en) | 2016-05-27 | 2017-11-30 | Qualcomm Incorporated | Video debanding using adaptive filter sizes and gradient based banding detection |
JP2018045401A (en) | 2016-09-14 | 2018-03-22 | 株式会社朋栄 | Color debanding processing and optimum parameter learning method thereof |
US20190285713A1 (en) * | 2018-03-13 | 2019-09-19 | Siemens Healthcare Gmbh | Noise suppression for wave-caipi |
US11259040B1 (en) * | 2019-04-25 | 2022-02-22 | Amazon Technologies, Inc. | Adaptive multi-pass risk-based video encoding |
US20220038749A1 (en) * | 2019-10-16 | 2022-02-03 | Tencent Technology (Shenzhen) Company Limited | Artifact removal method and apparatus based on machine learning, and method and apparatus for training artifact removal model based on machine learning |
US20220148146A1 (en) * | 2020-11-11 | 2022-05-12 | Dish Network L.L.C. | Systems and methods for compression artifact detection and remediation |
US11508053B2 (en) * | 2020-11-11 | 2022-11-22 | Dish Network L.L.C. | Systems and methods for compression artifact detection and remediation |
Non-Patent Citations (9)
Title |
---|
Changmeng Peng et al., "CNN-based bit-depth enhancement by the suppression of false contour and color distortion", Proceedings of APSIPA Annual Summit and Conference, Nov. 18-21, 2019, Lanzhou, China, pp. 1145-1151. |
Gary Baugh et al., "Advanced Video Debanding", European Conference on Visual Media & Production (CVMP), London, UK, Nov. 13-14, 2014, 10 pgs. |
International Search Report and Written Opinion of International Application No. PCT/US2020/033545, dated Feb. 16, 2020, 2020, 17 pgs. |
Ji Won Lee et al., "Two-Stage False Contour Detection Using Directional Contrast Features and Its Application to Adaptive False Contour Reduction", IEEE Transactions on Consumer Electronics, vol. 52, No. 1, Feb. 2006, pp. 179-188. |
Wang et al., "GIF2Video: Color Dequantization and Temporal Interpolation of GIF images", 2019, 10 pgs. |
Y. Wang, S.-U. Kum, C. Chen and A. Kokaram, "A perceptual visibility metric for banding artifacts," 2016 IEEE International Conference on Image Processing (ICIP), Phoenix, AZ, USA, 2016, pp. 2067-2071, doi: 10.1109/ICIP.2016.7532722. (Year: 2016). * |
Yilin Wang et al., "A perceptual visibility metric for banding artifacts", IEEE International Conference on Image Processing (ICIP) 2016, 5 pgs. |
Zhengzhong Tu et al., "BBAND Index: A No-Reference Banding Artifact PREDICTOR",2020 IEEE International Conferen ce on Acoustics, Speech and Signal Processing (ICASSP), May 4, 2020, pp. 2712-2716. |
Zhou, Raymond, et al. "Deep Image Debanding." 2022 IEEE International Conference on Image Processing (ICIP). IEEE, 2022. (Year: 2022). * |
Also Published As
Publication number | Publication date |
---|---|
US20230131228A1 (en) | 2023-04-27 |
EP3938998A1 (en) | 2022-01-19 |
WO2021236061A1 (en) | 2021-11-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11310501B2 (en) | Efficient use of quantization parameters in machine-learning models for video coding | |
US20210051322A1 (en) | Receptive-field-conforming convolutional models for video coding | |
US10805629B2 (en) | Video compression through motion warping using learning-based motion segmentation | |
US11689726B2 (en) | Hybrid motion-compensated neural network with side-information based video coding | |
US11025907B2 (en) | Receptive-field-conforming convolution models for video coding | |
US10848765B2 (en) | Rate/distortion/RDcost modeling with machine learning | |
US20220207654A1 (en) | Guided restoration of video data using neural networks | |
US11039131B2 (en) | Intra-prediction for smooth blocks in image/video | |
US9185414B1 (en) | Video encoding using variance | |
EP3571841B1 (en) | Dc coefficient sign coding scheme | |
US10277897B1 (en) | Signaling in-loop restoration filters for video coding | |
EP3743855A1 (en) | Receptive-field-conforming convolution models for video coding | |
US20220415039A1 (en) | Systems and Techniques for Retraining Models for Video Quality Assessment and for Transcoding Using the Retrained Models | |
US20220078446A1 (en) | Video stream adaptive filtering for bitrate reduction | |
US11297353B2 (en) | No-reference banding artefact predictor | |
US11854165B2 (en) | Debanding using a novel banding metric | |
CN114222127A (en) | Video coding method, video decoding method and device | |
CN111886868B (en) | Method and apparatus for adaptive temporal filtering for substitute reference frame rendering | |
Jung | Comparison of video quality assessment methods | |
US20190058880A1 (en) | Compressing groups of video frames using reversed ordering | |
TWI834087B (en) | Method and apparatus for reconstruct image from bitstreams and encoding image into bitstreams, and computer program product |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:WANG, YILIN;ADSUMILLI, BALINEEDU;YANG, FENG;REEL/FRAME:061916/0659Effective date: 20200505 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP, ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |