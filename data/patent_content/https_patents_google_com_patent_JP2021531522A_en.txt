JP2021531522A - Determination of vehicle occupant involvement using 3D line-of-sight vector - Google Patents
Determination of vehicle occupant involvement using 3D line-of-sight vector Download PDFInfo
- Publication number
- JP2021531522A JP2021531522A JP2020529676A JP2020529676A JP2021531522A JP 2021531522 A JP2021531522 A JP 2021531522A JP 2020529676 A JP2020529676 A JP 2020529676A JP 2020529676 A JP2020529676 A JP 2020529676A JP 2021531522 A JP2021531522 A JP 2021531522A
- Authority
- JP
- Japan
- Prior art keywords
- vehicle
- occupant
- line
- sight vector
- interest
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 239000013598 vector Substances 0.000 title claims abstract description 126
- 210000001508 eye Anatomy 0.000 claims abstract description 128
- 238000000034 method Methods 0.000 claims abstract description 92
- 230000009471 action Effects 0.000 claims abstract description 17
- 230000001815 facial effect Effects 0.000 claims description 42
- 238000003860 storage Methods 0.000 claims description 30
- 230000005484 gravity Effects 0.000 claims description 12
- 210000001747 pupil Anatomy 0.000 claims description 4
- 230000004044 response Effects 0.000 claims description 4
- 238000012549 training Methods 0.000 description 48
- 210000003128 head Anatomy 0.000 description 38
- 230000006870 function Effects 0.000 description 30
- 238000012545 processing Methods 0.000 description 26
- 239000010410 layer Substances 0.000 description 24
- 238000013528 artificial neural network Methods 0.000 description 22
- 238000010586 diagram Methods 0.000 description 21
- 238000010801 machine learning Methods 0.000 description 20
- 238000004891 communication Methods 0.000 description 15
- 238000001514 detection method Methods 0.000 description 14
- 238000005457 optimization Methods 0.000 description 12
- 230000008569 process Effects 0.000 description 11
- 230000000306 recurrent effect Effects 0.000 description 11
- 238000004458 analytical method Methods 0.000 description 10
- 238000005516 engineering process Methods 0.000 description 10
- 238000003066 decision tree Methods 0.000 description 7
- 230000033001 locomotion Effects 0.000 description 7
- 238000000513 principal component analysis Methods 0.000 description 7
- 238000012417 linear regression Methods 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 6
- 238000004806 packaging method and process Methods 0.000 description 6
- 230000002787 reinforcement Effects 0.000 description 6
- 238000003339 best practice Methods 0.000 description 5
- 210000001331 nose Anatomy 0.000 description 5
- 238000007637 random forest analysis Methods 0.000 description 5
- 230000009467 reduction Effects 0.000 description 5
- 230000008901 benefit Effects 0.000 description 4
- 238000013527 convolutional neural network Methods 0.000 description 4
- 230000001537 neural effect Effects 0.000 description 4
- 230000003044 adaptive effect Effects 0.000 description 3
- 238000013145 classification model Methods 0.000 description 3
- 238000013500 data storage Methods 0.000 description 3
- 238000009826 distribution Methods 0.000 description 3
- 210000000887 face Anatomy 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 238000007781 pre-processing Methods 0.000 description 3
- 230000000007 visual effect Effects 0.000 description 3
- 238000005481 NMR spectroscopy Methods 0.000 description 2
- 238000010521 absorption reaction Methods 0.000 description 2
- 239000008186 active pharmaceutical agent Substances 0.000 description 2
- 230000002776 aggregation Effects 0.000 description 2
- 238000004220 aggregation Methods 0.000 description 2
- 238000013459 approach Methods 0.000 description 2
- 239000003795 chemical substances by application Substances 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 238000002408 directed self-assembly Methods 0.000 description 2
- 238000011143 downstream manufacturing Methods 0.000 description 2
- 210000005069 ears Anatomy 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 210000004709 eyebrow Anatomy 0.000 description 2
- 239000000835 fiber Substances 0.000 description 2
- 238000007477 logistic regression Methods 0.000 description 2
- 230000007774 longterm Effects 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 238000012544 monitoring process Methods 0.000 description 2
- 210000000214 mouth Anatomy 0.000 description 2
- 238000012628 principal component regression Methods 0.000 description 2
- 238000005070 sampling Methods 0.000 description 2
- 230000011218 segmentation Effects 0.000 description 2
- 230000003595 spectral effect Effects 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 238000001931 thermography Methods 0.000 description 2
- 238000003325 tomography Methods 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 240000004244 Cucurbita moschata Species 0.000 description 1
- 235000009854 Cucurbita moschata Nutrition 0.000 description 1
- 235000009852 Cucurbita pepo Nutrition 0.000 description 1
- 244000024873 Mentha crispa Species 0.000 description 1
- 235000014749 Mentha crispa Nutrition 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 230000003321 amplification Effects 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000013473 artificial intelligence Methods 0.000 description 1
- 230000002457 bidirectional effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 239000000969 carrier Substances 0.000 description 1
- 210000004027 cell Anatomy 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 238000002790 cross-validation Methods 0.000 description 1
- 238000013501 data transformation Methods 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 238000003708 edge detection Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000007636 ensemble learning method Methods 0.000 description 1
- 238000013213 extrapolation Methods 0.000 description 1
- 230000008921 facial expression Effects 0.000 description 1
- 239000000446 fuel Substances 0.000 description 1
- 230000002068 genetic effect Effects 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 230000001939 inductive effect Effects 0.000 description 1
- 238000003064 k means clustering Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000013178 mathematical model Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000002156 mixing Methods 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 210000002569 neuron Anatomy 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 238000003199 nucleic acid amplification method Methods 0.000 description 1
- 238000013450 outlier detection Methods 0.000 description 1
- 238000013139 quantization Methods 0.000 description 1
- 239000002356 single layer Substances 0.000 description 1
- 230000011273 social behavior Effects 0.000 description 1
- 235000020354 squash Nutrition 0.000 description 1
- 238000006467 substitution reaction Methods 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013526 transfer learning Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
Images
Classifications
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W50/00—Details of control systems for road vehicle drive control not related to the control of a particular sub-unit, e.g. process diagnostic or vehicle driver interfaces
- B60W50/08—Interaction between the driver and the control system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/18—Eye characteristics, e.g. of the iris
- G06V40/19—Sensors therefor
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W40/00—Estimation or calculation of non-directly measurable driving parameters for road vehicle drive control systems not related to the control of a particular sub unit, e.g. by using mathematical models
- B60W40/08—Estimation or calculation of non-directly measurable driving parameters for road vehicle drive control systems not related to the control of a particular sub unit, e.g. by using mathematical models related to drivers or passengers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/73—Determining position or orientation of objects or cameras using feature-based methods
- G06T7/74—Determining position or orientation of objects or cameras using feature-based methods involving reference images or patches
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/10—Image acquisition
- G06V10/12—Details of acquisition arrangements; Constructional details thereof
- G06V10/14—Optical characteristics of the device performing the acquisition or on the illumination arrangements
- G06V10/143—Sensing or illuminating at different wavelengths
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/20—Image preprocessing
- G06V10/25—Determination of region of interest [ROI] or a volume of interest [VOI]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/50—Context or environment of the image
- G06V20/59—Context or environment of the image inside of a vehicle, e.g. relating to seat occupancy, driver state or inner lighting conditions
- G06V20/597—Recognising the driver's state or behaviour, e.g. attention or drowsiness
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/168—Feature extraction; Face representation
- G06V40/171—Local features and components; Facial parts ; Occluding parts, e.g. glasses; Geometrical relationships
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/366—Image reproducers using viewer tracking
- H04N13/383—Image reproducers using viewer tracking for tracking with gaze detection, i.e. detecting the lines of sight of the viewer's eyes
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W2420/00—Indexing codes relating to the type of sensors based on the principle of their operation
- B60W2420/40—Photo or light sensitive means, e.g. infrared sensors
- B60W2420/403—Image sensing, e.g. optical camera
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W2540/00—Input parameters relating to occupants
- B60W2540/225—Direction of gaze
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10048—Infrared image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30196—Human being; Person
- G06T2207/30201—Face
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30248—Vehicle exterior or interior
- G06T2207/30268—Vehicle interior
Abstract
本開示の技術に従うと、方法は、車両のカメラシステムを用いて車両の乗員の少なくとも１つの画像を取り込むステップと、上記乗員の少なくとも１つの画像に基づいて車両内の乗員の１つ以上の目の位置を求めるステップと、上記乗員の少なくとも１つの画像に基づいて視線ベクトルを求めるステップとを含む。この方法はまた、上記視線ベクトルと、上記乗員の１つ以上の目の位置と、上記車両の車両データファイルとに基づいて、車両の複数の関心領域のうちの、乗員が見ている関心領域を求めるステップを含み得る。車両データファイルは複数の関心領域の各々の位置を特定する。この方法はまた、上記関心領域に基づいてアクションを選択的に実行するステップを含み得る。According to the techniques of the present disclosure, a method captures at least one image of a vehicle occupant using the vehicle's camera system and one or more eyes of the occupant in the vehicle based on at least one image of the occupant. Includes a step of finding the position of the occupant and a step of finding the line-of-sight vector based on at least one image of the occupant. The method is also based on the line-of-sight vector, the position of one or more eyes of the occupant, and the vehicle data file of the vehicle, of which the occupant is looking at the region of interest of the vehicle. May include the step of finding. The vehicle data file identifies the location of each of the multiple regions of interest. The method may also include the step of selectively performing an action based on the area of interest described above.
Description
背景
半自律運転機能を備えた車両がより一般的になりつつある。車両は、車線境界線内で車両を維持すること、曲がり角を回るように車両を案内すること、または他の車両の存在に基づいて自動的に加速するかもしくはブレーキをかけることを支援する機能を備えている場合がある。これらの運転支援機能は、適切に使用される場合は有用であるが、一部の運転者はさまざまな時間帯においてこれらの機能に依存して車両を完全に制御している。運転者は、積極的に車両を運転するのではなく、車両のヘッドユニットと対話をしている、窓の外の景色を眺めている、携帯電話を見ている、食べている、またはそれ以外の作業を行っている場合がある。残念ながら、これらの運転支援システムはユーザとの対話を必要とするように設計されているので、不注意な運転者は事故を起こす可能性がある。
Background Vehicles with semi-autonomous driving capabilities are becoming more common. Vehicles have the ability to help keep the vehicle within lane boundaries, guide the vehicle around corners, or automatically accelerate or brake based on the presence of other vehicles. May have. While these driving assistance features are useful when used properly, some drivers rely on them at different times to take full control of the vehicle. Instead of actively driving the vehicle, the driver is interacting with the vehicle's head unit, looking out the window, looking at a cell phone, eating, or otherwise. May be working on. Unfortunately, these driver assistance systems are designed to require user interaction, so careless drivers can cause accidents.
概要
概して、本開示は、ユーザが車両の運転に関与しているのかまたは何か他のものに注意を向けているのかをコンピューティングシステムが３次元の車室空間内の３次元視線ベクトルを用いて判断することを可能にするための技術に関する。本開示の技術は、ユーザがハンドルに触れることを要求したり、運転者の目が開いていると判断したり、顔の表情または瞬きの速度を分析したりするのではなく、コンピューティングシステムが、ユーザが見ている車両内の場所を正確に判断し車両内のその場所に何が物理的に位置しているかを判断することを可能にすることができる。たとえば、コンピューティングシステムは、３次元の車室内における乗員の頭部および／または目の位置を求め、乗員の顔面平面に少なくとも一部基づいて３次元視線ベクトルを求めることができる。コンピューティングシステムは、車室空間内における乗員の頭部／目の３次元位置と３次元視線ベクトルとを用いて、ユーザが見ている３次元の車室空間内の場所、たとえばバックミラー、車両のヘッドユニット、車両の計器ディスプレイ、車両のフロントガラスなどを、より正確に判断することができる。場合によっては、コンピューティングシステムは、（たとえば乗員が見ている場所は窓に関連があるという理由で）乗員が車両外部の何かを見ていると判断することがある。このような場合、コンピューティングシステムは、乗員がサイドウィンドウから外を見ておりしたがって車両の運転に十分な注意を向けていない可能性があるか否かを判断することある。コンピューティングシステムは、ユーザが見ている場所を利用して、安全に関係するまたはその他の各種措置を講じることができる（たとえば、窓の外のユーザが見ているものの写真を撮影する、車両のヘッドユニットと対話するためのユーザの能力は制限せずに、ヘッドユニットと対話するための運転者の能力を制限する、運転者がどれほど道路規則を守っているかを評価するなど）。
Overview In general, the present disclosure uses a three-dimensional line-of-sight vector within a three-dimensional cabin space for a computing system to determine whether the user is involved in driving the vehicle or is paying attention to something else. Regarding technology to make it possible to make decisions. The technology of the present disclosure is that the computing system does not require the user to touch the steering wheel, determine that the driver's eyes are open, or analyze facial expressions or blink speeds. It is possible to accurately determine the location in the vehicle that the user is looking at and determine what is physically located at that location in the vehicle. For example, a computing system can determine the position of the occupant's head and / or eyes in a three-dimensional vehicle interior and can determine the three-dimensional line-of-sight vector based on at least a portion of the occupant's face plane. The computing system uses the 3D position of the occupant's head / eye in the cabin space and the 3D line-of-sight vector to place the user in the 3D cabin space, such as a rearview mirror or vehicle. The head unit, the instrument display of the vehicle, the windshield of the vehicle, etc. can be judged more accurately. In some cases, the computing system may determine that the occupant is looking at something outside the vehicle (for example, because the location the occupant is looking at is related to the window). In such cases, the computing system may determine if the occupant is looking out of the side window and therefore may not be paying sufficient attention to driving the vehicle. The computing system can take advantage of where the user is looking to take safety-related or other measures (for example, taking a picture of what the user is looking at outside the window, of the vehicle. It does not limit the user's ability to interact with the head unit, but limits the driver's ability to interact with the head unit, assesses how well the driver adheres to road rules, etc.).
このようにして、本開示の技術は、車両の乗員の車両に対する関与のレベルをより効果的に確かめるために、乗員がどこを見ているかをコンピューティングシステムがより正確に判断することを可能にすることができる。コンピューティングシステムは、乗員がどこを見ているかをより正確に判断することにより、乗員が道路に注意を向けているか否かを判断するだけでなく、乗員が見ている物に関連するその他の各種アクションを実行することもできる。このように、本明細書に記載の視線検出システムは、典型的なドライバーアテンションシステムと比較して、コンピューティングシステムがより多くの機能を提供することを可能にすることができる。 In this way, the techniques of the present disclosure allow the computing system to more accurately determine where the occupants are looking in order to more effectively ascertain the level of occupant's involvement in the vehicle. can do. The computing system not only determines whether the occupant is paying attention to the road by more accurately determining where the occupant is looking, but also other things related to what the occupant is looking at. You can also perform various actions. As such, the line-of-sight detection system described herein can allow a computing system to provide more functionality as compared to a typical driver attention system.
１つ以上の例の詳細を添付の図面および以下の説明に記載する。本開示の他の特徴、目的、および利点は、この説明および図面ならびに請求項から明らかになるであろう。 Details of one or more examples are given in the accompanying drawings and in the description below. Other features, objectives, and advantages of the present disclosure will be apparent from this description and drawings as well as claims.
詳細な説明
図１は、本開示の１つ以上の局面に係る、３次元（３Ｄ）視線ベクトルを用いて運転者の関与を判断するように構成された車両コンピューティングシステムの一例を含む車両の内部を示す概念図である。図１は、カメラ１０２Ａおよび１０２Ｂ（まとめて「カメラ１０２」）ならびに車両コンピューティングシステム１０４に加えて車両１００の内部（本明細書では車両１００の「車室」とも呼ぶ）の断面図を示す。図１に示される車両は自動車であってもよいが、本開示の局面は、トラック、オートバイ、航空機、船舶、列車、またはそれ以外の車両を含む、他の種類の車両にも適用できるものであってもよい。図１において、運転者は通常はこの座席に座っていてもよく、その他の同乗者の座席は運転者の座席の後方または隣に位置していてもよい。
Detailed Description FIG. 1 comprises an example of a vehicle computing system configured to determine driver involvement using a three-dimensional (3D) line-of-sight vector according to one or more aspects of the present disclosure. It is a conceptual diagram showing the inside. FIG. 1 shows a cross-sectional view of the interior of a vehicle 100 (also referred to herein as the “chamber” of the vehicle 100) in addition to the
カメラ１０２は、カメラまたは電荷結合素子等の、任意の適切な種類の画像捕捉装置のうちの１つ以上であってもよい。いくつかの例において、カメラ１０２は、高視野浅焦点深度の１つ以上の赤外線カメラであってもよく、概ね車両１００の１つ以上の座席の方向に向けられた背面照光赤外線カメラであってもよい。その他の例において、カメラ１０２は、その他１つ以上の赤外線カメラ、サーモグラフィーカメラ、熱撮像カメラ、感光カメラ、レンジセンサ、深度カメラ、トモグラフィーデバイス、レーダーデバイス、または超音波カメラを含み得る、１つ以上のその他の種類のカメラもしくは画像センサであってもよい、またはこれらを含んでいてもよい。いくつかの例において、カメラ１０２は、コンピュータビジョン技術の適用に適した画像キャプチャデバイスであってもよい。使用するセンサまたはカメラの種類に応じて、結果として得られる画像は、２次元画像、３次元ボリューム、または画像シーケンスを含み得る。画素値は、典型的には１つ以上のスペクトル帯における光度に対応するが、深度、超音波もしくは電磁波の吸収もしくは反射、または核磁気共鳴に関連していてもよい。図１には２つのカメラ１０２のみが示されているが、車両１００は、車両１００の内部に配置された３つ以上のカメラ１０２を含んでいてもよく、本開示の技術は任意の数のカメラ１０２が取り込んだ任意の数の画像を使用することができる。
The camera 102 may be one or more of any suitable type of image capture device, such as a camera or charge-coupled device. In some examples, the camera 102 may be one or more infrared cameras with a high field of view and shallow depth of focus, or a rear-illuminated infrared camera generally oriented towards one or more seats in the
一般的に、車両コンピューティングシステム１０４は、支援する、報知する、楽しませる、または、車両の乗員とのユーザ対話を必要とするその他のタスクを実行するように動作することができる。車両コンピューティングシステム１０４を、車両ヘッドユニット、インフォテイメント（infotainment）システム、またはそのサブコンポーネントと呼ぶこともできる。たとえば、車両コンピューティングシステム１０４は、車両の１人以上の乗員に代わって、機能を実行するまたは情報を処理する各種アプリケーションを実行することができる。たとえば、車両コンピューティングシステム１０４は、目的地への方向を提供するナビゲーションサービスを提供してもよい。車両コンピューティングシステム１０４はまた、クエリに応じておよび／またはプリエンプティブな支援またはレコメンデーションとして情報を提供する情報検索サービスを提供することができる。車両コンピューティングシステム１０４はまた、車両に関する車両データまたは音声もしくは動画等のマルチメディアを提供することもできる。車両コンピューティングシステム１０４が提供し得る機能のわずかな例にしか言及していないが、車両コンピューティングシステム１０４はその他多数の機能を提供することができる。このやり方およびその他のやり方で、車両コンピューティングシステム１０４は、車両の１人以上の乗員の運転または乗車体験を改善することができる。
In general, the
カメラシステムおよび／または車両コンピューティングシステム１０４は、カメラ１０２が取り込んだ画像を用いて、車両１００の内部における乗員の位置を求めることができ、かつ、乗員の３Ｄ視線ベクトル（たとえば３Ｄ視線ベクトル１１２）を求めることができる。カメラ１０２は、車両１００の乗員が車両１００を運転しているときのこの乗員（たとえば車両１００の運転者）の画像を取り込むことができる。いくつかの場合において、カメラ１０２は、車両１００のその他の乗員の画像を取り込むカメラを含み得る。カメラ１０２は、少なくとも１つ以上のプロセッサとメモリとを含むカメラシステムの一部であってもよい。カメラ１０２が取り込んだ画像は、カメラシステムまたは車両コンピューティングシステム１０４または双方によって分析されてもよい。さまざまな例において、車両コンピューティングシステム１０４に関連する本明細書に記載の技術は、その全体または一部がコンピューティングシステムによって実行されてもよい。
The camera system and / or the
車両コンピューティングシステム１０４は、乗員の頭部および／または目を車両１００内部に配置するために、カメラ１０２のうちの１つ以上から乗員の目（または頭部）までの距離を、カメラ１０２のうちの１つ以上が取り込んだ画像を用いて求める。いくつかの場合において、カメラ１０２のうちの少なくとも２つが乗員の画像を取り込む。車両コンピューティングシステム１０４は、少なくとも２つのカメラ１０２の各々の位置はわかっていると仮定して、画像間の視差角（parallax angle）を分析することができる。車両コンピューティングシステム１０４は、視差角とカメラ間の距離とを用いて、少なくとも２つのカメラ１０２のうちの１つ以上と乗員の目との間の距離を求める。別の例として、カメラ１０２のうちの１つは赤外線カメラであってもよい。車両コンピューティングシステム１０４は、カメラ１０２のうちの１つのカメラを用いて、赤外線カメラが取り込んだ画像の歪みを分析することにより、赤外線カメラ１０２と乗員の目との間の距離を求めることができる。
The
車両コンピューティングシステム１０４は、この求めたカメラ１０２と乗員の目との間の距離と、取り込んだ画像における乗員の目の位置とを用いて、乗員の目を３Ｄ空間においてカメラ１０２に対して配置することができる。すなわち、車両コンピューティングシステム１０４および／またはカメラシステムは、車両１００の内部における、カメラ１０２のうちの１つ以上に対する乗員の目の位置を求めることができる。乗員の目の位置は、３Ｄ空間内でカメラ１０２のうちの１つ以上に対して定められる位置である。たとえば、３Ｄ空間は球体でその重心がカメラ１０２のうちの１つの位置に対応していてもよい。このような例において、乗員の目の位置は（ｘ，ｙ，ｚ）座標で定めることができ、（０，０，０）はこの球体の重心として用いられているカメラ１０２のうちの１つの位置である。このような座標は「カメラベースの座標系」の内部にあると言うことができる。
The
車両コンピューティングシステム１０４は、車両１００の乗員の目を追跡することもできる。たとえば、車両コンピューティングシステム１０４は、カメラ１０２を用いて、カメラ１０２が取り込んだ複数の異なる画像にわたって乗員の目の位置および動きを追跡してもよい。車両コンピューティングシステム１０４は、目の位置および瞳孔または目のその他の特徴の動き（すなわちアイトラッキング）を用いて、第１の初期３Ｄ視線ベクトルを求めることができる。しかしながら、さまざまな場合において、乗員の目が遮られているためにカメラ１０２が取り込んだ画像が乗員の目の明瞭な画像を含んでいないことがある。よって、車両コンピューティングシステム１０４は、乗員の目の位置および動きの追跡に頼るだけではなく、取り込んだ画像をさらに分析することによって乗員の画像平面を求めてもよい。
The
乗員の顔面平面の計算において、車両コンピューティングシステム１０４は、カメラ１０２のうちの１つ以上が取り込んだ１つ以上の画像において複数の顔面ランドマークを識別することができる。顔面ランドマークは、口、目、鼻、耳、眉、あご、またはそれ以外の顔の特徴の輪郭を含み得る。車両コンピューティングシステム１０４は、識別した顔面ランドマークを用い、各種顔面ランドマーク間の幾何学的整合性に基づいて、画像に含まれる乗員の顔が何らかのピッチ、ロール、またはヨーを示しているか否かを判断することができる。たとえば、乗員の口と目との間の総距離と比較して、乗員の２つの目の間の距離が、乗員が真っすぐ前方を見ているときよりも短い場合、車両コンピューティングシステム１０４は、乗員が左または右を見ていると判断する。車両コンピューティングシステム１０４は、画像は乗員の右耳を含んでいるが左耳は含んでいないと判断した場合、乗員は左を見ていると判断する。顔面平面のピッチ角、ロール角、およびヨー角は、顔面ランドマーク間の距離の相対的変化に基づいて判断することができる。車両コンピューティングシステム１０４は、求めた顔面平面のピッチ角、ロール角、およびヨー角を用いて、第２の初期３Ｄ視線ベクトルを求めることができる。本明細書に記載の、ピッチ、ロール、およびヨーは、特定の軸を中心とするユーザの頭部の回転を意味し得る。一般的に、ロールは、頭部のその鉛直軸を中心とする回転を意味し得るものであり、頭部の左右の回転と言うこともできる。ピッチは、ユーザが、人がうなずいているかのように頭部を上下に動かすことを意味し得る。ヨーは、ユーザが頭部を左（右）から鉛直軸を通って右（左）に回すことを意味し得る。このときの回転は、ユーザの一方の耳から他方の耳までの水平面に沿った回転となる。
In calculating the occupant's facial plane, the
さまざまな場合において、車両コンピューティングシステム１０４は、トレーニングされた機械学習モデルを画像に適用することにより、顔面平面を求めることができる。機械学習モデルは、既に識別されている顔面ランドマークを有し顔面平面の角度が既に求められている他の人々の画像を用いてトレーニングすることができる。機械学習モデルは、ユーザフィードバックと、車両コンピューティングシステム１０４から与えられたフィードバックとに基づいて、継続的に学習することができる。たとえば、車両コンピューティングシステム１０４は、顔面平面を用いて求めた第２の３Ｄ視線ベクトルと比較される、アイトラッキングを用いて求めた第１の初期３Ｄ視線ベクトルに基づいて、機械学習モデルのパラメータをランク付けまたは調整することができる。機械学習システムが如何にして動作し得るかに関するその他の詳細は、以下で図６Ａ〜図６Ｅを参照しながら説明する。
In various cases, the
車両コンピューティングシステム１０４は、第１および第２の初期３Ｄ視線ベクトルを求めると、第１および第２の初期３Ｄ視線ベクトル双方に基づいて、乗員の３Ｄ視線ベクトル１１２を求めることができる。さまざまな場合において、車両コンピューティングシステム１０４は、第１および第２の初期３Ｄ視線ベクトルの平均を用いて３Ｄ視線ベクトル１１２を求めることができる。その他の例において、車両コンピューティングシステム１０４は、第１および第２の３Ｄ視線ベクトルのうちの１つ以上に重み付けを適用し重み付けされた値を用いることによって３Ｄ視線ベクトル１１２を求めることができる。車両コンピューティングシステム１０４は、第１または第２の初期３Ｄ視線ベクトルが正確に求められた確信度（confidence）に基づいて、第１および第２の初期３Ｄ視線ベクトルに適用する重みを求めることができる。たとえば、車両コンピューティングシステム１０４が顔面ランドマークのうちの一部のみを検出した場合（たとえば目と鼻だけ、口またはあごは検出なし）、顔面平面のピッチ角、ロール角、およびヨー角の誤差範囲が大きくなる可能性がある。このため、車両コンピューティングシステム１０４は、小さくした重みの値を第２の初期３Ｄ視線ベクトルに適用し大きくした重みの値をアイトラッキングを用いて求めた第１の初期３Ｄ視線ベクトルに適用することができる。
When the
第１および第２の初期３Ｄ視線ベクトル双方が必要と説明しているが、車両コンピューティングシステム１０４は、第１および第２の初期３Ｄ視線ベクトルのうちの一方を用いて３Ｄ視線ベクトル１１２を求めることもできる。たとえば、カメラ１０２が取り込んだ画像において乗員の目が閉じられているまたは乗員がサングラスを着用している場合、車両コンピューティングシステム１０４は、第２の初期３Ｄ視線ベクトル（すなわち乗員の顔面平面のピッチ、ロール、およびヨーに基づいて求めた３Ｄ視線ベクトル）を３Ｄ視線ベクトル１１２として使用してもよい。別の例として、カメラ１０２が取り込んだ画像に、（たとえば乗員の手によって）顔が部分的に隠れている乗員が含まれている場合、車両コンピューティングシステム１０４は、第１の初期３Ｄ視線ベクトル（すなわちアイトラッキングを用いて求めた３Ｄ視線ベクトル）を３Ｄ視線ベクトル１１２として使用してもよい。
Although it is explained that both the first and second initial 3D line-of-sight vectors are required, the
車両コンピューティングシステム１０４が３Ｄ視線ベクトルを求めるのではなく、または求めることに加えて、カメラシステムが３Ｄ視線ベクトル１１２を求めてもよい。すなわち、車両コンピューティングシステム１０４がカメラ１０２から１つ以上の画像を受けるのではなく、カメラシステムが、（たとえば各カメラ１０２の中でまたは車両コンピューティングシステム１０４と区別されるコンピューティングデバイスを用いて）画像を分析し、３Ｄ視線ベクトル１１２を求め、３Ｄ視線ベクトル１１２の値を車両コンピューティングシステム１０４に与えてもよい。いくつかの場合において、カメラシステムは、第１および第２の初期３Ｄ視線ベクトルを求めてこれらを車両コンピューティングシステム１０４に与える。そうすると、車両コンピューティングシステム１０４は、カメラシステムから受けた第１および第２の初期３Ｄ視線ベクトルを用いて３Ｄ視線ベクトル１１２を求めることができる。
In addition to the
車両コンピューティングシステム１０４は、３Ｄ視線ベクトル１１２と、カメラ１０２のうちの１つ以上に対する乗員の目の位置とに基づいて、乗員が見ている１つ以上の関心領域を求める。このように関心領域を求めるために、車両コンピューティングシステム１０４は、３Ｄ視線ベクトル１１２が車両１００の内部の１つ以上の位置と交差する場所を求める。一例として、車両１００は車両１００の第２の例と異なっていてもよく、車両コンピューティングシステム１０４には車両固有データファイルが与えられる。車両データファイルは、車両１００の内部の各関心領域の座標のセットを含み、その座標セットの各々は、車両の内部の重心を基準として（すなわち車両ベースの座標系を用いて）定められ、その座標のセットの各々は、関心領域の２次元平面を画定する。さらに、各種車両の内部の特徴を定める車両データファイルを用いることにより、本開示の技術を、プログラマーが異なる各車両に合わせて計算をカスタマイズすることを要求せずに、多種多様な車両に対して一層簡単に適用することができる。
The
乗員の位置は最初にカメラ１０２のうちの１つ以上に対して（すなわちカメラベースの座標系内で）求めることができるので、車両コンピューティングシステム１０４は、乗員の目の初期位置座標を、車両データファイルで指定されている重心に対して定められる座標のセットに変換することができる。すなわち、車両コンピューティングシステム１０４は、カメラ１０２のうちの１つ以上に対して定められた球体についての座標を用いるのではなく、乗員の目の位置を調整することにより、車両１００の内部を包む球体の重心に対する位置を定める。車両１００の内部を包みその重心がカメラ１０２のうちの１つの位置以外のいずれかの場所にある球体の内部の座標位置を、本明細書では「車両ベースの座標系」内の座標を有すると言うことができる。いくつかの例において、車両ベースの座標系の重心は、車両１００の内部の中心点に位置していてもよい。その他の例において、この重心は、運転席と車両１００のダッシュボードまたはハンドルとの間の空間内の中心点に位置していてもよい。
Since the occupant's position can initially be determined for one or more of the cameras 102 (ie, within the camera-based coordinate system), the
車両データファイルは、車両ベースの座標系を用いて、車両１００内における１つ以上のカメラ１０２の位置を定めることができる（たとえば（ｘ，ｙ，ｚ座標で））。車両コンピューティングシステム１０４は、１つ以上のカメラ１０２の座標位置と、カメラベースの座標系を用いて定めた乗員の目の座標位置とを使用し、車両ベースの座標系における乗員の目の新たな座標位置を生成する。
The vehicle data file can use a vehicle-based coordinate system to locate one or more cameras 102 within the vehicle 100 (eg (in x, y, z coordinates)). The
車両コンピューティングシステム１０４は、乗員の目の、車両ベースの座標系の位置を用いて、３Ｄ視線ベクトル１１２を、乗員の目から、関心領域に対応付けられた平面と交差するまで延ばす。車両コンピューティングシステム１０４は、３Ｄ視線ベクトル１１２が交差するこの平面が、車両１００の乗員が見ている関心領域であると判断する。図１に示されるように、３Ｄ視線１１２は車両１００のフロントガラスに向かって延びている。したがって、車両コンピューティングシステム１０４は、乗員が車両１００のフロントガラスから外を見ており車両１００の運転に関与していると判断することができる。
The
いくつかの場合において、車両コンピューティングシステム１０４は、車両１００の乗員がある時間にわたってどの関心領域を見ているかを、定期的にまたは連続的に判断してもよい。車両コンピューティングシステム１０４は、乗員がどの関心領域を見ているかをモニタリングすることにより、乗員の関与のレベルを判断することができるとともに、乗員がどれほど良く交通規則および安全運転のための最良の実施に従っているかを判断することができる。たとえば、乗員がサイドミラーを見ずに車線変更した場合、またはバックミラーを見ずに車両１００を後退させた場合、車両コンピューティングシステム１０４は、乗員が安全のための最善の実施に従って車両１００を運転していないと判断することができる。別の例として、車両１００が動いているときに乗員が長時間にわたって車両コンピューティングシステム１０４のディスプレイを見ている場合、車両コンピューティングシステム１０４は、ロックアウトする、または乗員が車両コンピューティングシステム１０４とやり取りするのを禁止することができる。
In some cases, the
このようにして、本開示の技術は、車両コンピューティングシステムが、車両の乗員がどこを見ているかをより正確に判断することを可能にし、乗員が車両の運転者である場合は、運転者の関与をより正確に判断することを可能にすることができる。さらに、車両コンピューティングシステム１０４は、乗員が何を見ているかについてのより正確な判断を用いることで、安全に関する各種アクションまたは任意の数のその他のアクションを自動的に実行することができる。このようにして、本開示の技術は改善された車両安全システムを可能にすることができる。
In this way, the techniques of the present disclosure allow the vehicle computing system to more accurately determine where the occupant of the vehicle is looking, and if the occupant is the driver of the vehicle, the driver. It can be made possible to judge the involvement of. In addition, the
図２は、本開示の１つ以上の局面に係る、内部カメラシステムを有する車両の一例を示す概念図である。図２に示されるように、車両内部２００は、車両内部２００のさまざまな異なる場所に配置された４つのカメラ２０２Ａ〜２０２Ｄ（まとめて「カメラ２０２」）を含む。各カメラ２０２は、図１に示され図１との関連で説明したカメラ１０２の一例であってもよい。
FIG. 2 is a conceptual diagram showing an example of a vehicle having an internal camera system according to one or more aspects of the present disclosure. As shown in FIG. 2,
車両内部２００における異なるカメラ配置は、本明細書に記載の乗員の画像の取り込みについて異なる長所または短所をもたらし得る。たとえば、カメラ２０２Ａは車両内部２００の計器クラスタ内に配置されている。カメラ２０２Ａは車両の運転者のおそらく頭部位置の正面に位置しているので、カメラ２０２Ａが取り込んだ画像は、運転者の顔面平面のピッチ、ロール、およびヨーを求めるための高品質画像を提供することができる。カメラ２０２Ｃは車両のヘッドユニットディスプレイの上方に位置しているので、車両の運転者と同乗者双方の顔を含む画像を取り込むことができる。このような画像を用いることにより、車両の同乗者の３Ｄ視線ベクトルを求めることができ、また、このような画像を用いることにより、車両の運転者か同乗者のどちらが車両のヘッドユニットディスプレイとやり取りしているかを区別することができる。別の例として、運転者側のＡピラー内に配置されたカメラ２０２Ｄは、カメラ２０２Ｄから運転者の目までの距離を求めるのに使用される画像を取り込む赤外線カメラであってもよい。これに代えてまたはこれに加えて、車両コンピューティングシステム１０４は、カメラ２０２Ｂおよび２０２Ｄ双方が取り込んだ画像を用いることにより、運転者の目についての視差角を求めることができ、カメラ２０２Ｂおよび２０２Ｄのうちの一方または双方（またはカメラ２０２Ｂおよび２０２Ｄの間の中心点）から運転者の目までの距離を求めることができる。車両２００は左ハンドル車であるが、他の例において、車両内部２００または図１の車両１００は右ハンドル車であってもよい。このような例ではカメラ２０２の配置を反転させてもよい（たとえばカメラ２０２Ｄが右側のＡピラーにあってもよい）。
Different camera arrangements within the
図３は、本開示の１つ以上の局面に係る、運転者の顔面平面の一例を示す概念図である。図３に示されるように、車両コンピューティングシステム１０４により、１つ以上の顔ランドマーク３０２および顔面平面３０４が識別されており、３Ｄ視線ベクトル３０６が求められている。
FIG. 3 is a conceptual diagram showing an example of a driver's facial plane according to one or more aspects of the present disclosure. As shown in FIG. 3, the
顔ランドマーク３０２は、運転者の２つの口角と、運転者の鼻の基部と、運転者の各目の角とを含む。車両コンピューティングシステム１０４は、これらの顔ランドマーク３０２を用いることで顔面平面を画定することができる。たとえば、車両コンピューティングシステム１０４は、１つ以上の顔ランドマーク３０２間の距離を求めてもよい。いくつかの場合において、車両コンピューティングシステム１０４は、運転者の２つの口角の間の距離、および／または左目の角と右目の角との間の距離を求めることができる。車両コンピューティングシステム１０４は、これらの距離を、基準または運転者の学習済の距離と比較することにより、顔面平面３０４を画定する座標を求めることができる。
The
車両コンピューティングシステム１０４は、顔面平面３０４を用いて３Ｄ視線ベクトル３０６（たとえば図１に関して説明した第２の初期３Ｄ視線ベクトル）を求めることができる。３Ｄ視線ベクトル３０６は、運転者の両目から出ている矢印として示されているが、運転者の目の一方または双方の位置から外に延ばすことができる１本のベクトルであってもよい。さらに、３Ｄ視線ベクトル３０６を、車両コンピューティングシステムがアイトラッキングを用いて求めた別の３Ｄ視線ベクトルと組み合わせてもよい。
The
図４は、本開示の１つ以上の局面に係る、関心領域を有する車両の内部の一例を示す概念図である。図４に示されるように、車両の内部は関心領域４００Ａ〜４００Ｇ（まとめて「関心領域４００」）を含む。関心領域４００は、図４に示される車両内部の特定の形状およびモデルについての車両データファイルにおいて定められていてもよい。車両データファイルは、車両ベースの座標系内の平面を画定する関心領域４００の各々の座標を特定する。一般的に、車両データファイルは、拡張可能マークアップ言語（ＸＭＬ）等の構造化されたデータフォーマットを用いて定められる。しかしながら、車両データファイルに含まれる情報は、車両コンピューティングシステム１０４が処理するように構成されている任意のフォーマットで符号化されてもよい。
FIG. 4 is a conceptual diagram showing an example of the inside of a vehicle having an area of interest according to one or more aspects of the present disclosure. As shown in FIG. 4, the inside of the vehicle includes the region of
各関心領域４００は、車両の物理的要素に対応付けることができる。関心領域は、車両内部の関心領域として説明しているが、さまざまな場合において、関心領域は車両外部に位置する物体に対応付けられていることがある。たとえば、関心領域４００Ａおよび４００Ｇは各々、車両のサイドミラーに対応付けられてもよい（たとえばそれぞれ運転者側のサイドミラーおよび同乗者側のサイドミラー）。
Each region of interest 400 can be associated with a physical element of the vehicle. The region of interest is described as the region of interest inside the vehicle, but in various cases, the region of interest may be associated with an object located outside the vehicle. For example, the regions of
その他の関心領域は、車両の１つの物理的要素の異なる領域に対応付けられていてもよい。たとえば、関心領域４００Ｂおよび４００Ｄはいずれも車両のフロントガラスに対応付けることができる。この例において、フロントガラスの異なる部分を運転者の異なる関与レベルに対応付けることができる。よって、車両コンピューティングシステム１０４が運転者は関心領域４００Ｂを見ていると判断した場合、車両コンピューティングシステム１０４は、ユーザは車両の前方の道路を見ている可能性が高く車両の運転に関与していると判断することができる。しかしながら、車両コンピューティングシステム１０４が運転者は関心領域４００Ｄを見ていると判断した場合、車両コンピューティングシステム１０４は、運転者は道路沿いにある何か外のものを見ており、したがって車両の前方の道路を見ている場合と比較して車両の運転への関与は少ないであろうと判断することができる。車両コンピューティングシステム１０４が運転者は安全運転のための最良の実施に従っているか否かを判断する場合に、車両コンピューティングシステム１０４は、車両が緑に変わったばかりの信号機の場所に位置していると判断することがある。車両コンピューティングシステム１０４は、たとえば運転者が関心領域４００Ｄを見ていたか否かを判断することにより、赤信号で走り過ぎた可能性のある他の車両を運転者が確認していたか否かを判断することができる。
Other regions of interest may be associated with different regions of one physical element of the vehicle. For example, the areas of
いくつかの例において、車両コンピューティングシステム１０４は、運転者が見ている関心領域を用いることにより、運転者が実行しているアクションと同乗者が実行しているアクションを区別することができる。たとえば、インフォテイメントシステムと対話している（たとえばナビゲーションアプリに住所を入力、音楽を選択、車両または同乗者設定を調整する、など）ユーザからのユーザ入力を車両コンピューティングシステム１０４が受けた場合、車両コンピューティングシステム１０４は、運転者が関心領域４００Ｆを見ているか否かを判断することができる。運転者は関心領域４００Ｆを見ていないと車両コンピューティングシステム１０４が判断した場合、車両コンピューティングシステム１０４は、同乗者が入力していると判断して同乗者が制限なしでインフォマントシステムを引続き使用することを認めることができる。
In some examples, the
しかしながら、運転者が関心領域４００Ｆを見ている場合、車両コンピューティングシステム１０４は、運転者がユーザ入力を与えている可能性が高いと判断することができる。運転者が、所定期間内にユーザ入力のしきい値数を超える数の入力を与えた場合、または所定期間を超えてインフォマントシステムと対話し続けた場合、車両コンピューティングシステム１０４は、道路に再び注意を向けるよう運転者に促すためのさまざまなアクションを実行することができる。たとえば、車両コンピューティングシステム１０４は、非限定的な例として、追加のユーザ入力の処理を停止してもよく、ディスプレイにグラフィカルユーザインターフェイスを出力するのをやめてもよく、または、道路に注意するよう運転者に促すメッセージを出力してもよい。
However, when the driver is looking at the region of
いくつかの例において、車両コンピューティングシステム１０４は、求めた関心領域を用いることにより、各種通知をどこに出力するかを判断することができる。たとえば、車両コンピューティングシステム１０４は、運転者が関心領域４００Ｅ（すなわち計器クラスタに相当）を見ていると判断した場合、計器クラスタを用いて、車両の燃料が減少しているという警告メッセージを出力することができる。車両コンピューティングシステム１０４は、運転者が関心領域４００Ｅではなく関心領域４００Ｂを見ていると判断した場合、警告メッセージを、関心領域４００Ｂ内において車両のフロントガラスに投影されるよう、ヘッドアップディスプレイを用いて出力することができる。
In some examples, the
図５は、本開示の１つ以上の局面に係る、３Ｄ視線ベクトルを用いて運転者の関与を判断するように構成されたコンピューティングデバイスの一例を示すブロック図である。コンピューティングデバイス５００は、図１の車両コンピューティングシステム１０４のより詳細な例である。図５は、コンピューティングデバイス５００のある特定の例のみを示しており、他の場合では、コンピューティングデバイス５００の他の多数の例が用いられてもよく、この一例としてのコンピューティングデバイス５００に含まれるコンポーネントのサブセットを含み得る、または図５に示されていないその他のコンポーネントを含み得る。
FIG. 5 is a block diagram illustrating an example of a computing device configured to determine driver involvement using a 3D line-of-sight vector according to one or more aspects of the present disclosure. The computing device 500 is a more detailed example of the
図５の例に示されるように、コンピューティングデバイス５００は、存在感知ディスプレイ５１２と、１つ以上のプロセッサ５４０と、１つ以上の通信ユニット５４２と、１つ以上の入力コンポーネント５４４と、１つ以上の出力コンポーネント５４６と、１つ以上の記憶装置５４８とを含む。コンピューティングデバイス５００の記憶装置５４８は、視線モジュール５２２と、目位置モジュール５２４と、関心領域モジュール５２６と、車両データ５２８とを含む。
As shown in the example of FIG. 5, the computing device 500 includes a
通信チャネル５５０は、コンポーネント５１２、５４０、５４２、５４６、および／または５４８の各々を、コンポーネント間通信のために（物理的に、通信可能に、および／または作動的に）相互接続することができる。いくつかの例において、通信チャネル５５０は、システムバス、ネットワーク接続、１つ以上のプロセス間通信データ構造、またはデータ（情報とも呼ぶ）通信のための任意の他のコンポーネントを含み得る。
The
コンピューティングデバイス５００の１つ以上の通信ユニット５４２は、データを送信および／または受信することによって外部デバイスと通信することができる。たとえば、コンピューティングデバイス５００は、通信ユニット５４２のうちの１つ以上を用いることにより、セルラー無線ネットワーク等の無線ネットワーク上で無線信号を送信および／または受信することができる。いくつかの例において、通信ユニット５４２は、グローバルポジショニングシステム（ＧＰＳ）ネットワーク等の衛星ネットワーク上で衛星信号を送信および／または受信することができる。通信ユニット５４２の例は、ネットワークインターフェイスカード（たとえばイーサネット（登録商標）カード等）、光トランシーバ、無線周波数トランシーバ、ＧＰＳ受信機、または情報を送信および／または受信することが可能な任意の他の種類のデバイスを含む。通信ユニット５４２のその他の例は、携帯電話で見受けられる短波無線（たとえばＮＦＣ、ブルートゥース（登録商標）（ＢＬＥを含む））、ＧＰＳ、３Ｇ、４Ｇ、５Ｇ、およびＷＩＦＩ（登録商標）無線、ならびにユニバーサルシリアルバス（ＵＳＢ）コントローラなどを含み得る。 One or more communication units 542 of the computing device 500 can communicate with external devices by transmitting and / or receiving data. For example, the computing device 500 can transmit and / or receive radio signals over a radio network such as a cellular radio network by using one or more of the communication units 542. In some examples, the communication unit 542 can transmit and / or receive satellite signals over a satellite network such as the Global Positioning System (GPS) network. Examples of communication units 542 are network interface cards (eg Ethernet cards, etc.), optical transceivers, radio frequency transceivers, GPS receivers, or any other type capable of transmitting and / or receiving information. Includes devices. Other examples of communication unit 542 are short wave radios found in mobile phones (eg NFC, Bluetooth® (including BLE)), GPS, 3G, 4G, 5G, and WIFI® radios, and universal. It may include a serial bus (USB) controller and the like.
コンピューティングデバイス５００の１つ以上の入力コンポーネント５４４は入力を受信することができる。入力の２、３の例を挙げると、触覚、音声、動的、および光入力である。コンピューティングデバイス５００の入力コンポーネント５４４は、一例において、マウス、キーボード、タッチパッド、音声応答システム、ビデオカメラ、ボタン、スクロールホイール、ダイヤル、コントロールパッド、マイク、または人間もしくはマシンからの入力を検出するための任意の他の種類のデバイスを含む。入力コンポーネント５４４は、図１のカメラ１０２のようなカメラを含み得る。いくつかの例において、入力コンポーネント５４４は、存在感知スクリーン、接触感知スクリーンなどを含み得る、存在感知入力コンポーネントであってもよい。 One or more input components 544 of the computing device 500 can receive inputs. A few examples of inputs are tactile, voice, dynamic, and optical inputs. The input component 544 of the computing device 500, in one example, is for detecting input from a mouse, keyboard, touchpad, voice response system, video camera, buttons, scroll wheel, dial, control pad, microphone, or human or machine. Includes any other type of device. The input component 544 may include a camera such as the camera 102 of FIG. In some examples, the input component 544 may be a presence sensing input component, which may include presence sensing screens, contact sensing screens, and the like.
コンピューティングデバイス５００の１つ以上の出力コンポーネント５４６は出力を生成することができる。出力の例は、触覚、音声、およびビデオ出力である。コンピューティングデバイス５００の出力コンポーネント５４６は、いくつかの例において、存在感知スクリーン、サウンドカード、ビデオグラフィックスアダプタカード、スピーカ、陰極線管（ＣＲＴ）モニタ、液晶ディスプレイ（ＬＣＤ）、有機発光ダイオード（ＯＬＥＤ）、または、人間もしくはマシンに対する触覚、音声および／または視覚出力を生成するための任意の他の種類のデバイスを含む。 One or more output components 546 of the computing device 500 can generate outputs. Examples of outputs are tactile, audio, and video outputs. The output component 546 of the computing device 500 is, in some examples, an presence sensing screen, a sound card, a videographics adapter card, a speaker, a cathode ray tube (CRT) monitor, a liquid crystal display (LCD), an organic light emitting diode (OLED). , Or any other type of device for producing tactile, audio and / or visual output to humans or machines.
いくつかの例において、コンピューティングデバイス５００の存在感知ディスプレイ５１２は、入力コンポーネント５４４および／または出力コンポーネント５４６の機能を含み得る。図５の例において、存在感知ディスプレイ５１２は、存在感知スクリーンまたは接触感知スクリーン等の存在感知入力コンポーネント５０４を含み得る。いくつかの例において、存在感知入力コンポーネント５０４は、存在感知入力コンポーネントの場所にあるおよび／またはその近くにある物体を検出することができる。レンジの一例として、存在感知入力コンポーネント５０４は、存在感知入力コンポーネント５０４から２インチ以内にある指またはスタイラス等の物体を検出することができる。存在感知入力コンポーネント５０４は、物体が検出された存在感知入力コンポーネントの位置（たとえば（ｘ，ｙ）座標）を求めることができる。レンジの別の例として、存在感知入力コンポーネント５０４は、存在感知入力コンポーネント５０４から２インチ以内の物体を検出することができ、その他のレンジも可能である。存在感知入力コンポーネント５０４は、容量性、誘導性、および／または光認識技術を用いて、ユーザの指で選択された存在感知入力コンポーネント５０４の位置を求めることができる。
In some examples, the presence-
いくつかの例において、存在感知ディスプレイ５１２も、出力コンポーネント５４６に関して述べたように、触覚、音声、またはビデオ刺激を用いて出力をユーザに与えることができる。たとえば、存在感知ディスプレイ５１２は、グラフィカルインターフェイスを表示するディスプレイコンポーネント５０２を含み得る。ディスプレイコンポーネント５０２は、出力コンポーネント５４６について述べたように、視覚的出力を提供する任意の種類の出力コンポーネントであればよい。存在感知ディスプレイ５１２はコンピューティングデバイス５００に一体化されたコンポーネントとして示されているが、いくつかの例では、入力および出力の送信および／または受信のためにコンピューティングデバイス５００の他のコンポーネントとデータまたは情報経路を共有する外部コンポーネントであってもよい。たとえば、存在感知ディスプレイ５１２は、コンピューティングデバイス５００の外部パッケージング内に位置しこのパッケージングに物理的に接続された、コンピューティングデバイス５００の内蔵コンポーネントであってもよい（たとえば車両のダッシュボードに搭載された車載スクリーン）。別の例において、存在感知ディスプレイ５１２は、コンピューティングデバイス５００のパッケージングの外側に位置しこのパッケージングから物理的に離れている、コンピューティングデバイス５００の外部コンポーネント（たとえば、車両の電子制御部と有線および／または無線データ経路を共有する、モニタ、プロジェクタなど）であってもよい。いくつかの例において、存在感知ディスプレイ５１２は、コンピューティングデバイス５００のパッケージングの外部に位置しこのパッケージングから物理的に離れている場合、出力を提供するために、別々の２つのコンポーネントとしての、入力を受けるための存在感知入力コンポーネント５０４と出力を提供するためのディスプレイコンポーネント５０２とにより、実現することができる。
In some examples, the presence-
コンピューティングデバイス５００内の１つ以上の記憶コンポーネント５４８は、コンピューティングデバイス５００の動作中に処理するために情報を格納することができる（たとえば、コンピューティングデバイス５００は、コンピューティングデバイス５００での実行中にモジュール５２２、５２４、５２６がアクセスするデータを格納することができる。いくつかの例において、記憶コンポーネント５４８は一時メモリである。このことは、記憶コンポーネント５４８の主な目的が長期保存ではないことを意味する。コンピューティングデバイス５００上の記憶コンポーネント５４８は、揮発性メモリとして短期記憶用に構成されていてもよく、したがって、電源がオフにされた場合は格納されているコンテンツを保持しない。揮発性メモリの例は、ランダムアクセスメモリ（ＲＡＭ）、ダイナミックランダムアクセスメモリ（ＤＲＡＭ）、スタティックランダムアクセスメモリ（ＳＲＡＭ）、および当該技術において周知の他の形態の揮発性メモリを含む。
One or more storage components 548 in the computing device 500 can store information for processing during the operation of the computing device 500 (eg, the computing device 500 may perform on the computing device 500). It can store data accessed by
いくつかの例において、記憶コンポーネント５４８はまた、１つ以上のコンピュータ読取可能記憶媒体を含む。いくつかの例において、記憶コンポーネント５４８は、１つ以上の非一時的なコンピュータ読取可能記憶媒体を含む。記憶コンポーネント５４８は、揮発性メモリが一般的に格納する情報の量よりも多い量の情報を格納するように構成することができる。記憶コンポーネント５４８はさらに、不揮発性メモリ空間として情報の長期記憶用に構成されて電源のオン／オフサイクル後も情報を保持するように構成することができる。不揮発性メモリの例は、磁気ハードディスク、光ディスク、フラシュメモリ、または、電気的プログラム可能メモリ（ＥＰＲＯＭ）もしくは電気的消去可能プログラム可能（ＥＥＰＲＯＭ）メモリの形態を含む。記憶コンポーネント５４８は、モジュール５２２、５２４、および５２６に対応付けられたプログラム命令および／または情報（たとえばデータ）を格納することができる。記憶コンポーネント５４８は、５２２、５２４、および５２６に対応付けられたデータまたはその他の情報、ならびに車両データ５２８を格納するように構成されたメモリを含み得る。
In some examples, the storage component 548 also includes one or more computer-readable storage media. In some examples, the storage component 548 includes one or more non-temporary computer readable storage media. The storage component 548 can be configured to store more information than the volatile memory typically stores. The storage component 548 can also be configured as a non-volatile memory space for long-term storage of information to retain information even after power on / off cycles. Examples of non-volatile memory include magnetic hard disks, optical disks, flash memory, or forms of electrically programmable memory (EPROM) or electrically erasable programmable (EEPROM) memory. The storage component 548 can store program instructions and / or information (eg, data) associated with
１つ以上のプロセッサ５４０は、コンピューティングデバイス５００に対応付けられた機能を実現するおよび／または命令を実行することができる。プロセッサ５４０の例は、アプリケーションプロセッサ、ディスプレイコントローラ、補助プロセッサ、１つ以上のセンサハブ、および、プロセッサ、処理ユニット、または処理デバイスとして機能するように構成された任意のその他のハードウェアを含む。モジュール５２２、５２４、および５２６は、コンピューティングデバイス５００の各種アクション、動作、または機能を実行するように、プロセッサ５４０が動作させることが可能であってもよい。たとえば、コンピューティングデバイス５００のプロセッサ５４０は、モジュール５２２、５２４、および５２６に帰する本明細書に記載の動作をプロセッサ５４０に実行させる、記憶コンポーネント５４８に格納された命令を、取り出して実行することができる。命令は、プロセッサ５４０によって実行されると、コンピューティングデバイス５００に、情報を記憶コンポーネント５４８に格納させる。
One or
目位置モジュール５２４は、車両内に位置する１つ以上のカメラから乗員の頭部または目までの距離を求めることができ、かつ、当該１つ以上のカメラに対する、３Ｄ空間内の乗員の頭部または目の位置を求めることができる。目位置モジュール５２４は、図１に関して述べた技術に従い、乗員の頭部および／または目の距離および／または位置を求めることができる。さらに、視線モジュール５２２と同様、目位置モジュール５２４の機能のうちのすべてまたは一部を、車両のカメラシステムにより、コンピューティングデバイス５００により、またはその組み合わせにより、実行することができる。
The
目位置モジュール５２４は、２つ以上の異なるカメラが取り込んだ画像間の視差角を、これら２つ以上のカメラの各々の位置はわかっていると仮定して、分析することができる。目位置モジュール５２４は、視差角およびカメラ間の距離を用いて、２つ以上のカメラのうちの１つ以上と乗員の目との間の距離を求める。別の例として、目位置モジュール５２４は、１つの赤外線カメラが取り込んだ画像を分析することにより、赤外線カメラが取りこんだ画像の歪みを求めて、赤外線カメラと乗員の目との間の距離を求めることができる。
The
目位置モジュール５２４は、この求めた２つ以上のカメラと乗員の目との間の距離と、取り込んだ画像における乗員の目の位置とに基づいて、乗員の目を、３Ｄ空間内において２つ以上のカメラに対して配置することができる。すなわち、目位置モジュール５２４は、少なくとも１つのカメラの位置に対する、車両内部の中の乗員の目の位置を、求めることができる。乗員の目の位置は、少なくとも１つのカメラを基準として定められた３Ｄ空間内の位置である。たとえば、３Ｄ空間は、球体であってもよく、カメラの位置に対応する重心を有していてもよい。このような例において、乗員の目の位置は、（ｘ，ｙ，ｚ）座標で定められてもよく、（０，０，０）は、この球体の重心として使用されているカメラの位置である。このような座標は「カメラベースの座標系」内に位置していると言うことができる。
The
視線モジュール５２２は、図１および図３に関して述べた技術に従い、車両の乗員の３Ｄ視線ベクトルを求めることができる。コンピューティングデバイス５００のコンポーネントとして示されているが、さまざまな例において、視線モジュール５２２の機能は、コンピューティングデバイス５００による実行の代わりにまたはコンピューティングデバイス５００による実行に加えて、車両のカメラシステムによって実行されてもよい。さらに、カメラシステムおよび視線モジュール５２２は、３Ｄ視線ベクトル決定プロセスの個々の部分を実行してもよい。
The line-of-
さまざまな場合において、視線モジュール５２２は、アイトラッキングを実行することによって第１の初期３Ｄ視線ベクトルを求めることができ、乗員の顔面平面を求めることによって第２の初期３Ｄ視線ベクトルを求めることができる。視線モジュール５２２は、第１および第２の初期３Ｄ視線ベクトルを組み合わせることにより、乗員の最終的な３Ｄ視線ベクトルを求めることができる。視線モジュール５２２は、目の位置および瞳孔または目のその他の特徴の動き（すなわちアイトラッキング）を用いて、第１の初期３Ｄ視線ベクトルを求めることができる。しかしながら、さまざまな場合において、乗員の目が遮られているためにカメラが取り込んだ画像が乗員の目の明瞭な画像を含んでいないことがある。よって、視線モジュール５２２は、乗員の目の位置および動きの追跡に頼るだけではなく、取り込んだ画像をさらに分析することによって乗員の画像平面を求めてもよい。
In various cases, the line-of-
乗員の顔面平面の計算において、視線モジュール５２２は、カメラ１０２のうちの１つ以上が取り込んだ１つ以上の画像において複数の顔面ランドマークを識別することができる。顔面ランドマークは、口、目、鼻、耳、眉、あご、またはそれ以外の顔の特徴の輪郭を含み得る。視線モジュール５２２は、識別した顔面ランドマークを用い、各種顔面ランドマーク間の幾何学的整合性に基づいて、画像に含まれる乗員の顔が何らかのピッチ、ロール、またはヨーを示しているか否かを判断することができる。たとえば、乗員の口と目との間の総距離と比較して、乗員の２つの目の間の距離が、乗員が真っすぐ前方を見ているときよりも短い場合、視線モジュール５２２は、乗員が左または右を見ていると判断する。視線モジュール５２２は、画像は乗員の右耳を含んでいるが左耳は含んでいないと判断した場合、乗員が左を見ていると判断する。顔面平面のピッチ角、ロール角、およびヨー角は、顔面ランドマーク間の距離の相対的変化に基づいて判断することができる。視線モジュール５２２は、求めた顔面平面のピッチ角、ロール角、およびヨー角を用いて、第２の初期３Ｄ視線ベクトルを求めることができる。
In calculating the facial plane of the occupant, the line-of-
さまざまな場合において、視線モジュール５２２は、トレーニングされた機械学習モデルを画像に適用することにより、顔面平面を求めることができる。機械学習モデルは、既に識別されている顔面ランドマークを有し顔面平面の角度が既に求められている他の人々の画像を用いてトレーニングすることができる。機械学習モデルは、ユーザフィードバックと、視線モジュール５２２から与えられたフィードバックとに基づいて、継続的に学習することができる。たとえば、視線モジュール５２２は、顔面平面を用いて求めた第２の３Ｄ視線ベクトルと比較される、アイトラッキングを用いて求めた第１の初期３Ｄ視線ベクトルに基づいて、機械学習モデルのパラメータをランク付けまたは調整することができる。
In various cases, the line-of-
視線モジュール５２２は、第１および第２の初期３Ｄ視線ベクトルを求めると、第１および第２の初期３Ｄ視線ベクトル双方に基づいて、乗員の３Ｄ視線ベクトルを求めることができる。さまざまな場合において、視線モジュール５２２は、第１および第２の初期３Ｄ視線ベクトルの平均を用いて３Ｄ視線ベクトル１１２を求めることができる。その他の例において、視線モジュール５２２は、第１および第２の３Ｄ視線ベクトルのうちの１つ以上に重み付けを適用し重み付けされた値を用いることによって３Ｄ視線ベクトルを求めることができる。視線モジュール５２２は、第１または第２の初期３Ｄ視線ベクトルが正確に求められた確信度に基づいて、第１および第２の初期３Ｄ視線ベクトルに適用する重みを求めることができる。たとえば、視線モジュール５２２が顔面ランドマークのうちの一部のみを検出した場合（たとえば目と鼻だけ、口またはあごは検出なし）、顔面平面のピッチ角、ロール角、およびヨー角の誤差範囲が大きくなる可能性がある。このため、視線モジュール５２２は、小さくした重みの値を第２の初期３Ｄ視線ベクトルに適用し大きくした重みの値をアイトラッキングを用いて求めた第１の初期３Ｄ視線ベクトルに適用することができる。
When the line-of-
第１および第２の初期３Ｄ視線ベクトル双方が必要と説明しているが、視線モジュール５２２は、第１および第２の初期３Ｄ視線ベクトルのうちの一方を用いて３Ｄ視線ベクトルを求めることもできる。たとえば、２つ以上のカメラが取り込んだ画像において乗員の目が閉じられているまたは乗員がサングラスを着用している場合、視線モジュール５２２は、第２の初期３Ｄ視線ベクトル（すなわち乗員の顔面平面のピッチ、ロール、およびヨーに基づいて求めた３Ｄ視線ベクトル）を、求めた３Ｄ視線ベクトルとして使用してもよい。別の例として、２つ以上のカメラが取り込んだ画像に、（たとえば乗員の手によって）顔が部分的に隠れている乗員が含まれている場合、視線モジュール５２２は、第１の初期３Ｄ視線ベクトル（すなわちアイトラッキングを用いて求めた３Ｄ視線ベクトル）を３Ｄ視線ベクトルとして使用してもよい。
Although it is explained that both the first and second initial 3D line-of-sight vectors are required, the line-of-
関心領域モジュール５２６は、図１および図４に関して述べた技術に従い、車両の乗員が見ているのはどの関心領域であるかを判断することができる。関心領域モジュール５２６は、車両データ５２８から車両固有データをロードすることができる。車両データ５２８は、乗員がどの関心領域を見ているかを関心領域モジュールが判断するために使用できるテキスト情報または符号化された情報を格納するのに適した、ファイル、データベース、またはその他のデータ構造等の、任意の種類のデータストアであればよい。車両データは、車両のさまざまな関心領域に対応付けられた２次元平面を画定する座標を含む。いくつかの場合において、各平面は、車両の異なる物理的要素（たとえばバックミラー、ヘッドユニットディスプレイ、計器パネルなど）に対応付けられていてもよく、または、車両の同一の物理的要素の異なる部分（たとえばフロントガラスの異なる領域）に対応付けられていてもよい。 The region of interest module 526 can determine which region of interest the vehicle occupant is looking at according to the techniques described with respect to FIGS. 1 and 4. The region of interest module 526 can load vehicle-specific data from vehicle data 528. The vehicle data 528 is a file, database, or other data structure suitable for storing textual or encoded information that can be used by the region of interest module to determine which region of interest the occupant is looking at. Any kind of data store such as, etc. may be used. Vehicle data includes coordinates that define a two-dimensional plane associated with the various regions of interest of the vehicle. In some cases, each plane may be associated with a different physical element of the vehicle (eg, rearview mirror, head unit display, instrument panel, etc.), or different parts of the same physical element of the vehicle. It may be associated with (for example, different regions of the windshield).
関心領域モジュールは、３Ｄ視線ベクトル情報を視線モジュール５２２から受けることができ、かつ、乗員の頭部および／または目の位置情報を目位置モジュール５２４から受けることができる。関心領域モジュール５２６は、頭部および／または目位置情報を、カメラベースの座標系から車両ベースの座標系に変換することができる。座標を車両ベースの座標系に変換することにより、関心領域モジュール５２６は、車両データにおいて特定されている車両のさまざまな物理的物体の座標位置に対する、乗員の頭部および／または目の位置を特定することができる。関心領域モジュール５２６は、３Ｄ視線ベクトルを、乗員の頭部および／または目の、車両ベースの座標系の位置から延ばし、この３Ｄ視線ベクトルが交差する１つ以上の平面を求めることができる。関心領域モジュール５２６は、交差した面を、乗員が見ている関心領域として識別する。
The region of interest module can receive 3D line-of-sight vector information from the line-of-
いくつかの場合において、関心領域モジュール５２６は、車両の乗員がある時間にわたってどの関心領域を見ているかを、定期的にまたは連続的に判断してもよい。関心領域モジュール５２６は、乗員がどの関心領域を見ているかをモニタリングすることにより、乗員の関与レベルを判断することができるとともに、乗員がどれほど良く交通規則および安全運転のための最良の実施に従っているかを判断することができる。たとえば、乗員がサイドミラーを見ずに車線変更した場合、またはバックミラーを見ずに車両を後退させた場合、コンピューティングデバイス５００は、乗員が安全のための最善の実施に従って車両を運転していないと判断することができる。別の例として、車両が動いているときに乗員が長時間にわたってコンピューティングデバイス５００のディスプレイを見ている場合、コンピューティングデバイス５００は、ロックアウトする、または乗員がコンピューティングデバイス５００とやり取りするのを禁止することができる。 In some cases, the region of interest module 526 may periodically or continuously determine which region of interest the vehicle occupant is looking at over a period of time. The area of interest module 526 can determine the level of involvement of the occupant by monitoring which area of interest the occupant is looking at, and how well the occupant follows traffic rules and best practices for safe driving. Can be judged. For example, if the occupant changes lanes without looking at the side mirrors, or if the vehicle is retracted without looking at the rear-view mirrors, the computing device 500 will allow the occupants to drive the vehicle according to the best practices for safety. It can be judged that there is no such thing. As another example, if the occupant is looking at the display of the computing device 500 for an extended period of time while the vehicle is moving, the computing device 500 locks out or the occupant interacts with the computing device 500. Can be banned.
視線モジュール５２２、目位置モジュール５２４、および関心領域モジュール５２６が実行する技術のうちの１つ以上を、機械学習を用いて実行してもよい。図５には示されていないが、コンピューティングデバイス５００は、カメラが取り込んだ画像に適用されるトレーニングされた１つ以上の機械学習モジュールを含み得る。
One or more of the techniques performed by the line-of-
図６Ａ〜図６Ｅは、本開示の実装例に係る、機械学習済モデルの一例の局面を示す概念図である。以下、図６Ａ〜図６Ｅを、図１の車両１００の文脈で説明する。たとえば、いくつかの場合において、以下で言及する機械学習済モデル６００は、３Ｄ視線ベクトル、乗員の頭部および／または目の位置、乗員が見ている関心領域、またはそのコンポーネントをカメラシステムまたは車両コンピューティングシステム１０４が求めるために使用する、任意のモデルの一例であってもよい。
6A to 6E are conceptual diagrams showing an example of a machine-learned model according to the implementation example of the present disclosure. Hereinafter, FIGS. 6A to 6E will be described in the context of the
図６Ａは、本開示の実装例に係る、機械学習済モデルの一例の概念図を示す。図６Ａに示されるように、いくつかの実装例において、機械学習済モデル６００は、１つ以上の種類の入力データを受け、それに応じて１つ以上の種類のデータを出力するようにトレーニングされる。よって、図６Ａは推論を実行する機械学習済モデル６００を示す。
FIG. 6A shows a conceptual diagram of an example of a machine-learned model according to the implementation example of the present disclosure. As shown in FIG. 6A, in some implementation examples, the machine-learned
入力データは、ある場合またはある例に対応付けられた１つ以上の特徴を含み得る。いくつかの実装例において、この場合または例に対応付けられた１つ以上の特徴を、特徴ベクトルに組織することができる。いくつかの実装例において、出力データは１つ以上の予測を含み得る。予測（prediction）は推論（inference）と言うこともできる。したがって、特定の場合に対応付けられた特徴が与えられると、機械学習済モデル６００は、この特徴に基づいてこのような場合についての予測を出力することができる。
The input data may include one or more features associated with some cases or examples. In some implementation examples, one or more features in this case or associated with the example can be organized into feature vectors. In some implementations, the output data may contain one or more predictions. Prediction can also be called inference. Therefore, given a feature associated with a particular case, the machine-learned
機械学習済モデル６００は、種類が異なる各種の機械学習済モデルのうちの１つ以上であってもよい、またはこれを含み得る。特に、いくつかの実装例において、機械学習済モデル６００は、分類、回帰、クラスタリング、異常検知、レコメンデーション生成、顔ランドマーク検出、３Ｄ視線の決定、および／またはその他のタスクを実行することができる。
The machine-learned
いくつかの実装例において、機械学習済モデル６００は、入力データに基づいて、さまざまな種類の分類を実行することができる。たとえば、機械学習済モデル６００は、二項分類または多クラス分類を実行することができる。二項分類の場合、出力データは、入力データを異なる２つのクラスのうちの一方に分類したものを含み得る。多クラス分類の場合、出力データは、入力データを３つ以上のクラスのうちの１つ（以上）に分類したものを含み得る。分類は、シングルラベルまたはマルチラベルであってもよい。機械学習済モデル６００は、離散カテゴリ分類を実行してもよく、この分類では入力データを単純に１つ以上のクラスまたはカテゴリに分類する。
In some implementation examples, the machine-learned
いくつかの実装例において、入力データは対応するクラスに分類されるべきであると考えられる程度を記述する数値を１つ以上のクラス各々について機械学習済モデル６００が提供する、という分類を、機械学習済モデル６００は実行することができる。いくつかの場合において、機械学習済モデル６００が提供する数値を、それぞれのクラスへの入力の分類に対応付けられたそれぞれの確信度を示す「確信度スコア」と呼ぶことができる。いくつかの実装例において、確信度スコアを１つ以上のしきい値と比較することにより、離散カテゴリ予測を提供することができる。いくつかの実装例において、確信度スコアが相対的に最も大きい、特定数（たとえば１つ）のクラスのみを選択して離散カテゴリ予測を提供することができる。
In some implementations, the machine learning classification that the machine-learned
機械学習済モデル６００は確率的予測を出力することができる。たとえば、機械学習済モデル６００は、サンプル入力を与えられて、１組のクラスについての確率分布を予測することができる。したがって、機械学習済モデル６００は、サンプル入力が属すべき最も可能性が高いクラスだけを出力するのではなく、クラスごとに、サンプル入力がこのようなクラスに属する確率を出力することができる。いくつかの実装例において、可能性のあるすべてのクラスについての確率分布の総和は１になり得る。いくつかの実装例において、Ｓｏｆｔｍａｘ関数またはその他の種類の関数もしくはレイヤを用いることにより、可能なクラスにそれぞれ対応付けられた１組の実数値を、合計が１になる、範囲（０，１）の１組の実数値に、スカッシュする（squash）ことができる。
The machine-learned
いくつかの例において、確率分布が提供する確率を、１つ以上のしきい値と比較することにより、離散カテゴリ予測を提供することができる。いくつかの実装例において、予測された確率が相対的に最も高い、特定数（たとえば１つ）のクラスのみを選択して、離散カテゴリ予測を提供することができる。 In some examples, discrete category predictions can be provided by comparing the probabilities provided by the probability distribution with one or more thresholds. In some implementations, it is possible to select only a specific number (eg, one) of classes with the relatively highest predicted probabilities to provide discrete category predictions.
機械学習済モデル６００が分類を実行する場合、機械学習済モデル６００を、教師あり学習技術を用いてトレーニングしてもよい。たとえば、機械学習済モデル６００を、１つ以上のクラスに属する（または属さない）ものとしてラベル付けされたトレーニング例を含むトレーニングデータセットについてトレーニングすることができる。教師ありトレーニング技術に関するさらに他の詳細は、以下図６Ｂ〜図６Ｅの説明において示される。
If the machine-learned
いくつかの実装例において、機械学習済モデル６００は、回帰を実行することにより、連続する数値の形態の出力データを提供することができる。連続する数値は、たとえば通貨の値、スコア、またはその他の数値表現を含む、任意の数の異なるメトリックまたは数値表現に対応し得る。例として、機械学習済モデル６００は、線形回帰、多項回帰、または非線形回帰を実行することができる。例として、機械学習済モデル６００は、単純回帰または重回帰を実行することができる。先に述べたように、いくつかの実装例において、Ｓｏｆｔｍａｘ関数またはその他の関数もしくはレイヤを用いることにより、２つ以上の可能なクラスにそれぞれ対応付けられた１組の実数値を、合計が１になる、範囲（０，１）の１組の実数値に、スカッシュする（squash）ことができる。
In some implementations, the machine-learned
機械学習済モデル６００は、さまざまな種類のクラスタリングを実行することができる。たとえば、機械学習済モデル６００は、入力データが対応する可能性が最も高い、過去に定められた１つ以上のクラスタを識別することができる。機械学習済モデル６００は、入力データ内の１つ以上のクラスタを識別することができる。すなわち、入力データが複数のオブジェクト、文書、またはその他のエンティティを含む場合、機械学習済モデル６００は、入力データに含まれる複数のエンティティを、複数のクラスタにソートすることができる。機械学習済モデル６００がクラスタリングを実行するいくつかの実装例において、機械学習済モデル６００を、教師なし学習技術を用いてトレーニングすることができる。
The machine-learned
機械学習済モデル６００は、異常検知または外れ値検知を実行することができる。たとえば、機械学習済モデル６００は、予測されたパターンまたはその他の特徴（たとえば過去の入力データから過去に観察されたもの）と一致しない入力データを識別することができる。例として、異常検知は不正検知またはシステム障害検知に使用することができる。
The machine-learned
いくつかの実装例において、機械学習済モデル６００は、１つ以上のオブジェクト位置の形態の出力データを提供することができる。たとえば、機械学習済モデル６００は、３Ｄ視線ベクトル決定システムに含めることができる。一例として、機械学習済モデル６００は、特定の顔ランドマークについての過去の結果（たとえば、顔ランドマークの位置を示すスコア、ランキング、またはレーティング）を記述する入力データを与えられると、新たな画像における顔ランドマークの位置を出力することができる。一例として、図１の車両コンピューティングシステム１０４等のコンピューティングシステムは、図１の車両１００の乗員の１つ以上の画像等の、車両の乗員を記述する入力データが与えられると、乗員の顔ランドマークの位置を出力することができる。
In some implementation examples, the machine-learned
機械学習済モデル６００は、場合によっては環境内のエージェントの機能を果たすことができる。たとえば、機械学習済モデル６００を強化学習を用いてトレーニングすることができる。その詳細は以下で述べる。
The machine-learned
いくつかの実装例において、機械学習済モデル６００はパラメータモデルであってもよく、他の実装例において、機械学習済モデル６００は非パラメータモデルであってもよい。いくつかの実装例において、機械学習済モデル６００は線形モデルであってもよく、他の実装例において、機械学習済モデル６００は非線形モデルであってもよい。
In some implementation examples, the machine-learned
先に述べたように、機械学習済モデル６００は、さまざまな異なる種類の機械学習済モデルのうちの１つ以上であってもよい、またはこれを含み得る。このような異なる種類の機械学習済モデルの例を、説明のために以下に示す。下記のモデルの例のうちの１つ以上を使用する（たとえば組み合わせる）ことにより、入力データに応じて出力データを提供することができる。下記のモデルの例の範囲外のその他のモデルも同様に使用できる。
As mentioned earlier, the machine-learned
いくつかの実装例において、機械学習済モデル６００は、たとえば線形分類モデル、二次分類モデルなどのような、１つ以上の分類器モデルであってもよい、またはこれを含み得る。機械学習済モデル６００は、たとえば単純線形回帰モデル、多線形回帰モデル、ロジスティック回帰モデル、ステップワイズ回帰モデル、多変量適応的回帰スプライン、局所推定スキャタープロット平滑化モデルなどのような、１つ以上の回帰モデルであってもよい、またはこれを含み得る。
In some implementations, the machine-learned
いくつかの例において、機械学習済モデル６００は、たとえば、分類木および／または回帰木、反復二項化３（iterative dichotomiser 3）決定木、Ｃ４．５決定木、カイ二乗自動相互作用検出決定木、決定スタンプ、条件付き決定木などのような、決定木に基づくモデルであってもよい、またはこれを含み得る。
In some examples, the machine-learned
機械学習済モデル６００は、１つ以上のカーネルマシンであってもよい、またはこれを含み得る。いくつかの実装例において、機械学習済モデル６００は、１つ以上のサポートベクターマシンであってもよい、またはこれを含み得る。機械学習済モデル６００は、たとえば、学習ベクトル量子化モデル、自己組織化マップモデル、局所重み付き学習モデルなどのような、１つ以上のインスタンスベースの学習モデルであってもよい、またはこれを含み得る。いくつかの実装例において、機械学習済モデル６００は、たとえば、ｋ近傍分類モデル、ｋ近傍回帰モデルなどのような、１つ以上の近傍モデルであってもよい、またはこれを含み得る。機械学習済モデル６００は、たとえば、ナイーブベイズ（naive Bayes）モデル、ガウスナイーブベイズモデル、多項ナイーブベイズモデル、平均１依存推定器、ベイジアンネットワーク、ベイジアンビリーフネットワーク、隠れマルコフモデルなどのような、１つ以上のベイジアンモデルであってもよい、またはこれを含み得る。
The machine-learned
いくつかの実装例において、機械学習済モデル６００は、１つ以上の人工ニューラルネットワーク（簡単にニューラルネットワークとも呼ばれる）であってもよい、またはこれを含み得る。ニューラルネットワークは、ニューロンまたはパーセプトロンと呼ぶこともできる、接続された一群のノードを含み得る。ニューラルネットワークは、１つ以上の層に組織することができる。複数の層を含むニューラルネットワークを「ディープ」ネットワークと呼ぶことができる。ディープネットワークは、入力層と、出力層と、入力層と出力層の中に配置された１つ以上の隠れ層とを含み得る。ニューラルネットワークのノードは、接続されていてもよい、または不完全に接続されていてもよい。
In some implementation examples, the machine-learned
機械学習済モデル６００は、１つ以上のフィードフォワードニューラルネットワークであってもよい、またはこれを含み得る。フィードフォワードネットワークにおいて、ノード間の接続はサイクルを形成しない。たとえば、各接続は、前の層のノードを後の層のノードに接続することができる。
The machine-learned
いくつかの場合において、機械学習済モデル６００は、１つ以上の回帰型ニューラルネットワークであってもよい、またはこれを含み得る。いくつかの場合において、再帰型ニューラルネットワークのノードのうちの少なくとも一部はサイクルを形成することができる。再帰型ニューラルネットワークは、本質的に逐次的である入力データの処理に特に有用である。特に、いくつかの場合において、再帰型ニューラルネットワークは、入力データシーケンスの前の部分から、入力データシーケンスの後の部分へと、再帰または有向循環ノード接続の使用を通じて、情報を送るまたは保持することができる。
In some cases, the machine-learned
いくつかの例において、逐次入力データは、時系列データ（たとえば時間に対するセンサデータ、または異なる時間に取り込まれた画像）を含み得る。非限定的な一例として、たとえば、再帰型ニューラルネットワークは、時間に対するセンサデータを分析することにより、乗員が見ている関心領域の変化を検出することができる。 In some examples, the sequential input data may include time series data (eg, sensor data for time, or images captured at different times). As a non-limiting example, recurrent neural networks, for example, can detect changes in the region of interest seen by the occupant by analyzing sensor data over time.
再帰型ニューラルネットワークの例は、長・短期（long short-term）（ＬＳＴＭ）再帰型ニューラルネットワーク、ゲート付き再帰型ユニット、双方向性再帰型ニューラルネットワーク、連続時間再帰型ニューラルネットワーク、ニューラルヒストリコンプレッサ、エコーステートネットワーク、エルマンネットワーク、ジョーダンネットワーク、リカーシブニューラルネットワーク、ホップフィールドネットワーク、完全再帰型ネットワーク、シーケンス・トゥー・シーケンス構成などを含む。 Examples of recurrent neural networks are long short-term (LSTM) recurrent neural networks, gated recurrent units, bidirectional recurrent neural networks, continuous time recurrent neural networks, neural history compressors, etc. Includes echo state networks, Elman networks, Jordan networks, recurrent neural networks, hop field networks, fully recursive networks, sequence-to-sequence configurations, and more.
いくつかの実装例において、機械学習済モデル６００は、１つ以上の畳み込みニューラルネットワークであってもよい、またはこれを含み得る。いくつかの場合において、畳み込みニューラルネットワークは、学習済フィルタを用いて入力データに対し畳み込みを実行する１つ以上の畳み込み層を含み得る。
In some implementation examples, the machine-learned
フィルタはカーネルと呼ぶこともできる。畳み込みニューラルネットワークは、入力データが静止画像または動画等の画像を含む場合などの視覚問題には特に有用である可能性がある。しかしながら、畳み込みニューラルネットワークは自然言語処理に適用することもできる。 The filter can also be called the kernel. Convolutional neural networks may be particularly useful for visual problems, such as when the input data contains images such as still images or moving images. However, convolutional neural networks can also be applied to natural language processing.
いくつかの例において、機械学習済モデル６００は、たとえば敵対的生成ネットワーク等の１つ以上の生成ネットワークであってもよい、またはこれを含み得る。生成ネットワークを用いることにより、新たな画像またはその他のコンテンツ等の新たなデータを生成することができる。
In some examples, the machine-learned
機械学習済モデル６００は、オートエンコーダであってもよい、またはこれを含み得る。いくつかの場合において、オートエンコーダの目的は、典型的には次元削減のために、１組のデータの表現（たとえばより低次元の符号化）を学習することである。たとえば、いくつかの場合において、オートエンコーダは、入力データを符号化し入力データを符号化から再構成した出力データを提供することを求めることができる。最近、オートエンコーダの概念は、データの生成モデルの学習により広く使用されるようになっている。いくつかの場合において、オートエンコーダは、入力データの再構成を超える追加の損失を含み得る。
The machine-learned
機械学習済モデル６００は、たとえばディープボルツマンマシン、ディープビリーフネットワーク、積層オートエンコーダなどのような、１つ以上の他の形態の人工ニューラルネットワークであってもよい、またはこれを含み得る。本明細書に記載のニューラルネットワークのいずれかを組み合わせる（たとえば積層する）ことにより、より複雑なネットワークを形成することができる。
The machine-learned
１つ以上のニューラルネットワークを使用することにより、入力データに基づいた埋め込みを提供することができる。たとえば、埋め込みは、入力データから抽出した知識を１つ以上の学習済の次元にして表すことであってもよい。いくつかの場合において、埋め込みは、関連するエンティティを識別するための有用なソースとなり得る。いくつかの場合において、埋め込みはネットワークの出力から抽出することができ、他の場合において、埋め込みはネットワークの任意の隠れノードまたは層（たとえばネットワークの最後の層に近いが最後の層ではない）から抽出することができる。埋め込みは、次のビデオのオートサジェスト、プロダクトサジェスト、エンティティまたはオブジェクト認識などを実行するのに有用となり得る。いくつかの場合において、埋め込みは、ダウンストリームモデルの入力に有用である。たとえば、埋め込みは、ダウンストリームモデルまたは処理システムの入力データ（たとえばサーチクエリ）を生成するのに有用となり得る。 By using one or more neural networks, it is possible to provide embedding based on the input data. For example, embedding may represent the knowledge extracted from the input data in one or more learned dimensions. In some cases, embedding can be a useful source for identifying related entities. In some cases the embedding can be extracted from the output of the network, in other cases the embedding is from any hidden node or layer of the network (eg close to the last layer of the network but not the last layer). Can be extracted. Embedding can be useful for performing auto-suggestions, product suggestions, entity or object recognition, etc. for the next video. In some cases, embedding is useful for inputting downstream models. For example, embedding can be useful in generating input data for downstream models or processing systems (eg search queries).
機械学習済モデル６００は、たとえば、ｋ平均クラスタリングモデル、ｋ中央値クラスタリングモデル、予測最大化モデル、階層クラスタリングモデルなどのようなクラスタリングモデルを１つ以上含み得る。
The machine-learned
いくつかの実装例において、機械学習済モデル６００は、たとえば主成分分析、カーネル主成分分析、グラフベースのカーネル主成分分析、主成分回帰、部分的最小二乗回帰、サモンマッピング、多次元スケーリング、射影追跡、線形判別分析、混合判別分析、二次判別分析、一般化判別分析、フレキシブル判別分析、オートエンコードなどのような、次元削減技術を１つ以上実行することができる。
In some implementation examples, the machine-learned
いくつかの実装例において、機械学習済モデル６００は、マルコフ決定過程、動的プログラミング、Ｑ関数またはＱ学習、価値関数アプローチ、ディープＱネットワーク、微分可能ニューラルコンピュータ、非同期アドバンテージアクター・クリティック、決定型方策勾配法などのような強化学習技術を１つ以上実行する、または受けることができる。
In some implementation examples, the machine-learned
いくつかの実装例において、機械学習済モデル６００は自己回帰モデルであってもよい。いくつかの場合において、自己回帰モデルは、出力データが、自身の過去の値と確率項とに線形的に依存することを示すことができる。いくつかの場合において、自己回帰モデルは、確率微分方程式の形態を取ることができる。自己回帰モデルの一例は、生の音声の生成モデルであるＷａｖｅＮｅｔである。
In some implementation examples, the machine-learned
いくつかの実装例において、機械学習済モデル６００は、マルチモデルアンサンブルを含み得る、またはその一部を形成し得る。一例として、「バギング（bagging）」と呼ぶこともできるブートストラップ集約を実行することができる。ブートストラップ集約では、（たとえば置換ありのランダムサンプリングを通じて）トレーニングデータセットを複数のサブセットに分割し、複数のモデルをそれぞれ複数のサブセットについてトレーニングする。推論時に、複数モデルそれぞれの出力を（たとえば平均、投票、またはその他の技術により）組み合わせてアンサンブルの出力として使用することができる。
In some implementation examples, the machine-learned
アンサンブルの一例は、ランダム決定フォレストと呼ぶこともできるランダムフォレストである。ランダムフォレストは、分類、回帰、およびその他のタスクのためのアンサンブル学習方法である。ランダムフォレストは、トレーニング時に複数の決定木を生成することによって生成する。いくつかの場合において、推論時に、個々の木の、クラス（分類）または平均予測（回帰）のモードであるクラスを、フォレストの出力として使用することができる。ランダム決定フォレストは、そのトレーニングセットにオーバーフィットする決定木の傾向を修正することができる。 An example of an ensemble is a random forest, which can also be called a random decision forest. Random forest is an ensemble learning method for classification, regression, and other tasks. Random forests are created by generating multiple decision trees during training. In some cases, at the time of inference, a class of individual trees, which is a mode of class (classification) or average prediction (regression), can be used as the output of the forest. Random decision forests can correct the tendency of decision trees to overfit their training set.
アンサンブル技術の別の例はスタッキングであり、これはいくつかの場合ではスタック一般化（stacked generalization）と呼ばれることもある、スタッキングである。スタッキングは、コンバイナモデルをトレーニングすることにより、その他いくつかの機械学習済モデルの予測をブレンドするかそうでなければ組み合わせることを含む。このようにして、（たとえば同一または異なる種類の）複数の機械学習済モデルをトレーニングデータに基づいてトレーニングすることができる。加えて、コンバイナモデルをトレーニングすることにより、他の機械学習済モデルの予測を入力として取り込み、それに応じて、最終的な推論または予測を生成することができる。いくつかの場合において、単層ロジスティック回帰モデルをコンバイナモデルとして使用することができる。 Another example of an ensemble technique is stacking, which in some cases is sometimes referred to as stacked generalization. Stacking involves blending or otherwise combining the predictions of several other machine-learned models by training the combiner model. In this way, multiple machine-learned models (eg, the same or different types) can be trained based on training data. In addition, by training the combiner model, the predictions of other machine-learned models can be taken as input and the final inference or prediction can be generated accordingly. In some cases, a single-layer logistic regression model can be used as a combiner model.
もう１つのアンサンブル技術の例はブースティングである。ブースティングは、弱いモデルを繰り返しトレーニングしてから最終的な強いモデルに追加することにより、アンサンブルを増分的に構築することを含み得る。たとえば、いくつかの場合において、新たな各モデルをトレーニングすることにより、過去のモデルが誤解釈（たとえば誤分類）されたというトレーニング例を強調することができる。たとえば、このような誤解釈された例各々に対応付けられた重みを増すことができる。ブースティングの一般的な一実装例は、適応型ブースティングと呼ぶこともできるＡｄａＢｏｏｓｔである。ブースティング技術の他の例は、ＬＰＢｏｏｓｔ、ＴｏｔａｌＢｏｏｓｔ、ＢｒｏｗｎＢｏｏｓｔ、ｘｇｂｏｏｓｔ、ＭａｄａＢｏｏｓｔ、ＬｏｇｉｔＢｏｏｓｔ、勾配ブースティング（gradient boosting）などを含む。さらに、上記モデル（たとえば回帰モデルおよび人工ニューラルネットワーク）のうちのいずれかを組み合わせることによりアンサンブルを形成することができる。一例として、アンサンブルは、アンサンブルを形成するモデルの出力を組み合わせるおよび／または重み付けするためのトップレベル機械学習済モデルまたは発見的機能を含み得る。 Another example of ensemble technology is boosting. Boosting may include building an ensemble incrementally by repeatedly training a weak model and then adding it to the final strong model. For example, in some cases, training each new model can highlight a training example in which a past model was misinterpreted (eg, misclassified). For example, the weight associated with each such misinterpreted example can be increased. A common implementation of boosting is AdaBoost, which can also be referred to as adaptive boosting. Other examples of boosting techniques include LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, gradient boosting and the like. Furthermore, an ensemble can be formed by combining any of the above models (for example, a regression model and an artificial neural network). As an example, an ensemble may include a top-level machine-learned model or heuristic function to combine and / or weight the outputs of the models that form the ensemble.
いくつかの実装例において、（たとえばアンサンブルを形成する）複数の機械学習済モデルを（たとえばモデルアンサンブルを通した逐次的な誤差逆伝播により）共にリンクさせトレーニングすることができる。しかしながら、いくつかの実装例では、共にトレーニングしたモデルのサブセット（たとえば１つ）のみを推論に使用する。 In some implementations, multiple machine-learned models (eg, forming an ensemble) can be linked and trained together (eg, by sequential error backpropagation through the model ensemble). However, some implementations use only a subset of the models trained together (eg, one) for inference.
いくつかの実装例において、機械学習済モデル６００を用いて、入力データを、次に別のモデルに入力するために、前処理することができる。たとえば、機械学習済モデル６００は、次元削減技術および埋め込み（たとえば行列分解、主成分分析、特異値分解、ｗｏｒｄ２ｖｅｃ／ＧＬＯＶＥ、および／または関連する手法）、ならびにクラスタリングを実行することができ、また、ダウンストリーム消費のための分類および回帰さえも実行することができる。これらの技術のうちの多くは先に説明した通りであり以下ではこれ以上説明しない。
In some implementation examples, the machine-learned
上述のように、機械学習済モデル６００は、入力データを受けてそれに応じて出力データを提供するようにトレーニングするかそうでなければ構成することができる。入力データは、異なる種類もしくは形態の入力データ、または入力データの変形を含み得る。例として、各種実装例において、入力データは、最初にユーザが選択したコンテンツ（またはコンテンツの一部）を記述する特徴を含み得る。コンテンツは、たとえば、ユーザが選択した文書または画像のコンテンツ、ユーザ選択を示すリンク、デバイスまたはクラウド上で利用可能なその他のファイルに関連するユーザ選択内のリンク、ユーザ選択のメタデータなどである。加えて、ユーザの許可の下で、入力データは、アプリケーション自体からまたはその他のソースから得たユーザ使用のコンテキストを含む。使用コンテキストの例は、シェアの範囲（公的に共有、または大きなグループと共有、または私的に共有、または特定の人物と共有）、シェアのコンテキストなどを含む。ユーザに許可された場合、追加の入力データは、デバイスの状態、たとえばデバイスの位置、デバイス上で実行されているアプリケーションなどを含み得る。
As mentioned above, the machine-learned
いくつかの実装例において、機械学習済モデル６００は、入力データをその生の形態で受けて使用することができる。いくつかの実装例において、生の入力データを前処理することができる。よって、生の入力データに加えてまたはその代わりに、機械学習済モデル６００は、前処理された入力データを受けて使用することができる。
In some implementations, the machine-learned
いくつかの実装例において、入力データを前処理することは、生の入力データから１つ以上の追加の特徴を抽出することを含み得る。たとえば、特徴抽出技術を入力データに適用することにより、１つ以上の新たな追加特徴を生成することができる。特徴抽出技術の例は、エッジ検出、コーナー検出、ブロブ（blob）検出、リッジ（ridge）検出、スケール不変特徴量変換、モーション検出、光フロー、ハフ変換などを含む。 In some implementation examples, preprocessing the input data may include extracting one or more additional features from the raw input data. For example, by applying a feature extraction technique to the input data, one or more new additional features can be generated. Examples of feature extraction techniques include edge detection, corner detection, blob detection, ridge detection, scale-invariant feature quantity conversion, motion detection, optical flow, Hough transform, and the like.
いくつかの実装例において、抽出した特徴は、入力データを他のドメインおよび／または次元に変換したものを含み得る、またはこの変換したものから導出することができる。一例として、抽出した特徴は、入力データを周波数ドメインに変換したものを含み得る、または変換したものから導出することができる。たとえば、ウェーブレット変換および／または高速フーリエ変換を入力データに対して実行することにより、追加の特徴を生成することができる。 In some implementation examples, the extracted features may include or derive from this transformation of the input data into other domains and / or dimensions. As an example, the extracted features may include or derive from the converted input data into a frequency domain. For example, wavelet transforms and / or fast Fourier transforms can be performed on the input data to generate additional features.
いくつかの実装例において、抽出した特徴は、入力データから計算した統計、または入力データの特定の部分または次元を含み得る。統計の例は、入力データまたはその部分の、モード、平均値、最大値、最小値、またはその他のメトリックを含む。 In some implementation examples, the extracted features may include statistics calculated from the input data, or specific parts or dimensions of the input data. Examples of statistics include modes, averages, maximums, minimums, or other metrics of the input data or parts thereof.
いくつかの実装例において、上述のように、入力データは本質的に逐次的である。いくつかの場合において、逐次入力データは、入力データのストリームのサンプリング、そうでなければセグメンテーションにより、生成することができる。一例として、ビデオからフレームを抽出することができる。いくつかの実装例において、逐次データは要約（summarization）により非逐次データにすることができる。 In some implementations, as mentioned above, the input data is essentially sequential. In some cases, sequential input data can be generated by sampling a stream of input data, otherwise by segmentation. As an example, you can extract frames from video. In some implementations, sequential data can be made non-sequential by summarization.
別の前処理技術の例として、入力データの部分を補完（impute）することができる。たとえば、追加の合成入力データを内挿および／または外挿によって生成することができる。 As an example of another preprocessing technique, a portion of the input data can be impute. For example, additional synthetic input data can be generated by interpolation and / or extrapolation.
別の前処理技術の例として、入力データのうちの一部またはすべてを、スケーリング、標準化、正規化、一般化、および／又は正則化することができる。正則化技術の例は、リッジ回帰、ラッソ回帰（least absolute shrinkage and selection operator）(ＬＡＳＳＯ）、エラスティックネット、最小角度回帰、交差検証、Ｌ１正則化、Ｌ２正則化などを含む。一例として、入力データのうちの一部またはすべてを、個々の特徴値各々から所定の次元の特徴値の平均値を減算してから標準偏差またはその他のメトリックで除算することにより、正規化することができる。 As an example of another preprocessing technique, some or all of the input data can be scaled, standardized, normalized, generalized, and / or regularized. Examples of regularization techniques include ridge regression, lasso regression (least absolute shrinkage and selection operator) (LASTO), elastic nets, minimum angle regression, cross-validation, L1 regularization, L2 regularization, and the like. As an example, normalizing some or all of the input data by subtracting the average value of the feature values of a given dimension from each of the individual feature values and then dividing by the standard deviation or other metric. Can be done.
別の前処理技術の例として、入力データのうちの一部またはすべてを量子化または離散化することができる。いくつかの場合において、入力データに含まれる定性的な特徴または変数を量的な特徴または変数に変換することができる。たとえば、ワンホットエンコーディング（one hot encoding）を実行することができる。 As an example of another pretreatment technique, some or all of the input data can be quantized or discretized. In some cases, qualitative features or variables contained in the input data can be converted into quantitative features or variables. For example, one hot encoding can be performed.
いくつかの例において、次元削減技術は、機械学習済モデル６００に入力する前の入力データに適用することができる。次元削減技術のいくつかの例は、先に挙げた通りであり、たとえば、主成分分析、カーネル主成分分析、グラフベースのカーネル主成分分析、主成分回帰、部分最小二乗回帰、サモンマッピング、多次元スケーリング、射影追跡、線形判別分析、混合判別分析、二次判別分析、一般化判別分析、フレキシブル判別分析、オートエンコードなどを含む。
In some examples, the dimensionality reduction technique can be applied to the input data before it is input to the machine-learned
いくつかの実装例において、トレーニング中に入力データを任意の数のやり方で意図的に変形することにより、モデルロバストネス、一般化、またはその他の品質を高めることができる。入力データ変形技術の例は、ノイズ追加、色または濃淡または色調の変更、拡大、セグメンテーション、増幅などを含む。 In some implementation examples, the input data can be deliberately transformed in any number of ways during training to enhance model robustness, generalization, or other quality. Examples of input data transformation techniques include noise addition, color or shading or toning changes, enlargement, segmentation, amplification, and the like.
機械学習済モデル６００は、入力データを受けたことに応じて出力データを提供することができる。出力データは、異なる種類もしくは形態の出力データ、または出力データの変形を含み得る。例として、各種実装例において、出力データは、最初のコンテンツ選択とともに関連して共有可能な、ユーザデバイスにローカルに格納されているかまたはクラウドに格納されたコンテンツを含み得る。
The machine-learned
上述のように、いくつかの実装例において、出力データはさまざまな種類の分類データ（たとえば二項分類、多クラス分類、シングルラベル、マルチラベル、離散分類、回帰分類、確率分類など）を含み得る、または、さまざまな種類の回帰データ（たとえば線形回帰、多項回帰、非線形回帰、単純回帰、重回帰など）を含み得る。その他の場合において、出力データは、クラスタリングデータ、異常検知データ、レコメンデーションデータ、または上記その他の形態の出力データのうちのいずれかを、含み得る。 As mentioned above, in some implementations, the output data may contain different types of classification data (eg binomial classification, multiclass classification, single label, multilabel, discrete classification, regression classification, probability classification, etc.). , Or may contain various types of regression data (eg linear regression, polynomial regression, non-linear regression, simple regression, multiple regression, etc.). In other cases, the output data may include any of clustering data, anomaly detection data, recommendation data, or other forms of output data described above.
いくつかの実装例において、出力データは、ダウンストリームプロセスまたは意思決定に影響する可能性がある。一例として、いくつかの実装例において、出力データを、ルールベースのレギュレータによって解釈するおよび／またはその作用を受けることができる。 In some implementation examples, the output data can influence downstream processes or decision making. As an example, in some implementations, the output data can be interpreted and / or affected by a rule-based regulator.
本開示は、顔面平面、３Ｄ視線ベクトル、乗員の頭部および／または目の位置、ならびに乗員が見ている関心領域を求めるための、１つ以上の機械学習済モデルを含む、そうでなければ強化する、システムおよび方法を提供する。上記異なる種類または形態の入力データのうちのいずれかを、上記異なる種類または形態の機械学習済モデルのうちのいずれかと組み合わせることにより、上記異なる種類または形態の出力データのうちのいずれかを提供することができる。 The present disclosure includes a facial plane, a 3D line-of-sight vector, the position of the occupant's head and / or eyes, and one or more machine-learned models for determining the area of interest the occupant is looking at, otherwise. Provide systems and methods to enhance. By combining any of the above different types or forms of input data with any of the above different types or forms of machine-learned models, any of the above different types or forms of output data is provided. be able to.
本開示のシステムおよび方法は、１つ以上のコンピューティングデバイスによって実現する、そうでなければ１つ以上のコンピューティングデバイス上で実行することができる。コンピューティングデバイスの例は、ユーザコンピューティングデバイス（たとえばラップトップ、デスクトップ、および、タブレット、スマートフォン、ウェアラブルコンピューティングデバイス等のモバイルコンピューティングデバイスなど）、埋め込まれたコンピューティングデバイス（たとえば車両、カメラ、画像センサ、産業用機械、衛星、ゲーム機もしくはコントローラ、または、冷蔵庫、サーモスタット、電力量計、家庭用エネルギー管理装置、スマートホームアシスタント等の家庭用電気器具などに埋め込まれたデバイス）、サーバコンピューティングデバイス（たとえばデータベースサーバ、パラメータサーバ、ファイルサーバ、メールサーバ、プリントサーバ、ウェブサーバ、ゲームサーバ、アプリケーションサーバなど）、専用、専門モデル処理もしくはトレーニングデバイス、仮想コンピューティングデバイス、その他のコンピューティングデバイスもしくはコンピューティングインフラストラクチャ、またはその組み合わせを含む。 The systems and methods of the present disclosure can be implemented by one or more computing devices, or can be run on one or more computing devices. Examples of computing devices include user computing devices (eg laptops, desktops, and mobile computing devices such as tablets, smartphones, wearable computing devices, etc.), embedded computing devices (eg vehicles, cameras, images, etc.). Sensors, industrial machines, satellites, game machines or controllers, or devices embedded in home appliances such as refrigerators, thermostats, power meters, home energy management devices, smart home assistants), server computing devices (For example, database server, parameter server, file server, mail server, print server, web server, game server, application server, etc.), dedicated, specialized model processing or training device, virtual computing device, other computing device or computing Includes infrastructure, or a combination thereof.
図６Ｂは、図１の車両コンピューティングシステム１０４および／または図５のコンピューティングデバイス５００の一例であるコンピューティングデバイス６１０の概念図を示す。コンピューティングデバイス６１０は、処理コンポーネント６０２と、メモリコンポーネント６０４と、機械学習済モデル６００とを含む。コンピューティングデバイス６１０は、機械学習済モデル６００をローカルに（すなわちオンデバイスで）格納し実現することができる。よって、いくつかの実装例において、機械学習済モデル６００は、埋め込まれたデバイス、またはモバイルデバイス等のユーザコンピューティングデバイスにより、ローカルに格納および／または実現することができる。埋め込まれたデバイスまたはユーザコンピューティングデバイスにおいて機械学習済モデル６００をローカルに実現して得られた出力データを用いることにより、当該埋め込まれたデバイスまたはユーザコンピューティングデバイスのパフォーマンス（たとえば埋め込まれたデバイスまたはユーザコンピューティングデバイスによって実現されるアプリケーション）を改善することができる。
FIG. 6B shows a conceptual diagram of a computing device 610, which is an example of the
図６Ｃは、機械学習済モデルを含むサーバコンピューティングシステムの一例とネットワークを介して通信することが可能なクライアントコンピューティングデバイスの一例の概念図を示す。図６Ｃは、サーバデバイス６６０とネットワーク６３０を介して通信するクライアントデバイス６１０Ａを含む。クライアントデバイス６１０Ａは、図１の車両コンピューティングシステム１０４の一例および／または図５のコンピューティングデバイス５００の一例である。サーバデバイス６６０は機械学習済モデル６００を格納し実現する。いくつかの場合において、サーバデバイス６６０において機械学習済モデル６００を通して得られた出力データを用いることによって他のサーバタスクを改善することができる、または、この出力データを他の非ユーザデバイスが使用することにより、このような他の非ユーザデバイスが実行するサービスもしくはこのような他の非ユーザデバイスのためのサービスを改善することができる。たとえば、この出力データは、ユーザのコンピューティングデバイスまたは埋め込まれたコンピューティングデバイスのためにサーバデバイス６６０が実行する他のダウンストリームプロセスを改善することができる。その他の場合において、サーバデバイス６６０において機械学習済モデル６００を実現することにより得られた出力データは、ユーザコンピューティングデバイス、埋め込まれたコンピューティングデバイス、またはクライアントデバイス６１０Ａ等のその他何らかのクライアントデバイスに送信する、またはこのようなデバイスが使用することができる。たとえば、サーバデバイス６６０は機械学習をサービスとして実行すると言うことができる。
FIG. 6C shows a conceptual diagram of an example of a server computing system including a machine-learned model and an example of a client computing device capable of communicating via a network. FIG. 6C includes a client device 610A that communicates with the
さらに他の実装例において、機械学習済モデル６００の異なるそれぞれの部分を、ユーザコンピューティングデバイス、埋め込まれたコンピューティングデバイス、サーバコンピューティングデバイスなどの何らかの組み合わせに格納するおよび／またはこれによって実現することができる。言い換えると、機械学習済モデル６００の部分は、その全体または一部を、クライアントデバイス６１０Ａとサーバデバイス６６０とに分散させることができる。
In yet another implementation, different parts of the machine-learned
デバイス６１０Ａおよび６６０は、たとえばＴｅｎｓｏｒＦｌｏｗ、Ｃａｆｆｅ／Ｃａｆｆｅ２、Ｔｈｅａｎｏ、Ｔｏｒｃｈ／ＰｙＴｏｒｃｈ、ＭＸｎｅｔ、ＣＮＴＫなどのような、１つ以上の機械学習プラットフォーム、フレームワーク、および／またはライブラリを用いて、グラフ処理技術またはその他の機械学習技術を実行することができる。デバイス６１０Ａおよび６６０を、異なる物理的位置に分散させ、ネットワーク６３０を含む１つ以上のネットワークを介して接続することができる。デバイス６１０Ａおよび６６０は、分散型コンピューティングデバイスとして構成された場合、逐次コンピューティングアーキテクチャ、並列コンピューティングアーキテクチャ、またはその組み合わせに従って動作することができる。一例において、分散型コンピューティングデバイスは、パラメータサーバの使用を通じて制御またはガイドすることができる。
いくつかの実装例において、機械学習済モデル６００の複数のインスタンスを並列化することにより、処理スループットを高めることができる。たとえば、機械学習済モデル６００の複数のインスタンスを１つの処理デバイスもしくはコンピューティングデバイス上で並列化する、または、複数の処理デバイスもしくはコンピューティングデバイスにわたって並列化することができる。
In some implementation examples, the processing throughput can be increased by parallelizing a plurality of instances of the machine-learned
機械学習済モデル６００または本開示のその他の局面を実現する各コンピューティングデバイスは、本明細書に記載の技術の実行を可能にする複数のハードウェアコンポーネントを含み得る。たとえば、各コンピューティングデバイスは、機械学習済モデル６００のうちの一部またはすべてを格納する１つ以上のメモリデバイスを含み得る。たとえば、機械学習済モデル６００は、メモリに格納されている構造化された数値表現であってもよい。上記１つ以上のメモリデバイスはまた、機械学習済モデル６００を実現するためまたはその他の動作を実行するための命令を含み得る。メモリデバイスの例は、ＲＡＭ、ＲＯＭ、ＥＥＰＲＯＭ、ＥＰＲＯＭ、フラシュメモリデバイス、磁気ディスクなど、およびその組み合わせを含む。
Each computing device that implements the machine-learned
各コンピューティングデバイスはまた、機械学習済モデル６００のうちの一部またはすべてを実現するおよび／またはその他の関連する動作を実行する１つ以上の処理デバイスを含み得る。処理デバイスの例は、中央処理装置（ＣＰＵ）、仮想処理ユニット（virtual processing unit）（ＶＰＵ）、グラフィックス処理ユニット（graphics processing unit）（ＧＰＵ）、テンソル処理ユニット（tensor processing unit）（ＴＰＵ）、ニューラル処理ユニット（neural processing unit）（ＮＰＵ）、ニューラル処理エンジン、ＣＰＵ、ＶＰＵ、ＧＰＵ、ＴＰＵ、ＮＰＵもしくはその他の処理装置のコア、特定用途向け集積回路（ＡＳＩＣ）、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）、コプロセッサ、コントローラ、または上記処理装置の組み合わせ、のうちの１つ以上を含む。処理装置は、たとえば画像センサ、加速度計などのようなその他のハードウェアコンポーネントに埋め込むことができる。
Each computing device may also include one or more processing devices that implement some or all of the machine-learned
ハードウェアコンポーネント（たとえばメモリデバイスおよび／または処理装置）は、物理的に分散させたコンピューティングデバイスおよび／または仮想的に分散させたコンピューティングシステムにわたって分散させることができる。 Hardware components (eg, memory devices and / or processing devices) can be distributed across physically distributed computing devices and / or virtually distributed computing systems.
図６Ｄは、モデルトレーナーを含むトレーニングコンピューティングシステムの一例と通信するコンピューティングデバイスの一例の概念図を示す。図６Ｄは、トレーニングデバイス６７０とネットワーク６３０を介して通信するクライアントデバイス６１０Ｂを含む。クライアントデバイス６１０Ｂは、図１の車両コンピューティングシステム１０４および／または図５のコンピューティングデバイス５００の一例である。本明細書に記載の機械学習済モデル６００を、トレーニングデバイス６７０等のトレーニングコンピューティングシステムにおいてトレーニングした後に、クライアントデバイス６１０Ｂ等の１つ以上のコンピューティングデバイスにおいて格納および／または実現するために提供することができる。たとえば、モデルトレーナー６７２はトレーニングデバイス６７０においてローカルに実行される。しかしながら、いくつかの例において、モデルトレーナー６７２を含むトレーニングデバイス６７０は、クライアントデバイス６１０Ｂに、または機械学習済モデル６００を実現するその他任意のコンピューティングデバイスに、含まれていてもよい、またはこれと離れていてもよい。
FIG. 6D shows a conceptual diagram of an example of a computing device that communicates with an example of a training computing system that includes a model trainer. FIG. 6D includes a client device 610B that communicates with the training device 670 over the network 630. The client device 610B is an example of the
いくつかの実装例において、機械学習済モデル６００を、オフライン方式またはオンライン方式でトレーニングすることができる。オフライントレーニング（バッチ学習としても知られている）の場合、機械学習済モデル６００を、トレーニングデータのスタティックセット全体についてトレーニングする。オンライン学習の場合、機械学習済モデル６００を、新たなトレーニングデータが利用できるようになると（たとえば推論の実施のためにモデルが使用されている間に）連続的にトレーニング（再トレーニング）することができる。
In some implementation examples, the machine-learned
モデルトレーナー６７２は、機械学習済モデル６００の集中型トレーニングを（たとえば中央に格納されたデータセットに基づいて）実行することができる。その他の実装例において、分散型トレーニング、フェデレ―テッドラーニング（federated learning）その他のような非集中型トレーニングを用いることにより、機械学習済モデル６００をトレーニング、アップデート、またはパーソナライズすることができる。
The model trainer 672 can perform intensive training of the machine-learned model 600 (eg, based on a centrally stored data set). In other implementations, decentralized training such as distributed training, federated learning, etc. can be used to train, update, or personalize the machine-learned
本明細書に記載の機械学習済モデル６００は、各種の異なるトレーニングタイプまたは技術のうちの１つ以上に従ってトレーニングすることができる。たとえば、いくつかの実装例において、機械学習済モデル６００を、モデルトレーナー６７２が教師あり学習を用いてトレーニングすることができ、この教師あり学習では、機械学習済モデル６００が、ラベルを有するインスタンスまたは例を含むトレーニングデータセットについてトレーニングされる。ラベルは、専門家によって手作業で与えられてもよく、クラウドソーシングを通して生成されてもよく、または、その他の技術によって（たとえば物理学に基づくもしくは複雑な数学的モデルによって）提供されてもよい。いくつかの実装例において、ユーザが承諾している場合、トレーニング例をユーザコンピューティングデバイスが提供してもよい。いくつかの実装例において、このプロセスをモデルのパーソナライズと呼ぶことができる。
The machine-learned
図６Ｅは、ラベル６９３を有する入力データ例６９２を含むトレーニングデータ６９１について機械学習済モデル６００がトレーニングされるトレーニングプロセスの一例であるトレーニングプロセス６９０の概念図を示す。トレーニングプロセス６９０はトレーニングプロセスの一例であり、他のトレーニングプロセスも同様に使用できる。
FIG. 6E shows a conceptual diagram of the
トレーニングプロセス６９０が使用するトレーニングデータ６９１は、このようなデータをトレーニングに使用することをユーザが許可すると、共有フローの匿名使用ログ、たとえば、ともに共有されていたコンテンツアイテムや、たとえばナレッジグラフ（knowledge graph）のエンティティからの、ともに所属していることが既に識別されているバンドル化されたコンテンツなどを含み得る。いくつかの実装例において、トレーニングデータ６９１は、出力データ６９４に対応するラベル６９３が既に割り当てられている入力データ例６９２を含み得る。
The
いくつかの実装例において、機械学習済モデル６００を、目的関数６９５等の目的関数を最適化することによってトレーニングすることができる。たとえば、いくつかの実装例において、目的関数６９５は、トレーニングデータから当該モデルが生成した出力データと、トレーニングデータに対応付けられたラベル（たとえばグラウンドトゥルース（ground-truth）ラベル）とを比較する（たとえばこれらの差を求める）損失関数であってもよく、またはこの損失関数を含んでいてもよい。たとえば、損失関数は、出力データとラベルとの自乗差の合計または平均を評価することができる。いくつかの例において、目的関数６９５は、特定の結果または出力データのコストを記述するコスト関数であってもよく、またはこのコスト関数を含んでいてもよい。目的関数６９５のその他の例は、たとえばトリプレット損失または最大マージントレーニング等のマージンベースの技術を含み得る。
In some implementations, the machine-learned
各種最適化技術のうちの１つ以上を実行することにより、目的関数６９５を最適化することができる。たとえば、最適化技術は、目的関数６９５を最小または最大にすることができる。最適化技術の例は、ヘシアン（Hessian）に基づく技術、および、たとえば座標降下法（coordinate descent）、勾配降下法（gradient descent）（たとえば確率的勾配降下法（stochastic gradient descent））、サブ勾配法（subgradient method）のような、勾配に基づく技術を含む。その他の最適化技術は、ブラックボックス最適化技術およびヒューリスティックス（heuristics）を含む。
The
いくつかの実装例において、誤差逆伝播を最適化技術（たとえば勾配に基づく技術）とともに使用することにより、機械学習済モデル６００をトレーニングすることができる（たとえば機械学習済モデルが人工ニューラルネットワークのような多層モデルの場合）。たとえば、伝播およびモデルパラメータ（たとえば重み）アップデートの反復サイクルを実行することにより、機械学習済モデル６００をトレーニングすることができる。逆伝播技術の例は、打ち切り型通時的逆伝播（truncated backpropagation through time）、レーベンバーグ・マーカート逆伝播（Levenberg-Marquardt backpropagation）などを含む。
In some implementations, error backpropagation can be used with optimization techniques (eg, gradient-based techniques) to train the machine-learned model 600 (eg, the machine-learned model is like an artificial neural network). In the case of a multi-layer model). For example, the machine-learned
いくつかの実装例において、本明細書に記載の機械学習済モデル６００は、教師なし学習技術を用いてトレーニングすることができる。教師なし学習は、関数を推論することにより、ラベル付けされていないデータから隠れた構造を説明することを含み得る。たとえば、分類またはカテゴライズはデータに含まれていなくてもよい。教師なし学習技術を用いることにより、クラスタリング、異常検知、潜在変数モデルの学習、またはその他のタスクを実行することができる。
In some implementation examples, the machine-learned
機械学習済モデル６００を、教師あり学習および教師なし学習の側面を組み合わせた半教師あり技術を用いてトレーニングすることができる。機械学習済モデル６００を、進化的技術または遺伝的アルゴリズムを通じてトレーニングする、そうでなければ生成することができる。いくつかの実装例において、本明細書に記載の機械学習済モデル６００を、強化学習を用いてトレーニングすることができる。強化学習の場合、エージェント（たとえばモデル）が環境においてアクションを実行しこのようなアクションから生じた報酬を最大にするおよび／またはペナルティを最小にするよう学習することができる。強化学習と教師あり学習問題との違いは、正しい入力／出力ペアが提示されず最適下限アクションが明確に補正されない点にある。
The machine-learned
いくつかの実装例において、１つ以上の一般化技術をトレーニング中に実行することにより、機械学習済モデル６００の一般化を改善することができる。一般化技術は、機械学習済モデル６００のトレーニングデータへの過剰適合を低減するのに役立ち得る。一般化技術の例は、ドロップアウト（dropout）技術、重み減衰（weight decay）技術、バッチ正規化（batch normalization）、早期終了（early stopping）、サブセット選択（subset selection）、ステップワイズ選択（stepwise selection）などを含む。
In some implementation examples, the generalization of the machine-learned
いくつかの実装例において、本明細書に記載の機械学習済モデル６００は、たとえば、学習レート、層の数、各層のノードの数、木の葉の数、クラスタの数などのような複数のハイパーパラメータを含み得る、そうでなければ複数のハイパーパラメータの影響を受ける可能性がある。ハイパーパラメータはモデルパフォーマンスに影響を与える可能性がある。ハイパーパラメータは、手で選択する、または、たとえばグリッドサーチ、ブラックボックス最適化技術（たとえばベイズ最適化（Bayesian optimization）、ランダムサーチなど）、勾配に基づく最適化などのような技術を適用することにより、自動的に選択することができる。自動ハイパーパラメータ最適化を実行するための技術および／またはツールの例は、Ｈｙｐｅｒｏｐｔ、Ａｕｔｏ−ＷＥＫＡ、Ｓｐｅａｒｍｉｎｔ、メトリック最適化エンジン（Metric Optimization Engine）（ＭＯＥ）などを含む。
In some implementation examples, the machine-learned
いくつかの実装例において、各種技術を用いることにより、モデルがトレーニングされるときの学習レートを最適化および／または適応化することができる。学習レート最適化または適応化を実行するための技術および／またはツールの例は、Ａｄａｇｒａｄ、適応モーメント推定（Adaptive Moment Estimation）（ＡＤＡＭ）、Ａｄａｄｅｌｔａ、ＲＭＳｐｒｏｐなどを含む。 In some implementation examples, various techniques can be used to optimize and / or adapt the learning rate when the model is trained. Examples of techniques and / or tools for performing learning rate optimization or adaptation include Adagrad, Adaptive Moment Estimation (ADAM), Addaleta, RMSprop, and the like.
いくつかの実装例において、転移学習技術を用いることにより、初期モデルを提供することができ、この初期モデルから、本明細書に記載の機械学習済モデル６００のトレーニングが開始される。
In some implementation examples, transfer learning techniques can be used to provide an initial model, from which training of the machine-learned
いくつかの実装例において、本明細書に記載の機械学習済モデル６００は、コンピューティングデバイス上のコンピュータ読取可能コードの異なる部分に含まれていてもよい。一例において、機械学習済モデル６００は、特定のアプリケーションまたはプログラムに含まれこのような特定のアプリケーションまたはプログラムによって（たとえば独占的に）使用されてもよい。よって、一例において、コンピューティングデバイスは複数のアプリケーションを含むことができ、このようなアプリケーションのうちの１つ以上は、それぞれの機械学習ライブラリおよび機械学習済モデルを含むことができる。
In some implementation examples, the machine-learned
別の例において、本明細書に記載の機械学習済モデル６００は、コンピューティングデバイスのオペレーティングシステムに（たとえばオペレーティングシステムの中央知能層に）含まれ当該オペレーティングシステムとやり取りする１つ以上のアプリケーションによってコールされる、そうでなければ使用されることができる。いくつかの実装例において、各アプリケーションは、中央知能層（およびそこに格納されているモデル）と、アプリケーションプログラミングインターフェイス（ＡＰＩ）（たとえばすべてのアプリケーションの共通するパブリックＡＰＩ）を用いて通信することができる。
In another example, the machine-learned
いくつかの実装例において、中央知能層は、中央デバイスデータ層と通信することができる。中央デバイスデータ層は、コンピューティングデバイスのための集中データリポジトリであってもよい。中央デバイスデータ層は、コンピューティングデバイスのその他の複数のコンポーネント、たとえば１つ以上のセンサ、コンテキストマネージャ、デバイス状態コンポーネント、および／またはその他のコンポーネントと通信することができる。いくつかの実装例において、中央デバイスデータ層は、ＡＰＩ（たとえばプライベートＡＰＩ）を用いて各デバイスコンポーネントと通信することができる。 In some implementations, the central intelligence layer can communicate with the central device data layer. The central device data layer may be a centralized data repository for computing devices. The central device data layer can communicate with several other components of the computing device, such as one or more sensors, context managers, device state components, and / or other components. In some implementation examples, the central device data layer can communicate with each device component using APIs (eg, private APIs).
本明細書に記載の技術は、サーバ、データベース、ソフトウェアアプリケーション、およびその他のコンピュータベースのシステム、ならびに実行されるアクションおよびこのようなシステムとの間で送信される情報を参照する。コンピュータベースのシステムに固有の柔軟性により、コンポーネント間のタスクおよび機能の多様な可能な構成、組み合わせ、および分割が可能である。たとえば、本明細書に記載のプロセスは、単一のデバイスもしくはコンポーネント、または組み合わせとして働く複数のデバイスもしくはコンポーネントを用いて実現することができる。 The techniques described herein refer to servers, databases, software applications, and other computer-based systems, as well as the actions performed and the information transmitted to and from such systems. The inherent flexibility of computer-based systems allows for a wide variety of possible configurations, combinations, and divisions of tasks and functions between components. For example, the process described herein can be implemented with a single device or component, or with multiple devices or components acting as a combination.
データベースおよびアプリケーションは、単一のシステム上で実現することができる、または複数のシステムに分散させることができる。分散型コンポーネントは逐次的にまたは並列に動作することができる。 Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
加えて、本明細書に記載の機械学習技術は、容易に交換および組み合わせが可能である。特定の技術例について説明してきたが、その他多数の技術が存在しそれらは本開示の局面に関連して使用することができる。 In addition, the machine learning techniques described herein are easily interchangeable and combinable. Although specific technical examples have been described, there are many other technologies that can be used in connection with aspects of the present disclosure.
本開示では機械学習済モデルおよび関連技術の例の簡単な概要を示した。その他の詳細については以下の参考文献、Machine Learning A Probabilistic Perspective (Murphy)、Rules of Machine Learning: Best Practices for ML Engineering (Zinkevich)、Deep Learning (Goodfellow)、Reinforcement Learning: An Introduction (Sutton)、および、Artificial Intelligence: A Modern Approach (Norvig)を検討すべきである。 This disclosure provides a brief overview of machine-learned models and examples of related technologies. For more details, see the following references, Machine Learning A Probabilistic Perspective (Murphy), Rules of Machine Learning: Best Practices for ML Engineering (Zinkevich), Deep Learning (Goodfellow), Reinforcement Learning: An Introduction (Sutton), and Artificial Intelligence: A Modern Approach (Norvig) should be considered.
上記説明に加えて、本明細書に記載のシステム、プログラムまたは特徴により、ユーザ情報（たとえばユーザのソーシャルネットワーク、社会的行動または活動、職業、ユーザの好み、またはユーザの現在の位置）の収集を可能にすることができるか否かまたはいつ可能にすることができるかと、コンテンツまたは通信がサーバからユーザに送信されるか否かとの双方についてユーザが選択できるよう、ユーザに管理権が与えられてもよい。加えて、特定のデータを、格納または使用前に、個人識別可能情報が削除されるように１つ以上のやり方で処理してもよい。たとえば、ユーザのＩＤを、このユーザについて個人識別可能情報が判断できないように処理してもよく、または、位置情報が得られる場合はユーザの特定の位置が判断できないようにユーザの地理的位置を（都市、郵便番号または州レベルなどに）一般化してもよい。よって、ユーザは、当該ユーザに関してどのような情報が収集されるか、この情報が如何にして使用されるか、および、どのような情報がユーザに提供されるかについて、管理することができる。 In addition to the above description, the system, program or feature described herein collects user information (eg, the user's social network, social behavior or activity, occupation, user preference, or the user's current location). The user is given control over whether or not it can be enabled and when it can be enabled and whether or not the content or communication is sent from the server to the user. May be good. In addition, certain data may be processed in one or more ways so that personally identifiable information is removed prior to storage or use. For example, the user's ID may be processed so that personally identifiable information about this user cannot be determined, or the user's geographic location can be determined so that the user's specific location cannot be determined if location information is available. It may be generalized (at the city, zip code or state level, etc.). Thus, the user can manage what information is collected about the user, how this information is used, and what information is provided to the user.
図７は、本開示の１つ以上の局面に係る、３Ｄ視線ベクトルを用いて運転者の関与を判断するように構成されたコンピューティングシステムの動作の例を示すフローチャートである。以下、図７の動作を図１の車両１００および図５の一例としてのコンピューティングデバイス５００の文脈で説明する。１つ以上のカメラ１０２は車両１００の乗員の少なくとも１つの画像を取り込むことができる（７０２）。
FIG. 7 is a flow chart illustrating an example of the operation of a computing system configured to determine driver involvement using a 3D line-of-sight vector according to one or more aspects of the present disclosure. Hereinafter, the operation of FIG. 7 will be described in the context of the
コンピューティングデバイス５００の目位置モジュール５２４は、上記少なくとも１つの画像を分析し、車両１００内の乗員の頭部および／または目の位置を求めることができる（７０４）。たとえば、上記少なくとも１つの画像は２つの画像を含み得る。これら２つの画像の各々は、異なるカメラ１０２のうちの対応する１つのカメラが取り込んだものである。目位置モジュール５２４は、画像に取り込まれている乗員の頭部に対するカメラ１０２の視差角を求めることができる。目位置モジュール５２４は、この視差角と、カメラ１０２間の距離とを用いて、カメラ１０２のうちの１つ以上から乗員の頭部および／または目までの距離を求めることができる。目位置モジュール５２４は、カメラ１０２のうちの１つ以上から乗員の頭部および／または目までの距離と、３次元空間におけるカメラ１０２のうちの１つ以上の各々の相対的な位置とを用いて、３次元空間におけるカメラ１０２のうちの１つ以上に対する乗員の頭部および／または目の位置を求めることができる。別の例として、カメラ１０２のうちの１つは、乗員の赤外線画像を取り込む赤外線カメラであってもよい。目位置モジュール５２４は、赤外線画像における歪みを分析し、歪みに基づいてカメラから乗員の頭部および／または目までの距離を求めてもよい。また、目位置モジュール５２４は、画像内における頭部および／または目の位置に基づいて、カメラに対する乗員の頭部および／または目の位置を求めてもよい。
The
視線モジュール５２２は、車両の乗員の３Ｄ視線ベクトルを求めることができる（７０６）。いくつかの場合において、視線モジュール５２２は、カメラ１０２のうちの１つ以上が取り込んだ１つ以上の画像を分析し、乗員の顔面平面を求めることができる。たとえば、視線モジュール５２２は、機械学習済モデルを上記１つ以上の画像に適用することにより、画像内の乗員の各種顔面ランドマークを識別することができ、また、機械学習済モデルを識別した顔面ランドマークの位置に適用することにより、乗員の顔面平面のピッチ、ロール、およびヨーを求めることができる。いくつかの場合において、１つの機械学習済モデルが、顔面ランドマークを識別するとともに顔面平面のピッチ角、ロール角、およびヨー角を求めてもよい。別の例として、視線モジュール５２２は、ヒューリスティックスを実行することによって顔面ランドマークの位置を求め、機械学習以外の技術を用いて顔面平面のピッチ角、ロール角、およびヨー角を計算してもよい。視線モジュール５２２は、この顔面平面のヨー角、ピッチ角、およびロール角を用いて、乗員の３Ｄ視線ベクトルを求めることができる。
The line-of-
いくつかの場合において、視線モジュール５２２はまた、アイトラッキングを用いて３Ｄ視線ベクトルを求めることができる。しかしながら、乗員の目が遮られているために画像では見えない場合がある。このような場合、視線モジュール５２２は、アイトラッキングを用いて３Ｄ視線ベクトルを求めるのではなく、顔面平面を用いて３Ｄ視線ベクトルを求めることができる。視線モジュール５２２がアイトラッキングおよび顔面平面双方に基づいて初期３Ｄ視線ベクトルを求めることができる場合、視線モジュール５２２は、双方の初期３Ｄ視線ベクトルの組み合わせを用いて最終３Ｄ視線ベクトルを求めてもよい。
In some cases, the line-of-
関心領域モジュール５２６は、視線モジュール５２２が求めた３Ｄ視線ベクトルと、目位置モジュール５２４が求めた頭部および／または目の位置との両方を用いることにより、乗員が見ている１つ以上の関心領域を求めることができる（７０８）。さまざまな場合において、目位置モジュール５２４が求めた頭部および／または目の位置は、１つ以上のカメラ１０２の位置に対して定められた座標のセットである。すなわち、頭部および／または目の位置は、カメラベースの座標系を用いて特定することができる。このような場合、関心領域モジュール５２６は、位置データをカメラベースの座標系から車両ベースの座標系に変換してもよい。車両ベースの座標系は、車両コンピューティングシステム１０４に与えられる車両データファイルによって定められてもよい。車両データファイルは、車両ベースの座標系を用いて車両の複数の関心領域を定める座標を含み得る。
The region of interest module 526 uses both the 3D line-of-sight vector determined by the line-of-
関心領域モジュール５２６は、視線モジュール５２２が求めた３Ｄ視線ベクトルを、乗員の目のうちの１つ以上の車両ベースの座標系の位置から延ばすことができる。関心領域モジュール５２６は、もしあれば、３Ｄ視線ベクトルが関心領域の面のうちのいずれと交差するかを判断するとともに、各面内の、３Ｄ視線ベクトルが交差する位置を求めることができる。乗員の目および関心領域の座標位置は同一の座標系を用いて特定するので、関心領域モジュール５２６は、各関心領域と乗員の目の位置との間の距離を求めることが可能である。関心領域モジュール５２６は、この距離を３Ｄ視線ベクトル方向とともに用いて、３Ｄ視線ベクトルが、関心領域の面のうちの１つ以上と交差する位置を求めることが可能である。関心領域モジュール５２６は、３Ｄ視線ベクトルが交差する関心領域の面が、乗員が見ている１つ以上の関心領域であると判断する。
The region of interest module 526 can extend the 3D line-of-sight vector obtained by the line-of-
さまざまな場合において、関心領域モジュール５２６が、乗員が見ている少なくとも１つの関心領域を求めると、車両コンピューティングシステム１０４は、１つ以上のアクションを実行することができる（７１０）。たとえば、車両１００の乗員がサイドウィンドウから外を少なくともしきい値期間見ている場合、車両コンピューティングシステム１０４は、サイドウィンドウの外に位置する何か、たとえば風景を乗員が見ていると判断することができる。車両コンピューティングシステム１０４は、車両１００の外部に位置するカメラを自動制御し、乗員がカメラを持ち上げて自身で写真を撮影しなくても、その風景の画像をカメラに取り込ませることができる。
In various cases, the
別の例として、車両１００の乗員が車両１００の運転者である場合、車両コンピューティングシステム１０４は、運転者が道路を注視しているのではなく車両コンピューティングシステム１０４のディスプレイを見ていると判断する場合がある。このような場合、車両コンピューティングシステム１０４は、運転者がディスプレイをしきい値期間よりも長く見ていたか否かを（たとえば運転者がディスプレイに対応付けられた関心領域をまだ見ていることを定期的に確認することによって）判断するように構成されていてもよい。車両コンピューティングシステム１０４は、運転者がディスプレイをしきい値期間よりも長く見ていたと判断した場合、ディスプレイに情報を出力するのを中止する、またはディスプレイに警告メッセージを出力する、または車両コンピューティングシステム１０４が受けたユーザ入力の処理を中止することができる。車両コンピューティングシステム１０４は、運転者がディスプレイをしきい値期間よりも長く見ていないと判断した場合、ディスプレイに対応付けられた関心領域を運転者が見ていることに基づいたアクションの実行を控えてもよい。このようにして、車両コンピューティングシステム１０４は、車両の乗員が見ている関心領域に基づいてアクションを選択的に実行することができる。
As another example, if the occupant of the
１つ以上の例において、記載された機能は、ハードウェア、ソフトウェア、ファームウェア、またはその任意の組み合わせで実現されてもよい。ソフトウェアで実現される場合、当該機能は、１つ以上の命令またはコードとして、コンピュータ読取可能媒体上に格納またはコンピュータ読取可能媒体を介して送信され、ハードウェアに基づいた処理ユニットによって実行されてもよい。コンピュータ読取可能媒体は、データ記憶媒体等の有形の媒体に対応するコンピュータ読取可能記憶媒体、または、たとえば通信プロトコルに従ってある場所から別の場所までのコンピュータプログラムの転送を促進する任意の媒体を含む通信媒体を、含み得る。このように、コンピュータ読取可能媒体は一般的に、（１）非一時的な有形のコンピュータ読取可能記憶媒体、または、（２）信号もしくは搬送波等の通信媒体に対応し得る。データ記憶媒体は、本開示に記載されている技術の実現のために、命令、コードおよび／またはデータ構造を取り出すよう１つ以上のコンピュータまたは１つ以上のプロセッサがアクセス可能な、利用可能な任意の媒体であってもよい。コンピュータプログラムプロダクトはコンピュータ読取可能媒体を含み得る。 In one or more examples, the functions described may be realized by hardware, software, firmware, or any combination thereof. When implemented in software, the function may be stored on or transmitted over a computer-readable medium as one or more instructions or codes and performed by a hardware-based processing unit. good. A computer-readable medium is a communication including a computer-readable storage medium corresponding to a tangible medium such as a data storage medium, or any medium that facilitates the transfer of a computer program from one place to another according to a communication protocol, for example. The medium may be included. As described above, the computer-readable medium can generally correspond to (1) a non-temporary tangible computer-readable storage medium, or (2) a communication medium such as a signal or a carrier wave. The data storage medium is any available, accessible to one or more computers or one or more processors to retrieve instructions, codes and / or data structures for the realization of the techniques described in this disclosure. It may be a medium of. Computer program products may include computer readable media.
限定ではなく例示として、そのようなコンピュータ読取可能記憶媒体は、所望のプログラムコードを命令またはデータ構造の形態で格納するために使用することができるとともにコンピュータがアクセスできる、ＲＡＭ、ＲＯＭ、ＥＥＰＲＯＭ、ＣＤ−ＲＯＭもしくは他の光ディスクストレージ、磁気ディスクストレージもしくは他の磁気ストレージデバイス、フラッシュメモリ、または任意の他の記憶媒体を含み得る。また、任意の接続は適切にコンピュータ読取可能媒体と呼ばれる。たとえば、同軸ケーブル、光ファイバケーブル、撚り対線、デジタル加入者線（ＤＳＬ）、または、赤外線、無線、およびマイクロ波等の無線技術を使用して、命令をウェブサイト、サーバまたは他のリモートソースから送信する場合、当該同軸ケーブル、光ファイバケーブル、撚り対線、ＤＳＬ、または、赤外線、無線、およびマイクロ波等の無線技術は、媒体の定義に含まれる。しかしながら、コンピュータ読取可能記憶媒体およびデータ記憶媒体は、接続、搬送波、信号、または他の一時的な媒体を含まないがその代わりに非一時的な有形の記憶媒体に向けられることが理解されるはずである。使用されるディスク（ｄｉｓｋおよびｄｉｓｃ）は、コンパクトディスク（ＣＤ）、レーザーディスク（登録商標）、光ディスク、デジタルバーサタイルディスク（ＤＶＤ）、フロッピー（登録商標）ディスクおよびブルーレイ（登録商標）ディスク、ウルトラブルーレイなどを含み、ディスク（ｄｉｓｋ）は通常磁気的にデータを再生するものであり、ディスク（ｄｉｓｃ）はレーザでデータを光学的に再生するものである。これらの組み合わせもコンピュータ読取可能媒体の範囲に含まれねばならない。 By way of example, but not by limitation, such computer-readable storage media can be used to store the desired program code in the form of instructions or data structures and can be accessed by a computer, such as RAM, ROM, EEPROM, CD. -Can include ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, flash memory, or any other storage medium. Also, any connection is properly referred to as a computer readable medium. For example, using coaxial cables, fiber optic cables, twisted pairs, digital subscriber wires (DSL), or wireless technologies such as infrared, wireless, and microwave to send instructions to websites, servers, or other remote sources. When transmitting from, such coaxial cables, fiber optic cables, twisted pairs, DSLs, or radio technologies such as infrared, radio, and microwave are included in the definition of medium. However, it should be understood that computer-readable and data storage media do not include connections, carriers, signals, or other temporary media, but instead are directed to non-temporary tangible storage media. Is. The discs (disc and disc) used are compact discs (CDs), laser discs (registered trademarks), optical discs, digital versatile discs (DVDs), floppy discs and Blu-ray® discs, ultra-Blu-rays, etc. A disc (disk) normally reproduces data magnetically, and a disc (disk) optically reproduces data by a laser. These combinations must also be included in the range of computer readable media.
命令は、１つ以上のプロセッサによって実行されてもよく、たとえば、１つ以上のデジタル信号プロセッサ（ＤＳＰ）、汎用マイクロプロセッサ、特定用途向け集積回路（ＡＳＩＣ）、フィールドプログラマブルロジックアレイ（ＦＰＧＡ）、または他の同等の集積もしくは離散論理回路によって実行されてもよい。したがって、使用されている「プロセッサ」という用語は、上記構造、または、記載されている技術の実現に適した任意の他の構造のうちのいずれかを指す。加えて、いくつかの局面において、記載されている機能は、専用ハードウェアおよび／またはソフトウェアモジュール内に与えられてもよい。また、当該技術は１つ以上の回路または論理素子において完全に実現することが可能である。 Instructions may be executed by one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or It may be performed by other equivalent integrated or discrete logic circuits. Accordingly, the term "processor" as used refers to either the above structure or any other structure suitable for the realization of the techniques described. In addition, in some aspects, the functionality described may be provided within dedicated hardware and / or software modules. Also, the technique can be fully implemented in one or more circuits or logic elements.
本開示の技術は、無線ハンドセット、集積回路（ＩＣ）またはＩＣのセット（たとえばチップセット）を含む多様なデバイスまたは装置において実現されてもよい。各種コンポーネント、モジュール、またはユニットは、本開示において、開示されている技術を実行するように構成されたデバイスの機能的側面を強調するように記載されているが、必ずしも異なるハードウェアユニットによる実現を要求していない。むしろ、上述のように、各種ユニットは、ハードウェアユニットにおいて組み合わされてもよい、または、好適なソフトウェアおよび／またはファームウェアとともに上記１つ以上のプロセッサを含む共同作業するハードウェアユニットの集まりによって提供されてもよい。 The techniques of the present disclosure may be implemented in a variety of devices or devices, including wireless handsets, integrated circuits (ICs) or sets of ICs (eg, chipsets). The various components, modules, or units are described in this disclosure to highlight the functional aspects of a device configured to perform the disclosed technology, but not necessarily with different hardware units. Not requesting. Rather, as described above, the various units may be combined in a hardware unit or provided by a collection of collaborative hardware units that include one or more of the above processors with suitable software and / or firmware. You may.
さまざまな例について説明した。これらのおよびその他の例は以下の請求項の範囲に含まれる。 I explained various examples. These and other examples are within the scope of the following claims.
カメラ１０２は、カメラまたは電荷結合素子等の、任意の適切な種類の画像捕捉装置のうちの１つ以上であってもよい。いくつかの例において、カメラ１０２は、高視野浅焦点深度の１つ以上の赤外線カメラであってもよく、概ね車両１００の１つ以上の座席の方向に向けられた背面照光赤外線カメラであってもよい。その他の例において、カメラ１０２は、その他１つ以上の赤外線カメラ、サーモグラフィーカメラ、熱撮像カメラ、感光カメラ、レンジセンサ、深度カメラ、トモグラフィーデバイス、レーダーデバイス、または超音波カメラを含み得る、１つ以上のその他の種類のカメラもしくは画像センサであってもよい、またはこれらを含んでいてもよい。いくつかの例において、カメラ１０２は、コンピュータビジョン技術の適用に適した画像キャプチャデバイスであってもよい。使用するセンサまたはカメラの種類に応じて、結果として得られる画像は、２次元画像、３次元ボリューム画像、または画像シーケンスを含み得る。画素値は、典型的には１つ以上のスペクトル帯における光度に対応するが、深度、超音波もしくは電磁波の吸収もしくは反射、または核磁気共鳴に関連していてもよい。図１には２つのカメラ１０２のみが示されているが、車両１００は、車両１００の内部に配置された３つ以上のカメラ１０２を含んでいてもよく、本開示の技術は任意の数のカメラ１０２が取り込んだ任意の数の画像を使用することができる。
The camera 102 may be one or more of any suitable type of image capture device, such as a camera or charge-coupled device. In some examples, the camera 102 may be one or more infrared cameras with a high field of view and shallow depth of focus, or a backlit infrared camera generally oriented towards one or more seats in the
Claims (13)
前記乗員の前記少なくとも１つの画像に基づいて前記車両内の前記乗員の１つ以上の目の位置を求めるステップと、
前記乗員の前記少なくとも１つの画像に基づいて視線ベクトルを求めるステップと、
前記視線ベクトルと、前記乗員の１つ以上の目の位置と、前記車両の車両データファイルとに基づいて、前記車両の複数の関心領域のうちの、前記乗員が見ている関心領域を求めるステップとを含み、前記車両データファイルは前記複数の関心領域の各々の位置を特定し、
前記求めた関心領域に基づいてアクションを選択的に実行するステップを含む、方法。 With the step of acquiring at least one image of the occupant of the vehicle via the vehicle's camera system,
A step of determining the position of one or more eyes of the occupant in the vehicle based on the at least one image of the occupant.
A step of obtaining a line-of-sight vector based on the at least one image of the occupant,
A step of finding an area of interest seen by the occupant among a plurality of areas of interest of the vehicle based on the line-of-sight vector, the position of one or more eyes of the occupant, and the vehicle data file of the vehicle. The vehicle data file identifies the location of each of the plurality of areas of interest, including
A method comprising the step of selectively performing an action based on the determined area of interest.
前記少なくとも１つの画像における１つ以上の顔面ランドマークを識別するステップと、
前記１つ以上の顔面ランドマークに基づいて、前記乗員の顔面平面のピッチ角、ロール角、およびヨー角を求めるステップと、
前記ピッチ角、前記ロール角、および前記ヨー角に基づいて、前記視線ベクトルを求めるステップとを含む、請求項１に記載の方法。 The step of obtaining the line-of-sight vector is
The step of identifying one or more facial landmarks in the at least one image,
A step of determining the pitch angle, roll angle, and yaw angle of the occupant's facial plane based on the one or more facial landmarks.
The method according to claim 1, comprising the step of obtaining the line-of-sight vector based on the pitch angle, the roll angle, and the yaw angle.
前記少なくとも１つの画像に基づいて前記乗員の少なくとも１つの瞳孔の角度を求めるステップと、
前記少なくとも１つの瞳孔の角度に基づいて前記視線ベクトルを求めるステップとを含む、請求項１に記載の方法。 The step of obtaining the line-of-sight vector is
The step of determining the angle of at least one pupil of the occupant based on the at least one image,
The method of claim 1, comprising the step of obtaining the line-of-sight vector based on the angle of at least one pupil.
前記少なくとも１つの画像に基づいて前記乗員の顔面平面を求めるステップと、
前記顔面平面に基づいて第１の初期視線ベクトルを求めるステップと、
前記画像に基づきアイトラッキングを用いて第２の初期視線ベクトルを求めるステップと、
前記第１の初期視線ベクトルと前記第２の初期視線ベクトルとを少なくとも組み合わせることによって前記視線ベクトルを求めるステップとを含む、請求項１〜３のいずれか１項に記載の方法。 The step of obtaining the line-of-sight vector is
The step of obtaining the facial plane of the occupant based on the at least one image, and
The step of obtaining the first initial line-of-sight vector based on the facial plane, and
A step of obtaining a second initial line-of-sight vector using eye tracking based on the image, and
The method according to any one of claims 1 to 3, comprising a step of obtaining the line-of-sight vector by at least combining the first initial line-of-sight vector and the second initial line-of-sight vector.
前記２つ以上の異なるカメラの各々が取り込んだ少なくとも１つの画像に基づいて視差角を求めるステップと、
前記２つ以上の異なるカメラの各々の位置と前記視差角とに基づいて、前記２つ以上の異なるカメラのうちの少なくとも１つのカメラから前記乗員の１つ以上の目までの距離を求めるステップと、
前記距離と前記２つ以上の異なるカメラの各々の位置とに基づいて、前記乗員の１つ以上の目の位置を求めるステップとを含む、請求項１〜５のいずれか１項に記載の方法。 The at least one image comprises at least one image captured by each of two or more different cameras in the camera system, and the step of determining the position of one or more eyes of the occupant in the vehicle is a step.
A step of determining the parallax angle based on at least one image captured by each of the two or more different cameras.
A step of determining the distance from at least one of the two or more different cameras to one or more eyes of the occupant based on the respective positions of the two or more different cameras and the parallax angle. ,
The method of any one of claims 1-5, comprising the step of determining the position of one or more eyes of the occupant based on the distance and the position of each of the two or more different cameras. ..
前記画像の歪みに基づいて、前記赤外線カメラから前記乗員の１つ以上の目までの距離を求めるステップと、
前記赤外線カメラの位置と前記距離とに基づいて、前記乗員の１つ以上の目の位置を求めるステップとを含む、請求項１〜５のいずれか１項に記載の方法。 The at least one image includes an image captured by the infrared camera of the camera system, and the step of determining the position of one or more eyes of the occupant in the vehicle is a step.
A step of determining the distance from the infrared camera to one or more eyes of the occupant based on the distortion of the image.
The method of any one of claims 1-5, comprising the step of finding the position of one or more eyes of the occupant based on the position of the infrared camera and the distance.
前記複数の関心領域の各々の位置は車両ベースの座標系を用いて特定され、前記車両ベースの座標系の重心は前記車両の内部に位置し前記１つのカメラの位置と異なっており、
前記乗員が見ている関心領域を求めるステップは、
前記１つ以上の目の位置を、前記カメラベースの座標系から前記車両ベースの座標系に変換するステップと、
前記視線ベクトルを、前記車両ベースの座標系を用いて特定した前記１つ以上の目の位置から延ばしたものが、前記複数の関心領域のうちのいずれかと交差するか否かを判断するステップと、
前記視線ベクトルが、前記複数の関心領域のうちの特定の関心領域と交差すると判断したことに応じて、前記特定の関心領域は前記乗員が見ている関心領域であると判断するステップとを含む、請求項１〜７のいずれか１項に記載の方法。 The position of one or more eyes of the occupant in the vehicle is identified using a camera-based coordinate system centered on one camera of the camera system.
The position of each of the plurality of areas of interest is specified using a vehicle-based coordinate system, and the center of gravity of the vehicle-based coordinate system is located inside the vehicle and is different from the position of the one camera.
The step of finding the area of interest that the occupant is looking at is
A step of converting the one or more eye positions from the camera-based coordinate system to the vehicle-based coordinate system.
A step of determining whether an extension of the line-of-sight vector from the position of one or more eyes identified using the vehicle-based coordinate system intersects any of the plurality of regions of interest. ,
The step includes a step of determining that the specific region of interest is the region of interest seen by the occupant in response to the determination that the line-of-sight vector intersects the specific region of interest among the plurality of regions of interest. , The method according to any one of claims 1 to 7.
前記少なくとも１つのプロセッサによって実行されると請求項１〜９のいずれか１項に記載の方法を前記少なくとも１つのプロセッサに実行させる命令を含むメモリとを備える、コンピューティングデバイス。 With at least one processor
A computing device comprising a memory comprising an instruction to cause the at least one processor to perform the method of any one of claims 1-9 when executed by the at least one processor.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2023006841A JP2023052530A (en) | 2019-06-17 | 2023-01-19 | Method to be executed by computer, computing device, computing system, computer program, and vehicle |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962862561P | 2019-06-17 | 2019-06-17 | |
US62/862,561 | 2019-06-17 | ||
PCT/US2019/061025 WO2020256764A1 (en) | 2019-06-17 | 2019-11-12 | Vehicle occupant engagement using three-dimensional eye gaze vectors |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2023006841A Division JP2023052530A (en) | 2019-06-17 | 2023-01-19 | Method to be executed by computer, computing device, computing system, computer program, and vehicle |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2021531522A true JP2021531522A (en) | 2021-11-18 |
JP7402796B2 JP7402796B2 (en) | 2023-12-21 |
Family
ID=69165553
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2020529676A Active JP7402796B2 (en) | 2019-06-17 | 2019-11-12 | Method, computing device, computing system, computer program, and vehicle for determining vehicle occupant involvement using three-dimensional gaze vectors |
JP2023006841A Pending JP2023052530A (en) | 2019-06-17 | 2023-01-19 | Method to be executed by computer, computing device, computing system, computer program, and vehicle |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2023006841A Pending JP2023052530A (en) | 2019-06-17 | 2023-01-19 | Method to be executed by computer, computing device, computing system, computer program, and vehicle |
Country Status (6)
Country | Link |
---|---|
US (2) | US11527082B2 (en) |
EP (1) | EP3776347A1 (en) |
JP (2) | JP7402796B2 (en) |
KR (1) | KR102385874B1 (en) |
CN (1) | CN112424788A (en) |
WO (1) | WO2020256764A1 (en) |
Families Citing this family (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN110281950B (en) * | 2019-07-08 | 2020-09-04 | 睿镞科技(北京)有限责任公司 | Three-dimensional acoustic image sensor-based vehicle control and visual environment experience |
US11574244B2 (en) * | 2019-09-12 | 2023-02-07 | International Business Machines Corporation | States simulator for reinforcement learning models |
CN111176524B (en) * | 2019-12-25 | 2021-05-28 | 歌尔股份有限公司 | Multi-screen display system and mouse switching control method thereof |
US11381730B2 (en) * | 2020-06-25 | 2022-07-05 | Qualcomm Incorporated | Feature-based image autofocus |
US11908208B2 (en) * | 2020-10-20 | 2024-02-20 | Toyota Motor Engineering & Manufacturing North America, Inc. | Interface sharpness distraction mitigation method and system |
JP2022094744A (en) * | 2020-12-15 | 2022-06-27 | キヤノン株式会社 | Subject motion measuring device, subject motion measuring method, program, and imaging system |
WO2022232875A1 (en) * | 2021-05-05 | 2022-11-10 | Seeing Machines Limited | Systems and methods for detection of mobile device use by a vehicle driver |
US11951833B1 (en) * | 2021-05-16 | 2024-04-09 | Ambarella International Lp | Infotainment system permission control while driving using in-cabin monitoring |
US20220392261A1 (en) * | 2021-06-04 | 2022-12-08 | Rockwell Collins, Inc. | Pilot safety system with context-sensitive scan pattern monitoring and alerting |
CN113525402B (en) * | 2021-07-20 | 2023-06-02 | 张鹏 | Advanced assisted driving and unmanned visual field intelligent response method and system |
KR102644877B1 (en) * | 2021-08-20 | 2024-03-08 | 주식회사 경신 | Apparatus and method for controlling vehicle |
KR102597068B1 (en) * | 2021-11-16 | 2023-10-31 | 전남대학교 산학협력단 | Vehicle device for determining a driver's gaze state using artificial intelligence and control method thereof |
US20230161564A1 (en) * | 2021-11-22 | 2023-05-25 | Jpmorgan Chase Bank, N.A. | System and method for adding no-code machine learning and artificial intelligence capabilities to intelligence tools |
US20240005698A1 (en) * | 2022-06-29 | 2024-01-04 | Microsoft Technology Licensing, Llc | Accurate head pose and eye gaze signal analysis |
US11862016B1 (en) * | 2022-07-19 | 2024-01-02 | Jiangsu University | Multi-intelligence federal reinforcement learning-based vehicle-road cooperative control system and method at complex intersection |
CN115797453B (en) * | 2023-01-17 | 2023-06-16 | 西南科技大学 | Positioning method and device for infrared weak target and readable storage medium |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2000113186A (en) * | 1998-10-01 | 2000-04-21 | Mitsubishi Electric Inf Technol Center America Inc | System and method for classifying gazing direction |
JP2017516219A (en) * | 2014-05-01 | 2017-06-15 | ジャガー ランド ローバー リミテッドＪａｇｕａｒ Ｌａｎｄ Ｒｏｖｅｒ Ｌｉｍｉｔｅｄ | Control device and related method |
Family Cites Families (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7508979B2 (en) | 2003-11-21 | 2009-03-24 | Siemens Corporate Research, Inc. | System and method for detecting an occupant and head pose using stereo detectors |
JP2007265367A (en) | 2006-03-30 | 2007-10-11 | Fujifilm Corp | Program, apparatus and method for detecting line of sight |
JP4725595B2 (en) * | 2008-04-24 | 2011-07-13 | ソニー株式会社 | Video processing apparatus, video processing method, program, and recording medium |
CN102378998B (en) * | 2009-12-10 | 2014-12-10 | 松下电器产业株式会社 | Information display apparatus and information display method |
US8408706B2 (en) * | 2010-12-13 | 2013-04-02 | Microsoft Corporation | 3D gaze tracker |
US9405982B2 (en) * | 2013-01-18 | 2016-08-02 | GM Global Technology Operations LLC | Driver gaze detection system |
US9625723B2 (en) * | 2013-06-25 | 2017-04-18 | Microsoft Technology Licensing, Llc | Eye-tracking system using a freeform prism |
KR101795264B1 (en) | 2016-05-31 | 2017-12-01 | 현대자동차주식회사 | Face landmark detection apparatus and verification method of the same |
US20170353714A1 (en) * | 2016-06-06 | 2017-12-07 | Navid Poulad | Self-calibrating display system |
US10049571B2 (en) * | 2016-06-29 | 2018-08-14 | Toyota Jidosha Kabushiki Kaisha | Situational understanding of unknown roadway conditions that are ahead for a connected vehicle |
KR101914362B1 (en) | 2017-03-02 | 2019-01-14 | 경북대학교 산학협력단 | Warning system and method based on analysis integrating internal and external situation in vehicle |
CN108334810B (en) * | 2017-12-25 | 2020-12-11 | 北京七鑫易维信息技术有限公司 | Method and device for determining parameters in gaze tracking device |
CN109493305A (en) * | 2018-08-28 | 2019-03-19 | 初速度（苏州）科技有限公司 | A kind of method and system that human eye sight is superimposed with foreground image |
US11301677B2 (en) * | 2019-06-14 | 2022-04-12 | Tobil AB | Deep learning for three dimensional (3D) gaze prediction |
-
2019
- 2019-11-12 KR KR1020207016245A patent/KR102385874B1/en active IP Right Grant
- 2019-11-12 EP EP19836702.1A patent/EP3776347A1/en active Pending
- 2019-11-12 WO PCT/US2019/061025 patent/WO2020256764A1/en unknown
- 2019-11-12 CN CN201980007318.3A patent/CN112424788A/en active Pending
- 2019-11-12 US US16/764,313 patent/US11527082B2/en active Active
- 2019-11-12 JP JP2020529676A patent/JP7402796B2/en active Active
-
2022
- 2022-11-09 US US18/054,102 patent/US11847858B2/en active Active
-
2023
- 2023-01-19 JP JP2023006841A patent/JP2023052530A/en active Pending
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2000113186A (en) * | 1998-10-01 | 2000-04-21 | Mitsubishi Electric Inf Technol Center America Inc | System and method for classifying gazing direction |
JP2017516219A (en) * | 2014-05-01 | 2017-06-15 | ジャガー ランド ローバー リミテッドＪａｇｕａｒ Ｌａｎｄ Ｒｏｖｅｒ Ｌｉｍｉｔｅｄ | Control device and related method |
Non-Patent Citations (2)
Title |
---|
CONSTANTIN CARAPENCEA ET AL.: "REAL-TIME GAZE TRACKING WITH A SINGLE CAMERA", JOURNAL OF INFORMATION SYSTEMS & OPERATIONS MANAGEMENT, vol. Vol.9(1), JPN7021005373, 1 January 2015 (2015-01-01), US, pages 1 - 13, ISSN: 0004877220 * |
CUDALBU C ET AL.: "Driver monitoring with a single high-speed camera and IR illumination", SIGNALS, CIRCUITS AND SYSTEMS, 2005. ISSCS 2005. INTERNATIONAL SYMPOSI UM ON IASI, vol. Vol.1(14), JPN6021048989, 14 July 2005 (2005-07-14), US, pages 219 - 222, ISSN: 0004877221 * |
Also Published As
Publication number | Publication date |
---|---|
US20230088021A1 (en) | 2023-03-23 |
KR102385874B1 (en) | 2022-04-12 |
US20210397859A1 (en) | 2021-12-23 |
WO2020256764A1 (en) | 2020-12-24 |
US11847858B2 (en) | 2023-12-19 |
US11527082B2 (en) | 2022-12-13 |
EP3776347A1 (en) | 2021-02-17 |
JP7402796B2 (en) | 2023-12-21 |
JP2023052530A (en) | 2023-04-11 |
KR20200145825A (en) | 2020-12-30 |
CN112424788A (en) | 2021-02-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7402796B2 (en) | Method, computing device, computing system, computer program, and vehicle for determining vehicle occupant involvement using three-dimensional gaze vectors | |
Das et al. | Opportunities and challenges in explainable artificial intelligence (xai): A survey | |
Seliya et al. | A literature review on one-class classification and its potential applications in big data | |
US10037471B2 (en) | System and method for image analysis | |
Dequaire et al. | Deep tracking in the wild: End-to-end tracking using recurrent neural networks | |
Mafeni Mase et al. | Benchmarking deep learning models for driver distraction detection | |
Liu et al. | DSDCLA: Driving style detection via hybrid CNN-LSTM with multi-level attention fusion | |
WO2020190480A1 (en) | Classifying an input data set within a data category using multiple data recognition tools | |
US20220249906A1 (en) | On-device activity recognition | |
Chen et al. | Vehicles driving behavior recognition based on transfer learning | |
US20210279514A1 (en) | Vehicle manipulation with convolutional image processing | |
Kim et al. | Anomaly monitoring framework in lane detection with a generative adversarial network | |
Mou et al. | Isotropic self-supervised learning for driver drowsiness detection with attention-based multimodal fusion | |
Li et al. | Smart work package learning for decentralized fatigue monitoring through facial images | |
Zhang et al. | Integrating visual large language model and reasoning chain for driver behavior analysis and risk assessment | |
US10943099B2 (en) | Method and system for classifying an input data set using multiple data representation source modes | |
Fusek et al. | Driver state detection from in-car camera images | |
Fayyad | Out-of-distribution detection using inter-level features of deep neural networks | |
Du et al. | DeepBP: A bilinear model integrating multi-order statistics for fine-grained recognition | |
Wu et al. | Forward collision warning system using multi-modal trajectory prediction of the intelligent vehicle | |
EP4200844A1 (en) | Enhanced computing device representation of audio | |
COMUNI et al. | Driver Behavior Classification in Electric Vehicles | |
Subbaiah et al. | Driver drowsiness detection system based on infinite feature selection algorithm and support vector machine | |
Cardinal et al. | Identifying Impaired State for a Driver | |
Du et al. | Incorporating bidirectional feature pyramid network and lightweight network: a YOLOv5-GBC distracted driving behavior detection model |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20200904 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20200904 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20211207 |
|
A601 | Written request for extension of time |
Free format text: JAPANESE INTERMEDIATE CODE: A601Effective date: 20220304 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20220419 |
|
A02 | Decision of refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A02Effective date: 20220920 |
|
C60 | Trial request (containing other claim documents, opposition documents) |
Free format text: JAPANESE INTERMEDIATE CODE: C60Effective date: 20230119 |
|
C22 | Notice of designation (change) of administrative judge |
Free format text: JAPANESE INTERMEDIATE CODE: C22Effective date: 20230307 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230920 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20231211 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7402796Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |