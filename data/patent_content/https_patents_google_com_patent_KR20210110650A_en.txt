KR20210110650A - Supplement your automatic assistant with voice input based on selected suggestions - Google Patents
Supplement your automatic assistant with voice input based on selected suggestions Download PDFInfo
- Publication number
- KR20210110650A KR20210110650A KR1020217023853A KR20217023853A KR20210110650A KR 20210110650 A KR20210110650 A KR 20210110650A KR 1020217023853 A KR1020217023853 A KR 1020217023853A KR 20217023853 A KR20217023853 A KR 20217023853A KR 20210110650 A KR20210110650 A KR 20210110650A
- Authority
- KR
- South Korea
- Prior art keywords
- user
- utterance
- spoken utterance
- speech
- determining
- Prior art date
Links
- 239000013589 supplement Substances 0.000 title claims description 7
- 230000009471 action Effects 0.000 claims abstract description 143
- 238000000034 method Methods 0.000 claims description 125
- 230000004044 response Effects 0.000 claims description 77
- 238000012545 processing Methods 0.000 claims description 74
- 238000006243 chemical reaction Methods 0.000 claims description 58
- 230000003993 interaction Effects 0.000 claims description 38
- 230000000875 corresponding effect Effects 0.000 claims description 34
- 230000006870 function Effects 0.000 claims description 28
- 230000001276 controlling effect Effects 0.000 claims description 10
- 238000009877 rendering Methods 0.000 claims description 6
- 230000004048 modification Effects 0.000 claims description 2
- 238000012986 modification Methods 0.000 claims description 2
- 238000012544 monitoring process Methods 0.000 claims description 2
- 238000004590 computer program Methods 0.000 claims 1
- 230000008859 change Effects 0.000 description 31
- 238000004891 communication Methods 0.000 description 9
- 230000008569 process Effects 0.000 description 8
- 230000002452 interceptive effect Effects 0.000 description 6
- 230000002093 peripheral effect Effects 0.000 description 5
- 230000000153 supplemental effect Effects 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 230000015654 memory Effects 0.000 description 3
- 230000009466 transformation Effects 0.000 description 3
- 230000001934 delay Effects 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000005108 dry cleaning Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000000737 periodic effect Effects 0.000 description 2
- 230000001502 supplementing effect Effects 0.000 description 2
- 239000002699 waste material Substances 0.000 description 2
- 241000699670 Mus sp. Species 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 230000002547 anomalous effect Effects 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 238000013475 authorization Methods 0.000 description 1
- 238000012512 characterization method Methods 0.000 description 1
- 230000003111 delayed effect Effects 0.000 description 1
- 230000001771 impaired effect Effects 0.000 description 1
- 230000000116 mitigating effect Effects 0.000 description 1
- 239000010813 municipal solid waste Substances 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000001737 promoting effect Effects 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3322—Query formulation using system suggestions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3325—Reformulation based on results of preceding query
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
- G06F16/90324—Query formulation using system suggestions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
- G06F16/90332—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/04—Segmentation; Word boundary detection
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/225—Feedback of the input speech
Abstract
본 명세서에 기술된 구현예는 사용자가 자동 어시스턴트와의 현재 및/또는 후속 대화 세션에 참여할 빈도 및/또는 시간 길이를 감소시키기 위해 디스플레이 양식을 통해 자동 어시스턴트에 대한 음성 발화를 완성하기 위한 제안들을 제공하는 것과 관련된다. 사용자 요청은 진행 중인 음성 발화의 콘텐츠 및 임의의 선택된 제안 요소들의 콘텐츠로부터 컴파일될 수 있다. 사용자 요청의 현재 컴파일된 부분(선택된 제안(들)의 콘텐츠 및 불완전한 음성 발화)이 자동 어시스턴트를 통해 수행될 수 있는 경우, 사용자 요청의 현재 컴파일된 부분에 대응하는 모든 액션이 자동 어시스턴트를 통해 수행될 수 있다. 또한, 식별 가능한 컨텍스트와 함께 액션들의 수행으로 인한 추가 콘텐츠를 추가 제안을 제공하는데 사용할 수 있다.An implementation described herein provides suggestions for completing a spoken utterance for an automated assistant via a display modality to reduce the frequency and/or length of time a user will participate in a current and/or subsequent conversation session with the automated assistant. related to doing The user request may be compiled from the content of the voice utterance in progress and the content of any selected suggestion elements. If the currently compiled portion of the user request (the content of the selected suggestion(s) and incomplete speech utterances) can be performed through the automatic assistant, then all actions corresponding to the currently compiled portion of the user request will be performed through the automatic assistant. can Additionally, additional content resulting from the performance of actions along with an identifiable context may be used to provide additional suggestions.
Description
사람은 본 명세서에서 "자동화된 어시스턴트"("디지털 에이전트", "챗봇", "인터랙티브 퍼스널 어시스턴트", "지능형 퍼스널 어시스턴트", "어시스턴트 애플리케이션", "대화형 에이전트들"로도 지칭됨)로 지칭되는 인터렉티브 소프트웨어 어플리케이션과의 사람-컴퓨터 간 대화에 참여할 수 있다. 예를 들어, 사람(자동화된 어시스턴트와 인터렉션할 때 "사용자"라고 할 수 있음)은 일부 경우에 텍스트로 변환된 다음 프로세싱될 수 있는 발화된 자연어 입력(즉, 발언)을 사용하여 및/또는 텍스트(예를 들어, 타이핑된) 자연어 입력을 제공함으로써 명령 및/또는 요청을 자동화된 어시스턴트에 제공할 수 있다. 자동화된 어시스턴트는 청각 및/또는 시각적 사용자 인터페이스 출력을 포함할 수 있는 응답 사용자 인터페이스 출력을 제공함으로써 요청에 응답한다. 따라서, 자동 어시스턴트는 음성 기반 사용자 인터페이스를 제공할 수 있다. Humans are referred to herein as "automated assistants" (also referred to as "digital agents", "chatbots", "interactive personal assistants", "intelligent personal assistants", "assistant applications", "interactive agents"). Participate in human-computer conversations with interactive software applications. For example, a human (which may be referred to as a “user” when interacting with an automated assistant) uses spoken natural language input (i.e., utterances), which in some cases may be converted to text and then processed, and/or text Commands and/or requests may be provided to the automated assistant by providing natural language input (eg, typed). The automated assistant responds to the request by providing a responsive user interface output, which may include audible and/or visual user interface output. Accordingly, the automated assistant may provide a voice-based user interface.
일부 경우에, 사용자는 자동 어시스턴트 액션의 수행을 유발하도록 사용자에 의해 의도되었지만 의도된 자동 어시스턴트 액션의 수행을 발생시키지 않는 음성 발화를 제공할 수 있다. 예를 들어, 음성 발화는 자동 어시스턴트가 이해할 수 없는 구문으로 제공될 수 있고 및/또는 자동 어시스턴트 액션에 대한 필수 파라미터(들)가 없을 수 있다. 그 결과, 자동 어시스턴트는 음성 발화를 완전히 프로세싱하지 못하고, 그것이 자동 어시스턴트 액션에 대한 요청인지 판단할 수 없다. 이로 인해 자동 어시스턴트가 음성 발화에 응답하지 않거나 "죄송합니다. 도와드릴 수 없습니다" 및/또는 오류 톤과 같은 응답으로 오류를 제공한다. 자동 어시스턴트가 음성 발화의 의도된 액션을 수행하지 못함에도 불구하고, 다양한 컴퓨터 및/또는 네트워크 리소스가 음성 발화를 프로세싱하고 적절한 액션을 해결하려고 시도하는데 소비된다. 예를 들어, 음성 발화에 대응하는 오디오 데이터가 전송되고, 음성-텍스트 변환 프로세싱 및/또는 자연어 프로세싱이 수행될 수 있다. 의도된 자동 어시스턴트 액션이 수행되지 않기 때문에 이러한 리소스 소비는 낭비이며, 사용자는 나중에 의도된 자동 어시스턴트 액션의 수행을 추구할 때 재구성된 음성 발화를 제공하려고 시도할 가능성이 높다. 더욱이, 그러한 재구성된 음성 발화도 프로세싱 처리되어야 할 것이다. 또한, 이는 사용자가 처음에 자동 어시스턴트에 적절한 음성 발화를 제공한 경우와 비교할 때 자동 어시스턴트 액션의 수행에 지연을 초래한다.In some cases, the user may provide a spoken utterance intended by the user to cause the performance of the automatic assistant action, but that does not result in the performance of the intended automatic assistant action. For example, the spoken utterance may be provided in a syntax that the automated assistant does not understand and/or may lack the required parameter(s) for the automated assistant action. As a result, the automatic assistant does not fully process the spoken utterance and cannot determine whether it is a request for an automatic assistant action. This causes the automated assistant to not respond to spoken utterances or provide errors with responses such as "I'm sorry, I can't help you" and/or an error tone. Although the automated assistant fails to perform the intended action of the spoken utterance, various computer and/or network resources are expended in processing the voice utterance and attempting to resolve the appropriate action. For example, audio data corresponding to a voice utterance may be transmitted, and voice-to-text conversion processing and/or natural language processing may be performed. This consumption of resources is wasteful because the intended automatic assistant action is not performed, and the user is likely to later attempt to provide a reconstructed speech utterance when seeking performance of the intended automatic assistant action. Moreover, such reconstructed speech utterances would also have to be processed. Also, this results in a delay in performance of the automated assistant action as compared to the case where the user initially provided an appropriate speech utterance to the automated assistant.
본 명세서에 기술된 구현예는 자동 어시스턴트로 향하는 음성 발화를 완성하기 위한 제안을 디스플레이 양식을 통해 제공하고, 음성 발화가 진행되는 동안 다양한 요인에 기초하여 업데이트된 제안을 선택적으로 지속적으로 제공하는 것과 관련된다. 예를 들어, 자동 어시스턴트가 사용자에 의해 처음에 의도된 자동 어시스턴트 액션(들)을 종합적으로 수행하는 동시에 사용자와 자동 어시스턴트 간의 인터렉션을 간소화하는 실행 가능한 대화 문구를 개발하기 위해 제안이 제공될 수 있다. 추가적으로, 또는 대안적으로, 사용자가 자동 어시스턴트와의 현재 대화 세션 및/또는 후속 대화 세션에 참여할 빈도 및/또는 시간 길이를 줄일 수 있는 액션들을 수행하기 위해 자동 어시스턴트를 사용하도록 사용자를 장려하는 제안이 제공될 수 있다. Implementations described herein relate to providing, via a display form, suggestions for completing a spoken utterance directed to an automated assistant, and selectively and continuously providing updated suggestions based on various factors while the spoken utterance is in progress. do. For example, suggestions may be provided to develop actionable dialogue phrases that streamline the interaction between the user and the automated assistant while the automated assistant collectively performs the automated assistant action(s) initially intended by the user. Additionally, or alternatively, there is a proposal to encourage a user to use an automated assistant to perform actions that may reduce the frequency and/or length of time the user will participate in a current and/or subsequent conversation session with the automated assistant. may be provided.
일부 구현예에서, 사용자와 자동 어시스턴트 사이의 대화 세션 동안, 사용자는 자동 어시스턴트로 하여금 하나 이상의 액션들을 수행하게 하는 음성 발화를 제공할 수 있다. 음성 발화는 오디오 데이터에 의해 캡처될 수 있고, 오디오 데이터에 대해 수행되는 음성-텍스트 변환 프로세싱은 각각 음성 발화의 대응하는 해석인 하나 이상의 텍스트 세그먼트들을 생성한다. 하나 이상의 텍스트 세그먼트들은 음성 발화가 완전한 요청 및/또는 실행 가능한 요청에 대응하는지를 결정하기 위해 프로세싱될 수 있다. 본 명세서에서 사용된 바와 같이, "완전한 요청" 또는 "실행 가능한 요청"은 자동 어시스턴트에 의해 완전히 프로세싱될 때 하나 이상의 대응하는 자동 어시스턴트 액션을 수행하게 하는 것이며, 수행된 자동 어시스턴트 액션(들)은 기본 오류 유형 액션들이 아니다(예: "죄송합니다. 도와 드릴 수 없습니다"라는 음성 응답이 아님). 예를 들어, 실행 가능한 요청은 하나 이상의 스마트 디바이스들이 제어되게 할 수 있고, 특정 가청 및/또는 그래픽 콘텐츠가 렌더링되게 할 수 있다. 음성 발화가 불완전한 요청에 대응하는 것으로 결정되면, 하나 이상의 텍스트 세그먼트들이 요청을 완성하기 위한 하나 이상의 제안을 결정하고 제공하기 위해 추가로 프로세싱될 수 있다. 하나 이상의 제안들 중 각 제안은 하나 이상의 텍스트 세그먼트들 중 대응하는 텍스트 세그먼트와 결합될 때 완전한 요청을 제공하는 추가 텍스트를 포함할 수 있다. 즉, 사용자가 특정 제안의 텍스트(또는 특정 제안을 기반으로 하는 텍스트와 일반적으로 일치하는 대체 텍스트)를 말하여 대화 세션을 계속하면, 자동 어시스턴트는 대응하는 자동 어시스턴트 액션을 수행할 것이다. 따라서 제안 생성은 사용자가 작업을 수행하는데 도움이 된다. 사용자가 요청을 입력하는 동안, 사용자에게 어시스턴트와 인터렉션할 때 사용자를 안내하는 정보가 제공된다. 사용자에게 제시되는 정보는 사용자의 기존 입력에 기초하며, 객관적인 분석을 사용하여 사용자에게 정보를 제공한다. 따라서 사용자는 작업을 수행하는데 도움이 되도록 객관적으로 관련된 정보를 제공받는다.In some implementations, during a conversation session between the user and the automated assistant, the user can provide a spoken utterance that causes the automated assistant to perform one or more actions. A spoken utterance may be captured by audio data, and the speech-to-text conversion processing performed on the audio data generates one or more text segments, each of which is a corresponding interpretation of the spoken utterance. One or more text segments may be processed to determine if the spoken utterance corresponds to a complete request and/or an actionable request. As used herein, a "full request" or "executable request" is one that, when fully processed by an automatic assistant, causes one or more corresponding automatic assistant actions to be performed, the automatic assistant action(s) performed being the default Not error-type actions (eg, not a voice response saying "I'm sorry, I can't help you"). For example, the executable request may cause one or more smart devices to be controlled, and may cause certain audible and/or graphical content to be rendered. If it is determined that the spoken utterance corresponds to an incomplete request, one or more text segments may be further processed to determine and provide one or more suggestions for completing the request. Each one of the one or more suggestions may include additional text providing a complete request when combined with a corresponding one of the one or more text segments. That is, if the user continues the conversation session by saying the text of a specific suggestion (or alternative text that generally matches the text based on the specific suggestion), the automated assistant will perform the corresponding automated assistant action. Thus, the creation of suggestions helps the user to get the job done. While the user is entering a request, information is provided to guide the user in interacting with the assistant. The information presented to the user is based on the user's existing input, and objective analysis is used to present the information to the user. Therefore, users are provided with objectively relevant information to help them perform tasks.
일부 구현예에서, 컴퓨팅 디바이스(들)는 음성 발화의 상이한 해석을 생성할 수 있고, 이러한 해석의 변화는 음성 발화에 응답하여 제공되는 제안에 대응하는 변화를 초래할 수 있다. 예를 들어, "Assistant, could you please change…"와 같은 음성 발화는 컴퓨팅 디바이스에 의한 다수의 해석에 대응할 수 있다. 앞서 언급된 음성 발화는 자동 어시스턴트를 통한 IoT 디바이스 설정 변경 요청 또는 자동 어시스턴트를 통한 애플리케이션 설정 변경 요청으로 해석될 수 있다. 컴퓨팅 디바이스와 통신하는 디스플레이 패널을 통해 사용자에게 제시될 수 있는 대응하는 제안을 생성하기 위해 각 해석이 프로세싱될 수 있다. 예를 들어, 컴퓨팅 디바이스는 디스플레이 패널로 하여금 음성 발화의 두 해석에 기반한 제안들의 리스트와 함께 음성 발화의 초기 텍스트(예: "Assistant, could you please change…")를 표시하게 할 수 있다. 제안들은 하나의 해석에 기반한 하나 이상의 제안과 다른 해석에 기반한 하나 이상의 다른 제안을 포함할 수 있다.In some implementations, the computing device(s) may generate different interpretations of a spoken utterance, and a change in such interpretation may result in a change corresponding to a suggestion provided in response to the spoken utterance. For example, a spoken utterance such as "Assistant, could you please change..." may correspond to multiple interpretations by the computing device. The aforementioned voice utterance may be interpreted as a request to change IoT device settings through an automatic assistant or a request to change application settings through an automatic assistant. Each interpretation may be processed to generate a corresponding offer that may be presented to a user via a display panel in communication with the computing device. For example, the computing device may cause the display panel to display the initial text of the spoken utterance (eg, “Assistant, could you please change…”) along with a list of suggestions based on the two interpretations of the spoken utterance. The proposals may include one or more proposals based on one interpretation and one or more other proposals based on another interpretation.
예를 들어, "IoT 디바이스 설정 변경" 해석에 기초한 적어도 하나의 제안은 "온도 조절기를 72로 설정"일 수 있으며, 이는 사용자가 말한 경우 원래 음성 발화를 보충하는 텍스트일 수 있다. 예를 들어, 디스플레이 패널은 사용자가 적어도 하나의 제안의 텍스트를 말하는 것에 응답하여, 원래의 음성 발화 및 적어도 하나의 제안을 함께 제시할 수 있다. 즉, 디스플레이 패널에 "Assistant, could you please change the thermostat to 72"라는 텍스트가 제시될 수 있다. 사용자는 제시된 텍스트를 보고, 제안되었던 부분(즉, "the thermostat to 72") 및 원래 음성 발화에 포함되지 않은 제안된 부분을 식별할 수 있다(즉, "Assistant, could you please change…"). 제안된 부분을 식별하면, 사용자는 제안된 부분의 텍스트를 말하여, 자동 어시스턴트로 하여금 사용자의 온도 조절기 설정을 72로 변경하는 등의 액션을 완료하게 할 수 있다. 일부 구현예에서, 디스플레이 패널은 제안된, 아직 말하지 않은 부분만을 포함하는 텍스트를 제안으로서 제시할 수 있다(예를 들어, "Assistant, could you please"를 제시하지 않고 "the thermostat to 72"만 제시). 이렇게 하면 제안을 쉽게 볼 수 있고 사용자가 제안을 신속하게 보게 하고, "the thermostat to 72" 또는 "the thermostat to 70"의 추가 음성 입력과 같은 실행 가능한 요청을 만들어내기 위해 사용자에게 제공할 수 있는 추가 음성 입력을 확인할 수 있게 한다. For example, at least one suggestion based on the interpretation of “change IoT device settings” may be “set thermostat to 72”, which may be text that supplements the original spoken utterance if spoken by the user. For example, in response to the user speaking the text of the at least one suggestion, the display panel may present the original voice utterance and the at least one suggestion together. That is, the text "Assistant, could you please change the thermostat to 72" may be presented on the display panel. The user can view the presented text and identify which portion was suggested (ie, “the thermostat to 72”) and the suggested portion that was not included in the original spoken utterance (ie, “Assistant, could you please change…”). Once the suggested portion is identified, the user can speak the text of the suggested portion to have the automatic assistant complete an action, such as changing the user's thermostat setting to 72. In some implementations, the display panel can present as a suggestion text that includes only the suggested, not-yet-spoken portion (eg, “the thermostat to 72” rather than “Assistant, could you please”). ). This makes it easy to view the offer and allows the user to quickly view the offer, plus additional things that can be provided to the user to create an actionable request, such as an additional voice input of "the thermostat to 72" or "the thermostat to 70". Allows you to check voice input.
다양한 구현예에서, 제안의 텍스트 부분은 제안의 텍스트 부분에 따라 추가 음성 입력을 제공한 결과로 수행될 액션을 나타내는 아이콘 또는 다른 그래픽 요소와 함께 선택적으로 제시될 수 있다. 예를 들어, 제안은 "please change the thermostat to 72"라는 텍스트 부분과 함께 온도 조절기의 그래픽 표시 및 제안에 따라 추가 음성 입력이 온도 조절기의 설정 포인트를 변경할 것이라고 표시하기 위한 위쪽 및 아래쪽 화살표를 포함할 수 있다. 다른 예로서, 제안은 재생 아이콘이 있는 텔레비전 아웃라인과 함께 "play TV station on living room TV"이라는 텍스트 부분을 포함하여, 제안에 따라 추가 음성 입력이 스트리밍 콘텐츠를 텔레비전에 재생하게 될 것임을 표시할 수 있다. 또 다른 예로서, 제안은 X가 겹쳐진 전구 아이콘과 함께 "turn off the living room lights"을 포함할 수 있고, 제안에 따라 추가 음성 입력이 조명이 끌 것임을 표시할 수 있다. 이러한 아이콘은 제안을 더 쉽게 볼 수 있게 하여, 사용자가 먼저 아이콘을 빠르게 볼 수 있게 하고, 아이콘이 사용자가 원하는 액션과 일치하는 경우 제안의 텍스트 부분을 볼 수 있게 한다. 이를 통해 사용자는 동시에 제시된 다수의 제안의 그래픽 요소를 빠르게 볼 수 있고(해당 텍스트 부분을 초기적으로 읽지 않고), 사용자의 의도에 맞는 것을 확인한 다음 해당 텍스트 부분만 읽을 수 있다. 이렇게 하면 다수의 제안을 제공할 때 사용자가 추가 음성 입력을 제공하는 지연 시간을 줄일 수 있으며, 결과적으로 결과 액션 수행의 지연 시간을 줄일 수 있다. In various implementations, the textual portion of the proposal may optionally be presented along with an icon or other graphical element representing an action to be performed as a result of providing additional voice input in accordance with the textual portion of the proposal. For example, the suggestion could include a graphical display of the thermostat along with the text part "please change the thermostat to 72" and up and down arrows to indicate that additional voice input will change the thermostat's setpoint according to the suggestion. can As another example, the offer could include the text portion "play TV station on living room TV" with a television outline with a play icon to indicate that additional voice input according to the proposal will play streaming content on the television. have. As another example, a suggestion may include "turn off the living room lights" with a light bulb icon overlaid with an X, and additional voice input according to the suggestion may indicate that the lights will turn off. These icons make the suggestion easier to see, allowing the user to quickly see the icon first, and then view the text portion of the suggestion if the icon matches the user's desired action. This allows the user to quickly see the graphical elements of multiple proposals presented at the same time (without initially reading the corresponding text part), then confirm that it suits the user's intent, and then read only that text part. This can reduce the delay time for the user to provide additional voice input when providing multiple suggestions, and consequently reduce the delay time for performing the resulting action.
일부 구현예에서, 음성 발화는 사용자가 특정 단어를 표현 및/또는 발음한 방식의 결과로 다르게 해석될 수 있다. 예를 들어, 앞서 언급한 "Assistant, could you please change…"라는 음성 발화를 수신하는 컴퓨팅 디바이스는 음성 발화가 "change"라는 단어를 포함할 X% 확실성을 결정할 수 있고, 음성 발화가 "arrange"라는 단어를 포함하여 "arrange" 기능을 수행하기 위한 요청을 지칭할 Y% 확실성을 결정할 수 있다. 예를 들어, 음성 발화의 음성-텍스트 변환 프로세싱은 서로 다른 확실성을 가진 두 개의 개별 해석("change" 및 "arrange")을 결과로 할 수 있다. 결과적으로, 음성 발화에 응답하는 작업을 맡은 자동 어시스턴트는 각 해석에 기초한 제안을 제시할 수 있다("change" 및 "arrange"). 예를 들어, 자동 어시스턴트는 "change" 해석에 대응하는 "…the thermostat to 72"와 같은 제1 제안과 "arrange" 해석에 대응하는 "…my desktop file folder"와 같은 제2 제안을 그래픽으로 제시할 수 있다. 또한, 사용자가 "the thermostat to 72"(또는 "the thermostat to 80"과 같이 유사한 콘텐츠)를 말하여 제1 제안 콘텐츠를 말하는 경우, (그리고 사용자의 추가 발언에 기초하며) 사용자는 자동 어시스턴트를 통해 "change"기능이 수행되게 할 수 있다. 그러나, 사용자가 "my desktop file folder"(또는 "my documents folder"와 같은 유사한 컨텐츠)를 말하여 제2 제안의 콘텐츠를 말하는 경우, (그리고 사용자의 추가 발언에 기초하며) 사용자는 자동 어시스턴트를 통해 "arrange" 기능이 수행되게 할 수 있다. 즉, 자동 어시스턴트는 말한 제안에 부합하는 해석("change" 및 "arrange")을 활용할 것이다. 예를 들어 "arrange"는 20%의 확률로 예측되었고 "change"은 80%의 확률로 예측되었더라도, 사용자가 "my desktop file folder"를 더 말하면 "arrange"을 활용할 수 있다. 또 다른 예로, "change" 및 "arrange" 예시의 변형으로 "please change the thermostat to 75"라는 제1 제안과 "please arrange my desktop file folder"라는 제2 제안이 제시될 수 있다. 사용자가 터치 입력을 통해 "please arrange my desktop file folder"를 선택하거나 "please arrange my desktop file folder" 또는 "두번째 것"과 같은 추가 음성 입력을 제공하는 경우, "arrange" 해석이 "change" 해석에 대신하여 사용될 수 있다. 즉, 초기적으로 "change"가 더 큰 확률에 기초하여 올바른 해석으로 선택되었음에도 불구하고, "arrange"는 사용자 선택 또는 "arrange" 제안에 관한 추가 음성 입력에 기초하여 선택을 대체할 수 있다. 이러한 방식과 다른 방식으로 음성-텍스트 변환 정확도가 개선될 수 있으며, 특정 상황에서 낮은 확실성 해석이 활용될 수 있다. 이것은 초기 음성 발화의 잘못된 해석이 선택되었고 사용자가 올바른 해석을 선택하게 하기 위해 초기 음성 발화를 반복해야 하는 경우 소비될 수 있는 계산 리소스를 보존할 수 있다. 또한 이것은 사용자가 반복하고 자동 어시스턴트가 가청 응답을 렌더링하기를 기다려야 하는 것을 고려하면, 사용자가 자동 어시스턴트를 사용하는데 소비하는 시간을 줄일 수 있다.In some implementations, spoken utterances may be interpreted differently as a result of the way the user expressed and/or pronounced certain words. For example, a computing device that receives the aforementioned spoken utterance “Assistant, could you please change…” can determine with an X% certainty that the spoken utterance will contain the word “change” and that the spoken utterance “arrange” We can determine the Y% certainty that we will refer to a request to perform the "arrange" function by including the word For example, speech-to-text processing of a speech utterance may result in two separate interpretations (“change” and “arrange”) with different certainties. Consequently, the automated assistant tasked with responding to spoken utterances can present suggestions based on each interpretation (“change” and “arrange”). For example, the automated assistant graphically presents a first suggestion such as "...the thermostat to 72" corresponding to the "change" interpretation and a second suggestion such as "...my desktop file folder" corresponding to the "arrange" interpretation. can do. Also, if the user speaks of the first suggested content by saying "the thermostat to 72" (or similar content such as "the thermostat to 80"), (and based on the user's further utterances) the user can use the automatic assistant to Allows the "change" function to be performed. However, if the user refers to the content of the second offer by saying "my desktop file folder" (or similar content such as "my documents folder"), then (and based on the user's further utterances) the user can use the automatic assistant to Allows an "arrange" function to be performed. That is, the automatic assistant will utilize interpretations ("change" and "arrange") that match the suggested suggestions. For example, even if "arrange" is predicted with a probability of 20% and "change" is predicted with a probability of 80%, "arrange" can be utilized if the user further says "my desktop file folder". As another example, a first suggestion of “please change the thermostat to 75” and a second suggestion of “please arrange my desktop file folder” may be presented as variations of the examples of “change” and “arrange”. If the user selects "please arrange my desktop file folder" via touch input or provides additional voice input, such as "please arrange my desktop file folder" or "second one," the interpretation of "arrange" is replaced by the interpretation of "change". may be used instead. That is, although "change" was initially selected as the correct interpretation based on a greater probability, "arrange" may replace the selection based on user selection or additional voice input regarding "arrange" suggestion. In this and other ways, speech-to-text conversion accuracy may be improved, and low certainty interpretation may be utilized in certain situations. This may conserve computational resources that may be consumed if an incorrect interpretation of the initial spoken utterance has been selected and the initial spoken utterance has to be repeated to get the user to select the correct interpretation. This can also reduce the time the user spends using the automatic assistant, given that the user has to repeat and wait for the automatic assistant to render an audible response.
일부 구현예에서, 제공된 제안 요소는 제안의 제공에 응답하여 수신된 추가 음성 발화의 음성-텍스트 변환 프로세싱을 편향하기 위한 기초로서 사용될 수 있다. 예를 들어, 음성-텍스트 변환 프로세싱은 제공된 제안 요소의 용어(들) 및/또는 예상 카테고리 또는 제안에 대응하는 다른 유형의 콘텐츠를 따르는 용어(들) 쪽으로 편향될 수 있다. 예를 들어, "[musician’s name]"이라는 제공된 제안에 대해, 음성-텍스트 변환 프로세싱은 음악가의 이름(일반적으로 또는 사용자 라이브러리의 음악가)에 편향될 수 있고; "2:00시"라는 제공된 제안에 대해, 음성-텍스트 변환 프로세싱은 "2:00" 또는 일반적으로 시간에 편향될 수 있고, "[smart device]라는 제공된 제안에 대해, 음성-텍스트 변환 프로세싱은 사용자의 스마트 디바이스의 이름으로 편향될 수 있다(예를 들어, 사용자의 저장된 디바이스 토폴로지로부터 확인됨). 음성-텍스트 변환 프로세싱은 예를 들어 음성-텍스트 변환 프로세싱을 수행하는데 사용하기 위한 특정 음성-텍스트 변환 모델(복수의 후보 음성-텍스트 변환 모델로부터)을 선택하고, 특정 용어(들)에 대응하는 후보 변환에 기초하여 후보 변환(음성-텍스트 변환 프로세싱 중에 생성됨)의 점수를 승격하고, 특정 용어(들)에 대응하는 상태 디코딩 그래프의 경로(들)(음성-텍스트 변환 프로세싱에서 사용됨)를 승격함으로써 및/또는 추가 또는 대안적인 음성-텍스트 변환 편향 기법(들)에 의해 특정 용어들을 향해 편향될 수 있다. 예를 들어, 사용자는 "Assistant, set a calendar event for"과 같은 초기 음성 발화를 제공할 수 있으며, 초기 음성 발화 수신에 대한 응답으로 자동 어시스턴트는 다수의 제안 요소들이 컴퓨팅 디바이스의 디스플레이 패널에 나타나게 할 수 있다. 제안 요소들은 예를 들어 "August… September… [Current Month]…"등일 수 있으며, 사용자는 "August"과 같은 후속 음성 발화를 제공하여 특정 제안 요소를 선택할 수 있다. 제안 요소가 1년 중 몇 달을 포함한다는 결정에 응답하여, 자동 어시스턴트는 후속 음성 발화의 음성-텍스트 변환 프로세싱을 1년 중 몇 달로 편향시킬 수 있다. 또 다른 예로서, 수치 입력을 포함(또는 표시)하는 제안 요소를 제공하는 것에 응답하여, 음성-텍스트 변환 프로세싱이 숫자쪽으로 편향될 수 있다. 이러한 방식 및 다른 방식에서, 후속 발화의 음성-텍스트 변환 프로세싱은 제공된 제안 요소에 유리하게 편향될 수 있으며, 이에 따라 음성-텍스트 변환 프로세싱으로부터 예측된 텍스트가 정확할 확률을 높일 수 있다. 이것은 예측된 텍스트가 부정확한 경우(예를 들어, 부정확한 해석을 수정하기 위해 사용자로부터의 추가 입력에 기초하여) 필요한 추가 대화 턴(들)의 발생을 줄일 수 있다. In some implementations, the provided suggestion element may be used as a basis for biasing speech-to-text conversion processing of additional speech utterances received in response to providing the suggestion. For example, speech-to-text conversion processing may be biased towards term(s) of a given suggestion element and/or term(s) following an expected category or other type of content corresponding to the suggestion. For example, for a given suggestion of “[musician’s name]”, speech-to-text processing may be biased towards the name of the musician (generally or a musician in the user library); For a given offer of "2:00 o'clock", speech-to-text processing may be biased towards "2:00" or generally time, and for a given offer of "[smart device], speech-to-text processing is The name of the user's smart device may be biased (eg, ascertained from the user's stored device topology).Speech-to-text processing is, for example, a specific voice-to-text for use in performing voice-to-text processing. Select a transformation model (from a plurality of candidate speech-to-text conversion models), promote the score of the candidate transformation (generated during speech-to-text conversion processing) based on the candidate transformation corresponding to the particular term(s), and select the particular term(s) ( may be biased towards certain terms by promoting the path(s) (used in speech-to-text conversion processing) of the state decoding graph corresponding to For example, the user may provide an initial spoken utterance such as “Assistant, set a calendar event for,” and in response to receiving the initial spoken utterance, the automatic assistant displays a number of suggested elements on the display panel of the computing device. Suggestions can be made to appear, for example "August… September… [Current Month]… "etc., the user may select a particular suggested element by providing a subsequent spoken utterance, such as "August." In response to determining that the suggested element includes months of the year, the automated assistant may select the voice of the subsequent spoken utterance. -text-to-text processing may be biased towards several months of the year As another example, in response to providing a suggestion element that includes (or indicates) numeric input, speech-to-text processing may be biased toward numbers In this and other manners, the speech-to-text conversion processing of subsequent utterances may be biased in favor of a given suggestion factor, thus increasing the probability that the text predicted from the speech-to-text conversion processing is correct. It may reduce the occurrence of additional dialog turn(s) needed if the text is incorrect (eg, based on additional input from the user to correct the incorrect interpretation).
일부 구현예에서, 자동 어시스턴트에게 특정 요청을 완성하기 위한 제안 요소를 제공하기 위한 타이밍은 사용자가 특정 요청을 개시한 컨텍스트에 기초할 수 있다. 예를 들어, 사용자가 차량을 운전하는 동안 음성 발화를 통해 요청의 적어도 일부를 제공한 경우, 자동 어시스턴트는 사용자가 차량을 운전하고 있다고 판단하고 요청 완성을 위한 제안 표시를 지연할 수 있다. 이러한 방식으로, 사용자는 차량 내 디스플레이 패널에 제시되는 그래픽으로 인해 주의가 산만해지는 경우를 줄일 것이다. 추가적으로 또는 대안적으로, 제안 요소의 콘텐츠(예를 들어, 후보 텍스트 세그먼트)는 또한 사용자가 음성 발화를 개시한 컨텍스트에 기초할 수 있다. 예를 들어, 자동 어시스턴트는 사용자의 사전 승인을 받아, 사용자가 거실에서 "Assistant, turn on…"과 같은 음성 발화를 제공했는지 확인할 수 있다. 이에 응답하여, 자동 어시스턴트는 컴퓨팅 디바이스가 사용자의 거실 내의 특정 디바이스를 식별하는 제안 요소를 생성하게 할 수 있다. 제안 요소는 "Living Room TV… Living Room Lights… Living Room Stereo"와 같은 자연어 콘텐츠를 포함할 수 있다. 사용자는 "Living Room Lights"과 같은 후속 음성 발화를 제공하여 제안 요소들 중 하나를 선택할 수 있으며, 이에 대한 응답으로 자동 어시스턴트가 거실 조명을 켤 수 있다. 일부 구현예에서, 제안 요소가 식별하는 디바이스는 사용자와 연관된 다양한 디바이스 간의 관계를 특징 짓는 디바이스 토폴로지 데이터 및/또는 사용자의 위치(예: 가정 또는 사무실) 내의 컴퓨팅 디바이스의 구성에 기초할 수 있다. 디바이스 토폴로지 데이터는 위치 내의 다양한 영역에 대한 식별자뿐만 아니라 디바이스가 각 영역에 있는지 여부를 특징 짓는 디바이스 식별자들을 포함할 수 있다. 예를 들어, 디바이스 토폴로지 데이터는 "거실"과 같은 영역을 식별하고, "TV", "조명" 및 "스테레오"와 같은 거실 내의 디바이스들을 식별할 수 있다. 따라서, 사용자가 앞서 언급한 "Assistant, turn on" 발화를 제공하면, 디바이스 토폴로지에 액세스하여, 해당 영역 내의 특정한 디바이스들에 대응하는 제안들을 제공하기 위해 사용자가 있는 영역과 비교할 수 있다(예: 발언이 디바이스 토폴로지 데이터에 의해 "거실"에 있는 것으로 정의된 디바이스를 통해 제공된 것에 기초하여 "거실"을 식별).In some implementations, the timing for providing the automated assistant with a suggestion element for completing a particular request may be based on the context in which the user initiated the particular request. For example, if the user provided at least a portion of the request via a voice utterance while driving the vehicle, the automated assistant may determine that the user is driving the vehicle and delay displaying the suggestion for completing the request. In this way, the user will be less distracted by the graphics presented on the in-vehicle display panel. Additionally or alternatively, the content of the suggestion element (eg, a candidate text segment) may also be based on the context in which the user initiated the spoken utterance. For example, with the user's pre-approval, the automated assistant can verify that the user has provided a spoken utterance such as "Assistant, turn on..." in the living room. In response, the automated assistant may cause the computing device to generate a suggestion element that identifies a particular device within the user's living room. Suggestion elements may include natural language content such as "Living Room TV...Living Room Lights...Living Room Stereo". The user can select one of the suggested elements by providing subsequent voice utterances, such as "Living Room Lights," and in response, an automatic assistant can turn on the living room lights. In some implementations, the devices the suggestive element identifies may be based on device topology data characterizing relationships between various devices associated with the user and/or the configuration of computing devices within the user's location (eg, home or office). The device topology data may include identifiers for the various areas within the location as well as device identifiers that characterize whether the device is in each area. For example, the device topology data may identify an area such as “living room” and devices within the living room such as “TV”, “light” and “stereo”. Thus, when the user provides the aforementioned “Assistant, turn on” utterance, the device topology can be accessed and compared to the area the user is in to provide suggestions corresponding to specific devices within that area (e.g., speak Identifies the “living room” based on what is provided through the device defined as being in the “living room” by this device topology data).
일부 구현예에서, 수신된 요청의 적어도 일부에 대응하는 액션은 요청을 완성하기 위해 제안 요소가 사용자에게 제시되는 동안 수행될 수 있다. 예를 들어, 사용자는 "Assistant, create a calendar event for…"와 같은 음성 발화를 제공할 수 있다. 이에 대한 응답으로 자동 어시스턴트는 기본 콘텐츠가 포함된 기본 캘린더를 생성되게 하고, 캘린더 이벤트를 생성하기 위한 요청을 완성하기 위해 제안 요소가 생성되게 할 수 있다. 예를 들어, 제안 요소는 "오늘 밤… 내일… 토요일…"을 포함할 수 있고, 기본 캘린더 이벤트가 생성되기 전, 도중 또는 후에 제시될 수 있다. 사용자가 제안 요소들 중 하나를 선택하면, 사용자가 선택한 제안 요소에 따라 이전에 생성된 기본 캘린더 이벤트가 수정될 수 있다. In some implementations, an action corresponding to at least a portion of the received request may be performed while the suggestion element is presented to the user to complete the request. For example, the user may provide a voice utterance such as “Assistant, create a calendar event for…”. In response, the automated assistant may cause a default calendar with default content to be created, and a suggestion element to be generated to complete the request to create a calendar event. For example, the suggestion element may include “tonight… tomorrow… Saturday…” and may be presented before, during, or after the default calendar event is created. When the user selects one of the suggested elements, a previously generated default calendar event may be modified according to the user's selected suggestion element.
일부 구현예에서, 자동 어시스턴트는 일반적으로 사용자로부터의 특정 입력과 연관된 음성 처리량 및/또는 어시스턴트 인터렉션 시간의 양을 줄이기 위한 제안을 제공할 수 있다. 예를 들어, 사용자가 캘린더 이벤트를 생성하기 위해 자동 어시스턴트와의 대화 세션을 완료한 경우, 그럼에도 불구하고 자동 어시스턴트가 추가 제안을 제공할 수 있다. 예를 들어, 사용자가 "Assistant, create a calendar event for Matthew’s birthday, next Monday"와 같은 음성 발화를 제공한다고 가정한다. 이에 응답하여 자동 어시스턴트는 캘린더 이벤트가 생성되게 하고, 사용자가 제안 요소를 선택하게 캘린더 이벤트가 매년 반복되게 하기 위해 "매년 반복"과 같은 제안 요소가 제시되게 한다. 이렇게 하면 사용자가 매년 캘린더 이벤트를 반복적으로 만들 필요가 없기 때문에 사용자와 자동 어시스턴트 간의 예상되는 인터렉션 수를 줄일 수 있다. 이것은 또한 사용자에게 "매년 반복"이 미래에 음성을 통해 제공될 수 있음을 알려줌으로써 사용자가 "매년 반복"을 포함하는 미래의 음성 입력을 제공할 수 있게 함으로써, 미래의 음성 참여를 보다 효율적으로 만들 수 있다. 더욱이, 일부 구현예에서, 제안 요소는 사용자에 의해 제공되는 이전의 음성 발화의 압축된 버전으로 제안될 수 있다. 이러한 방식으로, 사용자가 자동 어시스턴트와 인터렉션하는 총 시간을 줄여 전력 및 네트워크 대역폭과 같은 계산 리소스를 보존할 수 있다.In some implementations, the automated assistant may provide suggestions to reduce the amount of voice throughput and/or assistant interaction time generally associated with certain input from the user. For example, if a user completes a chat session with an automated assistant to create a calendar event, the automated assistant may nevertheless provide additional suggestions. For example, suppose a user provides a voice utterance such as "Assistant, create a calendar event for Matthew's birthday, next Monday". In response, the automatic assistant causes a calendar event to be created and a suggested element such as “repeat yearly” is presented for the user to select a suggested element and the calendar event repeats annually. This reduces the expected number of interactions between the user and the automated assistant because the user does not have to create a calendar event repeatedly every year. It also informs the user that “Repeat Yearly” may be provided via voice in the future, allowing the user to provide future voice input that includes “Repeat Annually”, thereby making future voice engagements more efficient. can Moreover, in some implementations, the suggestion element may be suggested as a condensed version of a previous spoken utterance provided by the user. In this way, computing resources such as power and network bandwidth can be conserved by reducing the total time the user interacts with the automated assistant.
일부 구현예에서, 사용자에게 제공되는 제안 요소는 사용자가 이미 익숙한 특정 음성 발화를 피하기 위해 제공될 수 있다. 예를 들어, 사용자가 "Assistant, dim my lights"와 같은 음성 발화를 제공한 이력이 있는 경우, 자동 어시스턴트는 사용자가 나중에 "Assistant, dim…"라고 말하면 "my lights"라는 제안을 제공하지 않을 수 있다. 오히려, 사용자는 "my monitor… my tablet screen… an amount of [color] of my lights"과 같이 익숙하지 않은 다른 제안을 받을 수 있다. 대안적으로 또는 추가적으로, 전술한 제안은 사용자가 불완전한 음성 발화(예: "Assistant, play…")를 제공한 상황 및 사용자가 완전한 음성 발화(예: "Assistant, play some music")를 제공한 다른 상황에서 사용자에게 제시될 수 있다.In some implementations, a suggestion element provided to the user may be provided to avoid certain spoken utterances that the user is already familiar with. For example, if the user has a history of providing a spoken utterance such as "Assistant, dim my lights", the automatic assistant may not offer the suggestion "my lights" if the user later says "Assistant, dim…" have. Rather, the user may be presented with other unfamiliar offers, such as "my monitor… my tablet screen… an amount of [color] of my lights". Alternatively or additionally, the foregoing suggestions may include situations in which the user has provided incomplete utterances (eg, "Assistant, play…") and other situations in which the user has provided complete utterances (eg, "Assistant, play some music"). may be presented to the user in context.
다양한 구현예에서, 사용자는 음성 발화가 사용자에 의해 완성되었다고 간주되는 시기를 명시적으로 나타내지 않을 것이다. 예를 들어, 사용자가 음성 발화가 완전한 것으로 간주될 때 사용자는 "제출" 버튼을 누르거나 "끝", "실행", "완료" 또는 기타 종료 문구를 말하지 않는다. 따라서, 이러한 다양한 구현예에서 그래픽 제안(들)은 이미 말한 음성 발화에 응답하여 제시될 수 있으며, 자동 어시스턴트는 제안 중 하나에 부합하고 이미 제공된 음성 발화의 연속이거나, 이미 제공된 음성 발화에만 적용되는 추가 음성 발화를 제공(또는 터치 입력을 통해 제안 중 하나를 선택)하기 위해 사용자에게 더 많은 시간을 제공할지 여부를 결정할 필요가 있을 것이다. In various implementations, the user will not explicitly indicate when a spoken utterance is considered complete by the user. For example, when the user considers the spoken utterance to be complete, the user does not press a "submit" button or say "finish", "run", "done" or other end phrases. Thus, in these various implementations, graphical suggestion(s) may be presented in response to already spoken spoken utterances, where the automated assistant matches one of the suggestions and is a continuation of an already presented spoken utterance, or additional that applies only to an already presented spoken utterance. You will need to decide whether to give the user more time to provide a voice utterance (or select one of the suggestions via touch input).
상기 구현예 중 일부에서, 자동 어시스턴트는 사용자로부터 음성 입력의 부족한 지속시간을 검출하는 것에 응답하여 이미 제공된 음성에 대해서만 동작한다. 지속시간은 다양한 구현예에서 동적으로 결정될 수 있다. 예를 들어, 지속시간은 이미 제공된 음성 발화가 "불완전한" 요청인지, "완전한 요청"인지 여부에 따라 달라질 수 있으며, 및/또는 이미 제공된 음성 발화의 "완전한 요청"의 특성(들)에 따라 달라질 수 있다. 상기 언급된 바와 같이, "완전한 요청"은 자동 어시스턴트에 의해 완전히 프로세싱될 때 하나 이상의 대응하는 자동 어시스턴트 액션을 수행하게 하는 것이며, 수행된 자동 어시스턴트 액션(들)은 기본 오류 유형 액션들이 아니다(예: "죄송합니다. 도와 드릴 수 없습니다"라는 음성 응답이 아님). "불완전한 요청"은 자동 어시스턴트에 의해 완전히 프로세싱될 때 기본 오류 유형 액션을 수행하게 하는 요청이다(예: 오류 톤 및/또는 "죄송합니다. 도와드릴 수 없습니다"와 같은 기본 음성 응답). In some of the above implementations, the automatic assistant only operates on voice already provided in response to detecting insufficient duration of voice input from the user. The duration may be determined dynamically in various embodiments. For example, the duration may vary depending on whether an already provided spoken utterance is an "incomplete" request or a "complete request," and/or may vary depending on the characteristic(s) of a "full request" of an already provided spoken utterance. can As noted above, a "complete request" is one that, when fully processed by the automated assistant, causes one or more corresponding automated assistant actions to be performed, and the automated assistant action(s) performed are not basic error type actions (e.g.: It's not a voice response saying "I'm sorry, I can't help you"). An "incomplete request" is a request that, when fully processed by an automated assistant, causes a basic error type action to be performed (eg an error tone and/or a basic voice response such as "I'm sorry, I can't help you").
다양한 구현예에서, 이미 말한 음성 발화가 완전한 요청일 때 보다 불완전한 요청일 때 지속시간이 더 길 수 있다. 예를 들어, 이미 말한 발화가 불완전한 요청인 경우 지속시간은 5초일 수 있지만, 이미 말한 발화가 완전한 요청인 경우 더 짧을 수 있다. 예를 들어, "create a"라는 음성 발화에 대한 응답으로 "calendar entry for [date] at X:00" 및 "reminder to do X at [time] or [location]"의 제안이 제시될 수 있다. "create a"의 음성 발화가 불완전하기 때문에, 자동 어시스턴트는 추가 음성 발화를 위해 5초를 기다릴 수 있으며, 추가 음성 입력 없이 5초가 지나야만 자동 어시스턴트가 불완전한 음성 발화를 완전히 프로세싱하고 기본 오류 유형 액션을 수행할 것이다. 또한, 예를 들어, "create a calendar entry called call mom"라는 음성 발화에 대한 응답으로 "for [date] at X:00"라는 제안이 제시될 수 있다. "create a calendar entry called call mom"의 음성 발화가 완료되었으므로(즉, 제목을 가진 캘린더 항목을 만들고, 선택적으로 날짜 및 시간 세부 정보를 추가로 묻는 메시지가 표시됨), 자동 어시스턴트는 추가 음성 발화를 위해 3초 또는 더 짧은 지속시간을 기다릴 수 있고, 추가 음성 입력없이 3초가 지나야 자동 어시스턴트는 이미 말한 음성 발화를 완전히 프로세싱하고, 캘린더 항목을 생성하고, 선택적으로 사용자에게 캘린더 항목에 대한 날짜 및/또는 시간을 제공하도록 추가 프롬프트를 제시한다. In various implementations, an already spoken spoken utterance may have a longer duration when it is an incomplete request than when it is a complete request. For example, the duration may be 5 seconds if the already spoken utterance is an incomplete request, but may be shorter if the already spoken utterance is a complete request. For example, suggestions of “calendar entry for [date] at X:00” and “reminder to do X at [time] or [location]” may be presented in response to the voice utterance “create a”. Because the spoken utterance of "create a" is incomplete, the automated assistant can wait 5 seconds for an additional spoken utterance, and only after 5 seconds without additional voice input can the automated assistant fully process the incomplete spoken utterance and take the default error type action. will perform Also, for example, in response to a voice utterance of “create a calendar entry called call mom”, a proposal “for [date] at X:00” may be presented. Now that the spoken utterance of "create a calendar entry called call mom" is complete (i.e., it creates a calendar entry with a title, and optionally prompts for additional date and time details), the automatic assistant will You can wait for a duration of 3 seconds or less, and after 3 seconds without additional voice input, the automatic assistant will fully process the spoken utterance, create a calendar entry, and optionally prompt the user the date and/or time for the calendar entry. provide additional prompts to provide
또 다른 예로서, 모든 필수 파라미터를 포함하지 않는 완전한 요청에 대한 지속시간과 비교할 때 모든 필수 파라미터를 포함하는 완전한 요청의 지속기간은 더 짧을 수 있다(필수 파라미터에 대한 추가 프롬프트가 발생함). 예를 들어, 캘린더 항목을 만들기위한 완전한 요청은 "제목", "날짜" 및 "시간"의 필수 파라미터를 포함할 수 있다. "create a calendar entry called call mom at 2:00 PM today"라는 음성 발화는 모든 필수 파라미터를 포함하지만, 그럼에도 불구하고 선택적 파라미터에 대한 제안(예: "and repeat daily" 제안)이 제공될 수 있다. 모든 필수 파라미터가 음성 발화에 포함되어 있기 때문에, 예를 들어 음성 발화가 모든 필수 파라미터를 포함하지 않은 "create a calendar entry called call mom"인 경우("@ X:00 on [date]"와 같이 제시되는 제안(들)을 발생시킴)보다 지속시간이 더 짧을 수 있다. 또 다른 예로서, 요청 및 디스플레이(들) 검색 결과(들)에 기초하여 일반 검색을 발행하는 일반 검색 에이전트 대신 특정 자동 어시스턴트 에이전트가 호출되는 완전한 요청에 대한 기간이 더 짧을 수 있다. 예를 들어, "What is time"의 음성 발화는 "시간" 개념과 관련된 정보를 제공하는 검색 결과의 일반적인 검색 및 제공으로 이어지는 완전한 요청이다. 음성 발화에 대한 응답으로 "in [city]" 제안이 디스플레이될 수 있으며, 2초 또는 다른 지속시간 동안 디스플레이될 수 있다. 반면에, "turn on the kitchen lights"라는 음성 발화는 "kitchen lights"이 켜지게 하는 특정 자동 어시스턴트 에이전트를 호출하는 완전한 요청이다. "to X% brightness" 제안은 음성 발화에 대한 응답으로 디스플레이될 수 있지만, 이미 제공된 음성 발화가 완료되고 특정 자동 어시스턴트 에이전트를 호출하므로(일반 검색의 수행을 발생시키는 대신) 1초(또는 다른 짧은 시간) 동안만 디스플레이될 수 있다. As another example, the duration of a complete request that includes all required parameters may be shorter (additional prompts for required parameters are generated) compared to the duration for a complete request that does not include all required parameters. For example, a complete request to create a calendar entry may include the required parameters of "title", "date" and "time". The spoken utterance "create a calendar entry called call mom at 2:00 PM today" includes all required parameters, but may nevertheless provide suggestions for optional parameters (eg "and repeat daily" suggestions). Since all required parameters are included in the speech utterance, for example, if the speech utterance is "create a calendar entry called call mom" which does not include all required parameters (present as "@X:00 on [date]") may be shorter in duration than the proposed proposal(s). As another example, a shorter period for a complete request in which a specific automated assistant agent is invoked instead of a generic search agent issuing a generic search based on the request and display(s) search result(s) may be shorter. For example, a spoken utterance of “What is time” is a complete request leading to a general retrieval and presentation of search results that provide information related to the concept of “time”. A suggestion “in [city]” may be displayed in response to the spoken utterance, which may be displayed for two seconds or another duration. On the other hand, the spoken utterance "turn on the kitchen lights" is a complete request to invoke a specific automated assistant agent that turns on "kitchen lights". The "to X% brightness" suggestion may be displayed in response to a spoken utterance, but only for 1 second (or other short time) as the already provided voice utterance completes and calls a specific automated assistant agent (instead of causing a general search to be performed). ) can only be displayed.
상기 설명은 본 개시의 일부 구현예의 개요로서 제공된다. 이러한 구현예 및 기타 구현예에 대한 추가 설명은 아래에서 더 자세히 설명된다. The above description is provided as an overview of some implementations of the present disclosure. Additional descriptions of these and other implementations are described in greater detail below.
다른 구현예들은 하나 이상의 프로세서(예를 들어, 중앙 프로세싱 유닛(CPU), 그래픽 프로세싱 유닛(GPU) 및/또는 텐서 프로세싱 유닛(TPU))에 의해 실행가능한 명령어들을 저장하는 비일시적 컴퓨터 판독가능 저장 매체를 포함할 수 있으며, 상기 명령어들은 본 명세서에 기술된 하나 이상의 방법들과 같은 방법을 수행한다. 또 다른 구현예는 하나 이상의 컴퓨터들 및/또는 본 명세서에 기술된 방법들 중 하나 이상과 같은 방법을 수행하기 위해 저장된 명령어들을 실행하도록 동작가능한 하나 이상의 프로세서들을 포함하는 하나 이상의 로봇들의 시스템을 포함할 수 있다. Other implementations include a non-transitory computer-readable storage medium that stores instructions executable by one or more processors (eg, central processing unit (CPU), graphics processing unit (GPU), and/or tensor processing unit (TPU)). , wherein the instructions perform a method such as one or more methods described herein. Another implementation would include a system of one or more computers and/or one or more robots comprising one or more processors operable to execute stored instructions to perform a method, such as one or more of the methods described herein. can
본 명세서에서 매우 상세히 기술된 상기 개념들 및 추가적 개념들의 모든 조합들은 본 명세서에 개시된 본 발명의 일부인 것으로 고려되어야 한다. 예를 들면, 본 명세서의 끝부분에 나타나는 청구된 발명의 모든 조합들은 본 명세서에 개시된 본 발명의 일부인 것으로 고려된다.All combinations of the above and additional concepts described in great detail herein are to be considered as being part of the invention disclosed herein. For example, all combinations of claimed invention appearing at the end of this specification are considered to be part of the invention disclosed herein.
도 1a 및 도 1b는 음성 발화를 제공하는 사용자 및 음성 발화를 완성하기 위한 제안을 제공하는 자동 어시스턴트의 뷰를 도시한다.
도 2a 및 도 2b는 음성 발화에 응답하여 및/또는 자동 어시스턴트의 하나 이상의 동작들에 기초하여 카테고리적 제안을 수신하는 사용자를 도시하는 뷰를 제공한다.
도 3은 자동 어시스턴트에 의해 렌더링되는 가청 프롬프트 대신에, 음성 발화를 완성하기 위한 제안을 수신하는 사용자의 뷰를 도시한다.
도 4는 디스플레이 양식을 통해 제안을 제공하고, 자동 어시스턴트를 통해 음성 발화를 완성하기 위한 시스템을 도시한다.
도 5는 사용자에 의해 제공되는 음성 발화를 완성 및/또는 보충하기 위한 하나 이상의 제안 요소들을 자동 어시스턴트, 디바이스, 애플리케이션 및/또는 임의의 다른 장치 또는 모듈에 제공하는 방법을 도시한다.
도 6는 예시적 컴퓨터 시스템의 블록도이다.1A and 1B show views of a user providing a spoken utterance and an automated assistant providing a suggestion to complete a spoken utterance.
2A and 2B provide views illustrating a user receiving a categorical suggestion in response to a spoken utterance and/or based on one or more actions of the automated assistant.
3 shows a view of a user receiving a suggestion to complete a spoken utterance, instead of an audible prompt rendered by an automatic assistant.
4 illustrates a system for providing suggestions via a display modality and completing a spoken utterance via an automated assistant.
5 illustrates a method for providing an automated assistant, device, application, and/or any other apparatus or module with one or more suggested elements for completing and/or supplementing a spoken utterance provided by a user.
6 is a block diagram of an exemplary computer system.
도 1a 및 도 1b는 음성 발화를 제공하는 사용자 및 음성 발화를 완성하기 위한 제안을 제공하는 자동 어시스턴트의 뷰(100) 및 뷰(120)를 도시한다. 사용자(112)는 컴퓨팅 디바이스(114)의 자동 어시스턴트 인터페이스에 음성 발화(116)를 제공할 수 있지만, 적어도 자동 어시스턴트와 관련하여 음성 발화(116)는 불완전할 수 있다. 예를 들어, 사용자는 컴퓨팅 디바이스(114) 및/또는 서버 디바이스와 같은 원격 컴퓨팅 디바이스에서 프로세싱되고 될 수 있는 "Assistant, could you lower..."와 같은 음성 발화(116)를 제공하고, 컴퓨팅 디바이스(114)의 디스플레이 패널(110)에서 그래픽으로 렌더링된다. 음성 발화(116)의 그래픽 렌더링은 컴퓨팅 디바이스(114)의 어시스턴트 인터렉티브 모듈(104)에서 그래픽 요소(106)로서 제시될 수 있다. 음성 발화(116)의 수신에 응답하여, 컴퓨팅 디바이스는 음성 발화(116)가 불완전한 음성 발화임을 결정할 수 있다. 불완전한 음성 발화는 자동 어시스턴트 및/또는 다른 애플리케이션 또는 어시스턴트 에이전트를 통해 수행되는 적어도 하나의 액션을 초래하지 않는 음성 발화일 수 있다. 대안적으로 또는 추가적으로, 불완전한 음성 발화는 기능이 실행되거나 제어되는데 필요한 하나 이상의 파라미터 값이 없는 음성 발화일 수 있다. 예를 들어, 불완전한 음성 발화는 자동 어시스턴트를 통해 실행되는 어시스턴트 기능에 필요한 하나 이상의 파라미터 값이 없는 음성 발화일 수 있으므로, 사용자 또는 기타 소스로부터 추가 또는 보충 입력이 필요하다.1A and 1B show
음성 발화(116)가 불완전한 음성 발화라는 결정에 응답하여, 자동 어시스턴트는 하나 이상의 제안 요소(108)가 제1 사용자 인터페이스(102)에 제시되게 할 수 있다. 그러나, 일부 구현예에서, 하나 이상의 제안 요소(108) 및/또는 그 콘텐츠는 시각 장애가 있거나 그렇지 않으면 제1 사용자 인터페이스(102)를 쉽게 볼 수 없는 사용자를 위해 청각적으로 렌더링될 수 있다. 제안 요소(108)는 후보 텍스트 세그먼트들을 특징 짓는 자연어 콘텐츠를 포함할 수 있으며, 여기서 사용자(112)는 제1 사용자 인터페이스(102)를 탭하고, 후보 텍스트 세그먼트들의 텍스트를 구두로 말하고, 및/또는 적어도 하나의 제안 요소(108)를 제1 사용자 인터페이스(102)에 제시된 대로 설명함으로써 선택할 수 있다. 예를 들어, "the sound"와 같은 추가 발화를 제공하는 대신에, 사용자(112)는 "the first one"와 같은 다른 추가 발화를 제공할 수 있다. 대안적으로, 사용자가 다른 제안 요소(108)를 선택하기를 원한다면, 사용자(112)는 "the lights"라고 말하는 제안 요소(108)에 터치 입력을 제공할 수 있고, 원하는 제안 요소(108)에 대응하는 위치에서 제1 사용자 인터페이스(102)를 탭할 수 있고 및/또는 "the second one"와 같은 다른 추가 음성 발화를 제공한다. 다양한 구현예에서, 하나 이상의 제안 요소(108)는 그들의 선택에 응답하여 수행될 액션을 표시하기 위해 아이콘 및/또는 다른 그래픽 요소와 함께 제시될 수 있다. 예를 들어 "the sound"는 스피커 아이콘으로, "the lights"은 조명 아이콘으로, "the garage door"은 차고문 아이콘으로 제시될 수 있다. In response to determining that the spoken
각각의 제안 요소(108)와 함께 제공된 텍스트 세그먼트의 자연어 콘텐츠는 불완전한 음성 발화(116) 및 후보 텍스트 세그먼트들의 조합이 자동 어시스턴트로 하여금 액션을 수행하게 할 것인지 여부에 기초하여 생성될 수 있다. 예를 들어, 각각의 후보 텍스트 세그먼트는 불완전한 음성 발화(116) 및 후보 텍스트 세그먼트의 조합이 사용자(112)에 의해 함께 발화된 때 액션이 자동 어시스턴트를 통해 수행되도록 할 것인지 여부에 기초하여 생성될 수 있다. 일부 구현예에서, 사용자(112)가 완전한 음성 발화를 제공한 경우, 자동 어시스턴트, 컴퓨팅 디바이스(114) 및/또는 연관된 서버 디바이스는 제안 요소(108) 생성을 우회할 수 있다. 대안적으로, 사용자(112)가 완전한 음성 발화를 제공한 경우, 자동 어시스턴트, 컴퓨팅 디바이스(114) 및/또는 연관된 서버 디바이스는 제1 사용자 인터페이스(102)에서 제공하기 위한 제안 요소를 생성할 수 있다. The natural language content of the text segment provided with each
일부 구현예에서, 디바이스 토폴로지 데이터는 제안 요소(108)의 후보 텍스트 세그먼트를 생성하기 위한 기초로서 사용될 수 있다. 예를 들어, 컴퓨팅 디바이스(114)는 사용자(112)와 연관된 하나 이상의 디바이스들을 식별하는 디바이스 토폴로지 데이터에 액세스할 수 있다. 또한, 디바이스 토폴로지 데이터는 하나 이상의 디바이스들 중 각 디바이스의 위치를 표시할 수 있다. 사용자(112)의 포지션이 적어도 컨텍스트가 사용자(112)의 위치를 특성화함에 따라 사용자(112)의 사전 허가를 받아 사용자(112)의 컨텍스트를 결정하는데 사용될 수 있다. 디바이스 토폴로지 데이터는 사용자(112) 근처에 있을 수 있는 디바이스를 결정하기 위해 포지션 데이터와 비교될 수 있다. 또한, 주변 디바이스가 식별되면, 불완전한 음성 발화(116)의 자연어 콘텐츠를 각 주변 디바이스의 상태와 비교하기 위해 각 주변 디바이스의 상태를 결정할 수 있다. 예를 들어, 도 1a에 도시된 바와 같이, 사용자(112)가 어시스턴트에게 무언가 "낮추기" 요청을 제공했기 때문에, "낮추기" 액션을 통해 수정될 수 있는 상태를 갖는 디바이스가 제안 요소(108) 생성의 기초로 사용될 수 있다. 따라서 스피커가 근처에서 음악을 재생하는 경우 "소리" 제안이 제공될 수 있다. 또한, 계단이 사용자 근처에 있는 경우 "조명" 제안이 제공될 수 있다. 추가적으로 또는 대안적으로, 디바이스 토폴로지 데이터는 사용자(112) 근처에 있지 않을 수 있지만 그럼에도 불구하고 불완전한 음성 발화(116)와 연관될 수 있는 디바이스를 특징 짓을 수 있다. 예를 들어, 사용자(112)는 침실에 있을 수 있지만, 불완전한 음성 발화(116)는 사용자(112)의 집 차고에 있는 차고 문과 연관될 수 있다. 따라서, 자동 어시스턴트는 사용자가 차고에 있지 않더라도 사용자가 "차고 문을 닫을" 수 있게 하는 제안 요소(108)를 제공할 수 있다. In some implementations, the device topology data may be used as a basis for generating candidate text segments of the
도 1b는 도 1a에 도시된 제1 사용자 인터페이스(102)에서 제공되었던 제안 요소(108)를 선택하는 것을 촉진하기 위해 추가 음성 발화(128)를 제공하는 사용자(112)의 뷰(120)를 도시한다. 사용자(112)는 "차고 문"과 같은 추가 음성 발화(128)를 제공함으로써 이전에 디스플레이된 제안 요소(108) 중 하나를 선택할 수 있다. 컴퓨팅 디바이스(114)의 자동 어시스턴트 인터페이스에서 음성 발화를 수신하는 것에 응답하여, 선택된 제안 요소(108)에 대응하는 후보 텍스트는 불완전한 음성 발화(116)의 자연어 콘텐츠에 인접하게 제시될 수 있다. 예를 들어, 도 1b에 도시된 바와 같이, 완성된 후보 텍스트(122)는 불완전한 음성 발화(116) 및 선택된 제안 요소(108)로부터의 자연어 콘텐츠(예를 들어, 추가 음성 발화(128)의 자연어 콘텐츠)의 조합으로서 제2 사용자 인터페이스(126)에 제시될 수 있다. FIG. 1B shows a
컴퓨팅 디바이스(114), 자동 어시스턴트 및/또는 서버 디바이스는 초기 음성 발화(116)와 결합된 추가 발화(128)가 완전한 음성 발화를 초래했다고 결정할 수 있다. 응답으로, 상기 하나 이상의 액션들이 상기 자동 어시스턴트에 의해 수행될 수 있다. 예를 들어, 완성된 음성 발화가 완성된 후보 텍스트(122)에 대응하는 경우, 자동 어시스턴트는 사용자(112)의 집에 있는 차고 문을 전기 기계적으로 닫힌 포지션으로 낮게 할 수 있다. 일부 구현예에서, 추가 제안 요소(124)는 완전한 음성 발화가 형성되었을 때 및/또는 불완전한 음성 발화가 형성되었을 때 제2 사용자 인터페이스(126)에 제공될 수 있다. 예를 들어, 완성된 후보 텍스트(122)가 자동 어시스턴트에 의해 실행 가능함에도 불구하고, 컴퓨팅 디바이스(114), 자동 어시스턴트 및/또는 서버 디바이스는 추가 제안 요소(124)가 제2 사용자 인터페이스(126)에 제시되게 할 수 있다. 본 명세서에 설명된 바와 같이, 다양한 구현예에서 자동 어시스턴트는 추가 음성 입력 또는 추가 제안 요소(124) 중 임의의 하나의 선택없이 시간의 지속시간이 경과한 경우 추가 제안 요소(124) 중 하나의 선택 없이도 완성된 후보 텍스트(122)에 기초하여 액션을 실행할 수 있다(즉, 차고 문이 내려 가게 함). 본 명세서에 또한 설명된 바와 같이, 상기 시간의 지속시간은 자동 어시스턴트가 도 1a에서 대기하는 시간의 지속시간보다 더 짧을 수 있다. 이 더 짧은 시간의 지속시간은 예를 들어 완성된 후보 텍스트(122)가 "완전한 요청"에 기초하는 반면 후보 텍스트(106)(도 1b)는 "불완전한 요청"에 기초할 수 있다. The
일부 구현예에서, 수행될 수 있고 완성된 후보 텍스트(122)와 관련된 추가 기능 또는 액션이 존재하는지 여부에 대한 결정이 이루어질 수 있다. 대안적으로 또는 추가적으로, 결정은 추가 기능 또는 액션이 그러한 인터렉션에서 소비될 수 있는 계산 리소스 및/또는 네트워크 리소스의 양을 제한하기 위해 사용자(112)와 자동 어시스턴트 사이의 후속 인터렉션 양을 줄일 수 있는지 여부에 기초할 수 있다. 예를 들어, 일부 구현예에서, 완성된 후보 텍스트(122)의 콘텐츠는 사용자(112)와 자동 어시스턴트 간의 이전 인터렉션의 콘텐츠 및/또는 사용자(112) 및 하나 이상의 다른 사용자로부터 사전 허가를 받아 하나 이상의 다른 사용자 및 자동 어시스턴트와의 이전 인터렉션의 콘텐츠와 비교될 수 있다. 또한, 후보 텍스트(122)와 관련된 인터렉션에 참여할 때 사용자와 그 각각의 자동 어시스턴트 간의 인터렉션 시간이 비교될 수 있다. 예를 들어, 다른 사용자는 일상적인 활동에 대한 응답으로 액션이 수행되도록 요청하는 것과 같이, 주기적인 액션이 수행되도록 요청함으로써 자신의 자동 어시스턴트와만 인터렉션할 수 있다. 따라서, 적어도 후보 텍스트(122)의 주제와 관련하여 다른 사용자가 그들 각각의 자동 어시스턴트와 인터렉션하는데 소비하는 시간의 양은 주기적 및/또는 조건부 액션이 수행되기를 요청하지 않은 사용자(112)의 시간의 양보다 상대적으로 적을 수 있다. 따라서, 제2 사용자 인터페이스(126)는 사용자가 추가 제안 요소들(124) 중 하나 이상을 선택할 수 있도록 추가 제안 요소(124)와 함께 생성될 수 있으며, 이에 따라 계산 리소스 및 네트워크 리소스의 낭비를 완화할 수 있다. 예를 들어, 사용자(112)가 "each morning after I leave for work"이라는 문구(130)를 말함으로써 제1 추가 제안 요소(124)를 선택하면, 사용자(112)는 디스플레이될 필요가 없을 것이다. 더욱이, 음성 발화(128)는 더 이상 프로세싱될 필요가 없고 및/또는 완성된 후보 텍스트(122)는 컴퓨팅 디바이스(114)에서 디스플레이하기 위해 더 이상 생성될 필요가 없을 것이다. 더욱이, 사용자(112)는 "each night at 9 P.M." 및 "each night when my spouse and I are home"과 같은 추가 조건문을 학습할 수 있으며, 이는 사용자가 자주 요청하는 다른 명령과 관련된 계산 리소스의 낭비를 완화하는데 사용될 수 있다. 예를 들어, 사용자(112)는 추가 제안 요소(124)를 보고 이를 기억하고 나중에 "Assistant, could you turn on the home security system?"와 같은 음성 발화를 제공할 때 사용할 수 있다.In some implementations, a determination may be made as to whether an additional function or action exists that may be performed and associated with the completed
도 2a 및 도 2b는 음성 발화(218)에 응답하여 및/또는 자동 어시스턴트의 하나 이상의 동작들에 기초하여 카테고리적 제안을 수신하는 사용자(214)를 도시하는 뷰(200) 및 뷰(220)를 제공한다. 도 2a에 도시된 바와 같이, 사용자(214)는 컴퓨팅 디바이스(216)의 자동 어시스턴트 인터페이스에 음성 발화(218)를 제공할 수 있다. 음성 발화(218)는 예를 들어 "Assistant, could you turn on"일 수 있다. 음성 발화(218)를 수신하는 것에 응답하여, 컴퓨팅 디바이스(216), 컴퓨팅 디바이스(216)과 통신하는 원격 컴퓨팅 디바이스 및/또는 자동 어시스턴트는 제1 사용자 인터페이스(204)가 컴퓨팅 디바이스(216)의 디스플레이 패널(212)에 제시되게 할 수 있다. 제1 사용자 인터페이스(204)는 자동 어시스턴트의 하나 이상의 액션들에 따라 다양한 그래픽 요소를 제시할 수 있는 어시스턴트 인터렉션 모듈(206)을 포함할 수 있다. 예를 들어, 음성 발화(218)의 해석은 제1 사용자 인터페이스(204)에서 불완전한 텍스트 세그먼트(208)로서 제시될 수 있다.2A and 2B illustrate
제1 사용자 인터페이스(204)는 또한 하나 이상의 제안 요소들(210)을 포함할 수 있으며, 이는 불완전한 텍스트 세그먼트(208)를 완성하기 위한 추가 입력에 대한 후보 카테고리들을 특징 짓는 콘텐츠를 포함할 수 있다. 제안 요소들(210)의 콘텐츠에 의해 특징 지어 지는 후보 카테고리들은 하나 이상의 상이한 소스들로부터의 데이터에 기초할 수 있다. 예를 들어, 제안 요소(210)의 콘텐츠는 사용자가 음성 발화(218)를 제공한 컨텍스트에 기초할 수 있다. 컨텍스트는 컴퓨팅 디바이스(216), 컴퓨팅 디바이스(216)(서버 디바이스와 같은)와 통신하는 원격 디바이스, 자동 어시스턴트 및/또는 사용자(214)와 연관된 임의의 다른 애플리케이션 또는 디바이스에서 이용 가능한 컨텍스트 데이터에 의해 특성화될 수 있다. 예를 들어, 디바이스 토폴로지 데이터는 컴퓨팅 디바이스(216)가 하나 이상의 다른 디바이스와 쌍을 이루거나 다른 디바이스와 통신하고 있음을 나타낼 수 있다. 이 디바이스 토폴로지 데이터에 기초하여, 제안 요소들(210) 중 하나 이상은 "[디바이스 이름]"과 같은 후보 카테고리를 제공할 수 있다. 대안적으로 또는 추가적으로, 하나 이상의 소스로부터의 데이터는 사용자(214)가 최근에 액세스한 미디어를 나타내는 애플리케이션 데이터를 포함할 수 있다. 최근 미디어가 영화를 포함하는 경우, "[영화 제목]"과 같은 컨텐츠를 포함하도록 제안 요소(210)를 생성할 수 있다. 대안적으로 또는 추가적으로, 최근 미디어가 음악을 포함하는 경우, "[노래 및 아티스트의 이름]"과 같은 콘텐츠를 포함하도록 제안 요소(210)가 생성될 수 있다.The
도 2b는 도 2a에 제공된 제안 요소(210)로부터 후보 카테고리를 선택하기 위해 추가 음성 발화(228)를 제공하는 사용자(214)의 뷰(220)를 도시한다. 예시된 바와 같이, 사용자(214)는 "주방 조명"의 추가 음성 발화(228)를 제공할 수 있다. 추가 음성 발화(228)를 수신하는 것에 응답하여, 추가 제안 요소(226)가 디스플레이 패널(212)을 통해 사용자(214)에게 제시될 수 있다. 추가 제안 요소(226)는 사용자가 추가 발화(228)를 제공함으로써 초기 발화(218)를 완성하는 것에 응답하여, 또는 추가 발화(228)가 초기 발화(218)를 완성하지 않는 경우 사용자(214)에게 제시될 수 있다. 일부 구현예에서, 추가 제안 요소(226)는 제안 요소(210)와 유사한 형식일 수 있는 후보 카테고리와 같은 콘텐츠를 포함할 수 있다. 추가적으로 또는 대안적으로, 추가 제안 요소(226)는 사용자가 말할 때 음성의 콘텐츠가 자동 어시스턴트를 통해 하나 이상의 액션들을 완료, 수정 및/또는 초기화하는데 사용될 수 있는 콘텐츠를 포함할 수 있다. 따라서, 어시스턴트 인터렉티브 모듈(222)은 후보 카테고리(예를 들어, 제안 요소(210)에 제공된 것), 후보 요청(예를 들어, 추가 제안 요소(226)에 제공된 것) 및/또는 자동 어시스턴트 및/또는 기타 애플리케이션 또는 디바이스에 제공되는 명령에 포함하기 위해 사용될 수 있는 기타 콘텐츠를 포함할 수 있는 다양한 형식의 후보 텍스트 세그먼트를 제공할 수 있다.FIG. 2B shows a
일부 구현예에서, 하나 이상의 제안 요소들의 제공 및/또는 하나 이상의 제안 요소들의 제시 순서는 특정 콘텐츠, 액션, 음성 발화, 후보 텍스트 세그먼트에 할당된 우선 순위 및/또는 제안의 기반이 될 수 있는 기타 데이터에 기초할 수 있다. 예를 들어, 일부 구현예에서, 제안 요소(226) 내의 특정 콘텐츠의 제공은 사용자(214)와 자동 어시스턴트, 사용자(214)와 컴퓨팅 디바이스(216), 사용자(214)와 하나 또는 하나 이상의 다른 디바이스, 사용자(214) 및 하나 이상의 다른 애플리케이션 및/또는 하나 이상의 다른 사용자와 하나 이상의 디바이스 및/또는 애플리케이션 간의 이전의 인터렉션을 특징 짓는 이력 데이터에 기초할 수 있다. 예를 들어, 컨텐츠 "[디바이스 이름]"을 갖는 제안 요소(210)의 선택을 수신하는 것에 응답하여, 컴퓨팅 디바이스(216) 및/또는 서버 디바이스는 사용자(214)와 사용자(214)와 연관된 하나 이상 디바이스들 간의 인터렉션을 특징 짓는 이력 데이터에 액세스할 수 있다. 이력 데이터는 제안 요소(226)에 대한 콘텐츠를 생성하기 위한 기초로 사용될 수 있다. 예를 들어, 사용자가 "주방 조명"을 선택했음에도 불구하고, 사용자가 제어하는데 관심을 가질 수 있는 다른 디바이스 및/또는 사용자가 명시적으로 선택하지 않은 디바이스를 수정하는데 관심을 가질 수 있는 다른 설정("켜기"와는 별도로)에 대한 제안을 생성하는데 이력 데이터를 사용할 수 있다. In some implementations, the presentation of the one or more suggested elements and/or the order of presentation of the one or more suggested elements is a specific content, action, spoken utterance, priority assigned to a candidate text segment, and/or other data that may be based on the suggestion. can be based on For example, in some implementations, presentation of certain content within
예를 들어, 사용자(214)의 사전 허가를 받아, 이력 데이터는 사용자(214)가 취침 몇 분 전에 주방 조명을 끄는 하나 이상의 이전 인터렉션을 특징 지을 수 있다. 이력 데이터에 의해 특징 지어 지는 이러한 인터렉션을 기반으로, 자동 어시스턴트는 "Assistant, could you turn on the kitchen lights?"와 같은 완전한 요청(224)과 관련된 인터렉션에 가장 높은 우선 순위를 할당할 수 있다. 또한, 자동 어시스턴트는 "자러 갈 때까지"와 같은 제안 요소(226)에 대한 콘텐츠를 생성할 수 있다. 가장 높은 우선 순위 인터렉션을 특징 짓는 이력 데이터의 콘텐츠에 기초하여, 적어도 그것이 완전한 요청(224)과 관련되므로, 자동 어시스턴트는 제안 요소(226), "자러 갈 때까지"를 가장 높은 우선 순위 또는 가장 높은 포지션으로서 제2 사용자 인터페이스(232)에서 제안 요소(226)를 제공할 수 있다.For example, with
대안적으로 또는 추가적으로, 사용자(214)가 자동 어시스턴트가 주방 조명을 절반 밝기 설정 또는 빨간색 설정으로 바꾸도록 요청한 하나 이상의 이전 인터렉션을 이력 데이터가 특성화하는 경우, 그러나 이러한 요청은 앞서 언급한 자러 가기 전에 조명을 끄는 요청보다 덜 자주 발생한 경우, 이러한 요청의 콘텐츠에는 앞서 언급된 요청의 콘텐츠에 할당된 이전 우선 순위보다 낮은 우선 순위가 할당될 수 있다. 즉, 사용자(214)가 자러 들어갈 때 주방 조명을 끄도록 요청하는 빈도가 높기 때문에 제안 요소 "자러 갈 때까지"가 우선 순위화되거나 다른 제안 "절반 밝기 설정으로" 및/또는 "빨간색으로 설정" 보다 높은 포지션을 갖게 된다. 일부 구현예에서, 제안 요소의 콘텐츠는 사용자(214)가 이전에 사용하지 않았던 디바이스 또는 애플리케이션의 기능 및/또는 서드파티 엔티티에 의해 애플리케이션 또는 디바이스에 푸시된 하나 이상의 업데이트에 기초할 수 있다. 서드파티 엔티티라는 용어는 컴퓨팅 디바이스(216), 자동 어시스턴트 및/또는 컴퓨팅 디바이스(216)와 통신하는 서버 디바이스의 제조업체와 다른 엔티티를 지칭할 수 있다. 따라서 주방 조명 제조업체(즉, 서드파티)가 주방 조명이 빨간색 설정에 따라 동작하게 하는 업데이트를 푸시하면, 동 어시스턴트가 업데이트를 특성화하는 데이터에 액세스하여 서드파티 제조업체(즉, 서드파티 엔터티)에서 제공한 업데이트에 기초하여 제안 요소(226) "그리고 빨간색 설정으로"를 생성할 수 있다.Alternatively or additionally, if the historical data characterizes one or more prior interactions in which the
대안적으로 또는 추가적으로, 업데이트에 기초한 제안 요소(226)의 콘텐츠는 사용자가 수행되도록 자주 요청한 하나 이상의 액션들에 기초하여 다른 제안 요소(226)에 대응하는 콘텐츠에 할당된 다른 우선 순위보다 낮은 우선 순위를 할당 받을 수 있다. 그러나, 업데이트에 기초한 콘텐츠는 디스플레이 패널(212) 및/또는 자동 어시스턴트를 통해 사용자(214)가 이전에 제시한 콘텐츠보다 더 높은 우선 순위를 할당할 수 있지만, 이전에 제시되지 않았던 콘텐츠를 선택하지 않도록 선택되었다. 이러한 방식으로, 사용자에게 제공되는 제안 요소는 할당된 우선 순위에 따라 순환되어 사용자를 위한 제안을 조정할 수 있다. 또한 이러한 제안이 사용자(214)와 자동 어시스턴트 간의 인터렉션 X를 줄이는 것과 관련될 때, 그러한 인터렉션을 줄이지 않거나 사용자와 관련이 없는 제안을 제한하면서 그러한 인터렉션을 줄이는 더 많은 제안을 제공함으로써 계산 리소스 및 네트워크 리소스를 보존할 수 있다.Alternatively or additionally, the content of the
도 3은 자동 어시스턴트에 의해 렌더링되는 가청 프롬프트 대신에, 음성 발화(316)를 완성하기 위한 제안을 수신하는 사용자(312)의 뷰(300)를 도시한다. 사용자(312)는 "Assistant, could you create a calendar entry?"와 같은 음성 발화(316)를 제공할 수 있다. 이에 응답하여, 자동 어시스턴트를 통해 제어될 수 있는 어시스턴트 인터렉티브 모듈(304)은 음성 발화(316)의 그래픽 표현(306)을 제공할 수 있고, 그래픽 표현(306)은 컴퓨팅 디바이스(314)의 디스플레이 패널(320)에 제공될 수 있다.3 shows a
음성 발화(316)는 실행될 기능에 대한 요청에 대응할 수 있고, 그 기능은 사용자(312)에 의해 지정되거나 그렇지 않으면 하나 이상의 이용 가능한 값에 의해 충족되어야 하는 하나 이상의 필수 파라미터를 가질 수 있다. 사용자에게 하나 이상의 필수 파라미터를 채우기 위한 하나 이상의 값을 제공하도록 프롬프트하기 위해, 자동 어시스턴트는 특정 파라미터에 대한 값을 요청하는 그래픽 프롬프트(308)를 선택적으로 제공할 수 있다. 예를 들어, 프롬프트(308)는 "What will the title be?"와 같은 콘텐츠를 포함할 수 있고, 이에 의해 사용자(312)가 캘린더 항목에 대한 제목을 제공해야 함을 표시한다. 그러나, 컴퓨팅 디바이스(314)의 전력 및 컴퓨팅 리소스를 보존하기 위해, 자동 어시스턴트는 컴퓨팅 디바이스(314)가 사용자(312)에 대한 청각적 프롬프트를 렌더링하는 것을 우회하게 할 수 있다. 더욱이, 이것은 사용자(312)가 청각적 프롬프트가 완전히 렌더링될 때까지 기다릴 필요가 없기 때문에 지연 시간을 절약할 수 있다.
청각적 프롬프트를 제공하는 자동 어시스턴트 우회 대신에, 자동 어시스턴트는 사용자(312)에 의해 요청된 기능에 대한 하나 이상의 필수 파라미터에 기초할 수 있는 하나 이상의 제안 요소(310)를 생성할 수 있다. 예를 들어, 기능이 캘린더 항목 생성에 대응하고 충족되어야 할 파라미터가 캘린더 항목의 "제목"이기 때문에, 제안 요소(310)는 캘린더 항목에 대한 제목 제안을 포함할 수 있다. 예를 들어, 제안의 콘텐츠는 "엄마의 생일", "쓰레기 버리기", "내일 드라이 클리닝 찾기"등을 포함할 수 있다.Instead of automatic assistant bypass providing an audible prompt, the automatic assistant may generate one or
제안 요소(310) 중 하나를 선택하기 위해, 사용자(312)는 적어도 하나의 제안 요소(310)의 콘텐츠를 식별하는 추가 음성 발화(318)를 제공할 수 있다. 예를 들어, 사용자(312)는 "내일 드라이 클리닝을 찾기"의 추가 음성 발화(318)를 제공할 수 있다. 그 결과, 컴퓨팅 디바이스(314) 및/또는 자동 어시스턴트의 관점에서, 사용자(312)와 자동 어시스턴트 간의 인터렉션 동안 렌더링된 유일한 대화는 사용자(312)에 의해 렌더링되었을 것이다. 따라서 오디오 렌더링이 필요하지 않거나 자동 어시스턴트가 사용되지 않는다. 또한, 사용자가 추가 발화(318)를 제공하는 것에 응답하여, 요청된 기능에 필요한 경우 사용자(312)가 다른 파라미터에 대한 값을 제공하도록 요청하기 위해 디스플레이 패널(320)에 다른 프롬프트가 제공될 수 있다. 또한, 기능을 위해 다른 파라미터가 필요한 경우, 디스플레이 패널(320)에 서로 다른 콘텐츠로 서로 다른 제안 요소가 제시될 수 있다. 더욱이, 후속 프롬프트 및 후속 제안 요소는 컴퓨팅 디바이스(314)에서 또는 자동 어시스턴트를 통해 임의의 오디오가 렌더링되지 않고 제공될 수 있다. 따라서, 사용자(312)는 그들이 요청한 원래 기능을 추가로 완료할 콘텐츠를 식별하기 위해 디스플레이 패널(320)을 동시에 보면서 진행 중인 음성 발화를 제공하는 것처럼 보일 것이다. To select one of the suggested
도 4는 디스플레이 양식을 통해 제안을 제공하고, 자동 어시스턴트(408)를 통해 음성 발화를 완성하기 위한 시스템(400)을 도시한다. 일부 구현예에서, 사용자가 음성 발화를 완성하거나 적어도 이전 음성 발화를 보완하기 위한 제안이 사용자에게 제공될 수 있다. 일부 구현예에서, 사용자가 자동 어시스턴트와의 후속 대화 세션에 참여할 빈도 및/또는 시간 길이를 줄이기 위해 제안이 제공될 수 있다.4 illustrates a
자동 어시스턴트(408)는 클라이언트 디바이스(434) 및/또는 서버 디바이스(402)와 같은 하나 이상의 컴퓨터 디바이스에 제공되는 자동 어시스턴트 애플리케이션의 일부로서 동작할 수 있다. 사용자는 하나 이상의 어시스턴트 인터페이스(436)를 통해 자동 어시스턴트(408)와 인터렉션할 수 있으며, 어시스턴트 인터페이스(436)는 마이크로폰, 카메라, 터치스크린 디스플레이, 사용자 인터페이스 및/또는 사용자와 애플리케이션 사이의 인터페이스를 제공할 수 있는 임의의 다른 디바이스 중 하나 이상을 포함할 수 있다. 예를 들어, 사용자는 어시스턴트 인터페이스에 구두, 텍스트 또는 그래픽 입력을 제공함으로써 자동 어시스턴트(408)를 초기화하여, 자동 어시스턴트(408)가 기능을 수행하게 한다(예를 들어, 데이터 제공, 디바이스 제어(예: IoT 디바이스(442) 제어, 에이전트에 액세스, 설정 수정, 애플리케이션(들) 제어, 등)). 클라이언트 디바이스(434) 및/또는 IoT 디바이스(442)는 디스플레이 디바이스를 포함하며, 이는 사용자가 터치 인터페이스를 통해 클라이언트 디바이스(434) 및/또는 서버 디바이스(402)의 애플리케이션(434)을 제어할 수 있게 하는 터치 입력 및/또는 제스처를 수신하기 위한 터치 인터페이스를 포함하는 디스플레이 패널일 수 있다. 터치 입력 및/또는 다른 제스처(예를 들어, 음성 발화)는 또한 사용자가 클라이언트 디바이스(434)를 통해 자동 어시스턴트(408), 자동 어시스턴트(438) 및/또는 IoT 디바이스(442)와 인터렉션하게 할 수 있다.
일부 구현예에서, 클라이언트 디바이스(434) 및/또는 IoT 디바이스(442)는 디스플레이 디바이스가 없을 수 있지만, 오디오 인터페이스(예: 스피커 및/또는 마이크)를 포함하여, 그래픽 사용자 인터페이스 출력을 제공하지 않고도 청각적 사용자 인터페이스 출력을 제공하고, 사용자로부터 음성 자연어 입력을 수신하기 위한 마이크와 같은 사용자 인터페이스를 제공한다. 예를 들어, 일부 구현예에서, IoT 디바이스(442)는 하나 이상의 버튼과 같은 하나 이상의 촉각 입력 인터페이스를 포함할 수 있고, 그래픽 처리 장치(GPU)로부터 그래픽 데이터가 제공될 디스플레이 패널을 생략할 수 있다. 이러한 방식으로 디스플레이 패널과 GPU를 포함하는 컴퓨팅 디바이스에 비해 에너지 및 프로세싱 리소스를 절약할 수 있다.In some implementations, the client device 434 and/or the
클라이언트 디바이스(434)는 인터넷과 같은 네트워크(446)를 통해 서버 디바이스(402)와 통신할 수 있다. 클라이언트 디바이스(434)는 클라이언트 디바이스(434) 및/또는 IoT 디바이스(442)에서 계산 리소스를 보존하기 위해 계산 작업을 서버 디바이스(402)로 오프로딩할 수 있다. 예를 들어, 서버 디바이스(402)는 어시스턴트 애플리케이션(408)을 호스팅할 수 있고, 클라이언트 디바이스(434)는 하나 이상의 어시스턴트 인터페이스(110) 및/또는 IoT 디바이스(442)의 사용자 인터페이스(444)에서 수신된 입력을 서버 디바이스(402)에 전송할 수 있다. 그러나, 일부 구현예에서, 어시스턴트 애플리케이션(408)은 컴퓨팅 디바이스(434)에서 호스팅될 수 있다. 다양한 구현예에서, 자동 어시스턴트(408)의 모든 양태 전부 또는 그 이하가 서버 디바이스(402) 및/또는 클라이언트 디바이스(434)에서 구현될 수 있다. 이러한 구현들 중 일부에서, 자동 어시스턴트(408)의 양태들은 컴퓨팅 디바이스(434)의 로컬 자동 어시스턴트를 통해 구현되고, 자동 어시스턴트(408)의 다른 양태들을 구현하는 서버 디바이스(402)와 인터페이싱한다. 서버 디바이스(402)는 복수의 스레드를 통해 복수의 사용자 및 연관된 어시스턴트 애플리케이션을 선택적으로 제공할 수 있다. 어시스턴트 애플리케이션(408)의 양태들의 전부 또는 일부가 클라이언트 디바이스(434)의 로컬 자동 어시스턴트(438)를 통해 구현되는 구현예에서, 로컬 자동 어시스턴트(438)는 클라이언트 디바이스(434)의 운영 체제와 별개인 애플리케이션(예를 들어, 운영 체제 "위에"에 설치된)일 수 있고, 또는 대안적으로 클라이언트 디바이스(434)의 운영 체제에 의해 직접 구현될 수 있다(예를 들어, 운영 체제의 애플리케이션이지만 운영 체제와 통합될 수 있다).The client device 434 may communicate with the
일부 구현예에서, 자동 어시스턴트(408) 및/또는 자동 어시스턴트(438)는 클라이언트 디바이스(434), 서버 디바이스(402) 및/또는 하나 이상의 IoT 디바이스(442)에 대한 입력 및/또는 출력을 프로세싱하기 위해 다수의 상이한 엔진을 사용할 수 있는 입력 프로세싱 엔진(412)을 포함할 수 있다. 일부 구현예에서, 입력 프로세싱 엔진(412)은 오디오 데이터에 수록된 요청을 식별하기 위해 어시스턴트 인터페이스(436)에서 수신된 오디오 데이터를 프로세싱할 수 있는 음성 프로세싱 엔진(414)을 포함할 수 있다. 오디오 데이터는 클라이언트 디바이스(434)에서 계산 리소스를 보존하기 위해 예를 들어 클라이언트 디바이스(434)로부터 서버 디바이스(402)로 전송될 수 있다.In some implementations, automated
오디오 데이터를 텍스트로 변환하는 프로세스는 음성 인식 알고리즘을 포함할 수 있으며, 이는 단어 또는 문구에 대응하는 오디오 데이터의 그룹을 식별하기 위해 신경 네트워크, word2vec 알고리즘 및/또는 통계 모델을 이용할 수 있다. 오디오 데이터로부터 변환된 텍스트는 데이터 파싱 엔진(416)에 의해 파싱될 수 있으며, 사용자로부터 명령 문구를 생성 및/또는 식별하는데 사용될 수 있는 텍스트 데이터로서 자동화된 어시스턴트(408)에 제공될 수 있다. 일부 구현예에서, 데이터 파싱 엔진(416)에 의해 제공된 출력 데이터는 사용자가 자동 어시스턴트(408) 및/또는 자동 어시스턴트(408)에 의해 액세스될 수 있는 애플리케이션, 에이전트 및/또는 디바이스에 의해 수행될 수 있는 특정 액션 및/또는 루틴에 대응하는 입력을 제공했는지 여부를 결정하기 위해 액션 엔진(418)에 제공될 수 있다. 예를 들어, 어시스턴트 데이터(422)는 클라이언트 데이터(440)로서 서버 디바이스(402) 및/또는 클라이언트 디바이스(434)에 저장될 수 있고, 자동 어시스턴트(408)에 의해 수행될 수 있는 하나 이상의 액션들을 정의하는 데이터 및 이러한 액션들을 수행하는 것과 관련된 파라미터들을 포함할 수 있다.The process of converting audio data to text may include a speech recognition algorithm, which may use a neural network, a word2vec algorithm, and/or a statistical model to identify groups of audio data corresponding to words or phrases. The text converted from the audio data may be parsed by the
입력 프로세싱 엔진(412)이 사용자가 특정 동작 또는 루틴의 수행을 요청했다고 결정한 경우, 액션 엔진(418)은 특정 액션 또는 루틴에 대한 하나 이상의 파라미터를 결정할 수 있고, 출력 생성 엔진(420)은 특정 액션, 루틴 및/또는 하나 이상의 파라미터에 기초하여 사용자에게 출력을 제공할 수 있다. 예를 들어, 일부 구현예에서, 어시스턴트 인터페이스(436)로 향하는 제스처와 같은 사용자 입력에 응답하여, 자동 어시스턴트(438)는 제스처를 특징 짓는 데이터가 사용자가 자동 어시스턴트(408) 및/또는 자동 어시스턴트(438)가 수행할 것을 의도한 액션을 결정하기 위해 서버 디바이스(402)로 전송되게 할 수 있다.When the input processing engine 412 determines that the user has requested performance of a particular action or routine, the
클라이언트 디바이스(들)(434) 및/또는 IoT 디바이스(들)(442)는 각각 사용자로부터의 사용자 입력에 응답하여 출력을 제공할 수 있는 하나 이상의 센서를 포함할 수 있다. 예를 들어, 하나 이상의 센서는 사용자로부터의 오디오 입력에 응답하여 출력 신호를 제공하는 오디오 센서(즉, 오디오 응답 디바이스)를 포함할 수 있다. 출력 신호는 블루투스, LTE, Wi-Fi 및/또는 임의의 다른 통신 프로토콜과 같은 통신 프로토콜을 통해 클라이언트 디바이스(434), 서버 디바이스(402), IoT 디바이스(442)에 전송될 수 있다. 일부 구현예에서, 클라이언트 디바이스(434)로부터의 출력 신호는 클라이언트 디바이스(434)의 하나 이상의 프로세서에 의해 사용자 입력 데이터로 변환될 수 있다. 사용자 입력 데이터는 통신 프로토콜을 통해 IoT 디바이스(442)(예를 들어, Wi-Fi 가능 전구)로 전송될 수 있고, 클라이언트 디바이스(434)에서 프로세싱되고 및/또는 네트워크(446)를 통해 서버 디바이스(402)에 전송될 수 있다. 예를 들어, 클라이언트 디바이스(434) 및/또는 서버 디바이스(402)는 사용자 입력 데이터에 기초하여 사용자 입력 데이터를 통해 제어될 IoT 디바이스(442)의 기능을 결정할 수 있다.Client device(s) 434 and/or IoT device(s) 442 may each include one or more sensors capable of providing an output in response to user input from a user. For example, the one or more sensors may include an audio sensor (ie, an audio response device) that provides an output signal in response to audio input from a user. The output signal may be transmitted to the client device 434 , the
사용자는 어시스턴트 인터페이스(436)에서 수신되고 IoT 디바이스(442), 클라이언트 디바이스(434), 서버 디바이스(402), 자동 어시스턴트, 에이전트 어시스턴트 및/또는 기타 디바이스 또는 애플리케이션을 제어하기 위해 사용자가 제공하는 음성 발화와 같은 입력을 제공할 수 있다. 일부 구현예에서, 시스템(400)은 음성 발화를 완성하고, 음성 발화를 보충하고, 및/또는 사용자를 지원하는 방법에 대한 제안을 사용자에게 제공할 수 있다. 예를 들어, 사용자는 IoT 디바이스(442)가 특정 기능을 수행하게 할 의도로 불완전한 음성 발화를 제공할 수 있다. 불완전한 음성 발화는 어시스턴트 인터페이스(436)에서 수신되어 오디오 데이터로 변환될 수 있으며, 오디오 데이터는 프로세싱을 위해 서버 디바이스(402)에 전송될 수 있다. 서버 디바이스(402)는 오디오 데이터 또는 불완전한 음성 발화에 기초한 다른 데이터를 수신하고 불완전한 음성 발화를 완성하거나 그렇지 않으면 보충하기 위한 하나 이상의 제안을 결정할 수 있는 제안 엔진(424)을 포함할 수 있다. A user receives voice utterances received at
일부 구현예에서, 클라이언트 디바이스(434) 및/또는 IoT 디바이스(442)의 하나 이상의 제안 요소 및 디스플레이 패널을 제공하기 위한 타이밍은 제안 엔진(424)에 의해 결정될 수 있다. 제안 엔진(424)은 추가 음성 발화에 대한 하나 이상의 제안이 사용자에게 제시되어야 하는 시기를 결정할 수 있는 타이밍 엔진(428)을 포함할 수 있다. 일부 구현예에서, 하나 이상의 임계치는 제안 엔진(424)에 이용 가능한 액션 데이터(444)에 의해 특성화될 수 있다. 예를 들어, 액션 데이터(444)는 사용자가 음성 발화의 일부를 제공한 후 지연할 수 있는 시간에 대응하는 임계 값을 특성화할 수 있다. 사용자가 불완전한 음성 발화를 제공한 후 사용자가 지연하는 경우, 지연 시간의 양을 액션 날짜(444)로 특징 지어지는 임계 값과 비교될 수 있다. 지연이 임계 값을 만족하는 경우, 타이밍 엔진(428)은 불완전한 음성 발화를 완성하기 위해 하나 이상의 제안 요소가 사용자에게 제시되게 할 수 있다.In some implementations, the timing for providing the display panel and one or more suggestion elements of the client device 434 and/or
일부 구현예에서, 타이밍 엔진(428)은 사용자가 불완전한 음성 발화를 제공하는 것과 음성 발화를 완성하기 위한 제안을 수신하는 것 사이의 지연 시간에 대한 값을 적응시킬 수 있다. 예를 들어, 제안 엔진(424)은 어시스턴트 데이터(422), 액션 데이터(444) 및/또는 클라이언트 데이터(440)에 구현된 컨텍스트 데이터에 기초하여 사용자의 컨텍스트를 결정할 수 있고, 상기 컨텍스트를 지연 시간에 대한 값을 결정하기 위한 기초로 사용할 수 있다. 예를 들어, 사용자가 집에서 TV를 시청하는 것을 특징으로 하는 제1 컨텍스트에 있을 때, 타이밍 엔진(428)은 지연 시간을 제1 값으로 설정할 수 있다. 그러나, 사용자가 운전하고 차량 컴퓨팅 디바이스와 인터렉션하는 사용자를 특성화하는 제2 컨텍스트에 있을 때, 타이밍 엔진(428)은 지연 시간을 제1 값보다 큰 제2 값으로 설정할 수 있다. 이러한 방식으로, 사용자가 운전하는 동안 주의를 산만하게 하는 것을 제한하기 위해 음성 발화를 완성하기 위한 제안 요소가 상대적으로 지연될 수 있다.In some implementations, the timing engine 428 may adapt a value for the delay time between the user providing an incomplete spoken utterance and receiving a suggestion to complete the spoken utterance. For example, the suggestion engine 424 may determine the user's context based on context data embodied in
일부 구현예에서, 지연 시간에 대한 값은 사용자가 시간이 지남에 따라 불완전한 음성 발화를 제공하는 빈도에 비례하거나 그에 따라 조정될 수 있다. 예를 들어, 처음에 사용자가 자동 어시스턴트와 인터렉션하기 시작하면, Y 기간마다 불완전한 음성 발화 수를 X개를 제공할 수 있다. 그 후, 언젠가 사용자는 불완전한 발화 수 Y를 제공한 기록을 가질 수 있다. Z가 X 미만인 경우, 타이밍 엔진(428)은 지연 시간 임계 값이 시간에 따라 증가하게 할 수 있다. 따라서, 지연 시간 임계 값이 증가함에 따라, 서버 디바이스(402) 및/또는 클라이언트 디바이스(434)의 계산 리소스가 보존될 수 있고, 시간이 지남에 따라 디스플레이 패널에 제시되는 내용에 대한 변경이 적어지므로, 디스플레이 패널의 그래픽 데이터 프로세싱을 담당하는 GPU에 대한 로드를 감소시킨다.In some implementations, the value for delay time may be proportional to or adjusted accordingly with the frequency with which the user provides incomplete spoken utterances over time. For example, when a user initially starts interacting with an automated assistant, it can provide X the number of incomplete speech utterances per Y period. Then, at some point, the user may have a record of giving an incomplete number of utterances Y. If Z is less than X, the timing engine 428 may cause the delay time threshold to increase over time. Thus, as the latency threshold increases, the computational resources of the
일부 구현예에서, 제안 엔진(424)은 음성-텍스트 변환 모델 엔진(430)을 포함할 수 있으며, 이는 사용자로부터 예상되는 음성 발화(들)에 따라 다수의 상이한 음성-텍스트 변환 모델들로부터 음성-텍스트 변환 모델을 선택할 수 있다. 추가적으로 또는 대안적으로, 음성-텍스트 변환 모델 엔진(430)은 사용자로부터 예상되는 음성 발화(들) 유형에 따라 다수의 상이한 음성-텍스트 변환 모델로부터 음성-텍스트 변환 모델을 선택할 수 있다. 대안적으로 또는 추가적으로, 음성-텍스트 변환 모델 엔진(430)은 사용자로부터 예상되는 음성 발화(들)에 기초하여 선택된 음성-텍스트 변환 모델을 편향할 수 있다. 일부 구현예에서, 사용자에게 제공된 제안 요소가 적어도 일부 숫자 텍스트를 갖는 콘텐츠를 포함할 때, 음성-텍스트 변환 모델 엔진(430)은 콘텐츠에 기초하여 숫자 정보를 특징 짓는 음성 발화를 프로세싱하기 위해 적응되는 음성-텍스트 변환 모델을 선택할 수 있다. 대안적으로 또는 추가적으로, 사용자에게 제공된 제안 요소가 도시 또는 사람의 이름과 같은 적어도 고유 명사를 포함하는 콘텐츠를 포함하는 경우, 음성-텍스트 변환 모델 엔진(430)은 선택된 음성-텍스트 변환 모델을 편향시켜 고유 명사를 보다 쉽게 해석할 수 있게 한다. 예를 들어, 선택된 음성-텍스트 변환 모델은 고유 명사 프로세싱에서 발생할 수 있는 오류율이 감소되도록 편향될 수 있다. 이것은 다른 방법으로 발생할 수 있는 지연을 제거할 수 있고 및/또는 제안 엔진(424)으로부터 관련없는 제안의 빈도를 줄일 수 있다. In some implementations, the suggestion engine 424 may include a speech-to-text
일부 구현예에서, 불완전한 음성 발화 또는 완전한 음성 발화에 응답하여 사용자에게 제공되는 하나 이상의 제안 요소의 각 제안의 콘텐츠는 사용자의 컨텍스트에 기초할 수 있다. 예를 들어, 사용자의 컨텍스트는 디바이스 토폴로지 엔진(426)에 의해 생성된 데이터에 의해 특성화될 수 있다. 디바이스 토폴로지 엔진(426)은 사용자로부터 불완전하거나 완전한 음성 발화를 수신한 디바이스를 식별할 수 있으며, 수신 디바이스에 연결된 다른 디바이스와 같은 해당 디바이스와 연관된 정보, 다른 디바이스에 대한 수신 디바이스의 위치, 수신 디바이스에 대한 식별자, 수신 디바이스와 연관된 하나 이상의 다른 사용자, 수신 디바이스의 기능, 수신 디바이스와 페어링되거나 다른 방법으로 통신하는 하나 이상의 다른 디바이스의 기능 및/또는 음성을 수신한 디바이스와 연관될 수 있는 기타 정보를 결정할 수 있다. 예를 들어, 수신 디바이스가 사용자 집의 거실에 위치한 독립형 스피커 디바이스인 경우, 디바이스 토폴로지 엔진(426)은 독립형 스피커 디바이스가 거실에 있음을 결정할 수 있으며, 또한 거실에 위치하는 다른 디바이스를 결정할 수 있다.In some implementations, the content of each suggestion of one or more suggestion elements provided to the user in response to an incomplete or complete spoken utterance may be based on the user's context. For example, the user's context may be characterized by data generated by the
제안 엔진(424)은 사용자에게 제시될 제안 요소를 생성하기 위해 거실에 있는 다른 디바이스의 설명과 같은 정보를 사용할 수 있다. 예를 들어, 사용자로부터 불완전한 음성 발화를 수신하는 독립형 스피커 디바이스에 응답하여, 디바이스 토폴로지 엔진(426)은 텔레비전이 독립형 스피커 디바이스와 함께 거실에 있다고 결정할 수 있다. 제안 엔진(424)은 텔레비전이 동일한 방에 있다는 결정에 기초하여 불완전한 음성 발화를 완성하기 위한 제안 요소에 대한 콘텐츠를 생성할 수 있다. 예를 들어 불완전한 음성 발화가 "Assistant, change"인 경우 제안 요소의 콘텐츠에 "the channel on the television"을 포함할 수 있다. 대안적으로 또는 추가적으로, 디바이스 토폴로지 엔진(426)은 텔레비전이 독립형 스피커 디바이스와 동일한 방에 있음을 결정할 수 있으며, 독립형 스피커 디바이스가 제안 요소를 그래픽으로 표현할 수 없지만 텔레비전은 가능하다고 결정할 수 있다. 따라서, 불완전한 음성 발화에 응답하여, 제안 엔진(424)은 텔레비전 디스플레이 패널이 "Change the channel of my television"과 같은 콘텐츠를 포함하는 하나 이상의 제안 요소를 제시하게 할 수 있다. 사용자가 "Change the channel of my television"이라는 제안 요소의 내용을 말하면 자동 어시스턴트가 TV 채널을 변경하는 동시에 텔레비전에서 렌더링되는 새 TV 채널에 동시에 제공될 수 있는 추가 제안 요소를 생성할 수 있다.The suggestion engine 424 may use information, such as descriptions of other devices in the living room, to generate a suggestion element to be presented to the user. For example, in response to a standalone speaker device receiving an incomplete speech utterance from a user, the
일부 구현예에서, 어시스턴트 데이터(422), 액션 데이터(444) 및/또는 클라이언트 데이터(440)와 같은 데이터는 사용자와 자동 어시스턴트(408) 및/또는 자동 어시스턴트(438) 간의 과거 인터렉션을 특성화할 수 있다. 제안 엔진(424)은 어시스턴트 인터페이스(436)에 제공되는 불완전한 음성 발화 또는 완전한 발화에 응답하여 사용자에게 제시되는 제안 요소에 대한 콘텐츠를 생성하기 위해 전술한 데이터를 사용할 수 있다. 이러한 방식으로 사용자가 특정 선호 명령을 일시적으로 잊어버린 경우 사용자에게 특정 선호 명령을 상기시키기 위해 제안 요소를 통해 자연어 텍스트를 제공할 수 있다. 그러한 선호되는 명령은 이전에 수신된 불완전하거나 완전한 음성 발화 콘텐츠를 비교하고 콘텐츠를 사용자로부터 수신된 이전 자연어 입력과 비교함으로써 식별될 수 있다. 가장 자주 발생하는 자연어 입력은 선호하는 명령에 해당하는 것으로 간주할 수 있다. 따라서 완전하거나 불완전한 음성 발화의 내용이 이러한 자연어 입력에 해당할 때, 이러한 자연어 입력의 내용을 기반으로 제안 요소가 렌더링 및/또는 생성될 수 있다. 또한, 가장 자주 프로세싱되는 자연어 입력을 식별하고 및/또는 특정 사용자와 연관된 선호 명령을 식별함으로써, 제안 엔진(424)은 자동 어시스턴트를 통해 가장 자주 수행되고 가장 성공적으로 수행된 명령에 기초하여 사용자에게 제안을 제공할 수 있다. 따라서 이러한 제안을 제공한 결과, 가장 적게 성공한 명령의 프로세싱이 줄어들어 계산 리소스를 보존할 수 있다.In some implementations, data such as
도 5는 사용자에 의해 제공되는 음성 발화를 완성 및/또는 보충하기 위한 하나 이상의 제안 요소들을 자동 어시스턴트, 디바이스, 애플리케이션 및/또는 임의의 다른 장치 또는 모듈에 제공하는 방법(500)을 도시한다. 방법(500)은 자동 어시스턴트와 연관될 수 있는 하나 이상의 컴퓨팅 디바이스, 애플리케이션 및/또는 임의의 기타 장치 또는 모듈에 의해 수행될 수 있다. 방법(500)은 사용자가 음성 발화의 적어도 일부를 제공했다고 결정하는 동작(502)을 포함할 수 있다. 음성 발화는 하나 이상의 컴퓨팅 디바이스의 하나 이상의 자동 어시스턴트 인터페이스에 사용자에 의해 청각적으로 제공되는 구두 자연어 입력일 수 있다. 음성 발화는 하나 이상의 단어, 하나 이상의 구 또는 음성 자연어의 다른 구성을 포함할 수 있다. 예를 들어, 자동 어시스턴트 인터페이스는 마이크를 포함할 수 있고, 컴퓨팅 디바이스는 다른 자동 어시스턴트 인터페이스로도 동작할 수 있는 터치 디스플레이 패널을 포함하는 스마트 디바이스일 수 있다. 일부 구현예에서, 컴퓨팅 디바이스 또는 컴퓨팅 디바이스와 통신하는 서버 디바이스는 사용자가 음성 발화를 제공했음을 결정할 수 있다.5 illustrates a
방법(500)은 음성 발화가 완전한지 여부를 결정하는 동작(504)을 더 포함할 수 있다. 음성 발화가 완전한지 여부를 결정하는 것은 기능의 하나 이상의 파라미터의 각 파라미터가 할당된 값을 갖는지 여부를 결정하는 것을 포함할 수 있다. 예를 들어, 음성 발화가 "Assistant, turn down"인 경우, 사용자는 디바이스의 출력 양식을 끄는 액션을 실행하도록 요청할 수 있다. 액션은 "볼륨 감소" 기능 또는 "밝기 감소" 기능과 같은 기능에 해당할 수 있으며 이러한 기능을 사용하려면 사용자가 디바이스 이름을 지정해야 할 수 있다. 대안적으로 또는 추가적으로, 음성 발화가 완전한지 여부를 결정하는 것은 자동 어시스턴트가 음성 발화 수신에 응답하여 액션을 수행할 수 있는지 여부를 결정하는 것을 포함할 수 있다.
선택적으로, 동작(504)은 음성 발화가 완전한지 여부를 결정하기 위해, 특정 카테고리의 액션이 음성 발화에 응답하여 수행될 수 있는지 여부를 결정하는 것을 포함할 수 있다. 예를 들어, 사용자에게 명확성을 위해 말한 내용을 반복하도록 요청하는 자동 어시스턴트와 같은 액션은 사용자가 음성에 응답하는 자동 어시스턴트에도 불구하고 완전한 음성을 제공했음을 나타내는 것이 아닌 것으로 간주될 수 있다. 그러나, 웹 쿼리 수행, 다른 디바이스 제어, 애플리케이션 제어 및/또는 의도된 동작을 수행하도록 자동 어시스턴트에게 요청하는 것과 같은 액션은 완전한 음성 발화로 간주될 수 있다. 예를 들어, 일부 구현예에서, 적어도 하나의 요청이 음성 발화로부터 식별되고 요청이 자동 어시스턴트를 통해 수행될 수 있을 때 음성 발화가 완전한 것으로 간주될 수 있다.Optionally, operation 504 can include determining whether a particular category of action can be performed in response to the spoken utterance to determine whether the spoken utterance is complete. For example, an action such as an automatic assistant asking the user to repeat what has been said for clarity may not be considered an indication that the user has provided complete speech despite the automatic assistant responding to the speech. However, actions such as performing a web query, controlling another device, controlling an application, and/or asking an automated assistant to perform an intended action may be considered full speech utterance. For example, in some implementations, a voice utterance may be considered complete when at least one request is identified from the voice utterance and the request may be performed via an automated assistant.
음성 발화가 완성되었다고 결정될 때(예를 들어, "어시스턴트, 텔레비전 끄기"), 방법(500)은 수행될 액션(들)에 기초하여 사용자에게 제안(들)을 렌더링 및/또는 생성하는 선택적인 동작(512) 및/또는 음성 발화(들)에 대응하는 액션(들)을 수행하는 동작(514)로 진행할 수 있다. 예를 들어, 자동 어시스턴트 인터페이스를 통해 음성 발화를 수신한 컴퓨팅 디바이스는 음성 발화를 통해 지정된 액션 또는 액션의 수행을 초기화할 수 있다. 그러나, 음성 발화가 불완전한 것으로 결정되면, 방법(500)은 동작(506)으로 진행할 수 있다.When it is determined that the spoken utterance is complete (eg, “assistant, turn off the television”), the
동작(506)은 불완전한 음성 발화 후에 지연의 임계 지속시간이 발생했는지를 결정하는 것을 포함하는 선택적 동작일 수 있다. 예를 들어, 음성 입력 무음의 임계 지속시간이 임계 값을 충족하는지 여부는 사용자의 허가를 받아 음성 발화를 제공하는 사용자, 사용자의 컨텍스트 및/또는 불완전한 음성 발화와 연관된 기타 정보에 기초하여 결정될 수 있다. 음성 입력 무음의 양이 지연의 임계 지속시간을 만족할 때, 방법(500)은 동작(508)으로 진행할 수 있다. 그러나, 음성 입력 무음의 양이 지연의 임계 지속시간을 만족하지 않는 경우, 방법(500)은 동작(510)으로 진행할 수 있다.Act 506 may be an optional act that includes determining whether a threshold duration of delay has occurred after an incomplete speech utterance. For example, whether a threshold duration of voice input silence meets the threshold value may be determined based on the user providing the voice utterance with the user's permission, the user's context, and/or other information associated with the incomplete voice utterance. . When the amount of speech input silence satisfies the threshold duration of the delay, the
방법(500)은 선택적으로 동작(504)에서 동작(508)으로 진행하거나, 선택적으로 동작(504)에서 동작(506)으로 진행할 수 있다. 지연의 임계 지속시간이 동작(506)에서 만족되는 것으로 결정될 때, 방법(500)은 음성 발화를 완료하기 위한 하나 이상의 제안을 렌더링하는 동작(508)으로 진행할 수 있다. 음성 발화를 완성하기 위한 하나 이상의 제안의 콘텐츠는 불완전한 음성 발화를 제공한 사용자, 불완전한 음성 발화의 콘텐츠, 음성 발화가 제공된 컨텍스트, 음성 발화를 수신한 디바이스와 연관된 디바이스 토폴로지를 나타내는 데이터,사용자와 자동 어시스턴트(또는 하나 이상의 사용자들과 자동 어시스턴트) 사이의 이전 인터렉션을 특징 짓는 이력 인터렉션 데이터 및/또는 제안이 직면될 수 있는 임의의 기타 정보에 기초할 수 있다. 예를 들어, 불완전한 음성 발화가 "Assistant, turn down"인 경우, 사용자에게 제공되는 제안 콘텐츠는 "텔레비전"을 포함할 수 있으며, 이는 TV 및/또는 사용자가 불완전한 음성 발화를 수신한 컴퓨팅 디바이스와 동일한 방에 위치된다고 표시하는 디바이스 토폴로지 데이터에 기초할 수 있다. 대안적으로, 불완전한 음성 발화가 "Assistant, send a message to"인 경우, 사용자에게 제공되는 제안 콘텐츠는 사용자가 자동 어시스턴트로 하여금 사용자 배우자 및 형제에게 메시지를 보내도록 요청한 이전 대화 세션을 특징 짓는 이력적 인터렉션 데이터에 기초할 수 있다. 따라서 제안의 콘텐츠는 "내 형제" 및 "내 아내"를 포함할 수 있다. 일부 구현예에서, 하나 이상의 제안 요소의 콘텐츠는 동작(502) 이후, 선택적 동작(506) 이전 및 동작(508) 전에 결정 및/또는 생성될 수 있다.
방법(500)은 동작(506) 또는 동작(508)으로부터 다른 음성 발화가 사용자에 의해 제공되었는지를 결정하는 동작(510)으로 진행할 수 있다. 예를 들어, 사용자는 불완전한 음성 발화를 제공한 후 추가 음성 발화를 제공할 수 있다. 그러한 경우에, 방법(500)은 동작(510)에서 동작(504)으로 다시 진행할 수 있다. 예를 들어, 사용자는 제안 요소의 콘텐츠에 대응하는 추가 음성 발화를 제공하여, 사용자가 제안 요소를 선택했음을 자동 어시스턴트에게 알릴 수 있다. 이는 사용자가 선택한 제안 요소의 콘텐츠에 대응하는 초기 불완전한 음성 발화 및 추가 음성 발화로 구성될 수 있는 음성 발화의 편집을 초래할 수 있다. 이러한 음성 발화의 편집은 음성 발화의 편집이 완전한 음성 발화에 대응하는지 여부를 결정하는 동작(504)을 거칠 수 있다. 구현예에서, 동작(510)에서, 시스템은 지속시간 동안 다른 음성 발화를 기다릴 수 있고, 지속시간 동안 다른 음성 발화가 수신되지 않으면 동작(514)(또는 512)로 진행할 수 있다. 다른 음성 발화가 지속시간 동안 수신되면, 시스템은 동작(504)로 다시 진행할 수 있다. 이러한 구현예 중 일부에서, 지속시간은 동적으로 결정될 수 있다. 예를 들어, 지속시간은 이미 제공된 음성 발화가 "불완전한" 요청인지, "완전한 요청"인지 여부에 따라 달라질 수 있으며, 및/또는 이미 제공된 음성 발화의 "완전한 요청"의 특성(들)에 따라 달라질 수 있다.
음성 발화의 편집이 완전한 음성 발화에 대응할 때, 방법(500)은 선택적 동작(512) 및/또는 동작(514)으로 진행할 수 있다. 그러나, 음성 발화의 편집이 완전한 음성 발화로 결정되지 않는 경우, 방법(500)은 선택적으로 동작(506)으로 진행할 수 있고 및/또는 선택적으로 동작(508)으로 진행할 수 있다. 예를 들어, 방법(500)이 동작(508)으로 다시 진행될 때, 음성 발화를 완성하기 위한 제안은 적어도 음성 발화의 편집 콘텐츠에 기초할 수 있다. 예를 들어, 사용자가 "내 형제" 제안을 선택하면, 불완전한 음성 발화 "Assistant, send a message to" 및 "내 형제"의 조합이 추가 제안을 제공하기 위한 기초로서 사용될 수 있다. 이러한 방식으로, 사용자가 각각의 음성 발화를 완성하기 위한 제안을 계속 선택함에 따라 제안 주기가 제공될 수 있다. 예를 들어, 사용자가 이전에 자동 어시스턴트와 별도의 애플리케이션을 통해 동일한 메시지를 사용자에게 보냈음을 나타내는 과거 인터렉션 데이터에 기초하여 후속 제안은 "I’m on my way"를 포함할 수 있다.When the editing of the spoken utterance corresponds to a complete spoken utterance, the
일부 구현예에서, 방법(500)은 수행될 액션에 기초하여 사용자에 대한 제안을 렌더링하는 동작(510)에서 선택적 동작(512)으로 진행할 수 있다. 예를 들어, 자동 어시스턴트가 사용자를 위해 경보 시스템을 켜는 등의 액션을 수행하도록 요청되면, 상기 액션 및/또는 상기 액션과 연관된 모든 정보를 사용자가 선택할 추가 제안 요소를 제공하기 위한 기초로서 사용될 수 있다. 예를 들어, 자동 어시스턴트를 통해 실행될 완전한 음성 발화가 "Assistant, turn on my alarm system"인 경우, 하나 이상의 다른 제안 요소가 동작(512)에서 렌더링될 수 있으며, "and set my alarm for", "turn off all my lights", "and turn on my audiobook" 및/또는 사용자에게 적절한 기타 제안 콘텐츠를 포함할 수 있다. 구현예에서, 동작(512)에서, 시스템은 추가 음성 입력을 수신하는 것에 응답하여 또는 다른 음성 발화가 지속시간 동안 수신되지 않는 것에 응답하여 동작(514)로 진행한다. 이러한 구현예 중 일부에서, 지속시간은 동적으로 결정될 수 있다. 예를 들어, 지속시간은 이미 제공된 음성 발화가 "불완전한" 요청인지, "완전한 요청"인지 여부에 따라 달라질 수 있으며, 및/또는 이미 제공된 음성 발화의 "완전한 요청"의 특성(들)에 따라 달라질 수 있다. In some implementations,
일부 구현예에서, 사용자와 자동 어시스턴트 간의 인터렉션의 양을 줄이기 위해 제안이 렌더링 및/또는 생성될 수 있으며, 이에 따라 전력, 계산 리소스 및/또는 네트워크 리소스를 보존할 수 있다. 예를 들어, 동작(512)에서 렌더링된 보충 제안 요소의 콘텐츠는 "each night after 10:00 PM"를 포함할 수 있다. 이러한 방식으로, 사용자가 동작(510)에서 결정된 바와 같이(예를 들어, 다른 음성 발화를 통해) 보충 제안 요소를 선택했다고 결정되면, 방법(500)은 동작(504)으로 진행하고, 이어서 동작(514)으로 진행하여, 자동 어시스턴트로 하여금 매일 밤 10:00 PM 이후에 알람 시스템이 켜지는 설정을 생성하게 한다. 따라서, 다음날 저녁에 사용자는 "Assistant, turn on…"이라는 음성 발화의 동일한 초반부를 제공할 필요가 없고, 오히려 선택된 다른 제안 요소를 통해 방금 사용한 설정에 의존할 수 있다. In some implementations, suggestions may be rendered and/or generated to reduce the amount of interaction between the user and the automated assistant, thereby conserving power, computational resources, and/or network resources. For example, the content of the supplemental suggestion element rendered in operation 512 may include "each night after 10:00 PM." In this way, if it is determined that the user has selected the supplemental suggestion element as determined in act 510 (eg, via another spoken utterance),
방법(500)은 사용자가 초기 및 완전한 음성 발화 후에 추가 음성 발화를 제공하지 않았거나 음성 발화의 편집에 대응하는 하나 이상의 명령이 실행되기를 선호한다고 표시하는 경우 동작(510)으로부터 선택적 동작(512) 및/또는 동작(514)로 진행할 수 있다. 예를 들어, 사용자가 "Assistant change the"과 같은 음성 발화를 제공한 후, "channel of the television"과 같은 콘텐츠를 포함하는 후속적으로 제안 요소를 선택한 경우, 사용자는 TV를 변경하기 위해 특정 채널을 지정하지 않았음에도 해당 요청이 실행되도록 선택할 수 있다. 그 결과, 동작(514)에서, 사용자가 명령에서 결과의 명령어를 더 좁히기 위한 임의의 보충 제안 요소를 선택하지 않더라도 텔레비전 채널이 변경될 수 있다(예를 들어, 채널 변경은 과거 인터렉션 데이터로부터 도출된 사용자의 학습된 선호도에 기초할 수 있음).
도 6는 예시적 컴퓨터 시스템(610)의 블록도이다. 컴퓨터 시스템(610)는 일반적으로 적어도 하나의 프로세서(614)를 포함하며, 버스 서브시스템(612)을 통해 다수의 주변 디바이스들과 통신한다. 이들 주변 디바이스들은 예를 들면, 메모리(625) 및 파일 저장 서브시스템(626)을 포함하는 저장 서브시스템(624), 사용자 인터페이스 출력 디바이스(620), 사용자 인터페이스 입력 디바이스(622) 및 네트워크 인터페이스 서브시스템(616)을 포함할 수 있다. 입력 및 출력 디바이스는 컴퓨터 시스템(610)과 사용자 인터렉션을 하게 한다. 네트워크 인터페이스 서브시스템(616)은 외부 네트워크에 대한 인터페이스를 제공하며, 다른 컴퓨터 시스템들의 대응하는 인터페이스 디바이스들과 연결된다. 6 is a block diagram of an
사용자 인터페이스 입력 디바이스(622)는 키보드, 마우스, 트랙볼, 터치패드 또는 그래픽 태블릿, 스캐너, 디스플레이에 통합된 터치스크린과 같은 포인팅 디바이스, 음성 인식 시스템, 마이크로폰과 같은 오디오 입력 디바이스 및/또는 다른 유형의 입력 디바이스를 포함한다. 일반적으로, 용어 "입력 디바이스"의 사용은 정보를 컴퓨터 시스템(610) 또는 통신 네트워크에 입력하기 위한 모든 가능한 유형의 디바이스들과 방식들을 포함하도록 의도된다.User
사용자 인터페이스 출력 디바이스(620)는 디스플레이 서브시스템, 프린터, 팩스 기계 또는 오디오 출력 디바이스와 같은 비-시각적 디스플레이를 포함할 수 있다. 디스플레이 서브시스템은 CRD, LCD와 같은 평면 패널 디바이스, 프로젝션 디바이스 또는 시각적 이미지를 생성하기 위한 일부 기타 메커니즘을 포함할 수 있다. 또한, 디스플레이 서브시스템은 오디오 출력 디바이스와 같은 비-시각적 디스플레이를 제공할 수 있다. 일반적으로, 용어 "출력 디바이스"의 사용은 정보를 컴퓨터 시스템(610)로부터 사용자에게 또는 다른 기계 또는 컴퓨터 시스템에 정보를 출력하기 위한 모든 가능한 유형의 디바이스들과 방식들을 포함하도록 의도된다.User
저장 서브시스템(624)은 본 명세서에 기술된 일부 또는 전부의 모듈들의 기능을 제공하기 위한 프로그래밍 및 데이터 구조를 저장한다. 예를 들어, 저장 서브시스템(624)은 방법(400) 및/또는 컴퓨팅 디바이스(114), 서버 디바이스, 컴퓨팅 디바이스(216), 어시스턴트 인터렉션 모듈, 클라이언트 디바이스(들) (434), 컴퓨팅 디바이스(314), 서버 디바이스(들)(402), IoT 디바이스(들)(442), 자동 어시스턴트(408), 자동 어시스턴트(434), 자동 어시스턴트(들) 및/또는 본 명세서에서 논의된 임의의 다른 디바이스, 장치, 애플리케이션 및/또는 모듈의 선택된 양태를 수행하기 위한 로직을 포함할 수 있다.Storage subsystem 624 stores programming and data structures for providing the functionality of some or all modules described herein. For example, storage subsystem 624 may include
이들 소프트웨어 모듈들은 일반적으로 프로세서(614) 단독으로 또는 다른 프로세서들과의 조합에 의해 실행된다. 저장 서브시스템(624)에서 사용된 메모리(625)는 프로그램 실행 중에 명령어들 및 데이터의 저장을 위한 메인 RAM(630) 및 고정된 명령어들이 저장되는 ROM(632)을 포함하는 다수의 메모리들을 포함할 수 있다. 파일 저장 서브시스템(626)은 프로그램 및 데이터 파일에 대한 영구적 저장을 제공할 수 있고, 하드 디스크 드라이브, 연관된 이동식 매체와 함께인 플로피 디스크 드라이브, CD-ROM 드라이브, 광학 드라이브 또는 이동식 매체 카트리지들을 포함할 수 있다. 특정 구현예들의 기능을 구현하는 모듈들은 파일 저장 서브시스템(626)에 의해 저장 서브시스템(624)에 또는 프로세서(들)(614)에 의해 액세스가능한 다른 기계에 저장될 수 있다.These software modules are typically executed by the
버스 서브시스템(612)은 의도된 대로 컴퓨터 시스템(610)의 다양한 컴포넌트들 및 서브시스템들이 서로 통신하게 하기 위한 메커니즘을 제공한다. 버스 서브시스템(612)이 개략적으로 단일의 버스로 도시되었지만, 버스 서브시스템의 대안적 구현예들은 다수의 버스들을 사용할 수 있다.
컴퓨터 시스템(610)은 워크스테이션, 서버, 컴퓨팅 클러스터, 블레이드 서버, 서퍼팜 또는 임의의 기타 데이터 프로세싱 시스템 또는 컴퓨팅 디바이스를 포함하는 다양한 유형들일 수 있다. 컴퓨터 및 네트워크의 끊임없이 변화하는 특성으로 인해, 도 6에 도시된 컴퓨터 시스템(610)는 일부 구현예들을 설명하기 위한 목적의 특정 예로서 만 의도된다. 컴퓨터 시스템(610)의 많은 다른 구성들이 도 6에 도시된 컴퓨터 시스템보다 많거나 적은 컴포넌트들을 가질 수 있다.The
본 명세서에서 논의된 시스템들이 사용자들(또는 "참여자들"로 종종 지칭됨)에 관한 개인 정보를 수집하거나 또는 개인 정보를 사용하는 경우들에 있어서, 사용자들에게 프로그램들 또는 구성들이 사용자 정보(예를 들면, 사용자의 소셜 네트워크, 소셜 액션들 또는 활동들, 직업, 사용자의 선호들 또는 사용자의 현재 지리적 위치)에 관한 정보를 수집할 것인지 여부를 제어할, 사용자와 더 관련된 콘텐츠 서버로부터의 콘텐츠를 수신할지 및/또는 어떻게 수신할지 제어할 기회가 제공될 수 있다. 추가로, 특정 데이터는 그것이 저장되거나 사용되기 전에 하나 이상의 다양한 방식들로 취급되어, 개인적으로 식별가능한 정보는 제거된다. 예를 들면, 사용자의 신원은 사용자에 관한 개인적으로 식별가능한 정보가 결정될 수 없도록 취급되거나 또는 사용자의 지리적 위치는 위치 정보가 획득된 곳에서 일반화되어(시, 우편번호 또는 주 수준으로), 사용자의 특정한 지리적 위치가 결정될 수 없도록 한다. 따라서, 사용자는 사용자에 관한 정보가 어떻게 수집되는지 그리고 사용되는지에 관한 제어를 가질 수 있다.In cases where the systems discussed herein collect or use personal information about users (or sometimes referred to as "participants"), programs or configurations to users content from a content server that is more relevant to the user, which will control whether to collect information about the user's social network, social actions or activities, occupation, user's preferences or the user's current geographic location, for example. Opportunities may be provided to control whether and/or how to receive. Additionally, certain data is treated in one or more various ways before it is stored or used, so that personally identifiable information is removed. For example, the user's identity may be treated such that personally identifiable information about the user cannot be determined, or the user's geographic location may be generalized (at the city, zip code or state level) from where the location information was obtained. Ensure that no specific geographic location can be determined. Thus, the user can have control over how information about the user is collected and used.
일부 구현예에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되고, 사용자에 의해 제공되는 음성 발화를 특징 짓는 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것을 포함한다. 음성 발화는 자연어 콘텐츠를 포함하며, 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신된다. 상기 방법은 상기 음성 발화를 특징 짓는 상기 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여, 적어도 자동 어시스턴트가 상기 자연어 콘텐츠에 기초하여 하나 이상의 액션들이 수행되도록 할 수 있는지 여부를 결정하는 것을 포함하여, 상기 음성 발화가 완전한지 여부를 결정하는 단계를 포함한다. 상기 방법은 음성 발화가 불완전한 것으로 결정되는 경우 상기 음성 발화가 불완전하다고 결정함에 응답하여, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 하는 단계를 더 포함한다. 상기 하나 이상의 제안 요소들은 상기 사용자에 의해 상기 자동 어시스턴트 인터페이스에 발화된 경우 상기 자동 어시스턴트로 하여금 액션을 완료하도록 동작하게 하는 다른 자연어 콘텐츠를 상기 디스플레이 패널을 통해 제공하는 특정 제안 요소를 포함한다. 상기 방법은 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 한 후, 상기 사용자가 상기 특정 제안 요소의 다른 자연어 콘텐츠와 연관된 다른 음성 발화를 제공했음을 결정하는 단계를 더 포함한다. 상기 방법은 상기 사용자가 다른 음성 발화를 제공했다는 결정에 응답하여, 상기 음성 발화와 상기 다른 음성 발화의 조합이 완전한지 여부를 결정하는 단계를 더 포함한다. 상기 방법은 음성 발화 및 다른 음성 발화의 조합이 완전한 것으로 결정된 경우: 자연어 콘텐츠 및 다른 음성 발화에 기초하여 자동 어시스턴트를 통해 하나 이상의 액션들이 수행되게 하는 단계를 더 포함한다.In some implementations, a method implemented by one or more processors is provided and includes performing speech-to-text conversion processing on data characterizing a spoken utterance provided by a user. The spoken utterances include natural language content and are received via an automated assistant interface of a computing device coupled to the display panel. The method includes, based on performing speech-to-text processing on the data characterizing the speech utterance, determining whether at least an automated assistant can cause one or more actions to be performed based on the natural language content and determining whether the voice utterance is complete. The method further comprises, if the voice utterance is determined to be incomplete, causing a display panel of the computing device to provide one or more suggestive elements in response to determining that the voice utterance is incomplete. The one or more suggestion elements include a specific suggestion element that provides, via the display panel, other natural language content that, when uttered by the user in the automated assistant interface, causes the automated assistant to complete an action. The method further includes causing the display panel of the computing device to provide one or more suggested elements, and then determining that the user has provided another spoken utterance associated with other natural language content of the particular suggested element. The method further comprises, in response to determining that the user has provided another spoken utterance, determining whether the combination of the spoken utterance and the other spoken utterance is complete. The method further includes when the combination of the spoken utterance and the other spoken utterance is determined to be complete: causing one or more actions to be performed via the automated assistant based on the natural language content and the other spoken utterance.
본 명세서에서 논의된 기술의 이들 또는 다른 구현예들은 다음 구성들 중 하나 이상을 포함할 수 있다. These or other implementations of the techniques discussed herein may include one or more of the following configurations.
일부 구현예에서, 상기 사용자에 의해 제공되는 음성 발화를 특징 짓는 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 단계는 상기 데이터로부터 하나 이상의 후보 텍스트 세그먼트들을 생성하는 단계를 포함하고, 그리고 상기 방법은 상기 음성 발화가 불완전한 것으로 결정되는 경우 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 후보 텍스트 세그먼트들 중 적어도 하나의 후보 텍스트 세그먼트의 그래픽 표현을 제공하게 하는 단계를 더 포함한다. 이러한 구현예의 일부 버전에서, 상기 방법은 상기 음성 발화가 완전하고 상기 하나 이상의 액션들에 대한 모든 필수 파라미터들을 포함하는 것으로 결정되는 경우: 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 후보 텍스트 세그먼트들 중 적어도 하나의 후보 텍스트 세그먼트의 그래픽 표현을 제공하는 것을 우회하게 하고, 상기 하나 이상의 제안 요소들을 제공하는 것을 우회하게 하는 단계를 더 포함한다. 이러한 구현예의 일부 다른 버전에서, 상기 방법은 상기 음성 발화가 완전한 것으로 결정되는 경우: 한 명 이상의 사용자들이 자동 어시스턴트 인터페이스에 말할 때 상기 사용자로부터 상기 음성 발화와 연관된 음성-텍스트 변환 프로세싱의 양에 비해 음성-텍스트 변환 프로세싱의 양을 감소시키고, 상기 자동 어시스턴트로 하여금 상기 액션을 완료하도록 동작하게 하는 자연어 명령을 식별하는 단계; 상기 자연어 명령을 식별하는 것에 기초하여, 상기 자연어 명령을 특징 짓는 하나 이상의 다른 제안 요소들을 생성하는 단계; 및 적어도 상기 음성 발화가 완전하다고 결정함에 응답하여, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 다른 제안 요소들을 제공하게 하는 단계를 더 포함한다.In some implementations, performing speech-to-text conversion processing on data characterizing a spoken utterance provided by the user comprises generating one or more candidate text segments from the data, and the method comprises: The method further includes causing the display panel of the computing device to provide a graphical representation of at least one of the one or more candidate text segments if the spoken utterance is determined to be incomplete. In some versions of this implementation, when it is determined that the spoken utterance is complete and includes all required parameters for the one or more actions: causes a display panel of the computing device to select one of the one or more candidate text segments circumventing providing a graphical representation of the at least one candidate text segment, and circumventing providing the one or more suggestion elements. In some other versions of this implementation, the method comprises: when the spoken utterance is determined to be complete: a voice compared to the amount of speech-to-text conversion processing associated with the spoken utterance from the user when one or more users speak to an automated assistant interface - identifying natural language commands that reduce the amount of text translation processing and cause the automatic assistant to operate to complete the action; generating, based on identifying the natural language command, one or more other suggestion elements characterizing the natural language command; and in response to determining that at least the spoken utterance is complete, causing a display panel of the computing device to present the one or more other suggestion elements.
일부 구현예에서, 상기 방법은 상기 음성 발화가 불완전한 것으로 결정되는 경우: 적어도 하나의 제안 요소가 상기 디스플레이 패널을 통해 제공될 때 상기 사용자로부터 예상되는 음성 발화의 유형에 기초하여, 다수의 상이한 음성-텍스트 변환 프로세싱 모델들로부터 음성-텍스트 변환 프로세싱 모델을 선택하는 단계를 더 포함한다.In some embodiments, the method further comprises: when it is determined that the spoken utterance is incomplete: based on a type of spoken utterance expected from the user when at least one suggestion element is provided via the display panel, a plurality of different voice- The method further includes selecting a speech-to-text conversion processing model from the text conversion processing models.
일부 구현예에서, 상기 방법은 상기 음성 발화가 불완전한 것으로 결정되는 경우: 상기 하나 이상의 제안 요소들 중 하나 이상의 용어들을 향해 및/또는 상기 하나 이상의 제안 요소들에 대응하는 콘텐츠의 하나 이상의 예상된 유형들 향해 음성-텍스트 변환 프로세싱을 편향하는 단계를 더 포함한다.In some implementations, the method comprises: when the spoken utterance is determined to be incomplete: one or more expected types of content towards and/or corresponding to one or more of the one or more suggested elements. biasing the speech-to-text conversion processing towards
일부 구현예에서, 상기 방법은 음성 입력 무음의 임계 지속시간이 상기 음성 발화에 후속하는지 여부를 결정하는 단계를 더 포함한다. 상기 구현예 중 일부에서, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 제안 요소들을 제공하게 하는 것은 적어도 상기 음성 입력 무음의 임계 지속시간이 상기 음성 발화에 후속한다고 결정함에 응답하여 이루어진다.In some implementations, the method further comprises determining whether a threshold duration of silence in a speech input follows the speech utterance. In some of the above implementations, causing the display panel of the computing device to provide the one or more suggested elements is in response to determining that at least the threshold duration of silence of the speech input follows the speech utterance.
일부 구현예에서, 상기 음성-텍스트 변환 프로세싱을 수행하는 단계는 제1 후보 텍스트 세그먼트 및 제2 후보 텍스트 세그먼트를 결정하는 단계를 포함하고, 상기 제1 후보 텍스트 세그먼트 및 상기 제2 후보 텍스트 세그먼트는 상기 음성 발화의 상이한 해석들에 대응한다. 상기 구현예 중 일부에서, 상기 특정 제안 요소는 상기 제1 후보 텍스트 세그먼트에 기초하여 결정되고, 적어도 하나의 다른 제안 요소는 상기 제2 후보 텍스트 세그먼트에 기초하여 결정된다. 상기 구현예 중 일부 버전에서, 상기 컴퓨팅 디바이스에 연결된 상기 디스플레이 패널로 하여금 상기 하나 이상의 제안 요소들을 제공하게 하는 단계는 상기 컴퓨팅 디바이스에 연결된 상기 디스플레이 패널로 하여금 상기 특정 제안 요소에 인접하게 상기 제1 후보 텍스트 세그먼트를 그래픽으로 표현하게 하고 상기 적어도 하나의 다른 제안 요소에 인접하게 상기 제2 후보 텍스트 세그먼트를 그래픽으로 표현하게 하는 단계를 포함한다. 상기 버전 중 일부에서, 상기 사용자가 상기 특정 제안 요소의 다른 자연어 콘텐츠와 연관된 다른 음성 발화를 제공했음을 결정하는 단계는 상기 다른 음성 발화에 기초하여, 상기 사용자가 상기 제1 후보 텍스트 세그먼트 또는 상기 제2 후보 텍스트 세그먼트를 식별했는지 여부를 결정하는 단계를 포함한다.In some implementations, performing the speech-to-text conversion processing comprises determining a first candidate text segment and a second candidate text segment, wherein the first candidate text segment and the second candidate text segment are Corresponds to different interpretations of a spoken utterance. In some of the above implementations, the specific suggestion element is determined based on the first candidate text segment, and at least one other suggestion element is determined based on the second candidate text segment. In some versions of the implementations, causing the display panel coupled to the computing device to provide the one or more suggested elements causes the display panel coupled to the computing device to be adjacent to the particular suggested element. graphically representing the text segment and graphically representing the second candidate text segment adjacent the at least one other suggested element. In some of the versions above, the determining that the user has provided another spoken utterance associated with other natural language content of the particular suggested element comprises, based on the other spoken utterance, the user selecting the first candidate text segment or the second and determining whether a candidate text segment has been identified.
일부 구현예에서, 상기 방법은 상기 음성 발화가 불완전한 것으로 결정되는 경우: 상기 음성 발화가 불완전하다고 결정함에 응답하여, 이전 액션은 상기 자동 어시스턴트를 통해 수행되고, 상기 사용자는 상기 하나 이상의 이전 인터렉션들 동안 상기 다른 자연어 콘텐츠의 적어도 일부를 식별한, 상기 사용자와 상기 자동 어시스턴트 간의 하나 이상의 이전 인터렉션들을 특징 짓는 히스토리 데이터에 기초하여 상기 다른 자연어 콘텐츠를 생성하는 단계를 더 포함하는 방법.In some implementations, the method comprises determining that the spoken utterance is incomplete: in response to determining that the spoken utterance is incomplete, a previous action is performed via the automated assistant, and wherein the user is selected during the one or more previous interactions. generating the other natural language content based on historical data characterizing one or more previous interactions between the user and the automated assistant that identified at least a portion of the other natural language content.
일부 구현예에서, 상기 방법은 상기 음성 발화가 불완전한 것으로 결정되는 경우: 상기 음성 발화가 불완전하다고 결정함에 응답하여, 상기 사용자와 연관된 다양한 디바이스들 간의 관계를 특징 짓는 디바이스 토폴로지 데이터에 기초하여 상기 다른 자연어 콘텐츠를 생성하는 단계를 더 포함하고, 상기 특정 제안 요소는 상기 디바이스 토폴로지 데이터에 기초하고, 상기 사용자와 연관된 다양한 디바이스들 중 하나 이상의 디바이스들을 식별한다.In some implementations, the method comprises: determining that the spoken utterance is incomplete: in response to determining that the spoken utterance is incomplete, the other natural language based on device topology data characterizing a relationship between various devices associated with the user. The method further comprises generating content, wherein the specific suggestion element is based on the device topology data and identifies one or more of the various devices associated with the user.
일부 구현예에서, 상기 하나 이상의 액션들이 상기 자연어 콘텐츠 및 상기 다른 음성 발화에 기초하여 상기 자동 어시스턴트를 통해 수행되게 하는 단계는 디바이스를 제어하는 단계를 포함한다. In some implementations, causing the one or more actions to be performed via the automatic assistant based on the natural language content and the other spoken utterance comprises controlling a device.
일부 구현예에서, 상기 특정 제안 요소는 액션을 나타내는 그래픽 요소를 더 포함한다.In some implementations, the specific suggestion element further comprises a graphic element representing an action.
일부 구현예에서, 상기 방법은 상기 음성 발화가 불완전한 것으로 결정되는 경우: 상기 하나 이상의 제안 요소들을 제공한 후에 다른 음성 발화를 기다리는 특정 지속시간을 결정하는 단계를 더 포함하며, 상기 특정 지속시간은 불완전한 것으로 결정되는 상기 음성 발화에 기초하여 결정된다. 이러한 구현예의 일부 버전에서, 상기 방법은 상기 음성 발화가 완전한 것으로 결정되는 경우: 상기 완전한 음성 발화에 기초하여 하나 이상의 다른 제안 요소들을 생성하는 단계; 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 다른 제안 요소들을 제공하게 하는 단계; 및 상기 하나 이상의 다른 제안 요소들을 제공한 후에 추가적 음성 발화를 기다리는 대안적 특정 지속시간을 결정하는 단계를 더 포함하며, 상기 대안적 특정 지속시간은 상기 특정 지속시간보다 짧으며, 상기 대안적 특정 지속시간은 완전한 것으로 결정되는 상기 음성 발화에 기초하여 결정된다.In some embodiments, the method further comprises when the voice utterance is determined to be incomplete: determining a specific duration to wait for another voice utterance after providing the one or more suggested elements, wherein the specific duration is incomplete. It is determined based on the voice utterance which is determined to be In some versions of this embodiment, the method further comprises: when the spoken utterance is determined to be complete: generating one or more other suggestion elements based on the complete spoken utterance; causing a display panel of the computing device to present the one or more other suggestion elements; and determining an alternative specific duration to wait for a further spoken utterance after providing the one or more other suggested elements, wherein the alternative specific duration is shorter than the specific duration, and wherein the alternative specific duration is less than the specific duration. Time is determined based on the spoken utterance determined to be complete.
일부 구현예에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되고, 자동 어시스턴트로 하여금 액션을 수행하게 하는 사용자에 의해 제공되는 음성 발화를 특징 짓는 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것을 포함한다. 음성 발화는 자연어 콘텐츠를 포함하며, 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신된다. 상기 방법은 상기 음성 발화를 특징 짓는 상기 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여, 적어도 상기 자연어 콘텐츠에 상기 액션과 연관된 기능을 제어하기 위한 하나 이상의 파라미터 값들이 없는지 여부를 결정하는 것을 포함하여, 상기 음성 발화가 완전한지 여부를 결정하는 단계를 더 포함한다. 상기 방법은 음성 발화가 불완전한 것으로 결정되는 경우 상기 음성 발화가 불완전하다고 결정함에 응답하여, 상기 컴퓨팅 디바이스에 연결된 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 하는 단계를 더 포함한다. 상기 하나 이상의 제안 요소들은 상기 사용자에 의해 상기 자동 어시스턴트 인터페이스에 발화된 경우 상기 자동 어시스턴트로 하여금 액션을 완료하도록 동작하게 하는 다른 자연어 콘텐츠를 상기 디스플레이 패널을 통해 제공하는 특정 제안 요소를 포함한다. 상기 방법은 상기 사용자가 자동 어시스턴트 인터페이스에서 수신된 다른 음성 발화를 통해 하나 이상의 제안 요소들 중 상기 특정 제안 요소를 선택했다고 결정하는 단계를 더 포함한다. 상기 방법은 상기 사용자가 상기 특정 제안 요소를 선택했다는 결정에 응답하여, 상기 특정 제안 요소 중 다른 자연어 콘텐츠 및 상기 음성 발화의 자연어 콘텐츠에 기초하여 상기 액션이 수행되게 하는 단계를 더 포함한다. 상기 방법은 상기 음성 발화가 완전한 것으로 결정되는 경우: 상기 음성 발화가 완전하다는 결정에 응답하여, 상기 음성 발화의 상기 자연어 콘텐츠에 기초하여 상기 액션이 수행되게 하는 단계를 더 포함한다.In some implementations, a method implemented by one or more processors is provided, comprising performing speech-to-text processing on data characterizing a spoken utterance provided by a user that causes an automated assistant to perform an action. . The spoken utterances include natural language content and are received via an automated assistant interface of a computing device coupled to the display panel. The method further comprises, based on performing speech-to-text conversion processing on the data characterizing the speech utterance, determining whether at least the natural language content is free of one or more parameter values for controlling a function associated with the action. , further comprising determining whether the speech utterance is complete. The method further comprises causing a display panel coupled to the computing device to provide one or more suggested elements in response to determining that the spoken utterance is incomplete if it is determined that the spoken utterance is incomplete. The one or more suggestion elements include a specific suggestion element that provides, via the display panel, other natural language content that, when uttered by the user in the automated assistant interface, causes the automated assistant to complete an action. The method further includes determining that the user has selected the particular one of the one or more suggested elements via another spoken utterance received at the automated assistant interface. The method further includes, in response to determining that the user has selected the particular suggested element, causing the action to be performed based on other natural language content of the particular suggested element and the natural language content of the spoken utterance. The method further includes when it is determined that the spoken utterance is complete: in response to determining that the spoken utterance is complete, causing the action to be performed based on the natural language content of the spoken utterance.
본 명세서에서 논의된 기술의 이들 또는 다른 구현예들은 다음 구성들 중 하나 이상을 포함할 수 있다. These or other implementations of the techniques discussed herein may include one or more of the following configurations.
일부 구현예에서, 상기 방법은 상기 음성 발화가 불완전한 것으로 결정되는 경우: 상기 사용자가 상기 특정 제안 요소를 선택했다는 결정에 응답하여, 상기 음성 발화가 불완전한 것으로 결정되기 전에, 상기 다른 자연어 콘텐츠와 연관된 우선 순위가 상기 다른 자연어 콘텐츠와 연관되었던 이전 우선 순위로부터 수정되게 하는 단계를 더 포함하는, 방법. 상기 구현예 중 일부에서, 상기 디스플레이 패널에서 적어도 하나의 다른 제안 요소와 관련하여 상기 특정 제안 요소의 제시 순서는 상기 다른 자연어 콘텐츠에 할당된 우선 순위에 적어도 부분적으로 기초한다.In some implementations, the method further comprises: when the spoken utterance is determined to be incomplete: in response to determining that the user has selected the particular suggested element, prior to determining that the spoken utterance is incomplete, associated with the other natural language content causing the ranking to be modified from a previous priority that was associated with the other natural language content. In some of the above implementations, the order of presentation of the particular suggested element in relation to the at least one other suggested element in the display panel is based, at least in part, on a priority assigned to the other natural language content.
일부 구현예에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되고, 자동 어시스턴트로 하여금 액션을 수행하게 하는 사용자에 의해 제공되는 음성 발화를 특징 짓는 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것을 포함한다. 음성 발화는 자연어 콘텐츠를 포함하며, 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신된다. 상기 방법은 상기 음성 발화를 특징 짓는 상기 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여, 적어도 상기 자연어 콘텐츠에 상기 액션과 연관된 기능을 제어하기 위한 하나 이상의 파라미터 값들이 없는지 여부를 결정하는 것을 포함하여, 상기 음성 발화가 완전한지 여부를 결정하는 단계를 더 포함한다. 상기 방법은 상기 음성 발화가 불완전한 것으로 결정되는 경우: 상기 컴퓨팅 디바이스를 통해 액세스할 수 있는 컨텍스트 데이터에 기초하여, 상기 사용자가 상기 자동 어시스턴트 인터페이스에 음성 발화를 제공한 컨텍스트를 특징 짓는 컨텍스트 데이터를 결정하는 단계; 상기 컨텍스트 데이터에 기초하여, 상기 디스플레이 패널을 통해 하나 이상의 제안들을 제시할 시간을 결정하는 단계, 상기 컨텍스트 데이터는 상기 사용자가 상기 컨텍스트에서 자동 어시스턴트에게 별도의 음성 발화를 이전에 제공했음을 나타내며; 그리고 상기 디스플레이 패널을 통해 상기 하나 이상의 제안들을 제시할 시간을 결정하는 것에 기초하여, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 하는 단계를 더 포함한다. 상기 하나 이상의 제안 요소들은 상기 사용자에 의해 상기 자동 어시스턴트 인터페이스에 발화된 경우 상기 자동 어시스턴트로 하여금 액션을 완료하도록 동작하게 하는 다른 자연어 콘텐츠를 상기 디스플레이 패널을 통해 제공하는 특정 제안 요소를 포함한다.In some implementations, a method implemented by one or more processors is provided, comprising performing speech-to-text processing on data characterizing a spoken utterance provided by a user that causes an automated assistant to perform an action. . The spoken utterances include natural language content and are received via an automated assistant interface of a computing device coupled to the display panel. The method further comprises, based on performing speech-to-text conversion processing on the data characterizing the speech utterance, determining whether at least the natural language content is free of one or more parameter values for controlling a function associated with the action. , further comprising determining whether the speech utterance is complete. The method includes determining, based on context data accessible via the computing device, context data characterizing a context in which the user provided a spoken utterance to the automated assistant interface when it is determined that the spoken utterance is incomplete. step; determining, based on the context data, a time to present one or more suggestions via the display panel, the context data indicating that the user has previously provided a separate spoken utterance to the automated assistant in the context; and based on determining a time to present the one or more suggestions via the display panel, causing the display panel of the computing device to present the one or more suggestion elements. The one or more suggestion elements include a specific suggestion element that provides, via the display panel, other natural language content that, when uttered by the user in the automated assistant interface, causes the automated assistant to complete an action.
본 명세서에서 논의된 기술의 이들 또는 다른 구현예들은 다음 구성들 중 하나 이상을 포함할 수 있다. These or other implementations of the techniques discussed herein may include one or more of the following configurations.
일부 구현예에서, 상기 하나 이상의 제안들을 제시할 시간을 결정하는 단계는 상기 컨텍스트 데이터를 상기 컨텍스트에 있는 동안 상기 사용자 또는 하나 이상의 다른 사용자가 하나 이상의 다른 음성 발화를 제공한 이전 인스턴스를 특징 짓는 다른 컨텍스트 데이터와 비교하는 단계를 포함한다.In some implementations, the determining a time to present the one or more suggestions comprises providing the context data to another context characterizing a previous instance in which the user or one or more other users provided one or more other spoken utterances while in the context. and comparing with the data.
일부 구현예에서, 상기 방법은 상기 음성 발화가 불완전한 것으로 결정되는 경우: 한 명 이상의 사용자들이 자동 어시스턴트 인터페이스에 말할 때 상기 사용자로부터 상기 음성 발화와 연관된 음성-텍스트 변환 프로세싱의 양에 비해 음성-텍스트 변환 프로세싱의 양을 감소시키고, 상기 자동 어시스턴트로 하여금 상기 액션을 완료하도록 동작하게 하는 자연어 명령을 식별하는 단계를 더 포함한다. 상기 자연어 명령은 상기 하나 이상의 제안 요소들 중 다른 제안 요소의 특정 콘텐츠에 포함된다.In some implementations, the method further comprises: when the speech utterance is determined to be incomplete: speech-to-text conversion relative to the amount of speech-to-text processing associated with the speech utterance from the user when one or more users speak to an automated assistant interface and identifying natural language commands that reduce the amount of processing and cause the automated assistant to operate to complete the action. The natural language command is included in specific content of another of the one or more suggested elements.
일부 구현예에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되고, 사용자에 의해 제공되는 음성 발화를 특징 짓는 오디오 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것을 포함한다. 음성 발화는 자연어 콘텐츠를 포함하며, 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신된다. 상기 방법은 상기 음성 발화를 특징 짓는 오디오 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여, 제1 액션이 상기 자연어 콘텐츠에 기초하여 수행되게 하는 단계를 더 포함한다. 상기 방법은 상기 사용자가 상기 자연어 콘텐츠를 포함하는 상기 음성 발화를 제공했다고 결정함에 응답하여, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 하는 단계를 더 포함한다. 상기 하나 이상의 제안 요소들은 상기 사용자에 의해 상기 자동 어시스턴트 인터페이스에 발화된 경우 상기 자동 어시스턴트로 하여금 액션을 완료하도록 동작하게 하는 다른 자연어 콘텐츠를 상기 디스플레이 패널을 통해 제공하는 특정 제안 요소를 포함한다. 상기 방법은 상기 사용자가 자동 어시스턴트 인터페이스에서 수신된 후속 음성 발화를 통해 하나 이상의 제안 요소들 중 상기 특정 제안 요소를 선택했다고 결정하는 단계; 및 상기 사용자가 상기 특정 제안 요소를 선택했다는 결정에 응답하여, 상기 특정 제안 요소에 의해 식별된 다른 자연어 콘텐츠에 기초하여 제2 액션이 수행되게 하는 단계를 포함한다.In some implementations, a method implemented by one or more processors is provided and includes performing speech-to-text conversion processing on audio data characterizing a spoken utterance provided by a user. The spoken utterances include natural language content and are received via an automated assistant interface of a computing device coupled to the display panel. The method further comprises causing a first action to be performed based on the natural language content based on performing speech-to-text conversion processing on audio data characterizing the speech utterance. The method further includes, in response to determining that the user has provided the spoken utterance comprising the natural language content, causing a display panel of the computing device to provide one or more suggestive elements. The one or more suggestion elements include a specific suggestion element that provides, via the display panel, other natural language content that, when uttered by the user in the automated assistant interface, causes the automated assistant to complete an action. The method further comprises: determining that the user has selected the particular one of one or more suggested elements via a subsequent spoken utterance received at an automated assistant interface; and in response to determining that the user has selected the particular suggested element, causing a second action to be performed based on the other natural language content identified by the particular suggested element.
본 명세서에서 논의된 기술의 이들 또는 다른 구현예들은 다음 구성들 중 하나 이상을 포함할 수 있다. These or other implementations of the techniques discussed herein may include one or more of the following configurations.
일부 구현예에서, 상기 제2 액션의 수행은 액션 데이터를 생성하며, 상기 액션 데이터는 상기 제1 액션의 수행으로부터 발생하는 다른 액션 데이터의 수정 및/또는 상기 제1 액션의 수행으로부터 발생하는 다른 액션 데이터를 보충한다. 상기 구현예들 중 일부에서, 상기 방법은 상기 사용자가 상기 음성 발화를 제공했다는 결정에 응답하여, 다른 자연어 콘텐츠를 생성하는 단계를 더 포함하며, 상기 다른 자연어 콘텐츠는 상기 제2 액션의 수행 동안 사용되는 파라미터에 대해 적어도 하나의 제안된 값을 식별한다.In some implementations, the performance of the second action generates action data, wherein the action data includes modification of other action data resulting from performance of the first action and/or other actions resulting from performance of the first action. supplement the data. In some of the above implementations, the method further comprises, in response to determining that the user provided the spoken utterance, generating other natural language content, wherein the other natural language content is used during performance of the second action. Identifies at least one suggested value for the parameter being
일부 구현예에서, 상기 제1 액션이 수행되게 하는 단계는 상기 사용자와 연관된 미리 결정된 기본 콘텐츠에 기초한 데이터가 상기 컴퓨팅 디바이스에서 제공되게 하는 단계를 포함한다. 상기 구현예 중 일부에서, 상기 방법은 상기 사용자가 상기 특정 제안 요소를 선택함에 기초하여, 상기 기본 콘텐츠를 상기 사용자로부터 후속 불완전한 요청이 수신된 때 사용 가능한 수정된 기본 콘텐츠로 수정하는 단계를 더 포함한다.In some implementations, causing the first action to be performed comprises causing data based on a predetermined base content associated with the user to be provided at the computing device. In some of the above implementations, the method further comprises, based on the user selecting the particular suggestion element, modifying the base content with modified base content available upon receipt of a subsequent incomplete request from the user. do.
일부 구현예에서, 하나 이상의 프로세서에 의해 구현되는 방법이 제공되고, 사용자에 의해 제공되는 음성 발화를 캡처하는 오디오 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여 텍스트를 생성하는 단계를 포함한다. 음성 발화는 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신된다. 상기 방법은 상기 텍스트에 기초하여, 상기 음성 발화가 완전 또는 불완전한지 여부를 결정하는 단계를 더 포함한다. 상기 방법은 음성 발화가 완전 또는 불완전한 것으로 결정되는지 여부에 기초하여 특정 지속시간을 결정하는 단계를 더 포함한다. 상기 특정 지속시간은 상기 음성 발화가 불완전하다고 결정되는 경우보다 음성 발화가 완료되었다고 결정되는 경우 더 짧다. 상기 방법은 상기 텍스트에 기초하여 하나 이상의 제안 요소들을 생성하는 단계를 더 포함하며, 상기 하나 이상의 제안 요소들은 각각 대응하는 추가 텍스트를 나타내며, 상기 추가 텍스트는 상기 텍스트와 결합되는 경우 상기 자동 어시스턴트로 하여금 대응하는 액션을 동작하게 한다. 상기 방법은 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 제안 요소들을 제공하게 하는 단계; 및 상기 하나 이상의 제안 요소들을 제공한 후, 상기 결정된 지속시간 동안 추가 음성 입력을 모니터링하는 단계를 더 포함한다. 상기 방법은 상기 지속시간 내에 상기 추가 음성 입력이 수신된 경우: 상기 추가 음성 입력을 캡처하는 추가 오디오 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하여 생성된 추가 텍스트 및 상기 후보 텍스트에 기초하여, 자동 어시스턴트 명령을 생성하는 단계; 상기 자동 어시스턴트로 하여금 상기 자동 어시스턴트 명령을 실행하게 하는 단계를 더 포함한다. 상기 방법은 상기 지속시간 내에 상기 추가 음성 입력이 수신되지 않는 경우: 상기 자동 어시스턴트로 하여금 상기 후보 텍스트에만 기초하여 대체 명령을 실행하게 하는 단계를 더 포함한다.In some implementations, a method implemented by one or more processors is provided, comprising generating text based on performing speech-to-text conversion processing on audio data that captures a spoken utterance provided by a user. . The spoken utterance is received via an automatic assistant interface of a computing device coupled to the display panel. The method further includes determining, based on the text, whether the spoken utterance is complete or incomplete. The method further includes determining a specific duration based on whether the spoken utterance is determined to be complete or incomplete. The specific duration is shorter when it is determined that the spoken utterance is complete than when it is determined that the spoken utterance is incomplete. The method further comprises generating one or more suggestion elements based on the text, each of the one or more suggestion elements indicating a corresponding additional text, the additional text causing the automatic assistant to cause the automatic assistant when combined with the text trigger the corresponding action. The method includes causing a display panel of the computing device to present the one or more suggestion elements; and after providing the one or more suggestion elements, monitoring for further voice input for the determined duration. The method includes: if the additional voice input is received within the duration: based on the additional text and the candidate text generated by performing speech-to-text conversion processing on additional audio data capturing the additional voice input, the automatic assistant generating an instruction; and causing the automated assistant to execute the automated assistant command. The method further includes if the additional speech input is not received within the duration: causing the automatic assistant to execute a replacement command based only on the candidate text.
본 명세서에서 논의된 기술의 이들 또는 다른 구현예들은 다음 구성들 중 하나 이상을 포함할 수 있다. These or other implementations of the techniques discussed herein may include one or more of the following configurations.
일부 구현예에서, 상기 음성 발화는 완전한 것으로 결정되고, 상기 음성 발화가 완전 또는 불완전한 것으로 결정되는지 여부에 기초하여 상기 특정 지속시간을 결정하는 단계는 상기 음성 발화가 완전한 것으로 결정되고, 일반 검색 에이전트가 아닌 특정 자동 어시스턴트 에이전트로 향하는 경우, 상기 특정 지속시간을 제1 특정 지속시간으로 결정하는 단계, 그리고 상기 음성 발화가 완전한 것으로 결정되고, 일반 검색 에이전트로 향하는 경우 상기 특정 지속시간을 제2 특정 지속시간으로 결정하는 단계를 포함한다. 상기 제2 특정 지속시간은 상기 제1 특정 지속시간보다 길다.In some embodiments, the spoken utterance is determined to be complete, and the determining of the specific duration based on whether the spoken utterance is determined to be complete or incomplete comprises determining that the spoken utterance is complete and the general search agent determining the specific duration as a first specific duration if directed to a specific automated assistant agent other than a specific automatic assistant agent; including the step of determining The second specific duration is longer than the first specific duration.
일부 구현예에서, 상기 음성 발화는 완전한 것으로 결정되고, 상기 음성 발화가 완전 또는 불완전한 것으로 결정되는지 여부에 기초하여 상기 특정 지속시간을 결정하는 단계는 상기 음성 발화가 완전하고 모든 필수 파라미터들을 포함하는 것으로 결정된 경우 상기 특정 지속시간을 제1 특정 지속시간으로 결정하는 단계; 및 상기 음성 발화가 완전하었지만 모든 필수 파라미터들을 포함하지 않는 것으로 결정된 경우 상기 특정 지속시간을 제2 특정 지속시간으로 결정하는 단계를 포함한다. 상기 제2 특정 지속시간은 상기 제1 특정 지속시간보다 길다.In some embodiments, the spoken utterance is determined to be complete, and determining the specific duration based on whether the spoken utterance is determined to be complete or incomplete comprises determining that the spoken utterance is complete and includes all essential parameters. determining the specific duration as a first specific duration if determined; and determining the specific duration as a second specific duration when it is determined that the speech utterance is complete but does not include all essential parameters. The second specific duration is longer than the first specific duration.
각 구현예에서, 음성 발화가 불완전하다고 결정될 수 있고, 연관된 프로세싱이 수행될 수 있다. 다른 구현에서, 음성 발화가 완전하다고 결정될 수 있고, 연관된 프로세싱이 수행될 수 있다.In each implementation, it can be determined that the spoken utterance is incomplete, and associated processing can be performed. In another implementation, the spoken utterance may be determined to be complete, and associated processing may be performed.
Claims (43)
사용자에 의해 제공된 음성 발화를 특징 짓는 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 단계, 상기 음성 발화는 자연어 콘텐츠를 포함하고 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신되며;
상기 음성 발화를 특징 짓는 상기 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여, 적어도 자동 어시스턴트가 상기 자연어 콘텐츠에 기초하여 하나 이상의 액션들이 수행되도록 할 수 있는지 여부를 결정하는 것을 포함하여, 상기 음성 발화가 완전한지 여부를 결정하는 단계;
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 음성 발화가 불완전하다고 결정함에 응답하여, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 하는 단계;
상기 하나 이상의 제안 요소들은 상기 사용자에 의해 상기 자동 어시스턴트 인터페이스에 발화된 경우 상기 자동 어시스턴트로 하여금 액션을 완료하도록 동작하게 하는 다른 자연어 콘텐츠를 상기 디스플레이 패널을 통해 제공하는 특정 제안 요소를 포함하고,
상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 한 후, 상기 사용자가 상기 특정 제안 요소의 다른 자연어 콘텐츠와 연관된 다른 음성 발화를 제공했음을 결정하는 단계,
상기 사용자가 다른 음성 발화를 제공했다는 결정에 응답하여, 상기 음성 발화와 상기 다른 음성 발화의 조합이 완전한지 여부를 결정하는 단계, 그리고
상기 음성 발화와 상기 다른 음성 발화의 조합이 완전한 것으로 결정된 경우:
상기 하나 이상의 액션들이 상기 자연어 콘텐츠 및 상기 다른 음성 발화에 기초하여 자동 어시스턴트를 통해 수행되게 하는 단계를 포함하는, 방법.A method performed by one or more processors, comprising:
performing speech-to-text conversion processing on data characterizing a speech utterance provided by a user, wherein the speech utterance includes natural language content and is received via an automatic assistant interface of a computing device coupled to the display panel;
based on performing speech-to-text conversion processing on the data characterizing the speech utterance, determining whether at least an automated assistant can cause one or more actions to be performed based on the natural language content; determining whether the spoken utterance is complete;
If the spoken utterance is determined to be incomplete:
in response to determining that the spoken utterance is incomplete, causing a display panel of the computing device to provide one or more suggested elements;
wherein the one or more suggestion elements include a specific suggestion element that provides, via the display panel, other natural language content that, when uttered by the user to the automated assistant interface, causes the automated assistant to complete an action;
after causing the display panel of the computing device to provide one or more suggested elements, determining that the user has provided another spoken utterance associated with other natural language content of the particular suggested element;
in response to determining that the user provided another spoken utterance, determining whether the combination of the spoken utterance and the other spoken utterance is complete; and
When it is determined that the combination of the spoken utterance and the other spoken utterance is complete:
causing the one or more actions to be performed via an automated assistant based on the natural language content and the other spoken utterance.
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 후보 텍스트 세그먼트들 중 적어도 하나의 후보 텍스트 세그먼트의 그래픽 표현을 제공하게 하는 단계를 더 포함하는, 방법.The method of claim 1 , wherein performing speech-to-text conversion processing on data characterizing a spoken utterance provided by the user comprises generating one or more candidate text segments from the data:
If the spoken utterance is determined to be incomplete:
causing a display panel of the computing device to provide a graphical representation of at least one of the one or more candidate text segments.
상기 음성 발화가 완전하고 상기 하나 이상의 액션들에 대한 모든 필수 파라미터들을 포함하는 것으로 결정되는 경우:
상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 후보 텍스트 세그먼트들 중 적어도 하나의 후보 텍스트 세그먼트의 그래픽 표현을 제공하는 것을 우회하게 하고, 상기 하나 이상의 제안 요소들을 제공하는 것을 우회하게 하는 단계를 포함하는, 방법.3. The method according to claim 2,
If it is determined that the spoken utterance is complete and includes all required parameters for the one or more actions:
causing a display panel of the computing device to bypass providing a graphical representation of at least one of the one or more candidate text segments and bypass providing the one or more suggestion elements; Way.
상기 음성 발화가 완전한 것으로 결정되는 경우:
한 명 이상의 사용자들이 자동 어시스턴트 인터페이스에 말할 때 상기 사용자로부터 상기 음성 발화와 연관된 음성-텍스트 변환 프로세싱의 양에 비해 음성-텍스트 변환 프로세싱의 양을 감소시키고, 상기 자동 어시스턴트로 하여금 상기 액션을 완료하도록 동작하게 하는 자연어 명령을 식별하는 단계;
상기 자연어 명령을 식별하는 것에 기초하여, 상기 자연어 명령을 특징 짓는 하나 이상의 다른 제안 요소들을 생성하는 단계; 및
적어도 상기 음성 발화가 완전하다고 결정함에 응답하여, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 다른 제안 요소들을 제공하게 하는 단계를 더 포함하는, 방법.4. The method of claim 2 or 3,
If the spoken utterance is determined to be complete:
reduce an amount of speech-to-text conversion processing relative to an amount of speech-to-text conversion processing associated with the speech utterance from the user when one or more users speak to an automatic assistant interface, and cause the automatic assistant to complete the action identifying a natural language instruction that causes
generating, based on identifying the natural language command, one or more other suggestion elements characterizing the natural language command; and
in response to determining that at least the spoken utterance is complete, causing a display panel of the computing device to present the one or more other suggestion elements.
상기 음성 발화가 불완전한 것으로 결정되는 경우:
적어도 하나의 제안 요소가 상기 디스플레이 패널을 통해 제공될 때 상기 사용자로부터 예상되는 음성 발화의 유형에 기초하여, 다수의 상이한 음성-텍스트 변환 프로세싱 모델들로부터 음성-텍스트 변환 프로세싱 모델을 선택하는 단계를 더 포함하는, 방법.In any preceding claim,
If the spoken utterance is determined to be incomplete:
selecting a speech-to-text conversion processing model from a plurality of different speech-to-text conversion processing models based on the type of speech utterance expected from the user when at least one suggestion element is provided via the display panel. Including method.
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 하나 이상의 제안 요소들 중 하나 이상의 용어들을 향해 및/또는 상기 하나 이상의 제안 요소들에 대응하는 콘텐츠의 하나 이상의 예상된 유형들 향해 음성-텍스트 변환 프로세싱을 편향하는 단계를 더 포함하는, 방법.In any preceding claim,
If the spoken utterance is determined to be incomplete:
biasing speech-to-text conversion processing towards terms of one or more of the one or more suggested elements and/or toward one or more expected types of content corresponding to the one or more suggested elements.
음성 입력 무음의 임계 지속시간이 상기 음성 발화에 후속하는지 여부를 결정하는 단계를 더 포함하고, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 제안 요소들을 제공하게 하는 것은 적어도 상기 음성 입력 무음의 임계 지속시간이 상기 음성 발화에 후속한다고 결정함에 응답하여 이루어지는, 방법.In any preceding claim,
further comprising determining whether a threshold duration of speech input silence follows the speech utterance, wherein causing a display panel of the computing device to provide the one or more suggested elements is at least the threshold duration of speech input silence in response to determining that a time follows the spoken utterance.
상기 음성-텍스트 변환 프로세싱을 수행하는 단계는 제1 후보 텍스트 세그먼트 및 제2 후보 텍스트 세그먼트를 결정하는 단계를 포함하고, 상기 제1 후보 텍스트 세그먼트 및 상기 제2 후보 텍스트 세그먼트는 상기 음성 발화의 상이한 해석들에 대응하고, 그리고
상기 특정 제안 요소는 상기 제1 후보 텍스트 세그먼트에 기초하여 결정되고, 적어도 하나의 다른 제안 요소는 상기 제2 후보 텍스트 세그먼트에 기초하여 결정되는, 방법.8. The method according to any one of claims 1 to 7,
Performing the speech-to-text conversion processing includes determining a first candidate text segment and a second candidate text segment, wherein the first candidate text segment and the second candidate text segment are different interpretations of the speech utterance. respond to them, and
wherein the specific suggestion element is determined based on the first candidate text segment and at least one other suggestion element is determined based on the second candidate text segment.
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 음성 발화가 불완전하다고 결정함에 응답하여, 이전 액션은 상기 자동 어시스턴트를 통해 수행되고, 상기 사용자는 상기 하나 이상의 이전 인터렉션들 동안 상기 다른 자연어 콘텐츠의 적어도 일부를 식별한, 상기 사용자와 상기 자동 어시스턴트 간의 하나 이상의 이전 인터렉션들을 특징 짓는 히스토리 데이터에 기초하여 상기 다른 자연어 콘텐츠를 생성하는 단계를 더 포함하는 방법.11. The method according to any one of claims 1 to 10,
If the spoken utterance is determined to be incomplete:
In response to determining that the spoken utterance is incomplete, a prior action is performed via the automated assistant, wherein the user has identified at least a portion of the other natural language content during the one or more previous interactions between the user and the automated assistant. The method further comprising generating the other natural language content based on historical data characterizing one or more previous interactions.
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 음성 발화가 불완전하다고 결정함에 응답하여, 상기 사용자와 연관된 다양한 디바이스들 간의 관계를 특징 짓는 디바이스 토폴로지 데이터에 기초하여 상기 다른 자연어 콘텐츠를 생성하는 단계를 더 포함하고,
상기 특정 제안 요소는 상기 디바이스 토폴로지 데이터에 기초하고, 상기 사용자와 연관된 다양한 디바이스들 중 하나 이상의 디바이스들을 식별하는, 방법.11. The method according to any one of claims 1 to 10,
If the spoken utterance is determined to be incomplete:
in response to determining that the spoken utterance is incomplete, generating the other natural language content based on device topology data characterizing relationships between various devices associated with the user;
wherein the particular suggestion element is based on the device topology data and identifies one or more of the various devices associated with the user.
상기 하나 이상의 제안 요소들을 제공한 후에 다른 음성 발화를 기다리는 특정 지속시간을 결정하는 단계를 더 포함하며, 상기 특정 지속시간은 불완전한 것으로 결정되는 상기 음성 발화에 기초하여 결정되는, 방법.14. The method of any one of claims 1 to 13, wherein when the spoken utterance is determined to be incomplete:
determining a specific duration to wait for another spoken utterance after providing the one or more suggestion elements, wherein the specific duration is determined based on the voice utterance determined to be incomplete.
상기 완전한 음성 발화에 기초하여 하나 이상의 다른 제안 요소들을 생성하는 단계;
상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 다른 제안 요소들을 제공하게 하는 단계; 및
상기 하나 이상의 다른 제안 요소들을 제공한 후에 추가적 음성 발화를 기다리는 대안적 특정 지속시간을 결정하는 단계를 더 포함하며, 상기 대안적 특정 지속시간은 상기 특정 지속시간보다 짧으며, 상기 대안적 특정 지속시간은 완전한 것으로 결정되는 상기 음성 발화에 기초하여 결정되는, 방법.16. The method of claim 15, wherein when the spoken utterance is determined to be complete:
generating one or more other suggestion elements based on the complete speech utterance;
causing a display panel of the computing device to present the one or more other suggestion elements; and
determining an alternative specific duration to wait for a further spoken utterance after providing the one or more other suggested elements, wherein the alternative specific duration is less than the specific duration, and wherein the alternative specific duration is less than the specific duration. is determined based on the spoken utterance that is determined to be complete.
자동 어시스턴트가 액션을 수행하게 한 후, 사용자에 의해 제공된 음성 발화를 특징 짓는 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 단계, 상기 음성 발화는 자연어 콘텐츠를 포함하고 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신되며;
상기 음성 발화를 특징 짓는 상기 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여, 적어도 상기 자연어 콘텐츠에 상기 액션과 연관된 기능을 제어하기 위한 하나 이상의 파라미터 값들이 없는지 여부를 결정하는 것을 포함하여, 상기 음성 발화가 완전한지 여부를 결정하는 단계;
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 음성 발화가 불완전하다고 결정함에 응답하여, 상기 컴퓨팅 디바이스에 연결된 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 하는 단계;
상기 하나 이상의 제안 요소들은 상기 사용자에 의해 상기 자동 어시스턴트 인터페이스에 발화된 경우 상기 자동 어시스턴트로 하여금 상기 액션을 완료하도록 동작하게 하는 다른 자연어 콘텐츠를 상기 디스플레이 패널을 통해 제공하는 특정 제안 요소를 포함하고,
상기 사용자가 자동 어시스턴트 인터페이스에서 수신된 다른 음성 발화를 통해 하나 이상의 제안 요소들 중 상기 특정 제안 요소를 선택했다고 결정하는 단계, 그리고
상기 사용자가 상기 특정 제안 요소를 선택했다는 결정에 응답하여, 상기 특정 제안 요소 중 다른 자연어 콘텐츠 및 상기 음성 발화의 자연어 콘텐츠에 기초하여 상기 액션이 수행되게 하는 단계; 및
상기 음성 발화가 완전한 것으로 결정되는 경우:
상기 음성 발화가 완전하다는 결정에 응답하여, 상기 음성 발화의 상기 자연어 콘텐츠에 기초하여 상기 액션이 수행되게 하는 단계를 포함하는, 방법.A method performed by one or more processors, comprising:
after causing the automated assistant to perform the action, performing speech-to-text conversion processing on data characterizing a spoken utterance provided by the user, the voice utterance comprising natural language content and connected to the display panel by an automated assistant of a computing device received via the interface;
determining, based on performing speech-to-text conversion processing on the data characterizing the speech utterance, whether at least the natural language content is free of one or more parameter values for controlling a function associated with the action, determining whether the speech utterance is complete;
If the spoken utterance is determined to be incomplete:
in response to determining that the spoken utterance is incomplete, causing a display panel coupled to the computing device to provide one or more suggested elements;
wherein the one or more suggestion elements include a specific suggestion element that provides, via the display panel, other natural language content that, when uttered to the automated assistant interface by the user, causes the automated assistant to complete the action;
determining that the user has selected the particular one of the one or more suggested elements via another spoken utterance received at the automated assistant interface; and
in response to determining that the user has selected the particular suggested element, causing the action to be performed based on other natural language content of the particular suggested element and the natural language content of the spoken utterance; and
If the spoken utterance is determined to be complete:
in response to determining that the spoken utterance is complete, causing the action to be performed based on the natural language content of the spoken utterance.
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 사용자가 상기 특정 제안 요소를 선택했다는 결정에 응답하여, 상기 음성 발화가 불완전한 것으로 결정되기 전에, 상기 다른 자연어 콘텐츠와 연관된 우선 순위가 상기 다른 자연어 콘텐츠와 연관되었던 이전 우선 순위로부터 수정되게 하는 단계를 더 포함하는, 방법.18. The method of claim 17,
If the spoken utterance is determined to be incomplete:
in response to determining that the user has selected the particular suggested element, before the spoken utterance is determined to be incomplete, causing a priority associated with the other natural language content to be modified from a previous priority associated with the other natural language content; further comprising the method.
음성 입력 무음의 임계 지속시간이 상기 음성 발화에 후속하는지 여부를 결정하는 단계를 더 포함하고, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 제안 요소들을 제공하게 하는 것은 적어도 상기 음성 입력 무음의 임계 지속시간이 상기 음성 발화에 후속한다고 결정함에 응답하여 이루어지는, 방법.20. The method according to any one of claims 17 to 19,
further comprising determining whether a threshold duration of speech input silence follows the speech utterance, wherein causing a display panel of the computing device to provide the one or more suggested elements is at least the threshold duration of speech input silence in response to determining that a time follows the spoken utterance.
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 음성 발화가 불완전하다고 결정함에 응답하여, 이전 액션은 상기 자동 어시스턴트를 통해 수행되고, 상기 사용자는 상기 하나 이상의 이전 인터렉션들 동안 상기 다른 자연어 콘텐츠의 적어도 일부를 식별한, 상기 사용자와 상기 자동 어시스턴트 간의 하나 이상의 이전 인터렉션들을 특징 짓는 히스토리 데이터에 기초하여 상기 다른 자연어 콘텐츠를 생성하는 단계를 더 포함하는 방법.21. The method according to any one of claims 17 to 20,
If the spoken utterance is determined to be incomplete:
In response to determining that the spoken utterance is incomplete, a prior action is performed via the automated assistant, wherein the user has identified at least a portion of the other natural language content during the one or more previous interactions between the user and the automated assistant. The method further comprising generating the other natural language content based on historical data characterizing one or more previous interactions.
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 음성 발화가 불완전했다고 결정함에 응답하여, 상기 사용자와 연관된 다양한 디바이스들 간의 관계를 특징 짓는 디바이스 토폴로지 데이터에 기초하여 상기 다른 자연어 콘텐츠를 생성하는 단계를 더 포함하고,
상기 특정 제안 요소는 상기 디바이스 토폴로지 데이터에 기초하고, 상기 사용자와 연관된 다양한 디바이스들 중 하나 이상의 디바이스들을 식별하는, 방법.21. The method according to any one of claims 17 to 20,
If the spoken utterance is determined to be incomplete:
in response to determining that the spoken utterance was incomplete, generating the other natural language content based on device topology data characterizing relationships between various devices associated with the user;
wherein the particular suggestion element is based on the device topology data and identifies one or more of the various devices associated with the user.
상기 음성-텍스트 변환 프로세싱을 수행하는 단계는 제1 후보 텍스트 세그먼트 및 제2 후보 텍스트 세그먼트를 결정하는 단계를 포함하고, 상기 제1 후보 텍스트 세그먼트 및 상기 제2 후보 텍스트 세그먼트는 상기 음성 발화의 상이한 해석들에 대응하고, 그리고
상기 하나 이상의 제안 요소들 중 적어도 하나의 제안 요소는 상기 제1 후보 텍스트 세그먼트에 기초하여 결정되고, 적어도 하나의 다른 제안 요소는 상기 제2 후보 텍스트 세그먼트에 기초하여 결정되는, 방법.23. The method according to any one of claims 17 to 22,
Performing the speech-to-text conversion processing includes determining a first candidate text segment and a second candidate text segment, wherein the first candidate text segment and the second candidate text segment are different interpretations of the speech utterance. respond to them, and
at least one suggestion element of the one or more suggestion elements is determined based on the first candidate text segment, and at least one other suggestion element is determined based on the second candidate text segment.
자동 어시스턴트가 액션을 수행하게 한 후, 사용자에 의해 제공된 음성 발화를 특징 짓는 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 단계, 상기 음성 발화는 자연어 콘텐츠를 포함하고 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신되며;
상기 음성 발화를 특징 짓는 상기 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여, 적어도 상기 자연어 콘텐츠에 상기 액션과 연관된 기능을 제어하기 위한 하나 이상의 파라미터 값들이 없는지 여부를 결정하는 것을 포함하여, 상기 음성 발화가 완전한지 여부를 결정하는 단계; 및
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 컴퓨팅 디바이스를 통해 액세스할 수 있는 컨텍스트 데이터에 기초하여, 상기 사용자가 상기 자동 어시스턴트 인터페이스에 음성 발화를 제공한 컨텍스트를 특징 짓는 컨텍스트 데이터를 결정하는 단계,
상기 컨텍스트 데이터에 기초하여, 상기 디스플레이 패널을 통해 하나 이상의 제안들을 제시할 시간을 결정하는 단계, 상기 컨텍스트 데이터는 상기 사용자가 상기 컨텍스트에서 자동 어시스턴트에게 별도의 음성 발화를 이전에 제공했음을 나타내며, 그리고
상기 디스플레이 패널을 통해 상기 하나 이상의 제안들을 제시할 시간을 결정하는 것에 기초하여, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 하는 단계를 포함하며,
상기 하나 이상의 제안 요소들은 상기 사용자에 의해 상기 자동 어시스턴트 인터페이스에 발화된 경우 상기 자동 어시스턴트로 하여금 상기 액션을 완료하도록 동작하게 하는 다른 자연어 콘텐츠를 상기 디스플레이 패널을 통해 제공하는 특정 제안 요소를 포함하는, 방법.A method performed by one or more processors, comprising:
after causing the automated assistant to perform the action, performing speech-to-text conversion processing on data characterizing a spoken utterance provided by the user, the voice utterance comprising natural language content and connected to the display panel by an automated assistant of a computing device received via the interface;
determining, based on performing speech-to-text conversion processing on the data characterizing the speech utterance, whether at least the natural language content is free of one or more parameter values for controlling a function associated with the action, determining whether the speech utterance is complete; and
If the spoken utterance is determined to be incomplete:
determining, based on context data accessible via the computing device, context data characterizing a context in which the user provided a spoken utterance to the automated assistant interface;
determining, based on the context data, a time to present one or more suggestions via the display panel, the context data indicating that the user has previously provided a separate spoken utterance to the automated assistant in the context, and
based on determining a time to present the one or more suggestions via the display panel, causing the display panel of the computing device to present the one or more suggestion elements;
wherein the one or more suggestion elements include a specific suggestion element that provides, via the display panel, other natural language content that, when uttered in the automated assistant interface by the user, causes the automated assistant to complete the action. .
상기 하나 이상의 제안들을 제시할 시간을 결정하는 단계는 상기 컨텍스트 데이터를 상기 컨텍스트에 있는 동안 상기 사용자 또는 하나 이상의 다른 사용자가 하나 이상의 다른 음성 발화를 제공한 이전 인스턴스를 특징 짓는 다른 컨텍스트 데이터와 비교하는 단계를 포함하는, 방법.26. The method of claim 25,
Determining the time to present the one or more suggestions may include comparing the context data to other context data characterizing a previous instance in which the user or one or more other users provided one or more other spoken utterances while in the context. A method comprising
상기 음성 발화가 불완전한 것으로 결정되는 경우:
한 명 이상의 사용자들이 자동 어시스턴트 인터페이스에 말할 때 상기 사용자로부터 상기 음성 발화와 연관된 음성-텍스트 변환 프로세싱의 양에 비해 음성-텍스트 변환 프로세싱의 양을 감소시키고, 상기 자동 어시스턴트로 하여금 상기 액션을 완료하도록 동작하게 하는 자연어 명령을 식별하는 단계를 더 포함하며,
상기 자연어 명령은 상기 하나 이상의 제안 요소들 중 다른 제안 요소의 특정 콘텐츠에 포함되는, 방법.27. The method of claim 25 or 26,
If the spoken utterance is determined to be incomplete:
reduce an amount of speech-to-text conversion processing relative to an amount of speech-to-text conversion processing associated with the speech utterance from the user when one or more users speak to an automatic assistant interface, and cause the automatic assistant to complete the action further comprising the step of identifying a natural language command that causes
wherein the natural language command is included in specific content of another of the one or more suggested elements.
상기 다른 자연어 콘텐츠는 이전에 적어도 상기 액션 또는 적어도 다른 액션이 상기 자동 어시스턴트를 통해 수행되었던, 하나 이상의 사용자들과 자동 어시스턴트 사이의 이전 인터렉션에 기초하며, 그리고
상기 액션 또는 상기 다른 액션은 상기 컴퓨팅 디바이스 또는 별도의 컴퓨팅 디바이스에서 각각 기능 또는 다른 기능의 실행에 대응하는, 방법.28. The method according to any one of claims 25 to 27,
the other natural language content is based on a previous interaction between the automated assistant and one or more users, wherein at least the action or at least another action was previously performed via the automated assistant; and
wherein the action or the other action corresponds to the execution of a function or other function, respectively, on the computing device or a separate computing device.
상기 음성 발화가 불완전한 것으로 결정되는 경우:
상기 디스플레이 패널을 통해 제공된 상기 하나 이상의 제안 요소들 중 적어도 하나의 제안 요소에 기초하여, 상기 적어도 하나의 제안 요소가 상기 디스플레이 패널을 통해 제공될 때 상기 사용자로부터 예상되는 음성 발화의 유형에 기초하여, 다수의 상이한 음성-텍스트 변환 프로세싱 모델들로부터 음성-텍스트 변환 프로세싱 모델을 선택하는 단계를 더 포함하는, 방법.29. The method according to any one of claims 25 to 28,
If the spoken utterance is determined to be incomplete:
Based on at least one suggestion element among the one or more suggested elements provided through the display panel, based on the type of speech utterance expected from the user when the at least one suggestion element is provided through the display panel, selecting a speech-to-text conversion processing model from a plurality of different speech-to-text conversion processing models.
상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 제안 요소들을 제공하게 한 후, 상기 하나 이상의 제안들에 기초하여 상기 발화를 완성하는 사용자 입력을 수신하는 단계; 및
상기 자동 어시스턴트로 하여금 상기 액션을 완료하도록 동작하게 하는 단계를 더 포함하는, 방법.30. The method according to any one of claims 25 to 29,
after causing the display panel of the computing device to provide the one or more suggestion elements, receiving a user input for completing the utterance based on the one or more suggestions; and
and causing the automated assistant to operate to complete the action.
사용자에 의해 제공된 음성 발화를 특징 짓는 오디오 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 단계, 상기 음성 발화는 자연어 콘텐츠를 포함하고 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신되며;
상기 음성 발화를 특징 짓는 오디오 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여, 제1 액션이 상기 자연어 콘텐츠에 기초하여 수행되게 하는 단계;
상기 사용자가 상기 자연어 콘텐츠를 포함하는 상기 음성 발화를 제공했다고 결정함에 응답하여, 상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 하나 이상의 제안 요소들을 제공하게 하는 단계,
상기 하나 이상의 제안 요소들은 상기 사용자에 의해 상기 자동 어시스턴트 인터페이스에 발화된 경우 상기 자동 어시스턴트로 하여금 상기 액션을 완료하도록 동작하게 하는 다른 자연어 콘텐츠를 상기 디스플레이 패널을 통해 제공하는 특정 제안 요소를 포함하며;
상기 사용자가 자동 어시스턴트 인터페이스에서 수신된 후속 음성 발화를 통해 하나 이상의 제안 요소들 중 상기 특정 제안 요소를 선택했다고 결정하는 단계; 및
상기 사용자가 상기 특정 제안 요소를 선택했다는 결정에 응답하여, 상기 특정 제안 요소에 의해 식별된 다른 자연어 콘텐츠에 기초하여 제2 액션이 수행되게 하는 단계를 포함하는, 방법.A method performed by one or more processors, comprising:
performing speech-to-text conversion processing on audio data characterizing a speech utterance provided by a user, wherein the speech utterance includes natural language content and is received via an automatic assistant interface of a computing device coupled to the display panel;
based on performing speech-to-text conversion processing on audio data characterizing the speech utterance, causing a first action to be performed based on the natural language content;
in response to determining that the user has provided the spoken utterance comprising the natural language content, causing a display panel of the computing device to present one or more suggested elements;
the one or more suggestion elements include a specific suggestion element that provides, via the display panel, other natural language content that, when uttered to the automated assistant interface by the user, causes the automated assistant to complete the action;
determining that the user has selected the particular one of the one or more suggested elements via a subsequent voice utterance received at the automated assistant interface; and
in response to determining that the user has selected the particular suggested element, causing a second action to be performed based on other natural language content identified by the particular suggested element.
상기 사용자가 상기 음성 발화를 제공했다는 결정에 응답하여, 다른 자연어 콘텐츠를 생성하는 단계를 더 포함하며, 상기 다른 자연어 콘텐츠는 상기 제2 액션의 수행 동안 사용되는 파라미터에 대해 적어도 하나의 제안된 값을 식별하는, 방법.33. The method of claim 32,
in response to determining that the user provided the spoken utterance, generating another natural language content, wherein the other natural language content includes at least one suggested value for a parameter used during performance of the second action. How to identify.
상기 사용자가 상기 하나 이상의 제안 요소들 중 상기 특정 제안 요소를 선택했다는 결정에 응답하여, 상기 하나 이상의 제안 요소들 중 각 제안 요소와 연관된 우선 순위를 수정하는 단계를 더 포함하며,
각각의 우선 순위는 상기 자동 어시스턴트가 상기 액션을 수행하게 하기 위한 후속 요청에 응답하여, 각각의 대응하는 제안 요소가 상기 디스플레이 패널에 제시될 것인지 여부를 나타내는, 방법.34. The method according to any one of claims 31 to 33,
in response to determining that the user has selected the particular one of the one or more suggested elements, modifying a priority associated with each one of the one or more suggested elements;
each priority indicates whether a respective corresponding suggestion element will be presented on the display panel in response to a subsequent request for the automated assistant to perform the action.
상기 사용자가 상기 특정 제안 요소를 선택함에 기초하여, 상기 기본 콘텐츠를 상기 사용자로부터 후속 불완전한 요청이 수신된 때 사용 가능한 수정된 기본 콘텐츠로 수정하는 단계를 더 포함하는, 방법.36. The method of claim 35,
based on the user selecting the particular suggestion element, modifying the base content to modified base content available upon receipt of a subsequent incomplete request from the user.
사용자에 의해 제공되는 음성 발화를 캡처하는 오디오 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하는 것에 기초하여 텍스트를 생성하는 단계, 상기 음성 발화는 디스플레이 패널에 연결된 컴퓨팅 디바이스의 자동 어시스턴트 인터페이스를 통해 수신되며;
상기 텍스트에 기초하여, 상기 음성 발화가 완전 또는 불완전한지 여부를 결정하는 단계;
상기 음성 발화가 완전 또는 불완전한 것으로 결정되는지 여부에 기초하여, 특정 지속시간을 결정하는 단계, 상기 특정 지속시간은 상기 음성 발화가 완전한 것으로 결정되는 경우 상기 음성 발화가 불완전한 것으로 결정될 때 보다 더 짧으며;
상기 텍스트에 기초하여 하나 이상의 제안 요소들을 생성하는 단계, 상기 하나 이상의 제안 요소들은 각각 대응하는 추가 텍스트를 나타내며, 상기 추가 텍스트는 상기 텍스트와 결합되는 경우 상기 자동 어시스턴트로 하여금 대응하는 액션을 동작하게 하며;
상기 컴퓨팅 디바이스의 디스플레이 패널로 하여금 상기 하나 이상의 제안 요소들을 제공하게 하는 단계;
상기 하나 이상의 제안 요소들을 제공한 후, 상기 결정된 지속시간 동안 추가 음성 입력을 모니터링하는 단계;
상기 지속시간 내에 상기 추가 음성 입력이 수신된 경우:
상기 추가 음성 입력을 캡처하는 추가 오디오 데이터에 대해 음성-텍스트 변환 프로세싱을 수행하여 생성된 추가 텍스트 및 상기 후보 텍스트에 기초하여, 자동 어시스턴트 명령을 생성하는 단계;
상기 자동 어시스턴트로 하여금 상기 자동 어시스턴트 명령을 실행하게 하는 단계;
상기 지속시간 내에 상기 추가 음성 입력이 수신되지 않는 경우:
상기 자동 어시스턴트로 하여금 상기 후보 텍스트에만 기초하여 대체 명령을 실행하게 하는 단계를 포함하는, 방법.A method performed by one or more processors, comprising:
generating text based on performing speech-to-text conversion processing on audio data that captures a speech utterance provided by a user, the speech utterance being received via an automatic assistant interface of a computing device coupled to the display panel;
determining whether the spoken utterance is complete or incomplete based on the text;
determining a specific duration based on whether the voiced utterance is determined to be complete or incomplete, wherein the specific duration is shorter when the voiced utterance is determined to be complete than when the voiced utterance is determined to be incomplete;
generating one or more suggestion elements based on the text, each of the one or more suggestion elements representing a corresponding additional text, wherein the additional text, when combined with the text, causes the automatic assistant to perform a corresponding action; ;
causing a display panel of the computing device to present the one or more suggested elements;
after providing the one or more suggested elements, monitoring for further voice input for the determined duration;
If the additional voice input is received within the duration:
performing speech-to-text conversion processing on the additional audio data that captures the additional voice input to generate an automatic assistant command based on the additional text and the candidate text;
causing the automated assistant to execute the automated assistant command;
If the additional voice input is not received within the duration:
causing the automated assistant to execute a replacement instruction based solely on the candidate text.
상기 음성 발화가 완전한 것으로 결정되고, 일반 검색 에이전트가 아닌 특정 자동 어시스턴트 에이전트로 향하는 경우, 상기 특정 지속시간을 제1 특정 지속시간으로 결정하는 단계, 그리고
상기 음성 발화가 완전한 것으로 결정되고, 일반 검색 에이전트로 향하는 경우 상기 특정 지속시간을 제2 특정 지속시간으로 결정하는 단계를 포함하며, 상기 제2 특정 지속시간은 상기 제1 특정 지속시간보다 긴, 방법.39. The method of claim 38, wherein the spoken utterance is determined to be complete, and wherein determining the specific duration based on whether the spoken utterance is determined to be complete or incomplete comprises:
if the spoken utterance is determined to be complete and is directed to a specific automated assistant agent rather than a generic search agent, determining the specific duration as a first specific duration; and
determining the specific duration as a second specific duration when the spoken utterance is determined to be complete and directed to a generic search agent, wherein the second specific duration is longer than the first specific duration. .
상기 음성 발화가 완전하고 모든 필수 파라미터들을 포함하는 것으로 결정된 경우 상기 특정 지속시간을 제1 특정 지속시간으로 결정하는 단계, 및
상기 음성 발화가 완전하었지만 모든 필수 파라미터들을 포함하지 않는 것으로 결정된 경우 상기 특정 지속시간을 제2 특정 지속시간으로 결정하는 단계를 포함하며, 상기 제2 특정 지속시간은 상기 제1 특정 지속시간보다 긴, 방법.39. The method of claim 38, wherein the spoken utterance is determined to be complete, and wherein determining the specific duration based on whether the spoken utterance is determined to be complete or incomplete comprises:
determining the specific duration as a first specific duration when it is determined that the spoken utterance is complete and includes all required parameters; and
determining the specific duration as a second specific duration when it is determined that the spoken utterance is complete but does not include all essential parameters, wherein the second specific duration is longer than the first specific duration , Way.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862785842P | 2018-12-28 | 2018-12-28 | |
US62/785,842 | 2018-12-28 | ||
PCT/US2019/017043 WO2020139408A1 (en) | 2018-12-28 | 2019-02-07 | Supplementing voice inputs to an automated assistant according to selected suggestions |
Publications (1)
Publication Number | Publication Date |
---|---|
KR20210110650A true KR20210110650A (en) | 2021-09-08 |
Family
ID=65494656
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020217023853A KR20210110650A (en) | 2018-12-28 | 2019-02-07 | Supplement your automatic assistant with voice input based on selected suggestions |
Country Status (6)
Country | Link |
---|---|
US (2) | US11238857B2 (en) |
EP (2) | EP4250287A3 (en) |
JP (2) | JP7286772B2 (en) |
KR (1) | KR20210110650A (en) |
CN (1) | CN113330512A (en) |
WO (1) | WO2020139408A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2024085592A1 (en) * | 2022-10-19 | 2024-04-25 | 삼성전자 주식회사 | Electronic device and speech assistant service providing method of electronic device |
Families Citing this family (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
CN113330512A (en) * | 2018-12-28 | 2021-08-31 | 谷歌有限责任公司 | Supplementing an automated assistant with speech input according to a selected suggestion |
US11238868B2 (en) * | 2019-05-06 | 2022-02-01 | Google Llc | Initializing non-assistant background actions, via an automated assistant, while accessing a non-assistant application |
KR20200129346A (en) * | 2019-05-08 | 2020-11-18 | 삼성전자주식회사 | Display apparatus and method for controlling thereof |
US11749265B2 (en) * | 2019-10-04 | 2023-09-05 | Disney Enterprises, Inc. | Techniques for incremental computer-based natural language understanding |
KR20210050901A (en) * | 2019-10-29 | 2021-05-10 | 엘지전자 주식회사 | Voice recognition method and device |
US11410653B1 (en) * | 2020-09-25 | 2022-08-09 | Amazon Technologies, Inc. | Generating content recommendation based on user-device dialogue |
EP4064279A4 (en) * | 2021-01-28 | 2022-12-21 | Samsung Electronics Co., Ltd. | Device and method for providing recommended sentences related to user's speech input |
US11955137B2 (en) * | 2021-03-11 | 2024-04-09 | Apple Inc. | Continuous dialog with a digital assistant |
US20230074406A1 (en) * | 2021-09-07 | 2023-03-09 | Google Llc | Using large language model(s) in generating automated assistant response(s |
KR20230045333A (en) * | 2021-09-28 | 2023-04-04 | 삼성전자주식회사 | Electronic device and operation method thereof |
EP4248304A1 (en) | 2022-02-09 | 2023-09-27 | Google LLC | Providing contextual automated assistant action suggestion(s) via a vehicle computing device |
US20230386467A1 (en) * | 2022-04-24 | 2023-11-30 | Discern Science International, Inc. | Distributed discernment system |
US11880645B2 (en) | 2022-06-15 | 2024-01-23 | T-Mobile Usa, Inc. | Generating encoded text based on spoken utterances using machine learning systems and methods |
Family Cites Families (47)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2001034292A (en) | 1999-07-26 | 2001-02-09 | Denso Corp | Word string recognizing device |
US7949536B2 (en) * | 2006-08-31 | 2011-05-24 | Microsoft Corporation | Intelligent speech recognition of incomplete phrases |
US9318108B2 (en) * | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8027964B2 (en) | 2007-07-13 | 2011-09-27 | Medio Systems, Inc. | Personalized query completion suggestion |
US8126715B2 (en) * | 2008-11-26 | 2012-02-28 | Microsoft Corporation | Facilitating multimodal interaction with grammar-based speech applications |
JP5587119B2 (en) | 2010-09-30 | 2014-09-10 | キヤノン株式会社 | CHARACTER INPUT DEVICE, ITS CONTROL METHOD, AND PROGRAM |
US8645825B1 (en) | 2011-08-31 | 2014-02-04 | Google Inc. | Providing autocomplete suggestions |
GB2495222B (en) | 2011-09-30 | 2016-10-26 | Apple Inc | Using context information to facilitate processing of commands in a virtual assistant |
CN108337380B (en) | 2011-09-30 | 2022-08-19 | 苹果公司 | Automatically adjusting user interface for hands-free interaction |
KR102022318B1 (en) | 2012-01-11 | 2019-09-18 | 삼성전자 주식회사 | Method and apparatus for performing user function by voice recognition |
US9280981B2 (en) * | 2013-02-27 | 2016-03-08 | Blackberry Limited | Method and apparatus for voice control of a mobile device |
US9218819B1 (en) * | 2013-03-01 | 2015-12-22 | Google Inc. | Customizing actions based on contextual data and voice-based inputs |
US9111546B2 (en) * | 2013-03-06 | 2015-08-18 | Nuance Communications, Inc. | Speech recognition and interpretation system |
JP6229287B2 (en) | 2013-04-03 | 2017-11-15 | ソニー株式会社 | Information processing apparatus, information processing method, and computer program |
US10120532B2 (en) * | 2014-01-06 | 2018-11-06 | Samsung Electronics Co., Ltd. | Control apparatus for controlling an operation of at least one electronic device |
US8849675B1 (en) * | 2013-12-18 | 2014-09-30 | Google Inc. | Suggested query constructor for voice actions |
US9489171B2 (en) * | 2014-03-04 | 2016-11-08 | Microsoft Technology Licensing, Llc | Voice-command suggestions based on user identity |
US9966065B2 (en) * | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US10235996B2 (en) * | 2014-10-01 | 2019-03-19 | XBrain, Inc. | Voice and connection platform |
WO2016094807A1 (en) * | 2014-12-11 | 2016-06-16 | Vishal Sharma | Virtual assistant system to enable actionable messaging |
US9959129B2 (en) * | 2015-01-09 | 2018-05-01 | Microsoft Technology Licensing, Llc | Headless task completion within digital personal assistants |
US9767091B2 (en) * | 2015-01-23 | 2017-09-19 | Microsoft Technology Licensing, Llc | Methods for understanding incomplete natural language query |
US9666192B2 (en) * | 2015-05-26 | 2017-05-30 | Nuance Communications, Inc. | Methods and apparatus for reducing latency in speech recognition applications |
US10504509B2 (en) * | 2015-05-27 | 2019-12-10 | Google Llc | Providing suggested voice-based action queries |
US10018977B2 (en) | 2015-10-05 | 2018-07-10 | Savant Systems, Llc | History-based key phrase suggestions for voice control of a home automation system |
GB2544070B (en) * | 2015-11-04 | 2021-12-29 | The Chancellor Masters And Scholars Of The Univ Of Cambridge | Speech processing system and method |
US10229671B2 (en) * | 2015-12-02 | 2019-03-12 | GM Global Technology Operations LLC | Prioritized content loading for vehicle automatic speech recognition systems |
US10431215B2 (en) * | 2015-12-06 | 2019-10-01 | Voicebox Technologies Corporation | System and method of conversational adjustment based on user's cognitive state and/or situational state |
US10049663B2 (en) * | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
CN109564757A (en) * | 2016-08-17 | 2019-04-02 | 索尼公司 | Session control and method |
KR20180060328A (en) * | 2016-11-28 | 2018-06-07 | 삼성전자주식회사 | Electronic apparatus for processing multi-modal input, method for processing multi-modal input and sever for processing multi-modal input |
US11100384B2 (en) * | 2017-02-14 | 2021-08-24 | Microsoft Technology Licensing, Llc | Intelligent device user interactions |
KR102416782B1 (en) * | 2017-03-28 | 2022-07-05 | 삼성전자주식회사 | Method for operating speech recognition service and electronic device supporting the same |
DK201770383A1 (en) * | 2017-05-09 | 2018-12-14 | Apple Inc. | User interface for correcting recognition errors |
US10984041B2 (en) * | 2017-05-11 | 2021-04-20 | Commvault Systems, Inc. | Natural language processing integrated with database and data storage management |
DK201770427A1 (en) * | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
US10395659B2 (en) * | 2017-05-16 | 2019-08-27 | Apple Inc. | Providing an auditory-based interface of a digital assistant |
US11436265B2 (en) * | 2017-06-13 | 2022-09-06 | Microsoft Technology Licensing, Llc | System for presenting tailored content based on user sensibilities |
US10176808B1 (en) * | 2017-06-20 | 2019-01-08 | Microsoft Technology Licensing, Llc | Utilizing spoken cues to influence response rendering for virtual assistants |
US20190004821A1 (en) * | 2017-06-29 | 2019-01-03 | Microsoft Technology Licensing, Llc | Command input using robust input parameters |
KR102426704B1 (en) * | 2017-08-28 | 2022-07-29 | 삼성전자주식회사 | Method for operating speech recognition service and electronic device supporting the same |
JP2019153133A (en) * | 2018-03-05 | 2019-09-12 | オムロン株式会社 | Device, method, and program for inputting characters |
US10818288B2 (en) * | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10496705B1 (en) * | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US11145313B2 (en) * | 2018-07-06 | 2021-10-12 | Michael Bond | System and method for assisting communication through predictive speech |
JP7215118B2 (en) * | 2018-11-30 | 2023-01-31 | 株式会社リコー | Information processing device, information processing system, program and method |
CN113330512A (en) * | 2018-12-28 | 2021-08-31 | 谷歌有限责任公司 | Supplementing an automated assistant with speech input according to a selected suggestion |
-
2019
- 2019-02-07 CN CN201980089807.8A patent/CN113330512A/en active Pending
- 2019-02-07 JP JP2021537862A patent/JP7286772B2/en active Active
- 2019-02-07 WO PCT/US2019/017043 patent/WO2020139408A1/en unknown
- 2019-02-07 KR KR1020217023853A patent/KR20210110650A/en unknown
- 2019-02-07 US US16/343,683 patent/US11238857B2/en active Active
- 2019-02-07 EP EP23189459.3A patent/EP4250287A3/en active Pending
- 2019-02-07 EP EP19706216.9A patent/EP3788620B1/en active Active
-
2022
- 2022-01-31 US US17/588,451 patent/US20220157309A1/en active Pending
-
2023
- 2023-05-24 JP JP2023085646A patent/JP2023120205A/en active Pending
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2024085592A1 (en) * | 2022-10-19 | 2024-04-25 | 삼성전자 주식회사 | Electronic device and speech assistant service providing method of electronic device |
Also Published As
Publication number | Publication date |
---|---|
EP4250287A3 (en) | 2024-01-17 |
US20210280180A1 (en) | 2021-09-09 |
JP2022516101A (en) | 2022-02-24 |
US11238857B2 (en) | 2022-02-01 |
EP4250287A2 (en) | 2023-09-27 |
EP3788620B1 (en) | 2023-09-06 |
EP3788620A1 (en) | 2021-03-10 |
CN113330512A (en) | 2021-08-31 |
JP2023120205A (en) | 2023-08-29 |
JP7286772B2 (en) | 2023-06-05 |
US20220157309A1 (en) | 2022-05-19 |
WO2020139408A1 (en) | 2020-07-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3788620B1 (en) | Supplementing voice inputs to an automated assistant according to selected suggestions | |
KR102505597B1 (en) | Voice user interface shortcuts for an assistant application | |
CN111033492B (en) | Providing command bundle suggestions for automated assistants | |
CN113557566B (en) | Dynamically adapting assistant responses | |
KR20210005254A (en) | Provides a complex graphic assistant interface to control various connected devices | |
CN110998717A (en) | Automatically determining language for speech recognition of a spoken utterance received through an automated assistant interface | |
JP7384976B2 (en) | determining whether to automatically resume the first automated assistant session upon termination of the interrupting second session; | |
US20200294497A1 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
JP7170739B2 (en) | Reduced client device latency in rendering remotely generated Automation Assistant content | |
KR20210010523A (en) | Coordination of execution of a series of actions requested to be performed through an automated assistant | |
KR20160132748A (en) | Electronic apparatus and the controlling method thereof | |
KR20230110788A (en) | Passive disambiguation of assistant commands | |
KR20240007261A (en) | Use large-scale language models to generate automated assistant response(s) | |
JP2023536563A (en) | Cancellation of application actions through user interaction with automated assistants | |
JP2023549015A (en) | Enabling natural conversations about automated assistants | |
CN115769298A (en) | Automated assistant control of external applications lacking automated assistant application programming interface functionality | |
JP2024510698A (en) | Contextual suppression of assistant commands | |
US20230252984A1 (en) | Providing contextual automated assistant action suggestion(s) via a vehicle computing device | |
US20240038246A1 (en) | Non-wake word invocation of an automated assistant from certain utterances related to display content | |
WO2023154080A1 (en) | Providing contextual automated assistant action suggestion(s) via a vehicle computing device | |
KR20230158615A (en) | Enable natural conversations using soft endpointing for automated assistants | |
CN116711283A (en) | Providing deterministic reasoning about fulfilling assistant commands |