JP2021504803A - Image selection proposal - Google Patents
Image selection proposal Download PDFInfo
- Publication number
- JP2021504803A JP2021504803A JP2020528152A JP2020528152A JP2021504803A JP 2021504803 A JP2021504803 A JP 2021504803A JP 2020528152 A JP2020528152 A JP 2020528152A JP 2020528152 A JP2020528152 A JP 2020528152A JP 2021504803 A JP2021504803 A JP 2021504803A
- Authority
- JP
- Japan
- Prior art keywords
- images
- image
- user
- selection
- user interface
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 claims abstract description 130
- 230000009471 action Effects 0.000 claims description 55
- 230000004044 response Effects 0.000 claims description 11
- 230000033458 reproduction Effects 0.000 claims 1
- 238000010801 machine learning Methods 0.000 description 47
- 238000012545 processing Methods 0.000 description 26
- 230000015654 memory Effects 0.000 description 24
- 238000004891 communication Methods 0.000 description 15
- 230000000875 corresponding effect Effects 0.000 description 13
- 230000006870 function Effects 0.000 description 13
- 238000012549 training Methods 0.000 description 13
- 230000000694 effects Effects 0.000 description 12
- 238000004458 analytical method Methods 0.000 description 9
- 238000013528 artificial neural network Methods 0.000 description 9
- 238000010586 diagram Methods 0.000 description 9
- 230000006855 networking Effects 0.000 description 8
- 230000008569 process Effects 0.000 description 8
- 238000004364 calculation method Methods 0.000 description 7
- 230000000007 visual effect Effects 0.000 description 7
- 230000008901 benefit Effects 0.000 description 5
- 238000013459 approach Methods 0.000 description 3
- 239000002131 composite material Substances 0.000 description 3
- 238000001514 detection method Methods 0.000 description 3
- 239000011521 glass Substances 0.000 description 3
- 238000010191 image analysis Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000033001 locomotion Effects 0.000 description 3
- 239000011159 matrix material Substances 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 241000282414 Homo sapiens Species 0.000 description 2
- 230000004913 activation Effects 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 241000766754 Agra Species 0.000 description 1
- 241000282412 Homo Species 0.000 description 1
- 244000181025 Rosa gallica Species 0.000 description 1
- 235000000533 Rosa gallica Nutrition 0.000 description 1
- 239000008186 active pharmaceutical agent Substances 0.000 description 1
- 230000002730 additional effect Effects 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 238000013475 authorization Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 230000009977 dual effect Effects 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 235000013305 food Nutrition 0.000 description 1
- 235000015243 ice cream Nutrition 0.000 description 1
- 238000012886 linear function Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 230000029305 taxis Effects 0.000 description 1
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/54—Browsing; Visualisation therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/532—Query formulation, e.g. graphical querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/56—Information retrieval; Database structures therefor; File system structures therefor of still image data having vectorial format
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
Abstract
実施の形態は、画像選択の提案を提供することに関連する。ある実施の形態では、方法は、画像ライブラリ内の１つまたは複数の第１の画像の選択を示す第１のユーザ入力を受信することと、当該１つまたは複数の第１の画像のうちの１つまたは複数の第１の画像特性を判断することとを含む。当該方法は、当該画像ライブラリ内の１つまたは複数の第２の画像を識別することをさらに含む。当該１つまたは複数の第２の画像の各画像は、１つまたは複数の第１の画像特性のうちの少なくとも１つに一致する少なくとも１つの第２の画像特性に関連付けられている。当該方法は、ユーザインターフェイスを表示させることをさらに含む。当該ユーザインターフェイスは、当該１つまたは複数の第２の画像を含み、ユーザによる当該１つまたは複数の第２の画像の選択を可能にする。The embodiment relates to providing an image selection proposal. In certain embodiments, the method is to receive a first user input indicating the selection of one or more first images in the image library and of the one or more first images. Includes determining one or more first image characteristics. The method further comprises identifying one or more second images in the image library. Each image of the one or more second images is associated with at least one second image property that matches at least one of the one or more first image properties. The method further includes displaying the user interface. The user interface includes the one or more second images and allows the user to select the one or more second images.
Description
関連出願の参照
本出願は、（２０１７年１２月２２日に出願され、「画像選択の提案」と題された）米国特許仮出願６２/６０９，５４３に対する優先権を主張し、その全体が参照により本明細書に組み込まれる。
Reference to Related Application This application claims priority to US Patent Provisional Application 62 / 609,543 (filed December 22, 2017, entitled "Proposal for Image Selection"), which is in its entirety. Is incorporated herein by.
背景
デジタルカメラ装置の人気および利便性は、デジタル写真およびビデオのような視覚的コンテンツをユビキタスにしてきた。例えば、様々な種類の多数の画像は、ユーザデバイスによってキャプチャされ、記憶され、表示されることができる。いくつかのデバイスは、ユーザの画像または他のコンテンツが、コレクション内のコンテンツアイテムを含むレイアウトで表示されることを可能にする。通常、コンテンツアイテムのビューは画面に表示され、ユーザは、コンテンツアイテムのコレクションの異なる部分がビュー内に表示されるようにコンテンツアイテムをスクロールさせることができる。ユーザはまた、他のユーザとの共有、ピクチャコラージュやフォトブックの生成など、様々な目的のために１つまたは複数のコンテンツアイテムを選択し得る。
Background The popularity and convenience of digital camera devices has made visual content such as digital photographs and videos ubiquitous. For example, a large number of images of various types can be captured, stored and displayed by the user device. Some devices allow a user's image or other content to be displayed in a layout that includes content items in the collection. A view of a content item is typically displayed on the screen, and the user can scroll the content item so that different parts of the collection of content items are displayed in the view. Users may also select one or more content items for a variety of purposes, such as sharing with others, generating picture collages and photobooks.
本明細書において提供される背景技術の説明は、本開示の文脈を概して提示することを目的としている。現在表示されている発明者らの研究は、この背景技術の項で説明される限り、さもなければ出願時に従来技術としての資格がないかもしれない説明の局面と同様に、本開示に対する従来技術として明示的にも暗黙的にも認めるものではない。 The background art description provided herein is intended to provide a general presentation of the context of this disclosure. The presently presented inventor's work, as described in this Background Techniques section, is a prior art for the present disclosure, as well as an explanatory aspect that may otherwise be disqualified as prior art at the time of filing. It is not explicitly or implicitly acknowledged as.
概要
本出願の実施の形態は、画像選択提案に関する。ある実施の形態では、コンピュータで実施される方法は、画像ライブラリ内の１つまたは複数の第１の画像の選択を示す第１のユーザ入力を受信することと、１つまたは複数の第１の画像の１つまたは複数の第１の画像特性を決定することと、画像ライブラリ内の１つまたは複数の第２の画像を識別することとを含み、当該１つまたは複数の第２の画像の各々は、１つまたは複数の第１の画像特性のうちの少なくとも１つに一致する少なくとも１つの第２の画像特性に関連付けられており、、ユーザインターフェイスを表示させることを含み、ユーザインターフェイスは、１つまたは複数の第２の画像を含み、ユーザインターフェイスは、１つまたは複数の第２の画像の選択を可能にする。
Summary Embodiments of this application relate to image selection proposals. In certain embodiments, the method performed on a computer is to receive a first user input indicating the selection of one or more first images in an image library and one or more first images. Determining the properties of one or more first images of an image and identifying one or more second images in an image library of the one or more second images. Each is associated with at least one second image characteristic that matches at least one of the one or more first image characteristics, including displaying the user interface. Includes one or more second images and the user interface allows selection of one or more second images.
これらおよび他の実施の形態は、それぞれ、以下の特徴のうちの１つまたは複数を任意選択で含み得る：１つまたは複数の第２の画像のうちの少なくとも１つの選択を示す第２のユーザ入力を受信すること、第２のユーザ入力の受信に応答して、１つまたは複数の第２の画像のうちの少なくとも１つの１つまたは複数の第２の画像特性を決定すること、画像ライブラリ内の１つまたは複数の第３の画像を識別することとを含み、１つまたは複数の第３の画像の各々は、１つまたは複数の第１の画像特性のうちの少なくとも１つおよび１つまたは複数の第２の画像特性のうちの少なくとも１つに一致する少なくとも１つの第３の画像特性に関連付けられており、更新されたユーザインターフェイスを表示させることを含み、更新されたユーザインターフェイスは、１つまたは複数の第３の画像を含み、更新されたユーザインターフェイスは、１つまたは複数の第３の画像の選択を可能にする。 These and other embodiments may optionally include one or more of the following features: a second user showing at least one selection of one or more second images. Receiving input, determining at least one or more second image characteristics of one or more second images in response to receiving a second user input, image library. Each of the one or more third images includes at least one and one of the one or more first image characteristics, including identifying one or more third images within. The updated user interface is associated with at least one third image characteristic that matches at least one of the one or more second image characteristics, including displaying the updated user interface. The updated user interface, which includes one or more third images, allows selection of one or more third images.
これらおよび他の実施の形態は、それぞれ、以下の特徴のうちの１つまたは複数を任意選択で含み得る：画像アルバムを生成することを含み、当該画像アルバムは、１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つを含み、１つまたは複数の第２の画像のうちの少なくとも１つの選択を示す第２のユーザ入力を受信することと、画像コラージュを生成することとを含み、画像コラージュは、１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つを含み、１つまたは複数の第１の画像特性を決定することは、画像が不鮮明であることを決定することを含み、１つまたは複数の第２の画像を識別することは、１つまたは複数の第２の画像の不鮮明さに基づいて１つまたは複数の第２の画像の各々を識別することを含む。 These and other embodiments may optionally include one or more of the following features: producing an image album, wherein the image album is one or more of the first. Receiving a second user input that includes an image and at least one of the one or more second images and indicates a selection of at least one of the one or more second images and the images. Including producing a collage, an image collage comprises at least one of one or more first images and one or more second images and one or more first images. Determining the characteristics involves determining that the image is blurry, and identifying one or more second images is based on the blurring of the one or more second images. Includes identifying each of the one or more second images.
これらおよび他の実施の形態は、それぞれ、以下の特徴のうちの１つまたは複数を任意選択で含み得る：１つまたは複数の第１の画像特性を決定することは、第１の画像に関連する位置を決定することを含み、１つまたは複数の第２の画像を識別することは、第１の画像に関連する位置の閾値距離内にあるそれぞれの位置に関連する画像を画像ライブラリから選択することを含み、方法は、画像選択のコンテキストを決定することをさらに含み、１つまたは複数の第２の画像を識別することは、画像選択のコンテキストに基づく。画像選択のコンテキストは、画像ベースの創作物の生成、および/または画像をターゲットソフトウェアアプリケーションに提供することであり得る。画像ベースの創作物は、画像アルバム、画像コラージュ、ビデオ、もしくは印刷された出版物のうちの少なくとも１つ、またはこれらの任意の組合せを含むことができる。 These and other embodiments may optionally include one or more of the following features: determining one or more first image properties is relevant to the first image. Identifying one or more second images, including determining which position to use, selects an image associated with each position within the threshold distance of the position associated with the first image from the image library. The method further comprises determining the context of image selection, and identifying one or more second images is based on the context of image selection. The context of image selection can be to generate image-based creations and / or provide images to the target software application. Image-based creations can include at least one of an image album, image collage, video, or printed publication, or any combination thereof.
これらおよび他の実施の形態は、それぞれ、以下の特徴のうちの１つまたは複数を任意選択で含み得る：第１の画像の複製であり、１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つに基づく、１つまたは複数の第２の画像のうちの少なくとも１つの選択を示す第２のユーザ入力を受信すること、提案されたアクション要素をユーザインターフェイスに表示させること。本方法は、提案されたアクション要素のユーザ選択を受信すること、および、ユーザ選択を受信することに応答して、提案されたアクション要素に関連付けられたアクションを実行することを含むことができ、当該アクションは、以下のうちの１つ以上を含む：１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つをアーカイブすること、１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つを削除すること、および/または１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つの自動的な強調を実行すること。 These and other embodiments may optionally include one or more of the following features: a reproduction of the first image, one or more of the first images and one or more. Receiving a second user input indicating a selection of at least one of one or more second images based on at least one of the plurality of second images, the user using the proposed action element Display on the interface. The method can include receiving a user selection of the proposed action element and performing an action associated with the proposed action element in response to receiving the user selection. The action includes one or more of the following: archiving at least one of one or more first images and one or more second images, one or more first images. Deleting one image and at least one of the one or more second images, and / or at least one of the one or more first images and one or more second images. Performing one automatic emphasis.
ある実施の形態では、コンピュータで実施される方法は、ターゲットソフトウェアアプリケーションを示すコンテキスト情報を決定することと、コンテキスト情報に少なくとも部分的に基づいて、画像ライブラリ内の１つまたは複数の第１の画像を識別することと、ユーザインターフェイスを表示させることとを含み、ユーザインターフェイスは、１つまたは複数の第１の画像を含み、ユーザインターフェイスは、１つまたは複数の第１の画像の選択を可能にし、１つまたは複数の第１の画像のうちの少なくとも１つの画像の選択を示す第１のユーザ入力を受信することと、第１のユーザ入力の受信に応答して、選択された少なくとも１つの画像をターゲットソフトウェアアプリケーションに提供することとを含む。 In certain embodiments, the method performed on a computer is to determine contextual information indicating a target software application and, at least in part, based on the contextual information, one or more first images in an image library. The user interface includes one or more first images, and the user interface allows the selection of one or more first images, including identifying and displaying the user interface. Receiving a first user input indicating selection of at least one of the one or more first images and at least one selected in response to receiving the first user input. Includes providing images to the target software application.
これらおよび他の実施の形態は、それぞれ、以下の特徴のうちの１つまたは複数を任意選択で含み得る：選択された少なくとも１つの画像の１つまたは複数の第１の画像特性を決定することと、画像ライブラリ内の１つまたは複数の第２の画像を識別することとを含み、１つまたは複数の第２の画像の各々が１つまたは複数の第１の画像特性のうちの少なくとも１つを有しており、更新されたユーザインターフェイスを表示させることを含み、更新されたユーザインターフェイスは、１つまたは複数の第２の画像を含み、ユーザインターフェイスは、１つまたは複数の第２の画像の選択を可能にする。 These and other embodiments may optionally include one or more of the following features: determining one or more first image characteristics of at least one selected image. And identifying one or more second images in the image library, each of which one or more second images is at least one of one or more first image characteristics. The updated user interface includes one or more second images, and the user interface includes one or more second images, including displaying the updated user interface. Allows image selection.
これらおよび他の実施の形態は、それぞれ、以下の特徴のうちの１つまたは複数を任意選択で含み得る：コンテキスト情報を決定することは、ターゲットソフトウェアアプリケーションのアプリケーションタイプを決定することを含む。アプリケーションタイプは、画像共有アプリケーションを含むことができる。１つまたは複数の第１の画像を識別することは、品質閾値を満たす１つまたは複数の第１の画像を画像ライブラリから選択することを含むことができる。アプリケーションタイプは、金融アプリケーションを含むことができ、１つまたは複数の第１の画像を識別することは、レシート、文書、またはスクリーンショットのうちの１つまたは複数を含む画像ラベルに関連付けられた１つまたは複数の第１の画像を画像ライブラリから選択することを含むことができる。アプリケーションタイプは、メッセージングアプリケーションを含むことができ、コンテキスト情報を決定することは、メッセージングアプリケーションにおけるメッセージング会話における参加者の識別情報を受信することをさらに含むことができ、１つまたは複数の第１の画像を識別することは、メッセージング会話における参加者のうちの少なくとも１つを示す画像を画像ライブラリから選択することを含むことができる。 These and other embodiments may optionally include one or more of the following features: Determining contextual information includes determining the application type of the target software application. The application type can include an image sharing application. Identifying one or more first images can include selecting one or more first images that meet quality thresholds from an image library. The application type can include a financial application and identifying one or more first images is associated with an image label containing one or more of a receipt, document, or screenshot. It can include selecting one or more first images from the image library. The application type can include a messaging application, and determining contextual information can further include receiving participant identification information in a messaging conversation in a messaging application, one or more first. Identifying an image can include selecting an image from the image library that represents at least one of the participants in the messaging conversation.
これらおよび他の実施の形態は、それぞれ、以下の特徴のうちの１つまたは複数を任意選択で含み得る：コンテキスト情報を決定することは、ターゲットソフトウェアアプリケーションからアプリケーションコンテキストを受信することを含み、１つまたは複数の第１の画像を識別することは、アプリケーションコンテキストに基づいて１つまたは複数のセマンティック概念を決定することと、１つまたは複数の第１の画像を選択することとを含み、選択された画像の各々の少なくとも１つの画像特性がセマンティック概念の少なくとも１つと一致する。 Each of these and other embodiments may optionally include one or more of the following features: determining context information includes receiving an application context from a target software application. Identifying one or more first images includes determining one or more semantic concepts based on the application context and selecting one or more first images. At least one image property of each of the resulting images matches at least one of the semantic concepts.
ある実施の形態では、非一時的なコンピュータ可読媒体は、そこに格納された命令を含み、当該命令は、１つまたは複数のハードウェアプロセッサによって実行されると、１つまたは複数のハードウェアプロセッサに動作を実行させ、当該動作は、画像ライブラリ内の１つまたは複数の第１の画像の選択を示す第１のユーザ入力を受信することと、１つまたは複数の第１の画像の１つまたは複数の第１の画像特性を決定することと、画像ライブラリ内の１つまたは複数の第２の画像を識別することとを含み、１つまたは複数の第２の画像の各々が、１つまたは複数の第１の画像特性のうちの少なくとも１つに一致する少なくとも１つの第２の画像特性に関連付けられており、ユーザインターフェイスを表示させることを含み、ユーザインターフェイスは、１つまたは複数の第２の画像を含み、ユーザインターフェイスは、１つまたは複数の第２の画像の選択を可能にし、１つまたは複数の第２の画像のうちの少なくとも１つの選択を示す第２のユーザ入力を受信することと、１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つに基づいて、提案されたアクション要素をユーザインターフェイスに表示させることと、提案されたアクション要素のユーザ選択の受信に応答して、提案されたアクション要素に関連付けられたアクションを実行することとを含む。ある実施の形態では、アクションは、以下のうちの１つまたは複数を含む：１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つをアーカイブすること、１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つを削除すること、１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つを含む画像ベースの創作物を生成すること、または、１つまたは複数の第１の画像および１つまたは複数の第２の画像のうちの少なくとも１つを自動的に強調することを実行すること。 In certain embodiments, the non-temporary computer-readable medium includes instructions stored therein, which, when executed by one or more hardware processors, are one or more hardware processors. To perform an operation, the operation receiving a first user input indicating the selection of one or more first images in the image library and one of the one or more first images. Alternatively, each of the one or more second images includes determining one or more first image characteristics and identifying one or more second images in the image library. Or associated with at least one second image characteristic that matches at least one of the plurality of first image characteristics, including displaying the user interface, the user interface is one or more first. Containing two images, the user interface allows selection of one or more second images and receives a second user input indicating selection of at least one of the one or more second images. And to display the proposed action element in the user interface based on at least one of the one or more first images and the one or more second images. Includes performing an action associated with a proposed action element in response to receiving a user selection of the action element. In certain embodiments, the action comprises: archiving one or more first images and at least one of one or more second images, comprising one or more of the following: Deleting at least one of one or more first images and one or more second images of one or more first images and one or more second images Generate an image-based creation that includes at least one of them, or automatically highlight at least one of one or more first images and one or more second images. To do that.
１つまたは複数の実施の形態は、ユーザが画像を選択するのを補助する。本明細書で説明する１つまたは複数の実施の形態は、ユーザが選択するための提案された画像を提供することを含む。実施態様は、ユーザが、キーワードまたはフレーズを使用して画像を検索すること、スクロールによって画像ライブラリをブラウジングすることなど、低減された入力でユーザが画像を選択することを可能にするユーザインターフェイスを提供することによって利益を提供する。実施態様は、そのようなアクティビティなしにユーザが画像を選択することを可能にするユーザインターフェイスを提供することによって、検索またはブラウジングするユーザをサポートするために使用されるコンピューティングリソースを低減することができる。 One or more embodiments assist the user in selecting an image. One or more embodiments described herein include providing a proposed image for the user to select. The embodiment provides a user interface that allows the user to select an image with reduced input, such as searching for an image using a keyword or phrase, or scrolling through an image library. Providing benefits by doing. An embodiment may reduce the computing resources used to support a user searching or browsing by providing a user interface that allows the user to select an image without such activity. it can.
ユーザは、様々な目的のために、例えば画像ライブラリから、他のユーザと画像を共有するために、画像アルバム、画像コラージュ、印刷されたフォトブック、または画像を含むビデオなどの画像ベースの創作物を生成するために、画像を選択する。ユーザは、多数の画像、例えば、数百、数千、または数百万もの画像を含む画像ライブラリを有し得る。画像を見つけて選択することは、ユーザにとって困難であり得る。例えば、画像ライブラリ内の画像が時系列に編成されて表示される場合、ユーザは、特定の目的のために画像を閲覧および選択するために複数のスクロール操作を行う必要があるかもしれない。関心のある画像を見つけるために前後にスクロールすることは、面倒であり、ユーザは、そのようなスクロールをイライラすると思うかもしれない。さらに、画像をスクロールするそのようなユーザアクティビティは、コンピューティングデバイスがメモリまたはストレージから追加の画像を取り出し、異なる画像を示すためにユーザインターフェイスをリフレッシュすることを必要とし、コンピューティングデバイスのコンピューティングリソースの使用を必要とする。 Users can use image-based creations such as image albums, image collages, printed photobooks, or videos containing images for a variety of purposes, such as from an image library to share images with other users. Select an image to generate. A user may have an image library containing a large number of images, such as hundreds, thousands, or millions of images. Finding and selecting images can be difficult for the user. For example, if the images in the image library are displayed in chronological order, the user may need to perform multiple scrolling operations to view and select the images for a particular purpose. Scrolling back and forth to find the image of interest is tedious, and users may find such scrolling frustrating. In addition, such user activity of scrolling images requires the computing device to retrieve additional images from memory or storage and refresh the user interface to show different images, and the computing resources of the computing device. Requires the use of.
画像ライブラリのアプリケーションは、画像検索の特徴を含むことができる。例えば、ユーザはテキストクエリを指定することができ、画像ライブラリのアプリケーションは、検索に関連する画像をライブラリから返すことができ、ユーザは、返された画像から画像を選択することができる。しかしながら、これは、ユーザがクエリを定義することを必要とする。さらに、ユーザは、選択すべき画像の組全体を得るために、複数の異なるクエリを指定する必要があり得る。クエリを介して画像を検索して選択することは、面倒である。さらに、ユーザの検索アクティビティは、コンピューティングデバイスが、例えば、当該コンピューティングデバイス上にローカルに格納された、またはネットワークを介した遠隔サーバに格納された画像ライブラリにアクセスすることを、および、ユーザ指定の検索クエリに一致する画像を識別することを要求し、当該コンピューティングデバイスのコンピューティングリソースの使用を必要とする。 Image library applications can include image search features. For example, the user can specify a text query, the image library application can return images related to the search from the library, and the user can select an image from the returned images. However, this requires the user to define the query. In addition, the user may need to specify a number of different queries to get the entire set of images to select. Searching and selecting images via queries can be tedious. In addition, the user's search activity indicates that the computing device accesses, for example, an image library stored locally on the computing device or on a remote server over a network, and user-specified. Requirees the identification of images that match the search query of, and requires the use of computing resources for that computing device.
本文書に記載される実施の形態は、多数の画像を含む画像ライブラリから画像を発見および選択することにおける問題のいくつかに対処する。ある実施の形態では、画像ライブラリ中の１つまたは複数の第１の画像の選択を示す第１のユーザ入力が受け取られる。１つまたは複数の第１の画像の１つまたは複数の第１の画像特性が決定される。画像ライブラリ内の１つまたは複数の第２の画像は、自動的に識別され、１つまたは複数の第１の画像特性のうちの少なくとも１つに一致する少なくとも１つの第２の画像特性にそれぞれ関連付けられる。当該１つまたは複数の第２の画像を含み、当該１つまたは複数の第２の画像の選択を可能にするユーザインターフェイスが表示される。 The embodiments described in this document address some of the problems in finding and selecting images from an image library that contains a large number of images. In certain embodiments, a first user input indicating the selection of one or more first images in the image library is received. One or more first image characteristics of one or more first images are determined. The one or more second images in the image library are automatically identified and each have at least one second image characteristic that matches at least one of the one or more first image characteristics. Be associated. A user interface is displayed that includes the one or more second images and allows selection of the one or more second images.
ユーザ選択に合致する画像を自動的に識別して提示することによって、本明細書で説明する実施の形態は、ユーザが画像ライブラリをスクロールするときに画像を表示する、またはユーザ入力によって指定されたテキスト検索クエリを満たす結果として得られる画像を表示する必要性を排除する。ユーザ入力が画像を選択すると、適合する特性を有するユーザの画像ライブラリからの追加画像は、自動的に識別され、ユーザが選択するために提示される。このようにして、本明細書で説明される技法は、画像の記憶、表示、および選択からの効率的な検索を可能にし、画像の容易な発見および選択を可能にするユーザインターフェイスを提示する。スクロールされた画像の長い表示を低減または回避することによって、および、テキスト検索クエリからの検索結果の複数の検索および表示を低減または回避することによって、デバイスリソースの消費を低減することが可能になる。 By automatically identifying and presenting an image that matches the user's choice, the embodiments described herein display the image as the user scrolls through the image library, or are specified by user input. Eliminate the need to display the resulting image that satisfies a text search query. When user input selects an image, additional images from the user's image library with matching properties are automatically identified and presented for user selection. In this way, the techniques described herein provide a user interface that allows efficient retrieval from image storage, display, and selection, and enables easy discovery and selection of images. It is possible to reduce device resource consumption by reducing or avoiding the long display of scrolled images, and by reducing or avoiding multiple searches and display of search results from text search queries. ..
記載される提案された画像選択は、ユーザインターフェイスにおける画像のより迅速かつより効率的な表示、および画像を位置付けおよび選択するためのユーザインターフェイスとのユーザのより効率的な相互作用を可能にすることができる。例えば、提案された画像選択の表示は、ユーザが容易に画像を選択することを可能にし、選択すべき画像を決定するために検索クエリを指定するテキストまたは他の複雑な入力を手動で提供する必要性を低減または排除する。さらに、提案された画像選択は、画像を表示する時間および処理を低減し、画像を見つけるために手動で指定された検索が受け取られる回数を低減する。提案された画像選択はまた、クライアントデバイスが、検索（例えば、スマートウォッチまたはキーボードまたはマイクロフォンを含まない他のウエアラブルデバイス）のための容易で、単純な、または任意のテキスト入力機能がないときに有用である。そのような特徴は、低減されたユーザ入力および低減された時間を伴うコンテンツ表示プログラムとの対話を可能にし、したがって、そうでなければ、そのようなプログラムにおいてユーザ入力を受信し、処理し、結果を表示するために必要とされるであろう、デバイスリソースの消費を低減する。 The proposed image selection described allows for faster and more efficient display of the image in the user interface and more efficient user interaction with the user interface for positioning and selecting the image. Can be done. For example, the proposed image selection display allows the user to easily select an image and manually provides text or other complex input to specify a search query to determine which image to select. Reduce or eliminate the need. In addition, the proposed image selection reduces the time and processing of displaying the image and reduces the number of times a manually specified search is received to find the image. The proposed image selection is also useful when the client device does not have an easy, simple, or arbitrary text input feature for searching (eg, smartwatches or other wearable devices that do not include a keyboard or microphone). Is. Such features allow interaction with content display programs with reduced user input and reduced time, and thus otherwise receive, process, and result in user input in such programs. Reduces the consumption of device resources that would be required to display.
したがって、１つまたは複数の説明される実施の形態の技術的効果は、ユーザインターフェイスにおけるコンテンツデータアイテムの表示が、結果を得るのに費やされるより少ない計算時間およびより少ない計算リソースで提供されることである。例えば、記載される技術および特徴の技術的効果は、当該記載される技術または特徴のうちの１つまたは複数を提供しない従来のシステムと比較して、特定の画像を表示および選択するために利用されるシステム処理リソースの消費の低減である。例えば、以前のシステムは、検索クエリをテキストとして受信するための以前の技術を使用することができ、この場合、どの検索クエリが選択すべき画像を提供するのに関連するかをユーザが決定するときに検索結果の反復を表示するために、プロセッサ、メモリ、およびディスプレイリソースなどの（対応する電力消費を伴う）追加の計算リソースが必要である。 Therefore, the technical effect of one or more of the described embodiments is that the display of content data items in the user interface is provided with less computational time and less computational resources spent to obtain results. Is. For example, the technical effects of the described techniques and features may be utilized to display and select a particular image as compared to conventional systems that do not provide one or more of the described techniques or features. This is a reduction in the consumption of system processing resources. For example, older systems can use previous techniques for receiving search queries as text, in which case the user determines which search query is relevant to provide the image to select. Sometimes additional computational resources (with corresponding power consumption) such as processor, memory, and display resources are needed to display iterative search results.
記載される技術および特徴のさらなる技術的効果は、当該記載される技術または特徴のうちの１つまたは複数を提供しないシステムによって利用される、表示および検索処理などのシステム処理リソースの消費ならびにおよび電力消費の低減である。例えば、そのような従来のシステムでは、ユーザは、コンテンツ要素のビューを手動でスクロールし、ユーザは、そのような手動スクロールを介して所望のコンテンツアイテムを手動で位置付けなければならず、（例えば、コンテンツデータアイテムを前方向および後方向への繰り返しの表示およびスクロール、ユーザからコンテンツアイテムを表示および/又は検索するためのコマンドを繰り返し受信すること等のために）システムリソースの非効率的な使用につながる。 Further technical effects of the described technology and features are consumption and power consumption of system processing resources such as display and search processing utilized by systems that do not provide one or more of the described technologies or features. It is a reduction in consumption. For example, in such a traditional system, the user manually scrolls the view of the content element, and the user must manually position the desired content item through such manual scrolling (eg, for example. For inefficient use of system resources (for repeating display and scrolling of content data items forward and backward, repeatedly receiving commands from the user to display and / or retrieve content items, etc.) Connect.
本明細書で記載されるある実施の形態が、ユーザに関する個人情報（例えば、ユーザデータ、ユーザのソーシャルネットワークに関する情報、その場所におけるユーザの場所および時間、ユーザのバイオメトリック情報、ユーザの活動および人口統計情報）を収集または使用し得る状況では、ユーザは、情報が収集されるかどうか、個人情報が記憶されるかどうか、個人情報が使用されるかどうか、および情報がどのようにユーザに関して収集され、記憶され、使用されるかを制御する１つまたは複数の機会を提供される。すなわち、本明細書で説明されるシステムおよび方法は、特に、それを行なうように適切なユーザからの明示的な許可を受けて、ユーザ個人情報を収集し、記憶し、および/または使用する。例えば、ユーザは、プログラムまたは特徴が、その特定のユーザまたはそのプログラムまたは特徴に関連する他のユーザに関するユーザ情報を収集するかどうかに関する制御を提供される。個人情報が収集されるべき各ユーザには、そのユーザに関連する情報収集の制御を可能にし、情報が収集されるかどうか、および情報のどの部分が収集されるべきかに関する許可または認可を提供するために、１つまたは複数のオプションが提示される。例えば、ユーザは、通信ネットワークを介して１つまたは複数のそのような制御オプションを提供され得る。さらに、特定のデータは、個人的に識別可能な情報が取り除かれるように保存または使用される前に、１つまたは複数の方法で取り扱われてもよい。一例として、ユーザの識別情報は、個人的に識別可能な情報が決定されないように取り扱うことができる。別の例として、ユーザの特定の位置を決定することができないように、ユーザデバイスの地理的位置をより大きな領域にまで一般化することができる。 Certain embodiments described herein include personal information about a user (eg, user data, information about a user's social network, a user's location and time at that location, a user's biometric information, a user's activity and population. In situations where (statistics) may be collected or used, the user may collect information about the user, whether the information is collected, whether the personal information is stored, whether the personal information is used, and how the information is collected about the user. It is provided with one or more opportunities to control whether it is stored, stored, and used. That is, the systems and methods described herein collect, store, and / or use user personal information, in particular with the express permission of the appropriate user to do so. For example, a user is provided with control over whether a program or feature collects user information about that particular user or other users associated with that program or feature. Each user for whom personal information should be collected allows control of the collection of information related to that user and provides permission or authorization regarding whether the information is collected and what portion of the information should be collected One or more options are offered to do so. For example, the user may be provided with one or more such control options via a communication network. In addition, certain data may be treated in one or more ways before being stored or used to remove personally identifiable information. As an example, the user's identification information can be handled so that personally identifiable information is not determined. As another example, the geographic location of a user device can be generalized to a larger area so that a particular location of the user cannot be determined.
本明細書において言及される画像は、１つまたは複数の画素値（例えば、色値、輝度値など。）を有するピクセルを有するデジタル画像である。画像は、静止画像または単一画像であり得るか、または一連の画像、たとえば、ビデオフレームのビデオシーケンスにおけるフレームに含まれる画像、または異なるタイプのシーケンスもしくは画像のアニメーションにおける画像であり得る。ビデオは、複数の画像のシーケンスを含む。たとえば、本明細書で記載される実施の形態は、単一画像または静止画像（例えば、写真、絵文字、または他の画像）、ビデオ、または動画画像（例えば、シネマグラフまたは動きを含む他の動画、アニメーションおよび音声を含むステッカーなど。）であるコンテンツデータアイテムとともに使用され得る。テキストは、本明細書で参照されるように、英数字、絵文字、記号、または他の文字を含むことができる。音声セグメントは、例えばスピーカから音声を提供するために処理することができる標準的な音声フォーマットで提供される音声データを含むことができる。 The image referred to herein is a digital image having pixels having one or more pixel values (eg, color values, luminance values, etc.). The image can be a still image or a single image, or it can be a series of images, eg, an image contained within a frame in a video sequence of a video frame, or an image in an animation of a different type of sequence or image. The video contains a sequence of multiple images. For example, the embodiments described herein are single images or still images (eg, photographs, pictograms, or other images), videos, or moving images (eg, cinemagraphs or other moving images including motion). , Such as stickers containing animations and sounds.) Can be used with content data items. The text may include alphanumeric characters, pictograms, symbols, or other characters as referred to herein. The audio segment can include audio data provided in a standard audio format that can be processed, for example, to provide audio from a speaker.
図１は、本明細書で記載されるある実施の形態で使用され得る例示的なネットワーク環境１００のブロック図を示す。ある実施の形態では、ネットワーク環境１００は、１つまたは複数のサーバシステム、たとえば、図１の例ではサーバシステム１０２および第２のサーバシステム１４０を含む。サーバシステム１０２および１４０は、例えば、ネットワーク１３０と通信することができる。サーバシステム１０２は、サーバデバイス１０４およびデータベース１０６または他のストレージデバイスを含むことができる。ある実施の形態では、サーバデバイス１０４は、画像ライブラリアプリケーション１５６bを提供し得る。第２のサーバシステム１４０は、１つまたは複数のアプリケーション、たとえば、メッセージングアプリケーションA１４４、画像共有アプリケーションB１４６、および金融アプリケーションC１４８を提供するように構成された第２のサーバデバイス１４２を含むことができる。 FIG. 1 shows a block diagram of an exemplary network environment 100 that may be used in certain embodiments described herein. In certain embodiments, the network environment 100 includes one or more server systems, such as the server system 102 and the second server system 140 in the example of FIG. The server systems 102 and 140 can communicate with, for example, the network 130. The server system 102 can include a server device 104 and a database 106 or other storage device. In certain embodiments, the server device 104 may provide an image library application 156b. The second server system 140 can include a second server device 142 configured to provide one or more applications, such as messaging application A144, image sharing application B146, and financial application C148.
ネットワーク環境１００はまた、ネットワーク１３０を介して互いに、および/またはサーバシステム１０２および/または第２のサーバシステム１４０と通信することができる、１つまたは複数のクライアントデバイス、たとえば、クライアントデバイス１２０，１２２，１２４および１２６を含むことができる。ネットワーク１３０は、インターネット、ローカルエリアネットワーク（ＬＡＮ）、ワイヤレスネットワーク、スイッチまたはハブ接続などのうちの１つまたは複数を含む、任意のタイプの通信ネットワークであり得る。ある実施の形態では、ネットワーク１３０は、たとえば、ピアツーピアワイヤレスプロトコル（例えば、Bluetooth（登録商標）、Wi-Fi（登録商標） Directなど。）などを使用する、デバイス間のピアツーピア通信を含み得る。２つのクライアントデバイス１２０と１２２との間のピアツーピア通信の一例が矢印１３２によって示されている。 The network environment 100 can also communicate with each other and / or with the server system 102 and / or the second server system 140 over the network 130 with one or more client devices, such as client devices 120, 122. , 124 and 126 can be included. The network 130 can be any type of communication network, including one or more of the Internet, a local area network (LAN), a wireless network, a switch or hub connection, and the like. In certain embodiments, the network 130 may include peer-to-peer communication between devices using, for example, peer-to-peer wireless protocols (eg, Bluetooth®, Wi-Fi® Direct, etc.). An example of peer-to-peer communication between the two client devices 120 and 122 is indicated by arrow 132.
説明を容易にするために、図１は、サーバシステム１０２、サーバデバイス１０４、データベース１０６、第２のサーバシステム１４０、および第２のサーバデバイス１４２の１つのブロックを示し、クライアントデバイス１２０，１２２，１２４および１２６の４つのブロックを示す。サーバブロック１０２，１０４，１０６，１４０および１４２は、複数のシステム、サーバデバイス、およびネットワークデータベースを表してもよく、ブロックは、示されるものとは異なる構成で提供されてもよい。例えば、サーバシステム１０２および/または第２のサーバシステム１４０は、ネットワーク１３０を介して他のサーバシステムと通信することができる複数のサーバシステムを表すことができる。 For ease of explanation, FIG. 1 shows one block of server system 102, server device 104, database 106, second server system 140, and second server device 142, client devices 120, 122, The four blocks 124 and 126 are shown. Server blocks 102, 104, 106, 140 and 142 may represent multiple systems, server devices, and network databases, and blocks may be provided in configurations different from those shown. For example, the server system 102 and / or the second server system 140 can represent a plurality of server systems capable of communicating with other server systems via the network 130.
ある実施の形態では、サーバシステム１０２および/または第２のサーバシステム１４０は、たとえば、クラウドホスティングサーバを含み得る。いくつかの例では、データベース１０６および/または他の記憶デバイスは、サーバデバイス１０４とは別個のサーバシステムブロック内に提供され、ネットワーク１３０を介してサーバデバイス１０４および他のサーバシステムと通信することができる。また、任意の数のクライアントデバイスが存在してもよい。各クライアントデバイスは、任意のタイプの電子デバイス、例えば、デスクトップコンピュータ、ラップトップコンピュータ、携帯またはモバイルデバイス、携帯電話、スマートフォン、タブレットコンピュータ、テレビ、テレビセットトップボックスまたはエンターテインメントデバイス、ウェアラブルデバイス（例えば、表示眼鏡またはゴーグル、腕時計、ヘッドセット、アームバンド、ジュエリーなど）、携帯情報端末（PDA）、メディアプレーヤ、ゲーム装置などであり得る。いくつかのクライアントデバイスはまた、データベース１０６または他のストレージと同様のローカルデータベースを有し得る。ある実施の形態では、ネットワーク環境１００は、示される構成要素のすべてを有さなくてもよく、および/または本明細書で記載される構成要素の代わりに、またはそれに加えて、他のタイプの構成要素を含む他の構成要素を有してもよい。 In certain embodiments, the server system 102 and / or the second server system 140 may include, for example, a cloud hosting server. In some examples, the database 106 and / or other storage devices may be provided in a server system block separate from the server device 104 and communicate with the server device 104 and other server systems over network 130. it can. Also, there may be any number of client devices. Each client device is an electronic device of any type, such as a desktop computer, laptop computer, mobile or mobile device, mobile phone, smartphone, tablet computer, television, television settop box or entertainment device, wearable device (eg, display). It can be glasses or goggles, watches, headsets, armbands, jewelry, etc.), personal digital assistants (PDAs), media players, gaming devices, and the like. Some client devices may also have a local database similar to database 106 or other storage. In certain embodiments, the network environment 100 may not have all of the components shown and / or in place of or in addition to the components described herein of other types. It may have other components, including components.
様々な実施の形態では、エンドユーザＵ１、Ｕ２、Ｕ３、およびＵ４は、それぞれのクライアントデバイス１２０，１２２，１２４および１２６を使用して、サーバシステム１０２と、および/または互いに通信することができる。いくつかの例では、ユーザＵ１、Ｕ２、Ｕ３、およびＵ４は、それぞれのクライアントデバイスおよび/またはサーバシステム１０２もしくは第２のサーバシステム１４０上で実行されるアプリケーションを介して、および/またはサーバシステム１０２もしくは第２のサーバシステム１４０上に実施されるネットワークサービス、たとえばソーシャルネットワークサービスもしくは他のタイプのネットワークサービスを介して、互いに相互作用することができる。たとえば、それぞれのクライアントデバイス１２０，１２２，１２４および１２６は、１つまたは複数のサーバシステム（例えば、システム１０２、第２のサーバシステム１４０）との間でデータを通信することができる。 In various embodiments, the end users U1, U2, U3, and U4 can use their respective client devices 120, 122, 124, and 126 to communicate with and / or communicate with the server system 102. In some examples, users U1, U2, U3, and U4 pass through their respective client devices and / or applications running on server system 102 or second server system 140 and / or server system 102. Alternatively, they can interact with each other via network services implemented on the second server system 140, such as social network services or other types of network services. For example, the respective client devices 120, 122, 124 and 126 can communicate data with one or more server systems (eg, system 102, second server system 140).
ある実施の形態では、サーバシステム１０２および/または第２のサーバシステム１４０は、各クライアントデバイスがサーバシステム１０２または第２のサーバシステム１４０および/またはネットワークサービスにアップロードされた通信コンテンツまたは共有コンテンツを受信することができるように、適切なデータをクライアントデバイスに提供することができる。いくつかの例では、ユーザＵ１〜Ｕ４は、オーディオもしくはビデオ会議、オーディオ、ビデオ、もしくはテキストチャット、または他の通信モードもしくはアプリケーションを介して相互作用することができる。サーバシステム１０２または第２のサーバシステム１４０によって実施されるネットワークサービスは、ユーザが様々な通信を実行すること、リンクおよび関連付けを形成すること、画像、テキスト、ビデオ、オーディオ、および他のタイプのコンテンツなどの共有コンテンツをアップロードおよびポストすること、かつ/または他の機能を実行することを可能にするシステムを含むことができる。たとえば、クライアントデバイスは、クライアントデバイスに送信またはストリーミングされ、サーバおよび/またはネットワークサービスを介して（または異なるクライアントデバイスから直接）、異なるクライアントデバイスから発信されるか、またはサーバシステムおよび/またはネットワークサービスから発信されるコンテンツポストなどの受信データを表示することができる。ある実施の形態では、クライアントデバイスは、たとえば、上述したように、クライアントデバイス間のピアツーピア通信を使用して、互いに直接通信することができる。ある実施の形態では、「ユーザ」は、システムまたはネットワークとインターフェイスする人と同様に、１つまたは複数のプログラムまたは仮想エンティティを含み得る。 In certain embodiments, the server system 102 and / or the second server system 140 receives communication or shared content that each client device has uploaded to the server system 102 or the second server system 140 and / or network service. Appropriate data can be provided to the client device so that it can be. In some examples, users U1-U4 can interact via audio or video conferencing, audio, video, or text chat, or other communication modes or applications. Network services performed by server system 102 or second server system 140 allow users to perform various communications, form links and associations, images, text, video, audio, and other types of content. It can include systems that allow you to upload and post shared content such as and / or perform other functions. For example, a client device is sent or streamed to the client device and originates from a different client device through a server and / or network service (or directly from a different client device), or from a server system and / or network service. Received data such as transmitted content posts can be displayed. In certain embodiments, the client devices can communicate directly with each other, for example, using peer-to-peer communication between the client devices, as described above. In certain embodiments, a "user" may include one or more programs or virtual entities, as well as a person who interfaces with a system or network.
ある実施の形態では、クライアントデバイス１２０、１２２、１２４、および/または１２６のいずれかが、１つまたは複数のアプリケーションを提供することができる。例えば、図１に示されるように、クライアントデバイス１２０は、カメラアプリケーション１５２、画像ライブラリアプリケーション１５６a、および１つまたは複数の他のアプリケーション１５４を提供し得る。クライアントデバイス１２２〜１２６はまた、同様のアプリケーションを提供してもよい。例えば、カメラアプリケーション１５２は、各クライアント装置のユーザ（例えばユーザＵ１〜Ｕ４）に例えば、各クライアント装置のカメラを用いて、画像をキャプチャする能力を提供し得る。例えば、カメラアプリケーション１５２は、クライアント装置１２０において実行するソフトウェアアプリケーションであり得る。ある実施の形態では、カメラアプリケーション１５２は、ユーザインターフェイスを提供してもよい。 In certain embodiments, any of the client devices 120, 122, 124, and / or 126 can provide one or more applications. For example, as shown in FIG. 1, the client device 120 may provide a camera application 152, an image library application 156a, and one or more other applications 154. Client devices 122-126 may also provide similar applications. For example, the camera application 152 may provide users of each client device (eg, users U1 to U4) with the ability to capture an image, eg, using the camera of each client device. For example, the camera application 152 can be a software application that runs on the client device 120. In certain embodiments, the camera application 152 may provide a user interface.
ある実施の形態では、クライアントデバイス１２０は、画像ライブラリアプリケーション１５６aを含み得る。画像ライブラリアプリケーション１５６aは、図６を参照して説明されるように、クライアントデバイス１２０のハードウェアおよび/またはソフトウェアを使用して実施され得る。異なる実施の形態では、画像ライブラリアプリケーション１５６aは、たとえば、クライアントデバイス１２０〜１２４のいずれかで実行されるスタンドアロンの画像ライブラリアプリケーションであってもよく、またはサーバシステム１０２上に提供される画像ライブラリアプリケーション１５６bと連携して動作してもよい。画像ライブラリアプリケーション１５６aおよび画像ライブラリアプリケーション１５６bは、画像管理機能を提供することができる。画像管理機能は、カメラアプリケーション１５２を使用してキャプチャされた写真およびビデオを記憶する機能と、他の画像、例えば、スキャンされた画像、スクリーンショットなどを記憶する機能と、画像編集機能と、画像コラージュ、ビデオ、画像アルバム、または印刷された出版物（例えば、フォトブック）のような画像ベースの創作物を生成する機能とを含み得る。画像ライブラリアプリケーション１５６はまた、画像、例えば、画像ピクセルおよび画像メタデータをプログラム的に分析して、１つまたは複数の画像特性を決定し得る。画像ライブラリアプリケーション１５６は、例えば、クライアントデバイスのローカルストレージ、および/またはネットワーク１３０を介して通信する１つまたは複数の他のデバイスのリモートストレージ、例えば、サーバシステム１０２および/または１４０のデータベース１０６、１つまたは複数の異なるクライアントデバイスのストレージなどに画像データおよび特徴を記憶し得る。 In certain embodiments, the client device 120 may include an image library application 156a. The image library application 156a can be implemented using the hardware and / or software of the client device 120, as described with reference to FIG. In different embodiments, the image library application 156a may be, for example, a stand-alone image library application running on any of client devices 120-124, or the image library application 156b provided on the server system 102. It may operate in cooperation with. The image library application 156a and the image library application 156b can provide an image management function. The image management function includes a function to store photos and videos captured using the camera application 152, a function to store other images such as scanned images and screenshots, an image editing function, and an image. It may include the ability to generate image-based creations such as collages, videos, image albums, or printed publications (eg, photobooks). The image library application 156 can also programmatically analyze images, such as image pixels and image metadata, to determine one or more image characteristics. The image library application 156 may include, for example, local storage of client devices and / or remote storage of one or more other devices communicating over network 130, such as databases 106, 1 of server systems 102 and / or 140. Image data and features can be stored, such as in the storage of one or more different client devices.
ある実施の形態では、クライアントデバイス１２０は、１つまたは複数の他のアプリケーション１５４を含み得る。例えば、他のアプリケーション１５４は、様々な種類の機能、例えば、カレンダー、アドレス帳、電子メール、ウェブブラウザ、ショッピング、輸送（例えば、タクシー、電車、エアライン予約など。）、エンターテインメント（例えば、音楽プレーヤ、ビデオプレーヤ、ゲームアプリケーションなど。）、ソーシャルネットワーキング（例えば、メッセージングまたはチャット、音声/映像呼び出し、画像/映像共有など。）などを提供するアプリケーションであり得る。ある実施の形態では、他のアプリケーション１５４のうちの１つまたは複数は、クライアントデバイス１２０上で実行するスタンドアロン型アプリケーションであり得る。ある実施の形態では、他のアプリケーション１５４のうちの１つまたは複数は、アプリケーション１５４のデータおよび/または機能を提供するサーバシステム、たとえば、第２のサーバシステム１４０にアクセスすることができる。たとえば、第２のサーバシステム１４０によって提供されるものとして示されているアプリケーション１４４，１４６および１４８のいずれかは、データおよび/またはコマンドを他のアプリケーション１５４のうちの１つまたは複数に提供することができる。ある実施の形態では、サーバアプリケーション１４４-１４８は、たとえば、ウェブブラウザまたは他のクライアント側プログラムを介してクライアントデバイスによってアクセスされるスタンドアロン型アプリケーションであり得る。 In certain embodiments, the client device 120 may include one or more other applications 154. For example, the other application 154 may include various types of functions such as calendars, address books, emails, web browsers, shopping, transportation (eg taxis, trains, airline bookings, etc.), entertainment (eg music players, etc.). , Video players, gaming applications, etc.), social networking (eg, messaging or chat, audio / video calling, image / video sharing, etc.), and the like. In one embodiment, one or more of the other applications 154 may be stand-alone applications running on the client device 120. In certain embodiments, one or more of the other applications 154 can access a server system that provides data and / or functionality for the application 154, such as a second server system 140. For example, any of the applications 144, 146 and 148 shown as provided by the second server system 140 provides data and / or commands to one or more of the other applications 154. Can be done. In certain embodiments, the server application 144-148 can be, for example, a stand-alone application accessed by a client device via a web browser or other client-side program.
クライアントデバイス１２０、１２２、１２４、および/または１２６上のユーザインターフェイスは、通信、プライバシー設定、通知、および他のデータと同様に、ユーザコンテンツ、ならびに、画像、ビデオ、データ、および他のコンテンツを含む他のコンテンツの表示を可能にすることができる。そのようなユーザインターフェイスは、クライアントデバイス上のソフトウェア、サーバデバイス上のソフトウェア、および/または、クライアントソフトウェアとサーバデバイス１０４および/または第２のサーバデバイス１４２上で実行するサーバソフトウェアとの組み合わせ、たとえば、アプリケーションソフトウェア、またはサーバシステム１０２および/または第２のサーバデバイス１４２と通信するクライアントソフトウェアを使用して表示することができる。ユーザインターフェイスは、クライアントデバイスまたはサーバデバイスのディスプレイデバイス、たとえば、タッチスクリーンまたは他のディスプレイスクリーン、プロジェクタなどによって表示され得る。ある実施の形態では、サーバシステム上で実行されるアプリケーションプログラムは、クライアントデバイスにおいてユーザ入力を受信し、当該クライアントデバイスにおいて視覚データ、音声データなどのデータを出力するために、当該クライアントデバイスと通信することができる。 User interfaces on client devices 120, 122, 124, and / or 126 include user content, as well as images, videos, data, and other content, as well as communications, privacy settings, notifications, and other data. It is possible to display other contents. Such a user interface is a combination of software on the client device, software on the server device, and / or the client software and server software running on the server device 104 and / or the second server device 142, eg. It can be displayed using application software or client software that communicates with the server system 102 and / or the second server device 142. The user interface may be displayed by the display device of the client or server device, such as a touch screen or other display screen, a projector, and the like. In certain embodiments, an application program running on a server system communicates with a client device in order to receive user input on the client device and output data such as visual data, audio data, etc. on the client device. be able to.
ある実施の形態では、サーバシステム１０２、第２のサーバシステム１４０、および/または１つまたは複数のクライアントデバイス１２０〜１２６のいずれかが、通信アプリケーションプログラムを提供することができる。通信プログラムは、システム（例えば、クライアントデバイスまたはサーバシステム）が他のデバイスと通信するためのオプションを提供することを可能にし得る。通信プログラムは、サーバシステムまたはクライアントデバイスに関連付けられたディスプレイデバイスに表示される１つまたは複数の関連付けられたユーザインターフェイスを提供することができる。ユーザインターフェイスは、通信モード、通信するユーザまたはデバイスなどを選択するための様々なオプションをユーザに提供し得る。通信プログラムは、たとえば、様々なフォーマットのいずれかで、送信されたコンテンツポストおよび受信されたコンテンツポストを表示またはそうでなければ出力することができる。 In certain embodiments, any one of the server system 102, the second server system 140, and / or one or more client devices 120-126 can provide the communication application program. A communication program may allow a system (eg, a client device or server system) to provide options for communicating with other devices. The communication program can provide one or more associated user interfaces that are displayed on the display device associated with the server system or client device. The user interface may provide the user with various options for selecting the communication mode, the user or device to communicate with, and the like. The communication program can display or otherwise output the transmitted content post and the received content post, for example in any of various formats.
本明細書で説明する特徴の他の実施の形態は、任意のタイプのシステムおよび/またはサービスを使用することができる。例えば、ソーシャルネットワーキングサービスの代わりに、またはソーシャルネットワーキングサービスに加えて、他のネットワークサービス（例えば、インターネットに接続される）が使用され得る。任意の種類の電子デバイスは、本明細書に記載の特徴を利用することができる。ある実施の形態は、コンピュータネットワークから切断されるかまたは断続的に接続される１つまたは複数のクライアントデバイスまたはサーバデバイス上に、本明細書で説明される１つまたは複数の特徴を提供することができる。いくつかの例では、ディスプレイデバイスを含むか、またはディスプレイデバイスに接続されたクライアントデバイスは、当該クライアントデバイスにローカルなストレージデバイスに記憶された、たとえば、通信ネットワークを介して以前に受信された画像を表示することができる。 Other embodiments of the features described herein can use any type of system and / or service. For example, other network services (eg, connected to the Internet) may be used in place of or in addition to social networking services. Any type of electronic device can take advantage of the features described herein. One embodiment provides one or more features as described herein on one or more client or server devices that are disconnected or intermittently connected from a computer network. Can be done. In some examples, a client device that includes or is connected to a display device has images stored in a storage device local to that client device, for example, previously received over a communication network. Can be displayed.
図２は、ある実施の形態に従って画像選択提案を提供するための例示的な方法を示すフロー図である。ある実施の形態では、方法２００は、たとえば、図１に示されるように、サーバシステム１０２上で実施され得る。ある実施の形態では、方法２００の一部または全部は、図１に示されるように、１つまたは複数のクライアントデバイス１２０，１２２，１２４，または１２６上で、１つまたは複数のサーバデバイス上で、および/またはサーバデバイスとクライアントデバイスの両方上で実施され得る。説明される例では、実施システムは、１つまたは複数のデジタルプロセッサまたは処理回路（「プロセッサ」）と、１つまたは複数の記憶デバイス（例えば、データベース１０６または他の記憶装置）とを含む。ある実施の形態では、１つまたは複数のサーバおよび/またはクライアントの異なる構成要素は、方法２００の異なるブロックまたは他の部分を実行することができる。いくつかの例では、第１のデバイスは、方法２００のブロックを実行するものとして説明される。ある実施の形態は、結果またはデータを第１のデバイスに送信することができる１つまたは複数の他のデバイス（例えば、他のクライアントデバイスまたはサーバデバイス）によって実行される方法２００の１つまたは複数のブロックを有することができる。 FIG. 2 is a flow diagram illustrating an exemplary method for providing an image selection proposal according to an embodiment. In certain embodiments, the method 200 can be implemented on the server system 102, for example, as shown in FIG. In certain embodiments, some or all of the method 200 is on one or more client devices 120, 122, 124, or 126 and on one or more server devices, as shown in FIG. , And / or can be implemented on both server and client devices. In the examples described, the implementation system includes one or more digital processors or processing circuits (“processors”) and one or more storage devices (eg, database 106 or other storage devices). In certain embodiments, different components of one or more servers and / or clients may execute different blocks or other parts of Method 200. In some examples, the first device is described as performing the block of method 200. One embodiment is one or more of methods 200 performed by one or more other devices (eg, other client or server devices) capable of transmitting the results or data to the first device. Can have blocks of.
ある実施の形態では、方法２００または当該方法の一部は、システムによって自動的に開始され得る。ある実施の形態では、実施システムは第１のデバイスである。例えば、本方法（またはその部分）は、定期的に実行することができ、あるいは１つまたは複数の特定のイベントまたは条件、例えば、ユーザによって開始されるアプリケーション、ユーザによって選択される画像、および/または本方法によって読み取られる設定において指定され得る１つまたは複数の他の条件に基づいて実行することができる。ある実施の形態では、そのような条件は、格納された当該ユーザのカスタム選好においてユーザによって指定され得る。 In certain embodiments, method 200 or part of the method may be initiated automatically by the system. In certain embodiments, the implementation system is the first device. For example, the method (or part thereof) can be performed on a regular basis, or one or more specific events or conditions, such as a user-initiated application, a user-selected image, and /. Alternatively, it can be performed based on one or more other conditions that may be specified in the settings read by this method. In certain embodiments, such conditions may be specified by the user in the stored custom preference of the user.
一例では、クライアントデバイスは、カメラ、携帯電話、スマートフォン、タブレットコンピュータ、ウェアラブルデバイス、または、コンテンツ入力（例えば、画像キャプチャ）を受信し、かつ、ユーザによる当該クライアントデバイスへのユーザ入力（例えば、タッチスクリーンを使用すること、ジェスチャを使用すること、マウスまたは他のポインティングデバイスを使用すること、キーボードを使用することなど。）を受信することができ、方法２００を実行することができる他のクライアントデバイスであり得る。別の例では、クライアントデバイスまたはサーバデバイスが方法２００を実行することができる。ある実施の形態は、ユーザ入力に基づいて方法２００を開始することができる。ユーザ（例えば、オペレータまたはエンドユーザ）は、例えば、ユーザインターフェイス、例えば、アプリケーションユーザインターフェイスまたは他のユーザインターフェイスから方法２００の開始を選択してもよい。ある実施の形態では、方法２００は、クライアントデバイスによって実施され得る。ある実施の形態では、方法２００は、サーバデバイスによって実施され得る。 In one example, the client device receives a camera, mobile phone, smartphone, tablet computer, wearable device, or content input (eg, image capture) and the user inputs to the client device (eg, touch screen). With other client devices that can receive (use a mouse, use a gesture, use a mouse or other pointing device, use a keyboard, etc.) and can perform method 200. possible. In another example, a client device or server device can perform method 200. In certain embodiments, method 200 can be initiated based on user input. The user (eg, operator or end user) may choose to start Method 200 from, for example, a user interface, eg, an application user interface or another user interface. In certain embodiments, method 200 can be performed by a client device. In certain embodiments, method 200 can be performed by a server device.
本明細書で言及する画像は、１つまたは複数の画素値（例えば、色値、輝度値など。）を有するピクセルを有するデジタル画像を含むことができる。画像は、静止画像（例えば、静止写真、単一フレームを有する画像など。）、動画像（例えば、アニメーション、動画GIF、画像の一部分が動きを含み、他の部分が静止しているシネマグラフなど。）、およびビデオ（例えば、オーディオを含むことができる画像または画像フレームのシーケンス）であり得る。本明細書の残りは静止画像として画像を言及するが、本明細書で説明される技術は、動画像、ビデオなどに適用可能であることを理解されたい。たとえば、本明細書で説明する実施の形態は、静止画像（例えば、写真、絵文字、または他の画像）、ビデオ、または動画像）とともに使用され得る。テキストは、本明細書で参照されるように、英数字、絵文字、記号、または他の文字を含むことができる。 The images referred to herein can include digital images having pixels having one or more pixel values (eg, color values, luminance values, etc.). Images include still images (eg, still photos, images with a single frame, etc.), moving images (eg, animations, moving images, GIFs, cinemagraphs in which part of the image contains motion and others are still) ), And video (eg, a sequence of images or image frames that can contain audio). Although the rest of this specification refers to images as still images, it should be understood that the techniques described herein are applicable to moving images, video and the like. For example, the embodiments described herein can be used with still images (eg, photographs, pictograms, or other images), video, or moving images. The text may include alphanumeric characters, pictograms, symbols, or other characters as referred to herein.
ブロック２０２では、方法２００の実施において、ユーザデータを使用するためにユーザの同意（例えば、ユーザ許可）が得られたかどうかがチェックされる。たとえば、ユーザデータは、クライアントデバイスを使用してユーザによってキャプチャされた画像、たとえば、クライアントデバイスを使用してユーザによって記憶またはアクセスされた画像、画像メタデータ、画像共有アプリケーションの使用に関連するユーザデータ、メッセージングアプリケーションの使用に関連するユーザデータ、金融アプリケーションの使用に関連するユーザデータ、ソーシャルネットワーキングアプリケーションの使用に関連するユーザデータ、ユーザ選好、ユーザバイオメトリック情報、ユーザ特性（同一性、名前、年齢、性別、職業など。）、ユーザのソーシャルネットワークおよび連絡先に関する情報、ソーシャルおよび他の種類のアクションおよびアクティビティ、ユーザによって作成または提出されたコンテンツ、格付け、および意見、ユーザの現在位置、過去のユーザデータ、ユーザによって生成、受信、および/またはアクセスされる画像、ユーザによって閲覧または共有される画像などを含み得る。本明細書で説明される方法の１つまたは複数のブロックは、ある実施の形態では、そのようなユーザデータを使用し得る。 In block 202, in the implementation of method 200, it is checked whether the user's consent (eg, user permission) has been obtained to use the user data. For example, user data is images captured by the user using the client device, such as images stored or accessed by the user using the client device, image metadata, user data related to the use of image sharing applications. , User data related to the use of messaging applications, user data related to the use of financial applications, user data related to the use of social networking applications, user preferences, user biometric information, user characteristics (identity, name, age, Gender, occupation, etc.), information about the user's social networks and contacts, social and other types of actions and activities, content created or submitted by the user, ratings and opinions, the user's current location, past user data. , Images generated, received, and / or accessed by the user, images viewed or shared by the user, and the like. One or more blocks of the methods described herein may use such user data in certain embodiments.
方法２００においてユーザデータが使用され得る関連ユーザからユーザ同意が得られた場合、ブロック２０４において、本明細書の方法のブロックは、それらのブロックに関して記載されたようなユーザデータの可能な使用を用いて実施され得ると判定され、方法はブロック２１２に続く。ユーザの同意が得られていない場合、ブロック２０６において、ブロックがユーザデータを使用せずに実施されるべきであると決定され、方法はブロック２１２に続く。ある実施の形態では、ユーザの同意が得られていない場合、ブロックは、ユーザデータを使用せずに、合成データおよび/または汎用データもしくは公にアクセス可能かつ公的に使用可能なデータを用いて実施される。ユーザの同意が取得されていない場合、方法２００は実行されない。 User data may be used in method 200 If user consent is obtained from the relevant user, then in block 204, the blocks of the methods herein use the possible use of user data as described for those blocks. It is determined that this can be done and the method follows block 212. If the user's consent has not been obtained, in block 206 it is determined that the block should be performed without the use of user data and the method follows block 212. In certain embodiments, without user consent, the block uses synthetic data and / or general purpose data or publicly accessible and publicly available data, without the use of user data. Will be implemented. If the user's consent has not been obtained, method 200 will not be performed.
方法２００のブロック２１２では、第１の画像の選択が受け取られる。例えば、ユーザの画像ライブラリからの複数の画像が、デバイスの表示画面に表示されてもよい。ある実施の形態では、画像サムネイルが表示され得、サムネイルの選択は、対応する画像の選択に対応し得る。例えば、画像またはサムネイルは、グリッド構成、例えば、１行当たり３つの画像、１行当たり４つの画像等で表示されてもよい。 At block 212 of method 200, the selection of the first image is received. For example, multiple images from the user's image library may be displayed on the display screen of the device. In certain embodiments, image thumbnails may be displayed and the selection of thumbnails may correspond to the selection of the corresponding image. For example, the image or thumbnail may be displayed in a grid configuration, for example, 3 images per line, 4 images per line, and the like.
ある実施の形態では、画像は、１つまたは複数の画像特性によって、たとえば、対応する画像に関連付けられた日付または時間によって、対応する画像に関連付けられたロケーション（たとえば、地理的ロケーション）によって、対応する画像が属する画像アルバムによって、などにより、グループ化され得る。ある実施の形態では、ユーザが、顔認識の使用、および/または画像内に描写される人物によって画像をグループ化する画像タグ付けの使用を許可する場合、画像は、例えば、「アンドリューとエニカ」、「アンドリューとジョイス」、「エニカ」などの人物によってグループ化され得る。ある実施の形態では、グループは、複数の画像特性または因子、たとえば、「森の小道にいるアンドリューとエニカ」、「ジョイスとの昨日」などを利用することができる。 In certain embodiments, the images are addressed by one or more image characteristics, eg, by the date or time associated with the corresponding image, by the location associated with the corresponding image (eg, geographical location). It can be grouped by, for example, by the image album to which the image to be created belongs. In certain embodiments, if the user permits the use of facial recognition and / or the use of image tagging to group images by the persons depicted in the image, the image is, for example, "Andrew and Enika". Can be grouped by people such as "Andrew and Joyce", "Enika". In certain embodiments, the group can take advantage of multiple image characteristics or factors, such as "Andrew and Enika in the Forest Path," "Yesterday with Joyce," and so on.
ある実施の形態では、１つまたは複数の第１の画像の選択を示す第１のユーザ入力が受信される。ある実施の形態では、ユーザ入力を受信することは、タッチスクリーンに表示された画像上でのタップまたはプレスジェスチャを検出すること、音声入力、注視入力、入力デバイス（マウス、ジョイスティック、トラックパッドなど）の操作を検出することなどを含み得る。ユーザは、画面（例えば、画像サムネイルもしくは他の画像表現、または画像全体のディスプレイ）に表示された対応する画像要素を選択することによって、複数の画像のうちの１つまたは複数を選択することができる。ある実施の形態では、画像はスクロール可能であってもよく、またはユーザが当該複数の画像の様々なおよび/または追加の画像を選択することができるように、複数のページまたはセクションに編成されてもよい。１つまたは複数の第１の画像のユーザ選択を受信すると、方法はブロック２１４に進む。 In certain embodiments, a first user input indicating the selection of one or more first images is received. In certain embodiments, receiving user input detects a tap or press gesture on an image displayed on a touch screen, voice input, gaze input, input device (mouse, joystick, trackpad, etc.). It may include detecting the operation of. The user may select one or more of a plurality of images by selecting the corresponding image element displayed on the screen (eg, an image thumbnail or other image representation, or a display of the entire image). it can. In certain embodiments, the images may be scrollable or organized into multiple pages or sections so that the user can select various and / or additional images of the plurality of images. May be good. Upon receiving the user selection of one or more first images, the method proceeds to block 214.
ブロック２１４において、選択された画像の第１の画像特性、例えば、１つまたは複数の第１の画像が決定される。ある実施の形態では、第１の画像特性を決定することは、第１の画像に関連して記憶された画像メタデータから画像特性を検索することを含み得る。ある実施の形態では、第１の画像特性を決定することは、画像データベースから、たとえば、画像および画像特性をデータベースに格納する画像ライブラリから、画像特性を検索することを含み得る。 At block 214, the first image characteristic of the selected image, eg, one or more first images, is determined. In certain embodiments, determining the first image property may include searching for the image property from the image metadata stored in connection with the first image. In certain embodiments, determining the first image property may include searching the image property from an image database, eg, from an image library that stores the image and the image property in the database.
ある実施の形態では、第１の画像特性は、画像コンテンツ（例えば、第１の画像の画素値）および/または画像メタデータ（例えば、キャプチャの日付、キャプチャの位置、カメラ作成/モデル、カメラ設定、画像解像度、画像色深度など。）の分析に基づいて決定され得る。ある実施の形態では、画像コンテンツの分析は、たとえば、画像のユーザの選択を受信する前に、オフラインで実行され得る。例えば、ある画像の画像内容の分析は、当該画像が生成される（例えば、カメラを使用してキャプチャされ、スクリーンショットとしてキャプチャされる等の）とき、または、ライブラリに追加された（例えば、画像ライブラリを有するデバイス上にダウンロードまたは格納された）ときに実行され得る。 In certain embodiments, the first image characteristic is image content (eg, pixel values of the first image) and / or image metadata (eg, capture date, capture position, camera creation / model, camera settings). , Image resolution, image color depth, etc.) can be determined based on the analysis. In certain embodiments, the analysis of the image content can be performed offline, for example, before receiving the user's selection of images. For example, an analysis of the image content of an image is made when the image is generated (eg, captured using a camera and captured as a screenshot), or added to a library (eg, an image). Can be executed when downloaded or stored on a device that has a library.
ある実施の形態では、第１の画像特性は、画像をプログラム的に分析することによって決定され得る。ある実施の形態では、第１の画像特性は、画像に関連する１つまたは複数のラベルを含み得る。ある実施の形態では、１つまたは複数のラベルは、画像ピクセル値および/または画像メタデータに基づくことができる。ある実施の形態では、１つまたは複数のラベルは、画像に関連する１つまたは複数のセマンティック概念を含み得る。ある実施の形態では、セマンティック概念は、概念の階層、たとえば、「食品→ケーキ→誕生日ケーキ」、「スポーツ→ベースボール→用具→バット」などに編成され得、範囲が狭い概念は、範囲が広い概念カテゴリに含まれ得る。画像に関連する任意の数のセマンティック概念が存在し得る。セマンティック概念は、画像内の１つまたは複数のオブジェクトを検出すること、焦点、照明レベル、画像合成、色分布などの画像属性、位置、時間、および他のメタデータなどのメタデータ、および他の因子のうちの１つまたは複数に基づいて決定され得る。ある実施の形態では、セマンティック概念は、画像からセマンティック概念を認識するようにトレーニングされた機械学習モデルを使用して決定され得る。 In certain embodiments, the first image property can be determined by programmatically analyzing the image. In certain embodiments, the first image property may include one or more labels associated with the image. In certain embodiments, one or more labels can be based on image pixel values and / or image metadata. In certain embodiments, one or more labels may include one or more semantic concepts associated with an image. In one embodiment, semantic concepts can be organized into a hierarchy of concepts, such as "food-> cake-> birthday cake", "sports-> baseball-> equipment-> bat", and narrower concepts have a range. Can be included in a wide range of concept categories. There can be any number of semantic concepts associated with the image. Semantic concepts are the detection of one or more objects in an image, image attributes such as focus, lighting level, image composition, color distribution, metadata such as position, time, and other metadata, and other metadata. It can be determined based on one or more of the factors. In certain embodiments, the semantic concept can be determined using a machine learning model trained to recognize the semantic concept from an image.
ある実施の形態では、画像特性、たとえば、１つまたは複数のラベルは、信頼スコアに関連付けられ得る。信頼スコアは、特定のラベルが画像に適用される信頼の程度を示し得る。１つまたは複数のラベルおよび対応する信頼スコアは、画像をプログラム的に分析することに基づくことができる。ある実施の形態では、画像をプログラム的に分析することは、トレーニングされた機械学習モデルを使用して実行され得る。ある実施の形態では、画像をプログラム的に分析することは、オブジェクト検出および認識技術を使用して実行され得る。ある実施の形態では、様々な画像分析技術の組合せが使用され得る。 In certain embodiments, image characteristics, such as one or more labels, may be associated with a confidence score. The confidence score can indicate the degree of confidence that a particular label applies to an image. One or more labels and the corresponding confidence score can be based on a programmatic analysis of the image. In certain embodiments, programmatic analysis of the image can be performed using a trained machine learning model. In certain embodiments, programmatic analysis of the image can be performed using object detection and recognition techniques. In certain embodiments, a combination of various image analysis techniques can be used.
ユーザが同意を提供するある実施の形態では、画像をプログラム的に分析することは、画像ピクセルデータから１つまたは複数のセマンティック概念を識別することを含み得る。例えば当該画像が、屋内で撮影されたか屋外で撮影されたかを判定することができる。別の例では、機械学習モデルは、画像が様々なオブジェクト、例えば、木、花、建物、河または海、山などのうちの１つまたは複数を示すかどうかを決定することができる。ある実施の形態では、機械学習モデルは、たとえば、「赤いバラ」、「１２階建ての建物」、「コーヒーショップ」、「バースデーケーキ」などに示されるオブジェクトの特定の特徴を決定することができる。 In certain embodiments in which the user provides consent, programmatic analysis of the image may include identifying one or more semantic concepts from the image pixel data. For example, it can be determined whether the image was taken indoors or outdoors. In another example, a machine learning model can determine whether an image represents one or more of various objects, such as trees, flowers, buildings, rivers or seas, mountains, and so on. In certain embodiments, the machine learning model can determine certain characteristics of an object, such as a "red rose," "12-story building," "coffee shop," "birthday cake," and so on. ..
ユーザが同意を提供するある実施の形態では、画像内の１つまたは複数のオブジェクト、たとえば「タージマハル」、「ベースボールバット」、「アイスクリーム」などが認識され得る。ユーザが顔検出の使用の同意を提供する実施の形態では、画像が１人または複数の人物を示す（例えば、「画像は３人の人を有する」または「画像は２人の女性を示す」）かどうかを決定することができる。ユーザが顔認識技術の使用の同意を提供する実施の形態では、画像内の人、例えば「ジョイス」、「アンドリュー」等を認識してもよい。 In certain embodiments in which the user provides consent, one or more objects in the image, such as the "Taj Mahal," "baseball bat," "ice cream," etc., may be recognized. In an embodiment in which the user provides consent to use face detection, the image shows one or more people (eg, "the image has three people" or "the image shows two women". ) Can be determined. In an embodiment in which the user provides consent to use the face recognition technique, a person in the image, such as "Joyce", "Andrew", etc., may be recognized.
ユーザが同意を提供するある実施の形態では、画像特性は、画像メタデータから決定され得る。例えば、そのような画像特性は、カメラ製造/型式、カメラ設定（例えば、絞り、フラッシュの使用、カメラモードなど。）、画像キャプチャの日付および/または時間、（ユーザが位置データの使用を許可する場合）画像キャプチャの位置を含み得る。 In certain embodiments in which the user provides consent, the image characteristics can be determined from the image metadata. For example, such image characteristics include camera make / model, camera settings (eg aperture, use of flash, camera mode, etc.), date and / or time of image capture, (user allows use of position data). If) can include the location of the image capture.
ユーザが同意を提供するある実施の形態では、１つまたは複数のセマンティック概念は、画像および/または画像メタデータで示されるオブジェクトに基づいて決定されてもよく、たとえば、フラッシュがオンにされて画像がキャプチャされるとき、示されるオブジェクトは、ダイニングテーブルおよび料理を含み、画像キャプチャの位置および時間は、当該画像が夜間にキャプチャされたことを示し、セマンティック概念は、「ディナー」および「屋内」であり得る。別の例では、画像が水およびボートを示すとき、「航海」というセマンティック概念は、当該画像に関連付けられてもよい。 In certain embodiments in which the user provides consent, one or more semantic concepts may be determined based on the objects represented by the image and / or image metadata, eg, an image with flash turned on. When captured, the objects shown include dining tables and dishes, the location and time of the image capture indicates that the image was captured at night, and the semantic concepts are "dinner" and "indoor". possible. In another example, when an image shows water and a boat, the semantic concept of "voyage" may be associated with the image.
画像内のオブジェクトの認識および/または画像に対応するセマンティック概念の決定時に、１つまたは複数のラベルを画像に関連付けることができる。画像特性、たとえば１つまたは複数のラベルは、たとえば画像ファイル内の画像メタデータとして、データベースに記憶され得る。例えば、関連付けられたラベルは「ジョイス、ディナー、屋内、家」、「ハイキング、ヨセミテ国立公園」等であり得る。 One or more labels can be associated with an image when recognizing objects in the image and / or determining the semantic concept that corresponds to the image. Image characteristics, such as one or more labels, may be stored in the database, for example as image metadata in an image file. For example, the associated label could be "Joyce, dinner, indoors, home", "hiking, Yosemite National Park" and the like.
ある実施の形態では、画像特性は、画像タイプ、たとえば、画像をプログラム的に分析することに基づいて決定される「文書」、「レシート」、「スクリーンショット」、「写真」などを含み得る。ある実施の形態では、画像特性は、画像をプログラム的に分析することに基づいて決定された画像属性、たとえば、「不鮮明」、「暗」などを含み得る。ブロック２１４の後には、ブロック２１６が続くことができる。 In certain embodiments, image properties may include image types, such as "documents", "receipts", "screenshots", "photographs", etc., which are determined based on programmatic analysis of the image. In certain embodiments, the image characteristics may include image attributes determined based on programmatic analysis of the image, such as "blurred", "dark" and the like. Block 214 can be followed by block 216.
ブロック２１６では、画像ライブラリ内の１つまたは複数の第２の画像が識別される。ある実施の形態では、１つまたは複数の第２の画像は、ユーザによって選択された１つまたは複数の第１の画像のうちの１つまたは複数の第１の画像特性の画像特性と同じまたは類似する少なくとも１つの画像特性を有し得る。例えば、ユーザが不鮮明な第１の画像を選択した場合、不鮮明という画像特性にそれぞれ関連付けられる１つまたは複数の第２の画像が識別される。 At block 216, one or more second images in the image library are identified. In certain embodiments, the one or more second images are the same as or the same as the image characteristics of one or more of the first image characteristics of the one or more first images selected by the user. It may have at least one similar image property. For example, if the user selects a blurry first image, one or more second images, respectively, associated with the blurry image property are identified.
別の例では、ユーザが文書の２つの画像（例えば、文書であると判定されたテキストまたは視覚コンテンツを示す画像）を選択した場合、それぞれが画像特性「文書」に関連する１つまたは複数の第２の画像が識別される。別の例では、ユーザがスクリーンショット（例えば、デバイスの表示画面のコンテンツのキャプチャ）である２つの画像を選択した場合、各スクリーンショットである１つまたは複数の第２の画像が識別され、例えば画像特性「スクリーンショット」に関連付けられる。別の例では、ユーザが、２人の個人、例えば、ジョイスおよびアンドリューを示す画像を選択した場合、それぞれ「ジョイス」および「アンドリュー」のうちの少なくとも１つの画像ラベルに関連する１つまたは複数の第２の画像が識別される。 In another example, if the user selects two images of a document (eg, an image showing text or visual content determined to be a document), one or more images, each associated with the image property "document". The second image is identified. In another example, if the user selects two images that are screenshots (eg, a capture of the content of the display screen of the device), one or more second images that are each screenshot are identified, eg. Associated with the image characteristic "screenshot". In another example, if the user selects images showing two individuals, eg Joyce and Andrew, one or more associated with at least one image label of "Joyce" and "Andrew", respectively. The second image is identified.
別の例では、第１の画像のうちの１つまたは複数が場所、たとえば「タージマハル」に関連付けられている場合、同じまたは同様の場所、たとえば「タージマハル」、「アグラ」などに関連付けられている１つまたは複数の第２の画像が選択され得る。ある実施の形態では、１つまたは複数の第２の画像は、第２の画像に関連付けられた場所が、第１の画像に関連付けられた位置の閾値距離内にあるように選択され得る。例えば、閾値距離は、数値距離、例えば「５マイル」であり得る。ある実施の形態では、１つまたは複数の第２の画像は、場所、たとえば、市名、国名などのマッチングに基づいて選択され得る。ある実施の形態では、１つまたは複数の第２の画像は、第１の場所で第１の画像によって描写されるものと同じ特性のうちの１つまたは複数を示す画像、たとえば、モニュメント、ブリッジ、または摩天楼などに基づいて選択され得る。 In another example, if one or more of the first images are associated with a location, such as the Taj Mahal, then the same or similar location, such as the Taj Mahal, Agra, etc. One or more second images may be selected. In certain embodiments, the one or more second images may be selected such that the location associated with the second image is within the threshold distance of the position associated with the first image. For example, the threshold distance can be a numerical distance, eg, "5 miles". In certain embodiments, the one or more second images may be selected based on matching of locations, such as city names, country names, and the like. In certain embodiments, the one or more second images are images showing one or more of the same properties as depicted by the first image in the first place, such as monuments, bridges. , Or can be selected based on skyscrapers, etc.
ある実施の形態では、画像選択のコンテキストが決定され得る。例えば、ユーザが第１のユーザ入力を与えるユーザインターフェイスは、例えば画像コラージュ、画像ライブラリ内の画像に基づくビデオ、画像アルバム、または印刷された出版物、例えばフォトブックなど、１つまたは複数のデバイスによって画像ベースの創作物を生成することに関連付けられていると決定され得る。ある実施の形態では、画像選択のコンテキストは、画像選択のためのユーザインターフェイスが開始された、関連付けられたターゲットソフトウェアアプリケーション、たとえば、メッセージングアプリケーション、画像共有またはソーシャルネットワーキングアプリケーション、金融アプリケーションなどであってもよく、または、これらに基づいてもよい。 In certain embodiments, the context of image selection may be determined. For example, the user interface to which the user gives a first user input may be by one or more devices, such as an image collage, an image-based video in an image library, an image album, or a printed publication, such as a photobook. It can be determined to be associated with producing image-based creations. In certain embodiments, the context of image selection may be the associated target software application, such as a messaging application, image sharing or social networking application, financial application, etc., from which the user interface for image selection has been initiated. Well, or may be based on these.
画像選択のコンテキストが決定される実施の形態では、１つまたは複数の第２の画像を識別することは、コンテキストに基づいてもよい。たとえば、コンテキストは、第２の画像を選択するための１つまたは複数の選択基準と（たとえば、ブロック２１６の前に）関連付けられ得る。たとえば、コンテキストが、画像共有アプリケーションを介してフォトブックを生成することまたは共有することとして決定される場合、１つまたは複数の第２の画像は、そのコンテキストに関連付けられる品質基準、たとえば、画像解像度基準、画像色深度基準、画像焦点基準などに基づいて選択され得る。いくつかの例では、画像解像度閾値基準を満たさない画像は、そのような画像が第１の画像特性に合致する特性を有する場合であっても、１つまたは複数の第２の画像から除外され得る。別の例では、コンテキストが金融アプリケーションとして決定される場合、ある特性、たとえば、画像タイプ「レシート」、「請求書」、「財務諸表」、「文書」などを有する画像を１つまたは複数の第２の画像に含めることができ、他の画像を除外することができる。 In embodiments where the context of image selection is determined, identifying one or more second images may be context-based. For example, the context can be associated with one or more selection criteria (eg, before block 216) for selecting a second image. For example, if the context is determined to generate or share a photobook through an image sharing application, the one or more second images will have a quality criterion associated with that context, eg, image resolution. It can be selected based on criteria, image color depth criteria, image focus criteria, and so on. In some examples, images that do not meet the image resolution threshold criteria are excluded from one or more second images, even if such images have properties that match the first image characteristics. obtain. In another example, when the context is determined as a financial application, one or more images with certain characteristics, such as the image types "receipt", "invoice", "financial statements", "document", etc. It can be included in 2 images and other images can be excluded.
ある実施の形態では、１つまたは複数の第２の画像は、第１の画像の画像特性に一致するいくつかの画像特性に基づいて選択またはフィルタリングされる。例えば、第１の画像が「ビーチにいるジョイスとアンドリュー」を示す場合、第２の画像は、３つの特性「ジョイス」、「アンドリュー」および「ビーチ」のうちのどれだけ多くが第２の画像の各々に関連するかに基づいて選択されてもよい。一例では、画像ライブラリが、３つの特性の全てを有する画像Ａ、画像Ｂ、および画像Ｃと、３つの特性のうちの２つを有するさらなる画像Ｄ，ＥおよびＦとを含む場合、画像Ａ，ＢおよびＣは、１つまたは複数の第２の画像として選択されてもよく、画像Ｄ，ＥおよびＦは、第２の画像から除外されてもよい。ある実施の形態では、画像Ａ〜Ｆは、第２の画像に含まれ、３つの特性のうちの１つだけを有する画像を含む他の画像は、第２の画像から除外される。 In certain embodiments, the one or more second images are selected or filtered based on some image characteristics that match the image characteristics of the first image. For example, if the first image shows "Joyce and Andrew on the beach", then the second image is how many of the three characteristics "Joyce", "Andrew" and "Beach" are the second image. It may be selected based on whether it is related to each of the above. In one example, if the image library includes images A, B, and C having all three properties, and additional images D, E, and F having two of the three properties, image A, B and C may be selected as one or more second images, and images D, E and F may be excluded from the second image. In certain embodiments, images A through F are included in the second image, and other images, including images having only one of the three properties, are excluded from the second image.
ある実施の形態では、信頼閾値を使用して、１つまたは複数の第２の画像をフィルタリングすることができる。例えば、第１の画像が、９０%の信頼スコアを有する特性「レシート」に関連付けられる場合、少なくとも９０%の信頼スコアを有する特性「レシート」に関連付けられる第２の画像が選択され得る。この例では、より低い信頼スコアを有する画像は、画像が特性「レシート」に関連付けられる場合であっても、１つまたは複数の第２の画像から除外され得る。ある実施の形態では、第１の画像の画像特性に一致する特性の数と、特性に関連する信頼スコアとの組合せを使用して、１つまたは複数の第２の画像を選択することができる。ある実施の形態では、各画像特性は、１つまたは複数の第２の画像を決定するために重み付けされ得る。例えば、「タージマハル」または「ジョイス」などのラベルなどのいくつかの特性は、他の特性、例えば「屋外」より高い重みを割り当てられ得る。 In certain embodiments, confidence thresholds can be used to filter one or more second images. For example, if the first image is associated with a characteristic "receipt" having a 90% confidence score, then a second image associated with the characteristic "receipt" having at least a 90% confidence score may be selected. In this example, an image with a lower confidence score can be excluded from one or more second images, even if the image is associated with the characteristic "receipt". In certain embodiments, a combination of the number of characteristics that match the image characteristics of the first image and the confidence score associated with the characteristics can be used to select one or more second images. .. In certain embodiments, each image characteristic may be weighted to determine one or more second images. For example, some properties, such as labels such as "Taj Mahal" or "Joyce," may be assigned higher weights than other properties, such as "outdoors."
ある実施の形態では、ユーザが、画像選択に関連するユーザデータおよびユーザによって生成される画像ベースの創作物の使用の同意を提供するとき、そのようなデータは、１つまたは複数の第２の画像を選択するために使用され得る。ユーザが、例えばフォトブックなどの、花の写真を含む印刷された出版物を以前に生成した場合、花を示す１つまたは複数の第２の画像が含まれ得る。この例では、以前のフォトブックに含まれていた画像は、たとえば、ユーザが最近キャプチャされた画像を含むフォトブックを定期的に生成することをユーザデータが示す場合、除外され得る。 In certain embodiments, when the user provides consent to use user data related to image selection and image-based creations generated by the user, such data is one or more second. Can be used to select an image. If the user has previously generated a printed publication containing a picture of a flower, such as a photobook, it may include one or more second images showing the flower. In this example, the images contained in the previous photobook can be excluded, for example, if the user data indicates that the user will periodically generate a photobook containing recently captured images.
別の例では、ユーザが、あるタイプの写真、たとえば、人間の顔を描写しない写真を選択する提案を以前に無視している場合、そのような画像は、１つまたは複数の第２の画像から除外され得る。別の例では、ユーザが特定の個人、例えば、家族、ペットなどの画像を含む写真コラージュを生成することをユーザデータが示す場合、その個人を示す画像は、１つまたは複数の第２の画像において選択されてもよく、その個人を描写しない画像は除外されてもよい。ある実施の形態では、たとえば、高い不鮮明さ、暗い画像、低品質画像、アーカイブ画像など、ある特性をもつ画像は、そのような画像が１つまたは複数の第１の画像に一致する特性を有する場合であっても、１つまたは複数の第２の画像から除外され得る。 In another example, if the user has previously ignored the suggestion of choosing a type of photo, eg, a photo that does not depict a human face, such an image is one or more second images. Can be excluded from. In another example, if the user data indicates that the user will generate a photo collage containing images of a particular individual, such as a family member, pet, etc., then the image representing that individual is one or more second images. May be selected in, and images that do not depict the individual may be excluded. In certain embodiments, an image having certain properties, such as, for example, high blur, dark image, low quality image, archived image, has the property that such image matches one or more first images. Even in some cases, it may be excluded from one or more second images.
ある実施の形態では、たとえば、ユーザが画像ベースの創作物がビデオであることを示すとき、１つまたは複数の第２の画像は、他の画像ベースの創作物、たとえば、画像コラージュのために除外され得る、動きを伴うビデオクリップまたは画像を含み得る。ある実施の形態では、ビデオクリップは、非視覚パラメータ、たとえば、ビデオクリップの長さ、ビデオクリップの決定された音声部分に対するセマンティック概念またはラベルなどに基づいて選択され得る。ブロック２１６の後にはブロック２１８が続くことができる。 In one embodiment, for example, when the user indicates that the image-based creation is a video, the one or more second images may be for another image-based creation, such as an image collage. It may include moving video clips or images that may be excluded. In certain embodiments, the video clip may be selected based on non-visual parameters such as the length of the video clip, a semantic concept or label for a determined audio portion of the video clip, and the like. Block 216 can be followed by block 218.
ブロック２１８では、ユーザインターフェイスが表示させられる。たとえば、ユーザインターフェイスは、クライアントデバイス１２０〜１２４のいずれかによって表示され得る。ユーザインターフェイスは、１つまたは複数の第２の画像を含み得る。例えば、ユーザインターフェイスは、例えば、画像のグリッドの表示を含むカードまたはオーバーレイとして表示されてもよい。ある実施の形態では、１つまたは複数の第２の画像は、これらの画像が提案された選択であることの表示とともに表示され得、たとえば、ユーザインターフェイスの「提案」セクションに表示され得る。１つまたは複数の第２の画像は、ユーザ選択可能である。ある実施の形態では、ユーザインターフェイスはまた、ユーザが選択可能な他の画像、たとえば、現在の時間の直前の特定の時間内にキャプチャまたは受信されて、１つまたは複数の第２の画像には含まれない最近の画像を含み得る。ある実施の形態では、他の画像は、「提案」セクションとは別個のユーザインターフェイスのセクション、たとえば、「最近」セクションにグループ化され得る。ある実施の形態では、現在選択されている画像は、ユーザインターフェイスのセクション、たとえば「選択トレイ」セクションに表示され得る。「選択トレイ」は、第１の画像およびユーザによる選択に利用できる１つまたは複数の追加の画像を含むユーザインターフェイスの他の部分とは別個であってもよい。ブロック２１８の後には、ブロック２２０が続くことができる。 At block 218, the user interface is displayed. For example, the user interface may be viewed by any of the client devices 120-124. The user interface may include one or more second images. For example, the user interface may be displayed as, for example, a card or overlay containing a display of a grid of images. In certain embodiments, one or more second images may be displayed with an indication that these images are the proposed selections, for example, in the "suggestions" section of the user interface. The one or more second images are user selectable. In certain embodiments, the user interface is also captured or received within a particular time immediately preceding the current time, for other images that the user can select, such as one or more second images. It may include recent images that are not included. In one embodiment, other images may be grouped into a section of the user interface that is separate from the "suggestions" section, eg, the "recent" section. In certain embodiments, the currently selected image may be displayed in a section of the user interface, such as the "Selection Tray" section. The "selection tray" may be separate from the first image and other parts of the user interface that contain one or more additional images available for user selection. Block 218 can be followed by block 220.
ブロック２２０では、さらなる画像選択が受信されたかどうかが判定される。例えば、さらなる画像選択は、第２のユーザ入力を介して受信され得る。ある実施の形態では、さらなる画像選択によって示される画像は、１つまたは複数の第２の画像のうちの少なくとも１つを含み得る。ある実施の形態では、さらなる画像選択によって示される画像は、１つまたは複数の最近の画像のうちの少なくとも１つを含み得る。さらなる画像選択が受信された場合、方法はブロック２１４に進む。ある実施の形態では、画像ライブラリ内の１つまたは複数の第３の画像が識別され得る。たとえば、１つまたは複数の第３の画像は、さらなる画像選択から、たとえば、１つまたは複数の第２の画像から、および/またはユーザインターフェイスに表示された他の画像から、選択された画像の特性を決定することに基づいて識別され得る。 At block 220, it is determined whether additional image selection has been received. For example, further image selection may be received via a second user input. In certain embodiments, the image represented by further image selection may include at least one of one or more second images. In certain embodiments, the image represented by further image selection may include at least one of one or more recent images. If more image selections are received, the method proceeds to block 214. In certain embodiments, one or more third images in the image library may be identified. For example, one or more third images may be selected from further image selections, eg, from one or more second images, and / or from other images displayed in the user interface. It can be identified based on determining its properties.
１つまたは複数の第３の画像は、（第１のユーザ入力において選択された）第１の画像の特性と第２のユーザ入力を介して選択された画像とのマッチングによって識別され得る。たとえば、１つまたは複数の第３の画像は、第３の画像の各々が、選択された画像の特性に一致する少なくとも１つの特性、たとえば、第１の画像のうちの１つまたは複数に関連付けられ、第２の画像のうちの１つまたは複数に関連付けられる少なくとも１つの特性を有するように識別され得る。例えば、（第１のユーザ入力によって選択された）第１の画像が「ビーチにいるジョイスとアンドリュー」（例えば、ラベル「ジョイス」、「アンドリュー」、「ビーチ」に関連する）を示し、（第２のユーザ入力によって選択された）選択された画像が「ジョイスとアンドリュー」を示す場合、１つまたは複数の第３の画像を、ジョイスおよびアンドリューの両方を示す画像ライブラリ内の画像として識別することができる。この例では、第３の画像を識別しながら「ビーチ」特性は考慮されない。 One or more third images can be identified by matching the characteristics of the first image (selected in the first user input) with the image selected via the second user input. For example, one or more third images associate each of the third images with at least one characteristic that matches the characteristics of the selected image, eg, one or more of the first images. And can be identified as having at least one property associated with one or more of the second images. For example, the first image (selected by the first user input) shows "Joyce and Andrew on the beach" (eg, related to the labels "Joyce", "Andrew", "Beach") and (the first If the selected image (selected by user input in 2) indicates "Joyce and Andrew", identify one or more third images as images in the image library showing both Joyce and Andrew. Can be done. In this example, the "beach" property is not considered while identifying the third image.
異なる実施の形態では、１つまたは複数の第３の画像は、１つまたは複数の第２の画像を選択するために使用されるものと同様の技術を使用して選択され得る。１つまたは複数の第３の画像が識別される実施の形態では、ユーザインターフェイスは、１つまたは複数の第３の画像を含めるように更新され得る。たとえば、ユーザインターフェイスを更新することは、１つまたは複数の第２の画像の代わりに、またはそれに加えて１つまたは複数の第３の画像を表示するために「提案」セクションを更新することを含み得る。ある実施の形態では、１つまたは複数の第３の画像を識別すると、「提案」セクションは、ユーザインターフェイスの他のセクション（たとえば、「最近」）より高い優先度で表示され得る。 In different embodiments, the one or more third images may be selected using a technique similar to that used to select the one or more second images. In embodiments where one or more third images are identified, the user interface may be updated to include one or more third images. For example, updating the user interface may update the "Suggestions" section to display one or more third images in place of or in addition to one or more second images. Can include. In certain embodiments, identifying one or more third images may display the "suggestion" section with a higher priority than the other sections of the user interface (eg, "recent").
ある実施の形態では、画像の追加の選択を示すさらなるユーザ入力が受信され得（例えば、ブロック２１４〜２２０の反復による）、それに応答して、ユーザインターフェイスは、画像ライブラリ内の画像の画像特性と、ユーザ入力によって選択された画像の画像特性とのマッチングに基づいて、追加の画像を含むように更新され得る。さらなる画像選択が受信されない場合、ブロック２２０の後にブロック２２２が続くことができる。 In certain embodiments, additional user input indicating additional selection of images may be received (eg, by repeating blocks 214-220), and in response, the user interface may interact with the image characteristics of the image in the image library. , Can be updated to include additional images based on matching with the image characteristics of the image selected by user input. If no further image selection is received, block 220 can be followed by block 222.
ブロック２２２では、１つまたは複数のアクション（たとえば、オペレーション）のユーザ選択が、たとえば、ユーザインターフェイスを介して受け取られる。ユーザが選択したアクションは、ユーザが選択した画像を使用して実行される。例えば、ユーザは、選択された画像、例えば、第１のユーザ入力、第２のユーザ入力、およびもしあればその後のユーザ入力によって示される画像選択に基づいて、画像ベースの創作物（例えば、コンテンツの創作物）が生成されることを示すことができる。例えば、画像ベースの創作物は、画像コラージュ、選択された画像を例えばショートビデオクリップまたはスライドショーとして示すビデオ、選択された画像を含む画像アルバム、または選択された画像を含む、印刷された出版物、例えばフォトブックを含み得る。 At block 222, user selection of one or more actions (eg, operations) is received, for example, through the user interface. The user-selected action is performed using the user-selected image. For example, the user can create an image-based creation (eg, content) based on the selected image, eg, the image selection indicated by the first user input, the second user input, and the subsequent user input, if any. It can be shown that the creation of) is produced. For example, image-based creations include image collages, videos showing selected images as, for example, short video clips or slideshows, image albums containing selected images, or printed publications containing selected images. For example, it may include a photo book.
別の例では、ユーザによって選択された１つまたは複数のアクションは、選択された画像をターゲットソフトウェアアプリケーション、たとえばメッセージングアプリケーション、画像共有アプリケーション、ソーシャルネットワーキングアプリケーション、金融アプリケーションなどに提供することとすることができる。この例では、選択された画像は、例えば、アプリケーションプログラミングインターフェイス（ＡＰＩ）を介して、ターゲットソフトウェアアプリケーションに提供される。ある実施の形態では、選択された画像は、画像をターゲットソフトウェアアプリケーションに提供する前に修正され得る（例えば、自動的に強化され、トリミングされ、ファイルサイズが縮小される等。）。 In another example, one or more actions selected by the user may provide the selected image to a target software application, such as a messaging application, an image sharing application, a social networking application, a financial application, and so on. it can. In this example, the selected image is provided to the target software application, for example, via an application programming interface (API). In certain embodiments, the selected image can be modified before the image is served to the target software application (eg, automatically enhanced, cropped, reduced file size, etc.).
ある実施の形態では、ユーザによって選択された１つまたは複数のアクションは、たとえば、画像ライブラリ内の画像に対する修正を実行するための、画像ライブラリアプリケーションのためのアクションであり得る。例えば、ユーザが、互いまたは他の画像の複製を含む画像を選択（例えば、全てが１つまたは複数の他の選択された画像に類似する主題を描写し、類似のメタデータに関連付けられるか、または、ライブラリ内の１つまたは複数の他の画像に類似する主題を描写する画像を選択）した場合、アクションは、複製を削除することであり得る。例えば、他の複製画像よりもスコアの低い画像特性を有する複製画像は、例えば、不鮮明さ、露出、色ノイズ、画像境界に対するオブジェクト位置などの視覚的特性に基づいて削除することができる。別の例では、ユーザが１つまたは複数の共通特性（例えば、「暗」、「非強調」である）をそれぞれ有する画像を選択する場合、アクションは、たとえば、画像フィルタを適用することによって、画像強調のためにトレーニングされた機械学習モデルを使用して画像を修正することによって等の、たとえば、画像強調技法を使用してピクセル値を修正することによる、選択された画像の自動強調を実行することなどであり得る。 In certain embodiments, the one or more actions selected by the user can be, for example, an action for an image library application for performing modifications to an image in an image library. For example, the user selects images that contain duplicates of each other or other images (eg, all portraying a subject similar to one or more other selected images and associated with similar metadata, or Alternatively, if selected an image that depicts a subject similar to one or more other images in the library), the action could be to remove the duplicate. For example, a duplicate image having image characteristics with a lower score than other duplicate images can be removed based on visual characteristics such as blurring, exposure, color noise, and object position with respect to the image boundary. In another example, if the user selects an image that has one or more common characteristics (eg, "dark", "non-enhanced"), the action is, for example, by applying an image filter. Perform automatic enhancement of selected images, for example by modifying pixel values using image enhancement techniques, such as by modifying an image using a machine learning model trained for image enhancement. It can be something like that.
ある実施の形態では、アクションは、選択された画像をアーカイブすることであってよく、これにより、選択された画像が、画像ライブラリの画像のうちの１つまたは複数のビュー、たとえば、メインビューから隠される。例えば、選択された画像に関連するラベルは、選択された画像がアーカイブされていることを示すように更新され得る。アーカイブされた画像は、例えば、ユーザ入力を介して、アーカイブされた画像を見るためのコマンドが受信された場合に、表示され得る。ある実施の形態では、アーカイブされる画像に関連する画像メタデータまたはラベルは、画像がアーカイブされることを示すために更新され得る。そのようなメタデータまたはラベルは、たとえば、画像ライブラリ内の画像のメインビューを生成する際に、メインビューからアーカイブされた画像を除外するために使用され得る。 In certain embodiments, the action may be to archive the selected images so that the selected images are from one or more views of the images in the image library, such as the main view. Hidden. For example, the label associated with the selected image may be updated to indicate that the selected image is archived. The archived image may be displayed, for example, when a command to view the archived image is received via user input. In certain embodiments, the image metadata or label associated with the archived image may be updated to indicate that the image will be archived. Such metadata or labels can be used, for example, to exclude archived images from the main view when generating the main view of the images in the image library.
ある実施の形態では、１つまたは複数のアクションは、ユーザインターフェイスに表示された提案されたアクション要素に対応するアクションを含み得る。これらの実施の形態では、提案されたアクション要素は、選択された画像、たとえば、第１の画像のうちの１つまたは複数および第２の画像のうちの１つまたは複数に基づいて決定され得る。ある実施の形態では、提案されたアクション要素に関連付けられたアクションは、選択された画像をアーカイブすること、選択された画像を削除すること（例えば、画像ライブラリから選択された画像を除去すること、ストレージから画像を削除することなど。）、または選択された画像の自動拡張を実行することのうちの１つまたは複数を含み得る。 In certain embodiments, one or more actions may include actions corresponding to the proposed action elements displayed in the user interface. In these embodiments, the proposed action element may be determined based on the selected image, eg, one or more of the first image and one or more of the second image. .. In certain embodiments, the action associated with the proposed action element is archiving the selected image, deleting the selected image (eg, removing the selected image from the image library). It may include one or more of deleting images from storage, etc.), or performing auto-expansion of selected images.
例えば、選択された画像は「不鮮明な」という画像特性を有すると判定され得る。決定に基づいて、提案されたアクション要素、例えば「全ての不鮮明を削除」が表示される。ユーザが提案されたアクション要素を選択した場合、対応するアクションは、例えば、デバイスによって実行されてもよい。ある実施の形態では、選択された画像に一致する追加の画像、たとえば、ライブラリ内の他の不鮮明な画像であって、選択された画像と同じ場所またはその近くで、かつ同様の時間に撮影された他の画像を含めることができ、選択されたアクションは、ユーザ選択された画像と、選択された画像に一致する追加の画像とに対して実行される。 For example, the selected image may be determined to have an image characteristic of "blurred". Based on the decision, the suggested action element, such as "Remove All Blurred", is displayed. If the user selects the proposed action element, the corresponding action may be performed, for example, by the device. In certain embodiments, additional images that match the selected image, such as other blurry images in the library, are taken at or near the selected image and at similar times. Other images can be included and the selected action is performed on the user-selected image and additional images that match the selected image.
方法２００は、図２の様々なブロックを参照して説明されてきたが、本開示で説明する技術は、図２のブロックのいくつかを実行せずに実行され得ることを理解されたい。ある実施の形態では、図２に示されるブロックのうちの１つまたは複数が組み合わされ得る。例えば、ブロック２１４，２１６および２１８のうちの２つ以上が組み合わされてもよい。様々な実施の形態では、方法２００のブロックのうちのいくつかは、並列に、または図２に示されるものとは異なる順序で実行され得る。ある実施の形態では、方法２００は、クライアントデバイス、たとえば、クライアントデバイス１２０〜１２４のうちの１つまたは複数によって実行される。ある実施の形態では、方法２００は、サービスデバイス、たとえば、サーバデバイス１０４によって実行される。ある実施の形態では、方法２００は、クライアントデバイスとサーバデバイスとの組合せによって実行される。例えば、ある実施の形態では、ブロック２０２，２０４，２０６，２１２，２１８および２２０は、クライアントデバイスによって実行され、ブロック２１４，２１６および２２２は、サーバデバイスによって実行される。例えば、このようなアプローチは、クライアントデバイス１２０が、例えば、画像をプログラム的に分析することによって画像特性を決定する能力が限られている場合、画像データを記憶する記憶容量が限られている場合などに有用であり得る。 Although method 200 has been described with reference to the various blocks of FIG. 2, it should be understood that the techniques described herein can be performed without performing some of the blocks of FIG. In certain embodiments, one or more of the blocks shown in FIG. 2 may be combined. For example, two or more of blocks 214, 216 and 218 may be combined. In various embodiments, some of the blocks of Method 200 may be performed in parallel or in a different order than that shown in FIG. In certain embodiments, the method 200 is performed by one or more of the client devices, eg, client devices 120-124. In certain embodiments, method 200 is performed by a service device, such as a server device 104. In certain embodiments, method 200 is performed by a combination of client and server devices. For example, in some embodiments, blocks 202, 204, 206, 212, 218 and 220 are executed by the client device and blocks 214, 216 and 222 are executed by the server device. For example, such an approach is when the client device 120 has limited storage capacity to store image data, for example, when the ability to determine image characteristics by programmatically analyzing images is limited. Can be useful for such things.
図３は、ある実施の形態による、画像選択提案を提供するための方法３００の一例を示すフロー図である。ある実施の形態では、方法３００は、たとえば、図１に示されるように、サーバシステム１０２上で実施され得る。ある実施の形態では、方法３００の一部または全部は、図１に示されるように、１つまたは複数のクライアントデバイス１２０，１２２，１２４または１２６上で、１つまたは複数のサーバデバイス上で、および/またはサーバデバイスとクライアントデバイスとの両方上で実施され得る。説明される例では、実施システムは、１つまたは複数のデジタルプロセッサまたは処理回路（「プロセッサ」）と、１つまたは複数の記憶デバイス（例えば、データベース１０６または他の記憶装置）とを含む。ある実施の形態では、１つまたは複数のサーバおよび/またはクライアントの異なる構成要素は、方法３００の異なるブロックまたは他の部分を実行することができる。いくつかの例では、第１のデバイスは、方法３００のブロックを実行するものとして説明される。ある実施の形態は、結果またはデータを第１のデバイスに送信することができる１つまたは複数の他のデバイス（例えば、他のクライアントデバイスまたはサーバデバイス）によって実行される方法３００の１つまたは複数のブロックを有することができる。 FIG. 3 is a flow chart showing an example of a method 300 for providing an image selection proposal according to an embodiment. In certain embodiments, method 300 can be implemented on the server system 102, for example, as shown in FIG. In certain embodiments, some or all of the method 300 is on one or more client devices 120, 122, 124 or 126, on one or more server devices, as shown in FIG. And / or can be implemented on both server and client devices. In the examples described, the implementation system includes one or more digital processors or processing circuits (“processors”) and one or more storage devices (eg, database 106 or other storage devices). In certain embodiments, different components of one or more servers and / or clients may execute different blocks or other parts of Method 300. In some examples, the first device is described as performing the block of method 300. One embodiment is one or more of methods 300 performed by one or more other devices (eg, other client or server devices) capable of transmitting the results or data to the first device. Can have blocks of.
ある実施の形態では、方法３００または当該方法の一部は、システムによって自動的に開始され得る。ある実施の形態では、実施システムは第１のデバイスである。例えば、本方法（またはその部分）は、定期的に実行することができ、あるいは１つまたは複数の特定のイベントまたは条件、例えば、アプリケーションがユーザによって開始されること、画像がユーザによって選択されること、および/または本方法によって読み取られる設定において指定され得る１つまたは複数の他の条件が発生することに基づいて実行することができる。ある実施の形態では、そのような条件は、ユーザの格納されたカスタム選好においてユーザによって指定され得る。 In certain embodiments, method 300 or part of the method may be initiated automatically by the system. In certain embodiments, the implementation system is the first device. For example, the method (or part thereof) can be performed on a regular basis, or one or more specific events or conditions, such as the application being started by the user, the image being selected by the user. It can be done based on that and / or one or more other conditions that may be specified in the settings read by this method. In certain embodiments, such conditions may be specified by the user in the user's stored custom preferences.
一例では、クライアントデバイスは、カメラ、携帯電話、スマートフォン、タブレットコンピュータ、ウェアラブルデバイス、または、ユーザによるクライアントデバイスへのコンテンツ入力（たとえば、画像キャプチャ）およびユーザ入力（例えば、タッチスクリーンを使用すること、ジェスチャを使用すること、マウスまたは他のポインティングデバイスを使用すること、キーボードを使用することなど。）を受信することができ、方法３００を実行することができる他のクライアントデバイスとすることができる。別の例では、クライアント装置またはサーバ装置が方法３００を実行することができる。ある実施の形態は、ユーザ入力に基づいて方法３００を開始することができる。ユーザ（例えば、オペレータまたはエンドユーザ）は、例えば、ユーザインターフェイス、例えば、アプリケーションユーザインターフェイスまたは他のユーザインターフェイスから方法３００の開始を選択してもよい。ある実施の形態では、方法３００は、クライアントデバイスによって実施され得る。ある実施の形態では、方法３００は、サーバデバイスによって実施され得る。 In one example, the client device is a camera, mobile phone, smartphone, tablet computer, wearable device, or user input to the client device (eg, image capture) and user input (eg, using a touch screen, gesture). , Using a mouse or other pointing device, using a keyboard, etc.) can be received and can be any other client device capable of performing method 300. In another example, the client device or server device can perform method 300. In certain embodiments, method 300 can be initiated based on user input. The user (eg, operator or end user) may choose to start Method 300 from, for example, a user interface, eg, an application user interface or another user interface. In certain embodiments, method 300 can be performed by a client device. In certain embodiments, method 300 can be performed by a server device.
ブロック３０２において、方法３００の実施において、ユーザデータを使用するためにユーザの同意（例えば、ユーザ許可）が得られたかどうかがチェックされる。たとえば、ユーザデータは、クライアントデバイスを使用してユーザによってキャプチャされた画像、たとえば、クライアントデバイスを使用してユーザによって記憶またはアクセスされた画像、画像メタデータ、画像共有アプリケーションの使用に関連するユーザデータ、メッセージングアプリケーションの使用に関連するユーザデータ、金融アプリケーションの使用に関連するユーザデータ、ソーシャルネットワーキングアプリケーションの使用に関連するユーザデータ、ユーザ選好、ユーザバイオメトリック情報、ユーザ特性（同一性、名前、年齢、性別、職業など。）、ユーザのソーシャルネットワークおよび連絡先に関する情報、ソーシャルおよび他の種類のアクションおよびアクティビティ、ユーザによって作成または提出されたコンテンツ、格付け、および意見、ユーザの現在位置、履歴ユーザデータ、ユーザによって生成、受信、および/またはアクセスされた画像、ユーザによって閲覧または共有された画像などを含み得る。本明細書で説明する方法の１つまたは複数のブロックは、ある実施の形態では、そのようなユーザデータを使用し得る。 At block 302, in the implementation of method 300, it is checked whether the user's consent (eg, user permission) has been obtained to use the user data. For example, user data is images captured by the user using the client device, such as images stored or accessed by the user using the client device, image metadata, user data related to the use of image sharing applications. , User data related to the use of messaging applications, user data related to the use of financial applications, user data related to the use of social networking applications, user preferences, user biometric information, user characteristics (identity, name, age, Gender, occupation, etc.), information about the user's social networks and contacts, social and other types of actions and activities, content created or submitted by the user, ratings and opinions, the user's current location, historical user data, It may include images generated, received and / or accessed by the user, images viewed or shared by the user, and the like. One or more blocks of the methods described herein may use such user data in certain embodiments.
方法３００においてユーザデータが使用され得る関連ユーザからユーザ同意が得られた場合、ブロック３０４において、本明細書の方法のブロックは、それらのブロックに関して記載されたようなユーザデータの可能な使用を用いて実施され得ると判定され、方法はブロック３１２に続く。ユーザの同意が取得されていない場合、ブロックがユーザデータを使用せずに実施されるべきであるとブロック３０６において決定され、方法はブロック３１２に続く。ある実施の形態では、ユーザの同意が得られていない場合、ブロックは、ユーザデータを使用せずに、合成データおよび/または汎用データもしくは公にアクセス可能かつ公的に使用可能なデータを用いて実施される。ユーザの同意が取得されていない場合、方法３００は実行されない。 User data may be used in method 300 If user consent is obtained from the relevant user, then in block 304, the blocks of the methods herein use the possible use of user data as described for those blocks. It is determined that it can be performed, and the method follows block 312. If no user consent has been obtained, it is determined in block 306 that the block should be performed without the use of user data, and the method follows block 312. In certain embodiments, without user consent, the block uses synthetic data and / or general purpose data or publicly accessible and publicly available data, without the use of user data. Will be implemented. If the user's consent has not been obtained, method 300 will not be executed.
方法３００のブロック３１２において、ターゲットソフトウェアアプリケーションを示すコンテキスト情報が決定される。たとえば、方法３００が、ユーザが画像ベースの創作物、たとえば、画像アルバム、画像コラージュ、ビデオ、または印刷された出版物を生成することを可能にする画像ライブラリアプリケーションの一部として実施されるとき、コンテキスト情報は、画像創作物のタイプを含み得る。 At block 312 of method 300, contextual information indicating the target software application is determined. For example, when Method 300 is performed as part of an image library application that allows a user to generate an image-based creation, such as an image album, image collage, video, or printed publication. The contextual information may include the type of image creation.
別の例では、方法３００が、たとえば、アプリケーションプログラミングインターフェイス（ＡＰＩ）を介して他のアプリケーションとの対話をサポートする画像ライブラリアプリケーションの一部として実施されるとき、コンテキスト情報を決定することは、画像ライブラリアプリケーションを呼び出すアプリケーションのアプリケーションタイプ、たとえば、画像共有アプリケーション（例えば、スタンドアロン型画像共有アプリケーション、ソーシャルネットワーキングアプリケーションなど。）、金融アプリケーション（例えば、経費管理アプリケーション、請求書発行アプリケーションなど。）、メッセージアプリケーション（例えば、チャットまたはインスタントメッセージングアプリケーション、電子メールアプリケーション、コラボレーションアプリケーションなど。）を決定することを含む。本例では、画像ライブラリアプリケーションを呼び出すアプリケーションが、ターゲットソフトウェアと呼ばれる。 In another example, determining contextual information is an image when Method 300 is performed, for example, as part of an image library application that supports interaction with other applications via an application programming interface (API). Application types of applications that call library applications, such as image sharing applications (eg, stand-alone image sharing applications, social networking applications, etc.), financial applications (eg, expense management applications, billing applications, etc.), messaging applications (eg, For example, determining a chat or instant messaging application, an email application, a collaboration application, etc.). In this example, the application that calls the image library application is called the target software.
さらに、ある実施の形態では、コンテキスト情報を決定することは、ターゲットソフトウェアアプリケーションから、選択されるべきいくつかの画像、たとえば、１つの画像、２つの画像などを受信することを含み得る。別の例では、そのようなコンテキスト情報は、選択されるべき画像のタイプ、たとえば、動きを伴う画像（たとえば、ビデオクリップ）、特定のアスペクト比および/または向きの画像（例えば、正方形、縦向き、横向きなど。）、静止画像、動画像などを含み得る。 Further, in certain embodiments, determining contextual information may include receiving several images to be selected, such as one image, two images, etc., from the target software application. In another example, such contextual information is the type of image to be selected, such as an image with motion (eg, a video clip), an image with a particular aspect ratio and / or orientation (eg, square, portrait). , Sideways, etc.), still images, moving images, etc. may be included.
別の例では、ユーザの同意が得られた場合、コンテキスト情報を決定することは、メッセージング会話（たとえば、電子メール受信者）における参加者の識別情報（例えば、ユーザ名、ログインID等のユーザ識別子。）、画像共有アプリケーションを介して共有されている以前の画像の特性（例えば、風景画像、食品画像、自己画像など。）などを受信することを含み得る。別の例では、コンテキスト情報は、例えば経費報告を提出する、請求書を生成するなど、金融アプリケーションなどのターゲットソフトウェアアプリケーションを使用して実行されるユーザ活動を含み得る。 In another example, with the consent of the user, determining the contextual information is the identification of the participant in the messaging conversation (eg, the email recipient) (eg, the user identifier such as the username, login ID, etc.) ), Receiving characteristics of previous images shared via an image sharing application (eg, landscape images, food images, self-images, etc.) and the like. In another example, contextual information may include user activity performed using a target software application, such as a financial application, such as filing an expense report or generating an invoice.
ある実施の形態では、ターゲットソフトウェアアプリケーションは、ＡＰＩを介してコンテキスト情報を提供することができる。ある実施の形態では、ユーザの同意が得られた場合、コンテキスト情報は、１つまたは複数の画像特性、たとえば、ユーザの画像ライブラリ内の画像と関連付けられた画像特性とマッチすることができる「自己画像」、「レシート」、「休暇」などとして提供され得る。そのような画像特性は、ユーザの同意が得られた場合、画像コンテンツ（たとえば、描写された画像特徴）および/または画像メタデータ（例えば、キャプチャの時間、キャプチャの位置、キャプチャしたカメラの属性および設定など。）に基づくことができる。ある実施の形態では、コンテキスト情報は、セマンティック概念、たとえば、「経費報告」、「アンドリューとのチャット会話」などとして提供され得る。 In certain embodiments, the target software application can provide contextual information via APIs. In certain embodiments, with the consent of the user, the contextual information can match one or more image characteristics, eg, an image characteristic associated with an image in the user's image library, "self". It may be provided as an "image", "receipt", "vacation", etc. Such image characteristics, with the consent of the user, include image content (eg, depicted image features) and / or image metadata (eg, capture time, capture position, captured camera attributes and It can be based on settings etc.). In certain embodiments, contextual information may be provided as semantic concepts such as "expense reporting", "chat conversations with Andrew", and the like.
ユーザが同意を提供するとき、コンテキスト情報は、ターゲットソフトウェアアプリケーションからのアプリケーションコンテキストを含み得る。例えば、アプリケーションコンテキストは、メッセージングアプリケーションを介して行われる会話に関連する会話またはセマンティック概念の概要、金融アプリケーションにおける準備中の経費報告の期間などを含み得る。例えば、アプリケーションコンテキストは、「この会話は、バリにおける休暇に関する」「２０１７年１１月の経費報告」等を含むことができる。ブロック３１２の後にはブロック３１４が続くことができる。 When the user provides consent, the context information may include application context from the target software application. For example, an application context may include an overview of conversations or semantic concepts related to conversations that take place through a messaging application, a period of expense reporting in preparation in a financial application, and so on. For example, the application context can include "this conversation is about vacations in Bali", "expense report for November 2017" and the like. Block 312 can be followed by block 314.
ブロック３１４では、１つまたは複数の第１の画像がコンテキスト情報に基づいて識別される。例えば、第１の画像は、コンテキスト情報を画像ライブラリ内の画像の画像特性とマッチングすることに基づくことができる。ある実施の形態では、マッチングは、ターゲットアプリケーションのアプリケーションタイプに基づいてもよい。例えば、ブロック３１４の前に、特定の画像特性を特定の種類のアプリケーションに関連付けることができる。例えば、アプリケーションタイプが「財務アプリケーション」である場合、画像が「レシート」、「文書」、「スクリーンショット」などであることを示すラベルに関連付けられている画像は、、１つまたは複数の第１の画像として選択されてもよく、他の特性（例えば、ラベル）を有する画像、例えば、「写真」、「映像」等は除外され得る。 At block 314, one or more first images are identified based on contextual information. For example, the first image can be based on matching contextual information with the image characteristics of the image in the image library. In certain embodiments, matching may be based on the application type of the target application. For example, a particular image property can be associated with a particular type of application prior to block 314. For example, if the application type is "Financial Application", the image associated with the label indicating that the image is "Receipt", "Document", "Screenshot", etc. may be one or more first. Images with other characteristics (eg, labels), such as "photographs", "videos", etc., may be excluded.
ある実施の形態では、第１の画像は、コンテキスト情報内で指定された画像特性および/またはセマンティック概念を画像ライブラリ内の画像の画像特性にマッチングさせることに基づくことができる。たとえば、アプリケーションタイプがメッセージングアプリケーションであり、コンテキスト情報がメッセージング会話における参加者の識別情報を含み、たとえば、コンテキスト情報が、メッセージングアプリケーションによって提供されるチャット会話がユーザアンドリューおよびジョイスを含むことを指定する場合、アンドリューおよびジョイスのうちの少なくとも１つを示す画像が、第１の画像として選択され得る。 In certain embodiments, the first image can be based on matching the image characteristics and / or semantic concepts specified in the contextual information with the image characteristics of the image in the image library. For example, if the application type is a messaging application and the contextual information includes participant identification in a messaging conversation, for example, the contextual information specifies that the chat conversation provided by the messaging application includes User Andrew and Joyce. An image showing at least one of Andrew and Joyce may be selected as the first image.
別の例では、アプリケーションコンテキストが、メッセージングアプリケーションによって提供されるチャット会話はバリにおける休暇についてであることを指定する場合、１つまたは複数のセマンティック概念が、アプリケーションコンテキストに基づいて決定される。例えば、セマンティック概念は、「休暇」、「バリ」、「ビーチ」、「インドネシア」、「寺」等を含むことができる。アプリケーションコンテキストから決定されるか、またはアプリケーションコンテキスト内で指定されたセマンティック概念に関連付けられる画像は、第１の画像として選択され得る。別の例では、コンテキスト情報が２０１７年１１月の経費報告を指定する場合、２０１７年１１月に対応する関連したタイムスタンプを有するレシートの画像を第１の画像として選択することができる。ブロック３１４の後には、ブロック３１６が続くことができる。 In another example, if the application context specifies that the chat conversation provided by the messaging application is about vacation in Bali, one or more semantic concepts are determined based on the application context. For example, the semantic concept can include "vacation", "Bali", "beach", "Indonesia", "temple" and the like. An image determined from the application context or associated with a semantic concept specified within the application context may be selected as the first image. In another example, if the contextual information specifies an expense report for November 2017, the image of the receipt with the associated time stamp corresponding to November 2017 can be selected as the first image. Block 314 can be followed by block 316.
ブロック３１６では、第１の画像を含むユーザインターフェイスが表示される。たとえば、ユーザインターフェイスは、クライアントデバイス１２０〜１２４のいずれかによって表示され得る。ユーザインターフェイスは、ブロック２１８を参照して上述したユーザインターフェイスと同様に、ユーザが第１の画像のうちの１つまたは複数を選択することを可能にする。ブロック３１６の後には、ブロック３１８が続くことができる。 At block 316, a user interface containing the first image is displayed. For example, the user interface may be viewed by any of the client devices 120-124. The user interface allows the user to select one or more of the first images, similar to the user interface described above with reference to block 218. Block 316 can be followed by block 318.
ブロック３１８では、表示された画像（たとえば、第１の画像）のうちの１つまたは複数のユーザの選択が受け取られる。例えば、ブロック３１８は、図２のブロック２１２と同様であり得る。ブロック３１８の後には、ブロック３２０が続くことができる。 At block 318, the selection of one or more of the displayed images (eg, the first image) is received. For example, block 318 can be similar to block 212 in FIG. Block 318 can be followed by block 320.
ブロック３２０では、ブロック３１８で選択された画像の画像特性が、例えば、上述のブロック２１４と同様の方法で決定される。ブロック３２０の後には、ブロック３２２が続くことができる。 In block 320, the image characteristics of the image selected in block 318 are determined, for example, in the same manner as in block 214 described above. Block 320 can be followed by block 322.
ブロック３２２では、１つまたは複数の追加の画像が識別される。ある実施の形態では、追加の画像は、ブロック３２０で決定された画像特性に基づいて識別され得る。例えば、ユーザ選択がラベル「バリ」および「アンドリュー」に関連付けられた画像に対応する場合、ラベルのうちの少なくとも１つを有する追加画像が識別される。ある実施の形態では、追加の画像を識別することは、たとえば、上述のブロック３１４と同様に、ブロック３２０で決定された画像特性およびコンテキスト情報に基づく。ブロック３２２の後にブロック３２４が続くことができる。 At block 322, one or more additional images are identified. In certain embodiments, additional images can be identified based on the image characteristics determined in block 320. For example, if the user selection corresponds to an image associated with the labels "Bali" and "Andrew", then an additional image with at least one of the labels is identified. In certain embodiments, identifying additional images is based on image characteristics and context information determined in block 320, for example, similar to block 314 described above. Block 322 can be followed by block 324.
ブロック３２４において、ユーザインターフェイスは、１つまたは複数の追加の画像を含むように更新される。例えば、１つまたは複数の追加画像は、第１画像の代わりに、または第１画像に加えて表示されてもよい。ある実施の形態では、更新されたユーザインターフェイスは、ユーザによって選択された画像を含むセクション、たとえば「選択トレイ」セクションを含み得る。「選択トレイ」は、第１の画像およびユーザによる選択に利用できる１つまたは複数の追加の画像を含む、ユーザインターフェイスの他の部分とは別個であってもよい。他の特徴は、図２のブロック２１８に関して説明したものと同様に提供することができる。ブロック３２４の後には、ブロック３２６が続くことができる。 At block 324, the user interface is updated to include one or more additional images. For example, one or more additional images may be displayed in place of or in addition to the first image. In certain embodiments, the updated user interface may include a section containing images selected by the user, such as a "selection tray" section. The "selection tray" may be separate from other parts of the user interface, including a first image and one or more additional images available for user selection. Other features can be provided similar to those described for block 218 of FIG. Block 324 can be followed by block 326.
ブロック３２６では、さらなる画像選択がユーザから、たとえば更新されたユーザインターフェイスに表示された画像から受信されたかどうかが判定される。さらなる画像選択が受信された場合、方法はブロック３２０に進み、ユーザによって選択された追加画像の画像特性が決定される。ブロック３２０〜３２６を繰り返して、追加の画像選択提案をユーザに提供することができる。さらなる画像選択が受信されない場合、方法はブロック３２８に進む。 At block 326, it is determined whether additional image selection has been received from the user, eg, from an image displayed in an updated user interface. If further image selection is received, the method proceeds to block 320 to determine the image characteristics of the additional image selected by the user. Blocks 320-326 can be repeated to provide the user with additional image selection suggestions. If no further image selection is received, the method proceeds to block 328.
ブロック３２８では、ユーザによって選択された画像が、ターゲットソフトウェアアプリケーションに提供される。例えば、ターゲットソフトウェアアプリケーションが画像ライブラリアプリケーションである場合、選択された画像は、画像ベースの創作物に利用されるべき画像として提供されてもよい。別の例では、ターゲットソフトウェアアプリケーションが画像共有またはメッセージングアプリケーションである場合、選択された画像は、画像が画像共有またはメッセージングアプリケーションを介して１つまたは複数の他のデバイスに（たとえば、ネットワークを介して）送信するために利用可能となるように提供され得る。ある実施の形態では、選択された画像は、画像をターゲットソフトウェアアプリケーションに提供する前に、処理され、たとえば、自動的に強調され、トリミングされ、圧縮され、異なるフォーマットに変換され得る。ある実施の形態では、画像の処理は、ターゲットソフトウェアアプリケーションから受信されたコンテキスト情報に基づいてもよい。 At block 328, the image selected by the user is provided to the target software application. For example, if the target software application is an image library application, the selected image may be provided as an image to be utilized in an image-based creation. In another example, if the target software application is an image sharing or messaging application, the selected image will be sent to one or more other devices (eg, over a network) via the image sharing or messaging application. ) Can be provided to be available for transmission. In certain embodiments, the selected image can be processed, for example, automatically highlighted, cropped, compressed, and converted to a different format before the image is served to the target software application. In certain embodiments, image processing may be based on contextual information received from the target software application.
方法３００は、図３の様々なブロックを参照して説明されてきたが、本開示で説明する技術は、図３のブロックのいくつかを実行せずに実行され得ることを理解されたい。ある実施の形態では、図３に示されるブロックのうちの１つまたは複数が組み合わされ得る。例えば、ブロック３１４，３１６および３１８のうちの２つ以上が組み合わされてもよい。別の例では、ブロック３２０，３２２および３２４のうちの２つ以上が組み合わされてもよい。様々な実施の形態では、方法３００のブロックのうちのいくつかは、並列に、または図３に示されるものとは異なる順序で実行され得る。 Although method 300 has been described with reference to the various blocks of FIG. 3, it should be understood that the techniques described herein can be performed without performing some of the blocks of FIG. In certain embodiments, one or more of the blocks shown in FIG. 3 may be combined. For example, two or more of blocks 314, 316 and 318 may be combined. In another example, two or more of blocks 320, 322 and 324 may be combined. In various embodiments, some of the blocks of method 300 may be performed in parallel or in a different order than that shown in FIG.
ある実施の形態では、方法３００は、クライアントデバイス、たとえば、クライアントデバイス１２０〜１２４のうちの１つまたは複数によって実行される。ある実施の形態では、方法３００は、サービスデバイス、たとえば、サーバデバイス１０４によって実行される。ある実施の形態では、方法３００は、クライアントデバイスとサーバデバイスとの組合せによって実行される。例えば、ある実施の形態では、ブロック３０２，３０４，３０６，３１２，３１８，３２６および３２８は、クライアントデバイスによって実行され、ブロック３１４，３１６，３２０，３２２および３２４は、サーバデバイスによって実行される。例えば、このようなアプローチは、クライアントデバイス１２０が、例えば、画像をプログラム的に分析することによって画像特性を決定する能力が限られている場合、画像データを記憶する記憶容量が限られている場合などに有用であり得る。 In certain embodiments, method 300 is performed by a client device, eg, one or more of client devices 120-124. In certain embodiments, method 300 is performed by a service device, such as a server device 104. In certain embodiments, method 300 is performed by a combination of client and server devices. For example, in some embodiments, blocks 302, 304, 306, 312, 318, 326 and 328 are executed by the client device and blocks 314, 316, 320, 322 and 324 are executed by the server device. For example, such an approach is when the client device 120 has limited storage capacity to store image data, for example, when the ability to determine image characteristics by programmatically analyzing images is limited. Can be useful for such things.
図４は、ある実施の形態による、画像選択提案を含む例示的なユーザインターフェイス４００の概略図である。様々な実施の形態では、ユーザインターフェイス４００は、ディスプレイデバイスによって、たとえば、ある実施の形態では、図１のクライアントデバイス１２０、１２２、１２４および/または１２６の表示画面によって、またはサーバシステム１０２によって表示され得る。 FIG. 4 is a schematic diagram of an exemplary user interface 400, including an image selection proposal, according to an embodiment. In various embodiments, the user interface 400 is displayed by the display device, for example, in some embodiments, by the display screens of client devices 120, 122, 124 and / or 126 of FIG. 1, or by the server system 102. obtain.
図４に示されるように、ユーザの画像ライブラリの一部分がユーザインターフェイスに表示される。図４に示される例では、画像ライブラリからの９つの画像が示されている。図４は、１行当たり３つの正方形画像を有する画像の３つの行を示すが、利用可能な画面空間に基づいて、グリッド内の任意の数の行および列を使用することができる。さらに、異なる実施の形態は、他の構成または配列で画像を表示してもよい。ある実施の形態では、ライブラリからのより多いまたはより少ない画像が示され得る。ある実施の形態では、画像は、画像に関連付けられたタイムスタンプに基づいて、特定の順序、たとえば、時系列の逆の順序で編成され得る。 As shown in FIG. 4, a portion of the user's image library is displayed in the user interface. In the example shown in FIG. 4, nine images from the image library are shown. FIG. 4 shows three rows of an image with three square images per row, but any number of rows and columns in the grid can be used based on the available screen space. In addition, different embodiments may display images in other configurations or sequences. In certain embodiments, more or less images from the library may be shown. In certain embodiments, the images may be organized in a particular order, eg, in the reverse order of the time series, based on the time stamps associated with the images.
図４に示される例では、ユーザは、画像上のチェックマーク４０２によって示されるように、最上段の行の最も左の画像を選択した。図４から分かるように、ユーザによって選択された画像（例えば、第１の画像）は、写真ID（「Jane Doe（身元不明の女性の仮称）」）である。本明細書で説明する技術を使用して、ユーザ選択された画像に基づく１つまたは複数の提案された画像選択（たとえば、第２の画像）がユーザインターフェイスに表示される。例えば、提案された画像選択は、ユーザインターフェイス内の３つの他の画像上の対応するチェックマーク４０４，４０６および４０８によって示される。提案された画像選択は、写真ID（ＸＹＺ社のJohn Doe（身元不明の男性の仮称））の画像およびレシートの２つの画像に対応する。この例における提案された画像選択は、第１の画像（例えば、「文書」、「アイデンティティカード」）の画像特性を画像ライブラリ内の他の画像のものと一致させることに基づく。 In the example shown in FIG. 4, the user has selected the leftmost image in the top row, as indicated by the check mark 402 on the image. As can be seen from FIG. 4, the image selected by the user (eg, the first image) is a photo ID (“Jane Doe (tentative name for an unidentified woman)”). Using the techniques described herein, one or more proposed image selections (eg, a second image) based on the user-selected image are displayed in the user interface. For example, the proposed image selection is indicated by the corresponding checkmarks 404, 406 and 408 on three other images in the user interface. The proposed image selection corresponds to two images, a photo ID (John Doe of XYZ (tentative name for an unidentified man)) and a receipt. The proposed image selection in this example is based on matching the image properties of the first image (eg, "document", "identity card") with those of other images in the image library.
ユーザインターフェイス４００は、提案されたアクション要素４１０をさらに示す。図４に示される例では、提案されるアクションは、「文書およびレシートを選択する」ことである。ユーザが提案されたアクション要素４１０を選択すると、文書かつレシートであるユーザの画像ライブラリからの他の画像が、関連する画像特性に基づいて自動的に選択される。例えば、自動的に選択された画像は、そのような画像がスクロールされるか、またはそうでなければユーザインターフェイス４００のビューに表示されるようにユーザ入力を介して命令される場合、チェックマーク４０４，４０６および４０８と同様のチェックマークとともに表示され得る。ある実施の形態では、そのような自動的に選択された画像は、画像ライブラリ内の画像の表示とは別個のユーザインターフェイスの表示領域に表示され得る。ある実施の形態では、他のまたは追加のアクション、たとえば、「選択画像をアーカイブ」、「選択画像を自動強化」、「選択画像を削除」などは、提案されたアクション要素内に示され得る。 The user interface 400 further illustrates the proposed action element 410. In the example shown in FIG. 4, the suggested action is to "select documents and receipts." When the user selects the proposed action element 410, other images from the user's image library, which are documents and receipts, are automatically selected based on the associated image characteristics. For example, if an automatically selected image is instructed via user input that such an image is scrolled or otherwise displayed in the view of user interface 400, checkmark 404. , 406 and 408 may be displayed with similar checkmarks. In certain embodiments, such automatically selected images may be displayed in a display area of the user interface separate from the display of the images in the image library. In certain embodiments, other or additional actions, such as "archive selected image", "auto-enhance selected image", "delete selected image", etc., may be indicated within the proposed action element.
ある実施の形態では、提案されたアクションは、画像特性に基づいてよく、たとえば、異なる特定の提案されたアクションを、それぞれの特定の画像特性に関連付けることができる。例えば、ユーザが不鮮明な画像を選択した場合、提案されたアクションは、画像を削除することであり得る。別の例では、ユーザが暗い画像を選択した場合、提案されたアクションは、輝度またはコントラスト強調を適用することであり得る。ある実施の形態では、提案されるアクションは、複合アクション、たとえば「画像を選択する、画像フィルタを適用する、およびメッセージングアプリケーションを介して画像を共有する」であり得る。ユーザが提案されたアクション要素４１０を選択すると、対応するアクションが自動的に実行される。 In certain embodiments, the proposed actions may be based on image characteristics, for example, different specific proposed actions can be associated with each particular image characteristic. For example, if the user selects a blurry image, the suggested action could be to delete the image. In another example, if the user selects a dark image, the suggested action could be to apply brightness or contrast enhancement. In certain embodiments, the proposed action can be a composite action, such as "selecting an image, applying an image filter, and sharing an image through a messaging application." When the user selects the proposed action element 410, the corresponding action is automatically executed.
ユーザインターフェイス４００は、ユーザが、メモリ、処理リソース、および電力を含むデバイスリソースの削減された労力および削減された消費で画像を選択することを可能にする。例えば、ユーザがデバイス上で多くのスクロールおよび検索操作で手動で各ぼかし画像を選択しなければならない代わりに、ユーザインターフェイスは、ユーザが、単純かつ低減されたデバイス操作で提示された提案された選択を承認することを可能にし得る。さらに、ユーザインターフェイス４００に表示されないが、ユーザが選択した画像と一致する特性を有する画像ライブラリ内の他の画像は、ユーザが画像をスクロールし、各画像を手動で選択することなく選択することができる。 The user interface 400 allows the user to select an image with reduced effort and reduced consumption of device resources, including memory, processing resources, and power. For example, instead of having to manually select each blurred image on the device with many scrolling and searching operations, the user interface is a suggested selection presented by the user with simple and reduced device operation. Can be made possible to approve. In addition, other images in the image library that are not visible in the user interface 400 but have properties that match the image selected by the user can be selected by the user scrolling through the images without having to manually select each image. it can.
図５Ａは、ある実施の形態による、画像選択提案を提供する例示的なユーザインターフェイス５００の概略図である。様々な実施の形態では、ユーザインターフェイス５００は、ディスプレイデバイスによって、たとえば、ある実施の形態では、図１のクライアントデバイス１２０、１２２、１２４および/または１２６もしくはサーバシステム１０２の表示画面によって表示され得る。 FIG. 5A is a schematic diagram of an exemplary user interface 500 that provides an image selection proposal according to an embodiment. In various embodiments, the user interface 500 may be displayed by the display device, for example, in certain embodiments, by the client devices 120, 122, 124 and / or 126 of FIG. 1 or the display screen of the server system 102.
図５Ａに示すように、ユーザインターフェイス５００は、ユーザが選択した画像５０２を含む選択トレイと、追加の画像を選択してトレイに追加できることを示す２つのブランクスポットとを含む。ユーザインターフェイス５００は、ユーザが選択することができる画像を含む。例えば、ユーザは、ユーザインターフェイスの「最近」セクション（５０４）から１つまたは複数の最近の画像を選択することができる。ある実施の形態では、「最近」セクション内の画像は、ユーザの画像ライブラリ内の画像に関連するキャプチャタイムスタンプに基づいて、時間の降順に編成され得る。 As shown in FIG. 5A, the user interface 500 includes a selection tray containing a user-selected image 502 and two blank spots indicating that additional images can be selected and added to the tray. User interface 500 includes images that can be selected by the user. For example, the user can select one or more recent images from the "Recent" section (504) of the user interface. In certain embodiments, the images in the "Recent" section can be organized in descending order of time based on the capture timestamp associated with the image in the user's image library.
ユーザインターフェイス５００は、本明細書で説明する技術を使用して決定される提案された画像選択を含む提案５０６をさらに含む。図５に示される例では、提案された画像選択は、「提案」カードの形態で表示され、「最近」セクション５０４の上に重ねられる。異なる実施の形態では、提案された画像選択は、ユーザインターフェイスの異なるセクションに表示されてもよく、チェックマークまたは他の視覚的インジケータを使用して、提案された画像選択を示す等してもよい。ある実施の形態では、提案された画像選択は、ユーザインターフェイスの優先セクションに、たとえば、選択トレイの近くに表示され得る。 User interface 500 further includes Proposal 506, which includes proposed image selection determined using the techniques described herein. In the example shown in FIG. 5, the proposed image selection is displayed in the form of a "suggestion" card and is overlaid on the "recent" section 504. In different embodiments, the proposed image selection may appear in different sections of the user interface, checkmarks or other visual indicators may be used to indicate the proposed image selection, and so on. .. In certain embodiments, the proposed image selection may appear in the preferred section of the user interface, for example, near the selection tray.
図５Ａに示される例では、ユーザは花の画像５０２を選択した。ユーザ選択に基づいて、画像５０２の１つまたは複数の特性に一致する特性を有するユーザの画像ライブラリ内の他の画像が提案５０６に含まれる。例えば、画像５０２に関連付けられる位置と同様の位置でキャプチャされた花の他の画像、例えば、画像５１２，５１４および５１６が表示されてもよい。図５に示される例では、マッチング特性に基づく要約「森林の小道にある花」が、提案カード内のセクションヘッダ５１０として示されている。ヘッダ「森林の小道の友人」（５２０）を有する第２のセクションは、画像５２２，５２４および５２６の追加の画像提案を含む。追加の画像提案は、関連付けられた場所が「森林の小道」であることに基づく。異なる実施の形態では、任意の数の画像提案が提供され得る。ユーザインターフェイス５００は、ユーザが、最近セクション５０４から、および/または提案カード５０６から１つまたは複数の画像を選択することを可能にする。 In the example shown in FIG. 5A, the user selected flower image 502. Proposal 506 includes other images in the user's image library that have properties that match one or more of the properties of image 502, based on user selection. For example, other images of the flower captured at positions similar to those associated with image 502, such as images 512, 514 and 516, may be displayed. In the example shown in FIG. 5, a summary "flowers in a forest path" based on matching characteristics is shown as section header 510 in the proposal card. The second section with the header "Friends of the Forest Path" (520) contains additional image suggestions for images 522, 524 and 526. Additional image suggestions are based on the associated location being a "forest path". In different embodiments, any number of image suggestions may be provided. The user interface 500 allows the user to select one or more images from section 504 and / or suggestion card 506 recently.
ある実施の形態では、画像提案は、ユーザによってすでに選択された画像、たとえば、画像５０２との一致の程度に基づいて、ユーザインターフェイスにおいて編成（たとえば、グループ化）され得る。例えば、画像５１２〜５１６は、選択された画像５０２と同様の主題を含み、森林の小道でキャプチャされているので、提案カード５０６の第１の部分に示される。画像５２２〜５２６は、画像は森林の小道上でキャプチャされたが、画像は花を描いておらず選択された画像５０２とは異なるので、提案カード５０６の第２の部分に示される。 In certain embodiments, image proposals can be organized (eg, grouped) in the user interface based on the degree of matching with an image already selected by the user, eg, image 502. For example, images 521-516 contain the same subject as selected image 502 and are captured in a forest path and are therefore shown in the first part of proposal card 506. Images 522-526 are shown in the second part of the proposal card 506 because the image was captured on a forest path, but the image does not depict flowers and is different from the selected image 502.
図５Ｂは、ある実施の形態による、画像選択提案を提供する例示的なユーザインターフェイス５３０の概略図である。例えば、ユーザインターフェイス５３０は、ユーザがユーザインターフェイス５００を介して画像５２２および５１２を選択した後に表示することができる。追加の選択画像は、選択トレイの一部として示されている。画像５２２および５１２は、提案カードから除去される。追加の画像５２８は、例えば、画像ライブラリ内の画像の画像特性を選択された画像５０２，５１２および５２２とマッチングさせることに基づいて、提案カードに追加される。 FIG. 5B is a schematic view of an exemplary user interface 530 that provides an image selection proposal according to an embodiment. For example, the user interface 530 can be displayed after the user has selected images 522 and 512 via the user interface 500. Additional selection images are shown as part of the selection tray. Images 522 and 512 are removed from the proposed card. The additional image 528 is added to the proposal card, for example, based on matching the image characteristics of the images in the image library with the selected images 502, 512 and 522.
図５Ｃは、ある実施の形態による、画像選択提案を提供する例示的なユーザインターフェイス５４０の概略図である。図５Ｃに示されるように、画像ベースの創作物、たとえばピクチャコラージュ５４２は、ユーザによって選択された画像、たとえば画像５０２，５１２および５２２に基づいて生成された。ユーザインターフェイス５４０は、１つまたは複数の提案された画像を有する提案カードを含む。たとえば、ユーザインターフェイス５４０は、ユーザが、たとえば、提案カードからピクチャコラージュに画像をドラッグおよびドロップすることによって、提案された画像の１つまたは複数を選択し、ピクチャコラージュに画像を追加することを可能にし得る。 FIG. 5C is a schematic view of an exemplary user interface 540 that provides an image selection proposal according to an embodiment. As shown in FIG. 5C, image-based creations, such as picture collage 542, were generated based on images selected by the user, such as images 502, 512 and 522. User interface 540 includes a proposal card with one or more proposed images. For example, user interface 540 allows the user to select one or more of the proposed images and add the images to the picture collage, for example by dragging and dropping the images from the proposal card into the picture collage. Can be.
図６は、本明細書で説明する１つまたは複数の特徴を実施するために使用され得る例示的なデバイス６００のブロック図である。一例では、デバイス６００は、クライアントデバイス、たとえば、図１に示されるクライアントデバイス１１５のいずれかを実施するために使用され得る。あるいは、デバイス６００は、サーバデバイス、例えば、サーバ１０１を実施することができる。ある実施の形態では、デバイス６００は、クライアントデバイス、サーバデバイス、またはクライアントデバイスとサーバデバイスの両方を実施するために使用され得る。デバイス６００は、任意の適切なコンピュータシステム、サーバ、または上述の他の電子もしくはハードウェアデバイスであり得る。 FIG. 6 is a block diagram of an exemplary device 600 that can be used to implement one or more of the features described herein. In one example, device 600 can be used to implement any of the client devices, eg, the client device 115 shown in FIG. Alternatively, the device 600 can implement a server device, such as a server 101. In certain embodiments, the device 600 can be used to implement a client device, a server device, or both a client device and a server device. Device 600 can be any suitable computer system, server, or other electronic or hardware device described above.
本明細書で説明する１つまたは複数の方法は、任意のタイプのコンピューティングデバイス上で実行され得るスタンドアローンプログラム、ウェブブラウザ上で実行されるプログラム、モバイルコンピューティングデバイス（例えば、携帯電話、スマートフォン、タブレットコンピュータ、ウェアラブルデバイス（ウォッチ、アームバンド、ジュエリー、ヘッドウェア、仮想現実ゴーグルまたはメガネ、拡張現実ゴーグルまたはメガネ、ヘッドマウントディスプレイなど）、ラップトップコンピュータなど）上で実行されるモバイルアプリケーション（「app」）で実行され得る。一例では、クライアント/サーバアーキテクチャを使用することができ、例えば、（クライアントデバイスとして）モバイルコンピューティングデバイスは、ユーザ入力データをサーバデバイスに送信し、出力のための（例えば、表示のための）最終出力データをサーバから受信する。別の例では、すべての計算は、モバイルコンピューティングデバイス上のモバイルアプリ（および/または他のアプリ）内で実行することができる。別の例では、モバイルコンピューティングデバイスと１つまたは複数のサーバデバイスとの間で計算を分割することができる。 One or more of the methods described herein are stand-alone programs that can be run on any type of computing device, programs that run on a web browser, mobile computing devices (eg, mobile phones, smartphones). , Tablet computers, wearable devices (watches, armbands, jewelry, headwear, virtual reality goggles or glasses, augmented reality goggles or glasses, headmount displays, etc.), laptop computers, etc.), mobile applications ("app" ”) Can be executed. In one example, a client / server architecture can be used, for example, a mobile computing device (as a client device) sends user input data to a server device and is final for output (eg, for display). Receive output data from the server. In another example, all calculations can be performed within the mobile app (and / or other apps) on the mobile computing device. In another example, the computation can be split between the mobile computing device and one or more server devices.
ある実施の形態では、デバイス６００は、プロセッサ６０２と、メモリ６０４と、入出力（Ｉ／Ｏ）インターフェイス６０６とを含む。プロセッサ６０２は、プログラムコードを実行し、デバイス６００の基本動作を制御する１つまたは複数のプロセッサおよび/または処理回路であり得る。「プロセッサ」は、データ、信号、または他の情報を処理する任意の適切なハードウェアシステム、メカニズム、またはコンポーネントを含む。プロセッサは、（例えば、シングルコア、デュアルコア、またはマルチコア構成にある）１つまたは複数のコアを有する汎用中央処理ユニット（ＣＰＵ）、（たとえば、マルチプロセッサ構成における）複数の処理ユニット、グラフィック処理ユニット（ＧＰＵ）、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）、特定用途向け集積回路（ＡＳＩＣ）、複合プログラマブルロジックデバイス（ＣＰＬＤ）、機能を達成するための専用回路、ニューラルネットワークモデルベースの処理を実施するための専用プロセッサ、ニューラル回路、行列計算（例えば、行列乗算）のために最適化されたプロセッサ、または他のシステム、を備えたシステムを含み得る。ある実施の形態では、プロセッサ６０２は、ニューラルネットワーク処理を実施する１つまたは複数のコプロセッサを含み得る。ある実施の形態では、プロセッサ６０２は、確率的出力を生成するためにデータを処理するプロセッサであり得、たとえば、プロセッサ６０２によって生成される出力は、不正確なものであり得、または予想される出力からの範囲内において正確であり得る。処理は、特定の地理的位置に限定される必要はなく、または時間的制限を有する必要はない。例えば、プロセッサは、「リアルタイム」、「オフライン」、「バッチモード」等でその機能を実行することができる。処理の部分は、異なる（または同じ）処理システムによって、異なる時間および異なる場所で実行されてもよい。コンピュータは、メモリと通信する任意のプロセッサであり得る。 In certain embodiments, the device 600 includes a processor 602, memory 604, and an input / output (I / O) interface 606. The processor 602 can be one or more processors and / or processing circuits that execute program code and control the basic operation of the device 600. A "processor" includes any suitable hardware system, mechanism, or component that processes data, signals, or other information. The processor is a general purpose central processing unit (CPU) having one or more cores (eg, in a single core, dual core, or multi-core configuration), multiple processing units (eg, in a multi-processor configuration), a graphics processing unit. (GPU), Field Programmable Gate Array (FPGA), Application-Specific Integrated Circuits (ASIC), Multi-Programmable Logic Devices (CPLD), Dedicated Circuits to Achieve Functions, Dedicated to Perform Neural Network Model-Based Processing It may include a system with a processor, a neural circuit, a processor optimized for matrix computation (eg, matrix multiplication), or other system. In certain embodiments, the processor 602 may include one or more coprocessors that perform neural network processing. In certain embodiments, the processor 602 can be a processor that processes data to produce a stochastic output, for example, the output produced by the processor 602 can be inaccurate or expected. Can be accurate within the range from the output. The process need not be limited to a particular geographic location or have a time limit. For example, the processor can perform its functions in "real time", "offline", "batch mode" and the like. The parts of the process may be performed at different times and at different locations by different (or the same) processing systems. The computer can be any processor that communicates with memory.
メモリ６０４は、典型的には、プロセッサ６０２によるアクセスのためにデバイス６００内に提供され、プロセッサによる実行のための命令を記憶するのに適した、ランダムアクセスメモリ（ＲＡＭ）、読み取り専用メモリ（ＲＯＭ）、電気的消去可能読み取り専用メモリ（ＥＥＰＲＯＭ）、フラッシュメモリなど、任意の適切なプロセッサ可読記憶媒体であり得る。また、プロセッサ６０２から分離して、および/またはプロセッサ６０２と一体化して配置される。メモリ６０４は、オペレーティングシステム６０８、機械学習アプリケーション６３０、他のアプリケーション６１２、およびアプリケーションデータ６１４を含む、プロセッサ６０２によってサーバデバイス６００上で動作するソフトウェアを記憶することができる。他のアプリケーション６１２は、データ表示エンジン、ウェブホスティングエンジン、画像表示エンジン、通知エンジン、ソーシャルネットワーキングエンジンなどのアプリケーションを含み得る。ある実施の形態では、機械学習アプリケーション６３０および他のアプリケーション６１２は、それぞれ、プロセッサ６０２が本明細書で説明する機能、たとえば、図２および図３の方法の一部またはすべてを実行することを可能にする命令を含み得る。 Memory 604 is typically a random access memory (RAM), read-only memory (ROM) provided within the device 600 for access by the processor 602 and suitable for storing instructions for execution by the processor. ), Electrically erasable read-only memory (EEPROM), flash memory, and any other suitable processor-readable storage medium. It is also located separately from the processor 602 and / or integrated with the processor 602. Memory 604 can store software running on server device 600 by processor 602, including operating system 608, machine learning application 630, other applications 612, and application data 614. Other applications 612 may include applications such as data display engines, web hosting engines, image display engines, notification engines, social networking engines and the like. In certain embodiments, the machine learning application 630 and the other application 612 are capable of the processor 602 performing some or all of the functions described herein, eg, the methods of FIGS. 2 and 3, respectively. May include instructions to.
他のアプリケーション６１２は、例えば、画像編集アプリケーション、メディアディスプレイアプリケーション、通信アプリケーション、ウェブホスティングエンジンまたはアプリケーション、マッピングアプリケーション、メディア共有アプリケーション等を含むことができる。本明細書で開示される１つまたは複数の方法は、いくつかの環境およびプラットフォームにおいて、たとえば、任意のタイプのコンピューティングデバイス上で実行することができるスタンドアロンコンピュータプログラムとして、ウェブページを有するウェブアプリケーションとして、モバイルコンピューティングデバイス上で実行されるモバイルアプリケーション（「app」）などとして動作することができる。 Other applications 612 can include, for example, image editing applications, media display applications, communication applications, web hosting engines or applications, mapping applications, media sharing applications and the like. One or more of the methods disclosed herein are web applications that have web pages as stand-alone computer programs that can be run in several environments and platforms, eg, on any type of computing device. It can operate as a mobile application (“app”) or the like that runs on a mobile computing device.
様々な実施の形態では、機械学習アプリケーションは、ベイズ分類器、サポートベクターマシン、ニューラルネットワーク、または他の学習技法を利用することができる。ある実施の形態では、機械学習アプリケーション６３０は、トレーニングされたモデル６３４、推論エンジン６３６、およびデータ６３２を含み得る。ある実施の形態では、データ６３２は、トレーニングデータ、たとえば、トレーニングされたモデル６３４を生成するために使用されるデータを含み得る。たとえば、トレーニングデータは、テキスト、画像、オーディオ、ビデオなどの任意のタイプのデータを含み得る。トレーニングデータは、任意のソース、例えば、トレーニングのために特にマークされたデータレポジトリ、機械学習のためのトレーニングデータとして使用するために許可が提供されたデータなどから取得することができる。１人または複数のユーザが、機械学習モデル、たとえばトレーニングされたモデル６３４をトレーニングするためのそれぞれのユーザデータの使用を許可する実施の形態では、トレーニングデータは、そのようなユーザデータを含み得る。複数ユーザがそれぞれのユーザデータの使用を許可する実施の形態では、データ６３２は、画像（例えば、写真または他のユーザが生成した画像）、通信（例えば、eメール、テキストメッセージ、音声、ビデオ等のチャットデータ）、文書（例えば、スプレッドシート、テキスト文書、プレゼンテーションなど。）のような許可されたデータを含み得る。 In various embodiments, the machine learning application can utilize Bayes classifiers, support vector machines, neural networks, or other learning techniques. In certain embodiments, the machine learning application 630 may include a trained model 634, an inference engine 636, and data 632. In certain embodiments, the data 632 may include training data, eg, data used to generate the trained model 634. For example, training data can include any type of data such as text, images, audio, video. Training data can be obtained from any source, such as a data repository specifically marked for training, data for which permission has been provided for use as training data for machine learning, and so on. In embodiments that allow one or more users to use their respective user data to train a machine learning model, eg, a trained model 634, the training data may include such user data. In embodiments that allow multiple users to use their respective user data, the data 632 can be images (eg, photos or images generated by other users), communications (eg, emails, text messages, audio, video, etc.). Chat data), may include authorized data such as documents (eg spreadsheets, text documents, presentations, etc.).
ある実施の形態において、データ６３２は、地図データ、画像データ（例えば、衛星画像、オーバーヘッド画像など。）、ゲームデータ等のような収集したデータを含み得る。ある実施の形態では、トレーニングデータは、トレーニングされるコンテキストにおけるユーザ入力または活動に基づかないデータ、たとえば、シミュレートされた会話、コンピュータ生成画像などから生成されるデータなどのように、トレーニングの目的のために生成される合成データを含み得る。ある実施の形態では、機械学習アプリケーション６３０は、データ６３２を除外する。たとえば、これらの実施の形態では、トレーニングされたモデル６３４は、たとえば、異なるデバイス上で生成され、機械学習アプリケーション６３０の一部として提供され得る。様々な実施の形態では、トレーニングされたモデル６３４は、モデル構造または形態と、関連付けられた重みとを含むデータファイルとして提供され得る。推論エンジン６３６は、訓練されたモデル６３４のためのデータファイルを読み取り、訓練されたモデル６３４において指定されたモデル構造または形式に基づいてノード接続性、層、および重みを有するニューラルネットワークを実施することができる。 In certain embodiments, the data 632 may include collected data such as map data, image data (eg, satellite images, overhead images, etc.), game data, and the like. In certain embodiments, the training data is intended for training purposes, such as data that is not based on user input or activity in the context in which it is trained, such as data generated from simulated conversations, computer-generated images, and so on. May include synthetic data generated for. In certain embodiments, the machine learning application 630 excludes data 632. For example, in these embodiments, the trained model 634 may be generated, for example, on different devices and provided as part of the machine learning application 630. In various embodiments, the trained model 634 may be provided as a data file containing the model structure or morphology and associated weights. The inference engine 636 reads the data file for the trained model 634 and implements a neural network with node connectivity, layers, and weights based on the model structure or format specified in the trained model 634. Can be done.
機械学習アプリケーション６３０は、トレーニングされたモデル６３４も含む。ある実施の形態では、トレーニングされたモデルは、１つまたは複数のモデル形態または構造を含み得る。例えば、モデル形式または構造は、線形ネットワーク、複数の層（例えば、入力層と出力層との間の「隠れた層」であり、各層は線形ネットワークである）を実施するディープニューラルネットワーク、畳み込みニューラルネットワーク（例えば、入力データを複数の部分またはタイルに分割し、１つまたは複数のニューラルネットワーク層を使用して各タイルを別々に処理し、各タイルの処理から得られた結果を集約するネットワーク）、シーケンス間ニューラルネットワーク（例えば、文中の単語、ビデオ中のフレームなどの順次データを入力として取り、結果シーケンスを出力として生成するネットワーク）など、任意のタイプのニューラルネットワークを含み得る。モデル形式または構造は、様々なノード間の接続性およびノードの層への編成を指定することができる。たとえば、第１の層（たとえば、入力層）のノードは、データを入力データ６３２またはアプリケーションデータ６１４として受信することができる。そのようなデータは、例えば、トレーニングされたモデルが画像分析に使用される場合、ノード当たり１つまたは複数のピクセルを含むことができる。後続の中間層は、モデル形式または構造において指定された接続性ごとに、前の層のノードの出力を入力として受信してもよい。これらの層は隠し層と呼ばれることもある。最終層（例えば、出力層）は、機械学習アプリケーションの出力を生成する。例えば、出力は、特定のトレーニングされたモデルに応じて、画像に対するラベルのセット、画像の他の画像（例えば、画像の特徴ベクトル）との比較を可能にする画像の表現、入力文に応じた出力文、入力データの１つまたは複数のカテゴリなどとすることができる。ある実施の形態では、モデル形式または構造はまた、各層中のノードの数および/またはタイプを指定する。 The machine learning application 630 also includes a trained model 634. In certain embodiments, the trained model may include one or more model forms or structures. For example, the model format or structure is a linear network, a deep neural network that implements multiple layers (eg, a "hidden layer" between the input and output layers, each layer being a linear network), a convolutional neural network. A network (for example, a network that divides input data into multiple parts or tiles, processes each tile separately using one or more neural network layers, and aggregates the results obtained from the processing of each tile). , Inter-sequence neural networks (eg, networks that take sequential data such as words in sentences, frames in video as input and generate result sequences as output), and may include any type of neural network. The model format or structure can specify connectivity between various nodes and the organization of nodes into layers. For example, a node in the first layer (eg, the input layer) can receive the data as input data 632 or application data 614. Such data can include, for example, one or more pixels per node when the trained model is used for image analysis. Subsequent intermediate layers may receive the output of the nodes of the previous layer as inputs for each connectivity specified in the model format or structure. These layers are sometimes called hidden layers. The final layer (eg, the output layer) produces the output of the machine learning application. For example, the output depends on the set of labels for the image, the representation of the image that allows comparison with other images of the image (eg, the feature vector of the image), and the input text, depending on the particular trained model. It can be an output statement, one or more categories of input data, and so on. In certain embodiments, the model form or structure also specifies the number and / or type of nodes in each layer.
異なる実施の形態では、トレーニングされたモデル６３４は、モデル構造または形態ごとに層に配列される複数のノードを含み得る。ある実施の形態では、ノードは、たとえば、１単位の入力を処理して１単位の出力を生成するように構成された、メモリを有さない計算ノードであり得る。ノードによって実行される計算は、例えば、複数のノード入力の各々に重みを掛けること、重み付き和を得ること、およびノード出力を生成するためにバイアスまたは切片値で重み付き和を調整することを含み得る。ある実施の形態では、ノードによって実行される計算はまた、ステップ/アクティブ化関数を調整された加重和に適用することを含み得る。ある実施の形態では、ステップ/アクティブ化関数は、非線形関数であり得る。様々な実施の形態では、そのような計算は、行列乗算などの演算を含み得る。ある実施の形態では、複数のノードによる計算は、たとえば、マルチコアプロセッサの複数のプロセッサコアを使用して、ＧＰＵの個々の処理ユニットを使用して、または専用ニューラル回路を使用して、並列に実行され得る。ある実施の形態では、ノードはメモリを含んでもよく、たとえば、後続の入力を処理する際に１つまたは複数の以前の入力を記憶し使用することができる。例えば、メモリを有するノードは、長短記憶（ＬＳＴＭ）ノードを含み得る。ＬＳＴＭノードは、ノードが有限状態機械（ＦＳＭ）のように動作することを可能にする「状態」を維持するためにメモリを使用することができる。そのようなノードを有するモデルは、逐次データ、例えば、文または段落の単語、ビデオ、スピーチまたは他のオーディオのフレームなどを処理する際に有用であり得る。 In different embodiments, the trained model 634 may include multiple nodes arranged in layers for each model structure or form. In certain embodiments, the node can be, for example, a memoryless compute node configured to process one unit of input to produce one unit of output. Calculations performed by nodes include, for example, weighting each of multiple node inputs, obtaining a weighted sum, and adjusting the weighted sum with a bias or intercept value to produce a node output. Can include. In certain embodiments, the calculations performed by the nodes may also include applying the step / activation function to the adjusted weighted sum. In certain embodiments, the step / activation function can be a non-linear function. In various embodiments, such calculations may include operations such as matrix multiplication. In one embodiment, calculations by multiple nodes are performed in parallel, for example, using multiple processor cores of a multi-core processor, using individual processing units on the GPU, or using dedicated neural circuits. Can be done. In certain embodiments, the node may include memory, for example, it may store and use one or more previous inputs when processing subsequent inputs. For example, a node with memory may include a long short-term memory (LSTM) node. The LSTM node can use memory to maintain a "state" that allows the node to behave like a finite state machine (FSM). Models with such nodes can be useful in processing sequential data, such as sentences or paragraph words, videos, speeches or other audio frames.
ある実施の形態では、トレーニングされたモデル６３４は、個々のノードのための埋め込みまたは重みを含み得る。例えば、モデルは、モデル形式または構造によって指定された層に編成された複数のノードとして開始され得る。初期化時に、それぞれの重みを、モデル形式ごとに接続されるノード、例えばニューラルネットワークの連続する階層のノードの各対の接続に適用することができる。例えば、各重みはランダムに割り当てられてもよく、デフォルト値に初期化されてもよい。次いで、モデルを、例えばデータ６３２を用いてトレーニングして、結果を生成してもよい。 In certain embodiments, the trained model 634 may include embedding or weighting for individual nodes. For example, a model can be started as multiple nodes organized in layers specified by model format or structure. At initialization, each weight can be applied to the connection of each pair of nodes connected by model format, eg, nodes in a continuous hierarchy of neural networks. For example, each weight may be randomly assigned or initialized to a default value. The model may then be trained with, for example, data 632 to produce results.
例えば、トレーニングは、教師付き学習技術を適用することを含み得る。教師付き学習では、トレーニングデータは、複数の入力（例えば、画像のセット）と、各入力に対する対応する予想出力（例えば、各画像に対する１つまたは複数のラベル）とを含むことができる。モデルの出力と予想される出力との比較に基づいて、重みの値は、例えば、同様の入力が与えられた場合にモデルが予想される出力を生成する確率を増加させるように、自動的に調整される。 For example, training may include applying supervised learning techniques. In supervised learning, training data can include multiple inputs (eg, a set of images) and corresponding expected outputs for each input (eg, one or more labels for each image). Based on the comparison of the model's output with the expected output, the weight value will automatically increase the probability that the model will produce the expected output, for example, given similar inputs. Be adjusted.
ある実施の形態では、トレーニングは、教師なし学習技法を適用することを含み得る。教師なし学習では、入力データだけが提供されてもよく、モデルは、例えば入力データを複数のグループにクラスタ化するために、データを区別するようにトレーニングされてもよく、各グループは曲がりなりにも、類似する入力データを含む。例えば、モデルは、抽象画像（例えば、合成画像、人が描画した画像等である。）を自然画像（例えば写真）から区別するように、画像を区別するようにトレーニングされてもよい。モデルは、画像コンテンツデータおよび/または画像メタデータに基づいて１つまたは複数の画像特性を決定するようにトレーニングされ得る。決定された画像特性は、画像ラベルおよび/またはセマンティック概念を含み得る。 In certain embodiments, training may include applying unsupervised learning techniques. In unsupervised learning, only the input data may be provided, the model may be trained to distinguish the data, for example to cluster the input data into multiple groups, and each group may be bent or similar. Contains the input data to be used. For example, the model may be trained to distinguish between images, such as abstract images (eg, composite images, images drawn by humans, etc.) from natural images (eg, photographs). The model can be trained to determine one or more image characteristics based on image content data and / or image metadata. The determined image characteristics may include image labels and / or semantic concepts.
別の例では、教師なし学習を使用してトレーニングされるモデルは、入力文における単語の使用に基づいて単語をクラスタ化してもよい。ある実施の形態では、教師なし学習を使用して、たとえば、機械学習アプリケーション６３０によって使用され得る知識表現を生成することができる。様々な実施の形態では、トレーニングされたモデルは、モデル構造に対応する１組の重みまたは埋め込みを含む。データ６３２が省略される実施の形態では、機械学習アプリケーション６３０は、たとえば機械学習アプリケーション６３０の開発者によって、サードパーティによってなど、以前のトレーニングに基づくトレーニングされたモデル６３４を含み得る。ある実施の形態では、トレーニングされたモデル６３４は、固定された、たとえば、重みを提供するサーバからダウンロードされる重みのセットを含み得る。 In another example, a model trained using unsupervised learning may cluster words based on the use of words in input sentences. In certain embodiments, unsupervised learning can be used to generate knowledge representations that can be used, for example, by machine learning application 630. In various embodiments, the trained model comprises a set of weights or embeddings corresponding to the model structure. In embodiments where data 632 is omitted, the machine learning application 630 may include a trained model 634 based on previous training, for example by the developer of the machine learning application 630, by a third party, and so on. In certain embodiments, the trained model 634 may include a fixed set of weights, eg, downloaded from a server that provides the weights.
機械学習アプリケーション６３０はまた、推論エンジン６３６を含む。推論エンジン６３６は、トレーニングされたモデル６３４をアプリケーションデータ６１４などのデータに適用して推論を提供するように構成される。ある実施の形態では、推論エンジン６３６は、プロセッサ６０２によって実行されるソフトウェアコードを含み得る。ある実施の形態では、推論エンジン６３６は、プロセッサ６０２がトレーニングされたモデルを適用することを可能にする回路構成（例えば、プログラマブルプロセッサ用、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）用など。）を指定することができる。ある実施の形態では、推論エンジン６３６は、ソフトウェア命令、ハードウェア命令、または組合せを含み得る。ある実施の形態では、推論エンジン６３６は、オペレーティングシステム６０８および/または他のアプリケーション６１２によって使用されて推論エンジン６３６を呼び出し、たとえば、トレーニングされたモデル６３４をアプリケーションデータ６１４に適用して推論を生成することができるアプリケーションプログラミングインターフェイス（ＡＰＩ）を提供し得る。 The machine learning application 630 also includes an inference engine 636. The inference engine 636 is configured to apply the trained model 634 to data such as application data 614 to provide inference. In certain embodiments, the inference engine 636 may include software code executed by processor 602. In one embodiment, the inference engine 636 specifies a circuit configuration (eg, for a programmable processor, for a field programmable gate array (FPGA), etc.) that allows the processor 602 to apply a trained model. Can be done. In certain embodiments, the inference engine 636 may include software instructions, hardware instructions, or combinations. In one embodiment, the inference engine 636 is used by the operating system 608 and / or other application 612 to call the inference engine 636 and apply, for example, the trained model 634 to the application data 614 to generate inferences. It may provide an application programming interface (API) that can.
機械学習アプリケーション６３０は、いくつかの技術的利点を提供することができる。例えば、トレーニングされたモデル６３４が教師なし学習に基づいて生成されるとき、トレーニングされたモデル６３４は、入力データ、例えば、アプリケーションデータ６１４から知識表現（例えば、数値表現）を生成するために推論エンジン６３６によって適用され得る。例えば、画像分析のためにトレーニングされたモデルは、入力画像（例えば、１０MB）より小さいデータサイズ（例えば、１KB）を有する画像の表現を生成し得る。ある実施の形態では、そのような表現は、出力（例えば、ラベル、分類、画像を記述する文等）を生成するために処理コスト（例えば、計算コスト、メモリ使用など。）を低減するのに役立ち得る。ある実施の形態では、そのような表現は、推論エンジン６３６の出力から出力を生成する異なる機械学習アプリケーションへの入力として提供され得る。ある実施の形態では、機械学習アプリケーション６３０によって生成された知識表現は、たとえば、ネットワークを介してさらなる処理を行う異なるデバイスに提供され得る。そのような実施の形態では、画像よりもむしろ知識表現を提供することは、たとえば、削減されたコストでより迅速なデータ送信を可能にする、技術的利益を提供し得る。別の例では、文書をクラスタリングするためにトレーニングされたモデルは、入力文書から文書クラスタを生成してもよい。文書クラスタは、元の文書にアクセスする必要なしにさらなる処理（例えば、文書がトピックに関連するかどうかを決定すること、文書の分類カテゴリを決定することなど。）に適しており、したがって、計算コストを節約する。 The machine learning application 630 can provide some technical advantages. For example, when the trained model 634 is generated based on unsupervised learning, the trained model 634 is an inference engine to generate knowledge representations (eg, numerical representations) from input data, eg application data 614. Can be applied by 636. For example, a model trained for image analysis can generate a representation of an image with a data size (eg, 1KB) smaller than the input image (eg, 10MB). In certain embodiments, such representations reduce processing costs (eg, computational costs, memory usage, etc.) to produce output (eg, labels, classifications, statements describing images, etc.). Can be useful. In certain embodiments, such a representation may be provided as an input to a different machine learning application that produces an output from the output of the inference engine 636. In certain embodiments, the knowledge representation generated by the machine learning application 630 may be provided, for example, to different devices that perform further processing over the network. In such an embodiment, providing knowledge representation rather than images may provide technical benefits, for example, enabling faster data transmission at reduced cost. In another example, a model trained to cluster documents may generate document clusters from input documents. Document clusters are suitable for further processing without the need to access the original document (eg, determining whether a document is relevant to a topic, determining the classification category of a document, etc.) and therefore calculation. Save money.
ある実施の形態では、機械学習アプリケーション６３０は、オフライン方式とれで実施され得る。これらの実施の形態では、トレーニングされたモデル６３４は、第１の段階で生成され、機械学習アプリケーション６３０の一部として提供され得る。ある実施の形態では、機械学習アプリケーション６３０は、オンライン方式で実施され得る。たとえば、そのような実施の形態では、機械学習アプリケーション６３０（例えば、オペレーティングシステム６０８、他のアプリケーション６１２のうちの１つまたは複数）を呼び出すアプリケーションは、機械学習アプリケーション６３０によって生成された推論を利用し、たとえば、ユーザに推論を提供し、システムログ（例えば、ユーザが許可する場合、推論に基づいてユーザがとるアクション、または、さらなる処理のための入力として利用される場合、さらなる処理の結果）を生成することができる。システムログは、定期的に、例えば、１時間毎、１ヶ月毎、四半期毎に生成されてもよく、トレーニングされたモデル６３４を更新するために、例えば、トレーニングされたモデル６３４の埋め込みを更新するために、ユーザの許可と共に使用されてもよい。 In certain embodiments, the machine learning application 630 can be implemented offline. In these embodiments, the trained model 634 can be generated in the first stage and provided as part of the machine learning application 630. In certain embodiments, the machine learning application 630 may be implemented online. For example, in such an embodiment, the application that calls the machine learning application 630 (eg, operating system 608, one or more of the other applications 612) utilizes the inference generated by the machine learning application 630. , For example, providing inference to the user and displaying the system log (eg, if the user allows, the action taken by the user based on the inference, or the result of further processing if used as input for further processing). Can be generated. The system log may be generated on a regular basis, eg, hourly, monthly, quarterly, and to update the trained model 634, eg, update the embedding of the trained model 634. Therefore, it may be used with the permission of the user.
ある実施の形態では、機械学習アプリケーション６３０は、機械学習アプリケーション６３０が実行されるデバイス６００の特定の構成に適合し得る方法で実施され得る。例えば、機械学習アプリケーション６３０は、利用可能な計算リソース、例えば、プロセッサ６０２を利用する計算グラフを決定することができる。例えば、機械学習アプリケーション６３０が複数のデバイス上の分散アプリケーションとして実施される場合、機械学習アプリケーション６３０は、計算を最適化するように個々のデバイス上で実行される計算を決定することができる。別の例では、機械学習アプリケーション６３０は、プロセッサ６０２が特定の数（たとえば、１０００）のＧＰＵコアを有するＧＰＵを含むと判断し、それに応じて（例えば、１０００個の個々のプロセス又はスレッドとして）、推論エンジンを実施することができる。 In certain embodiments, the machine learning application 630 can be implemented in a manner that can be adapted to the particular configuration of the device 600 on which the machine learning application 630 is executed. For example, machine learning application 630 can determine computational graphs that utilize available computational resources, such as processor 602. For example, if the machine learning application 630 is implemented as a distributed application on multiple devices, the machine learning application 630 can determine the computations to be performed on the individual devices to optimize the computations. In another example, the machine learning application 630 determines that the processor 602 contains GPUs with a certain number of GPU cores (eg, 1000) and accordingly (eg, as 1000 individual processes or threads). , Inference engine can be implemented.
ある実施の形態では、機械学習アプリケーション６３０は、トレーニングされたモデルの集合を実施することができる。例えば、トレーニングされたモデル６３４は、各々が同じ入力データに適用可能な複数のトレーニングされたモデルを含み得る。これらの実施の形態では、機械学習アプリケーション６３０は、たとえば、利用可能な計算リソース、以前の推論による成功率などに基づいて、特定のトレーニングされたモデルを選択し得る。ある実施の形態では、機械学習アプリケーション６３０は、複数のトレーニングされたモデルが適用されるように推論エンジン６３６を実行することができる。これらの実施の形態では、機械学習アプリケーション６３０は、個々のモデルを適用することからの出力を、たとえば、個々の出力を各トレーニングされたモデルを適用することによってスコア化する投票技術を使用して、または１つまたは複数の特定の出力を選択することによって、組み合わせることができる。さらに、これらの実施の形態では、機械学習アプリケーションは、個々のトレーニングされたモデルを適用するための時間閾値（たとえば、０.５ms）を適用し、時間閾値内で利用可能であるそれらの個々の出力のみを利用することができる。時間閾値内で受信されない出力は、利用されない、例えば破棄される場合がある。例えば、そのようなアプローチは、機械学習アプリケーションを呼び出す間に、例えばオペレーティングシステム６０８または１つまたは複数のアプリケーション６１２によって指定された時間制限があるときに適切であり得る。 In certain embodiments, the machine learning application 630 can implement a set of trained models. For example, the trained model 634 may include multiple trained models, each applicable to the same input data. In these embodiments, the machine learning application 630 may select a particular trained model based on, for example, available computational resources, success rates from previous inferences, and so on. In certain embodiments, the machine learning application 630 can run the inference engine 636 so that multiple trained models are applied. In these embodiments, the machine learning application 630 uses a voting technique that scores the output from applying individual models, for example, by applying each trained model. , Or by selecting one or more specific outputs. Further, in these embodiments, the machine learning application applies a time threshold (eg, 0.5 ms) for applying the individual trained models and is available within the time threshold for each of them. Only the output is available. Output that is not received within the time threshold may be unused, eg, discarded. For example, such an approach may be appropriate when there is a time limit specified by, for example, operating system 608 or one or more applications 612, while invoking a machine learning application.
異なる実施の形態では、機械学習アプリケーション６３０は、異なるタイプの出力を生成することができる。例えば、機械学習アプリケーション６３０は、表現またはクラスタ（例えば、入力データの数値表現）、（例えば、画像、文書等を含む入力データに関する）ラベル、（例えば、画像または映像を説明した、入力文に対する応答として使用するのに適した、等の）フレーズまたは文、（例えば、入力に応答して機械学習アプリケーションによって生成された）画像、音声またはビデオを提供し得る（例えば、入力ビデオに応答して、機械学習アプリケーション６３０は、適用される特定の効果たとえば、トレーニングされたモデル６３４が、コミックブックまたは特定のアーチストからのトレーニングデータなどを使用してトレーニングされている場合には、コミックブックまたは特定のアーチストのスタイルでレンダリングされた出力ビデオを生成し得る等）。ある実施の形態において、機械学習アプリケーション６３０は、呼び出し元アプリケーション、例えばオペレーティングシステム６０８や１つまたは複数のアプリケーション６１２によって規定されたフォーマットに基づいて出力を生成しうる。ある実施の形態において、呼び出し元アプリケーションは、他の機械学習アプリケーションであってもよい。例えば、そのような構成は、生成型の敵対的ネットワークで使用することができ、呼び出している機械学習アプリケーションは、機械学習アプリケーション６３０からの出力を用いてトレーニングされ、逆もまた同様である。 In different embodiments, the machine learning application 630 can generate different types of output. For example, the machine learning application 630 responds to an input statement describing a representation or cluster (eg, a numerical representation of the input data), a label (eg, for input data including images, documents, etc.), (eg, an image or video). It may provide a phrase or sentence (eg, generated by a machine learning application in response to input), an image, audio or video (eg, in response to input video, etc.) suitable for use as. The machine learning application 630 applies specific effects, for example, if the trained model 634 is trained using training data from a comic book or specific artist, the comic book or specific artist. Can produce output video rendered in the style of, etc.). In certain embodiments, the machine learning application 630 may generate output based on the format specified by the calling application, eg, operating system 608 or one or more applications 612. In certain embodiments, the calling application may be another machine learning application. For example, such a configuration can be used in a generative hostile network, the calling machine learning application is trained with the output from the machine learning application 630, and vice versa.
あるいは、メモリ６０４内の任意のソフトウェアは、任意の他の適切な記憶場所またはコンピュータ可読媒体に記憶することができる。さらに、メモリ６０４（および/または他の接続された記憶装置）は、１つまたは複数の画像、メッセージ、１つまたは複数の分類、電子百科事典、辞書、シソーラス、知識ベース、メッセージデータ、文法、ユーザ選好、および/または本明細書に記載の特徴で使用される他の命令およびデータを記憶することができる。メモリ６０４および任意の他のタイプのストレージ（磁気ディスク、光ディスク、磁気テープ、または他の有形媒体）は、「ストレージ」または「ストレージデバイス」と見なすことができる。 Alternatively, any software in memory 604 can be stored on any other suitable storage location or computer-readable medium. In addition, memory 604 (and / or other connected storage) includes one or more images, messages, one or more classifications, electronic encyclopedias, dictionaries, thesauruses, knowledge bases, message data, grammar, User preferences and / or other instructions and data used in the features described herein can be stored. Memory 604 and any other type of storage (magnetic disk, optical disc, magnetic tape, or other tangible medium) can be considered a "storage" or "storage device".
Ｉ／Ｏインターフェイス６０６は、サーバデバイス６００を他のシステムおよびデバイスとインターフェイス接続することを可能にする機能を提供することができる。インターフェイスされたデバイスは、デバイス６００の一部として含めることができ、または分離されて、デバイス６００と通信することができる。例えば、ネットワーク通信デバイス、ストレージデバイス（例えば、メモリおよび/又はデータベース１０６）、および入出力デバイスは、Ｉ／Ｏインターフェイス６０６を介して通信することができる。ある実施の形態では、Ｉ／Ｏインターフェイスは、入力デバイス（キーボード、ポインティングデバイス、タッチスクリーン、マイクロフォン、カメラ、スキャナ、センサなど。）および/または出力デバイス（ディスプレイデバイス、スピーカデバイス、プリンタ、モータなど。）などのインターフェイスデバイスに接続することができる。 The I / O interface 606 can provide a function that allows the server device 600 to interface with other systems and devices. The interfaced device can be included as part of the device 600 or can be separated and communicate with the device 600. For example, network communication devices, storage devices (eg, memory and / or database 106), and I / O devices can communicate via the I / O interface 606. In certain embodiments, the I / O interface is an input device (keyboard, pointing device, touch screen, microphone, camera, scanner, sensor, etc.) and / or output device (display device, speaker device, printer, motor, etc.). ) Can be connected to interface devices such as.
Ｉ／Ｏインターフェイス６０６に接続することができるインターフェイス付きデバイスのいくつかの例は、コンテンツ、たとえば、画像、ビデオ、および/または本明細書で説明する出力アプリケーションのユーザインターフェイスを表示するために使用することができる１つまたは複数のディスプレイデバイス６２０を含むことができる。ディスプレイデバイス６２０は、ローカル接続（例えば、ディスプレイバス）を介して、および/またはネットワーク接続を介してデバイス６００に接続することができ、任意の適切なディスプレイデバイスとすることができる。ディスプレイデバイス６２０は、LCD、LED、またはプラズマディスプレイスクリーン、CRT、テレビ、モニタ、タッチスクリーン、３Dディスプレイスクリーン、または他の視覚ディスプレイデバイスなどの任意の適切なディスプレイデバイスを含み得る。例えば、ディスプレイデバイス６２０は、モバイルデバイス上に提供される平坦なディスプレイ画面、ゴーグルもしくはヘッドセットデバイス内に提供される複数のディスプレイ画面、またはコンピュータデバイスのためのモニタ画面であり得る。 Some examples of interfaced devices that can be connected to the I / O interface 606 are used to display content, such as images, videos, and / or the user interface of the output application described herein. It can include one or more display devices 620 that can. The display device 620 can be connected to the device 600 via a local connection (eg, a display bus) and / or via a network connection, making it any suitable display device. The display device 620 may include any suitable display device such as an LCD, LED, or plasma display screen, CRT, television, monitor, touch screen, 3D display screen, or other visual display device. For example, the display device 620 can be a flat display screen provided on a mobile device, multiple display screens provided within a goggles or headset device, or a monitor screen for a computer device.
Ｉ／Ｏインターフェイス６０６は、他の入力および出力デバイスとインターフェイスすることができる。いくつかの例は、画像をキャプチャすることができる１つまたは複数のカメラを含む。ある実施の形態は、（例えば、キャプチャされた画像の一部、音声コマンドなどとして）音をキャプチャするためのマイクロフォン、音を出力するためのオーディオスピーカデバイス、または他の入力デバイスおよび出力デバイスを提供することができる。 The I / O interface 606 can interface with other input and output devices. Some examples include one or more cameras capable of capturing images. Certain embodiments provide a microphone for capturing sound (eg, as part of a captured image, voice commands, etc.), an audio speaker device for outputting sound, or other input and output devices. can do.
説明を容易にするために、図６は、プロセッサ６０２、メモリ６０４、Ｉ／Ｏインターフェイス６０６、ならびにソフトウェアブロック６０８，６１２および６３０の各々に対する１つのブロックを示す。これらのブロックは、１つまたは複数のプロセッサまたは処理回路、オペレーティングシステム、メモリ、Ｉ／Ｏインターフェイス、アプリケーション、および/またはソフトウェアモジュールを表し得る。他の実施の形態では、デバイス６００は、示される構成要素のすべてを有しなくてもよく、および/または本明細書に示される構成要素の代わりに、またはそれに加えて、他のタイプの構成要素を含む他の構成要素を有してもよい。いくつかのコンポーネントは、本明細書のいくつかの実施において説明されるようなブロックおよび動作を実行するものとして説明されるが、環境１００、デバイス６００、同様のシステム、またはそのようなシステムに関連する任意の適切なプロセッサまたは複数のプロセッサの任意の適切なコンポーネントまたはコンポーネントの組み合わせは、説明されるブロックおよび動作を実行することができる。 For ease of explanation, FIG. 6 shows one block for each of processor 602, memory 604, I / O interface 606, and software blocks 608, 612, and 630. These blocks may represent one or more processors or processing circuits, operating systems, memory, I / O interfaces, applications, and / or software modules. In other embodiments, the device 600 may not have all of the components shown and / or in place of or in addition to the components shown herein, other types of configurations. It may have other components, including elements. Some components are described as performing blocks and operations as described in some implementations herein, but are associated with environment 100, device 600, similar systems, or such systems. Any suitable processor or any suitable component or combination of components of multiple processors can perform the blocks and actions described.
本明細書で説明する方法は、コンピュータ上で実行され得るコンピュータプログラム命令またはコードによって実施され得る。たとえば、コードは、１つまたは複数のデジタルプロセッサ（たとえば、マイクロプロセッサまたは他の処理回路）によって実施することができ、半導体または固体メモリ、磁気テープ、リムーバブルコンピュータディスケット、ランダムアクセスメモリ（ＲＡＭ）、読み取り専用メモリ（ＲＯＭ）、フラッシュメモリ、リジッド磁気ディスク、光ディスク、固体メモリドライブ等を含む磁気、光学、電磁、または半導体記憶媒体のような、非一時的コンピュータ可読媒体（たとえば、記憶媒体）を含むコンピュータプログラム製品に格納することができる。プログラム命令はまた、例えばサーバ（例えば、分散システムおよび/又はクラウドコンピューティングシステム）から配信されるサービス（SaaS）としてのソフトウェアの形態で、電子信号に含まれ、電子信号として提供されることもできる。あるいは、１つまたは複数の方法は、ハードウェア（論理ゲートなど）で、またはハードウェアとソフトウェアとの組み合わせで実施され得る。ハードウェアの例は、プログラム可能なプロセッサ（例えば、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）、複合プログラマブルロジックデバイス）、汎用プロセッサ、グラフィックプロセッサ、特定用途向け集積回路（ＡＳＩＣ）などであり得る。１つまたは複数の方法は、システム上で実行するアプリケーションの一部または構成要素として、または他のアプリケーションおよびオペレーティングシステムとともに実行するアプリケーションまたはソフトウェアとして実行することができる。 The methods described herein can be implemented by computer program instructions or code that can be executed on a computer. For example, the code can be implemented by one or more digital processors (eg, microprocessors or other processing circuits), including semiconductor or solid-state memory, magnetic tape, removable computer diskettes, random access memory (RAM), and reads. Computers that include non-temporary computer-readable media (eg, storage media), such as magnetic, optical, electromagnetic, or semiconductor storage media, including dedicated memory (ROM), flash memory, rigid magnetic disks, optical disks, solid-state memory drives, etc. Can be stored in program products. Program instructions can also be included in electronic signals and provided as electronic signals, for example in the form of software as a service (SaaS) delivered from servers (eg, distributed systems and / or cloud computing systems). .. Alternatively, one or more methods may be implemented in hardware (such as logic gates) or in combination with hardware and software. Examples of hardware can be programmable processors (eg, field programmable gate arrays (FPGAs), composite programmable logic devices), general purpose processors, graphics processors, application specific integrated circuits (ASICs), and the like. One or more methods can be run as part or component of an application running on the system, or as application or software running with other applications and operating systems.
説明は、その特定の実施に関して説明してきたが、これらの特定の実施は、例示にすぎず、限定的ではない。実施例において示される概念は、他の例および実施に適用され得る。 Although the description has described for that particular practice, these particular practices are merely exemplary and not limiting. The concepts presented in the examples may apply to other examples and practices.
本開示で説明する機能ブロック、動作、特徴、方法、デバイス、およびシステムは、当業者に知られているように、システム、デバイス、および機能ブロックの異なる組み合わせに統合または分割され得ることに留意されたい。任意の適切なプログラミング言語およびプログラミング技術を使用して、特定の実施のルーチンを実施することができる。異なるプログラミング技法、例えば手続き的またはオブジェクト指向の技法を採用することができる。ルーチンは、単一の処理装置または複数のプロセッサ上で実行することができる。ステップ、動作、または計算は、特定の順序で提示され得るが、順序は、異なる特定の実施の形態で変更され得る。ある実施の形態では、本明細書で順次として示される複数のステップまたは動作が同時に実行され得る。 It should be noted that the functional blocks, behaviors, features, methods, devices, and systems described herein can be integrated or subdivided into different combinations of systems, devices, and functional blocks, as known to those of skill in the art. I want to. A particular implementation routine can be performed using any suitable programming language and programming technique. Different programming techniques, such as procedural or object-oriented techniques, can be adopted. Routines can be run on a single processor or on multiple processors. The steps, actions, or calculations may be presented in a particular order, but the order may be changed in different specific embodiments. In certain embodiments, a plurality of steps or actions, as shown sequentially herein, may be performed simultaneously.
Claims (20)
前記１つまたは複数の第１の画像の１つまたは複数の第１の画像特性を決定することと、
前記画像ライブラリ内の１つまたは複数の第２の画像を識別することとを備え、前記１つまたは複数の第２の画像の各々は、前記１つまたは複数の第１の画像特性のうちの少なくとも１つに一致する少なくとも１つの第２の画像特性に関連付けられ、
ユーザインターフェイスを表示させることをさらに備え、前記ユーザインターフェイスは、前記１つまたは複数の第２の画像を含み、前記ユーザインターフェイスは、前記１つまたは複数の第２の画像の選択を可能にする、方法。 Receiving a first user input indicating the selection of one or more first images in the image library, and
Determining one or more first image characteristics of the one or more first images and
Each of the one or more second images comprises identifying one or more second images in the image library, each of the one or more first image properties. Associated with at least one second image property that matches at least one,
The user interface further comprises displaying the user interface, which comprises the one or more second images, which allows the selection of the one or more second images. Method.
前記第２のユーザ入力の受信に応答して、前記１つまたは複数の第２の画像のうちの前記少なくとも１つの１つまたは複数の第２の画像特性を決定することと、
前記画像ライブラリ内の１つまたは複数の第３の画像を識別することとを備え、前記１つまたは複数の第３の画像の各々は、前記１つまたは複数の第１の画像特性のうちの少なくとも１つおよび前記１つまたは複数の第２の画像特性のうちの少なくとも１つに一致する少なくとも１つの第３の画像特性に関連付けられ、
更新されたユーザインターフェイスを表示させることをさらに備え、前記更新されたユーザインターフェイスは、前記１つまたは複数の第３の画像を含み、前記更新されたユーザインターフェイスは、前記１つまたは複数の第３の画像の選択を可能にする、請求項１に記載の方法。 Receiving a second user input indicating a selection of at least one of the one or more second images.
Determining the at least one or more second image characteristics of the one or more second images in response to receiving the second user input.
Each of the one or more third images comprises identifying one or more third images in the image library, and each of the one or more third images is among the one or more first image characteristics. Associated with at least one and at least one third image property that matches at least one of the one or more second image properties.
Further comprising displaying an updated user interface, the updated user interface comprises the one or more third images, and the updated user interface is said one or more third images. The method of claim 1, which allows the selection of images of.
画像コラージュを生成することとをさらに備え、前記画像コラージュは、前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの少なくとも１つを含む、請求項１に記載の方法。 Receiving a second user input indicating a selection of at least one of the one or more second images.
The first aspect of the present invention further comprises generating an image collage, wherein the image collage comprises at least one of the one or more first images and the one or more second images. The method described.
前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの少なくとも１つに基づいて、提案されたアクション要素を前記ユーザインターフェイスに表示させることとをさらに備える、請求項１に記載の方法。 Receiving a second user input indicating a selection of at least one of the one or more second images.
A claim further comprising displaying a proposed action element in the user interface based on at least one of the one or more first images and the one or more second images. Item 1. The method according to item 1.
前記ユーザ選択の受信に応答して、前記提案されたアクション要素に関連付けられたアクションを実行することとをさらに備え、前記アクションは、前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの少なくとも１つをアーカイブすること、前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの前記少なくとも１つを削除すること、または、前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの前記少なくとも１つを自動的に強調することを行なうことの１つまたは複数を含む、請求項１１に記載の方法。 Receiving the user selection of the proposed action element and
Further comprising performing an action associated with the proposed action element in response to the reception of the user selection, the action includes said one or more first images and said one or more. To archive at least one of the second images, delete the one or more first images and the at least one of the one or more second images, or 11. One or more of the automatic emphasis of the one or more first images and the at least one of the one or more second images. The method described in.
前記コンテキスト情報に少なくとも部分的に基づいて、画像ライブラリ内の１つまたは複数の第１の画像を識別することと、
ユーザインターフェイスを表示させることとを備え、前記ユーザインターフェイスは、前記１つまたは複数の第１の画像を含み、前記ユーザインターフェイスは、前記１つまたは複数の第１の画像の選択を可能にし、
前記１つまたは複数の第１の画像のうちの少なくとも１つの画像の選択を示す第１のユーザ入力を受信することと、
前記第１のユーザ入力の受信に応答して、前記少なくとも１つの画像を前記ターゲットソフトウェアアプリケーションに提供することとをさらに備える、方法。 Determining contextual information that indicates the target software application
Identifying one or more first images in an image library based at least in part on the contextual information.
The user interface comprises displaying the user interface, the user interface includes the one or more first images, and the user interface allows selection of the one or more first images.
Receiving a first user input indicating the selection of at least one of the one or more first images.
A method further comprising providing the target software application with the at least one image in response to receiving the first user input.
前記画像ライブラリ内の１つまたは複数の第２の画像を識別することとをさらに備え、前記１つまたは複数の第２の画像の各々は、前記１つまたは複数の第１の画像特性のうちの少なくとも１つを有しており、
更新されたユーザインターフェイスを表示させることを含み、前記更新されたユーザインターフェイスは、前記１つまたは複数の第２の画像を含み、前記ユーザインターフェイスは、前記１つまたは複数の第２の画像の選択を可能にする、請求項１３に記載の方法。 Determining one or more first image characteristics of the at least one image and
Further comprising identifying one or more second images in the image library, each of the one or more second images is among the one or more first image properties. Have at least one of
The updated user interface includes displaying the updated user interface, the updated user interface includes the one or more second images, and the user interface is a selection of the one or more second images. 13. The method of claim 13.
品質閾値を満たす画像ライブラリから前記１つまたは複数の第１の画像を選択することを含む、請求項１５に記載の方法。 The application type includes an image sharing application, and identifying the one or more first images
15. The method of claim 15, comprising selecting the one or more first images from an image library that meets a quality threshold.
前記画像ライブラリから前記１つまたは複数の第１の画像を選択することを含み、前記１つまたは複数の第１の画像は、レシート、文書、またはスクリーンショットのうちの１つまたは複数を含む画像ラベルに関連付けられる、請求項１５に記載の方法。 The application type includes a financial application and identifying the one or more first images
Including selecting the one or more first images from the image library, the one or more first images include one or more of a receipt, document, or screenshot. 15. The method of claim 15, associated with a label.
前記メッセージング会話における参加者のうちの少なくとも１つを示す画像を前記画像ライブラリから選択することを含む、請求項１５に記載の方法。 The application type includes a messaging application, determining the context information further comprises receiving participant identification information in a messaging conversation in the messaging application, the one or more first images. To identify
15. The method of claim 15, comprising selecting from the image library an image showing at least one of the participants in the messaging conversation.
前記アプリケーションコンテキストに基づいて１つまたは複数のセマンティック概念を決定することと、
前記１つまたは複数の第１の画像を選択することとを含み、前記１つまたは複数の第１の画像の各々の少なくとも１つの画像特性は、前記１つまたは複数のセマンティック概念のうちの少なくとも１つと一致する、請求項１３に記載の方法。 Determining the context information includes receiving an application context from the target software application, and identifying the one or more first images.
Determining one or more semantic concepts based on the application context
At least one image property of each of the one or more first images, including selecting the one or more first images, is at least one of the one or more semantic concepts. 13. The method of claim 13, which is consistent with one.
画像ライブラリ内の１つまたは複数の第１の画像の選択を示す第１のユーザ入力を受信することと、
前記１つまたは複数の第１の画像の１つまたは複数の第１の画像特性を決定することと、
前記画像ライブラリにおける１つまたは複数の第２の画像を識別することとを備え、
前記１つまたは複数の第２の画像の各々は、前記１つまたは複数の第１の画像特性のうちの少なくとも１つに一致する少なくとも１つの第２の画像特性に関連付けられ、
ユーザインターフェイスを表示させることをさらに備え、前記ユーザインターフェイスは、前記１つまたは複数の第２の画像を含み、前記ユーザインターフェイスは、前記１つまたは複数の第２の画像の選択を可能にし、
前記１つまたは複数の第２の画像のうちの少なくとも１つの選択を示す第２のユーザ入力を受信することと、
前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの少なくとも１つに基づいて、提案されたアクション要素を前記ユーザインターフェイスに表示させることと、
前記提案されたアクション要素のユーザ選択の受信に応答して、前記提案されたアクション要素に関連付けられたアクションを実行することとをさらに備え、前記アクションは、前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの少なくとも１つをアーカイブすること、前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの少なくとも１つを削除すること、前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの前記少なくとも１つを含む画像ベースの創作物を生成すること、または、前記１つまたは複数の第１の画像および前記１つまたは複数の第２の画像のうちの前記少なくとも１つの自動的な強調を実行することの１つまたは複数を含む。 A non-transitory computer-readable medium in which instructions are stored, when executed by one or more hardware processors, causes the one or more hardware processors to perform the following operations, the following: The behavior of
Receiving a first user input indicating the selection of one or more first images in the image library, and
Determining one or more first image characteristics of the one or more first images and
It comprises identifying one or more second images in the image library.
Each of the one or more second images is associated with at least one second image property that matches at least one of the one or more first image properties.
Further comprising displaying a user interface, said user interface comprises said one or more second images, said user interface allows selection of said one or more second images.
Receiving a second user input indicating a selection of at least one of the one or more second images.
Displaying a proposed action element in the user interface based on at least one of the one or more first images and the one or more second images.
The action further comprises performing an action associated with the proposed action element in response to receiving a user selection of the proposed action element, the action being the one or more first images. And archiving at least one of the one or more second images, at least one of the one or more first images and the one or more second images. Deleting, producing an image-based creation containing said one or more first images and said at least one of said said one or more second images, or said one or more. Includes one or more of performing said at least one of the plurality of first images and said one or more second images.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2022163336A JP2023011633A (en) | 2017-12-22 | 2022-10-11 | Image selection suggestion |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762609543P | 2017-12-22 | 2017-12-22 | |
US62/609,543 | 2017-12-22 | ||
PCT/US2018/067242 WO2019126723A1 (en) | 2017-12-22 | 2018-12-21 | Image selection suggestions |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022163336A Division JP2023011633A (en) | 2017-12-22 | 2022-10-11 | Image selection suggestion |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2021504803A true JP2021504803A (en) | 2021-02-15 |
JP7158478B2 JP7158478B2 (en) | 2022-10-21 |
Family
ID=65139199
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2020528152A Active JP7158478B2 (en) | 2017-12-22 | 2018-12-21 | image selection suggestions |
JP2022163336A Pending JP2023011633A (en) | 2017-12-22 | 2022-10-11 | Image selection suggestion |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022163336A Pending JP2023011633A (en) | 2017-12-22 | 2022-10-11 | Image selection suggestion |
Country Status (6)
Country | Link |
---|---|
US (1) | US11775139B2 (en) |
EP (2) | EP4239498A3 (en) |
JP (2) | JP7158478B2 (en) |
KR (1) | KR102437640B1 (en) |
CN (1) | CN110678861B (en) |
WO (1) | WO2019126723A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2023007958A1 (en) * | 2021-07-30 | 2023-02-02 | 富士フイルム株式会社 | Data creation device, storage device, data processing system, data creation method, and program |
JP7443427B2 (en) | 2021-06-21 | 2024-03-05 | 株式会社カカオ | Emoticon recommendation method and user terminal that provides emoticon recommendation |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6590329B1 (en) * | 2019-06-26 | 2019-10-16 | 株式会社ラディウス・ファイブ | Image display system and program |
WO2022070825A1 (en) * | 2020-09-30 | 2022-04-07 | ソニーグループ株式会社 | Information processing device, information processing method, and program |
US20230095027A1 (en) * | 2021-09-24 | 2023-03-30 | Deep Sentinel Corp. | System and method for reducing surveillance detection errors |
WO2023206576A1 (en) * | 2022-04-29 | 2023-11-02 | 北京小米移动软件有限公司 | Image processing method and apparatus |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH0973530A (en) * | 1995-09-01 | 1997-03-18 | Sharp Corp | Picture filing device |
JP2003248699A (en) * | 2002-02-26 | 2003-09-05 | Minolta Co Ltd | Image control server and program |
JP2007188404A (en) * | 2006-01-16 | 2007-07-26 | Nikon Corp | Image search device, image search method, and image search program |
US20070220045A1 (en) * | 2006-03-17 | 2007-09-20 | Microsoft Corporation | Array-Based Discovery of Media Items |
JP2010146201A (en) * | 2008-12-17 | 2010-07-01 | Noritsu Koki Co Ltd | Image processor, image forming apparatus, image processing method, and image processing program |
JP2012079337A (en) * | 2011-12-19 | 2012-04-19 | Fujifilm Corp | Image arrangement device, method and program |
JP2012133507A (en) * | 2010-12-21 | 2012-07-12 | Fujitsu Marketing Ltd | Receipt data collation support apparatus and receipt data collation support program |
JP2015069277A (en) * | 2013-09-27 | 2015-04-13 | 株式会社ニコン | Image selection device and program |
Family Cites Families (32)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP4279083B2 (en) * | 2003-08-18 | 2009-06-17 | 富士フイルム株式会社 | Image processing method and apparatus, and image processing program |
US7860319B2 (en) * | 2005-05-11 | 2010-12-28 | Hewlett-Packard Development Company, L.P. | Image management |
US20150161175A1 (en) * | 2008-02-08 | 2015-06-11 | Google Inc. | Alternative image queries |
US8589402B1 (en) * | 2008-08-21 | 2013-11-19 | Adobe Systems Incorporated | Generation of smart tags to locate elements of content |
US9002120B2 (en) * | 2008-10-03 | 2015-04-07 | Intellectual Ventures Fund 83 Llc | Interactive image selection method |
US8422794B2 (en) * | 2009-07-30 | 2013-04-16 | Intellectual Ventures Fund 83 Llc | System for matching artistic attributes of secondary image and template to a primary image |
US9153056B2 (en) | 2013-03-07 | 2015-10-06 | Shutterfly, Inc. | Adaptive and fast image collage creation |
US8909625B1 (en) | 2011-06-02 | 2014-12-09 | Google Inc. | Image search |
US20130125069A1 (en) | 2011-09-06 | 2013-05-16 | Lubomir D. Bourdev | System and Method for Interactive Labeling of a Collection of Images |
JP6101896B2 (en) * | 2012-01-11 | 2017-03-29 | ノーリツプレシジョン株式会社 | Image processing program and image processing apparatus |
US8963962B2 (en) * | 2012-03-06 | 2015-02-24 | Apple Inc. | Display of multiple images |
US20140244514A1 (en) * | 2013-02-26 | 2014-08-28 | Digimarc Corporation | Methods and arrangements for smartphone payments and transactions |
US9247309B2 (en) * | 2013-03-14 | 2016-01-26 | Google Inc. | Methods, systems, and media for presenting mobile content corresponding to media content |
US9727901B2 (en) * | 2013-06-13 | 2017-08-08 | Yahoo! Inc. | Systems and methods for image-based recommendations |
CN103338256B (en) | 2013-06-28 | 2015-09-23 | 腾讯科技（深圳）有限公司 | Image sharing method, device, server and system |
US9542417B2 (en) * | 2013-11-27 | 2017-01-10 | Eagle View Technologies, Inc. | Preferred image retrieval |
WO2015120019A1 (en) | 2014-02-10 | 2015-08-13 | Google Inc. | Smart camera user interface |
WO2015132548A1 (en) | 2014-03-07 | 2015-09-11 | Fractograf, LLC | Method and apparatus for generating interactive photo mosaic images |
US9934222B2 (en) * | 2014-04-22 | 2018-04-03 | Google Llc | Providing a thumbnail image that follows a main image |
US9524540B2 (en) * | 2014-07-22 | 2016-12-20 | Adobe Systems Incorporated | Techniques for automatically correcting groups of images |
US9740963B2 (en) * | 2014-08-05 | 2017-08-22 | Sri International | Multi-dimensional realization of visual content of an image collection |
JP6149015B2 (en) * | 2014-09-10 | 2017-06-14 | 富士フイルム株式会社 | Image processing apparatus, image processing method, program, and recording medium |
WO2016109450A1 (en) | 2014-12-29 | 2016-07-07 | Neon Labs Inc. | Selecting a high-valence representative image |
US20170249339A1 (en) | 2016-02-25 | 2017-08-31 | Shutterstock, Inc. | Selected image subset based search |
US11227017B2 (en) * | 2016-05-17 | 2022-01-18 | Google Llc | Providing suggestions for interaction with an automated assistant in a multi-user message exchange thread |
CN106021455A (en) | 2016-05-17 | 2016-10-12 | 乐视控股（北京）有限公司 | Image characteristic relationship matching method, apparatus and system |
CN117634495A (en) | 2016-09-20 | 2024-03-01 | 谷歌有限责任公司 | Suggested response based on message decal |
US10140675B2 (en) | 2016-11-28 | 2018-11-27 | Google Llc | Image grid with selectively prominent images |
US20180164990A1 (en) * | 2016-12-14 | 2018-06-14 | Facebook, Inc. | Methods and Systems for Editing Content of a Personalized Video |
US10860854B2 (en) | 2017-05-16 | 2020-12-08 | Google Llc | Suggested actions for images |
US11163819B2 (en) * | 2017-10-23 | 2021-11-02 | Adobe Inc. | Image search and retrieval using object attributes |
US10891526B2 (en) | 2017-12-22 | 2021-01-12 | Google Llc | Functional image archiving |
-
2018
- 2018-12-21 EP EP23183169.4A patent/EP4239498A3/en active Pending
- 2018-12-21 EP EP18837058.9A patent/EP3729294A1/en not_active Ceased
- 2018-12-21 WO PCT/US2018/067242 patent/WO2019126723A1/en unknown
- 2018-12-21 JP JP2020528152A patent/JP7158478B2/en active Active
- 2018-12-21 CN CN201880035457.2A patent/CN110678861B/en active Active
- 2018-12-21 KR KR1020207015395A patent/KR102437640B1/en active IP Right Grant
-
2021
- 2021-12-06 US US17/543,068 patent/US11775139B2/en active Active
-
2022
- 2022-10-11 JP JP2022163336A patent/JP2023011633A/en active Pending
Patent Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH0973530A (en) * | 1995-09-01 | 1997-03-18 | Sharp Corp | Picture filing device |
JP2003248699A (en) * | 2002-02-26 | 2003-09-05 | Minolta Co Ltd | Image control server and program |
JP2007188404A (en) * | 2006-01-16 | 2007-07-26 | Nikon Corp | Image search device, image search method, and image search program |
US20070220045A1 (en) * | 2006-03-17 | 2007-09-20 | Microsoft Corporation | Array-Based Discovery of Media Items |
JP2010146201A (en) * | 2008-12-17 | 2010-07-01 | Noritsu Koki Co Ltd | Image processor, image forming apparatus, image processing method, and image processing program |
JP2012133507A (en) * | 2010-12-21 | 2012-07-12 | Fujitsu Marketing Ltd | Receipt data collation support apparatus and receipt data collation support program |
JP2012079337A (en) * | 2011-12-19 | 2012-04-19 | Fujifilm Corp | Image arrangement device, method and program |
JP2015069277A (en) * | 2013-09-27 | 2015-04-13 | 株式会社ニコン | Image selection device and program |
Non-Patent Citations (1)
Title |
---|
"類似画像検索を用いた目的地決定支援システム", ヒューマンインタフェース学会誌 ＶＯＬ．５ ＮＯ．３, JPN6021022247, 26 August 2003 (2003-08-26), ISSN: 0004702184 * |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP7443427B2 (en) | 2021-06-21 | 2024-03-05 | 株式会社カカオ | Emoticon recommendation method and user terminal that provides emoticon recommendation |
WO2023007958A1 (en) * | 2021-07-30 | 2023-02-02 | 富士フイルム株式会社 | Data creation device, storage device, data processing system, data creation method, and program |
Also Published As
Publication number | Publication date |
---|---|
US11775139B2 (en) | 2023-10-03 |
US20220091706A1 (en) | 2022-03-24 |
WO2019126723A1 (en) | 2019-06-27 |
KR20200080287A (en) | 2020-07-06 |
EP4239498A3 (en) | 2023-10-04 |
EP4239498A2 (en) | 2023-09-06 |
EP3729294A1 (en) | 2020-10-28 |
KR102437640B1 (en) | 2022-08-26 |
CN110678861B (en) | 2023-12-08 |
CN110678861A (en) | 2020-01-10 |
JP2023011633A (en) | 2023-01-24 |
JP7158478B2 (en) | 2022-10-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11209442B2 (en) | Image selection suggestions | |
US11778028B2 (en) | Automatic image sharing with designated users over a communication network | |
US11574470B2 (en) | Suggested actions for images | |
US10979373B2 (en) | Suggested responses based on message stickers | |
US11829404B2 (en) | Functional image archiving | |
JP7158478B2 (en) | image selection suggestions | |
KR102574279B1 (en) | Predicting topics of potential relevance based on retrieved/created digital media files | |
US10691740B1 (en) | Interface elements for directed display of content data items | |
EP3948659B1 (en) | Automatic generation of people groups and image-based creations |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20200526 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20200526 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20210526 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20210615 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20210914 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20220208 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20220506 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20220913 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20221011 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7158478Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |