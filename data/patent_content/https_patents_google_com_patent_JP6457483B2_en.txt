JP6457483B2 - Frame interpolation with motion compensation using smoothness constraints - Google Patents
Frame interpolation with motion compensation using smoothness constraints Download PDFInfo
- Publication number
- JP6457483B2 JP6457483B2 JP2016505598A JP2016505598A JP6457483B2 JP 6457483 B2 JP6457483 B2 JP 6457483B2 JP 2016505598 A JP2016505598 A JP 2016505598A JP 2016505598 A JP2016505598 A JP 2016505598A JP 6457483 B2 JP6457483 B2 JP 6457483B2
- Authority
- JP
- Japan
- Prior art keywords
- frame
- interpolated
- motion
- interpolation
- generating
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000033001 locomotion Effects 0.000 title claims description 289
- 239000013598 vector Substances 0.000 claims description 100
- 238000000034 method Methods 0.000 claims description 36
- 230000002123 temporal effect Effects 0.000 claims description 26
- 238000012545 processing Methods 0.000 claims description 16
- 230000004927 fusion Effects 0.000 claims description 3
- 238000004891 communication Methods 0.000 description 95
- 238000006243 chemical reaction Methods 0.000 description 22
- 230000000694 effects Effects 0.000 description 16
- 238000010586 diagram Methods 0.000 description 14
- 230000006870 function Effects 0.000 description 14
- 208000037170 Delayed Emergence from Anesthesia Diseases 0.000 description 8
- 230000005540 biological transmission Effects 0.000 description 8
- 238000013139 quantization Methods 0.000 description 7
- 238000012805 post-processing Methods 0.000 description 6
- 238000009499 grossing Methods 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 5
- 238000005457 optimization Methods 0.000 description 5
- 230000008569 process Effects 0.000 description 5
- 230000009466 transformation Effects 0.000 description 4
- 230000004048 modification Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 238000012935 Averaging Methods 0.000 description 2
- HBBGRARXTFLTSG-UHFFFAOYSA-N Lithium ion Chemical compound [Li+] HBBGRARXTFLTSG-UHFFFAOYSA-N 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 229910003460 diamond Inorganic materials 0.000 description 2
- 239000010432 diamond Substances 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 229910001416 lithium ion Inorganic materials 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- QELJHCBNGDEXLD-UHFFFAOYSA-N nickel zinc Chemical compound [Ni].[Zn] QELJHCBNGDEXLD-UHFFFAOYSA-N 0.000 description 2
- 230000000750 progressive effect Effects 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000005070 sampling Methods 0.000 description 2
- 238000003860 storage Methods 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- PXHVJJICTQNCMI-UHFFFAOYSA-N Nickel Chemical compound [Ni] PXHVJJICTQNCMI-UHFFFAOYSA-N 0.000 description 1
- 208000003028 Stuttering Diseases 0.000 description 1
- 230000001133 acceleration Effects 0.000 description 1
- 230000003044 adaptive effect Effects 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- OJIJEKBXJYRIBZ-UHFFFAOYSA-N cadmium nickel Chemical compound [Ni].[Cd] OJIJEKBXJYRIBZ-UHFFFAOYSA-N 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 239000002131 composite material Substances 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 230000006837 decompression Effects 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000011156 evaluation Methods 0.000 description 1
- 239000000446 fuel Substances 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 230000012447 hatching Effects 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 229910000652 nickel hydride Inorganic materials 0.000 description 1
- 239000013307 optical fiber Substances 0.000 description 1
- 238000007781 pre-processing Methods 0.000 description 1
- 238000011084 recovery Methods 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000006467 substitution reaction Methods 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
- 230000001131 transforming effect Effects 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/577—Motion compensation with bidirectional frame interpolation, i.e. using B-pictures
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/189—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding
- H04N19/192—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding the adaptation method, adaptation tool or adaptation type being iterative or recursive
- H04N19/194—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding the adaptation method, adaptation tool or adaptation type being iterative or recursive involving only two passes
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/44—Decoders specially adapted therefor, e.g. video decoders which are asymmetric with respect to the encoder
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/513—Processing of motion vectors
- H04N19/517—Processing of motion vectors by encoding
- H04N19/52—Processing of motion vectors by encoding by predictive encoding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/527—Global motion vector estimation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/53—Multi-resolution motion estimation; Hierarchical motion estimation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/553—Motion estimation dealing with occlusions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/587—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal sub-sampling or interpolation, e.g. decimation or subsequent interpolation of pictures in a video sequence
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/59—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving spatial sub-sampling or interpolation, e.g. alteration of picture size or resolution
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/597—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding specially adapted for multi-view video sequence encoding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N7/00—Television systems
- H04N7/01—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level
- H04N7/0127—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level by changing the field or frame frequency of the incoming video signal, e.g. frame rate converter
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N7/00—Television systems
- H04N7/01—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level
- H04N7/0135—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level involving interpolation processes
- H04N7/014—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level involving interpolation processes involving the use of motion vectors
Description
本発明は、ビデオフレーム補間（video frame interpolation）またはPull型フレーム補間（pull frame interpolation）のシステム、方法および装置に関するものである。 The present invention relates to systems, methods and apparatus for video frame interpolation or pull frame interpolation.
デジタルビデオは、例えば、テレビ会議を介した遠隔地でのビジネス会議、高解像度ビデオエンターテインメント、ビデオ広告、またはユーザが生成したビデオの共有に用いることができる。したがって、時間的および空間的なフレーム補間を行うことには利点がある。 Digital video can be used, for example, for remote business meetings via video conferencing, high resolution video entertainment, video advertising, or sharing of user generated video. Therefore, there are advantages to performing temporal and spatial frame interpolation.
Pull型フレーム補間の方法の一態様は、複数の入力ビデオフレームを特定すること、前記複数の入力ビデオフレームの第１のフレームから、前記複数の入力ビデオフレームの第２のフレームに、モーションを示す複数のモーションベクトルを生成すること、前記第１のフレームおよび前記第２のフレームとの間に補間ポイントを特定すること、前記複数のモーションベクトルに基づいて、前記第１のフレームから前記補間ポイントへのモーションおよび前記第２のフレームから前記補間ポイントまでを示す複数の候補補間モーションベクトルを生成すること、測定基準に基づいて、前記複数の候補補間モーションベクトルから、補間モーションベクトルを選択すること、および選択された前記補間モーションベクトルに基づいて、前記補間ポイントで補間されたフレームを生成することを含んでもよい。 One aspect of the Pull-type frame interpolation method is to identify a plurality of input video frames, and to show motion from a first frame of the plurality of input video frames to a second frame of the plurality of input video frames. Generating a plurality of motion vectors; identifying an interpolation point between the first frame and the second frame; and from the first frame to the interpolation point based on the plurality of motion vectors. Generating a plurality of candidate interpolated motion vectors indicative of a plurality of motions and from the second frame to the interpolation point, selecting an interpolated motion vector from the plurality of candidate interpolated motion vectors based on a metric, and Based on the selected interpolation motion vector, the interpolation poi Generating a frame-interpolated frame.
Pull型フレーム補間の方法の他の態様は、複数の入力ビデオフレームを特定すること、前記複数の入力ビデオフレームの第１のフレームから、前記複数の入力ビデオフレームの第２のフレームに、モーションを示す複数のモーションベクトルを生成すること、前記複数のモーションベクトルの平滑性の程度を判定すること、前記平滑性の程度が閾値を越える状態で、前記補間されたフレームとして前記第１のフレームまたは前記第２のフレームを使用することと、前記平滑性の程度が閾値内にある状態で、前記第１のフレームと前記第２のフレームの間の補間ポイントを特定することを共同して使用すること、組み合わせられたエネルギー機能を用いること、遮蔽を特定すること、および前記第１のフレームから前記補間ポイントへのモーションを示す候補補間モーションベクトルと、前記第２のフレームから前記補間ポイントへのモーションを示す候補補間モーションベクトルと、複数の隣接サイトに対するモーション予測に基づく候補補間モーションベクトルとを含む複数の候補補間モーションベクトルを、前記複数のモーションベクトルに基づいて生成することを含んでもよい。複数の補間サイト内の各補間サイトに対して、Pull型フレーム補間は、前記補間されたフレーム内の平滑性の制約および前記第１のフレームと前記第２のフレームの間の平滑性の制約に基づいて、前記複数の候補補間モーションベクトルから補間モーションベクトルを選択すること、補間モーションベクトルを選択すること、および前記選択された補間モーションベクトルに基づいて前記補間されたフレームを更新することを含んでもよい。Pull型フレーム補間は、前記選択された補間モーションベクトルに基づいて、前記補間ポイントで補間されたフレームを生成することを含んでもよい。前記補間されたフレームは、前記複数の補間サイトを含み、前記補間されたフレームを生成することは、前記補間されたフレームと前記第１のフレームおよび前記第２のフレームの平均とを融合することで、前記補間モーションベクトルに基づいて前記補間されたフレームの加工を修正することを含み、前記融合の程度は、高いモーション勾配を有する前記補間されたフレームの一部を、前記第１のフレームと前記第２のフレームの前記平均に対応する領域と置き換えるように、前記補間モーションベクトルに関連するモーションフィールドの勾配に基づくものでもよい。 Another aspect of the pull type frame interpolation method is to specify a plurality of input video frames, and to apply motion from a first frame of the plurality of input video frames to a second frame of the plurality of input video frames. Generating a plurality of motion vectors, determining a degree of smoothness of the plurality of motion vectors, and in a state where the degree of smoothness exceeds a threshold, the first frame or the Jointly using a second frame and identifying an interpolation point between the first frame and the second frame with the degree of smoothness being within a threshold. Using a combined energy function, identifying occlusion, and motion from the first frame to the interpolation point A plurality of candidate interpolation motions, including candidate interpolation motion vectors indicating motion, candidate interpolation motion vectors indicating motion from the second frame to the interpolation point, and candidate interpolation motion vectors based on motion prediction for a plurality of adjacent sites. Generating a vector based on the plurality of motion vectors may be included. For each interpolation site in a plurality of interpolation sites, Pull-type frame interpolation is subject to a smoothness constraint within the interpolated frame and a smoothness constraint between the first frame and the second frame. Based on, selecting an interpolated motion vector from the plurality of candidate interpolated motion vectors, selecting an interpolated motion vector, and updating the interpolated frame based on the selected interpolated motion vector. Good. Pull-type frame interpolation may include generating a frame interpolated at the interpolation point based on the selected interpolation motion vector. The interpolated frame includes the plurality of interpolation sites, and generating the interpolated frame fuses the interpolated frame with the average of the first frame and the second frame. And modifying the processing of the interpolated frame based on the interpolated motion vector, wherein the degree of fusion includes replacing the portion of the interpolated frame having a high motion gradient with the first frame. It may be based on the gradient of the motion field associated with the interpolated motion vector so as to replace the region corresponding to the average of the second frame.
上記の態様および他の態様における変形例は、以下に詳細に説明する。
本明細書の記載は、添付の図面を参照する。いくつかの図において、同じ参照番号は、同じ部分について述べるものである。
Modifications in the above and other aspects will be described in detail below.
The description herein refers to the accompanying drawings. In the several figures, the same reference numerals refer to the same parts.
本発明によれば、時間的および空間的なフレーム補間を行うことができる。 According to the present invention, temporal and spatial frame interpolation can be performed.
デジタルビデオは、例えば、テレビ会議による遠隔地とのビジネス会議、高解像度のビデオエンターテインメント、ビデオ広告、およびユーザが生成したビデオの共有等、さまざまな目的に用いることができる。ビデオ信号の生成および表示は、異なるフレームレートで実行してもよい。Pull型フレーム補間（Pull frame interpolation）を実行し、１つのフレームレートから別のフレームレートに変換し、スローモーション効果等、時間的または空間的なビデオ効果をもたらしてもよい。 Digital video can be used for a variety of purposes, for example, business meetings with remote locations via video conferencing, high resolution video entertainment, video advertising, and sharing of user generated video. Video signal generation and display may be performed at different frame rates. Pull frame interpolation may be performed to convert from one frame rate to another, resulting in temporal or spatial video effects such as slow motion effects.
ビデオ信号の生成には、アナログまたはデジタルフォーマットのビデオ信号を生成することが含まれてもよい。フォーマットによって、各々２フィールドのインタレース画像が含まれてもよく、各フレームで利用可能なラインの半分は、時間単位（またはフレームサンプリング周期）でサンプリングされる。時間単位（フレームレート）のフレーム数を変えてもよいし、あるフレームレートから別のフレームレートに変換してもよい。非モーション補償フレームレート変換は、フレームのドロップまたはリピートに基づくこともあるし、モーションを十分に維持しないこともある。フレーム補間等のモーション補償フレームレート変換では、モーションをより十分に維持してもよいし、ビデオ信号からモーション情報を使用して、新たな補間フレームを作成することを含めてもよい。 Generating the video signal may include generating a video signal in an analog or digital format. Depending on the format, two fields of interlaced images may be included, and half of the lines available in each frame are sampled in time units (or frame sampling periods). The number of frames in a time unit (frame rate) may be changed, or a certain frame rate may be converted to another frame rate. Non-motion compensated frame rate conversion may be based on dropped or repeated frames or may not maintain enough motion. In motion compensated frame rate conversion, such as frame interpolation, motion may be more fully maintained or may include creating new interpolated frames using motion information from the video signal.
Pull型フレーム補間を用いてフレームレートを変換してもよい。いくつかの実施態様において、Pull型フレーム補間を用いて、時間的または空間的なビデオ効果もたらすようにしてもよい。例えば、Pull型フレーム補間では、追加フレームを生成し、スローモーション効果への移行またはスローモーション効果の終了、あるいは空間動作効果をもたらすために空間的に近接した入力フレーム間にフレームを補間してもよい。 The frame rate may be converted using pull type frame interpolation. In some implementations, pull-type frame interpolation may be used to provide temporal or spatial video effects. For example, in Pull-type frame interpolation, additional frames can be generated and frames can be interpolated between input frames that are spatially close together to create a transition to a slow motion effect or end of a slow motion effect, or a spatial motion effect. Good.
Pull型フレーム補間は、連続的に存在するフレームから引き抜かれた（pulled）モーション情報を使用して補間フレームを生成することを含んでもよい。モーション情報は、いかなるモーション推定装置により生成されてもよい。補間されたモーションは、画像補間とは別に生成してもよい。Pull型フレーム補間は、モーションベクトル選択の候補に基づいて最適化することを含んでもよい。後処理を行うことにより、例えば、ぼやけているあるいは低品質な入力データの処理を改善してもよい。いくつかの実施態様において、Pull型フレーム補間は、例えば、大量のモーションを含む入力の性能を高めるマルチ解像マルチパススキーム（multiresolution multipass scheme）を使用してもよい。 Pull-type frame interpolation may include generating an interpolated frame using motion information pulled from continuously existing frames. The motion information may be generated by any motion estimation device. The interpolated motion may be generated separately from the image interpolation. Pull-type frame interpolation may include optimizing based on motion vector selection candidates. By performing post-processing, for example, processing of blurred or low-quality input data may be improved. In some implementations, pull-type frame interpolation may use, for example, a multiresolution multipass scheme that enhances the performance of inputs that contain a large amount of motion.
図１は、本開示の実施態様に係るコンピューティング装置１００の図である。コンピューティング装置１００は、通信インタフェース１１０、通信部１２０、ユーザインタフェース（ＵＩ）１３０、プロセッサ１４０、メモリ１５０、命令１６０、電源１７０、または、いかなるそれらの組み合わせを含む。本明細書で用いられる「コンピューティング装置（computing device）」という用語は、本明細書に開示のいかなる装置、いかなる方法も実行可能な装置の組み合わせ、またはいかなる部分をも含む。
FIG. 1 is a diagram of a
コンピューティング装置１００は、パーソナルコンピュータ（ＰＣ）、サーバ、ワークステーション、ミニコンピュータ、またはメインフレーム等、移動しないコンピューティング装置でも、自動車電話、個人情報機器（ＰＤＡ）、ラップトップ、またはタブレット型パソコン等のモバイルコンピュータでもよい。単一の装置として図示するが、コンピューティング装置１００の１つ以上のいかなる構成要素も、いかなる数の別の物理的装置に組み込むことができる。例えば、ＵＩ１３０およびプロセッサ１４０は第１の物理装置に組み込み、メモリ１５０は第２の物理装置に組み込むことができる。
The
通信インタフェース１１０は、図示するように無線アンテナでもよいし、有線の通信ポートでもよく、有線または無線の電子通信媒体１８０に連結可能なイーサネット（登録商標）ポート、赤外線ポート、直列ポートまたは他のいかなる有線または無線装置等でもよい。
The
通信部１２０は、有線または無線の媒体１８０を介して信号を送信または受信するように構成することができる。例えば、図示するように、通信部１２０は、無線信号で通信するように構成されたアンテナに、動作可能なように接続している。図１に明示されていないが、通信部１２０は、無線周波数（ＲＦ）、紫外線（ＵＶ）、可視光、光ファイバ、または有線配線、あるいはそれらの組み合わせ等、いかなる有線または無線の通信メディアを介しても、送信、受信、あるいは両方を行うように構成することができる。図１は、単一の通信部１２０および単一の通信インタフェース１１０を示すが、いかなる数の通信部およびいかなる数の通信インタフェースを用いることができる。
The
ＵＩ１３０は、仮想または物理キーパッド、タッチパッド、ディスプレイ、タッチディスプレイ、スピーカ、マイクロホン、ビデオカメラ、センサ、またはいかなるそれらの組み合わせ等、ユーザとのインターフェイスとなりうるいかなる装置も含むことができる。図示するように、ＵＩ１３０は、プロセッサまたは電源１７０等、通信装置１００の他のいかなる構成要素にも連結して動作可能とすることができる。単一の装置として図示するが、ＵＩ１３０は、１つ以上の物理装置を含んでもよい。例えば、ＵＩ１３０は、ユーザと音声通信を実行するための音声インターフェースと、ユーザとのコミュニケーションに基づいて、映像および接触を実行するためのタッチディスプレイとを含んでもよい。別々の装置として図示するが、通信インタフェース１１０、通信部１２０、およびＵＩ１３０、またはその一部は、複合装置として構成されてもよい。例えば、通信インタフェース１１０、通信部１２０およびＵＩ１３０は、外部のタッチスクリーン装置と連結可能な通信ポートとして実現されてもよい。
The
プロセッサ１４０は、光プロセッサ、量子プロセッサ、分子プロセッサ、またはそれらの組み合わせを含み、現存しているまたは今後開発される信号または他の情報を操作または処理可能ないかなる装置またはシステムも含むことができる。例えば、プロセッサ１４０は、汎用プロセッサ、専用プロセッサ、従来のプロセッサ、ディジタル信号処理装置（ＤＳＰ）複数のマイクロプロセッサ、ＤＳＰコアに関連した１つ以上のマイクロプロセッサ、コントローラ、マイクロコントローラ、エイシック（ＡＳＩＣ）、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）、プログラマブルロジックアレイ、プログラマブルロジックコントローラ、マイクロコード、ファームウェア、任意の集積回路（ＩＣ）、ステートマシン、または、いかなるそれらの組み合わせも含めることができる。本明細書で用いられる「プロセッサ」という用語は、シングルプロセッサまたはマルチプロセッサを含む。プロセッサ１４０は、通信インタフェース１１０、通信部１２０、ＵＩ１３０、メモリ１５０、命令１６０、電源１７０、または、いかなるそれらの組み合わせとも動作可能に連結することができる。
The
メモリ１５０は、非一時的な（non-transitory）、コンピュータ使用可能な、またはコンピュータ読み取り可能な、いかなる媒体も含むことができ、プロセッサ１４０によって、またはプロセッサ１４０と連結して、命令１６０またはそれに関連するいかなる情報についても、例えば、含有、格納、伝達または転送可能ないかなる有形装置でもよい。非一時的コンピュータ使用可能またはコンピュータ読み取り可能な媒体は、例えば、ソリッドステートドライブ、メモリーカード、取り外し可能媒体、読取り専用メモリ（ＲＯＭ）、ランダムアクセスメモリ（ＲＡＭ）、ハードディスクを含む任意のディスク、フロッピー（登録商標）ディスク、光ディスク、磁気または光カード、特定用途向けＩＣ（ＡＳＩＣ）、または電子情報格納に適する任意の非一時的なメディアとすることができる。メモリ１５０は、例えば、プロセッサ１４０に、例えば、メモリバス（明示的に図示せず）経由で接続することができる。
命令１６０には、いかなる方法または本明細書に開示の一部を実行するための指示を含めることができる。命令１６０は、ハードウェア、ソフトウェアまたはいかなるそれらの組み合わせにより実現することができる。例えば、命令１６０は、本明細書に開示のように、それぞれの方法、アルゴリズム、態様またはそれらの組み合わせのいずれかを実行するために、プロセッサ１４０によって実行されるコンピュータプログラムプログラム等、メモリ１５０に格納される情報として実現してもよい。命令１６０またはその一部は、本明細書に記載のように、方法、アルゴリズム、実施態様またはそれらの組み合わせのいずれかを実行するための専門ハードウェアを含むことが可能な専用プロセッサまたは回路として実現してもよい。命令１６０の一部を、同じ機械または異なる機械上の複数のプロセッサ全体に、または、ＬＡＮ、ＷＡＮ、インターネットまたはそれらの組み合わせ等のネットワーク全体に分配することができる。
電源１７０は、通信装置１１０に電力を供給するいかなる適切な装置とすることもできる。例えば、電源１７０は、有線の電源、ニッケル‐カドミウム（ＮｉＣｄ）、ニッケル‐亜鉛（ＮｉＺｎ）、ニッケル水素（ＮｉＭＨ）、リチウム‐イオン（リチウムイオン）等の１つ以上の乾電池、太陽電池、燃料電池、または、通信装置１１０に電力を供給することが可能な他のいかなる装置も含むことができる。通信インタフェース１１０、通信部１２０、ＵＩ１３０、プロセッサ１４０、命令１６０、メモリ１５０、またはいかなるそれらの組み合わせも、電源１７０に動作可能に連結することができる。
The
別々の構成要素として図示するが、通信インタフェース１１０、通信部１２０、ＵＩ１３０、プロセッサ１４０、命令１６０、電源１７０、メモリ１５０またはいかなるそれらの組み合わせも、１つ以上の電子装置、回路またはチップに一体化することができる。
Although illustrated as separate components,
図２は、本開示の実施態様に係るコンピューティングおよび通信システム２００である。コンピューティングおよび通信システム２００は、１つ以上のコンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃ、１つ以上のアクセスポイント２１０Ａ／２１０Ｂ、１つ以上のネットワーク２２０、または、それらの組み合わせを含んでもよい。例えば、コンピューティングおよび通信システム２００は、コンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃ等、１つ以上の有線または無線の通信装置に対して、音声、データ、ビデオ、メッセージ送信、同報通信、またはそれらの組み合わせ等の通信を可能にする複数のアクセス・システムとすることができる。説明を簡単にするために、図２は、３つのコンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃ、２つのアクセスポイント２１０Ａ／２１０Ｂ、および１つのネットワーク２２０を示すが、いかなる数のコンピューティングおよび通信装置、アクセスポイント、およびネットワークも用いることができる。
FIG. 2 is a computing and
コンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃは、例えば、図１に示すコンピューティング装置１００等のコンピューティング装置とすることができる。例えば、図示するように、コンピューティングおよび通信装置１００Ａ／１００Ｂは、モバイルコンピュータ装置、ラップトップ、シンクライアント（thin client）またはスマートフォン等のユーザ装置でもよく、コンピューティングおよび通信装置１００Ｃは、メインフレームまたはクラスタ等のサーバとしてもよい。コンピューティングおよび通信装置１００Ａ／１００Ｂは、ユーザ装置として説明し、コンピューティングおよび通信装置１００Ｃは、サーバとして説明するが、いかなるコンピューティングおよび通信装置も、サーバの機能のいくらかまたは全て、ユーザ装置の機能のいくらかまたは全て、または、サーバおよびユーザ装置の機能のいくらかまたは全てを実行することができる。
Computing and
各コンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃは、有線または無線通信を実行するよう構成することができる。例えば、コンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃは、有線または無線通信信号を送信または受信するように構成することができ、ユーザ機器（ＵＥ）、移動局（mobile station）、固定または移動の加入者ユニット、移動電話、パーソナルコンピュータ、タブレットコンピュータ、サーバ、家電、またはいかなる類似のデバイスも含むことができる。各コンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃを単一の装置として図示するが、コンピューティングおよび通信装置は、相互に連結したいかなる数の構成要素も含むことができる。
Each computing and
各アクセスポイント２１０Ａ／２１０Ｂは、有線または無線の通信リンク１８０Ａ／１８０Ｂ／１８０Ｃを介して、コンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃ、ネットワーク２２０、または両方に通信するように構成される任意デバイスとすることができる。例えば、アクセスポイント２１０Ａ／２１０Ｂは、基地局、基地送受信局（ＢＴＳ）、ノード‐Ｂ、拡張ノード‐Ｂ（ｅＮｏｄｅ‐Ｂ）、ホームノード‐Ｂ（ＨＮｏｄｅ‐Ｂ）、無線ルータ、有線ルータ、ハブ、リレー、スイッチ、またはいかなる類似の有線または無線の装置も含むことができる。各アクセスポイント２１０Ａ／２１０Ｂは、単一の装置として図示するが、アクセスポイントは、相互に連結したいかなる数の構成要素も含むことができる。
Each
ネットワーク２２０は、有線または無線の通信リンクを通して、音声、データ、アプリケーション、ＶｏＩＰ（ＶｏＩＰ）、または他のいかなる通信プロトコルあるいは通信プロトコルの組合せ等のサービスを提供するように構成される任意のネットワークとすることができる。例えば、ネットワーク２２０は、ローカルエリアネットワーク（ＬＡＮ）、ワイドエリアネットワーク（ＷＡＮ）、仮想プライベートネットワーク（ＶＰＮ）、モバイルまたはセルラー電話ネットワーク、インターネット、または、他のいかなる電子通信手段とすることもできる。ネットワーク２２０は、伝送制御プロトコル（ＴＣＰ）、ユーザデータグラムプロトコル（ＵＤＰ）、インターネットプロトコル（ＩＰ）、リアルタイムがプロトコル（ＲＴＰ）、ハイパーテキスト転送プロトコル（ＨＴＴＰ）、またはそれらの組み合わせ等の通信プロトコルを使用することができる。
コンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃは、１つ以上の有線または無線の通信リンクを用いるネットワーク２２０を介して、あるいは、有線および無線の通信リンクの組合せを介して、互いに通信することができる。例えば、図示するように、コンピューティングおよび通信装置１００Ａ／１００Ｂは、無線通信リンク１８０Ａ／１８０Ｂを介して通信することができ、コンピューティングおよび通信装置１００Ｃは、有線通信リンク１８０Ｃを介して通信することができる。コンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃのいずれも、いかなる有線または無線の通信リンクを用いても通信することができる。例えば、第１のコンピューティングおよび通信装置１００Ａは、第１のタイプの通信リンクを使用する第１のアクセスポイント２１０Ａを介して通信することができ、第２のコンピューティングおよび通信装置１００Ｂは、第２のタイプの通信リンクを使用する第２のアクセスポイント２１０Ｂを介して通信することができ、第３のコンピューティングおよび通信装置１００Ｃは、第３のタイプの通信リンクを使用する第３のアクセスポイント２１０Ｃを介して通信することができる。同様に、アクセスポイント２１０Ａ／２１０Ｂは、有線または無線の通信リンク２３０Ａ／２３０Ｂの１つ以上のタイプの通信リンクを介して、ネットワーク２２０と通信することができる。図２は、ネットワーク２２０を介して通信するコンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃを図示するが、コンピューティングおよび通信装置１００Ａ／１００Ｂ／１００Ｃは、直接の有線または無線の通信リンク等、いかなる数の通信リンクを介しても、それぞれ通信することができる。
The computing and
コンピューティングおよび通信システム２００の他の実施態様も可能である。例えば、ネットワーク２２０をアドホックネットワークとすることができ、１つ以上のアクセスポイント２１０Ａ／２１０Ｂを省略することができる。コンピューティングおよび通信システム２００は、図２に図示されていない装置、ユニット、または構成要素を含んでもよい。例えば、コンピューティングおよび通信システム２００は、より多くの通信装置、ネットワークおよびアクセスポイントを含んでもよい。
Other implementations of the computing and
図３は、本開示の実施形態に係る、符号化（encoding）、復号化（decoding）、フレーム補間（frame interpolation）またはいかなるそれらの組み合わせにも用いられるビデオストリーム３００の図である。例えば、コンピュータによって生成されるビデオカメラまたはビデオストリームによって取り込まれるビデオストリーム等のビデオストリーム３００は、ビデオシーケンス３１０を含んでもよい。ビデオシーケンス３１０は、一連の隣接するフレーム３２０を含んでもよい。３つの隣接するフレーム３２０を示すが、ビデオシーケンス３１０は、いかなる数の隣接するフレーム３２０を含んでもよい。隣接するフレーム３２０の各フレーム３３０は、ビデオストリームの単一の画像を表してもよい。フレーム３３０は、ブロック３４０を含んでもよい。図３に示されていないが、１ブロックには、複数のピクセルを含むことができる。例えば、１ブロックには、１６ｘ１６のピクセルグループ、８ｘ８のピクセルグループ、８ｘ１６のピクセルグループまたは他のいかなるピクセルのグループも含むことができる。本明細書において示されない限り、用語「ブロック」は、マクロブロック、セグメント、スライスまたはフレームの他のいかなる部分も含んでもよい。フレーム、ブロック、ピクセルまたはそれらの組み合わせは、輝度情報、クロミナンス情報（chrominance information）、またはビデオストリーム３００またはその一部分を、格納、修正、伝達、表示のために用いることが可能な他の情報等の表示情報を含むことができる。
FIG. 3 is a diagram of a
図４は、本開示の実施態様に係るエンコーダ４００のブロック図である。エンコーダ４００は、例えば、図１に示されるメモリ１５０等のデータ記憶部に格納されたコンピュータソフトウェアプログラムとして、図１に示すコンピュータ１００や図２に示すコンピューティング及び通信デバイス１ＯＯＡ／１ＯＯＢ／１ＯＯＣ等の装置において実行することができる。コンピュータソフトウェアプログラムは、図１に示すプロセッサ１６０等のプロセッサによって実行することができる機械語の指示を含むことができ、本明細書において記載するように、その装置にビデオデータを符号化させてもよい。エンコーダ４００は、例えば、コンピュータ１００に含まれる専門ハードウェアとして実行することができる。
FIG. 4 is a block diagram of an
エンコーダ４００は、図３に示すビデオストリーム３００等の入力ビデオストリーム４０２を符号化することができ、符号化された（圧縮された）ビットストリーム４０４を生成する。いくつかの実施態様において、エンコーダ４００は、圧縮されたビットストリーム４０４を生成するためのフォワードパス（forward path）を含んでもよい。フォワードパスは、イントラ／インター予測部４１０、変換部４２０、量子化部４３０、エントロピー復号部４４０、またはいかなるそれらの組み合わせを含んでもよい。いくつかの実施態様において、エンコーダ４００は、更なるブロックの符号化のためのフレームを復元するために、（破線の接続線によって示される）復元パス（reconstruction path）を含んでもよい。復元パスには、逆量子化部４５０、逆変換部４６０、再生部４７０、ループフィルター部４８０、またはいかなるそれらの組み合わせも含めることができる。エンコーダ４００の他の構造上の変形例を用いて、ビデオストリーム４０２を符号化することができる。
The
ビデオストリーム４０２を符号化するために、ビデオストリーム４０２の内の各フレームを、ブロック単位で処理することができる。このように、現在のブロックをフレーム内のブロックから特定し、現在のブロックを符号化してもよい。
To encode the
イントラ／インター予測部４１０で、現在のブロックは、単一のフレーム内にあるフレーム内予測またはフレームからフレームまでにあるフレーム間予測のいずれか使用して符号化することができる。フレーム内予測は、過去には符号化されていたが復元された現在のフレーム内のサンプルから予測ブロックを生成することを含んでもよい。フレーム間予測は、１つ以上の過去に再生された基準フレーム内のサンプルから予測ブロックを生成することを含んでもよい。現在のフレーム内の現在ブロックの予測ブロックを生成することは、基準フレーム内の適切な基準ブロックを示すモーションベクトルを生成するためのモーション推定（motion estimation）を行うことを含んでもよい。
In the intra /
イントラ／インター予測部４１０は、現在のブロック（生ブロック）から予測ブロックを減算し、残差ブロックを生成してもよい。変換部４２０は、ブロックに基づいた変換を実行してもよく、残差ブロックを、例えば、周波数領域の変換係数に変換することを含んでもよい。ブロックに基づいた変換の例として、クーネン−レープ変換（Karhunen-Loeve Transform）（ＫＬＴ）、離散コサイン変換（Discrete Cosine Transform）（ＤＣＴ）、および特異値分解変換（Singular Value Decomposition Transform）（ＳＶＤ）がある。一例において、ＤＣＴは、１ブロックを周波数領域に変えることを含んでもよい。ＤＣＴは、マトリックスの左上に最も低い周波数（すなわちＤＣ）係数を有し、マトリックスの右下に最も高い周波数係数を有する空間的周波数に基づいて変換係数値を使用することを含んでもよい。
The intra /
量子化部４３０は、変換係数を、量子化変換係数あるいは量子化レベルと呼ばれる別々の量子値に換算してもよい。量子化変換係数は、エントロピーが符号化された係数を生成するために、エントロピー復号部４４０によって符号化されるエントロピーとすることができる。エントロピー符号化は、確率分布法を使用することを含んでもよい。ブロックを復号化するために用いられるエントロピーが符号化された係数および情報は、使用される予測のタイプ、モーションベクトル、および量子化部の値を含んでもよく、圧縮されたビットストリーム４０４へ出力することができる。圧縮されたビットストリーム４０４は、ランレングス符号化（run-length encoding）（ＲＬＥ）およびゼロラン符号化（zero-run coding）等、さまざまな技術を用いてフォーマット化することができる。
The
再生パスを用いて、エンコーダ４００と、図５に示すデコーダ５００等の対応するデコーダとの間の基準フレーム同期化を維持することができる。再生パスは、後述する復号化プロセスと類似していてもよく、逆量子化部４５０で量子化された変換係数を逆量子化すること、および導関数残差ブロック（derivative residual block）を生成するように逆変換部４６０で逆量子化された変換係数を逆変換することを含んでもよい。再生部４７０は、イントラ／インター予測部４１０によって生成される予測ブロックを、再生ブロックを生成するために、導関数残差ブロックに追加してもよい。ブロックの加工等のひずみを減らすために、ループフィルター部４８０を再生ブロックに適用することができる。
The playback path can be used to maintain reference frame synchronization between the
エンコーダ４００の他の変形例を用いて、圧縮されたビットストリーム４０４を符号化することができる。例えば、非変換に基づくエンコーダ４００は、変換部４２０を用いずに直接残差ブロックを量子化することができる。いくつかの実施態様において、量子化部４３０および逆量子化部４５０を、単一の装置に結合してもよい。
Other variations of the
図５は、本開示の実施態様に係るデコーダ５００のブロック図である。デコーダ５００は、例えば、図１に示されるメモリ１５０等のデータ記憶部に格納されるコンピュータソフトウェアプログラムとして、図１に示すコンピュータ１００あるいは図２に示すコンピューティング及び通信デバイス１ＯＯＡ／１ＯＯＢ／１ＯＯＣ等の装置において実行することができる。コンピュータソフトウェアプログラムは、図１に示すプロセッサ１６０等のプロセッサによって実行することができる機械語の指示を含むことができ、本明細書において記載するように、その装置にビデオデータを符号化させてもよい。エンコーダ４００は、例えば、コンピュータ１００に含まれる専門ハードウェアとして実行することができる。
FIG. 5 is a block diagram of a
デコーダ５００は、図４に示される圧縮されたビットストリーム４０４等の圧縮されたビットストリーム５０２を受信してもよく、出力ビデオストリーム５０４を生成するために、圧縮されたビットストリーム５０２を復号化してもよい。デコーダ５００は、エントロピー復号化部５１０、逆量子化部５２０、逆変換部５３０、イントラ／インター予測部５４０、再生部５５０、ループフィルター部５６０、デブロッキングフィルタ部５７０、またはいかなるそれらの組み合わせを含んでもよい。デコーダ５００の他の構造上の変形例は、圧縮されたビットストリーム５０２を復号化するために用いることができる。
The
エントロピー復号化部５１０は、例えば、コンテキスト適応的なバイナリ算術的復号（Context Adaptive Binary Arithmetic Decoding）を用いて、一組の量子化された変換係数を生成するために、圧縮ビットストリーム５０２内でデータエレメントを復号化してもよい。逆量子化部５２０は、量子化された変換係数を逆量子化することができ、逆変換部５３０は、図４に示される逆変換部４６０によって生成される導関数残差ブロックに一致する導関数残差ブロックを生成するために、逆量子化された変換係数を逆変換することができる。圧縮されたビットストリーム５０２から復号化されるヘッダ情報を用いて、イントラ／インター予測部５４０は、エンコーダ４００で生成される予測ブロックに対応する予測ブロックを生成してもよい。再生部５５０で、予測ブロックを派生的な残差ブロックに追加し、再生ブロックを生成することができる。ブロックの加工を減らすために、ループフィルター部５６０を再生されたブロックに適用することができる。ブロックのひずみを減らすために、デブロッキングフィルタ部５７０を再生されたブロックに適用することができ、その結果を出力ビデオストリーム５０４として出力してもよい。
The
デコーダ５００の他の変形例を用いて、圧縮されたビットストリーム５０２を復号化することができる。例えば、デコーダ５００は、デブロッキングフィルタ部５７０を使用しないで出力ビデオストリーム５０４を生成してもよい。
Other variations of the
図６は、本開示の実施態様に係るビデオフレームレート変換のタイムラインの例を示す。いくつか実施態様において、ビデオフレームレート変換は、図１に示すコンピュータ１００等の装置、図４に示すエンコーダ４００によって実行して、図３に示されるビデオストリーム３００等の入力ビデオストリームのフレームレートを、出力ビデオストリームに変換してもよい。図示するように、入力ビデオストリームの各フレーム６０２は円で示され、対応する出力ビデオストリーム用の補間された出力フレーム６０４それぞれはダイヤモンドで示される。
FIG. 6 shows an example of a timeline for video frame rate conversion according to an embodiment of the present disclosure. In some embodiments, the video frame rate conversion is performed by a device such as the
一番上のタイムライン６１０は、フレームレート変換の一例を示し、出力フレームレートは、入力フレームレートの複数倍、例えば３倍としてもよい。例えば、図示するように、入力フレームレートは、毎秒２５フレーム（ｆｐｓ）でもよく、出力フレームレートは７５ｆｐｓでもよい。図示するように、補間フレーム６０４の１／３は、元のフレーム６０２と一致し、補間フレーム６０４の残りの２／３は元のフレーム６０２との中間でもよい。出力は、２５ｆｐｓの入力フレームレートで示してもよく、３倍スローダウンして現れるスローモーション効果を発生する。３倍のスローモーション指数を一例として記載するが、他のいかなる倍率のスローモーションを用いてもよい。
The
中央のタイムライン６２０は、フレームレート変換の一例を示す。入力フレームレートは２５ｆｐｓでもよく、出力フレームレートは３０ｆｐｓでもよい。図示すように、出力フレーム６０４の位置は、入力フレーム６０２に対して均一の間隔でなくてもよい。出力フレーム６０４の位置は、補間フレーム６０４の作成に用いることができる定期性を有してもよい。
The
一番下のタイムライン６３０は、フレームレート変換の一例を示し、入力フレームレートは２５ｆｐｓで、出力フレームレートは５０ｆｐｓでもよく、２５ｆｐｓから５０ｆｐｓへ出力フレームレートが線形に移行する。例えば、出力ビデオシーケンスは、時間の減速またはスローモーション効果を示してもよい。この最後の場合では、時間内の出力フレーム位置に単純な定期性がなくてもよい。
The
いくつかの実施態様において、１つのフレームまたはフィールド率と、別のものとの変換は、非動作補償変換（non-motion compensating conversion）を含んでもよい。０次保持変換（zero-order hold conversion）などのフレームリピート、またはサブサンプリング変換（subsampling conversion）などのフレームドロップを含んでもよい。例えば、３０ｆｐｓ（秒単位６０フィールド）でインタレースされたビデオを、２５ｆｐｓ（秒単位５０フィールド）でインタレースされたビデオに変換することは、３０ｆｐｓのソースから、３００フィールドごとに５０フィールドをドロップすることを含んでもよい。このように、ソースから６フィールドごとに１フィールド分をドロップしてもよい。インタレースされた２５ｆｐｓをインタレースされた３０ｆｐｓに変換することは、ソースから６フィールドごとに１フィールド分をリピートすることを含んでもよい。ドロップまたはリピートされるフィールドは、低品質な変換画像を生成し、６つのうち１つのフレームは、フレームにマージされる誤ったフィールド有することになる。これにより、不完全なモーションを表す結果となり、変換されたマテリアルのスタッカー効果（stutter effect）のように感じられるかもしれない。いくつかの実施態様において、ビデオデータから補間することで、失われたフィールドを推定してもよい。例えば、ある時刻で、垂直に線を平均化することで、偶数フィールドから奇数フィールドを推定してもよい。その次の時刻において、推定されたフィールドをリピートしたり、元のフィールドをドロップしたりしてもよい。 In some implementations, conversion of one frame or field rate to another may include non-motion compensating conversion. Frame repeat such as zero-order hold conversion or frame drop such as subsampling conversion may be included. For example, converting video interlaced at 30 fps (60 fields per second) to video interlaced at 25 fps (50 fields per second) drops 50 fields every 300 fields from a 30 fps source. You may include that. Thus, one field may be dropped every six fields from the source. Converting interlaced 25 fps to interlaced 30 fps may include repeating one field every six fields from the source. A field that is dropped or repeated produces a low quality transformed image, and one of the six frames will have the wrong field merged into the frame. This results in imperfect motion and may feel like a stutter effect in the transformed material. In some implementations, the lost field may be estimated by interpolating from the video data. For example, the odd field may be estimated from the even field by averaging the lines vertically at a certain time. At the next time, the estimated field may be repeated or the original field may be dropped.
非動作補償変換は、モーションを十分に維持しなくてもよい。例えば、フレームごとに５ピクセルのモーション等の大量のモーションは、十分に保存されないかもしれない。プログレッシブフォーマット（progressive formats）への変換またはプログレッシブフォーマット間の変換は、モーションを十分に維持しないことがある。いくつかの実施態様において、変換には、ビデオデータに由来するモーション情報を使用するモーション補償技術を含んでもよい。モーション補償変換は、モーションの軌道に沿って補間することを目的とすることで、新しいフィールドまたはフレームを補間することを含んでもよい。モーション補償変換は、遮蔽（occlusion）を取り扱うことを含んでもよい。フレームの一部があるフレームでは隠され、別のフレームでは可視となる。あるフレームまたは別のフレームにおいて遮蔽されるフレームの一部は、変換用に利用できなくてもよい。 Non-motion compensated transforms may not maintain enough motion. For example, a large amount of motion, such as 5 pixel motion per frame, may not be adequately preserved. Conversion to progressive formats or between progressive formats may not maintain enough motion. In some implementations, the conversion may include motion compensation techniques that use motion information derived from video data. The motion compensated transform may include interpolating a new field or frame with the goal of interpolating along the motion trajectory. The motion compensation transformation may include handling occlusion. Part of the frame is hidden in one frame and visible in another. Some of the frames that are occluded in one frame or another may not be available for conversion.
いくつかの実施態様において、モーション補償は、回路等の専用のモーション補償ハードウェアによって実行されてもよい。例えば、リアルタイム変換は、モーション補償回路を使用して行ってもよい。ハードウェアに基づくモーション補償は、ソフトウェアまたはハードウェアおよびソフトウェアの組合せにおいて行うモーション補償と比べると、複雑性においては限界があるかもしれない。 In some implementations, motion compensation may be performed by dedicated motion compensation hardware such as circuitry. For example, real-time conversion may be performed using a motion compensation circuit. Hardware-based motion compensation may be limited in complexity compared to motion compensation performed in software or a combination of hardware and software.
いくつかの実施態様において、モーション補償は、ポストプロダクション（post-production）ソフトウェア等のソフトウェアで行ってもよい。例えば、ソフトウェアに基づく変換は、映画などのビデオにおけるスローモーション効果を生成するために用いてもよい。ソフトウェアに基づく非リアルタイム変換は、時間または空間の任意のポイントでフレームを補間することを含んでもよい。このように、変換は、スローモーション効果を生成するためにフレームレートを下げ、スローモーション効果から移行してフレームレートを上げることを含んでもよい。 In some embodiments, motion compensation may be performed with software such as post-production software. For example, software-based conversion may be used to generate slow motion effects in videos such as movies. Software-based non-real-time conversion may include interpolating frames at any point in time or space. Thus, the conversion may include decreasing the frame rate to generate a slow motion effect and shifting from the slow motion effect to increase the frame rate.
いくつかの実施態様において、変換は、非時間的に連続するフレームの間に補間することを含んでもよい。例えば、空間的に連続するフレームを補間することで、滑らかなスペース移動効果等の効果をもたらしてもよい。いくつかの実施態様において、空間的に連続するフレームを、並行して、または、ほとんど並行して取り込んでもよい。 In some implementations, the transformation may include interpolating between non-temporal consecutive frames. For example, effects such as a smooth space movement effect may be brought about by interpolating spatially continuous frames. In some embodiments, spatially contiguous frames may be captured in parallel or nearly in parallel.
いくつかの実施態様において、既存のフレームからの画像データを、関連する画像の間の最少勾配の輪郭に沿って、補間フレームに押し込んで（push）もよい。Push型補間は、補間フレームに、既存のフレームからピクセル値を複製することを含んでもよい。Push型補間は、説得力のあるフレーム補間を生成することができるが、モーションの方向に沿って最適というわけではない。例えば、入力ビデオシーケンスを目標フレームレートで見ると、変換におけるモーションの適合性が正確ではないかもしれない。 In some embodiments, image data from an existing frame may be pushed into the interpolated frame along a minimal gradient contour between related images. Push-type interpolation may include duplicating pixel values from an existing frame in an interpolated frame. Push interpolation can generate compelling frame interpolation, but is not optimal along the direction of motion. For example, looking at the input video sequence at the target frame rate, the motion fit in the conversion may not be accurate.
いくつかの実施態様において、フレーム補間は、保管された映画フィルムやビデオ映画フィルムの失なわれたフレームを回復することを含んでもよい。フレーム回復のためのフレーム補間は、任意の瞬間で、その瞬間のモーションフィールドを回復することによって、フレームを復元することを含んでもよい。 In some embodiments, frame interpolation may include recovering lost frames of stored motion picture or video motion picture film. Frame interpolation for frame recovery may include restoring the frame at any moment by restoring the motion field at that moment.
図７は、本開示の実施態様に係るPull型フレーム補間の一例の図を示す。入力フレーム７１０／７２０／７３０は、ダイヤモンドの形状によって示され、単純化したシーンの左上から右下へ移動するオブジェクト７４０を取り込む。図７に示すように、このシーンは明らかに白い背景であるが、他の内容を含んでもよい。第１のフレーム７１０および第２のフレーム７２０間の補間フレーム７５０および第２のフレーム７２０および第３のフレーム７３０間の補間フレーム７６０を破線で示す。
FIG. 7 shows a diagram of an example of pull-type frame interpolation according to an embodiment of the present disclosure. The
例えば、第１の入力フレーム７１０は、第１の時刻Ｔ１でそのシーンを取り込み、第２の入力フレーム７２０は、第２の時刻Ｔ２でそのシーンを取り込み、第３の入力フレーム７３０は、第３の時刻Ｔ３でそのシーンを取り込んでもよい。第１の補間フレーム７５０は、第１の時刻Ｔ１および第２の時刻Ｔ２の間の時点でそのシーンを補完し、第２の補間フレーム７６０は、第２の時刻Ｔ２および第３の時刻Ｔ３の間の時点でそのシーンを補完してもよい。
For example, the
いくつかの実施態様において、Pull型フレーム補間は、時間停止またはタイムスライス効果を発生させることを含み、カメラが空間を移動するように見え、時間は減速または停止したかのように見えてもよい。例えば、時間停止効果は、ある期間、複数の異なる空間位置に配置される複数のカメラによって並行して記録されるフレームを用いて生成されてもよい。第１の入力フレーム７１０は、空間Ｔ１の第１の位置で、そのシーンを取り込み、第２の入力フレーム７２０は、空間Ｔ２の第２の位置で、そのシーンを取り込み、第３の入力フレーム７３０は、空間Ｔ３の第３の位置で、そのシーンを取り込んでもよい。入力フレーム７１０／７２０／７３０は、同じ時点または実質的に同じ時点でそのシーンを取り込んでもよい。第１の補間フレーム７５０は、第１の空間点Ｔ１および第２の空間点Ｔ２との間の空間の一点で、そのシーンを補完し、第２の補間フレーム７６０は、第２の空間点Ｔ２および第３の空間点Ｔ３との間の空間の一点で、そのシーンを補完してもよい。補間フレーム７５０／７６０は、入力フレーム７１０／７２０／７３０と、時間において同じ点または実質的に同じ点と関連してもよい。
In some embodiments, pull-type frame interpolation includes generating a time stop or time slice effect, the camera may appear to move in space, and the time may appear to slow down or stop . For example, the time stop effect may be generated using frames that are recorded in parallel by a plurality of cameras arranged in a plurality of different spatial positions for a certain period of time. The
オブジェクト７４０により隠されたり覆われたりする背景のように、そのシーンの遮蔽された領域７７０が、十字ハッチングで示されている。あるフレームに示され、次のフレームで遮蔽されたシーンの覆われていない領域７８０が、斑点で示されている。モーション軌道ライン７９０行も示されている。いくつかの実施態様において、Pull型フレーム補間では、遮蔽された領域７７０、覆われていない領域７８０、およびモーション軌道７９０を保存することを含んでもよい。
Like the background hidden or covered by the
いくつかの実施態様において、Pull型フレーム補間は、入力フレーム７１０／７２０／７３０のデータに基づいて、補間フレーム７５０／７６０のピクセル強度を推定することを含んでもよい。モーション情報を用いて、モーションの方向に沿って補間される位置に、入力フレーム７１０／７２０／７３０からのピクセル強度を複製してもよく、このように、ピクセル単位で、補間フレーム７５０／７６０ピクセルを形成する。遮蔽された領域７７０のピクセルは、次のフレーム用に利用できなくてもよい。覆われてない領域７８０のピクセルは、以前のフレーム用に利用できなくてもよい。
In some implementations, pull-type frame interpolation may include estimating the pixel intensity of the interpolated
図８は、本開示の実施態様に係るPull型フレーム補間の一例の他の図を示す。いくつかの実施態様において、Pull型フレーム補間は、第１の入力フレーム８１０と第２の入力フレーム８１２との間に、破線で示される補間フレーム８００を生成することを含んでもよい。いくつかの実施態様において、Pull型フレーム補間は、隣接する２つの入力フレームの間に、補間フレームを作成するために４つの入力フレーム８１０／８１２／８１４／８１６を使用することを含んでもよい。補間フレーム８００は、時間または空間、時点ｔ＋Δと推定されてもよい。図８は、単一の補間フレーム８００を示すが、いかなる数の補間フレームも、入力フレームに基づいて第１のフレーム８１０および第２のフレーム８１２の間に生成してもよい。４つの入力フレームに基づくPull型フレーム補間を本明細書において説明するが、２つ以上のフレームのいかなるシーケンスに基づいて、Pull型フレーム補間を実行してもよい。
FIG. 8 shows another example of pull-type frame interpolation according to an embodiment of the present disclosure. In some implementations, pull-type frame interpolation may include generating an interpolated
入力フレーム８１０／８１２／８１４／８１６は、空間的または時間的シーケンスとして取り込まれるシーンを含んでもよい。例えば、第１の入力フレーム８１０は、第１の時刻ｔでそのシーンを取り込み、第２の入力フレーム８１２は、次の時刻ｔ＋１でそのシーンを取り込み、第３の入力フレーム８１４は、他の次の時刻ｔ＋２でそのシーンを取り込み、第４の入力フレーム８１６は、以前の時刻ｔ−１でそのシーンを取り込んでもよい。
他の例において、第１の入力フレーム８１０は、空間ｔの第１のポイントでそのシーンを取り込み、第２の入力フレーム８１２は、空間ｔ＋１の次のポイントでそのシーンを取り込み、第３の入力フレーム８１４は、空間ｔ＋２の他の次のポイントでそのシーンを取り込み、第４の入力フレーム８１６は、空間ｔ−１の以前のポイントでそのシーンを取り込んでもよい。補間フレーム８００は、ｔの第１のフレーム８１０とｔ＋１の第２のフレーム８１２の間のポイントｔ＋Δで生成してもよい。１つの補間フレームを示すが、いかなる数の補間フレームを第１のフレーム８１０と第２のフレーム８１２の間のポイントで生成してもよい。
The
In another example, the
補間フレーム８００は、ｔの第１のフレーム８１０から時間または空間間隔Δで、かつｔ＋１の第２のフレーム８１２から１−Δでオフセットしてもよい。オブジェクト等、取り込まれたシーンのエレメントを、フレームに沿って均一に変換する長方形として示す。例えば、オブジェクトは、ｔ−１のフレーム８１６の第１の位置８２０に、ｔのフレーム８１０の第２の位置８２２に、ｔ＋１のフレーム８１２の第３の位置８２４に、ｔ＋２のフレーム８１４の第４の位置８２６に示される。オブジェクトは、フレーム内で移動しているように示されるが、オブジェクトは、そのシーンのフレームおよび他のエレメント内で、静止していてもよいし、実質的に静止していてもよい。例えば、背景がオブジェクトと関連して移動してもよい。オブジェクトの補間された位置８３０を、ｔ＋Δの補間フレーム８００に破線長方形として示す。
The interpolated
いくつかの実施態様において、Pull型フレーム補間は、Pull型フレーム補間から独立して生成されてもよいモーション推定情報を使用することを含んでもよい。例えば、いかなるモーション推定技法を用いて、Pull型フレーム補間の前にモーション推定情報を生成してもよい。位置xにおいて、ｔのフレーム８１０とｔ＋１のフレーム８１２の間のモーションをdt,t+1（x) = [d1; d2]と表してもよい。ｄ１およびｄ２は、モーションの水平および垂直の構成要素を示す。フレームｔのｘでのピクセルの強度をIt(x)で表してもよい。
以前のフレームのモーションが補償されたピクセルの位置は、It-1(x+dt,t-1(x))で表してもよい。
In some implementations, pull-type frame interpolation may include using motion estimation information that may be generated independently of pull-type frame interpolation. For example, any motion estimation technique may be used to generate motion estimation information prior to pull-type frame interpolation. At the position x, the motion between the
The position of the pixel in which the motion of the previous frame is compensated may be expressed as I t−1 (x + d t, t−1 (x)).
ｔ−１のフレーム８１４とｔのフレーム８１０間のオブジェクトのモーションを、dt,t-1として表してもよいが、モーションベクトル８４０の一例を用いて表す。ｔのフレーム８１０とｔ＋１のフレーム８１２の間のオブジェクトのモーションを、dt,t+1として表してもよいが、モーションベクトル８４２の他の例を用いて表す。ｔ＋１のフレーム８１２とｔのフレーム８１０の間の背景のモーションを、dt+1,tとして表してもよいが、ゼロモーションベクトル８４４の一例を用いて表す。ｔ＋１のフレーム８１２とｔ＋２のフレーム８１４の間の背景のモーションを、dt+1,t+2として表してもよいが、モーションベクトル８４６の他の例を用いて表す。
The motion of the object between the t-1
ｔ＋Δの補間されたフレーム８００とｔのフレーム８１０の間に補間されたモーションをdt+Δ,tとして表してもよく、ｔ＋Δの補間されたフレーム８００とｔ＋１のフレーム８１２の間に補間されたモーションをdt+Δ,t+1として表してもよい。
The motion interpolated between the t + Δ interpolated
いくつかの実施態様において、Pull型フレーム補間は、遮蔽状態情報を使用することを含んでもよい。閉塞状態情報は、各ピクセルに関連した遮蔽状態をフレームに含んでもよい。例えば、フレームｔの位置ｘのピクセルと関連した遮蔽状態は、st(x)=[00; 01; 10]として表されてもよく、st(x)=00は、ピクセルが次および以前のフレームで遮蔽されていないことを示し、st(x)=01は、ピクセルが次のフレームで遮蔽される（前方の遮蔽）ことを示し、st(x)=10は、ピクセルが以前のフレームで遮蔽されることを示す（後方の遮蔽）。遮蔽状態を有するｔ＋Δの補間フレーム８００の各位置の関連を、それぞれハッチングおよび斑点を用いてｔ＋Δで示す。ｔのフレーム８１２およびｔ＋１のフレーム８１０に存在するシーンの内容に対応する補間された画像データの遮蔽状態をｓ＝００として表してもよい。パッチに相当する補間された画像データの遮蔽状態は、ｔのフレーム８１０では存在しないあるいは遮蔽されており、ｔ＋1のフレーム８１２は覆われていないので、ｓ＝１０として表してもよい。パッチに相当する補間された画像データの遮蔽状態は、ｔのフレーム８１０では存在し、ｔ＋１のフレーム８１２では存在しないあるいは遮蔽されており、ｓ＝０１として表してもよい。
In some implementations, pull-type frame interpolation may include using occlusion state information. The occlusion state information may include a occlusion state associated with each pixel in the frame. For example, shielding condition associated with the pixel position x of the
いくつかの実施態様において、Pull型フレーム補間モデルを以下のように表してもよい。
Pull型フレーム補間は、ｔ＋Δの補間フレーム８００とｔの入力フレーム８１０との間と、ｔ＋Δの補間フレーム８００とｔ＋1の入力フレーム８１２との間に、モーションフィールドを推定することを含んでもよく、ピクセルst+Δ(x)の状態を推定することを含んでもよい。ｔ＋Δでモーションを補間することをPull型プロセスと呼んでもよく、数２を用いて画像Ｉｔ＋Δを作成するために、ｔの入力フレーム８１０およびｔ＋１の入力フレーム８１２からピクセルを引きぬく（pull）ように、ｔ＋Δの補間フレーム８００でモーションを使用することを含んでもよい。
Pull-type frame interpolation may include estimating a motion field between the t +
いくつかの実施態様において、Ｄ、ｉは、既存のモーション推定を含み、画像データd(x)は、現在のサイトに近接する補間フレーム内のモーションを集めてもよく、ベイズ的方法（Bayesian fashion）において、後方確率分布p(dt+Δ;t+1, dt+Δt|D, i)を操作することを以下に表してもよい。
dt+δ,の推定は、補間されたモーションとして使用される、数２で後半部分を最大にしてもよい。
The estimation of d t + δ may be maximized in the latter half of
いくつかの実施態様において、Pull型フレーム補間は、画像尤度を使用することを含んでもよい。画像尤度を用いて、eI(x) = It(x+dt+Δ,t)-It+1(x+dt+Δ,t+1)は、次のフレームのピクセルと以前のフレームのピクセルの間で、モーションが補償されたピクセルの差を示してもよい。例えば、画像は色画像でもよく、eIは、３色の平面に対応する３つの差のベクトルであってもよい。いくつかの実施態様において、補間されたモーションは正確であり、遮蔽が発生しない限り、３色の平面に対応する差は小さくてもよい。 In some implementations, pull-type frame interpolation may include using image likelihood. Using image likelihood, e I (x) = I t (x + d t + Δ, t ) -I t + 1 (x + d t + Δ, t + 1 ) is the pixel of the next frame The motion compensated pixel difference may be indicated between pixels of the previous frame. For example, the image may be a color image and e I may be a vector of three differences corresponding to three color planes. In some implementations, the interpolated motion is accurate and the difference corresponding to the three color planes may be small as long as no occlusion occurs.
いくつかの実施態様において、ｔ＋Δの画像データは演繹的に理解されることはなく、以下に表されるように、モーションは、明確にs(・)を組み込むために用いてもよい。
いくつかの実施態様において、kIは10´2.72に等しくしてもよく、画像データの遮蔽から離間する強いバイアスを可能にする。色画像において、eI 2は、異なる３つの構成要素の二乗の平均等、測定されたベクトルの大きさでもよい。いくつかの実施態様において、δI 2は、ピクセルデータから測定可能であるか、または１：０に設定してもよい。 In some embodiments, k I may be equal to 10'2.7 2, allowing a strong bias away from the shield of the image data. In a color image, e I 2 may be a measured vector magnitude, such as the mean of the squares of three different components. In some embodiments, δ I 2 can be measured from pixel data or set to 1: 0.
いくつかの実施態様において、Pull型フレーム補間は、モーション尤度を含んでもよい。モーション尤度は、本当に補間されたモーションが、既存のフレーム間にすでに推定されたモーションに一致するように用いられてもよい。Pull型フレーム補間は、モーションが補償されたモーション相違（motion compensated motion differences）を小さくすることを促進し、モーションの一致を最大にすることを含んでもよい。モーションが補償されたモーション相違を小さくすることを促進することは、以下の通りにモーションが補償されたモーション相違を表すことを含んでもよい。
数４〜８において、補間されたモーションフィールドdt+Δのｘ引数は、明確にするため省略されている。 In equations 4-8, the x argument of the interpolated motion field d t + Δ is omitted for clarity.
いくつかの実施態様において、s(・)を組み込んでもよく、モーション尤度を以下のように表してもよい。
数９で、αは、遮蔽された状態１０，０１の時間的連続性の損失のバランスをとり、遮蔽された状態の発生を阻止するペナルティエネルギー（penalty energies）を表し、edは、加速を示すモーションベクトルペアにペナルティを課してもよい（penalize）。状態s(・) = 00のモーション尤度により、補間されたモーションがフレームｔ，ｔ−１；ｔ，ｔ＋１；ｔ＋１，ｔ＋２間の既存のモーションと一致するよう促してもよい。他の状態（０１；１０）において、時間的な平滑性は、それぞれｔ，ｔ−１およびｔ＝１，ｔ＋２の間のモーションによって促されてもよい。
By the number 9, alpha is balancing the temporal continuity of the loss of
図９は、本開示の実現態様に係る従うサイトグリッド（site grid）９００付近の（隣接した）一例の図を示す。グリッドとしてのフレームの表現において、現在のピクセル９１０は、近接する８つの隣接グリッド９２０を有してもよい。
FIG. 9 shows a diagram of an example near (adjacent) a
いくつかの実施態様において、Pull型フレーム補間は、モーション優先を用いることを含んでもよい。一例において、モーションフィールドは、マルコフ確率フィールド（Markov Random Fields）でもよい。モーション優先は、２つの要因からなり、pd(・)は、推定されたモーションフィールドの空間的な平滑性を実現し、pg(・)は、グローバルなモーションの予め計算された推定からモーションフィールドの大きい偏差にペナルティを課してもよい。補間されたモーションフィールドの空間的な平滑性は、以下のように表される通常のギブズエネルギー優先（Gibbs energy prior）を使用して実施してもよい。
反対方向のモーションを同様に表してもよい。数１０において、Λdは平滑性の強度を制御してもよい。例えば、Λd = 2.0も用いてもよい。排他的な用語（clique terms）のそれぞれからの負担部分は、ｘからの距離に反比例してλkで重みづけしてもよい。例えば、λk = 1/|vk|を用いてもよい。いくつかの実施態様において、現在のピクセルに近接する８つのピクセルがvkによってインデックスを付けられるように、Ｋは８としてもよい。
The motion in the opposite direction may be represented similarly. In
図９に示すように、オフセットされたベクトルは、水平および垂直方向に単位価値を有してもよい。いくつかの実施態様において、f(・)は、以下のように表される堅牢な機能でもよい。
いくつかの実施態様において、ｄｇは、補間フレームのグローバルな（またはカメラ）モーションの予め計算された推定でもよく、f(・)は、数１１で表される機能のような堅牢な機能でもよく、pg(・)は、以下のように表してもよい。
いくつかの実施態様において、現在のフレームのモーションは、知覚可能時に、カメラのグローバルなモーションに「パチッとはまる（snap）」ように促進されてもよい。いくつかの実施態様において、強度の制約小さいもの、例えばΛd = 1.0を用いてもよい。いくつかの実施態様において、制約は、堅牢性のためにオフ（例えばΛd = 0）にされてもよい。 In some implementations, the motion of the current frame may be facilitated to “snap” to the global motion of the camera when perceptible. In some embodiments, a small strength constraint may be used, for example, Λ d = 1.0. In some implementations, the constraint may be turned off (eg, Λ d = 0) for robustness.
いくつかの実施態様において、Pull型フレーム補間は、遮蔽優先を用いることを含んでもよい。推定された状態において、遮蔽のための優先p(s(・))が空間的な平滑性を促進し、以下のように表してもよい。
数１３で、h(s1; s2)は、以下のように表されるステートペア(s1; s2)に従ってエネルギーを割り当てるエネルギー関数でもよい。
数１３で表されるエネルギー関数は、遮蔽状態０１および１０が境界線を共有するのを防止し、その状態を近接の場合と同じようにすることを促進してもよい。エネルギー関数は、例えば、近接するピクセルのグループ内において、空間的な平滑性の遮蔽状態を促進してもよい。現在のピクセルに近接する８つのピクセルの状態は、０１であり、エネルギー関数は、現在のサイトのその状態を０１にすることを促進してもよい。他の例において、現在のサイトの周囲の５つのサイトの状態は００でもよく、エネルギー関数は、現在のサイトを、局所的なエリアで最も円滑性を有する構成において生成可能な００にすることを促進してもよい。
The energy function represented by Equation 13 may prevent the shielding states 01 and 10 from sharing a boundary, and may facilitate making the state the same as in the case of proximity. The energy function may facilitate spatial smoothness shielding, for example, within a group of adjacent pixels. The state of the eight pixels adjacent to the current pixel is 01, and the energy function may facilitate bringing that state at the current site to 01. In another example, the state of the five sites around the current site may be 00 and the energy function will make the
エネルギー関数も、０１および１０を、８つの最も近接ピクセルが一緒になることを防止するのにも役立つ。 The energy function also helps to prevent 01 and 10 from bringing the eight closest pixels together.
数１３で表されるエネルギー関数は、未知のモーションdt+Δを特定するために用いてもよい。例えば、グラフカット（Graph Cuts）、信頼性伝搬（Belief Propagation）、または他のいかなる局所的な更新方式を用いて、数２を最適化することを含んでもよい。
The energy function expressed by Equation 13 may be used to specify an unknown motion d t + Δ . For example, it may include optimizing
いくつかの実施態様において、Pull型フレーム補間は最適化を含んでもよい。Pull型フレーム補間の計算負荷は、時間的モーション予測技術を用いて、補間されたモーションの局所的な候補を提案し、各サイトで最適化された候補を選択するために数１３で表されるエネルギー関数を使用することによって減らしてもよい。各々を順に推定するよりも、モーションおよび遮蔽を共同で推定してもよい。最適化のプロセスは、結論がでるまで繰り返してもよい。いくつかの実施態様において、最適化は、局所的な重要性サンプリングと結合される反復条件付モード（Iterated Conditional Modes）（ＩＣＭ）最適化を含んでもよい。いくつかの実施態様において、候補の生成を容易にするために、最適化は、
モーション評価、時間的なヒットリスト（hit list）生成、最初の推定の生成、またはいかなるそれらの組み合わせを含んでもよい。Pull型フレーム補間モーション推定の要素として、本明細書に記載されているが、時間的なヒットリスト生成および最初の推定の生成は、Pull型フレーム補間の前に独立して実行してもよい。
In some implementations, pull-type frame interpolation may include optimization. The computational load of Pull-type frame interpolation is expressed by Equation 13 in order to propose local candidates for interpolated motion using temporal motion prediction technology and to select candidates optimized at each site. It may be reduced by using an energy function. Rather than estimating each in turn, motion and occlusion may be estimated jointly. The optimization process may be repeated until a conclusion is reached. In some implementations, the optimization may include Iterated Conditional Modes (ICM) optimization combined with local importance sampling. In some embodiments, to facilitate the generation of candidates, the optimization is
It may include motion evaluation, temporal hit list generation, initial estimation generation, or any combination thereof. Although described herein as an element of Pull type frame interpolation motion estimation, temporal hit list generation and initial estimation generation may be performed independently prior to Pull type frame interpolation.
図１０は、本開示の実施態様に係る候補補間モーションベクトル（ヒットリスト）のリストを生成する一例を示す。ヒットリストを生成することは、補間フレーム内の各サイトで時間的および空間的候補を特定することを含んでもよい。計算上の負荷は、Pull型フレーム補間の前に時間的または空間的モーション候補のリストを生成することによって減らしてもよい。いくつかの実施態様において、例えば、時間的モーション予測において、ｔ＋Δでピクセル位置に、モーション方向に沿って既存のフレームの間のモーションを複製することにより、補間された位置でモーションを予測することに基づいて、候補は推定されてもよい。フレームｔとｔ＋1との間の各モーションベクトルを用いて、補間されたフィールドdt+Δ, t+1のための候補ベクトルを予測してもよい。同様に、dt+1, tを用いて、dt+1, t+Δのための可能なベクトルを予測してもよい。 FIG. 10 shows an example of generating a list of candidate interpolation motion vectors (hit list) according to an embodiment of the present disclosure. Generating the hit list may include identifying temporal and spatial candidates at each site in the interpolated frame. The computational burden may be reduced by generating a list of temporal or spatial motion candidates prior to pull type frame interpolation. In some embodiments, for example, in temporal motion prediction, predicting motion at interpolated positions by duplicating motion between existing frames along the motion direction at the pixel location at t + Δ. Based on this, the candidates may be estimated. Each motion vector between frames t and t + 1 may be used to predict candidate vectors for interpolated fields d t + Δ, t + 1 . Similarly, d t + 1, t may be used to predict possible vectors for d t + 1, t + Δ .
図１０に示すように、ｔ＋Δで補間されたフレーム８００のヒットリストは、モーションフィールドdt, dt+1..を使用して生成してもよい。両側１０１０からヒットがあるｔ＋Δのサイトは、黒い正方形として示され、１つのヒット１０２０（ｔ＋１、ｔ方向において）を示しているサイトの一例を、斑点を有する正方形として示す。Ｄ→Ｅからの後方のベクトルは、dt+Δ, tの候補を生成してもよく、Ｆ→Ｇからのベクトルは、反対方向に類似のヒットを生成してもよく、dt+Δ, t+1の候補でもよい。同様のヒットをＪＫおよびＨＩのために特定してもよい。ベクトルＡＢは、逆方向（白い正方形）のヒットを生成してもよく、ヒットを生成するベクトルＢＡは存在しなくてもよい。Ｂで始まっているベクトルをＣにマッピングしてもよい。これは、遮蔽領域の指標であってもよい。
As shown in FIG. 10, the hit list of the
いくつかの実施態様において、ヒットリストを生成することは、ｔでフレーム８１０のすべてのｘに、全てのベクトルdt,t+1(x)を走査することで前方のヒットを特定することと、ｔ＋Δでフレーム８００の各サイトx+Δdt,t+1(x)で、そのサイトでヒットを示すレコード等のdt,t+1(x)の指標を格納することとを含んでもよい。
In some embodiments, generating the hit list includes identifying forward hits by scanning all vectors d t, t + 1 (x) at every x in
いくつかの実施態様において、ヒットリストを生成することは、ｔ＋1でフレーム８２０のｘごとに全てのベクトルdt+1,t(x)を走査することによって後方のヒットを特定することと、ｔ＋Δでフレーム８００の各サイトx+(1-Δ)dt+1,t(x)で、そのサイトでヒットを示すレコード等のdt+1,t(x)の指標を格納することとを含んでもよい。
In some embodiments, generating the hit list includes identifying a backward hit by scanning all vectors d t + 1, t (x) every x of
前方のヒットおよび後方のヒットは、ｔ＋Δで補間されたフレームの全てのサイトのための候補補間モーションベクトル（前方および後方の時間的な方向を指し示す）の、２つの同じ位置に配置されたリストCb/T’Cf/T’でもよい。いくつかの実施態様において、モーションフィールドは、不正確であってもよいので、遮蔽の処理は困難かもしれないが、ヒットリスト生成は、各リストにおいて１つ以上のヒットがあるサイトまたは全くヒットのないサイトを含んでもよい。 Forward hits and backward hits are two co-located lists Cb of candidate interpolated motion vectors (pointing forward and backward temporal directions) for all sites in the frame interpolated at t + Δ It may be / T'Cf / T '. In some implementations, the motion field may be inaccurate, so occlusion processing may be difficult, but hit list generation may be performed for sites that have one or more hits in each list or for all hits. May contain no sites.
図１１は、本開示の実施態様に係るPull型フレーム補間の一例の図を示す。いくつかの実施態様において、Pull型フレーム補間は、１１００で入力フレームを特定し、１１１０で入力モーションベクトルを生成し、１１２０でモーション平滑性を決定し、１１３０で候補補間モーションベクトルを生成し、１１４０で出力情報を初期化し、１１５０でローカルサイトの更新を行い、１１６０で補間されたフレームを構築するべきかを決定し、１１７０で補間されたフレームを構築し、１１８０で後処理を行い、１１９０で補間されたフレームを出力する、または、いかなるそれらの組み合わせを含んでもよい。いくつかの実施態様において、１１１０でモーションを推定し、１１２０で平滑性を測定し、１１３０でヒットリストを生成する、または、いかなるそれらの組み合わせも、前処理と考えてもよく、補間とは独立して実行してもよい。 FIG. 11 shows an example of a pull-type frame interpolation according to an embodiment of the present disclosure. In some embodiments, pull-type frame interpolation identifies an input frame at 1100, generates an input motion vector at 1110, determines motion smoothness at 1120, generates a candidate interpolated motion vector at 1130, 1140 The output information is initialized at 1150, the local site is updated at 1150, it is determined whether to construct an interpolated frame at 1160, an interpolated frame is constructed at 1170, post-processing is performed at 1180, and 1190 is performed. The interpolated frame may be output or any combination thereof may be included. In some embodiments, estimating motion at 1110, measuring smoothness at 1120, generating a hit list at 1130, or any combination thereof may be considered pre-processing and independent of interpolation. May be executed.
いくつかの実施態様において、図８に示される入力フレーム８１０／８１２／８１４／８１６等の入力フレームを１１００で特定してもよい。フレームのシーケンスは、位置ｔ−１のフレーム、位置ｔのフレーム、位置ｔ＋１のフレーム、および位置ｔ＋２のフレームを含んでもよい。いくつかの実施態様において、入力フレームを特定することは、補間されたフレームΔの位置がｔおよびｔ＋ｌの間にあるように、補間されたフレームごとに時間的または空間的な位置Δを特定することを含んでもよい。
In some implementations, an input frame such as
いくつかの実施態様において、モーションは、１１１０で入力フレームのために生成してもよい。例えば、モーションフィールドは、フレームペアｔ,t-1；ｔ,ｔ＋1；ｔ＋1, ｔ；ｔ＋1,ｔ＋２の間で計算されてもよい。ブロックマッチング（block matching）または光フローモーション推定（optic flow motion estimation）など、いかなるモーション推定（予測）プロセスを用いてもよい。モーションフィールドは、dt;t-1; dt;t+1; dt+1;t-1; dt+1;t+2をそれぞれ初期化するために用いてもよい。モーションフィールドは、補間の間、不変であってもよい。
In some implementations, motion may be generated for an input frame at 1110. For example, the motion field may be calculated between the frame pairs t, t−1; t, t + 1; t + 1, t; t + 1,
いくつかの実施態様において、モーション平滑は１１２０で決定されてもよい。モーション平滑を決定することは、１１２２でモーション平滑が低いかどうか決定すること、１１２４で補間されたフレームとして入力フレームを繰り返すこと、または両方を含んでもよい。照明が良くないシーン、あるいは高いモーションコンテントを含む低いオリジナルのフレームレートで撮影されたシーン等、いくらかのシーンで、既存のフレーム間のモーションフィールドが時間的または空間的に整合していないことがあるので（低いモーション平滑）高品質に補間されたフレームが生成される可能性は低いかもしれない。ｔのフレームまたはｔ＋１のフレーム等の入力フレームに対して、低いモーション平滑となることが、補間されたフレームとして繰り返されてもよい。 In some implementations, motion smoothing may be determined at 1120. Determining motion smoothing may include determining if motion smoothness is low at 1122, repeating the input frame as an interpolated frame at 1124, or both. In some scenes, such as poorly lit scenes or scenes shot at low original frame rates with high motion content, the motion fields between existing frames may not be temporally or spatially aligned. So (low motion smoothing) it is unlikely that high quality interpolated frames will be generated. For input frames such as t frames or t + 1 frames, low motion smoothing may be repeated as interpolated frames.
１１２２で時間的または空間的な不一致（低いモーション平滑）を特定することは、フレームを均一にはめ込む（tile）ブロック内のフレームｔおよびｔ＋１の間のモーションが補償されたモーション相違を判定することを含んでもよい。３つの水平ブロックと２つの垂直ブロックのグリッドを、画像フレームをはめ込むために測定されるブロックサイズとともに用いてもよい。各ブロックは、B1×B2サイトを含んでもよく、Ｂは、ブロックbにおいて、サイトｘを含んでもよく、ブロックｂ内のモーションを補償したモーション相違を算出することを含んでもよく、eb mは、以下のように表してもよい。
数１５において、dt,t+1(x)のxは、説明を簡単にするため省略する。モーションを補償したモーション相違は、平滑性の制限または閾値を越えてもよく、ｔのフレームまたはｔ＋１のフレーム等の入力フレームは、１１２４で補間されたフレームとして繰り返されてもよい。 In Equation 15, x in d t, t + 1 (x) is omitted for simplicity of explanation. Motion-compensated motion differences may exceed smoothness limits or thresholds, and input frames such as t frames or t + 1 frames may be repeated as frames interpolated at 1124.
閾値（制限）δbを超えたモーションを補償したモーション相違eb mは、モーション情報が信頼できないことを示しても良いし、ｔのフレームまたはｔ＋１のフレーム等の入力フレームが、１１２４で補間されたフレームとして繰り返されてもよい。いくつかの実施態様において、繰り返された入力フレームを、補間されたフレームΔへの近接に基づいて特定してもよい。例えば、Δは０．５以下でもよく、ｔのフレームは繰り返されてもよい。他の例において、Δは、０．５より大きくてもよく、ｔ＋１のフレームは繰り返されてもよい。いくつかの実施態様において、Δは０．５以上でもよく、ｔ＋１のフレームは繰り返されてもよい。 The motion difference e b m compensated for motion exceeding the threshold (limit) δ b may indicate that the motion information is not reliable, and an input frame such as a frame of t or a frame of t + 1 is interpolated in 1124. It may be repeated as another frame. In some implementations, a repeated input frame may be identified based on proximity to the interpolated frame Δ. For example, Δ may be 0.5 or less, and the frame of t may be repeated. In other examples, Δ may be greater than 0.5, and the t + 1 frame may be repeated. In some implementations, Δ may be greater than or equal to 0.5 and t + 1 frames may be repeated.
いくつかの実施態様において、一貫したモーションとしてモーションを特定することは、ビデオシーケンスのフレームのサイズによって、滑らかに変化してもよい。例えば、高解像度な画像には、高い閾値を用いても良いし、低解像度な画像には、低い閾値を用いてもよい。いくつかの実施態様において、閾値δbは、ピクセルNhの画像の横サイズに比例してもよい。例えば、閾値δbは、50×Nh/1920でもよい。 In some implementations, identifying the motion as a consistent motion may vary smoothly depending on the size of the frame of the video sequence. For example, a high threshold may be used for a high resolution image, and a low threshold may be used for a low resolution image. In some implementations, the threshold δ b may be proportional to the horizontal size of the image of pixel N h . For example, the threshold δ b may be 50 × N h / 1920.
いくつかの実施態様において、候補補間モーションベクトル（ヒット）は、１１３０で生成されてもよい。補間フレーム用の候補補間モーションベクトルのリスト（ヒットリスト）は、入力フレームのために特定されるモーションに基づいていてもよい。例えば、ヒットリストは、図１０に示すように生成されてもよい。 In some implementations, candidate interpolated motion vectors (hits) may be generated at 1130. The list of candidate interpolated motion vectors for the interpolated frame (hit list) may be based on the motion identified for the input frame. For example, the hit list may be generated as shown in FIG.
いくつかの実施態様において、出力情報は１１４０で初期化されてもよい。例えば、出力情報を初期化することは、無作為な割り当て、ヒットリストに基づく割り当て、または、それらの組み合わせを用いることを含んでも良い。いくつかの実施態様において、補間されたモーションフィールドの最初の推定を迅速に行うことは、ヒットリストを使用して行ってもよい。いくつかの実施態様において、Nb T(x)は、後向きの時間的候補（ヒット）の数を示してもよく、Nf T(x)は、前向きの時間的候補（ヒット）の数を示してもよい。最初の推定は、ｔ＋Δのサイトを探査することを含んでもよい。ヒット数は、Nb T(x)==1)&&(Nf T(x)==1)としてもよく、リスト内のモーションは、補間されたモーションに割り当てられてもよく、ｓは００にセットされてもよい。ヒット数は、Nb T(x)≧1)&&(Nf T(x)==0)でもよく、後向きの最初のモーションヒットは、補間されたモーションの両方の方向に割り当てられもよく、ｓは１０にセットされてもよい。ヒット数は、Nb T(x)==0))&&(Nf T(x)≧1)でもよく、前向きの最初のモーションヒットは、補間されたモーションの両方の方向に割り当てられもよく、ｓは０１にセットされてもよい。そうでなければ、補間されたモーションは０にセットされ、ｓは００にセットされてもよい。出力情報を初期化することは、反復カウンタをゼロに設定することを含んでもよい。 In some implementations, the output information may be initialized at 1140. For example, initializing output information may include using random assignments, hit list-based assignments, or combinations thereof. In some implementations, a quick initial estimation of the interpolated motion field may be performed using a hit list. In some embodiments, N b T (x) may indicate the number of backward temporal candidates (hits), and N f T (x) indicates the number of forward temporal candidates (hits). May be shown. The initial estimate may include exploring the t + Δ site. The number of hits may be N b T (x) == 1) && (N f T (x) == 1), the motion in the list may be assigned to the interpolated motion, and s is 00 May be set. The number of hits may be N b T (x) ≧ 1) && (N f T (x) == 0), and the first backward motion hit may be assigned in both directions of the interpolated motion, s may be set to 10. The number of hits may be N b T (x) == 0)) && (N f T (x) ≧ 1), and the forward first motion hit may be assigned in both directions of the interpolated motion , S may be set to 01. Otherwise, the interpolated motion may be set to 0 and s may be set to 00. Initializing the output information may include setting the iteration counter to zero.
いくつかの実施態様において、ローカルサイトの更新は１１５０で実行されてもよい。ローカルサイトの更新を行うことは、補間フレームの各サイトの補間モーションベクトルを選択し、更新することを含んでもよい。ローカルサイトの更新は、補間されたフレームのサイトごとに、反復して実行されてもよい。 In some implementations, a local site update may be performed at 1150. Updating the local site may include selecting and updating the interpolated motion vector for each site in the interpolated frame. The local site update may be performed iteratively for each site of the interpolated frame.
いくつかの実施態様において、ローカルサイトの更新を実行することは、１１３０で生成されるヒットリストを使用して、前向きおよび後向きの候補補間モーションベクトルを特定することを含んでもよい。ヒットリストは空のものでもよく、前向きおよび後向きのヒットが特定されないことがある。図９に示すように、現在のサイトの８つの近接するモーションを、前向きおよび後向きのモーション候補として特定してもよい。現在のサイトの現在のモーション情報が、候補として含まれてもよい。前向きおよび後向きのモーション候補リストの長さを、同じまたは類似のベクトルを削除することによって小さくしてもよい。例えば、閾値未満、例えば０．２５ピクセルの相違があるモーションベクトルを取り除いてもよい。ベクトル長の減少候補リストを、前方および後方の候補のdf k’db kと呼ぶことがある。例えば、ベクトル長の減少候補リストは、Ｋの候補ペアを含んでもよい。モーション候補の各ペアに対して、ｓ＝００、０１または１０という３つの考えられる状態で、各ペアを増加することにより、３つのモーション／遮蔽候補を生成してもよい。増加候補セットを、m1 K = [df k’db k’s = 00], m2 K = [df k’db k’s = 01], m3 K = [df k’db k’s = 10]と呼ぶこともある。例えば、増加候補セットは、３×Ｋモーション候補を含んでもよい。３Ｋのモーション候補の各々のために、数４〜８に、Λo=10.0, Λd=2.0, λk=1/|vk|で示されるef, eb, efb, ebf, edを使用することで、エネルギーを算出してもよく、以下のように表してもよい。
いくつかの実施態様において、ローカルサイトの更新を実行することは、補間されたモーションフィールドに最も低いエネルギーを有するモーション候補ペアを割り当てることを含んでもよく、そのフィールドにおける現在の値を置き換えることを含んでもよい。その候補に対して、状態値ｓを最小のエネルギーで示してもよい。例えば、Ｅｏｏが最小のエネルギーを有する場合、ｓ＝００である。 In some implementations, performing the local site update may include assigning a motion candidate pair having the lowest energy to the interpolated motion field, and replacing a current value in that field. But you can. For the candidate, the state value s may be indicated with the minimum energy. For example, if Eoo has the lowest energy, s = 00.
いくつかの実施態様において、ローカルサイトの更新を実行することは、１１５２で分離した遮蔽状態を取り除くこと、グローバルなモーションを１１５４で推定すること、あるいは両方を含んでもよい。 In some implementations, performing the local site update may include removing the isolated occlusion at 1152, estimating global motion at 1154, or both.
１１５２で分離した遮蔽状態を取り除くことは、s(x)がs(vk+x)に等しくなく、s(vk+x)がすべて同じとなるサイトの発生を検出し、s(・)を隣接の値と置き換えることを含んでもよい。サイトのモーションを、隣接する平均的なモーションと置き換えてもよい。分離した遮蔽状態を取り除くことにより、単一のサイトを一時的に加工する機会を減らしてもよい。
Removing the separated shielded
すべてのサイトが訪問された場合、１１５４でグローバルなモーションを推定してもよい。補間されたモーションフィールドに対してグローバルなモーションを新たに推定することは、密度の高いモーションフローの使用に基づいてグローバルなモーション推定法を用いることを含んでもよい。例えば、最も頻繁に発生しているモーションベクトル、すべてのベクトルの平均、またはベクトルフィールドに適合する多項式は、シーンのグローバルなモーションとして使われてもよい。 If all sites are visited, global motion may be estimated at 1154. New estimation of global motion for the interpolated motion field may include using a global motion estimation method based on the use of dense motion flow. For example, the most frequently occurring motion vector, the average of all vectors, or a polynomial that fits the vector field may be used as the global motion of the scene.
いくつかの実施態様において、補間されたフレームを構築するべきかどうかは、１１６０で決定されてもよい。１１５０でローカルサイトの更新を実行することは、反復カウンタを繰り返し適用することを含んでもよい。反復カウンタが閾値、例えば５を超えると、補間されたフレームを１１７０で構築してもよい。いくつかの実施態様において、いずれの推定されたモーションにも変化がなかった場合は、補間されたフレームを１１７０で構築してもよい。反復カウンタが閾値内にある場合、推定されたモーションまたは両方において変化があったので、１１４０で出力情報を初期化し、１１５０でローカルサイトの更新を実行し、１１６０で補間されたフレームを構築するべきかどうかを判定することを、反復して実行してもよい。 In some implementations, whether to construct an interpolated frame may be determined at 1160. Performing a local site update at 1150 may include repeatedly applying an iteration counter. When the iteration counter exceeds a threshold, eg, 5, an interpolated frame may be constructed at 1170. In some implementations, an interpolated frame may be constructed at 1170 if there is no change in any estimated motion. If the iteration counter is within the threshold, there was a change in the estimated motion or both, so the output information should be initialized at 1140, the local site update performed at 1150, and the interpolated frame constructed at 1160 The determination of whether or not may be performed iteratively.
いくつかの実施態様において、補間されたフレームを１１７０で構築してもよい。補間されたフレームは、推定されたモーションを使用することを含み、数２に基づいていてもよい。
In some implementations, an interpolated frame may be constructed at 1170. Interpolated frames include using estimated motion and may be based on
いくつかの実施態様において、後処理を１１８０で実行してもよい。そのモーションが急速な場合、または弱い光のもとで記録された場合、モーションを推定することがむずかしいので、後処理を行って、画像加工の外観を小さくしたり修正したりしてもよい。このような加工は、画像It+Δ内に穴として、または、大きく遮蔽されているかまたは覆われていない領域の近くの画像の奇妙なゆがみとして現れることがある。信頼性の低い画像であると推定し、将来および過去のフレームの平均的なものと継ぎ目がないように融合させてもよい。モーションフィールドの勾配を用いてもよく、補間における信頼の尺度として、いずれがより大きいかによって、前方または後方への選択をすることを含んでもよい。 In some implementations, post-processing may be performed at 1180. If the motion is rapid or recorded under weak light, it is difficult to estimate the motion, so post-processing may be performed to reduce or correct the appearance of image processing. Such processing may appear as a hole in the image It + Δ , or as a strange distortion of the image near a heavily shielded or uncovered area. The image may be estimated to be an unreliable image and may be merged so that it is seamless with the average of future and past frames. The gradient of the motion field may be used and may include making a forward or backward selection depending on which is greater as a measure of confidence in the interpolation.
後処理は、I*(x) = (1-Δ)It(x) + ΔIt+1(x)の平均化を用いて、補間されたフレームに対して伝統的な推定行うことを含んでもよい。簡潔にするために、後方の補間されたモーションdt+Δ,t-1(x)は、[d^ 1 b(h,k), d^ 2 b(h,k)]と表してもよく、前方の補間されたモーションdt+Δ,t(x)は、[d^ 1 f(h,k), d^ 2 f(h,k)]と表してもよく、x = [h,k]である。各サイトｘでモーション勾配gm(x)を測定し、重量w(x)を融合することを以下のように表してもよい。
例えば、δt = ４を用いてもよい。 For example, δ t = 4 may be used.
最終の出力画像を、I^(x) = w(x)I*(x)+(1-w(x))It+Δ(x)を用いて算出してもよく、非動作補償平均画像I*と前段It+Δからの出力画像の間で重みづけをして融合してもよい。いくつかの実施態様において、補間されたフレームを１１９０で出力してもよい。 The final output image may be calculated using I ^ (x) = w (x) I * (x) + (1-w (x)) I t + Δ (x) Weighting may be performed between the image I * and the output image from the previous stage It + Δ . In some implementations, the interpolated frame may be output at 1190.
数１を例として示し、以前のフレームおよび次のフレームで、モーション補償サイトから抽出された大量のピクセルに対して行われるメディアン演算（median operation）（または他の順序統計）等の他の再構成法を用いてもよい。 Other reconstructions such as the median operation (or other order statistics) performed on the large number of pixels extracted from the motion compensation site in the previous and next frames, as shown as an example The method may be used.
図１１に図示していないが、いくつかの実施態様において、Pull型フレーム補間は、マルチ解像度方式を使用することを含んでもよい。例えば、あるシーンは大きいモーションを含んでもよく、マルチ解像度方式を使用してもよい。マルチ解像度方式を使用することは、図１１に示すように、きめの粗いブロックに基づくモーションフィールドに、Pull型フレーム補間を実行することを含んでもよい。各サイトは、Ｂ×Ｂピクセルの１ブロックとして処理されてもよい（画像のサイズにより、Ｂ＝３，４，８，１６，４）。サイト画像の相違は、平均ピクセル輝度の差としてもよい。粗いレベルに補間されたブロックモーションフィールドを、次のより微細なレベルでの反復を初期化するために用いてもよい。例えば、高解像度ビデオシーケンス（１９２０×１０８０）を処理することは、サイズ９６０×５４０、４８０×２７０、２４０×１３５、１２０×７２、６０×３６、あるいはそれらのいかなる組合せの画像を生成すること、および画像ピラミッド(image pyramid）のすべてのレベルでブロック長Ｂ＝４を使用することを含んでもよい。最高のレベルで、反復が完了した場合に、ブロックに基づくベクトルフィールドを、最終的に補間されたモーションフィールドとして用いてもよい。 Although not illustrated in FIG. 11, in some implementations, pull-type frame interpolation may include using a multi-resolution scheme. For example, some scenes may contain large motion and may use a multi-resolution scheme. Using the multi-resolution scheme may include performing pull-type frame interpolation on a motion field based on a coarse block, as shown in FIG. Each site may be processed as one block of B × B pixels (B = 3, 4, 8, 16, 4 depending on the size of the image). The difference in the site image may be a difference in average pixel luminance. A block motion field interpolated to a coarse level may be used to initialize the next finer level iteration. For example, processing a high-resolution video sequence (1920 × 1080) generates an image of size 960 × 540, 480 × 270, 240 × 135, 120 × 72, 60 × 36, or any combination thereof, And using block length B = 4 at all levels of the image pyramid. At the highest level, if the iteration is complete, the block-based vector field may be used as the final interpolated motion field.
図１２は、本開示の実施態様に係るPull型フレーム補間の一例の簡略図を示す。いくつかの実施態様において、Pull型フレーム補間は、１２１０で入力フレームを特定すること、１２２０でモーションベクトルを生成すること、１２３０で補間位置を確認すること、１２４０で候補補間モーションベクトルを生成すること、１２５０で補間モーションベクトルを選択すること、１２６０で補間されたフレームを生成すること、または、いかなるそれらの組み合わせを含んでもよい。 FIG. 12 shows a simplified diagram of an example of pull-type frame interpolation according to an embodiment of the present disclosure. In some embodiments, pull-type frame interpolation may identify an input frame at 1210, generate a motion vector at 1220, verify an interpolation position at 1230, and generate a candidate interpolation motion vector at 1240. Selecting an interpolated motion vector at 1250, generating an interpolated frame at 1260, or any combination thereof may be included.
いくつかの実施態様において、入力フレームは１２１０で特定されてもよい。入力フレームを特定することは、特定されたフレームのシーケンスが位置ｔ−１のフレーム、位置ｔのフレーム、位置ｔ＋１のフレーム、および位置ｔ＋２のフレームを含むように、図８に示される入力フレーム８１０／８１２／８１４／８１６等のように、フレームを特定することを含んでもよい。
In some implementations, the input frame may be identified at 1210. Identifying the input frame is the
いくつかの実施態様において、図１１の１１１０でモーションを推定することと同じ様に、モーションベクトルは１２２０で生成してもよい。モーションベクトルを生成することは、図１１の１１２０で示すように、モーションの平滑性を計測すること、図１１の１１２２で示すように、モーションの平滑性が低いかどうか決定すること、図１１の１１２４で示すように、入力フレームを補間されたフレームとして繰り返すこと、または、いかなるそれらの組み合わせなど、追加的な処理を含んでもよい。 In some implementations, motion vectors may be generated at 1220, similar to estimating motion at 1110 of FIG. The generation of the motion vector includes measuring the smoothness of the motion as indicated by 1120 in FIG. 11, determining whether the smoothness of the motion is low as indicated by 1122 in FIG. As indicated at 1124, additional processing may be included, such as repeating the input frame as an interpolated frame, or any combination thereof.
いくつかの実施態様において、補間ポイントを１２３０で特定してもよい。補間ポイントの特定には、補間されたフレームΔの空間的位置がｔとｔ＋１の間にあるように、補間されたフレームごとに時間的または空間的位置Δを特定することを含んでもよい。 In some implementations, the interpolation point may be identified at 1230. Identifying the interpolation point may include identifying a temporal or spatial position Δ for each interpolated frame such that the spatial position of the interpolated frame Δ is between t and t + 1.
いくつかの実施態様において、候補補間モーションベクトルを１２４０で生成してもよい。候補補間モーションベクトルを生成することは、図１１の１１３０に示すようにヒットリストを生成することを含んでもよい。 In some implementations, candidate interpolated motion vectors may be generated at 1240. Generating candidate interpolated motion vectors may include generating a hit list as shown at 1130 in FIG.
いくつかの実施態様において、補間モーションベクトルを１２５０で選択してもよい。補間モーションベクトルを選択することは、図１１の１１４０に示すように出力情報を初期化すること、図１１の１１５０に示すようにローカルサイト更新を実行すること、図１１の１１６０に示すように補間されたフレームを構築するべきかどうか決定すること、または、いかなるそれらの組み合わせを含んでもよい。 In some implementations, an interpolated motion vector may be selected at 1250. Selecting an interpolation motion vector includes initializing output information as indicated by 1140 in FIG. 11, performing local site update as indicated by 1150 in FIG. 11, and interpolation as indicated by 1160 in FIG. Deciding whether to build a structured frame, or any combination thereof may be included.
いくつかの実施態様において、補間されたフレームを１２６０で生成してもよい。補間されたフレームを生成することは、図１１の１１７０に示すように補間されたフレームを組み込むこと、図１１の１１８０に示すような後処理、図１１の１１９０に示すように補間されたフレームを出力すること、または、いかなるそれらの組み合わせを含んでもよい。 In some implementations, an interpolated frame may be generated at 1260. Generating an interpolated frame includes incorporating the interpolated frame as shown at 1170 in FIG. 11, post-processing as shown at 1180 in FIG. 11, and interpolated frame as shown at 1190 in FIG. Output, or any combination thereof.
図１２に示すPull型フレーム補間の図の他の実施態様が利用可能である。実施態様において、Pull型フレーム補間の追加的な要素を加えてもよく、特定の要素は結合されてもよい、および／または特定の要素は取り外されてもよい。例えば、一実施態様において、最初のパス（first pass）のPull型フレーム補間は、きめの粗いブロックに基づくモーションフィールドに対して実行されてもよく、２番目のパス（second pass）のPull型フレーム補間は、補間されたフレームを生成するために最初のバスの出力を用いて実行されてもよい。 Other embodiments of the pull-type frame interpolation diagram shown in FIG. 12 can be used. In embodiments, additional elements of Pull type frame interpolation may be added, certain elements may be combined, and / or certain elements may be removed. For example, in one embodiment, a first pass pull frame interpolation may be performed on a motion field based on a coarse block, and a second pass pull frame. Interpolation may be performed using the output of the first bus to generate an interpolated frame.
Pull型フレーム補間またはそのいかなる部分も、図１に示されるコンピューティング装置１００等の装置において実現することができる。例えば、図４に示されるエンコーダ４００等のエンコーダにより、Pull型フレーム補間またはそのいかなる部分も、図１に示されるメモリ１５０等の、有形かつ非一時的なコンピュータ読み取り可能な媒体に保存される命令を用いて実現することができる。
Pull-type frame interpolation or any portion thereof may be implemented in a device such as
用語「例（example）」または「例示的な（exemplary）」は、本明細書において使用されるが、一例、一事例、実例として機能することを意味する。「例」または「例示的な」として本明細書において記載されているいかなる態様または設計も、他の態様または設計に対して、必ずしも好適または有利なものとして解釈されるわけではない。むしろ、用語「例」または「例示的な」は、具体的な形で概念を提示することを意図している。本出願において用いられている用語「または（or）」は、排他的な「または」よりも内包的な「または」意味することを意図している。すなわち、特に明記しない限り、または前後関係から明白な限り、「Ｘは、ＡまたはＢを含む」ことは、自然に内包的な置換のいずれかを意味することを意図している。すなわち、ＸがＡを含む、ＸはＢを含む、または、ＸはＡおよびＢの両方を含む場合、これらのいずれかの例において、「ＸはＡまたはＢを含む」が成り立つ。また、特に明記しない限り、または、単数形を意図する前後関係から明らかでない限り、本願発明および添付の請求の範囲において用いられる項目「ａ」および「ａn」は、通常、「１つ以上」を意味するために解釈されるべきである。さらに、全体で使用される用語「実施形態」または「一実施形態」または「実施態様」または「一実施態様」は、特段の記載がない限り、同じ実施形態または実施態様を意味することを意図していない。本明細書で使用するように、用語「判定する（determine）」、「特定する（identify）」、またはいかなるそれらの変形例も、選択する、確認する、計算する、検索する、受信する、決定する、確立する、取得する、あるいは、図１に示される装置の１つ以上を使用して、いかなる方法でも特定または判定することを含む。 The terms “example” or “exemplary” are used herein, but are meant to serve as an example, an example, or an illustration. Any aspect or design described herein as "example" or "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, the terms “example” or “exemplary” are intended to present concepts in a concrete fashion. As used in this application, the term “or” is intended to mean an inclusive “or” rather than an exclusive “or”. That is, unless otherwise specified or apparent from the context, “X includes A or B” is intended to mean any naturally inclusive substitution. That is, when X includes A, X includes B, or X includes both A and B, in any of these examples, “X includes A or B” holds. In addition, unless otherwise specified or apparent from the context of the singular, the items “a” and “an” used in the present invention and the appended claims generally refer to “one or more”. Should be construed to mean. Further, the terms "embodiment" or "one embodiment" or "implementation" or "one embodiment" used throughout are intended to mean the same embodiment or embodiment unless otherwise indicated. Not done. As used herein, the terms “determine”, “identify”, or any variation thereof, select, confirm, calculate, search, receive, determine Including, establishing, obtaining, or identifying or determining in any way using one or more of the devices shown in FIG.
さらに、説明を簡単にするために、図および本明細書の説明には、ステップまたはステージの連続を含むが、本明細書に開示される方法の構成要素はさまざまな順序および／または並行して行われることができる。さらに、本明細書において開示される方法の構成要素は、本明細書に明示的に提示または記載されていない他の構成要素とともに実現してもよい。さらにまた、本明細書に記載されている方法のすべての構成要素が、開示された主題に係る一の方法を実施するよう求められているというわけではない。 Further, for ease of explanation, the figures and description herein include a sequence of steps or stages, although the components of the methods disclosed herein may be in various orders and / or in parallel. Can be done. Further, the method components disclosed herein may be implemented in conjunction with other components not explicitly presented or described herein. Furthermore, not all components of the methods described herein are required to perform a method in accordance with the disclosed subject matter.
本明細書に記載されている符号化、復号化、およびフレーム補間の実現により、いくつかの例示的なフレーム補間技術を例示する。しかしながら、符号化および復号化は、それらの用語が本明細書において使われるように、圧縮、解凍、変換または他のいかなる処理もまたはデータの変更を含み、本明細書に用いられるように、フレーム補間およびPull型フレーム補間という用語は、新しいフレームがオリジナルフレームに取り込まれていない時間または空間のコンテンツを描くように、２つのオリジナルフレームの間に１つ以上の新しいフレームを生成することを含んでもよい。 Several exemplary frame interpolation techniques are illustrated by the encoding, decoding, and frame interpolation implementations described herein. However, encoding and decoding includes compression, decompression, conversion, or any other processing or modification of data, as those terms are used herein, and frame as used herein. The terms interpolation and pull-type frame interpolation may include generating one or more new frames between two original frames so as to depict temporal or spatial content where the new frames are not captured in the original frame. Good.
送信装置１００Ａおよび／または受信装置１００Ｂを実現すること（およびアルゴリズム、方法、命令など、その装置に保存および／またはその装置により実行されること）は、ハードウェア、ソフトウェアまたはいかなるそれらの組み合わせによって実現されてもよい。例えば、ハードウェアは、コンピュータ、知的財産（ＩＰ）コア、特定用途向け集積回路（ＡＳＩＣ）、プログラマブルロジックアレイ、光プロセッサ、プログラマブルロジックコントローラ、マイクロコード、マイクロコントローラ、サーバ、マイクロプロセッサ、ディジタル信号処理装置、または他のいかなる適切な回路をも含んでもよい。請求項において、用語「プロセッサ」は、単独で、または、組合せにより、前述のハードウェアのいずれかを含むとして理解されるべきである。用語「信号」及び「データ」は、交換可能に使用される。さらに、送信装置１００Ａおよび受信装置１００Ｂは、必ずしも同じように実行する必要があるというわけではない。
Realization of transmitting
さらに、一実施態様において、例えば、送信装置１００Ａまたは受信装置１００Ｂは、実行時に、本明細書に記載されているそれぞれの方法、アルゴリズムおよび／または命令のいずれかを実施する汎用コンピュータまたは汎用プロセッサを用いて実現することができる。追加して、または、代わりに、例えば、本明細書に記載されている方法、アルゴリズム、または命令のいずれかを実行するために特化されたハードウェアを含むことが可能な専用コンピュータ／プロセッサを利用できる。
Further, in one embodiment, for example, transmitting
送信装置１００Ａおよび受信装置１００Ｂは、例えば、リアルタイムビデオシステムのコンピュータに実現することができる。あるいは、送信装置１００Ａをサーバに実現してもよく、受信装置１００Ｂをサーバとは別の装置、例えば携帯通信装置に実現してもよい。この場合、送信装置１００Ａは、エンコーダ４００を用いてコンテンツを符号化し、符号化されたビデオ信号として、この符号化されたビデオ信号を通信装置に送信する。次に、通信装置は、その後、デコーダ５００を用いて、符号化されたビデオ信号を復号化してもよい。あるいは、通信装置は、通信装置にローカルに格納されたコンテンツ、例えば、送信装置１００Ａによって送信されなかったコンテンツを復号化してもよい。送信装置１００Ａおよび受信装置１００Ｂの、他の適切な実施方式が利用できる。例えば、携帯通信装置および／またはエンコーダ４００を含む装置がデコーダ５００を含んでもよいというよりも、受信装置１００Ｂは、一般に移動しないパーソナルコンピュータとすることができる。
The
更に、全てまたは一部の実施態様は、例えば、有形のコンピュータ使用可能またはコンピュータ読取可能な媒体からアクセス可能なコンピュータプログラムプロダクトの形をとることができる。コンピュータ使用可能またはコンピュータ読取可能な媒体は、例えば、いかなるプロセッサに用いられる、または、関連するプログラムを明らかに含有、格納、伝達または、搬送することが可能ないかなる装置とすることもできる。媒体は、例えば、電子、磁気、光学、電磁気、または半導体装置とすることができる。他の適切な媒体も利用できる。 Further, all or some embodiments may take the form of, for example, a computer program product accessible from a tangible computer usable or computer readable medium. The computer-usable or computer-readable medium can be any device that can, for example, be used by any processor or that can clearly contain, store, transmit, or carry related programs. The medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable media can also be used.
上記の実施態様は、本出願を容易に理解するためのものであり、限定するものではない。これに反して、本出願は、添付の請求の範囲内に含まれるさまざまな変形例および等価な調整についてもその範囲に含める。その範囲が、法律のもとで許可されるすべての変形例および等価の構成を含むように、最も広い解釈がなされるべきである。 The above embodiments are intended to facilitate understanding of the present application and are not intended to be limiting. On the contrary, this application also includes within its scope various modifications and equivalent adjustments that fall within the scope of the appended claims. The broadest interpretation should be made so that the scope includes all variations and equivalent configurations permitted under law.
Claims (11)
前記複数の入力ビデオフレームの第１のフレームから、前記複数の入力ビデオフレームの第２のフレームに、モーションを示す複数のモーションベクトルを生成すること、
前記第１のフレームおよび前記第２のフレームとの間に補間ポイントを特定すること、
前記複数のモーションベクトルに基づいて、前記第１のフレームから前記補間ポイントへのモーションおよび前記第２のフレームから前記補間ポイントまでを示す複数の候補補間モーションベクトルを生成すること、
補間されるフレーム内における平滑性の制約に基づいて、前記複数の候補補間モーションベクトルから、補間モーションベクトルを選択すること、および
選択された前記補間モーションベクトルに基づいて、前記補間ポイントで補間されたフレームを生成することを含み、
前記補間されたフレームを生成することは、複数の補間ピクセルを生成することを含み、
前記補間モーションベクトルを選択することは、
複数の補間モーションベクトルを含む補間モーションフィールドを選択することを含み、
前記複数の補間モーションベクトルの各補間モーションベクトルは、前記複数の補間ピクセルのうち１つの補間ピクセルと関連し、
前記補間されたフレームを生成することは、前記補間モーションフィールドに基づいて前記補間されたフレームの加工を修正することを含み、
前記加工を修正することは、前記補間されたフレームと、前記第１のフレームおよび前記第２のフレームの平均とを融合することを含み、
前記融合の程度は、前記補間モーションフィールドの勾配に基づく方法。 Identifying multiple input video frames,
Generating a plurality of motion vectors indicative of motion from a first frame of the plurality of input video frames to a second frame of the plurality of input video frames;
Identifying an interpolation point between the first frame and the second frame;
Generating a plurality of candidate interpolation motion vectors indicating motion from the first frame to the interpolation point and from the second frame to the interpolation point based on the plurality of motion vectors;
Selecting an interpolated motion vector from the plurality of candidate interpolated motion vectors based on smoothness constraints within the interpolated frame ; and interpolating at the interpolation point based on the selected interpolated motion vector look including generating a frame,
Generating the interpolated frame includes generating a plurality of interpolated pixels;
Selecting the interpolated motion vector includes
Selecting an interpolated motion field that includes a plurality of interpolated motion vectors;
Each interpolation motion vector of the plurality of interpolation motion vectors is associated with one of the plurality of interpolation pixels;
Generating the interpolated frame includes modifying the processing of the interpolated frame based on the interpolated motion field;
Modifying the processing includes fusing the interpolated frame with the average of the first frame and the second frame;
The degree of fusion is based on the gradient of the interpolated motion field .
前記複数の入力ビデオフレームの第１のフレームから、前記複数の入力ビデオフレームの第２のフレームに、モーションを示す複数のモーションベクトルを生成すること、
前記第１のフレームおよび前記第２のフレームとの間に補間ポイントを特定すること、
前記複数のモーションベクトルに基づいて、前記第１のフレームから前記補間ポイントへのモーションおよび前記第２のフレームから前記補間ポイントまでを示す複数の候補補間モーションベクトルを生成すること、
前記第１のフレームおよび前記第２のフレームとの間の平滑性の制約に基づいて、前記複数の候補補間モーションベクトルから、補間モーションベクトルを選択すること、および
選択された前記補間モーションベクトルに基づいて、前記補間ポイントで補間されたフレームを生成することを含み、
前記補間されたフレームを生成することは、複数の補間ピクセルを生成することを含み、
前記補間モーションベクトルを選択することは、
複数の補間モーションベクトルを含む補間モーションフィールドを選択することを含み、
前記複数の補間モーションベクトルの各補間モーションベクトルは、前記複数の補間ピクセルのうち１つの補間ピクセルと関連し、
前記補間されたフレームを生成することは、前記補間モーションフィールドに基づいて前記補間されたフレームの加工を修正することを含み、
前記加工を修正することは、前記補間されたフレームと、前記第１のフレームおよび前記第２のフレームの平均とを融合することを含み、
前記融合の程度は、前記補間モーションフィールドの勾配に基づく方法。 Identifying multiple input video frames,
Generating a plurality of motion vectors indicative of motion from a first frame of the plurality of input video frames to a second frame of the plurality of input video frames;
Identifying an interpolation point between the first frame and the second frame;
Generating a plurality of candidate interpolation motion vectors indicating motion from the first frame to the interpolation point and from the second frame to the interpolation point based on the plurality of motion vectors;
Before SL and based on the smoothness constraints between the first frame and the second frame, from the plurality of candidate interpolation motion vectors, selecting the interpolated motion vectors, and
Generating a frame interpolated at the interpolation point based on the selected interpolated motion vector;
Generating the interpolated frame includes generating a plurality of interpolated pixels;
Selecting the interpolated motion vector includes
Selecting an interpolated motion field that includes a plurality of interpolated motion vectors;
Each interpolation motion vector of the plurality of interpolation motion vectors is associated with one of the plurality of interpolation pixels;
Generating the interpolated frame includes modifying the processing of the interpolated frame based on the interpolated motion field;
Modifying the processing includes fusing the interpolated frame with the average of the first frame and the second frame;
The degree of fusion is based on the gradient of the interpolated motion field.
前記入力ビデオの時間的シーケンスでは、
前記第１のフレームは、前記入力ビデオの時間的シーケンスの第１の時間を表し、
前記第２のフレームは、前記入力ビデオの時間的シーケンスの第２の時間を表し、
前記第１の時間は、前記入力ビデオの時間的シーケンスにおいて前記第２の時間に隣接し、
前記補間ポイントは、前記第１の時間と前記第２の時間との間の時間を示す請求項１または２に記載の方法。 The plurality of input video frames includes a temporal sequence of input videos;
In the temporal sequence of the input video:
The first frame represents a first time of the temporal sequence of the input video;
The second frame represents a second time of the temporal sequence of the input video;
The first time is adjacent to the second time in the temporal sequence of the input video;
The method according to claim 1 or 2 , wherein the interpolation point indicates a time between the first time and the second time.
前記入力ビデオの空間的シーケンスでは、
前記第１のフレームは、前記入力ビデオの空間的シーケンスの第１の角度から取り込まれたコンテンツを表し、
前記第２のフレームは、前記入力ビデオの空間的シーケンスの第２の角度から取り込まれたコンテンツを表し、
前記第１の角度は、前記入力ビデオの空間的シーケンスにおいて前記第２の角度に隣接し、
前記補間ポイントは、前記第１の角度と前記第２の角度との間の第３の角度を示す請求項１または２に記載の方法。 The plurality of input video frames include a spatial sequence of input videos;
In the spatial sequence of the input video:
The first frame represents the content captured from a first angular spatial sequence of the input video,
The second frame represents the contents taken from the second angular spatial sequence of the input video,
The first angle is adjacent to the second angle in the spatial sequence of the input video;
The method according to claim 1 or 2 , wherein the interpolation point indicates a third angle between the first angle and the second angle.
前記補間モーションベクトルを選択することは、前記複数の補間サイトの補間サイトごとに、反復的に補間モーションベクトルを選択し、
前記選択された補間モーションベクトルに基づいて、前記補間されたフレームをアップデートすることを含む請求項１〜４のいずれか一項に記載の方法。 The interpolated frame includes a plurality of interpolation sites;
Selecting the interpolation motion vector repeatedly selects an interpolation motion vector for each interpolation site of the plurality of interpolation sites,
Based on the interpolation motion vector said selected method according to any one of claims 1 to 4, comprising updating the interpolated frame.
高いモーション勾配を有する前記補間されたフレームの一部を、前記第１のフレームと前記第２のフレームの前記平均に対応する領域と置き換えること、および
低いモーション勾配を有する前記補間されたフレームの一部を維持することを含む請求項１〜６のいずれか一項に記載の方法。 Fusing the interpolated frames is
Replacing a portion of the interpolated frame having a high motion gradient with a region corresponding to the average of the first frame and the second frame; and one of the interpolated frames having a low motion gradient. 7. The method according to any one of claims 1 to 6, comprising maintaining a part.
前記方法は、更に、前記平滑性の程度が閾値を越える状態で、前記補間されたフレームとして前記第１のフレームまたは前記第２のフレームを使用することを含む請求項１〜７のいずれか一項に記載の方法。 Generating the plurality of motion vectors includes determining a degree of smoothness of the plurality of motion vectors;
The method further, with the degree of the smoothness exceeds a threshold, any one of claims 1 to 7, which comprises using the first frame or the second frame as the interpolation frame The method according to item.
前記複数の候補補間モーションベクトルを生成すること、前記補間モーションベクトルを選択すること、および前記微細な解像度で補間されたフレームを生成することは、より高解像度のブロックに基づくモーションフィールドまたはピクセルに基づくモーションフィールドを用いることを含む請求項９に記載の方法。 Generating the plurality of candidate interpolated motion vectors, selecting the interpolated motion vector, and generating the interpolated frame at the coarse resolution include using a block-based motion field;
Generating the plurality of candidate interpolated motion vectors, selecting the interpolated motion vector, and generating an interpolated frame at the fine resolution are based on motion fields or pixels based on higher resolution blocks. The method of claim 9 , comprising using a motion field.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/853,354 | 2013-03-29 | ||
US13/853,354 US9300906B2 (en) | 2013-03-29 | 2013-03-29 | Pull frame interpolation |
PCT/US2014/032207 WO2014175995A1 (en) | 2013-03-29 | 2014-03-28 | Motion-compensated frame interpolation using smoothness constraints |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2016515776A JP2016515776A (en) | 2016-05-30 |
JP6457483B2 true JP6457483B2 (en) | 2019-01-23 |
Family
ID=50680188
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2016505598A Active JP6457483B2 (en) | 2013-03-29 | 2014-03-28 | Frame interpolation with motion compensation using smoothness constraints |
Country Status (6)
Country | Link |
---|---|
US (2) | US9300906B2 (en) |
EP (1) | EP2979243A1 (en) |
JP (1) | JP6457483B2 (en) |
KR (1) | KR102210415B1 (en) |
CN (1) | CN105247569B (en) |
WO (1) | WO2014175995A1 (en) |
Families Citing this family (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8737824B1 (en) | 2012-03-09 | 2014-05-27 | Google Inc. | Adaptively encoding a media stream with compound prediction |
US9185414B1 (en) | 2012-06-29 | 2015-11-10 | Google Inc. | Video encoding using variance |
US9628790B1 (en) | 2013-01-03 | 2017-04-18 | Google Inc. | Adaptive composite intra prediction for image and video compression |
US9609343B1 (en) | 2013-12-20 | 2017-03-28 | Google Inc. | Video coding using compound prediction |
US10104394B2 (en) | 2014-01-31 | 2018-10-16 | Here Global B.V. | Detection of motion activity saliency in a video sequence |
US20150221335A1 (en) * | 2014-02-05 | 2015-08-06 | Here Global B.V. | Retiming in a Video Sequence |
CN103929641A (en) * | 2014-05-12 | 2014-07-16 | 北京工商大学 | Intra-frame encoding method based on virtual reference frame |
US9286653B2 (en) | 2014-08-06 | 2016-03-15 | Google Inc. | System and method for increasing the bit depth of images |
US9153017B1 (en) | 2014-08-15 | 2015-10-06 | Google Inc. | System and method for optimized chroma subsampling |
JP6463967B2 (en) * | 2014-12-25 | 2019-02-06 | キヤノン株式会社 | Imaging apparatus and control method thereof |
EP3257039A1 (en) * | 2015-02-11 | 2017-12-20 | Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. | Method and device for emulating continuously varying frame rates |
WO2017030380A1 (en) * | 2015-08-20 | 2017-02-23 | Lg Electronics Inc. | Digital device and method of processing data therein |
US10778999B2 (en) * | 2016-09-30 | 2020-09-15 | Qualcomm Incorporated | Frame rate up-conversion coding mode with affine motion model |
US11202028B2 (en) | 2017-04-05 | 2021-12-14 | Samsung Electronics Co., Ltd. | Display device configuring multi display system and control method thereof |
US10805630B2 (en) * | 2017-04-28 | 2020-10-13 | Qualcomm Incorporated | Gradient based matching for motion search and derivation |
US11778195B2 (en) * | 2017-07-07 | 2023-10-03 | Kakadu R & D Pty Ltd. | Fast, high quality optical flow estimation from coded video |
KR102406513B1 (en) * | 2017-09-21 | 2022-06-15 | 현대자동차주식회사 | Image transforming System and Method for transforming Image |
CN108040217B (en) * | 2017-12-20 | 2020-01-24 | 深圳岚锋创视网络科技有限公司 | Video decoding method and device and camera |
MX2021007755A (en) | 2018-12-28 | 2021-08-05 | Godo Kaisha Ip Bridge 1 | Moving image encoding device, moving image encoding method, moving image encoding program, moving image decoding device, moving image decoding method, and moving image decoding program. |
KR20230163002A (en) | 2018-12-28 | 2023-11-29 | 가부시키가이샤 제이브이씨 켄우드 | Dynamic image decoding device, dynamic image decoding method, dynamic image decoding program, dynamic image encoding device, dynamic image encoding method, dynamic image encoding program, storing method and transmitting method |
CN113766313B (en) * | 2019-02-26 | 2024-03-05 | 深圳市商汤科技有限公司 | Video data processing method and device, electronic equipment and storage medium |
WO2021005609A1 (en) * | 2019-07-09 | 2021-01-14 | Shmuel Mangan | Real-time motion tracking in moving scenes |
US11303847B2 (en) * | 2019-07-17 | 2022-04-12 | Home Box Office, Inc. | Video frame pulldown based on frame analysis |
EP3987770A4 (en) * | 2019-08-20 | 2022-08-17 | Samsung Electronics Co., Ltd. | Electronic device for improving graphic performace of application program and operating method thereof |
US11847737B2 (en) * | 2021-04-09 | 2023-12-19 | Nvidia Corporation | Temporal denoiser quality in dynamic scenes |
Family Cites Families (98)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8396328B2 (en) | 2001-05-04 | 2013-03-12 | Legend3D, Inc. | Minimal artifact image sequence depth enhancement system and method |
ATE74219T1 (en) | 1987-06-02 | 1992-04-15 | Siemens Ag | METHOD FOR DETERMINING MOTION VECTOR FIELDS FROM DIGITAL IMAGE SEQUENCES. |
EP0466981B1 (en) | 1990-07-20 | 1997-02-26 | Koninklijke Philips Electronics N.V. | Motion vector processing device |
DE69226994T2 (en) * | 1991-07-19 | 1999-02-18 | Matsushita Electric Ind Co Ltd | Aliasing interference mitigation video signal processing apparatus |
EP0533195A2 (en) | 1991-09-20 | 1993-03-24 | Sony Corporation | Picture signal encoding and/or decoding apparatus |
JP3263960B2 (en) | 1991-10-22 | 2002-03-11 | ソニー株式会社 | Motion vector encoder and decoder |
FI94306C (en) | 1993-07-15 | 1995-08-10 | Nokia Technology Gmbh | Method for determining motion vectors of small TV image segments |
US5398068A (en) | 1993-09-02 | 1995-03-14 | Trustees Of Princeton University | Method and apparatus for determining motion vectors for image sequences |
US5929919A (en) | 1994-04-05 | 1999-07-27 | U.S. Philips Corporation | Motion-compensated field rate conversion |
JPH0837662A (en) | 1994-07-22 | 1996-02-06 | Hitachi Ltd | Picture encoding/decoding device |
US5883975A (en) | 1994-09-12 | 1999-03-16 | Nippon Steel Corporation | Compression and decompression methods on two-dimensional image data |
DE69619002T2 (en) | 1995-03-10 | 2002-11-21 | Toshiba Kawasaki Kk | Image coding - / - decoding device |
EP1274253A3 (en) | 1995-08-29 | 2005-10-12 | Sharp Kabushiki Kaisha | Video coding device and video decoding device with a motion compensated interframe prediction |
US6058141A (en) | 1995-09-28 | 2000-05-02 | Digital Bitcasting Corporation | Varied frame rate video |
US6005980A (en) | 1997-03-07 | 1999-12-21 | General Instrument Corporation | Motion estimation and compensation of video object planes for interlaced digital video |
US5991447A (en) | 1997-03-07 | 1999-11-23 | General Instrument Corporation | Prediction and coding of bi-directionally predicted video object planes for interlaced digital video |
US6359929B1 (en) | 1997-07-04 | 2002-03-19 | Matsushita Electric Industrial Co., Ltd. | Image predictive decoding method, image predictive decoding apparatus, image predictive coding apparatus, and data storage medium |
KR100244291B1 (en) | 1997-07-30 | 2000-02-01 | 구본준 | Method for motion vector coding of moving picture |
DE69836473T2 (en) | 1997-09-23 | 2007-09-20 | Koninklijke Philips Electronics N.V. | MOTION ESTIMATION AND MOTION COMPENSATED INTERPOLATION |
DE69834902T2 (en) | 1997-11-17 | 2007-02-01 | Koninklijke Philips Electronics N.V. | MOTION COMPENSATED PREDICTIVE PICTURE CODING AND DECODING |
KR100523908B1 (en) | 1997-12-12 | 2006-01-27 | 주식회사 팬택앤큐리텔 | Apparatus and method for encoding video signal for progressive scan image |
EP0940774A3 (en) | 1998-03-05 | 2000-07-05 | Matsushita Electric Industrial Co., Ltd. | Motion vector coding and decoding apparatus and method |
US6956898B1 (en) * | 1998-10-02 | 2005-10-18 | Lucent Technologies Inc. | Method and apparatus for dense motion field based coding |
JP4142180B2 (en) * | 1998-10-29 | 2008-08-27 | 富士通株式会社 | Motion vector encoding device and decoding device |
US6594313B1 (en) * | 1998-12-23 | 2003-07-15 | Intel Corporation | Increased video playback framerate in low bit-rate video applications |
US6735249B1 (en) | 1999-08-11 | 2004-05-11 | Nokia Corporation | Apparatus, and associated method, for forming a compressed motion vector field utilizing predictive motion coding |
JP3975629B2 (en) | 1999-12-16 | 2007-09-12 | ソニー株式会社 | Image decoding apparatus and image decoding method |
FI108900B (en) * | 1999-12-28 | 2002-04-15 | Martti Kesaeniemi | Optical Flow and Image Creation |
US6711211B1 (en) | 2000-05-08 | 2004-03-23 | Nokia Mobile Phones Ltd. | Method for encoding and decoding video information, a motion compensated video encoder and a corresponding decoder |
US6449312B1 (en) | 2000-06-08 | 2002-09-10 | Motorola, Inc. | Method of estimating motion in interlaced video |
JP3953710B2 (en) * | 2000-06-08 | 2007-08-08 | 株式会社エヌ・ティ・ティ・データ | Video surveillance system |
US7003174B2 (en) | 2001-07-02 | 2006-02-21 | Corel Corporation | Removal of block encoding artifacts |
US6865227B2 (en) * | 2001-07-10 | 2005-03-08 | Sony Corporation | Error concealment of video data using motion vector data recovery |
US8111754B1 (en) * | 2001-07-11 | 2012-02-07 | Dolby Laboratories Licensing Corporation | Interpolation of video compression frames |
US7133070B2 (en) | 2001-09-20 | 2006-11-07 | Eastman Kodak Company | System and method for deciding when to correct image-specific defects based on camera, scene, display and demographic data |
KR100396558B1 (en) | 2001-10-25 | 2003-09-02 | 삼성전자주식회사 | Apparatus and method for converting frame and/or field rate using adaptive motion compensation |
US7054367B2 (en) | 2001-12-31 | 2006-05-30 | Emc Corporation | Edge detection based on variable-length codes of block coded video |
KR100441509B1 (en) * | 2002-02-25 | 2004-07-23 | 삼성전자주식회사 | Apparatus and method for transformation of scanning format |
US7324120B2 (en) | 2002-07-01 | 2008-01-29 | Xerox Corporation | Segmentation method and system for scanned documents |
DE10236207A1 (en) | 2002-08-07 | 2004-02-26 | Micronas Gmbh | Method for movement-vector supported interpolation of inter-image picture element e.g. for television technology, involves multiplying image information values |
US6927804B2 (en) | 2002-09-09 | 2005-08-09 | Eastman Kodak Company | Reducing color aliasing artifacts from color digital images |
US7084906B2 (en) | 2002-10-15 | 2006-08-01 | Eastman Kodak Company | Reducing computation time in removing color aliasing artifacts from color digital images |
US7020201B2 (en) * | 2002-11-20 | 2006-03-28 | National Chiao Tung University | Method and apparatus for motion estimation with all binary representation |
US7158668B2 (en) | 2003-08-01 | 2007-01-02 | Microsoft Corporation | Image processing using linear light values and other image processing improvements |
KR20060060000A (en) | 2003-08-05 | 2006-06-02 | 코닌클리케 필립스 일렉트로닉스 엔.브이. | Video encoding and decoding methods and corresponding devices |
US7474355B2 (en) | 2003-08-06 | 2009-01-06 | Zoran Corporation | Chroma upsampling method and apparatus therefor |
US7346109B2 (en) * | 2003-12-23 | 2008-03-18 | Genesis Microchip Inc. | Motion vector computation for video sequences |
US7483577B2 (en) | 2004-03-02 | 2009-01-27 | Mitsubishi Electric Research Laboratories, Inc. | System and method for joint de-interlacing and down-sampling using adaptive frame and field filtering |
EP1583367A1 (en) * | 2004-03-30 | 2005-10-05 | Matsushita Electric Industrial Co., Ltd. | Motion estimation employing line and column vectors |
AR049593A1 (en) | 2004-07-20 | 2006-08-16 | Qualcomm Inc | METHOD AND APPARATUS FOR PREDICTING THE MOTION VECTOR IN TEMPORARY VIDEO COMPRESSION. |
US7724307B2 (en) | 2004-07-28 | 2010-05-25 | Broadcom Corporation | Method and system for noise reduction in digital video |
US7860158B2 (en) * | 2004-08-27 | 2010-12-28 | Mitsubishi Electric Research Laboratories Inc. | Coding correlated images using syndrome bits |
US9071847B2 (en) | 2004-10-06 | 2015-06-30 | Microsoft Technology Licensing, Llc | Variable coding resolution in video codec |
JP4877449B2 (en) * | 2004-11-04 | 2012-02-15 | カシオ計算機株式会社 | Moving picture coding apparatus and moving picture coding processing program |
GB0500174D0 (en) * | 2005-01-06 | 2005-02-16 | Kokaram Anil | Method for estimating motion and occlusion |
US8165215B2 (en) | 2005-04-04 | 2012-04-24 | Technion Research And Development Foundation Ltd. | System and method for designing of dictionaries for sparse representation |
US8422546B2 (en) | 2005-05-25 | 2013-04-16 | Microsoft Corporation | Adaptive video encoding using a perceptual model |
US7747082B2 (en) | 2005-10-03 | 2010-06-29 | Xerox Corporation | JPEG detectors and JPEG image history estimators |
US8582660B2 (en) * | 2006-04-13 | 2013-11-12 | Qualcomm Incorporated | Selective video frame rate upconversion |
US8243194B2 (en) * | 2006-05-31 | 2012-08-14 | Vestel Elektronik Sanayi Ve Ticaret A.S. | Method and apparatus for frame interpolation |
JP4176114B2 (en) | 2006-06-19 | 2008-11-05 | シャープ株式会社 | Image compression apparatus, image reading apparatus including the same, image processing apparatus including the image compression apparatus, image forming apparatus including the same, and image compression processing method |
EP1876823A1 (en) | 2006-07-07 | 2008-01-09 | Matsushita Electric Industrial Co., Ltd. | Video processing device and method for processing videos |
WO2008083500A1 (en) * | 2007-01-11 | 2008-07-17 | 360 Replays Ltd. | Method and system for generating a replay video |
US8335105B2 (en) | 2007-03-07 | 2012-12-18 | Headway Technologies, Inc. | Magnetic memory cell |
JP4513819B2 (en) * | 2007-03-19 | 2010-07-28 | 株式会社日立製作所 | Video conversion device, video display device, and video conversion method |
WO2008119480A2 (en) | 2007-03-31 | 2008-10-09 | Sony Deutschland Gmbh | Noise reduction method and unit for an image frame |
US20080285656A1 (en) * | 2007-05-17 | 2008-11-20 | The Hong Kong University Of Science And Technology | Three-loop temporal interpolation for error concealment of multiple description coding |
GB2450121A (en) | 2007-06-13 | 2008-12-17 | Sharp Kk | Frame rate conversion using either interpolation or frame repetition |
US8218638B2 (en) * | 2007-10-31 | 2012-07-10 | Broadcom Corporation | Method and system for optical flow based motion vector estimation for picture rate up-conversion |
WO2009085205A1 (en) * | 2007-12-20 | 2009-07-09 | Integrated Device Technology, Inc. | Image interpolation with halo reduction |
US8396129B2 (en) | 2007-12-28 | 2013-03-12 | Ati Technologies Ulc | Apparatus and method for single-pass, gradient-based motion compensated image rate conversion |
JP4930409B2 (en) * | 2008-02-21 | 2012-05-16 | 富士通株式会社 | Image coding apparatus, image coding method, and image coding program |
US8477848B1 (en) * | 2008-04-22 | 2013-07-02 | Marvell International Ltd. | Picture rate conversion system architecture |
KR101493325B1 (en) * | 2008-09-03 | 2015-02-16 | 삼성전자주식회사 | Apparatus and method for frame interpolation based on accurate motion estimation |
EP2237560A1 (en) * | 2009-03-30 | 2010-10-06 | Vestel Elektronik Sanayi ve Ticaret A.S. | Halo reducing motion-compensated interpolation |
US8289444B2 (en) | 2009-05-06 | 2012-10-16 | Samsung Electronics Co., Ltd. | System and method for reducing visible halo in digital video with covering and uncovering detection |
US8682094B2 (en) | 2009-05-12 | 2014-03-25 | Dynamic Invention Llc | Adaptive subpixel-based downsampling and filtering using edge detection |
JP2010288098A (en) * | 2009-06-12 | 2010-12-24 | Sony Corp | Device, method and program for interpolation of image frame |
US8761531B2 (en) | 2009-07-09 | 2014-06-24 | Qualcomm Incorporated | Image data compression involving sub-sampling of luma and chroma values |
US8593692B2 (en) | 2009-08-12 | 2013-11-26 | Xerox Corporation | Systems and methods for building a color lookup table for a printer |
CA2714932A1 (en) | 2009-09-23 | 2011-03-23 | Her Majesty The Queen In The Right Of Canada As Represented By The Minister Of Industry | Image interpolation for motion/disparity compensation |
JP5844263B2 (en) | 2009-10-05 | 2016-01-13 | ビーマル イメージング リミテッドＢｅａｍｒ Ｉｍａｇｉｎｇ Ｌｔｄ． | Apparatus and method for recompressing digital images |
US8724022B2 (en) * | 2009-11-09 | 2014-05-13 | Intel Corporation | Frame rate conversion using motion estimation and compensation |
US9445072B2 (en) * | 2009-11-11 | 2016-09-13 | Disney Enterprises, Inc. | Synthesizing views based on image domain warping |
JP4991890B2 (en) * | 2010-03-01 | 2012-08-01 | 株式会社東芝 | Interpolated frame generation apparatus and method |
US9635308B2 (en) * | 2010-06-02 | 2017-04-25 | Cisco Technology, Inc. | Preprocessing of interlaced video with overlapped 3D transforms |
WO2012033498A1 (en) | 2010-09-10 | 2012-03-15 | Hewlett-Packard Development Company, L.P. | Systems and methods for data compression |
HUE056453T2 (en) * | 2010-11-04 | 2022-02-28 | Ge Video Compression Llc | Picture coding supporting block merging and skip mode |
US9609349B2 (en) * | 2010-12-14 | 2017-03-28 | M & K Holdings Inc. | Apparatus for decoding a moving picture |
JP5645636B2 (en) * | 2010-12-16 | 2014-12-24 | 三菱電機株式会社 | Frame interpolation apparatus and method |
GB2501402B (en) | 2010-12-21 | 2015-06-24 | Syndiant Inc | Spatial light modulator with storage reducer |
EP4120686B1 (en) * | 2011-01-07 | 2023-08-23 | Ntt Docomo, Inc. | Predictive encoding method, predictive encoding device, and predictive encoding program of motion vector, and, predictive decoding method, predictive decoding device, and predictive decoding program of motion vector |
US9699456B2 (en) | 2011-07-20 | 2017-07-04 | Qualcomm Incorporated | Buffering prediction data in video coding |
WO2013088589A1 (en) * | 2011-12-14 | 2013-06-20 | パナソニック株式会社 | Video signal processing device, video signal output device, and video signal processing method |
US9324133B2 (en) | 2012-01-04 | 2016-04-26 | Sharp Laboratories Of America, Inc. | Image content enhancement using a dictionary technique |
US9191551B2 (en) | 2012-02-24 | 2015-11-17 | Apple Inc. | Pixel normalization |
US20130257851A1 (en) * | 2012-04-01 | 2013-10-03 | Chao-Hua Lee | Pipeline web-based process for 3d animation |
US9153073B2 (en) * | 2012-05-23 | 2015-10-06 | Qualcomm Incorporated | Spatially registered augmented video |
-
2013
- 2013-03-29 US US13/853,354 patent/US9300906B2/en active Active
-
2014
- 2014-03-28 JP JP2016505598A patent/JP6457483B2/en active Active
- 2014-03-28 KR KR1020157024660A patent/KR102210415B1/en active IP Right Grant
- 2014-03-28 EP EP14722486.9A patent/EP2979243A1/en not_active Ceased
- 2014-03-28 WO PCT/US2014/032207 patent/WO2014175995A1/en active Application Filing
- 2014-03-28 CN CN201480030013.1A patent/CN105247569B/en active Active
-
2016
- 2016-03-24 US US15/079,251 patent/US9888255B1/en active Active
Also Published As
Publication number | Publication date |
---|---|
US9300906B2 (en) | 2016-03-29 |
WO2014175995A1 (en) | 2014-10-30 |
CN105247569A (en) | 2016-01-13 |
KR102210415B1 (en) | 2021-02-01 |
EP2979243A1 (en) | 2016-02-03 |
US9888255B1 (en) | 2018-02-06 |
CN105247569B (en) | 2020-02-07 |
US20140294320A1 (en) | 2014-10-02 |
KR20150138186A (en) | 2015-12-09 |
JP2016515776A (en) | 2016-05-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6457483B2 (en) | Frame interpolation with motion compensation using smoothness constraints | |
CN110741640B (en) | Optical flow estimation for motion compensated prediction in video coding | |
US20150312575A1 (en) | Advanced video coding method, system, apparatus, and storage medium | |
US11743475B2 (en) | Advanced video coding method, system, apparatus, and storage medium | |
US20110206118A1 (en) | Data Compression for Video | |
TWI452907B (en) | Optimized deblocking filters | |
JP2007503776A (en) | Method and apparatus for minimizing the number of reference images used for inter coding | |
JP2000175193A (en) | Picture data interpolation method, frame speed-up converting method and method for deciding real motion vector associated with characteristic block in electronic digital picture sequence reproducing system | |
CN105850124B (en) | Method and apparatus for encoding and decoding video signal using additional control of quantization error | |
US20230308679A1 (en) | Motion prediction coding with coframe motion vectors | |
JP2012151576A (en) | Image coding method, image coding device, image decoding method and image decoding device | |
JP6339977B2 (en) | Video encoding apparatus and video encoding program | |
JP5566406B2 (en) | Method for predicting block of image data, decoding and encoding device for realizing the method | |
Ratnottar et al. | Comparative study of motion estimation & motion compensation for video compression | |
WO2016193949A1 (en) | Advanced video coding method, system, apparatus and storage medium | |
US10051268B2 (en) | Method for encoding, decoding video signal and device therefor | |
WO2023205371A1 (en) | Motion refinement for a co-located reference frame | |
JP5911982B2 (en) | Image decoding method | |
JP5750191B2 (en) | Image decoding method | |
CN118055253A (en) | Optical flow estimation for motion compensated prediction in video coding | |
KR100969135B1 (en) | Apparatus and method of decompressing distributed way coded video with successively improving side information according to?the reliability of the?decoded data | |
Pradeep Kumar et al. | Intra-frame Prediction Mode Decision for Efficient Compression and Signal Quality | |
Kang et al. | Power-rate-distortion optimized resource allocation for low-complexity multiview distributed video coding | |
Kanger et al. | Video Image Compression Using Absolute Moment Block Truncation Method with Orthogonal Search Motion Estimation Technique | |
Sarhan et al. | Spatio-Temporal Error Concealment Algorithms for Encoded Video Streams for Real-Time Applications |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20170309 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20180412 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20180424 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20181204 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20181220 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6457483Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |