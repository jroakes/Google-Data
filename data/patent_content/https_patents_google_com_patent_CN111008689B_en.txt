CN111008689B - Using SOFTMAX approximation to reduce neural network inference time - Google Patents
Using SOFTMAX approximation to reduce neural network inference time Download PDFInfo
- Publication number
- CN111008689B CN111008689B CN201910924950.6A CN201910924950A CN111008689B CN 111008689 B CN111008689 B CN 111008689B CN 201910924950 A CN201910924950 A CN 201910924950A CN 111008689 B CN111008689 B CN 111008689B
- Authority
- CN
- China
- Prior art keywords
- neural network
- output
- cluster
- vector
- subset
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 96
- 239000013598 vector Substances 0.000 claims abstract description 113
- 238000000034 method Methods 0.000 claims abstract description 48
- 238000012545 processing Methods 0.000 claims abstract description 29
- 238000012216 screening Methods 0.000 claims abstract description 27
- 238000012549 training Methods 0.000 claims description 25
- 230000008569 process Effects 0.000 claims description 22
- 238000013459 approach Methods 0.000 claims description 3
- 239000003550 marker Substances 0.000 claims description 3
- 238000005457 optimization Methods 0.000 claims 5
- 238000004590 computer program Methods 0.000 abstract description 15
- 230000006870 function Effects 0.000 description 9
- 238000010801 machine learning Methods 0.000 description 7
- 230000009471 action Effects 0.000 description 5
- 238000004891 communication Methods 0.000 description 5
- 230000003993 interaction Effects 0.000 description 3
- 230000015654 memory Effects 0.000 description 3
- 230000026676 system process Effects 0.000 description 3
- 238000004364 calculation method Methods 0.000 description 2
- 238000010606 normalization Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000013519 translation Methods 0.000 description 2
- 241000009334 Singa Species 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000015556 catabolic process Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 230000001143 conditioned effect Effects 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000006731 degradation reaction Methods 0.000 description 1
- 230000000593 degrading effect Effects 0.000 description 1
- 238000010586 diagram Methods 0.000 description 1
- 238000011478 gradient descent method Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/16—Matrix or vector computation, e.g. matrix-matrix or matrix-vector multiplication, matrix factorization
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/18—Complex mathematical operations for evaluating statistical data, e.g. average values, frequency distributions, probability functions, regression analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/23—Clustering techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/23—Clustering techniques
- G06F18/232—Non-hierarchical techniques
- G06F18/2321—Non-hierarchical techniques using statistics or function optimisation, e.g. modelling of probability density functions
- G06F18/23213—Non-hierarchical techniques using statistics or function optimisation, e.g. modelling of probability density functions with fixed number of clusters, e.g. K-means clustering
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2413—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on distances to training or reference patterns
- G06F18/24133—Distances to prototypes
- G06F18/24143—Distances to neighbourhood prototypes, e.g. restricted Coulomb energy networks [RCEN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
- G06N5/041—Abduction
Abstract
Use of a SOFTMAX approximation to reduce neural network inference time is disclosed. Methods, systems, and apparatus, including computer programs encoded on computer storage media, use softmax to approximately reduce neural network inference time. One of the methods includes maintaining data specifying a respective softmax weight vector for each output in the output vocabulary of the possible neural network; receiving a neural network input; processing the neural network input using one or more initial neural network layers to generate a context vector for the neural network input; and generating an approximate score distribution over the vocabulary of possible neural network outputs for the neural network inputs, comprising: processing the context vector using a screening model configured to input an appropriate subset of the predicted vocabulary for the context; and generating a respective logic for each output in the appropriate subset, including applying a softmax weight vector of the output to the context vector.
Description
Technical Field
The present description relates to processing inputs using neural networks.
Background
Neural networks are machine-learning models that employ one or more layers of nonlinear units to predict output for a received input. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as an input to the next layer (i.e., the next hidden layer or output layer) in the network. Each layer of the network generates an output from the received input in accordance with the current value of the respective parameter set.
Disclosure of Invention
The present specification describes a system implemented as a computer program on one or more computers in one or more locations that implements a neural network that replaces a conventional softmax output layer with an accurate approximation of the softmax layer that can be calculated using less computation and in less time. This greatly reduces the time required for the described system to calculate neural network reasoning for a given input relative to conventional systems.
In particular, the system maintains data specifying a respective softmax weight vector (and optionally a respective bias value) for each of the vocabularies of possible neural network outputs. Which outputs are in the vocabulary of possible neural network outputs depends on the task that the neural network is configured to perform.
For example, when the task is machine translation, the vocabulary includes words in the target language. In other words, when the task is machine translation, the input of the system is text in the source language and the output of the system is text in another language.
As another example, the task may be another type of natural language generation task that uses vocabulary that is a possible web output of words in natural language. One example of such a task is text summarization, where the system receives an input text sequence and generates an output text sequence that summarizes the input text sequence with fewer words. Another example of such a task is text paraphrasing, where the system receives an input text sequence and generates an output text sequence having a similar meaning to the input text sequence but using different words than the input text sequence. Another example of such a task is image captioning, where the system receives an input image and generates an output text sequence describing the content of the input image. Another example of such a task is a question-answer, where the system receives an input text sequence and generates an output text sequence as an answer to a question posed in the input text sequence. In general, a natural language generation task is a task in which a system generates a sequence of text conditioned on some input, which may be text or different types of data.
When the task is speech recognition, i.e., to convert input audio data to text, the vocabulary may include one or more of phonemes, characters, or words in the target natural language alphabet.
When the task is image classification, i.e. in order to classify an input image as belonging to one of a set of object categories, the vocabulary may comprise a set of object categories.
In general, the described techniques are applicable to any machine learning task with large vocabulary of possible outputs. As described below, when the size of the vocabulary is large and the neural network needs to produce a probability distribution on the output in order to generate the network output, the neural network inference time is hampered by the computation of the softmax layer.
To compute reasoning, the system receives the neural network input and processes the neural network input using one or more initial neural network layers to generate a context vector for the neural network input. As above, which neural network layers are part of the neural network depends on the task. For example, for sequence processing tasks, the layers may include one or more of a recursive layer (e.g., an LSTM layer), a feed forward layer, a self-care layer, or a convolutional layer. The context vector will typically be the output of the last initial neural network layer or a combination of outputs from multiple initial neural network layers, for example, if the neural network includes a skip connection.
The system generates an approximate score distribution for the neural network input over the vocabulary of possible neural network outputs, i.e., rather than generating an actual softmax score distribution by processing the context vector through a conventional softmax layer.
To generate the approximate distribution, the system processes the context vector using a screening model configured to input an appropriate subset, i.e., less than all, of the predicted vocabulary for the context.
The system then generates a respective logic for each output in the appropriate subset by applying the softmax weight vector for that output to the context vector (and then optionally applying the bias value for that output). In some cases, the system uses these logits as the scores in the score distribution (and assigns a score of zero for each output that is not in the proper subset). In other cases, the system normalizes the logit to generate a respective probability for each output that is in the proper subset (and assigns a probability of zero for each output that is not in the proper subset).
Each appropriate subset is typically small relative to the whole vocabulary. Thus, to calculate an approximate distribution, the system need only perform a small amount of inner products rather than calculating a different inner product for each output in the vocabulary as would be required for a conventional softmax.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
Many neural networks consist of: a set of initial layers that receive input and generate initial output (referred to herein as "context vectors"); and a softmax layer that receives the context vector generated by the initial layer and generates a probability distribution over the output vocabulary. The neural network then outputs the first k outputs in the vocabulary according to the probabilities or logits generated by the softmax layer or the first k probabilities or logits themselves. Thus, the softmax layer generates a logic for each output in the vocabulary and then normalizes the logic to generate probabilities in some cases. Generating a logic for an output requires multiplying the context vector by the weight vector corresponding to the logic, i.e., calculating the inner product between the two vectors, and then adding the offset value corresponding to the logic. Performing this calculation can be a significant bottleneck in computing neural network pushes when there are a large number of possible outputs. In fact, the need to perform the softmax layer processing may delay the overall inference speed to such an extent that a neural network cannot be implemented on a mobile device. On the other hand, the described techniques use lightweight screening models to accurately approximate the output of softmax with much fewer computations that would otherwise be required. This greatly reduces the time and computing resources required to perform reasoning using the neural network and allows for efficient deployment of the neural network in resource-constrained environments that require, for example, the generation of output on a mobile device in near real-time.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
Fig. 1 illustrates an example neural network system.
FIG. 2 illustrates an example of generating network output using a screening model.
FIG. 3 is a flow chart of an example process for generating a score distribution for neural network input.
FIG. 4 is a flow chart of an example process for training a screening model.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
Fig. 1 illustrates an example neural network system 100. The neural network system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations, in which the systems, components, and techniques described below are implemented.
This neural network system 100 implements a neural network that replaces a conventional softmax output layer with an accurate approximation of the softmax layer that can be calculated using fewer calculations and in less time. This greatly reduces the time required for the system 100 to calculate neural network reasoning for a given input relative to conventional systems.
The neural network implemented by the system 100 is configured to receive the neural network input 102 and generate a score distribution 132 for the neural network input 102 over the vocabulary of possible neural network outputs. As described above, which outputs are in the vocabulary of possible neural network outputs depends on the task that the neural network is configured to perform.
In general, the described techniques are applicable to any machine learning task having large vocabularies that may be output, such as tasks having output vocabularies of words of a target natural language or classification tasks having output vocabularies that span a very large number of possible classes. As described below, when the size of the vocabulary is large and the neural network needs to produce a probability distribution on the output in order to generate the network output, the neural network inference time is hampered by the computation of the softmax layer.
Once the score distribution 132 has been generated, the system 100 may provide the score distribution 132 as a network output of the system, e.g., for presentation to a user or to another system, or may select one or more of the possible outputs with the highest probabilities and provide one or more of the possible network outputs as network outputs of the system. Alternatively, the system 100 may store the score distribution 132 or one or more possible outputs with the highest scores in association with the network input 102 for later use.
The neural network implemented by the system 100 includes one or more initial neural network layers 110 and components that replace the softmax layer trained jointly with the neural network layers 110: a screening model 120, a softmax engine 130, and softmax data 140.
As above, which initial neural network layers 110 are part of the neural network depends on the task. For example, for sequence processing tasks, the layers may include one or more of a recursive layer (e.g., a (long term short term memory) LSTM layer), a feed forward layer, a self-care layer, or a convolutional layer. For image classification tasks, these layers may be convolutional neural networks that generate context vectors from the input image.
In particular, the system 100 maintains softmax data 140, which softmax data 140 specifies for each output in the vocabulary of possible neural network outputs a respective softmax weight vector and optionally a respective bias value. The Softmax data 140 is generated as a result of training of the neural network, i.e., with one or more initial neural network layers 140 and a conventional Softmax output layer. That is, the weight vectors and bias values in the softmax data 140 are trained parameter values for the conventional softmax output layer.
To compute reasoning, the system 100 receives the neural network input 102 and processes the neural network input 102 using one or more initial neural network layers 110 to generate a context vector 112 for the neural network input 102. The context vector 112 will typically be the output of the last initial neural network layer or a combination of outputs from multiple initial neural network layers, for example, if the neural network includes a skip connection.
The system 100 then generates an approximate softmax score distribution over the vocabulary of possible neural network outputs for the neural network input 102, i.e., rather than generating an actual softmax score distribution by processing the context vector through a conventional softmax layer.
Specifically, if the system 100 were to implement a conventional softmax layer, the system would generate a respective logic for each possible output in the vocabulary by applying the softmax weight vector for the possible output to the context vector (and then optionally applying the bias value for that output), i.e., by calculating the inner product between the context vector and the softmax weight vector. When the vocabulary contains a large number of outputs, this requires the computation of a very large number of inner products, which is both computationally and time consuming. In some cases, i.e., when the demand score is a probability, the system will then normalize the logit to generate a corresponding probability for each output in the appropriate subset. This normalization is also computationally and time consuming, as it requires a corresponding normalization factor to be calculated for each log.
Instead, to generate an approximate distribution, the system 100 uses the screening model 120 to process the context vector 112.
The screening model 120 is configured to predict an appropriate subset, i.e., less than all of the vocabulary of the contextual input. That is, the screening model 120 identifies a subset of words 122 that includes less than all possible outputs in the word.
The screening model 120 generally includes, for each cluster in the set of clusters, a respective parameter vector for each cluster in the plurality of clusters and a candidate appropriate output subset for that each cluster. The candidate proper subset of outputs of a cluster is the proper subset of outputs in the output vocabulary that have been assigned to the cluster.
To predict the appropriate subset of contextual inputs, the system 100 clusters the contextual vectors 112 into a particular cluster of the plurality of clusters using the parameter vector and then assigns the appropriate subset of contextual vectors 112 to a candidate appropriate output subset of the particular cluster.
To cluster the context vectors 112, the system 100 determines for each cluster an inner product between the context vector and the parameter vector for the cluster and clusters the context vector 112 into the cluster with the highest inner product.
Suitable subsets of the vocabulary of the predicted context input 112 are described in more detail below with reference to fig. 2 and 3.
Training the context model 120 to determine parameter vectors for clusters and candidate appropriate subsets of clusters is described in more detail below with reference to FIG. 4.
The Softmax engine 130 then uses the Softmax data 140 and the vocabulary subset 122 to generate the score distribution 132.
Specifically, the softmax engine 130 generates a respective logic for each output in the vocabulary subset 122 by applying a softmax weight vector for the output to the context vector and then optionally applying the bias value for the output. In some cases, the softmax engine 130 uses these logits as the scores in the score distribution, with the score of each output that is not in the proper subset being zero.
In other cases, the softmax engine 130 normalizes the logit to generate a respective probability for each output in the appropriate subset. The system 130 then generates a probability distribution that assigns a respective probability to each output in the proper subset and a zero probability to each output that is not in the proper subset.
Each appropriate subset is typically small relative to the whole vocabulary. Thus, to calculate the approximate distribution, the system 100 need only perform a small number of inner products rather than calculating a different inner product for each output in the vocabulary as is required by conventional softmax layers.
Fig. 2 illustrates an example of generating neural network output using the screening model 120. In the example of fig. 2, the system 100 is performing a language modeling task that requires the system 100 to predict the next word given a sequence of words. Thus, the system needs to generate a probability distribution over the vocabulary of english words.
In particular, in The example of FIG. 2, the system 100 has received The current sequence "The dog is"202 and needs to predict words following "is" in The sequence.
For prediction, the system 100 processes the sequence 202 using the initial neural network layer 110 to generate the context vector 112. In the example of fig. 2, the initial neural network layer 110 is an LSTM layer.
The system 100 then uses the screening model 120 to process the context vector 112 to generate a vocabulary subset 122, i.e., an appropriate subset of words in the predicted output vocabulary. In the example of FIG. 2, the appropriate subset of words contains the three words "cure", "quick" and "big".
The system 100 then calculates a respective logic for each word in the subset by calculating a respective inner product between the softmax weight vector 210 and the context vector 112 for the words in the subset (and optionally adding a respective bias value to each logic).
In the example of FIG. 2, the system then calculates argmax 220 for logits to select the word predicted to follow the word "is" in the input sequence 202. In other cases, the system may normalize the logits to generate a probability distribution as described above with reference to fig. 1 or simply provide the top k logits (optionally the logits themselves) to the word as the network output of the system.
As can be seen from fig. 2, even though the vocabulary of english words will typically include a very large number of words, the system 100 need only perform inner products for a small portion of the words in the vocabulary to generate predictions by utilizing the screening model 120. Because the screening model is used to generate the appropriate subset, the system can achieve this resource savings by only minimally degrading the prediction quality.
FIG. 3 is a flow chart of an example process 300 for generating a score distribution for a neural network input. For convenience, process 300 is described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural network system, such as the neural network system 100 of fig. 1, may perform the process 300.
The system maintains softmax data for each possible output of the respective softmax weight vector included in the vocabulary of possible outputs (step 302). The softmax data may also optionally include a corresponding bias value for each possible output.
The system receives a neural network input (step 304).
The system then generates an approximate score distribution over the vocabulary of possible neural network outputs for the neural network input.
Specifically, the system processes the context vector using a screening model configured to input an appropriate subset of predictive terms for the context (step 306).
For each cluster in the set of clusters, the screening model typically includes a respective parameter vector for each cluster in the plurality of clusters and a candidate appropriate output subset for each cluster. To predict the proper subset of the contextual inputs, the system clusters the contextual vectors into a particular cluster of the plurality of clusters using the parameter vector, and then assigns the proper subset of the contextual vectors to a candidate proper output subset of the particular cluster. To cluster the context vectors, the system determines, for each cluster, an inner product between the context vector and the parameter vector of the cluster, and clusters the context vector into the cluster with the highest inner product.
The system then generates a respective logic for each output in the appropriate subset by applying the softmax weight vector of the output to the context vector (step 308).
In some cases, the system uses logit as the score in the approximate score distribution, i.e., by assigning a score of zero to the output that is not in the proper subset.
In some other cases, the system normalizes the logits to generate a respective probability for each output in the appropriate subset to generate an approximate softmax probability distribution (step 310). That is, the approximate softmax probability distribution includes a generated probability for each output that is in the proper subset and a zero probability for each output that is not in the proper subset.
Fig. 4 is a flow diagram of an example process 400 for training a screening model. For convenience, process 400 is described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural network system, such as the neural network system 100 of fig. 1, may perform the process 400.
The system receives a plurality of training context vectors (step 402). For example, the training context vector may be a context vector generated by the trained initial neural network layer for some or all of the training inputs used in training the neural network.
The system generates a respective ground truth markup vector for each training context input by processing the training context vector through the softmax layer (step 404). That is, the system processes the training context vector using a conventional ("complete") softmax layer that calculates an inner product between the training context vector and each softmax weight vector to generate a respective logic for each output in the vocabulary. The system then generates a ground truth markup vector that includes a respective value for each output in the vocabulary. For each possible output with a logic in the top k highest logits generated by the conventional softmax layer, the system will set the value of the possible output in the ground truth flag vector to 1. For each possible output having a logic that is not in the top k highest logits, the system sets the value of that logic to zero. The value of k may be a predetermined value that is a small fraction of the total number of outputs in the vocabulary, e.g., k may be equal to 1, 3, 5, or 10.
The system initializes a cluster parameter vector and candidate appropriate output subset for each cluster (step 406). For example, the system may initialize each candidate appropriate subset of outputs to an empty set, i.e., such that no output is initially assigned to any cluster. The system may use conventional clustering techniques (e.g., spherical kmeans) to initialize the clustering parameters on the training context vector.
The system then trains a screening model to determine, for each cluster, a training value for the cluster parameter vector and a final candidate appropriate subset of outputs. In particular, the system trains a screening model to minimize an objective function that measures the mismatch between the ground truth marker vector and the appropriate subset of logits for which the system is generating using process 300 and the current screening model. Specifically, the goal penalizes the screening model when (i) it is possible to output a value assigned 1 by the ground truth flag but not in the proper subset for which the logic was generated and (ii) it is possible to output a value not assigned 1 but in the proper subset. Specifically, (i) is penalized for potential degradation of prediction accuracy due to no logit being generated for the high scoring possible outputs, and (ii) is penalized for computing inner products for the uncorrelated possible outputs, i.e., computing resources are wasted on computing inner products for outputs that should not be in the proper subset.
As a particular example, the system may minimize the following objective function:
where the minimization is the value of the cluster parameter vector v and the candidate proper output subset c across each cluster, r is the total number of clusters, N is the total number of training context vectors, h i Is the ith training context vector, y is The value of the output s in the ground truth token vector, which is the ith training context vector, L is the total number of possible outputs in the vocabulary, λ is a predetermined weight value, B is a constant value representing the desired average number of outputs in each cluster,is given byAverage number of outputs in each cluster of the current values of the candidate cluster sets of each cluster, +.>Is an output of a value of 1 allocated in a differentiable single-heat representation (differentiable one-hot representation) of argmax of a score generated by an approximation softmax using a current value of an aggregate parameter vector and a subset of candidate suitable outputs, and c ds The output s is 1 when in the proper subset of outputs of cluster d and zero when the output s is not in the proper subset of outputs of cluster d. The differentiable single-hot representation may be generated by re-parameterizing argmax using gummel techniques.
The system may optimize this objective function by iteratively alternating between performing steps 408 and 410.
At step 408, the system updates the cluster parameter vector while keeping the candidate proper subset of outputs fixed. Specifically, to perform this update, the system may convert the cluster size constraint into terms of the objective function and then update the following objective function using a random gradient descent:
the system can be back-propagated through this objective function to update the cluster parameter vector using a random gradient descent method because a differentiable one-hot representation of argmax is used, i.e., because "gummel trick" is used, of the scores generated by the approximation softmax using the current value of the aggregate parameter vector and the candidate appropriate subset of outputs.
In step 410, the system updates the candidate appropriate output subset while keeping the aggregate parameter vector fixed. In particular, the system may solve the Knapsack problem (Knapsack problem) to calculate a candidate subset of suitable outputs for the update. More specifically, is a systemThe system will each c t,s Consider a term in which the weight is proportional to the number of outputs belonging to the aggregate and minimizes the value defined by the objective function, with the goal of maximizing the value within the weight capacity B. The system may apply a greedy approach (greedy approach) to solve this problem by ordering the items by find value-to-capacity ratio and adding them one by one until maximum capacity B is reached to determine an updated candidate proper subset of outputs.
Training the screening model in this manner allows the system to consider not only similarities between context vectors but also the predicted output space of the context vectors when assigning the appropriate subset to the context vectors using the trained screening model.
The term "configuration" in relation to systems and computer program components is used in this specification. By a system of one or more computers configured to perform a particular operation or action, it is meant that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operation or action. By one or more computer programs to be configured to perform a particular operation or action is meant that the one or more programs comprise instructions which, when executed by a data processing apparatus, cause the apparatus to perform the operation or action.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in: in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium, to perform or control the operation of data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access storage device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and includes all types of devices, apparatus, and machines for processing data, including for example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for the computer program, such as code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may be referred to or described as a program, software application, module, software module, script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and the computer program may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document; may be stored in a single file or in multiple coordinated files dedicated to the program, such as files that store portions of one or more modules, subroutines, or code. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured in any particular way or structured at all, and it may be stored on a storage device in one or more locations. Thus, for example, an index database may include multiple data sets, each of which may be organized and accessed differently.
Similarly, in this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or in combination with, one or more programmed computers, special purpose logic circuitry, e.g., an FPGA or ASIC.
A computer suitable for executing a computer program may be based on a general purpose or special purpose microprocessor or both or on any other type of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. In addition, a computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, to name a few, such as a Universal Serial Bus (USB) flash drive.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having: display devices for displaying information to a user, such as CRT (cathode ray tube), LCD (liquid crystal display) monitors; and a keyboard and a pointing device, such as a mouse or a trackball, by which a user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, a web page is sent to a web browser on a user device in response to receiving a request from the web browser. Moreover, the computer may interact with the user by sending text messages or other forms of messages to a personal device, such as a smart phone running a messaging application, and then receiving response messages from the user.
The data processing apparatus for implementing the machine learning model may also include, for example, a dedicated hardware accelerator unit in order to handle the usual and computationally intensive portions of machine learning training or generating (i.e., inferring) workloads.
The machine learning model may be implemented and deployed using a machine learning framework, such as a TensorFlow framework, microsoft Cognitive Toolkit framework, apache Singa framework, or Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server; or include middleware components, such as application servers; or a client computer including a front-end component, such as a graphical user interface, web browser, or application through which a user may interact with an implementation of the subject matter described in this specification; or a combination of one or more of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship between client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, such as HTML pages, to a user device for the purpose of displaying data to and receiving user input from a user interacting with the device acting as a client. Data generated at the user device, e.g., as a result of a user interaction, may be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings and described in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (14)
1. A method for processing neural network inputs, the method comprising:
maintaining data specifying a respective softmax weight vector for each of the possible neural network output words;
receiving a neural network input;
processing the neural network input using one or more initial neural network layers to generate a context vector for the neural network input; and
generating an approximate score distribution over the vocabulary of possible neural network outputs for the neural network inputs, comprising:
processing the context vector using a screening model configured to predict an appropriate subset of the vocabulary for the context input; and
generating a respective logic for each output in the appropriate subset, comprising: a respective softmax weight vector of outputs in the appropriate subset is applied to the context vector.
2. The method of claim 1, wherein the approximate score distribution is an approximate softmax probability distribution, and wherein generating the approximate score distribution comprises:
the logit is normalized to generate a respective probability for each output in the appropriate subset to generate the approximate softmax probability distribution.
3. The method of claim 1, wherein generating a respective logic for each output in the appropriate subset comprises:
the logic is calculated only for outputs in the appropriate subset.
4. The method of claim 1, wherein the maintained data further comprises a respective bias value for each output in the vocabulary, and wherein generating a respective logic for each output in the proper subset further comprises: the bias values of the outputs in the appropriate subset are added to the result of applying the softmax weight vector to the context vector.
5. The method of claim 1, further comprising:
a neural network output is generated for the neural network input according to the approximate score distribution.
6. The method of claim 5, wherein the neural network output identifies k outputs from the vocabulary of outputs having the highest scores in the approximate score distribution, wherein k is an integer greater than or equal to 1.
7. The method according to any one of claim 1 to 6,
wherein the screening model comprises a respective parameter vector for each of the plurality of clusters and a candidate appropriate output subset for each cluster, an
Wherein processing the context vector using a screening model configured to predict an appropriate subset of the vocabulary for the context input comprises:
using the parameter vector to cluster the context vector into a particular cluster of the plurality of clusters; and
the appropriate subset of the context vectors is predicted as the candidate appropriate output subset of the particular cluster.
8. The method of claim 7, wherein clustering the context vector comprises:
for each cluster, determining an inner product between the context vector and the parameter vector of the cluster; and
the context vectors are clustered into the clusters with the highest inner product.
9. A method for processing neural network inputs, comprising training a screening model of the method of claim 7 or claim 8 to learn a subset of candidate suitable outputs and cluster parameter vectors for clustering, the method comprising:
obtaining a plurality of training context vectors;
determining a ground truth marker vector for each training context vector by processing the ground truth marker vector through a softmax layer with the maintained weights;
initializing, for each cluster, the cluster parameter vector and the candidate appropriate output subset; and
the following operations are repeatedly performed:
performing an iteration of a first optimization process for updating the cluster parameter vector while keeping the candidate proper subset of outputs fixed; and
an iteration of a second optimization process is performed for updating the candidate appropriate subset of outputs while keeping the cluster parameter vector fixed.
10. The method of claim 9, wherein the first optimization procedure employs gummel skills.
11. The method of claim 9, wherein the second optimization process includes using a greedy approach to solve a knapsack problem.
12. The method of any of claims 9 to 11, wherein performing iterations of the first and second optimization processes minimizes the following objective function:
wherein v denotes the cluster parameter vector, c denotes the candidate proper output subset, the minimizing operation is performed across the values of the cluster parameter vector v and the candidate proper output subset c for each cluster, r denotes the total number of clusters, N denotes the total number of training context vectors, h i Represents the ith training context vector, y is A value representing the output s in the ground truth flag vector of the ith training context vector, L representing the total number of possible outputs in the vocabulary, λ representing a predetermined weight value, B being a constant value representing the number of desired average outputs in each cluster,representing the average number of outputs in each cluster given the current value of the candidate cluster set for each cluster, +.>An output representing the value of 1 assigned in a differentiable single-heat representation of argmax using the current value of the cluster parameter vector and the candidate appropriate output subset for the score generated by the approximate softmax probability distribution, and c ds 1 when the output s is in the proper subset of outputs of cluster d and zero when the output s is not in the proper subset of outputs of cluster d.
13. A system for processing neural network inputs, the system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the operations of the method of any one of claims 1 to 12.
14. One or more computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the method of any one of claims 1 to 12.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862737909P | 2018-09-27 | 2018-09-27 | |
US62/737,909 | 2018-09-27 |
Publications (2)
Publication Number | Publication Date |
---|---|
CN111008689A CN111008689A (en) | 2020-04-14 |
CN111008689B true CN111008689B (en) | 2024-01-26 |
Family
ID=69945951
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201910924950.6A Active CN111008689B (en) | 2018-09-27 | 2019-09-27 | Using SOFTMAX approximation to reduce neural network inference time |
Country Status (2)
Country | Link |
---|---|
US (1) | US10671909B2 (en) |
CN (1) | CN111008689B (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3882823A1 (en) * | 2020-03-17 | 2021-09-22 | Samsung Electronics Co., Ltd. | Method and apparatus with softmax approximation |
EP3907663B1 (en) * | 2020-05-06 | 2024-02-21 | Robert Bosch GmbH | Predicting a state of a computer-controlled entity |
CN111985484A (en) * | 2020-08-11 | 2020-11-24 | 云南电网有限责任公司电力科学研究院 | CNN-LSTM-based temperature instrument digital identification method and device |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2017228224A (en) * | 2016-06-24 | 2017-12-28 | キヤノン株式会社 | Information processing device, information processing method, and program |
CN108205699A (en) * | 2016-12-20 | 2018-06-26 | 谷歌有限责任公司 | Generation is used for the output of neural network output layer |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3218854B1 (en) * | 2014-11-14 | 2021-01-06 | Google LLC | Generating natural language descriptions of images |
US10860887B2 (en) * | 2015-11-16 | 2020-12-08 | Samsung Electronics Co., Ltd. | Method and apparatus for recognizing object, and method and apparatus for training recognition model |
GB201619724D0 (en) * | 2016-11-22 | 2017-01-04 | Microsoft Technology Licensing Llc | Trained data input system |
US10380259B2 (en) * | 2017-05-22 | 2019-08-13 | International Business Machines Corporation | Deep embedding for natural language content based on semantic dependencies |
-
2019
- 2019-09-27 US US16/586,702 patent/US10671909B2/en active Active
- 2019-09-27 CN CN201910924950.6A patent/CN111008689B/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2017228224A (en) * | 2016-06-24 | 2017-12-28 | キヤノン株式会社 | Information processing device, information processing method, and program |
CN108205699A (en) * | 2016-12-20 | 2018-06-26 | 谷歌有限责任公司 | Generation is used for the output of neural network output layer |
Also Published As
Publication number | Publication date |
---|---|
US10671909B2 (en) | 2020-06-02 |
CN111008689A (en) | 2020-04-14 |
US20200104686A1 (en) | 2020-04-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11790214B2 (en) | Mixture of experts neural networks | |
US11934956B2 (en) | Regularizing machine learning models | |
US20210150355A1 (en) | Training machine learning models using task selection policies to increase learning progress | |
US11714993B2 (en) | Classifying input examples using a comparison set | |
CN111602148B (en) | Regularized neural network architecture search | |
US11928601B2 (en) | Neural network compression | |
US11544536B2 (en) | Hybrid neural architecture search | |
EP3459021B1 (en) | Training neural networks using synthetic gradients | |
US20210004677A1 (en) | Data compression using jointly trained encoder, decoder, and prior neural networks | |
US10860928B2 (en) | Generating output data items using template data items | |
US20190164084A1 (en) | Method of and system for generating prediction quality parameter for a prediction model executed in a machine learning algorithm | |
WO2019099193A1 (en) | Learning neural network structure | |
CN111008689B (en) | Using SOFTMAX approximation to reduce neural network inference time | |
CN110476173B (en) | Hierarchical device placement with reinforcement learning | |
US20200364617A1 (en) | Training machine learning models using teacher annealing | |
US20200410365A1 (en) | Unsupervised neural network training using learned optimizers | |
US20220383119A1 (en) | Granular neural network architecture search over low-level primitives | |
US20220188636A1 (en) | Meta pseudo-labels | |
CN111667069A (en) | Pre-training model compression method and device and electronic equipment | |
US20230107409A1 (en) | Ensembling mixture-of-experts neural networks | |
US20220253694A1 (en) | Training neural networks with reinitialization | |
US20220108174A1 (en) | Training neural networks using auxiliary task update decomposition | |
US20230206030A1 (en) | Hyperparameter neural network ensembles | |
US20230124177A1 (en) | System and method for training a sparse neural network whilst maintaining sparsity | |
JP2024519265A (en) | Neural network with feedforward spatial transformation units |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |