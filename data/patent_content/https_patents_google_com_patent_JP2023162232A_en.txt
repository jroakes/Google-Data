JP2023162232A - Intelligent systems and methods for visual search queries - Google Patents
Intelligent systems and methods for visual search queries Download PDFInfo
- Publication number
- JP2023162232A JP2023162232A JP2023129236A JP2023129236A JP2023162232A JP 2023162232 A JP2023162232 A JP 2023162232A JP 2023129236 A JP2023129236 A JP 2023129236A JP 2023129236 A JP2023129236 A JP 2023129236A JP 2023162232 A JP2023162232 A JP 2023162232A
- Authority
- JP
- Japan
- Prior art keywords
- user
- visual
- query
- image
- search query
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000000007 visual effect Effects 0.000 title claims abstract description 441
- 238000000034 method Methods 0.000 title claims abstract description 106
- 230000004044 response Effects 0.000 claims description 29
- 230000009471 action Effects 0.000 claims description 10
- 239000000470 constituent Substances 0.000 claims description 9
- 238000012552 review Methods 0.000 claims description 5
- 230000002123 temporal effect Effects 0.000 claims description 2
- 238000012545 processing Methods 0.000 abstract description 47
- 230000008569 process Effects 0.000 abstract description 17
- 238000001514 detection method Methods 0.000 abstract description 6
- 238000012015 optical character recognition Methods 0.000 abstract description 6
- 235000013339 cereals Nutrition 0.000 description 36
- 238000010586 diagram Methods 0.000 description 16
- 238000003708 edge detection Methods 0.000 description 11
- 235000013325 dietary fiber Nutrition 0.000 description 10
- 239000011159 matrix material Substances 0.000 description 8
- 238000004891 communication Methods 0.000 description 5
- 230000006870 function Effects 0.000 description 5
- 241001465754 Metazoa Species 0.000 description 4
- 230000004048 modification Effects 0.000 description 4
- 238000012986 modification Methods 0.000 description 4
- 230000008685 targeting Effects 0.000 description 4
- 230000003190 augmentative effect Effects 0.000 description 3
- 238000013500 data storage Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000001755 vocal effect Effects 0.000 description 3
- 241001582718 Xanthorhoe munitata Species 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 2
- 239000003086 colorant Substances 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 238000012549 training Methods 0.000 description 2
- 241000282326 Felis catus Species 0.000 description 1
- 235000008694 Humulus lupulus Nutrition 0.000 description 1
- 238000007476 Maximum Likelihood Methods 0.000 description 1
- 241000282376 Panthera tigris Species 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 238000010205 computational analysis Methods 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 238000001914 filtration Methods 0.000 description 1
- 239000012530 fluid Substances 0.000 description 1
- 235000013305 food Nutrition 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000010422 painting Methods 0.000 description 1
- 238000003909 pattern recognition Methods 0.000 description 1
- 230000026676 system process Effects 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/532—Query formulation, e.g. graphical querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/535—Filtering based on additional data, e.g. user or group profiles
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/55—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5846—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using extracted text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5854—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using shape and object relationship
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/901—Indexing; Data structures therefor; Storage structures
- G06F16/9024—Graphs; Linked lists
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/907—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
Abstract
Description
本開示は、概して、視覚検索クエリを処理するためのシステムおよび方法に関する。より詳細には、本開示は、視覚クエリ内に含まれた画像内のオブジェクトを検出および認識し、より個人化された(personalized)かつ/またはインテリジェントな検索結果を提供するために使用され得る、コンピュータ視覚検索システムに関する。 TECHNICAL FIELD This disclosure generally relates to systems and methods for processing visual search queries. More particularly, the present disclosure can be used to detect and recognize objects in images included within a visual query and provide more personalized and/or intelligent search results. Concerning computer visual search systems.
テキストベースまたは用語ベースの検索は、ユーザが語または句を検索エンジン内に入力し、様々な結果を受信するプロセスである。用語ベースのクエリは、ユーザが語、句、および/または他の用語の形態で検索用語を明示的に提供することを必要とする。したがって、用語ベースのクエリは、テキストベースの入力モダリティによって本質的に限定され、ユーザが像の視覚的特性に基づいて検索することを可能にしない。 A text-based or term-based search is a process in which a user enters words or phrases into a search engine and receives various results. Term-based queries require users to explicitly provide search terms in the form of words, phrases, and/or other terms. Therefore, term-based queries are inherently limited by text-based input modalities and do not allow users to search based on visual characteristics of images.
代替として、視覚検索クエリシステムは、1つまたは複数の画像を含む視覚クエリに応じて、ユーザに検索結果を提供し得る。コンピュータ視覚解析技法は、画像内のオブジェクトを検出および認識するために使用され得る。たとえば、光学文字認識(OCR)技法は、画像内のテキストを認識するために使用され得、かつ/またはエッジ検出技法または他のオブジェクト検出技法(たとえば、機械学習ベースの手法)は、画像内のオブジェクト(たとえば、製品、ランドマーク、動物など)を検出するために使用され得る。検出されたオブジェクトに関するコンテンツがユーザ(たとえば、オブジェクトが検出された画像をキャプチャしたユーザ、またはそうでなければ視覚クエリを提出した、もしく視覚クエリに関連するユーザ)に提供され得る。 Alternatively, a visual search query system may provide search results to a user in response to a visual query that includes one or more images. Computer vision analysis techniques may be used to detect and recognize objects in images. For example, optical character recognition (OCR) techniques may be used to recognize text within an image, and/or edge detection techniques or other object detection techniques (e.g., machine learning-based techniques) may be used to recognize text within an image. It can be used to detect objects (eg, products, landmarks, animals, etc.). Content regarding the detected object may be provided to a user (eg, a user who captured an image in which the object was detected or a user who otherwise submitted or is associated with a visual query).
しかしながら、いくつかの既存の視覚クエリシステムは、いくつかの欠点を有する。一例として、現在の視覚検索クエリシステムおよび方法は、配色、形状など、明示的な視覚的特性に関して視覚クエリに関係し得るだけの結果、または視覚クエリの画像と同じ品目/オブジェクトを示す結果をユーザに提供することができるにすぎない。言い方を変えれば、いくつかの既存の視覚クエリシステムは、クエリ画像に対するいくつかの同様の視覚的特性を含む他の画像の識別のみに焦点を当てており、これはユーザの真の検索意図を反映することができない可能性がある。 However, some existing visual query systems have some drawbacks. As an example, current visual search query systems and methods provide users with results that may only be related to the visual query in terms of explicit visual characteristics, such as color scheme, shape, etc., or results that indicate the same item/object as the image in the visual query. It can only be provided to In other words, some existing visual query systems only focus on identifying other images that contain some similar visual characteristics to the query image, which can be used to determine the user's true search intent. It may not be possible to reflect.
したがって、視覚クエリをよりインテリジェントに処理して、改善された検索結果をユーザに提供し得るシステムが望ましいことになる。 Accordingly, a system that can more intelligently process visual queries and provide improved search results to users would be desirable.
本開示の実施形態の態様および利点は、以下の説明に部分的に記載されることになるか、もしくはその説明から学ぶことが可能であるか、またはこれらの実施形態の実践を通して学ぶことが可能である。 Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description or can be learned from the description or through practice of these embodiments. It is.
本開示の1つの例示的な態様は、個人化された視覚検索クエリ結果通知を像上にオーバーレイされたユーザインターフェース内に提供するためのコンピュータ実装方法を対象とする。この方法は、1つまたは複数のコンピューティングデバイスを含むコンピューティングシステムが、ユーザに関連する視覚検索クエリを取得するステップであって、視覚検索クエリが画像を含む、取得するステップを含む。この方法は、コンピューティングシステムが、視覚検索クエリに対する複数の候補検索結果を識別するステップであって、各候補検索結果が、画像の特定のサブ部分に関連付けられ、複数の候補視覚結果通知が、複数の候補検索結果にそれぞれ関連付けられる、識別するステップを含む。この方法は、コンピューティングシステムが、ユーザに関連し、かつユーザの視覚的関心を記述する、ユーザ固有のユーザの関心データにアクセスするステップを含む。この方法は、コンピューティングシステムが、ユーザに関連するユーザ固有のユーザの関心データに対する複数の候補検索結果の比較に少なくとも部分的に基づいて、複数の候補検索結果のランク付けを生成するステップを含む。この方法は、コンピューティングシステムが、ランク付けに少なくとも部分的に基づいて、少なくとも1つの選択された検索結果として、複数の候補検索結果のうちの少なくとも1つを選択するステップを含む。この方法は、コンピューティングシステムが、選択された検索結果に関連する画像の特定のサブ部分上にオーバーレイするために、少なくとも1つの選択された検索結果にそれぞれ関連する少なくとも1つの選択された視覚結果通知を提供するステップを含む。 One example aspect of the present disclosure is directed to a computer-implemented method for providing personalized visual search query result notifications in a user interface overlaid on an image. The method includes the step of a computing system including one or more computing devices obtaining a visual search query associated with a user, the visual search query including an image. The method includes the steps of: a computing system identifying a plurality of candidate search results for a visual search query, each candidate search result being associated with a particular sub-portion of an image; and identifying each associated with a plurality of candidate search results. The method includes the step of the computing system accessing user interest data specific to the user that is related to the user and describes the user's visual interests. The method includes the computing system generating a ranking of the plurality of candidate search results based at least in part on a comparison of the plurality of candidate search results to user-specific user interest data associated with the user. . The method includes the computing system selecting at least one of the plurality of candidate search results as the at least one selected search result based at least in part on the ranking. The method includes determining at least one selected visual result, each related to the at least one selected search result, for overlaying a particular sub-portion of an image related to the selected search result. and providing a notification.
本開示の別の例示的な態様は、視覚検索クエリに応じて、複数のカノニカル項目に関するコンテンツを戻すコンピューティングシステムを対象とする。コンピューティングシステムは、1つまたは複数のプロセッサと、1つまたは複数のプロセッサによって実行されると、コンピューティングシステムに動作を実行させる命令を記憶した、1つまたは複数の非一時的コンピュータ可読媒体とを含む。これらの動作は、視覚検索クエリを取得することであって、視覚検索クエリが、オブジェクトを示す画像を含む、取得することを含む。これらの動作は、複数の異なる項目を記述するグラフにアクセスすることであって、コンテンツのそれぞれのセットが、複数の異なる項目の各々に関連付けられる、アクセスすることを含む。これらの動作は、視覚検索クエリに基づいて、画像が示すオブジェクトに関するグラフから複数の選択された項目を選択することを含む。これらの動作は、視覚検索クエリに応じて、コンテンツの組み合わされたセットを検索結果として戻すことであって、コンテンツの組み合わされたセットが、複数の選択された項目の各々に関連するコンテンツのそれぞれのセットの少なくとも一部分を含む、戻すことを含む。 Another example aspect of the present disclosure is directed to a computing system that returns content regarding a plurality of canonical items in response to a visual search query. A computing system includes one or more processors and one or more non-transitory computer-readable media that store instructions that, when executed by the one or more processors, cause the computing system to perform operations. including. These operations include obtaining a visual search query, the visual search query including an image showing the object. These operations include accessing a graph describing a plurality of different items, each set of content being associated with each of the plurality of different items. These operations include selecting a plurality of selected items from a graph related to the object represented by the image based on a visual search query. These operations are to return a combined set of content as search results in response to a visual search query, where the combined set of content includes each of the pieces of content associated with each of the multiple selected items. containing at least a portion of the set.
本開示の別の例示的な態様は、オブジェクト固有の視覚クエリとカテゴリー別(categorical)視覚クエリとの間を明確化する(disambiguate)ためのコンピューティングシステムを対象とする。コンピューティングシステムは、1つまたは複数のプロセッサと、1つまたは複数のプロセッサによって実行されると、コンピューティングシステムに動作を実行させる命令を記憶する1つまたは複数の非一時的コンピュータ可読媒体とを含む。これらの動作は、視覚検索クエリを取得することであって、視覚検索クエリが、1つまたは複数のオブジェクトを示す画像を含む、取得することを含む。これらの動作は、視覚検索クエリ内に含まれた画像の1つまたは複数の構成特性を識別することを含む。これらの動作は、視覚検索クエリ内に含まれた画像の1つまたは複数の構成特性に少なくとも部分的に基づいて、視覚検索クエリが、視覚検索クエリ内に含まれた画像内で識別された1つまたは複数のオブジェクトに特に関係するオブジェクト固有のクエリを含むかどうか、または視覚検索クエリが、視覚検索クエリ内に含まれた画像内で識別された1つまたは複数のオブジェクトの一般カテゴリーに関係するカテゴリー別クエリを含むかどうかを判定することを含む。これらの動作は、視覚検索クエリがオブジェクト固有のクエリを含むと判定されるとき、視覚検索クエリ内に含まれた画像内で識別された1つまたは複数のオブジェクトに特に関係する、1つまたは複数のオブジェクト固有の検索結果を戻すことを含む。これらの動作は、視覚検索クエリがカテゴリー別クエリを含むと判定されるとき、視覚検索クエリ内に含まれた画像内で識別された1つまたは複数のオブジェクトの一般カテゴリーに関係する、1つまたは複数のカテゴリー別検索結果を戻すことを含む。 Another example aspect of the present disclosure is directed to a computing system for disambiguating between object-specific and categorical visual queries. A computing system includes one or more processors and one or more non-transitory computer-readable media that store instructions that, when executed by the one or more processors, cause the computing system to perform operations. include. These operations include obtaining a visual search query, the visual search query including an image depicting one or more objects. These operations include identifying one or more constituent characteristics of images included within the visual search query. These behaviors are based at least in part on one or more constituent characteristics of the images included within the visual search query, such that the visual search query is identified within the images included within the visual search query. whether the visual search query involves a general category of one or more objects identified within the images contained within the visual search query; Includes determining whether a categorical query is included. These actions may be performed when a visual search query is determined to include an object-specific query. including returning object-specific search results for. These actions may include one or more general categories of objects identified within images included within the visual search query when the visual search query is determined to include a categorical query. Including returning search results by multiple categories.
本開示の別の例示的な態様は、複数の構成されたエンティティに関するコンテンツを視覚検索クエリに戻すためのコンピュータ実装方法を対象とする。この方法は、視覚検索クエリを取得するステップであって、視覚検索クエリが、第1のエンティティを示す画像を含む、取得するステップを含む。この方法は、1つまたは複数のコンテキスト信号に少なくとも部分的に基づいて、視覚検索クエリに関連する1つまたは複数の追加のエンティティを識別するステップを含む。この方法は、第1のエンティティと1つまたは複数の追加のエンティティの組合せに関するコンテンツに対して構成されたクエリを判定するステップを含む。この方法は、視覚検索クエリに応じて、コンテンツのセットを戻すステップであって、コンテンツのセットが、構成されたクエリに応じ、かつ第1のエンティティと1つまたは複数の追加のエンティティの組合せに関する、少なくとも1つのコンテンツ項目を含む、戻すステップを含む。 Another example aspect of the present disclosure is directed to a computer-implemented method for returning content about a plurality of configured entities to a visual search query. The method includes obtaining a visual search query, the visual search query including an image depicting a first entity. The method includes identifying one or more additional entities related to the visual search query based at least in part on the one or more context signals. The method includes determining a query configured for content regarding a combination of the first entity and one or more additional entities. The method includes returning a set of content in response to a visual search query, the set of content being responsive to the configured query and relating to a combination of the first entity and one or more additional entities. , including at least one content item.
本開示の他の態様は、様々なシステム、装置、非一時的コンピュータ可読媒体、ユーザインターフェース、および電子デバイスを対象とする。 Other aspects of the disclosure are directed to various systems, apparatus, non-transitory computer-readable media, user interfaces, and electronic devices.
本開示の様々な実施形態のこれらのおよび他の特徴、態様、および利点は、以下の説明および添付の請求項を参照するとより良く理解されるであろう。本明細書に組み込まれ、その一部分を構成する、添付の図面は、本開示の例示的な実施形態を示し、説明とともに、関係する原理の説明に役立つ。 These and other features, aspects, and advantages of various embodiments of the disclosure will be better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the disclosure and, together with the description, serve to explain the principles involved.
当業者を対象とする実施形態の詳細な考察が、添付の図面を参照する明細書に記載されている。 A detailed discussion of embodiments intended for those skilled in the art is set forth in the specification with reference to the accompanying drawings.
複数の図にわたって繰り返される参照番号は、様々な実装形態における同じ特徴を識別することを意図する。 Repeated reference numbers across multiple figures are intended to identify the same features in different implementations.
概要
概して、本開示は、視覚クエリ内のまたはそれに関するオブジェクトを検出および認識し、視覚クエリに応じて、より個人化されたかつ/またはインテリジェントな検索結果を(たとえば、視覚クエリを拡張するオーバーレイの形で)提供するために使用され得るコンピュータ実装視覚検索システムを対象とする。たとえば、ユーザは、1つまたは複数の画像を含む視覚クエリを提出し得る。テキスト(たとえば、その画像、周囲の画像の中のなど)を認識するために、光学文字認識(OCR)技法など、様々な処理技法が使用可能であり、かつ/または視覚クエリ内のまたはそれに関するオブジェクト(たとえば、製品、ランドマーク、動物、人間など)を検出するために、様々なオブジェクト検出技法(たとえば、機械学習済みオブジェクト検出モデルなど)が使用可能である。検出されたテキストまたはオブジェクトに関するコンテンツが識別され、検索結果としてユーザに潜在的に提供され得る。したがって、本開示の態様は、視覚検索システムが、視覚クエリをよりインテリジェントに処理して、より個人化された検索結果を含めて、改善された検索結果を提供すること、および/または視覚クエリおよび/またはユーザの検索意図の暗示的な特性を明らかにするためにコンテキスト信号を考慮することを可能にする。
Overview In general, the present disclosure provides the ability to detect and recognize objects in or about a visual query and provide more personalized and/or intelligent search results in response to the visual query (e.g., overlays that augment the visual query). A computer-implemented visual search system that can be used to provide information (in the form of a computer). For example, a user may submit a visual query that includes one or more images. Various processing techniques can be used, such as optical character recognition (OCR) techniques, to recognize text (e.g., in the image, surrounding images, etc.) and/or in or relating to a visual query. Various object detection techniques (eg, machine learned object detection models, etc.) can be used to detect objects (eg, products, landmarks, animals, people, etc.). Content related to the detected text or objects may be identified and potentially provided to the user as a search result. Accordingly, aspects of the present disclosure provide for visual search systems to more intelligently process visual queries to provide improved search results, including more personalized search results, and/or to provide improved search results, including more personalized search results. / or allow contextual signals to be taken into account to reveal implicit characteristics of the user's search intent.
本開示の例示的な態様は、視覚クエリに応じて、よりインテリジェントな検索結果を提供する。視覚クエリは、1つまたは複数の画像を含み得る。たとえば、視覚クエリ内に含まれた画像は、同時にキャプチャされた像であってよく、または前から存在していた画像であってよい。一例では、視覚クエリは、単一の画像を含み得る。別の例では、視覚クエリは、およそ3秒のビデオキャプチャからの10個の画像フレームを含み得る。さらに別の例では、視覚クエリは、たとえば、ユーザのフォトライブラリ内に含まれたすべての画像など、画像のコーパスを含み得る。たとえば、そのようなライブラリは、ユーザが最近キャプチャした動物園における動物の画像、少し前に(たとえば、2か月前に)ユーザがキャプチャした猫の画像、既存のソースから(たとえば、ウェブサイトまたはスクリーンキャプチャから)ユーザがライブラリ内に保存したトラの画像を含み得る。これらの画像は、ユーザに対する親和性の高い画像のクラスタを表し、ユーザが動物のようなものに「視覚的関心」を有し得るという抽象的なアイデアを(たとえば、グラフによって)具現し得る。いずれの所与のユーザも、各々が語によって十分キャプチャされない関心を表す、多くのそのようなノードクラスタを有し得る。 Example aspects of the present disclosure provide more intelligent search results in response to visual queries. A visual query may include one or more images. For example, the images included in the visual query may be images captured at the same time, or may be preexisting images. In one example, a visual query may include a single image. In another example, a visual query may include 10 image frames from an approximately 3 second video capture. In yet another example, a visual query may include a corpus of images, such as, for example, all images contained within a user's photo library. For example, such a library could include images of animals at the zoo that the user recently captured, images of cats that the user captured a while ago (e.g., two months ago), images from existing sources (e.g., from a website or screen may include images of tigers that the user has saved in the library (from captures). These images represent clusters of images that are highly relevant to the user and may embody (eg, graphically) the abstract idea that the user may have a "visual interest" in things such as animals. Any given user may have many such clusters of nodes, each representing an interest not well captured by words.
1つの例示的な態様によれば、視覚検索システムは、ユーザ中心の視覚的関心グラフを構築し活用して、より個人化された検索結果を提供し得る。1つの例示的な使用では、視覚検索システムは、ユーザの関心のグラフを使用して、視覚的発見警告、通知、または他の機会をフィルタリングし得る。したがって、ユーザの関心に基づく検索結果の個人化は、検索結果がクエリ画像上の拡張オーバーレイ内に視覚結果通知(たとえば、場合によっては、「グリーム(gleams)」と呼ばれることがある)として提示される例示的な実施形態において特に有利であり得る。 According to one example aspect, a visual search system may construct and utilize a user-centered visual interest graph to provide more personalized search results. In one example use, a visual search system may use a graph of a user's interests to filter visual discovery alerts, notifications, or other opportunities. Personalization of search results based on a user's interests can therefore be achieved by presenting search results as visual result notifications (e.g., sometimes referred to as "gleams") within an enhanced overlay on the query image. may be particularly advantageous in exemplary embodiments.
より詳細には、いくつかの実装形態では、視覚検索システムは、視覚クエリ内に含まれた画像上のオーバーレイとして検索結果に対する視覚結果通知を提供するのに役立つ拡張オーバーレイユーザインターフェースを含み得るか、または提供し得る。たとえば、視覚結果通知は、検索結果に関連する画像の部分に対応するロケーションにおいて提供され得る(たとえば、視覚結果通知は、対応する検索結果に関連するオブジェクトの「上部に」示され得る)。したがって、視覚検索クエリに応じて、複数の候補検索結果が識別され得、複数の候補視覚結果通知は、それぞれ、複数の候補検索結果に関連付けられ得る。しかしながら、基礎をなす視覚検索システムが特に強力かつ広範である場合、圧倒的な数の候補視覚結果通知が利用可能であり得、その結果、すべての候補視覚結果通知の提示は、混乱を引き起こすほどユーザインターフェースを混雑させることになるか、またはそうでなければ、基礎をなす画像を望ましくなく曖昧にさせることになる。したがって、本開示の一態様によれば、コンピュータ視覚検索システムは、ユーザ中心の視覚的関心グラフを構築し活用して、観測されたユーザの視覚的関心に基づいて、候補視覚結果通知をランク付け、選択、および/またはフィルタリングし、それにより、より直感的かつ合理化されたユーザ経験を提供し得る。 More particularly, in some implementations, the visual search system may include an enhanced overlay user interface that helps provide visual result notifications for search results as an overlay on images included within the visual query; or may be provided. For example, a visual result notification may be provided at a location corresponding to a portion of an image associated with a search result (eg, a visual result notification may be shown “on top” of an object associated with a corresponding search result). Thus, in response to a visual search query, multiple candidate search results may be identified, and multiple candidate visual result notifications may each be associated with the multiple candidate search results. However, if the underlying visual search system is particularly powerful and extensive, an overwhelming number of candidate visual result notifications may be available, such that the presentation of all candidate visual result notifications becomes confusingly This can clutter the user interface or otherwise undesirably obscure the underlying image. Accordingly, in accordance with one aspect of the present disclosure, a computer visual search system constructs and utilizes a user-centered visual interest graph to rank candidate visual result notifications based on observed user visual interests. , selection, and/or filtering, thereby providing a more intuitive and streamlined user experience.
いくつかの実装形態では、ユーザ固有の関心データ(たとえば、グラフを使用して表すことができる)は、ユーザが過去に関与した画像を解析することによって少なくとも部分的に、経時的にアグリゲートされ得る。言い方を変えれば、コンピューティングシステムは、ユーザが経時的に関与する画像を解析することによって、ユーザの視覚的関心の理解を試行し得る。ユーザが画像に関与するとき、ユーザが画像の一部の側面について関心を有すると推論され得る。したがって、そのような画像内に含まれるか、またはそうでなければ、それに関する項目(たとえば、オブジェクト、エンティティ、概念、製品など)は、ユーザ固有の関心データ(たとえば、グラフ)に追加され得るか、またはそうでなければ、その中で言及され得る。 In some implementations, user-specific interest data (e.g., which can be represented using a graph) is aggregated over time, at least in part, by analyzing images with which the user has engaged in the past. obtain. In other words, the computing system may attempt to understand the user's visual interests by analyzing images in which the user engages over time. When a user engages with an image, it can be inferred that the user is interested in some aspect of the image. Accordingly, items contained within or otherwise related to such images (e.g., objects, entities, concepts, products, etc.) may be added to user-specific interest data (e.g., graphs). , or otherwise may be mentioned therein.
一例として、ユーザが関与する画像は、ユーザがキャプチャした写真、ユーザがキャプチャしたスクリーンショット、またはユーザが閲覧したウェブベースまたはアプリケーションベースのコンテンツ内に含まれた画像を含み得る。別の潜在的に重複する例では、ユーザが関与する画像は、ユーザが画像に対してアクションが実行されることを要求することによってアクティブに関与した、アクティブに関与された画像を含み得る。たとえば、要求されるアクションは、画像に対する視覚検索を実行すること、または画像がユーザの視覚的関心を含むことをユーザが明示的にマーキングすることを含み得る。別の例として、ユーザが関与する画像は、ユーザに提示されたが、ユーザが特に関与しなかった、受動的に観測された画像を含み得る。視覚的関心はまた、ユーザが入力したテキストコンテンツ(たとえば、テキストまたは用語ベースのクエリ)から推論され得る。 As an example, a user-involved image may include a photo captured by the user, a screenshot captured by the user, or an image included within web-based or application-based content viewed by the user. In another potentially overlapping example, user-involved images may include actively engaged images in which the user actively engaged with by requesting that an action be performed on the image. For example, the requested action may include the user performing a visual search for the image, or explicitly marking the image as containing the user's visual interest. As another example, a user-involved image may include a passively observed image that is presented to the user but with no specific involvement by the user. Visual interest may also be inferred from user-entered textual content (eg, text or term-based queries).
いくつかの実装形態では、ユーザ中心の視覚的関心グラフ内で反映されるユーザの関心は、的確な、カテゴリー別、または抽象的な項目(たとえば、エンティティ)であり得る。たとえば、的確な関心は、特定の項目(たとえば、特定の芸術作品)に対応し得、カテゴリー別関心は、項目のカテゴリー(たとえば、アールヌーボー絵画)であり得、抽象的関心は、カテゴリー別にまたはテキストを用いてキャプチャすることが困難な関心に対応し得る(たとえば、「Gustav Klimtによる「The Kiss」に視覚的に類似するように見える芸術作品」)。 In some implementations, the user interests reflected within the user-centric visual interest graph may be precise, categorical, or abstract items (eg, entities). For example, a precise interest may correspond to a specific item (e.g., a particular work of art), a categorical interest may be a category of items (e.g., Art Nouveau paintings), and an abstract interest may correspond to a categorical or May address interests that are difficult to capture using text (e.g., ``A work of art that appears visually similar to ``The Kiss'' by Gustav Klimt'').
関心は、的確な関心項目、カテゴリー別関心項目、または抽象的関心項目にわたって可変加重関心レベルをオーバーレイまたは定義すること(たとえば、次いで、周期的に更新すること)によって、ユーザ中心の視覚的関心グラフ内に示され得る。様々な項目に対するユーザの関心は、項目に対するかつ/または関係する項目(たとえば、グラフ内の項目に接続された項目、またはそこからn個のホップ内の項目)に対する可変加重関心レベルを評価することによって推論または判定され得る。 Interests can be created in a user-centric visual interest graph by overlaying or defining (e.g., then periodically updating) variable weighted interest levels across precise, categorical, or abstract interests. can be shown within. A user's interest in various items may be evaluated by evaluating variable weighted interest levels for the item and/or for related items (e.g., items connected to the item in the graph, or items within n hops from it) can be inferred or determined by
いくつかの実装形態では、識別された視覚的関心に割り当てられた可変加重関心バイアス(variable weighted interest bias)は、ユーザ固有の関心データが表された関心の時間枠に少なくとも部分的に基づくように、経時的に減衰する。たとえば、ユーザは、ある持続時間にわたって特定の話題に強い関心を表し、次いで、その後、関心をまったく表さないことがある(たとえば、ユーザは、一年間、特定のバンドに強い関心を表すことがある)。ユーザの関心データは、関心に対するユーザの変化を反映するように経時的に減衰し得、十分な減衰が生じた場合、結果として、視覚検索システムは、ユーザがもはや関心をもたない話題に関するクエリ結果をユーザに引き続き示さなくてよい。 In some implementations, the variable weighted interest bias assigned to the identified visual interest is based at least in part on the time frame of interest in which the user-specific interest data was represented. , decays over time. For example, a user may express a strong interest in a particular topic for a period of time, and then express no interest at all thereafter (e.g., a user may express a strong interest in a particular band for a year). be). User interest data may decay over time to reflect the user's changes in interest, and if sufficient decay occurs, the visual search system may result in queries about topics in which the user is no longer interested. There is no need to continue showing the results to the user.
したがって、いくつかの実装形態では、視覚的関心グラフは、(たとえば、ユーザが履歴的に見た多くの画像に基づく)ユーザの関心/個人化の記述的収集物であり得る。グラフは、多くの画像の属性を履歴的に解析し、その情報を使用して、他の関心画像(たとえば、ニュース記事の収集物など、概して関連するコンテンツ)を見出すことによって構築され得る。 Thus, in some implementations, the visual interest graph may be a descriptive collection of the user's interests/personalization (e.g., based on many images the user has viewed historically). A graph may be constructed by historically analyzing the attributes of many images and using that information to find other images of interest (eg, generally related content, such as a collection of news articles).
視覚検索システムは、ユーザ固有のユーザの関心データを使用して、ユーザに関連するユーザ固有のユーザの関心データに対する複数の候補検索結果の比較に少なくとも部分的に基づいて、複数の候補検索結果のランク付けを生成し得る。たとえば、候補検索結果に関連する初期検索スコアを修正または再重み付けするために、項目に対する重みが加えられてよい。 The visual search system uses the user-specific user interest data to select a plurality of candidate search results based at least in part on a comparison of the plurality of candidate search results to the user-specific user interest data related to the user. A ranking may be generated. For example, weights may be added to items to modify or reweight initial search scores associated with candidate search results.
検索システムは、ランク付けに少なくとも部分的に基づいて、少なくとも1つの選択された検索結果として複数の候補検索結果のうちの少なくとも1つを選択し、次いで、選択された検索結果に関連する画像の特定のサブ部分上にオーバーレイするために、少なくとも1つの選択された検索結果にそれぞれ関連する少なくとも1つの選択された視覚結果通知を提供し得る。そのような様式で、ユーザの関心は、個人化された検索結果を提供し、ユーザインターフェース内のクラッターを低減するために使用され得る。 The search system selects at least one of the plurality of candidate search results as the at least one selected search result based at least in part on the ranking of the images associated with the selected search result. At least one selected visual result notification, each associated with at least one selected search result, may be provided for overlaying on a particular sub-portion. In such a manner, user interests may be used to provide personalized search results and reduce clutter within the user interface.
別の例では、ユーザ中心の視覚的関心グラフは、視覚的情報および関心に基づいて、ユーザ固有のフィードをキュレートするために使用され得る。具体的には、フィード内で、単一の特定の画像に基づかずに、個人化されたコンテンツのセットがユーザに表示され得る。むしろ、前の画像の収集物の解析は、たとえば、画像(および/または、示されたエンティティなど、画像メタデータ)をノードとして、次いで、これらのノード同士の間の接続を関心の強度を判定するエッジとして用いて、上記で説明したようにグラフを確立し得る。新しい視覚的メディア項目(たとえば、画像またはビデオ)はまた、その場合、そのグラフに鑑みて、ユーザの関心に対して関連する「強度」を有し得る。この新しいメディアは、次いで、前のクエリなしに(たとえば、ブラウザアプリケーション内で新しいタブを開くなど、一定のコンテキスト内でユーザに提供されるフィードの一部として)ユーザにプロアクティブに示唆され得る。 In another example, a user-centric visual interest graph may be used to curate user-specific feeds based on visual information and interests. Specifically, within a feed, a personalized set of content may be displayed to the user, not based on a single particular image. Rather, the analysis of a collection of previous images considers the image (and/or image metadata, such as the indicated entities) as nodes, and then determines the strength of interest in the connections between these nodes. can be used as edges to establish a graph as explained above. A new visual media item (eg, an image or video) may also then have an associated "intensity" relative to the user's interest in view of its graph. This new media can then be proactively suggested to the user without prior query (eg, as part of a feed provided to the user within a certain context, such as opening a new tab within a browser application).
上記の説明に加えて、明細書で説明するシステム、プログラム、または特徴が、ユーザ情報(たとえば、ユーザのソーシャルネットワーク、ソーシャルアクションもしくは活動、専門、ユーザの選好、またはユーザの現在のロケーションに関する情報)の収集を可能にし得る場合と、ユーザにサーバから同意または通信が送られる場合の両方に対して、ユーザが選択することを可能にする制御がユーザに提供され得る。加えて、一定のデータは、個人的に識別可能な情報が除去されるように、そのデータが記憶または使用される前に、1つまたは複数の方法で処理され得る。たとえば、ユーザの識別情報は、ユーザに関する個人的に識別可能な情報を判定することができないように扱われることが可能であり、または(市、郵便番号、または州レベルでなど)ロケーション情報が取得される場合、ユーザの特定のロケーションを判定することができないように、ユーザの地理的ロケーションは一般化され得る。したがって、ユーザは、ユーザに関して何の情報が収集されるか、その情報がどのように使用されるか、また何の情報がユーザに提供されるか、に対して制御を有し得る。 In addition to the above description, the system, program, or feature described in the specification may include user information (e.g., information regarding a user's social networks, social actions or activities, professions, user preferences, or user's current location). Controls may be provided to the user that allow the user to select both when the collection of data may be enabled and when consents or communications are sent to the user from the server. Additionally, certain data may be processed in one or more ways before it is stored or used so that personally identifiable information is removed. For example, a user's identity may be treated such that no personally identifiable information about the user can be determined, or location information (such as at the city, zip code, or state level) may be obtained. If the user's geographic location is determined, the user's geographic location may be generalized such that the user's specific location cannot be determined. Thus, the user may have control over what information is collected about the user, how that information is used, and what information is provided to the user.
別の態様によれば、コンピュータ実装視覚検索システムは、視覚検索クエリに応じて、複数のカノニカル項目に関するコンテンツ(たとえば、ユーザ生成コンテンツ)の組み合わされたセットを識別して戻すことができる。具体的には、視覚検索クエリは検索入力のより表現的かつ流動的なモダリティを可能にするため、ユーザ意図の粒度とオブジェクトの両方を理解することは困難なタスクである。たとえば、ユーザが特定の映画を観たばかりであると想像されたい。その映画に関するコンテンツを受信することに対するユーザの関心を反映することになる視覚クエリとしてユーザが提出し得るかなりの量の視覚的コンテンツが存在する。それは、エンドクレジット、映画の物理的媒体(たとえば、ディスク)、パッケージカバー、映画に対する領収書、または翌日に映画のことをユーザに思い出させる予告編であり得る。したがって、世界規模の像を特定の項目にマッピングすることは、困難な問題である。反対に、ユーザのクエリの意図された粒度を理解することは困難である。たとえば、特定の映画に対するパッケージカバーを示す画像を含む視覚クエリは、その特定の映画に関するコンテンツ、その映画の中の役者に関するコンテンツ、映画の監督に関するコンテンツ、パッケージカバーを生成したアーティストに関するコンテンツ、その映画と同じジャンル(たとえば、ホラー)の映画に関するコンテンツ、またはその映画の特定のバージョン(たとえば、2020年「ディレクターズカット」リリースとその映画のすべてのバージョン、DVDバージョンとBlu-Rayバージョンなど)に特に関するコンテンツなど、さらにより特定のコンテンツに対する検索を意図し得る。 According to another aspect, a computer-implemented visual search system can identify and return a combined set of content (eg, user-generated content) for multiple canonical items in response to a visual search query. Specifically, since visual search queries allow for a more expressive and fluid modality of search input, understanding both the granularity and objectivity of user intent is a difficult task. For example, imagine that a user has just watched a particular movie. There is a significant amount of visual content that a user can submit as a visual query that will reflect the user's interest in receiving content about the movie. It can be the end credits, the physical medium of the movie (eg, a disc), the package cover, the receipt for the movie, or a trailer that reminds the user about the movie the next day. Therefore, mapping the global picture to specific items is a difficult problem. Conversely, it is difficult to understand the intended granularity of a user's query. For example, a visual query that includes an image showing a package cover for a particular movie might include content about that specific movie, content about the actors in the movie, content about the director of the movie, content about the artist who created the package cover, content about the movie content about movies in the same genre as (for example, horror) or specifically about a particular version of that movie (for example, the 2020 "Director's Cut" release and all versions of that movie, DVD and Blu-Ray versions, etc.) A search for even more specific content, such as content, may be intended.
本開示は、視覚検索クエリに応じて、複数のカノニカル項目に関するコンテンツの組み合わされたセットを戻すことを可能にすることによって、これらの課題を解決する。具体的には、オブジェクトを示す画像を含む視覚検索クエリに応じて、視覚検索システムは、複数の異なる項目を記述するグラフにアクセスすることができ、コンテンツ(たとえば、製品レビューなど、ユーザ生成コンテンツ)のそれぞれのセットは、複数の異なる項目の各々に関連付けられる。視覚検索システムは、視覚検索クエリに基づいて、画像が示したオブジェクトに関するグラフから複数の選択された項目を選択し、次いで、コンテンツの組み合わされたセットを検索結果として戻すことができ、ここで、コンテンツの組み合わされたセットは、各選択された項目に関連するコンテンツのそれぞれのセットの少なくとも一部分を含む。複数のカノニカル項目に関するコンテンツを戻すことによって、視覚検索システムは、視覚クエリ内で認識され得る特定のエンティティに固有のオーバーレイである結果を提供することを回避し得る。上記の例を続けると、いくつかの既存のシステムは映画の2020年「ディレクターズカット」リリースのみに関するコンテンツを戻すことができるが、提案するシステムは、2020年「ディレクターズカット」リリースに関するコンテンツだけでなく、映画の中の役者に関するコンテンツ、映画の監督に関するコンテンツ、パッケージカバーを生成したアーティストに関するコンテンツなど、他の関係エンティティに関するコンテンツも戻すことができる。 The present disclosure solves these challenges by enabling the return of a combined set of content for multiple canonical items in response to a visual search query. Specifically, in response to a visual search query that includes an image depicting an object, a visual search system can access graphs that describe multiple different items and content (e.g., user-generated content, such as product reviews) Each set of is associated with each of a plurality of different items. The visual search system can select a plurality of selected items from a graph about the object depicted by the image based on the visual search query and then return a combined set of content as search results, where: The combined set of content includes at least a portion of the respective set of content associated with each selected item. By returning content for multiple canonical items, visual search systems may avoid providing results that are overlays specific to particular entities that may be recognized within the visual query. Continuing with the example above, some existing systems can return content only about the 2020 "Director's Cut" release of a movie, but the proposed system returns content about not only the 2020 "Director's Cut" release of a movie. , content about other related entities, such as content about the actors in the movie, content about the director of the movie, content about the artist who generated the package cover, etc., can also be returned.
グラフからの項目の選択を可能にするために、様々な技法が使用され得る。「美的な2次」視覚検索(たとえば、抽象的な美的特性ではなく、特定の項目に関する情報を探索している検索)の有利な処理を可能にし得る一例では、グラフは、複数の異なる項目の階層表現であり得る。グラフから複数の選択された項目を選択することは、視覚検索クエリに基づいて、画像内に示されたオブジェクト(たとえば、画像内に示された特定の映画)に対応する、グラフ内の1次項目を識別することを含み得る。次に、視覚検索システムは、グラフの階層表現内の1次項目に関する、グラフ内の1つまたは複数の項目を識別し、複数の選択された項目として、1次項目および1つまたは複数の追加項目を選択し得る。追加の項目は、同じ階層レベル(たとえば、同じ監督による他の映画)であってよく、「より高い」階層レベル(たとえば、同じジャンルの他の映画)であってよく、かつ/または「より低い」階層レベル(たとえば、映画の2020年「ディレクターズカット」リリースおよび1990年の元の劇場版リリース)であってよい。 Various techniques may be used to enable selection of items from the graph. In one example that may allow for advantageous processing of "aesthetic second-order" visual searches (e.g., searches that are searching for information about a particular item rather than abstract aesthetic properties), a graph It can be a hierarchical representation. Selecting multiple selected items from a graph is based on a visual search query, selecting the first order in the graph that corresponds to the object shown in the image (for example, a particular movie shown in the image). may include identifying the item. The visual search system then identifies one or more items in the graph with respect to the primary item in the hierarchical representation of the graph, and identifies the primary item and one or more additional items as the multiple selected items. Items can be selected. Additional items may be at the same hierarchical level (e.g., other films by the same director), may be at a "higher" hierarchical level (e.g., other films in the same genre), and/or may be at a "lower" hierarchical level (e.g., other films from the same genre). ” hierarchical level (e.g., the 2020 “Director's Cut” release of the movie and the 1990 original theatrical release).
「美的な1次」視覚検索(たとえば、特定のカノニカル項目ではなく、抽象的な視覚的または美的な特性に関するコンテンツを探求している検索)の有利な処理を可能にし得る別の例では、複数の異なる項目を記述するグラフは、複数のインデックス付き画像に対応する複数のノードを含み得る。複数のノードは、グラフ内の対のノード同士の間の距離が対応する対のインデックス付き画像同士の間の視覚的類似性と逆比例関係にある(すなわち、より類似性の高い画像に対するノードはグラフ内で互いに「より近い」)ように、インデックス付き画像同士の間の視覚的類似性に少なくとも部分的に基づいて、グラフ内に配置され得る。一例では、グラフの複数のノードは、複数のクラスタになるように配置可能であり、視覚検索クエリに基づいて、グラフから複数の選択された項目を選択することは、複数のクラスタの1次クラスタを識別するためのエッジしきい値アルゴリズムを実行することと、複数の選択された項目として、1次クラスタ内に含まれたノードを選択することとを含み得る。別の例では、視覚検索システムは、エッジしきい値アルゴリズムを実行して、(たとえば、クラスタを識別するのとは対照的に)画像が示したオブジェクトに視覚的に類似する複数の視覚的類似ノードを直接的に識別し得る。視覚検索システムは、複数の選択された項目として、視覚的類似ノードを選択し得る。製品画像検索がマッチし得る「エッジ」または「ディメンション」の例は、カテゴリー(たとえば、「ドレス」)、属性(たとえば、「ノースリーブ」)など、認識派生属性、または機械抽出視覚的特徴または機械生成視覚的埋込みなど、機械生成視覚属性を含めて、「明色アクセントがある暗色」、「明色アクセントが、色空間全体の40%を構成する細線」など、他の意味的ディメンションおよび/または視覚的属性を含む。 Another example that may allow for advantageous processing of "aesthetic first-order" visual searches (e.g., searches that explore content about abstract visual or aesthetic properties rather than specific canonical items) is that multiple A graph describing different items of may include multiple nodes corresponding to multiple indexed images. The nodes are such that the distance between pairs of nodes in the graph is inversely related to the visual similarity between the corresponding pairs of indexed images (i.e., nodes for more similar images are The indexed images may be arranged in the graph based at least in part on visual similarities between the indexed images such that they are "closer" to each other in the graph. In one example, multiple nodes of a graph can be arranged into multiple clusters, and selecting multiple selected items from the graph based on a visual search query is a primary cluster of multiple clusters. and selecting nodes included in the primary cluster as the plurality of selected items. In another example, a visual search system performs an edge thresholding algorithm to identify multiple visual similarities that are visually similar to objects depicted by an image (as opposed to, for example, identifying clusters). Nodes can be directly identified. The visual search system may select visually similar nodes as the plurality of selected items. Examples of "edges" or "dimensions" that a product image search may match are categories (e.g., "dress"), attributes (e.g., "sleeveless"), recognition-derived attributes, or machine-extracted or machine-generated visual features. Including machine-generated visual attributes such as visual embeddings, and other semantic and/or visual dimensions such as "dark colors with light accents," "thin lines where light accents make up 40% of the total color space." including physical attributes.
したがって、視覚検索システムが、ユーザのクエリを「オーバーフィット(overfitting)」させ、ユーザのクエリの意図された焦点ではない可能性がある単一の特定の項目のみに関するコンテンツを戻すのではなく、視覚クエリをよりインテリジェントに処理して、複数のカノニカル項目に関するコンテンツを戻すことを可能にする例示的な技法が提供される。 Therefore, rather than visual search systems "overfitting" a user's query and returning content about only a single specific item that may not be the intended focus of the user's query, the visual Example techniques are provided that allow queries to be more intelligently processed to return content regarding multiple canonical items.
別の態様によれば、コンピュータ実装視覚検索システムは、クエリ画像内に示された特定のオブジェクトとカテゴリー別結果との間の視覚クエリをインテリジェントに明確化し得る。具体的には、オブジェクト固有のクエリとカテゴリー別クエリとを明確化することは、ユーザ意図の粒度およびオブジェクトの理解に関連する課題のもう1つの例である。たとえば、ユーザがシリアルボックスの画像および「どちらが最も多く食物繊維を有するか」を要求するクエリ(たとえば、テキストまたは発話クエリ)を提出すると想像されたい。ユーザの意図が、視覚クエリ内に具体的に含まれたシリアルの中から最も食物繊維が多いシリアルを判定することであるか、またはユーザの意図が、一般的に最も食物繊維が多いシリアルを判定することであるかを見分けることは困難であり得る。提出された画像、ならびにユーザに同じ結果を戻させる可能性があるクエリに対してかなりの変形態が存在し、このタスクの困難さをさらに浮き彫りにする。 According to another aspect, a computer-implemented visual search system may intelligently disambiguate a visual query between specific objects shown within a query image and categorical results. Specifically, disambiguating object-specific queries from categorical queries is another example of a challenge related to the granularity of user intent and object understanding. For example, imagine a user submits an image of a cereal box and a query (eg, a text or spoken query) requesting "which one has the most dietary fiber?" The user's intent is to determine which cereal has the most dietary fiber among the cereals specifically included in the visual query, or the user's intent is to determine which cereal has the most dietary fiber in general. It can be difficult to tell what to do. There are considerable variations to the submitted images as well as the queries that can return the same results to the user, further highlighting the difficulty of this task.
本開示は、視覚検索クエリがオブジェクト固有のクエリを含むかまたはカテゴリー別クエリを含むかを判定し、次いで、本質的にオブジェクト固有またはカテゴリー別であるコンテンツを戻すことによって、これらの課題を解決する。具体的には、コンピュータ視覚検索システムは、追加のコンテキスト号または情報を使用して、視覚クエリ内に存在する複数の異なるオブジェクト同士の間の関係を明らかにする、よりインテリジェントな検索結果を提供することができる。具体的には、一例として、視覚検索システムは、視覚クエリ内に含まれた画像の1つまたは複数の構成特性を識別し得る。視覚検索システムは、構成特性を使用して、視覚クエリが、検索結果の拡張コーパスが関連するカテゴリー別クエリであるか、または視覚クエリ内で識別された1つまたは複数のオブジェクトに特に関係するオブジェクト固有のクエリであるかを予測し得る。上記で与えられた例を続けると、視覚検索システムは、シリアルを示す画像の構成特性を使用して、視覚クエリ画像が、すべてのシリアルに関係するか、あるブランドまたはタイプのすべてのシリアルに関係するか、または画像内に含まれたそれらのシリアルのみに関係するかを判定し得る。 The present disclosure solves these challenges by determining whether a visual search query includes an object-specific or a categorical query and then returning content that is object-specific or categorical in nature. . Specifically, computer visual search systems use additional context or information to provide more intelligent search results that reveal relationships between multiple different objects present within a visual query. be able to. Specifically, by way of example, a visual search system may identify one or more constituent characteristics of images included within a visual query. Visual search systems use configuration characteristics to determine whether a visual query is a categorical query in which an expanded corpus of search results is related, or an object that is specifically related to one or more objects identified within the visual query. It is possible to predict whether the query is unique. Continuing with the example given above, the visual search system uses the compositional properties of images that indicate serials to determine whether the visual query image is related to all cereals or related to all cereals of a certain brand or type. or only those serials contained within the image.
視覚検索クエリがオブジェクト固有のクエリを含むと判定するとすぐに、視覚検索システムは、視覚クエリ内で識別された1つまたは複数のオブジェクトを特に対象とする、1つまたは複数のオブジェクト固有の検索結果(たとえば、画像内でキャプチャされたシリアルからの最も高い食物繊維含有量を有するシリアル)を戻すことができる。代替として、視覚検索クエリがカテゴリー別クエリを含むと判定するとすぐに、視覚検索システムは、視覚クエリ内で識別された1つまたは複数のオブジェクトの一般的なカテゴリーを対象とする、1つまたは複数のカテゴリー別検索結果(たとえば、すべてのシリアルからの最も高い食物繊維含有量を有するシリアル、または別の例として、同じタイプまたはブランドのすべてのシリアルからの最も高い食物繊維含有量を有するシリアル)を戻すことができる。 Upon determining that the visual search query includes an object-specific query, the visual search system generates one or more object-specific search results that specifically target the one or more objects identified within the visual query. (e.g., the cereal with the highest dietary fiber content from the cereals captured in the image). Alternatively, upon determining that a visual search query includes a categorical query, the visual search system may search for one or more general categories of one or more objects identified within the visual query. Search results by category (for example, cereals with the highest dietary fiber content from all cereals, or as another example, cereals with the highest dietary fiber content from all cereals of the same type or brand) It can be returned.
視覚検索システムによって使用される構成特性は、画像の様々な属性を含み得る。一例では、画像の構成特性は、(たとえば、写真をキャプチャしたカメラから)画像内で識別された1つまたは複数のオブジェクトまでの距離を含み得る。たとえば、カメラに近く配置されたオブジェクトを含む画像は、示されたオブジェクトに固有である可能性がより高く、カメラからさらに離れて配置されたオブジェクトを含む画像は、本質的にカテゴリー別である可能性がより高い。例を与えるために、特定のシリアルに関する情報を探求するユーザは、特定のシリアルボックスの近くに立ち、シリアル通路全体に対する視覚クエリをキャプチャする可能性が高い。 Compositional characteristics used by the visual search system may include various attributes of the image. In one example, compositional characteristics of an image may include the distance (eg, from the camera that captured the photo) to one or more objects identified in the image. For example, images containing objects placed closer to the camera are more likely to be specific to the object shown, while images containing objects placed further away from the camera are likely to be categorical in nature. higher in gender. To give an example, a user seeking information about a particular cereal is likely to stand near a particular cereal box and capture a visual query for the entire cereal aisle.
別の例では、画像の構成特性は、画像内で識別された1つまたは複数のオブジェクトの数を含み得る。具体的には、視覚クエリがオブジェクト固有のクエリを対象とする尤度を示す可能性があり得る、より少ない数のオブジェクトが識別される画像と比較して、より多くのオブジェクトが識別された画像は、視覚クエリがカテゴリー別クエリを対象とする尤度を示す可能性があり得る(たとえば、視覚クエリ内で識別された1つまたは複数のオブジェクトの一般的なカテゴリーを対象とする視覚クエリを示す可能性があり得る、25個のシリアルボックスを有する画像と比較して、3個のシリアルボックスの画像は、視覚クエリ内で識別された1つまたは複数のオブジェクトを特に対象とする視覚クエリを示す可能性があり得る)。 In another example, compositional characteristics of an image may include the number of one or more objects identified within the image. Specifically, images in which more objects are identified compared to images in which fewer objects are identified, which may indicate the likelihood that visual queries target object-specific queries. may indicate the likelihood that a visual query targets a categorical query (e.g., indicates a visual query that targets a general category of one or more objects identified within the visual query) An image of 3 cereal boxes, compared to an image with 25 cereal boxes, which could potentially indicate a visual query specifically targeting one or more objects identified within the visual query possible).
別の例では、画像の構成特性は、画像内で識別された1つまたは複数のオブジェクトの互いに対する相対的な類似性を含み得る。具体的には、視覚クエリがオブジェクト固有のクエリを対象とする尤度を示す可能性があり得る、他のオブジェクトの類似性が低い複数のオブジェクトを含む画像と比較して、画像内の他のオブジェクトの類似性が高い複数のオブジェクトを含む画像は、視覚クエリがカテゴリー別クエリを対象とする尤度を示す可能性があり得る。一例として、視覚クエリが視覚クエリ内で識別された1つまたは複数のオブジェクトの一般的なカテゴリーを対象とすることを示す可能性があり得る、複数のシリアルボックスを含む画像と比較して、シリアルボックスおよびボウルを含む画像は、視覚クエリ内で識別された1つまたは複数のオブジェクトを特に対象とする視覚クエリを示す可能性があり得る。 In another example, compositional characteristics of an image may include the relative similarity of one or more objects identified within the image to each other. Specifically, compared to images containing multiple objects with low similarity of other objects, which may indicate the likelihood that a visual query targets an object-specific query, Images containing multiple objects with high object similarity may indicate a likelihood that the visual query is targeted to a categorical query. As an example, compared to an image containing multiple cereal boxes, which could indicate that the visual query targets a general category of one or more objects identified within the visual query, Images containing boxes and bowls may potentially indicate a visual query that specifically targets one or more objects identified within the visual query.
別の例として、画像の構成特性は、画像内の1つまたは複数のオブジェクトの角度方位を含み得る。具体的には、視覚クエリがオブジェクト固有のクエリを対象とする尤度を示す可能性があり得る、特定の角度方位を備えたオブジェクトを含む画像と比較して、偶然に角度方位を備えたオブジェクトの画像を含む画像は、視覚クエリがカテゴリー別クエリを対象とする尤度を示す可能性があり得る。たとえば、視覚クエリ内で識別された1つまたは複数のオブジェクトを特に対象とする視覚クエリを示す可能性があり得る、画像のエッジに対して90度の角度でシリアルボックスを含む画像(たとえば、ボックスの面が明瞭に示され、カメラに向いている)と比較して、画像のエッジに対して32度の角度でシリアルボックスを含む画像は、カテゴリー別クエリを対象とする視覚クエリを示す可能性があり得る。 As another example, compositional characteristics of an image may include the angular orientation of one or more objects within the image. Specifically, an object with an angular orientation that happens to have an angular orientation compared to an image that contains an object with a particular angular orientation, which may indicate a likelihood that a visual query targets an object-specific query. An image containing an image may indicate a likelihood that the visual query targets a categorical query. For example, an image containing a cereal box at a 90 degree angle to the edges of the image (for example, a box An image containing a cereal box at a 32 degree angle to the edges of the image is more likely to indicate a visual query targeting a categorical query (compared to is possible.
別の例として、画像の構成特性は、画像内の1つまたは複数のオブジェクトの中心性(centeredness)(すなわち、オブジェクトが画像の中央に配置される角度)を含み得る。具体的には、視覚クエリがオブジェクト固有のクエリを対象とする尤度を示す可能性があり得る、中央に置かれているか、または中心のしきい値内にあるオブジェクトを含む画像と比較して、中央に置かれていないか、または中心のしきい値内にないオブジェクトを含む画像は、視覚クエリがカテゴリー別クエリを対象とする尤度を示す可能性があり得る。さらに、画像のエッジから識別されたオブジェクトまでの測定比率を使用して、オブジェクトが視覚クエリ内でどの程度中央に配置されているかを得ることができる(たとえば、視覚クエリが視覚クエリ内で識別された1つまたは複数のオブジェクトを特に対象とすることを示す可能性があり得る、1:1:1:1の比率を有するシリアルボックスを含む画面と比較して、1:6:9:3の比率で配置されたシリアルボックスを含む画像は、視覚クエリがカテゴリー別クエリを対象とすることを示す可能性があり得る)。 As another example, compositional characteristics of an image may include the centeredness of one or more objects within the image (ie, the angle at which the object is centered in the image). Specifically, compared to images containing objects that are centered or within the center threshold, visual queries may exhibit a higher likelihood of targeting object-specific queries. , images containing objects that are not centered or within the center threshold may indicate a likelihood that the visual query is targeted to a categorical query. Additionally, the measured ratio from the edges of the image to the identified object can be used to obtain how centered the object is within the visual query (e.g., when the visual query is 1:6:9:3 compared to a screen containing a cereal box with a 1:1:1:1 ratio, which could indicate that one or more objects are specifically targeted. An image containing cereal boxes arranged in proportions could indicate that the visual query targets a categorical query).
いくつかの実施形態では、視覚クエリ内に存在する複数の異なるオブジェクト同士の間の関係を明らかにする、よりインテリジェントな検索結果を提供するためのコンテキスト信号または情報は、視覚検索クエリ時のユーザのロケーションを含み得る。具体的には、視覚クエリがオブジェクト固有のクエリを対象とする尤度を示す可能性があり得る他のロケーション(たとえば、個人宅)と比較して、視覚検索クエリを行う時点でユーザが配置された一定のロケーション(たとえば、食料品店)は、視覚クエリがカテゴリー別クエリを対象とする尤度を示す可能性があり得る。オブジェクト固有の視覚クエリを示す可能性があり得る、限定されたオプションを備えたロケーションと比較して、ロケーションは、ユーザに利用可能な複数のオプションが存在する尤度が存在する場合、カテゴリー別視覚クエリを示す可能性があり得る。 In some embodiments, context signals or information to provide more intelligent search results that reveal relationships between multiple different objects present in a visual query are May include location. Specifically, where the user is located at the time of making the visual search query relative to other locations (e.g., a private home) where the visual query might exhibit a higher likelihood of targeting an object-specific query. Certain locations (eg, grocery stores) may indicate a likelihood that a visual query is targeted to a categorical query. Compared to a location with limited options, which may indicate an object-specific visual query, a location has a categorical visual query when there is a likelihood that there are multiple options available to the user. It may be possible to indicate a query.
いくつかの実施形態では、視覚検索クエリがオブジェクト固有のクエリを含むかまたはカテゴリー別クエリを含むかを判定することは、視覚検索クエリに関連するフィルタにさらに基づき得る。具体的には、フィルタは、ユーザが視覚クエリの画像を含む1つまたは複数のオブジェクトに対してカテゴリー別クエリを行う可能性が高いかまたはオブジェクト固有のクエリを行う可能性が高いか情報としてユーザ履歴を組み込むことができる。代替または追加として、フィルタは、視覚クエリに関連して、ユーザが入力したテキストまたは口頭クエリを含み得る。たとえば、口頭クエリ「どのシリアルが最も健康によいですか?」は、カテゴリー別である可能性が高く、口頭クエリ「これらの3つのうちどれが最も健康によいですか?」は、オブジェクト固有である可能性が高い。 In some embodiments, determining whether the visual search query includes an object-specific query or a categorical query may be further based on filters associated with the visual search query. Specifically, the filter determines whether a user is likely to make a categorical query or an object-specific query for one or more objects containing images in a visual query. History can be included. Alternatively or in addition, the filter may include user-entered text or verbal queries in conjunction with visual queries. For example, the verbal query "Which cereal is the healthiest?" is likely to be categorical, while the verbal query "Which of these three is the healthiest?" is likely to be object-specific. There is a high possibility that there is.
いくつかの実施形態では、視覚検索システムは、視覚クエリ内で識別された1つまたは複数のオブジェクトの一般的なカテゴリーを対象とする、1つまたは複数のカテゴリー別検索結果を戻すことができる。具体的には、視覚クエリ内で識別された1つまたは複数のオブジェクトの一般的なカテゴリーを対象とする、1つまたは複数のカテゴリー別検索結果を戻すことは、画像内の1つまたは複数のオブジェクトのうちの少なくとも1つがその下で分類するオブジェクトの個別カテゴリー(たとえば、項目のカテゴリー、項目のカテゴリー内のブランドなど)の収集物を最初に生成することを含み得る。さらに、視覚検索システムは、次いで、オブジェクトの複数の選択された個別カテゴリーを収集物から選択し得る。より詳細には、視覚検索システムは、少なくとも1つのコンテキスト信号または情報を使用して、個別カテゴリーの収集物の中からどれを選択するかを判定し得る。最終的に、視覚検索システムは、検索結果として、コンテンツの組み合わされたセットを戻すことができ、コンテンツの組み合わされたセットは、オブジェクトの複数の選択された個別カテゴリーの各々に関連する結果を含む。具体的には、視覚検索システムは、複数の結果をユーザに戻すことができ、これらの結果は、最大尤度ごとに階層的に表示され得る。 In some embodiments, the visual search system may return one or more categorical search results that are directed to a general category of one or more objects identified within the visual query. Specifically, returning one or more categorical search results that target the general category of one or more objects identified in a visual query The method may include first generating a collection of distinct categories of objects (eg, categories of items, brands within categories of items, etc.) under which at least one of the objects classifies. Additionally, the visual search system may then select multiple selected discrete categories of objects from the collection. More particularly, the visual search system may use at least one context signal or information to determine which of the collections of discrete categories to select. Ultimately, the visual search system can return as a search result a combined set of content, the combined set of content including results related to each of a plurality of selected distinct categories of objects. . Specifically, the visual search system may return multiple results to the user, and these results may be displayed hierarchically by maximum likelihood.
したがって、視覚検索システムが、視覚クエリをよりインテリジェントに処理し、ユーザが提供する視覚クエリ内で提供されるコンテキスト情報に応じて、カテゴリー別コンテンツまたはオブジェクト固有コンテンツに関するコンテンツを戻すことを可能にする例示的な技法が提供される。 Thus, an example that allows a visual search system to process visual queries more intelligently and return content regarding categorical or object-specific content depending on the contextual information provided within the visual query provided by the user. techniques are provided.
別の態様によれば、コンピュータ実装視覚検索システムは、複数の構成されたエンティティに関するコンテンツを視覚検索クエリに戻すことができる。具体的には、ユーザがいつ視覚検索クエリ内の複数のエンティティの特定の構成に関する情報を探求しているかを理解することは、ユーザ意図の粒度およびオブジェクトの理解がどの程度困難なタスクであるかのもう1つの例である。たとえば、ユーザが、視覚検索クエリ内に含めるために、アカデミー賞におけるEmma WatsonとDaniel Radcliffeの画像を提出するか、またはそうでなければ選択すると想像されたい。ユーザの意図が、Emma Watsonに関して問い合わせることか、Daniel Radcliffeに関して問い合わせることか、アカデミー賞に関して問い合わせることか、アカデミー賞におけるEmma Watsonに関して問い合わせることか、Harry Potterに関して問い合わせることか、またはエンティティのその他の様々な組合せに関して問い合わせることかを見分けることは困難であり得る。提出された画像、ならびに同じ結果をユーザに戻させる可能性があるクエリに対してかなりの変形態が存在し、このタスクの困難さをさらに浮き彫りにする。いくつかの既存のシステムは、複数のエンティティのいずれの構成をもまったく明らかにすることができないことになり、代わりに、(たとえば、類似する背景色など、画素レベルにおいて)視覚的に最も類似する画像をそのような視覚クエリに単に戻すことになる。 According to another aspect, a computer-implemented visual search system can return content regarding a plurality of configured entities to a visual search query. Specifically, understanding when a user is exploring information about a specific configuration of multiple entities within a visual search query is a difficult task due to the granularity of user intent and how difficult it is to understand objects. This is another example. For example, imagine a user submits or otherwise selects an image of Emma Watson and Daniel Radcliffe at the Academy Awards for inclusion in a visual search query. Whether the user's intent is to inquire about Emma Watson, inquire about Daniel Radcliffe, inquire about the Academy Awards, inquire about Emma Watson at the Academy Awards, inquire about Harry Potter, or any other variety of the entity. It can be difficult to tell what to ask about combinations. There are considerable variations to the submitted images as well as the queries that can return the same results to the user, further highlighting the difficulty of this task. Some existing systems end up being unable to reveal the configuration of any of multiple entities at all, instead choosing the most visually similar (e.g., similar background colors, etc. at the pixel level) It would simply return the image to such a visual query.
対照的に、本開示は、視覚検索システムが、複数のエンティティの構成の判定、およびエンティティのそのような構成に対するクエリの構成に基づいて、コンテンツをユーザに戻すことを可能にすることによって、これらの課題を解決する。具体的には、コンピュータ視覚検索システムは、1つまたは複数のコンテキスト信号または情報に基づいて、視覚検索クエリに関連する1つまたは複数のエンティティを識別し得る。視覚検索クエリに関連する2つ以上のエンティティを識別するとすぐ、視覚検索システムは、第1のエンティティと1つまたは複数の追加のエンティティ(たとえば、「2011年アカデミー賞におけるHarry Potterの受賞」)の組合せに関するコンテンツに対して構成されたクエリを判定し得る。エンティティは、人々、オブジェクト、および/またはイベントなど抽象的なエンティティを含み得る。第1のエンティティと1つまたは複数の追加のエンティティの組合せに関するコンテンツに対して構成されたクエリを判定するとすぐ、視覚検索システムは、コンテンツのセットを取得して戻すことができ、ここで、コンテンツのセットは、構成されたクエリに応じた、第1のエンティティと1つまたは複数の追加のエンティティの組合せに関係する、少なくとも1つのコンテンツ項目を含む。上記で与えられた例を続けると、アカデミー賞におけるEmma WatsonとDaniel Radcliffeの画像に応じて、視覚検索システムは、構成されたクエリを構築し、2011年アカデミー賞においてHarry Potterのキャストおよびクルーが受けたノミネートおよび受賞に関する検索結果を戻すことができる。 In contrast, the present disclosure provides a visual search system that returns content to a user based on determining the configuration of multiple entities and configuring queries for such configurations of entities. solve the problems of Specifically, a computer visual search system may identify one or more entities related to a visual search query based on one or more contextual signals or information. Upon identifying two or more entities that are related to a visual search query, the visual search system identifies the first entity and one or more additional entities (for example, "Harry Potter's win at the 2011 Academy Awards"). A query constructed for content related to the combination may be determined. Entities may include abstract entities such as people, objects, and/or events. Upon determining a query configured for content for a combination of the first entity and one or more additional entities, the visual search system can retrieve and return a set of content, where the visual search system The set includes at least one content item related to a combination of the first entity and one or more additional entities in response to the configured query. Continuing with the example given above, in response to the images of Emma Watson and Daniel Radcliffe at the Academy Awards, the visual search system builds a structured query that shows the cast and crew of Harry Potter at the 2011 Academy Awards. Search results for nominations and awards can be returned.
視覚クエリが複数のエンティティの構成に関するかどうかを判定するために使用されるコンテキスト信号または情報は、画像の様々な属性、ユーザがその画像をどこでソースしたかに関する情報、画像の他の使用またはインスタンスに関する情報、および/または様々な他のコンテキスト情報を含み得る。一例では、視覚検索クエリ内で使用される画像は、ウェブドキュメント(たとえば、ウェブページ)内に存在する。より詳細には、ウェブドキュメントは、1つまたは複数の部分の中のエンティティを参照し得る。具体的には、それらの参照は、テキスト(たとえば、「2011年アカデミー賞デザイナー」)または像(たとえば、2011年アカデミー賞レッドカーペットの写真)であってよく、それらのエンティティは、視覚検索(たとえば、「Emma Watson 2011年アカデミー賞ドレスデザイナー」)に関連する追加のエンティティとして識別され得る。したがって、ユーザが視覚クエリとして提出するためにウェブページ内に含まれたEmma Watsonの画像を選択する場合、他のエンティティの参照(たとえば、テキストおよび/または視覚的参照)を使用して、複数のエンティティの構成を形成するために使用され得る潜在的な追加のエンティティを識別し得る。 Contextual signals or information used to determine whether a visual query is about a configuration of multiple entities include various attributes of the image, information about where the user sourced the image, and other uses or instances of the image. and/or various other contextual information. In one example, an image used within a visual search query resides within a web document (eg, a web page). More particularly, a web document may refer to entities within one or more parts. Specifically, those references may be text (e.g., "2011 Academy Awards Designers") or statues (e.g., 2011 Academy Awards red carpet photos), and those entities may be searched visually (e.g., , "Emma Watson 2011 Academy Awards Dress Designer"). Therefore, when a user selects an image of Emma Watson included within a web page to submit as a visual query, he or she may use references to other entities (e.g., textual and/or visual references) to Potential additional entities may be identified that may be used to form a configuration of entities.
別の例として、画像のコンテキスト信号または情報は、視覚検索クエリに関連する画像の追加のインスタンスを含む追加のウェブドキュメント(たとえば、Harry Potterがアカデミー賞をどのくらい独占したかを論じる複数の記事)を含み得る。具体的には、1つまたは複数の追加のウェブドキュメントが参照する1つまたは複数の追加のエンティティは、視覚検索(たとえば、「Harry Potter 2011年アカデミー賞」)に関連する追加のエンティティとして識別され得る。したがって、ユーザが視覚クエリとして提出するために第1のウェブページ内に含まれたEmma Watsonの画像の第1のインスタンスを選択する場合、同じ画像の追加のインスタンスが他の異なるウェブページ内で(たとえば、一般的な逆画像検索の実行により)識別され得、次いで、そのような他の異なるウェブページ内に含まれた他のエンティティの参照(たとえば、テキストおよび/または視覚的参照)を使用して、複数のエンティティの構成を形成するために使用され得る潜在的な追加のエンティティを識別し得る。 As another example, an image's context signal or information may generate additional web documents containing additional instances of the image that are relevant to the visual search query (e.g., multiple articles discussing how Harry Potter dominated the Academy Awards). may be included. Specifically, the one or more additional entities referenced by the one or more additional web documents are identified as additional entities relevant to the visual search (e.g., "Harry Potter 2011 Academy Awards"). obtain. Therefore, if a user selects a first instance of an image of Emma Watson included within a first web page to submit as a visual query, additional instances of the same image may be submitted within other different web pages ( (e.g., by performing a common reverse image search) and then using references (e.g., textual and/or visual references) of other entities contained within such other different web pages. may identify potential additional entities that may be used to form a configuration of multiple entities.
別の例として、画像のコンテキスト信号または情報は、テキストメタデータ(たとえば、「アカデミー賞における受賞後のEmma WatsonとDaniel Radcliffe」)を含み得る。具体的には、テキストメタデータは、アクセスされ得、視覚検索に関連する追加のエンティティ(たとえば、「Harry Potter 2011年アカデミー賞」)として識別され得る。詳細には、テキストメタデータは、ユーザが提出した視覚クエリ内で使用される画像に対する字幕を含み得る。 As another example, the image context signal or information may include textual metadata (eg, "Emma Watson and Daniel Radcliffe after winning at the Academy Awards"). Specifically, text metadata may be accessed and identified as additional entities related to visual search (eg, "Harry Potter 2011 Academy Awards"). In particular, the textual metadata may include subtitles for images used within the user-submitted visual query.
別の例として、画像のコンテキスト信号または情報は、ロケーションまたは時間メタデータ(たとえば、Kodak Theatre、Los Angeles)を含み得る。具体的には、ロケーションまたは時間メタデータは、アクセスされ得、視覚検索に関連する追加のエンティティとして識別され得、視覚検索クエリ内で使用される画像のソースのロケーションは、画像自体の中の他の場所で示されない可能性がある、関係する話題参照を示し得る(たとえば、Emma WatsonとDaniel Radcliffeの画像は、「2011年、Los Angeles、Kodak TheatreにおけるEmma WatsonとDaniel Radcliffe」などの検索クエリにつながる、彼らがアカデミー賞にいたことを表す何の象徴化もその裏に存在しない、レッドカーペット上の彼らの画像のうちの一般的な1つの画像であり得る)。 As another example, the image context signal or information may include location or time metadata (eg, Kodak Theater, Los Angeles). Specifically, location or temporal metadata may be accessed and identified as an additional entity relevant to the visual search, and the location of the source of an image used within the visual search query may be different from others within the image itself. (For example, an image of Emma Watson and Daniel Radcliffe would appear in a search query such as "Emma Watson and Daniel Radcliffe at the Kodak Theater, Los Angeles, 2011." (This could be a generic one of their images on the red carpet, with no symbolism behind it to represent that they were at the Academy Awards).
別の例として、画像のコンテキスト信号または情報は、予備的検索を含み得る。より詳細には、第1の検索は、識別された複数のエンティティ(たとえば、「Los Angeles、Kodak TheatreにおけるEmma WatsonとDaniel Radcliffe」)を使用して行われてよく、予備的検索結果の第1のセットを取得するとすぐに、予備的検索結果が参照するさらなるエンティティが識別され得る。具体的には、しきい値を超える、いくつかの予備的結果内で識別されたエンティティは、続くクエリ(たとえば、「アカデミー賞におけるEmma WatsonとDaniel Radcliffe」)内に含まれるのに十分関係すると判定され得る。 As another example, the image context signal or information may include a preliminary search. More specifically, a first search may be performed using multiple identified entities (e.g., "Emma Watson and Daniel Radcliffe at the Kodak Theater, Los Angeles"), and a first search of the preliminary search results Upon obtaining the set of , further entities to which the preliminary search results refer may be identified. Specifically, entities identified within some preliminary results that exceed a threshold are considered sufficiently relevant to be included within a subsequent query (e.g., "Emma Watson and Daniel Radcliffe at the Academy Awards"). can be judged.
したがって、視覚検索システムが、視覚クエリをよりインテリジェントに処理し、ユーザが提供する視覚クエリ内のまたはそれによって提供されるコンテキスト信号または情報に応じて、複数の構成されたエンティティに関するコンテンツを戻すことを可能にする例示的な技法が提供される。 Thus, visual search systems process visual queries more intelligently and return content about multiple configured entities depending on contextual signals or information within or provided by the visual query provided by the user. Exemplary enabling techniques are provided.
関連画像または他のコンテンツの識別は、明示的な検索クエリに応じて実行され得るか、またはユーザ向けのコンテンツに対する一般的なクエリに応じて、プロアクティブに実行され得る(たとえば、明示的なクエリなしに、コンテンツをプロアクティブに識別し、ユーザに提供する「発見フィード」などのフィードの部分として)。「検索結果」という用語は、特定の視覚クエリに応じて識別されたコンテンツおよび/またはフィードまたは他のコンテンツレビュー機構内のプロアクティブな結果として含めるためにプロアクティブに識別されたコンテンツの両方を含むことが意図される。たとえば、フィードは、ユーザによる特定の初期の意図の宣言を必要とせずに、ユーザの視覚的関心グラフに基づくコンテンツを含み得る。 Identification of relevant images or other content may be performed in response to an explicit search query or may be performed proactively in response to a general query for content for a user (e.g., an explicit query (as part of a feed, such as a "discovery feed" that proactively identifies and serves content to users without having to use it). The term "search results" includes both content identified in response to a specific visual query and/or content proactively identified for inclusion as a proactive result within a feed or other content review mechanism. It is intended that For example, a feed may include content based on a user's visual interest graph without requiring a specific initial declaration of intent by the user.
次に図を参照しながら、本開示の例示的な実施形態について以下でさらに詳細に論じる。 Exemplary embodiments of the disclosure will now be discussed in more detail below with reference to the figures.
例示的なデバイスおよびシステム
図1Aは、本開示の例示的な実施形態による、視覚クエリに少なくとも部分的に応じて、個人化されたかつ/またはインテリジェントな検索を実行する例示的なコンピューティングシステム100のブロック図を示す。システム100は、ネットワーク180を介して通信可能に結合されたユーザコンピューティングデバイス102および視覚検索システム104を含む。
Exemplary Devices and Systems FIG. 1A shows an example computing system 100 that performs personalized and/or intelligent searches in response at least in part to visual queries, according to example embodiments of the present disclosure. The block diagram is shown below. System 100 includes a user computing device 102 and a visual search system 104 communicatively coupled via a network 180.
ユーザコンピューティングデバイス102は、たとえば、パーソナルコンピューティングデバイス(たとえば、ラップトップまたはデスクトップ)、モバイルコンピューティングデバイス(たとえば、スマートフォンまたはタブレット)、ゲーム機またはコントローラ、ウェアラブルコンピューティングデバイス、埋込みコンピューティングデバイス、または任意の他のタイプのコンピューティングデバイスなど、任意のタイプのコンピューティングデバイスであってよい。 User computing device 102 may be, for example, a personal computing device (e.g., a laptop or desktop), a mobile computing device (e.g., a smartphone or tablet), a game console or controller, a wearable computing device, an embedded computing device, or It may be any type of computing device, such as any other type of computing device.
ユーザコンピューティングデバイス102は、1つまたは複数のプロセッサ112およびメモリ114を含む。1つまたは複数のプロセッサ112は、任意の好適な処理デバイス(たとえば、プロセッサコア、マイクロプロセッサ、ASIC、FPGA、コントローラ、マイクロコントローラなど)であってよく、1つのプロセッサまたは動作可能に接続された複数のプロセッサであってよい。メモリ114は、RAM、ROM、EEPROM、EPROM、フラッシュメモリデバイス、磁気ディスクなど、1つまたは複数の非一時的コンピュータ可読記憶媒体、またはそれらの組合せを含み得る。メモリ114は、データ116、およびユーザコンピューティングデバイス102に動作を実行させるためにプロセッサ112によって実行される命令118を記憶し得る。 User computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), including one processor or multiple operably connected processors. processor. Memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, or combinations thereof. Memory 114 may store data 116 and instructions 118 that are executed by processor 112 to cause user computing device 102 to perform operations.
いくつかの実装形態では、ユーザコンピューティングデバイス102のカメラアプリケーション126は、ユーザコンピューティングデバイス102のカメラ124のビューファインダー内で認識されたオブジェクトに関するコンテンツを提示する。 In some implementations, the camera application 126 of the user computing device 102 presents content regarding objects recognized within the viewfinder of the camera 124 of the user computing device 102.
カメラアプリケーション126は、特定のプラットフォームに対して開発されたネイティブアプリケーションであり得る。カメラアプリケーション126は、ユーザコンピューティングデバイス102のカメラ124を制御し得る。たとえば、カメラアプリケーション126は、カメラを制御するための専用アプリケーション、アプリケーションの他の特徴とともに使用するためにカメラ124を制御するカメラファースト(camera-first)アプリケーション、またはカメラ124にアクセスし制御し得る別のタイプのアプリケーションであってよい。カメラアプリケーション126は、カメラアプリケーション126のユーザインターフェース158内にカメラ124のビューファインダーを提示し得る。 Camera application 126 may be a native application developed for a particular platform. Camera application 126 may control camera 124 of user computing device 102. For example, camera application 126 may be a dedicated application for controlling a camera, a camera-first application that controls camera 124 for use with other features of the application, or another application that may access and control camera 124. It may be a type of application. Camera application 126 may present a viewfinder for camera 124 within camera application 126 user interface 158.
概して、カメラアプリケーション126は、ユーザが、カメラ124のビューファインダー内に示されたオブジェクトに関するコンテンツ(たとえば、情報またはユーザ経験)を閲覧すること、および/またはユーザコンピューティングデバイス102上に記憶されたまたはユーザコンピューティングデバイス102によってアクセス可能な別のロケーションにおいて記憶された画像内に示されるオブジェクトに関するコンテンツを閲覧することを可能にする。ビューファインダーは、カメラのレンズのビューのフィールド内にあるもののライブ画像を提示するユーザコンピューティングデバイス102のディスプレイの一部分である。ユーザがカメラ124を移動させるにつれて(たとえば、ユーザコンピューティングデバイス102を移動させることによって)、ビューファインダーは、レンズのビューの現在のフィールドを提示するように更新される。 Generally, camera application 126 allows a user to view content (e.g., information or user experience) about objects shown in the viewfinder of camera 124 and/or stored on user computing device 102 or Enables viewing content regarding objects shown in stored images at another location accessible by user computing device 102. The viewfinder is the portion of the display of the user computing device 102 that presents a live image of what is within the field of view of the camera lens. As the user moves camera 124 (eg, by moving user computing device 102), the viewfinder is updated to present the current field of view of the lens.
カメラアプリケーション126は、オブジェクト検出器128、ユーザインターフェース生成器130、およびオンデバイストラッカー132を含む。オブジェクト検出器128は、エッジ検出および/または他のオブジェクト検出技法を使用して、ビューファインダー内のオブジェクトを検出し得る。いくつかの実装形態では、オブジェクト検出器128は、画像がオブジェクトの1つまたは複数の特定のクラス(たとえば、カテゴリー)内のオブジェクトを含むかどうかを判定する粗分類器(coarse classifier)を含む。たとえば、粗分類器は、実際のオブジェクトを認識するかしないかにかかわらず、画像が特定のクラスのオブジェクトを含むと検出し得る。 Camera application 126 includes an object detector 128, a user interface generator 130, and an on-device tracker 132. Object detector 128 may detect objects within the viewfinder using edge detection and/or other object detection techniques. In some implementations, object detector 128 includes a coarse classifier that determines whether the image contains objects within one or more particular classes (eg, categories) of objects. For example, a coarse classifier may detect that an image contains a particular class of objects, whether or not it recognizes the actual objects.
粗分類器は、画像がオブジェクトのクラスを示す1つまたは複数の特徴を含む(たとえば、示す)か否かに基づいて、オブジェクトのクラスの存在を検出し得る。粗分類器は、オブジェクトのそのクラス内のオブジェクトの存在を検出するために低計算解析を実行するための軽量モデルを含み得る。たとえば、粗分類器は、オブジェクトの各クラスに対して、画像がオブジェクトのクラス内に入るオブジェクトを含むかどうかを判定するために、画像内に示された視覚的特徴の限定されたセットを検出し得る。特定の例では、粗分類器は、テキスト、バーコード、ランドマーク、人々、食品、メディアオブジェクト、植物などを含むが、これらに限定されない、クラスのうちの1つまたは複数の中で分類されるオブジェクトを示すかどうかを検出し得る。バーコードの場合、粗分類器は、画像が異なる幅を有する平行線を含むかどうかを判定し得る。同様に、機械可読コード(たとえば、QRコード(登録商標)など)の場合、粗分類器は、画像が機械可読コードの存在を示すパターンを含むかどうかを判定し得る。 A coarse classifier may detect the presence of a class of objects based on whether the image includes (eg, exhibits) one or more features indicative of the class of objects. A coarse classifier may include a lightweight model for performing low computational analysis to detect the presence of objects within its class of objects. For example, for each class of objects, a coarse classifier detects a limited set of visual features exhibited within the image to determine whether the image contains an object that falls within the class of objects. It is possible. In a particular example, the coarse classifier classifies within one or more of the classes, including but not limited to text, barcodes, landmarks, people, food, media objects, plants, etc. It can be detected whether an object is indicated. In the case of barcodes, a coarse classifier may determine whether the image contains parallel lines with different widths. Similarly, for machine-readable codes (eg, QR codes, etc.), the coarse classifier may determine whether the image contains a pattern that indicates the presence of a machine-readable code.
粗分類器は、オブジェクトのクラスが画像内で検出されているかどうかを指定するデータを出力し得る。粗分類器は、オブジェクトのクラスの存在が画像内で検出されている信頼性を示す信頼性値および/または実際のオブジェクト、たとえば、シリアルボックス、が画像内に示されている信頼性を示す信頼性値を出力することもできる。 A coarse classifier may output data specifying whether a class of objects has been detected in the image. A coarse classifier has a confidence value that indicates the confidence that the presence of a class of objects has been detected in the image and/or a confidence value that indicates the confidence that the presence of a real object, e.g., a cereal box, is shown in the image. It is also possible to output the gender value.
オブジェクト検出器128は、カメラ124の視野を表す画像データ(たとえば、ビューファインダー内に提示されているもの)を受信し、画像データ内の1つまたは複数のオブジェクトの存在を検出し得る。少なくとも1つのオブジェクトが画像データ内で検出される場合、カメラアプリケーション126は、ネットワーク180を介して画像データを視覚検索システム104に提供(たとえば、送信)することができる。以下で説明するように、視覚検索システム104は、画像データ内のオブジェクトを認識し、オブジェクトに関するコンテンツをユーザコンピューティングデバイス102に提供し得る。 Object detector 128 may receive image data representative of the field of view of camera 124 (eg, as presented in a viewfinder) and detect the presence of one or more objects within the image data. If at least one object is detected in the image data, camera application 126 may provide (eg, send) the image data to visual search system 104 via network 180. As described below, visual search system 104 may recognize objects within image data and provide content regarding the objects to user computing device 102.
視覚検索システム104は、1つまたは複数のフロントエンドサーバ136および1つまたは複数のバックエンドサーバ140を含む。フロントエンドサーバ136は、ユーザコンピューティングデバイス、たとえば、ユーザコンピューティングデバイス102、から画像データを受信し得る。フロントエンドサーバ136は、画像データをバックエンドサーバ140に提供し得る。バックエンドサーバ140は、画像データ内で認識されたオブジェクトに関するコンテンツを識別し、コンテンツをフロントエンドサーバ136に提供し得る。次に、フロントエンドサーバ136は、そこから画像データが受信されたモバイルデバイスにコンテンツを提供し得る。 Visual search system 104 includes one or more front end servers 136 and one or more back end servers 140. Front end server 136 may receive image data from a user computing device, eg, user computing device 102. Front end server 136 may provide image data to back end server 140. Back end server 140 may identify content regarding objects recognized within the image data and provide the content to front end server 136. Front end server 136 may then provide content to the mobile device from which the image data was received.
バックエンドサーバ140は、1つまたは複数のプロセッサ142およびメモリ146を含む。1つまたは複数のプロセッサ142は、任意の好適な処理デバイス(たとえば、プロセッサコア、マイクロプロセッサ、ASIC、FPGA、コントローラ、マイクロコントローラなど)であってもよく、1つのプロセッサまたは動作可能に接続された複数のプロセッサであってよい。メモリ146は、RAM、ROM、EEPROM、EPROM、フラッシュメモリデバイス、磁気ディスクなど、1つまたは複数の非一時的コンピュータ可読記憶媒体、およびそれらの組合せを含み得る。メモリ146は、データ148、および視覚検索システム104に動作を実行させるためにプロセッサ142によって実行される命令150を記憶し得る。バックエンドサーバ140はまた、オブジェクト認識器152、クエリ処理システム154、およびコンテンツランク付けシステム156を含み得る。オブジェクト認識器152は、モバイルデバイス(たとえば、ユーザコンピューティングデバイス102など)から受信された画像データを処理し、もしあれば、画像データ内のオブジェクトを認識し得る。一例として、オブジェクト認識器152は、コンピュータ視覚および/または他のオブジェクト認識技法(たとえば、エッジマッチング、パターン認識、グレースケールマッチング、勾配マッチングなど)を使用して画像データ内のオブジェクトを認識し得る。 Backend server 140 includes one or more processors 142 and memory 146. The one or more processors 142 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), including a single processor or operably connected It may be multiple processors. Memory 146 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and combinations thereof. Memory 146 may store data 148 and instructions 150 executed by processor 142 to cause visual search system 104 to perform operations. Backend server 140 may also include an object recognizer 152, a query processing system 154, and a content ranking system 156. Object recognizer 152 may process image data received from a mobile device (eg, user computing device 102, etc.) and recognize objects, if any, in the image data. As an example, object recognizer 152 may recognize objects in image data using computer vision and/or other object recognition techniques (eg, edge matching, pattern recognition, grayscale matching, gradient matching, etc.).
いくつかの実装形態では、視覚検索システム104は、1つまたは複数のサーバコンピューティングデバイスを含むか、またはそうでなければ、それによって実装される。視覚検索システム104が、複数のサーバコンピューティングデバイスを含む場合、そのようなサーバコンピューティングデバイスは、シーケンシャルコンピューティングアーキテクチャ、並列コンピューティングアーキテクチャ、またはそれらのいくつかの組合せに従って動作し得る。 In some implementations, visual search system 104 includes or is otherwise implemented by one or more server computing devices. If visual search system 104 includes multiple server computing devices, such server computing devices may operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
いくつかの実装形態では、オブジェクト認識器152は、複数のオブジェクト認識器モジュールを、たとえば、そのそれぞれのクラス内のオブジェクトを認識するオブジェクトの各クラスに対して1つ、含む。たとえば、オブジェクト認識器152は、画像データ内のテキストを認識する(たとえば、文字、語などを認識する)テキスト認識器モジュール、画像データ内の(QRコード(登録商標)など、機械可読コードを含む)バーコードを認識する(たとえば、復号する)バーコード認識器モジュール、画像データ内のランドマークを認識するランドマーク認識器モジュール、および/またはオブジェクトの特定のクラスを認識する他のオブジェクト認識器モジュールを含み得る。 In some implementations, object recognizer 152 includes multiple object recognizer modules, eg, one for each class of objects that recognizes objects within its respective class. For example, object recognizer 152 may include a text recognizer module that recognizes text within image data (e.g., recognizes characters, words, etc.), including machine-readable codes (such as QR codes) within image data. ) barcode recognizer modules that recognize (e.g., decode) barcodes, landmark recognizer modules that recognize landmarks in image data, and/or other object recognizer modules that recognize specific classes of objects. may include.
いくつかの実装形態では、クエリ処理システム154は、複数の処理システムを含む。1つの例示的なシステムは、システムが複数の候補検索結果を識別することを可能にし得る。たとえば、システムは、視覚クエリ画像を最初に受信するとすぐに、複数の候補検索結果を識別し得る。他方で、システムは、システムによるさらなる処理がすでに行われた後で、複数の検索結果を識別し得る。詳細には、システムは、システムが生成した、よりターゲットを絞ったクエリに基づいて、複数の検索結果を識別し得る。さらにより詳細には、システムは、システムが最初に視覚クエリ画像を受信したときに、複数の候補検索結果を生成し、次いで、さらなる処理の後、システムが生成した、よりターゲットを絞ったクエリに基づいて、複数の候補検索結果を再生成し得る。 In some implementations, query processing system 154 includes multiple processing systems. One example system may allow the system to identify multiple candidate search results. For example, the system may identify multiple candidate search results upon first receiving a visual query image. On the other hand, the system may identify multiple search results after further processing by the system has already occurred. In particular, the system may identify multiple search results based on more targeted queries generated by the system. Still more particularly, the system generates multiple candidate search results when the system first receives a visual query image, and then, after further processing, the system generates a more targeted query. Based on the search results, a plurality of candidate search results may be regenerated.
別の例として、クエリ処理システム154は、ユーザ固有の関心データ(たとえば、グラフを使用して表すことができる)を生成するシステムを含み得る。より詳細には、ユーザ固有の関心データは、ユーザが複数の候補結果からの何の結果に関心をもつ可能性が最も高いことになるかを判定するために部分的に使用され得る。詳細には、ユーザ固有の関心データは、それらの結果をユーザに示す価値があるように、何の結果がしきい値を超えるかを判定するために部分的に使用され得る。たとえば、視覚検索システムは、ユーザのインターフェース上のオーバーレイ内にすべての候補結果を出力することが可能でないことがある。ユーザ固有の関心データは、どの候補結果が出力されることになり、どれが出力されないことになるかを判定するのに役立ち得る。 As another example, query processing system 154 may include a system that generates user-specific interest data (eg, which may be represented using a graph). More particularly, user-specific interest data may be used in part to determine which results from a plurality of candidate results the user will most likely be interested in. In particular, user-specific interest data may be used in part to determine what results exceed a threshold so that those results are worth showing to the user. For example, a visual search system may not be able to output all candidate results in an overlay on a user's interface. User-specific interest data may help determine which candidate results will be output and which will not.
別の例として、クエリ処理システム154は、コンテンツの組み合わされたセットに関するシステムを含み得る。より詳細には、コンテンツの組み合わされたセットは、視覚検索クエリに応じた複数のカノニカル項目を指すことがある。いくつかの実装形態では、システムは、複数の項目を記述するグラフを含み得、コンテンツのそれぞれのセット(たとえば、製品レビューなど、ユーザ生成コンテンツ)は複数の異なる項目の各々に関連付けられる。コンテンツの組み合わされたセットは、ユーザが、視覚検索クエリに応じて、オブジェクトの視覚クエリ画像に複数のカノニカル項目を提供するときに適用され得る。たとえば、ユーザが特定の映画のBlu-Rayのパッケージカバーの視覚クエリ画像を提供する場合、コンテンツシステムの組み合わされたセットは、その特定の映画のBlu-Rayのみではなく、一般的な映画、キャスト、またはその映画に関する任意の数のコンテンツに関する結果を出力し得る。 As another example, query processing system 154 may include a system for combined sets of content. More particularly, the combined set of content may refer to multiple canonical items in response to a visual search query. In some implementations, the system may include a graph that describes a plurality of items, with each set of content (eg, user-generated content, such as product reviews) associated with each of a plurality of different items. The combined set of content may be applied when a user provides multiple canonical items to a visual query image of an object in response to a visual search query. For example, if a user provides a visual query image of a Blu-Ray package cover for a particular movie, the combined set of content systems will not only look at that particular movie's Blu-Ray, but also general movies, casts, etc. , or any number of content related to that movie.
別の例として、クエリ処理システム154は、視覚クエリ画像の構成特性に関するシステムを含み得る。より詳細には、視覚検索システムが使用する構成特性は、画像の様々な属性(画像内のオブジェクトの数、カメラからのオブジェクトの距離、画像の角度方位など)を含み得る。構成特性システムは、クエリ画像内に示された特定のオブジェクトとカテゴリー別結果との間で視覚クエリをインテリジェントに明確化する一部として使用され得る。たとえば、ユーザは、「最も高い食物繊維を有する」テキストクエリが付随する3つのシリアルボックスの画像を提出し得る。構成特性システムは、システムによって識別された構成特性に基づいて、クエリがキャプチャされた3つのシリアルを目的とするか、もしくはシリアル全体を目的とするか、または特定のブランドのシリアルを目的とするかを明確化するのに役立ち得る。 As another example, query processing system 154 may include a system for compositional characteristics of visual query images. More particularly, the compositional characteristics used by the visual search system may include various attributes of the image, such as the number of objects in the image, the distance of the object from the camera, the angular orientation of the image, etc. The constituent feature system may be used as part of intelligently disambiguating visual queries between specific objects shown within the query image and categorical results. For example, a user may submit an image of three cereal boxes accompanied by a text query that "has the highest dietary fiber." The configuration characteristics system determines whether the query is aimed at the three captured serials, the entire serial, or a specific brand of cereal based on the configuration characteristics identified by the system. may help clarify.
別の例として、クエリ処理システム154は、視覚クエリに関連する複数のエンティティに関するシステムを含み得る。より詳細には、複数のエンティティは、画像内に含まれた複数の主題、ならびにいかなる形でも画像を取り巻くコンテキスト(画像が撮影された場所のGPS座標、画像に付随するテキスト字幕、画像が見出されたウェブページなど)を参照する。複数のエンティティシステムは、ユーザが意図した可能性があるすべての候補検索結果を包含するエンティティの組合せで2次クエリをさらに構成し得る。たとえば、Los AngelesのKodak Theatreの前に立っているEmma WatsonとDaniel Radcliffeの画像は、「Emma Watson」または「Daniel Radcliffe」に対する検索を超えた任意の数のユーザ意図を背景に有し得る。複数のエンティティシステムは、複数の識別されたエンティティに応じて、「アカデミー賞におけるHarry Potter」、「Emma Watsonアカデミー賞デザイナー」など、2次クエリを構成し得る。 As another example, query processing system 154 may include a system for multiple entities related to a visual query. More specifically, the multiple entities include the multiple subjects contained within the image, as well as any context surrounding the image (GPS coordinates of where the image was taken, text subtitles accompanying the image, whether the image is a web pages etc.). The multiple entity system may further compose a secondary query with a combination of entities encompassing all candidate search results that the user may have intended. For example, an image of Emma Watson and Daniel Radcliffe standing in front of the Kodak Theater in Los Angeles may have any number of user intents behind it, beyond a search for "Emma Watson" or "Daniel Radcliffe." The multiple entity system may construct a secondary query depending on the multiple identified entities, such as "Harry Potter at the Academy Awards", "Emma Watson Academy Award Designer", etc.
いくつかの実装形態では、コンテンツランク付けシステム156は、候補検索結果をランク付けするための視覚検索システムプロセスの複数の異なる時点で使用され得る。1つの例示的なアプリケーションは、複数の検索結果が最初に識別された後、検索結果のランク付けを生成するためである。他方で、初期検索結果は、単に予備的であり得、ランク付けシステム156は、クエリ処理システムがよりターゲットを絞ったクエリを生成した後、検索結果のランク付けを生成し得る。さらにより詳細には、ランク付けシステム156は、システムが、初めに、候補検索結果のセットを識別し、次いで、よりターゲットを絞ったクエリが行われた後に再度識別するとき、複数の候補検索結果のランク付けを生成し得る(たとえば、複数のエンティティのどの組合せが最も可能性が高いかを判定するために、予備的ランク付けが使用され得る)。ランク付けシステム156によって生成されたランク付けは、検索結果が何の順序で出力されることになるか、かつ/または候補検索結果が出力されることになるか否かを判定することによって、ユーザに対する候補検索結果の最終的な出力を判定するために使用され得る。 In some implementations, content ranking system 156 may be used at multiple different points in the visual search system process to rank candidate search results. One example application is for generating a ranking of search results after multiple search results are first identified. On the other hand, the initial search results may be merely preliminary, and the ranking system 156 may generate a ranking of the search results after the query processing system generates a more targeted query. Even more particularly, the ranking system 156 may include a plurality of candidate search results when the system initially identifies a set of candidate search results and then identifies again after a more targeted query has been made. (e.g., a preliminary ranking may be used to determine which combinations of multiple entities are most likely). The rankings generated by the ranking system 156 are determined by the user by determining in what order the search results are to be output and/or whether candidate search results are to be output. can be used to determine the final output of candidate search results for.
クエリ処理システム154内に含まれた複数の処理システムは、最もインテリジェントな結果をユーザに提供するために、ユーザが提出した視覚クエリを最もインテリジェントな方法で処理するように、互いとの任意の組合せで、かつ任意の順序で、使用され得る。さらに、ランク付けシステム156は、任意の組合せでクエリ処理システム154とともに使用され得る。 The multiple processing systems included within the query processing system 154 may be used in any combination with each other to process visual queries submitted by users in the most intelligent manner to provide the most intelligent results to the user. and in any order. Additionally, ranking system 156 may be used with query processing system 154 in any combination.
コンテンツが選択された後、コンテンツは、そこから画像データが受信されたユーザコンピューティングデバイス102に提供され、視覚検索システム104のコンテンツキャッシュ130内に記憶され、かつ/またはフロントエンドサーバ136のメモリスタックの上部に記憶され得る。このようにして、コンテンツは、ユーザのコンテンツ要求に応じて、ユーザに直ちに提示され得る。コンテンツがユーザコンピューティングデバイス102に提供される場合、カメラアプリケーション126は、コンテンツをコンテンツキャッシュ134または他の高速アクセスメモリ内に記憶し得る。たとえば、カメラアプリケーション126は、カメラアプリケーション126が、オブジェクトに関するコンテンツを提示する判定に応じて、オブジェクトに対する適切なコンテンツを識別することができるように、オブジェクトに対する基準とともに、オブジェクトに関するコンテンツを記憶し得る。 After the content is selected, the content is provided to the user computing device 102 from which the image data was received, stored in the content cache 130 of the visual search system 104, and/or stored in the memory stack of the front end server 136. can be stored at the top of the . In this way, content can be immediately presented to the user in response to the user's content request. When content is provided to user computing device 102, camera application 126 may store the content in content cache 134 or other fast access memory. For example, camera application 126 may store content about an object along with criteria for the object such that camera application 126 can identify appropriate content for the object in response to a determination to present content about the object.
カメラアプリケーション126は、オブジェクトに対する視覚インジケータとのユーザ対話に応じて、オブジェクトに関するコンテンツを提示し得る。たとえば、カメラアプリケーション126は、オブジェクトに対する視覚インジケータとのユーザ対話を検出し、視覚検索システム104からオブジェクトに関するコンテンツを要求し得る。それに応じて、フロントエンドサーバ136は、コンテンツキャッシュ130またはメモリスタックの上部からコンテンツを取得し、コンテンツを、そこから要求が受信されたユーザコンピューティングデバイス102に提供し得る。ユーザ対話が検出されるのに先立って、コンテンツがユーザコンピューティングデバイス102に提供された場合、カメラアプリケーション126は、コンテンツキャッシュ134からコンテンツを取得し得る。 Camera application 126 may present content about the object in response to user interaction with a visual indicator for the object. For example, camera application 126 may detect user interaction with a visual indicator for an object and request content about the object from visual search system 104. In response, front end server 136 may retrieve content from content cache 130 or the top of the memory stack and provide the content to user computing device 102 from which the request was received. Camera application 126 may obtain content from content cache 134 if the content was provided to user computing device 102 prior to the user interaction being detected.
いくつかの実装形態では、視覚検索システム104は、たとえば、カメラアプリケーション126ではなく、オブジェクト検出器128を含む。そのような例では、カメラアプリケーション126がアクティブな間、またはユーザが要求コンテンツモードのカメラアプリケーション126を有する間、カメラアプリケーション126は、たとえば、画像のストリーム内で、画像データを視覚検索システム104に連続的に送信し得る。要求コンテンツモードは、カメラアプリケーション126が、画像データ内で認識されたオブジェクトに関するコンテンツを要求するために、画像データを視覚検索システム104に連続的に送ることを可能にし得る。視覚検索システム104は、画像内のオブジェクトを検出し、画像を処理し(たとえば、検出されたオブジェクトに対する視覚インジケータを選択し)、ユーザインターフェース(たとえば、ビューファインダー)内に提示するために結果(たとえば、視覚インジケータ)をカメラアプリケーション126に送ることができる。視覚検索システム104は、画像データを処理し続けて、オブジェクトを認識し、各認識されたオブジェクトに関するコンテンツを選択し、コンテンツをキャッシュするか、またはコンテンツをカメラアプリケーション126に送ることのいずれかを行うことも可能である。 In some implementations, visual search system 104 includes an object detector 128 rather than a camera application 126, for example. In such examples, while camera application 126 is active or while a user has camera application 126 in a requested content mode, camera application 126 continuously sends image data to visual retrieval system 104, e.g., within a stream of images. It can be sent directly. Request content mode may allow camera application 126 to continuously send image data to visual search system 104 to request content regarding objects recognized within the image data. Visual search system 104 detects objects in images, processes the images (e.g., selects visual indicators for detected objects), and generates results (e.g., , visual indicators) can be sent to the camera application 126. Visual search system 104 continues to process the image data to recognize objects, select content for each recognized object, and either cache the content or send the content to camera application 126. It is also possible.
いくつかの実装形態では、カメラアプリケーション126は、画像データ内のオブジェクトを認識するオンデバイスオブジェクト認識器を含む。この例では、カメラアプリケーション126は、オブジェクトを認識し、認識されたオブジェクトに関するコンテンツを視覚検索システム104から要求すること、またはオンデバイスコンテンツデータストアからコンテンツを識別することのいずれかが可能である。オンデバイスオブジェクト認識器は、オブジェクトのより限定されたセットを認識する、または視覚検索システム104のオブジェクト認識器152よりも計算的に費用がかからないオブジェクト認識技法を使用する、軽量オブジェクト認識器であってよい。これは、一般的なサーバよりも処理電力が少ないモバイルデバイスがオブジェクト認識プロセスを実行することを可能にする。いくつかの実装形態では、カメラアプリケーション126は、オンデバイス認識器を使用して、オブジェクトの初期識別を行い、確認のために画像データを視覚検索システム104(または、別のオブジェクト認識システム)に提供することができる。オンデバイスコンテンツデータストアは、コンテンツデータ記憶ユニット138よりも限定されたコンテンツセット、またはユーザコンピューティングデバイス102のデータ記憶リソースを保存するためのコンテンツを含むリソースへのリンクを記憶することも可能である。 In some implementations, camera application 126 includes an on-device object recognizer that recognizes objects in image data. In this example, camera application 126 may recognize the object and either request content about the recognized object from visual search system 104 or identify content from an on-device content data store. An on-device object recognizer is a lightweight object recognizer that recognizes a more limited set of objects or uses object recognition techniques that are less computationally expensive than the object recognizer 152 of the visual search system 104. good. This allows mobile devices with less processing power than a typical server to perform the object recognition process. In some implementations, camera application 126 uses an on-device recognizer to perform initial identification of the object and provides image data to visual search system 104 (or another object recognition system) for verification. can do. The on-device content data store may also store a more limited set of content than content data storage unit 138 or links to resources containing content for storing data storage resources of user computing device 102. .
ユーザコンピューティングデバイス102は、ユーザ入力を受信する1つまたは複数のユーザ入力構成要素122を含んでもよい。たとえば、ユーザ入力構成要素122は、ユーザ入力オブジェクトのタッチ(たとえば、指またはスタイラス)に敏感なタッチセンシティブ構成要素(たとえば、タッチセンシティブディスプレイスクリーンまたはタッチパッド)であってよい。タッチセンシティブ構成要素は、仮想キーボードを実装するのに役立ち得る。他の例示的なユーザ入力構成要素は、マイクロフォン、旧式キーボード、またはユーザがユーザ入力を提供し得る他の手段を含む。 User computing device 102 may include one or more user input components 122 that receive user input. For example, user input component 122 may be a touch-sensitive component (eg, a touch-sensitive display screen or touch pad) that is sensitive to the touch of a user input object (eg, a finger or stylus). Touch-sensitive components may help implement a virtual keyboard. Other example user input components include a microphone, an old-fashioned keyboard, or other means by which a user may provide user input.
ネットワーク180は、ローカルエリアネットワーク(たとえば、イントラネット)、広域ネットワーク(たとえば、インターネット)、またはそれらの何らかの組合せなど、任意のタイプの通信ネットワークであってよく、任意の数のワイヤードまたはワイヤレスリンクを含み得る。概して、ネットワーク180を介した通信は、幅広い通信プロトコル(たとえば、TCP/IP、HTTP、SMTP、FTP)、符号化もしくはフォーマット(たとえば、HTML、XML)、および/または保護方式(たとえば、VPN、セキュアHTTP、SSL)を使用して、任意のタイプのワイヤードおよび/またはワイヤレス接続を介して搬送され得る。図1は、本開示を実装するために使用され得る、1つの例示的なコンピューティングシステムを示す。構成要素の他の異なる分布が同様に使用され得る。たとえば、視覚検索システムの様々な態様のうちのいくつかまたはすべては、代わりに、ユーザコンピューティングデバイス102において配置されかつ/または実装されてよい。 Network 180 may be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the Internet), or some combination thereof, and may include any number of wired or wireless links. . Communications over network 180 typically involve a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection methods (e.g., VPN, secure HTTP, SSL) over any type of wired and/or wireless connection. FIG. 1 depicts one example computing system that may be used to implement this disclosure. Other different distributions of components may be used as well. For example, some or all of the various aspects of the visual search system may instead be located and/or implemented on user computing device 102.
例示的なモデル配置
図2は、本開示の例示的な実施形態による、例示的な視覚検索システム200のブロック図を示す。いくつかの実装形態では、視覚検索システム200は、視覚クエリ204を含む入力データのセットを受信し、入力データ204の受信の結果として、より個人化されたかつ/またはインテリジェントな結果をユーザに提供する出力データ206を提供するように構成される。一例として、いくつかの実装形態では、視覚検索システム200は、より個人化されたかつ/またはインテリジェントな視覚クエリ結果の出力を促すように動作可能であるクエリ処理システム202を含み得る。
Exemplary Model Deployment FIG. 2 depicts a block diagram of an exemplary visual search system 200, according to an exemplary embodiment of the present disclosure. In some implementations, the visual search system 200 receives a set of input data including a visual query 204 and provides more personalized and/or intelligent results to the user as a result of receiving the input data 204. The output data 206 is configured to provide output data 206. As an example, in some implementations, visual search system 200 may include a query processing system 202 that is operable to facilitate output of more personalized and/or intelligent visual query results.
いくつかの実装形態では、クエリ処理システム202は、より個人化された検索結果を提供するためのユーザ中心の視覚的関心グラフを含むか、またはそれを活用する。1つの例示的な使用では、視覚検索システム200は、ユーザの関心のグラフを使用して、視覚発見アラート、通知、または他の機会を含む検索結果をランク付けまたはフィルタリングし得る。ユーザの関心に基づく検索結果の個人化は、検索結果がクエリ画像上の拡張オーバーレイ内に視覚結果通知(たとえば、場合によっては、「グリーム」と呼ばれることがある)として提示される例示的な実施形態において特に有利であり得る。 In some implementations, query processing system 202 includes or leverages a user-centric visual interest graph to provide more personalized search results. In one example use, visual search system 200 may use a graph of a user's interests to rank or filter search results that include visual discovery alerts, notifications, or other opportunities. Personalization of search results based on user interests is an example implementation in which search results are presented as a visual result notification (e.g., sometimes referred to as a "Gleam") within an enhanced overlay over the query image. It can be particularly advantageous in morphology.
いくつかの実装形態では、ユーザ固有の関心データ(たとえば、グラフを使用して表され得る)は、ユーザが過去に関与した画像を解析することによって少なくとも部分的に、経時的にアグリゲートされ得る。言い方を変えれば、視覚検索システム200は、ユーザが経時的に関与する画像を解析することによって、ユーザの視覚的関心の理解を試行し得る。ユーザが画像に関与するとき、ユーザが画像の何らかの側面に関心をもっていると推論され得る。したがって、そのような画像内に含まれた、またはそうでなければ、関係する項目(たとえば、オブジェクト、エンティティ、概念、製品など)は、ユーザ固有の関心データ(たとえば、グラフ)に追加されるか、またはそうでなければ、その中で言及され得る。 In some implementations, user-specific interest data (e.g., which may be represented using a graph) may be aggregated over time, at least in part, by analyzing images with which the user has engaged in the past. . In other words, visual search system 200 may attempt to understand a user's visual interests by analyzing images with which the user engages over time. When a user engages with an image, it can be inferred that the user is interested in some aspect of the image. Accordingly, items contained or otherwise related to such images (e.g., objects, entities, concepts, products, etc.) may be added to user-specific data of interest (e.g., graphs) , or otherwise may be mentioned therein.
一例として、ユーザが関与する画像は、ユーザがキャプチャした写真、ユーザがキャプチャしたスクリーンショット、またはユーザが閲覧したウェブベースまたはアプリケーションベースのコンテンツ内に含まれる画像を含み得る。別の潜在的に重複する例では、ユーザが関与する画像は、ユーザが、その画像に対して実行されるアクションを要求することによってアクティブに関与した、アクティブに関与された画像を含み得る。たとえば、要求されるアクションは、画像に対して視覚検索を実行すること、またはユーザがその画像がユーザの視覚的関心を含むと明示的にマーキングすることを含み得る。別の例として、ユーザが関与する画像は、ユーザに提示されたが、ユーザが特に関与しなかった、受動的に観測された画像を含み得る。視覚的関心は、ユーザが入力したテキストコンテンツ(たとえば、テキストまたは用語ベースのクエリ)から推論されることも可能である。 By way of example, user-involved images may include photos captured by the user, screenshots captured by the user, or images included within web-based or application-based content viewed by the user. In another potentially redundant example, a user-engaged image may include an actively engaged image in which the user actively engaged by requesting an action to be performed on that image. For example, the requested action may include performing a visual search on the image, or the user explicitly marking the image as containing the user's visual interest. As another example, a user-involved image may include a passively observed image that is presented to the user but with no specific involvement by the user. Visual interest can also be inferred from user-entered textual content (eg, text or term-based queries).
上記の説明に加えて、本明細書で説明するシステム、プログラム、または特徴が、ユーザ情報(たとえば、ユーザのソーシャルネットワーク、ソーシャルアクションもしくは活動、専門、ユーザの選好、またはユーザの現在のロケーションに関する情報)の収集を可能にし得る場合と、ユーザにサーバからコンテンツまたは通信が送られる場合の両方に関して、ユーザが選択することを可能にする制御がユーザに提供され得る。加えて、一定のデータは、個人的に識別可能な情報が除去されるように、そのデータが記憶または使用される前に、1つまたは複数の方法で処理され得る。たとえば、ユーザの識別情報は、ユーザに関する個人的に識別可能な情報を判定することができないように扱われることが可能であり、または(市、郵便番号、または州レベルでなど)ロケーション情報が取得される場合、ユーザの特定のロケーションを判定することができないように、ユーザの地理的ロケーションは一般化され得る。したがって、ユーザは、ユーザに関して何の情報が収集されるか、その情報がどのように使用されるか、また何の情報がユーザに提供されるか、に対して制御を有し得る。 In addition to the above description, the systems, programs, or features described herein may include user information (e.g., information regarding the user's social networks, social actions or activities, professions, user preferences, or user's current location). Controls may be provided to the user that allow the user to make selections regarding both when content or communications may be sent to the user from the server. Additionally, certain data may be processed in one or more ways before it is stored or used so that personally identifiable information is removed. For example, a user's identity may be treated such that no personally identifiable information about the user can be determined, or location information (such as at the city, zip code, or state level) may be obtained. If the user's geographic location is determined, the user's geographic location may be generalized such that the user's specific location cannot be determined. Thus, the user may have control over what information is collected about the user, how that information is used, and what information is provided to the user.
視覚検索システム200は、クエリ処理システム202を使用して、ユーザに対する検索結果を選択し得る。ユーザ関心システム202は、クエリ修正、結果識別、および/または視覚検索プロセスの他の段階を含めて、検索プロセスの様々な段階において使用され得る。 Visual search system 200 may use query processing system 202 to select search results for a user. User interest system 202 may be used at various stages of the search process, including query modification, result identification, and/or other stages of the visual search process.
一例として、図3は、本開示の例示的な実施形態による例示的な視覚検索システム400のブロック図を示す。視覚検索システム400は、2段階プロセスで動作する。第1段階において、クエリ処理システム202は、入力データ204(たとえば、1つまたは複数の画像を含む視覚クエリ)を受信し、視覚クエリに応じる候補検索結果206のセットを生成し得る。たとえば、候補検索結果206は、ユーザ固有の関心を考慮せずに、取得され得る。第2の段階において、ランク付けシステム402は、最終検索結果として(たとえば、出力データ404として)ユーザに戻すための候補検索結果206のうちの1つまたは複数をランク付けするのを支援するために、視覚検索システム400によって使用され得る。 As an example, FIG. 3 depicts a block diagram of an example visual search system 400 according to an example embodiment of the present disclosure. Visual search system 400 operates in a two-step process. In a first stage, query processing system 202 may receive input data 204 (eg, a visual query that includes one or more images) and generate a set of candidate search results 206 responsive to the visual query. For example, candidate search results 206 may be obtained without considering the user's specific interests. In a second stage, ranking system 402 is configured to assist in ranking one or more of candidate search results 206 for return to the user as final search results (e.g., as output data 404). , may be used by visual search system 400.
一例として、視覚検索システム400は、ランク付けシステム402を使用して、クエリ処理システム202内で取得されたユーザに関連するユーザ固有のユーザの関心データに対する複数の候補検索結果206の比較に少なくとも部分的に基づいて、複数の候補検索結果206のランク付けを生成し得る。たとえば、候補検索結果206に関連する初期検索スコアを修正または再重み付けするために、ユーザの関心データ内でキャプチャされる一定の項目に対する重みが加えられてよく、これは、ユーザへの出力404に先立って、検索結果206の再ランク付けをさらにもたらし得る。 As an example, the visual search system 400 uses the ranking system 402 to at least partially compare the plurality of candidate search results 206 against user-specific user interest data associated with the user obtained within the query processing system 202. A ranking of the plurality of candidate search results 206 may be generated based on the search results. For example, to modify or reweight the initial search scores associated with candidate search results 206, weights may be added to certain items captured within the user's interest data, which may be added to the output 404 to the user. Prior to this, re-ranking of search results 206 may further be effected.
視覚検索システム400は、ランク付けに少なくとも部分的に基づいて、少なくとも1つの選択された検索結果として複数の候補検索結果206のうちの少なくとも1つを選択し、次いで、ユーザに表示するために、少なくとも1つの選択された検索結果にそれぞれ関連する少なくとも1つの選択された視覚結果通知を(たとえば、出力データ404)として提供し得る。一例では、選択された検索結果の各々は、選択された検索結果に関連する画像の特定のサブ部分上にオーバーレイするために提供され得る。そのような様式では、ユーザの関心は、個人化された検索結果を提供し、ユーザインターフェース内のクラッターを低減するために使用され得る。 The visual search system 400 selects at least one of the plurality of candidate search results 206 as the at least one selected search result based at least in part on the ranking, and then selects at least one of the plurality of candidate search results 206 for display to the user. At least one selected visual result notification may be provided (eg, as output data 404), each associated with at least one selected search result. In one example, each of the selected search results may be provided for overlay on a particular sub-portion of the image associated with the selected search result. In such a manner, user interests may be used to provide personalized search results and reduce clutter within the user interface.
別の例示的な変形態として、図4は、本開示の例示的な実施形態による例示的な視覚検索システム500のブロック図を示す。視覚検索システム500は、コンテキスト情報504を受信し、コンテキスト情報504を処理して、視覚クエリおよび/またはユーザの検索意図の暗示的な特性を明らかにするコンテキスト構成要素502をさらに含むことを除いて、視覚検索システム500は、図3の視覚検索システム400と同様である。 As another example variation, FIG. 4 depicts a block diagram of an example visual search system 500 according to an example embodiment of the present disclosure. The visual search system 500 further includes a context component 502 that receives context information 504 and processes the context information 504 to reveal implicit characteristics of the visual query and/or the user's search intent. , visual search system 500 is similar to visual search system 400 of FIG.
コンテキスト情報504は、任意の他の利用可能な信号またはクエリの暗示的な特性を理解するのを支援する情報を含み得る。たとえば、ロケーション、時刻、入力モダリティ、および/または様々な他の情報がコンテキストとして使用され得る。 Contextual information 504 may include information that assists in understanding any other available signals or implicit characteristics of the query. For example, location, time of day, input modality, and/or various other information may be used as context.
別の例として、コンテキスト情報504は、画像の様々な属性、画像がユーザによってどこでソースされたかに関する情報、画像の他の使用またはインスタンスに関する情報、および/または様々な他のコンテキスト情報を含み得る。一例では、視覚検索クエリ内で使用される画像は、ウェブドキュメント(たとえば、ウェブページ)内に存在する。ウェブドキュメント内に含まれる他のエンティティの参照(たとえば、テキストおよび/または視覚的参照)は、複数のエンティティの構成を形成するために使用され得る潜在的に追加のエンティティを識別するために使用され得る。 As another example, context information 504 may include various attributes of the image, information regarding where the image was sourced by the user, information regarding other uses or instances of the image, and/or various other contextual information. In one example, an image used within a visual search query resides within a web document (eg, a web page). References to other entities (e.g., textual and/or visual references) contained within the web document may be used to identify potentially additional entities that may be used to form a composition of multiple entities. obtain.
別の例では、コンテキスト情報504は、視覚検索クエリに関連する画像の追加のインスタンスを含む追加のウェブドキュメントから取得された情報を含み得る。別の例として、コンテキスト情報504は、画像に関連するテキストメタデータ(たとえば、EXIFデータ)を含み得る。具体的には、テキストメタデータは、アクセスされ得、視覚検索に関連する追加のエンティティとして識別され得る。詳細には、テキストメタデータは、ユーザが提出した視覚クエリ内で使用される画像に対する字幕を含み得る。 In another example, context information 504 may include information obtained from additional web documents that include additional instances of images related to the visual search query. As another example, context information 504 may include textual metadata (eg, EXIF data) associated with the image. Specifically, textual metadata may be accessed and identified as additional entities relevant to visual search. In particular, the textual metadata may include subtitles for images used within the user-submitted visual query.
別の例として、コンテキスト情報504は、視覚クエリに基づいて予備的検索によって取得された情報を含み得る。より詳細には、第1の検索は、視覚クエリからの情報を使用して行われてよく、予備的検索結果の第1のセットを取得するとすぐに、予備的検索結果が参照するさらなるエンティティが識別され得る。具体的には、しきい値を超える何らかの数の予備的結果の中で識別されるエンティティは、後続のクエリ内に含まれるのに十分関係すると判定され得る。 As another example, contextual information 504 may include information obtained through a preliminary search based on a visual query. More specifically, a first search may be performed using information from a visual query, such that upon retrieving a first set of preliminary search results, additional entities that the preliminary search results refer to are identified. can be identified. Specifically, entities identified among some number of preliminary results that exceed a threshold may be determined to be sufficiently relevant to be included within a subsequent query.
図2、図3、および図4の視覚検索システム200、400および/または500のうちのいずれかを参照すると、コンピューティングシステムは、視覚クエリ入力データ204として提供される画像内に示されるオブジェクトを処理するためにエッジ検出アルゴリズムを実装し得る。詳細には、収集された画像は、エッジ検出アルゴリズム(たとえば、勾配フィルタ)を用いてフィルタリングされ、それにより、得られた画像を取得することができ、この画像は、行列内で画像内に含まれるオブジェクトの位置を判定する、水平および垂直方向に測定され得るバイナリ行列を表す。加えて、得られた画像は、さらに、エッジの改善された検出のためにラプラシアンおよび/またはガウスフィルタを使用して有利にフィルタリングされ得る。オブジェクトは、次いで、「AND」および「OR」ブール演算など、ブール演算を用いて、複数のトレーニング画像および/もしくは任意の種類の履歴画像ならびに/またはコンテキスト情報504と比較され得る。ブール演算比較の利用は、好ましい、非常に高速かつ効率的な比較を実現するが、ある状況においては、非ブール演算が所望されることがある。 Referring to any of the visual search systems 200, 400, and/or 500 of FIGS. 2, 3, and 4, the computing system searches for objects depicted in images provided as visual query input data 204. An edge detection algorithm may be implemented for processing. In particular, the collected images can be filtered using an edge detection algorithm (e.g., a gradient filter) to obtain the resulting image, which is contained within the image in the matrix. represents a binary matrix that can be measured horizontally and vertically that determines the position of an object to be displayed. In addition, the obtained images may be further advantageously filtered using Laplacian and/or Gaussian filters for improved detection of edges. The object may then be compared to multiple training images and/or any type of historical images and/or context information 504 using Boolean operations, such as "AND" and "OR" Boolean operations. Although the use of Boolean comparisons provides a preferred, very fast and efficient comparison, there are certain situations in which non-Boolean operations may be desired.
さらに、類似性アルゴリズムが図2、図3、および/または図4の視覚検索システム200、400および/または500によってアクセスされ得、ここで、アルゴリズムは、上記で説明したエッジ検出アルゴリズムにアクセスし、出力データを記憶し得る。追加および/または代替として、類似性アルゴリズムは、各画像および/またはクエリ入力データ204と、複数の他の画像および/もしくはクエリならびに/または任意の種類のトレーニングデータおよび/または履歴データであってよいコンテキスト情報504との間のペアワイズ類似性関数を推定し得る。ペアワイズ類似性関数は、2つのデータポイントが類似するか否かを記述し得る。 Additionally, a similarity algorithm may be accessed by the visual search system 200, 400 and/or 500 of FIGS. 2, 3, and/or 4, where the algorithm accesses the edge detection algorithm described above; Output data may be stored. Additionally and/or alternatively, the similarity algorithm may have each image and/or query input data 204 and a plurality of other images and/or queries and/or any type of training and/or historical data. A pairwise similarity function between the context information 504 may be estimated. A pairwise similarity function may describe whether two data points are similar.
追加または代替として、図2、図3、および/または図4の視覚検索システム200、400 および/または500は、視覚クエリ入力データ204として提供される画像を処理するためにクラスタリングアルゴリズムを実装し得る。検索システムは、クラスタリングアルゴリズムを実行し、推定されたペアワイズ類似性関数に基づいて、画像および/またはクエリをクラスタに割り当てることができる。クラスタの数は、クラスタリングアルゴリズムを実行するのに先立って未知であり得、画像/視覚クエリ入力データ204、画像/クエリの各ペアに対して推定されたペアワイズ類似性関数、および各クラスタに割り当てられた初期画像/クエリのランダムまたは疑似ランダム選択に基づいてクラスタリングアルゴリズムの実行ごとに異なり得る。 Additionally or alternatively, the visual search systems 200, 400, and/or 500 of FIGS. 2, 3, and/or 4 may implement a clustering algorithm to process images provided as visual query input data 204. . The search system can execute a clustering algorithm and assign images and/or queries to clusters based on the estimated pairwise similarity function. The number of clusters may be unknown prior to running the clustering algorithm and is based on the image/visual query input data 204, the pairwise similarity function estimated for each pair of images/queries, and the number of clusters assigned to each cluster. Each run of the clustering algorithm may vary based on random or pseudo-random selection of initial images/queries.
視覚検索システム200、400および/または500は、画像/クエリ入力データ204のセットに対して一度または複数回数クラスタリングアルゴリズムを実行し得る。いくつかの例示的な実施形態では、視覚検索システム200、400および/または500は、所定数繰り返してクラスタリングアルゴリズムを実行し得る。いくつかの例示的な実施形態では、視覚検索システム200、400および/または500は、クラスタリングアルゴリズムを実行し、非推移的であるペアワイズ類似性関数からの距離の測定値に達するまで、結果をアグリゲートし得る。 Visual search system 200, 400 and/or 500 may perform a clustering algorithm on the set of image/query input data 204 one or more times. In some example embodiments, visual search system 200, 400 and/or 500 may perform the clustering algorithm for a predetermined number of iterations. In some example embodiments, the visual search system 200, 400 and/or 500 executes a clustering algorithm and aggregates the results until it reaches a measure of distance from a pairwise similarity function that is non-transitive. Can be gated.
例示的な方法
図9は、本開示の例示的な実施形態による、より個人化された検索結果を提供するための例示的な方法1000のフローチャート図を示す。図9は、例示および考察のためにステップが特定の順序で実行されることを示すが、本開示の方法は、特定の例示された順序または配置に限定されない。方法1000の様々なステップは、本開示の範囲から逸脱せずに、様々な方法で、省かれること、再配置されること、組み合わされること、かつ/または適応されることが可能である。
Exemplary Method FIG. 9 depicts a flowchart diagram of an example method 1000 for providing more personalized search results, according to an example embodiment of the present disclosure. Although FIG. 9 shows steps performed in a particular order for purposes of illustration and discussion, the methods of this disclosure are not limited to the particular illustrated order or arrangement. Various steps of method 1000 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of this disclosure.
1002において、コンピューティングシステムは、視覚クエリを取得し得る。たとえば、コンピューティングシステム(たとえば、図1のユーザコンピューティングデバイス102および視覚検索システム104)は、視覚クエリ入力データ(たとえば、図2の視覚クエリ入力データ204)をユーザから取得し得る。 At 1002, a computing system may obtain a visual query. For example, a computing system (eg, user computing device 102 and visual search system 104 of FIG. 1) may obtain visual query input data (eg, visual query input data 204 of FIG. 2) from a user.
1004において、コンピューティングシステムは、複数の候補検索結果および対応する検索結通知オーバーレイを識別し得る。たとえば、コンピューティングシステムは、図3の視覚検索結果モデル202の出力として、現在の複数の候補検索結果、および視覚クエリ内に含まれた画像上のオーバーレイとして検索結果に対する視覚結果通知の提供に役立つユーザインターフェース上の対応する拡張されたオーバーレイを受信し得る。 At 1004, the computing system may identify multiple candidate search results and corresponding search result notification overlays. For example, the computing system is useful for providing visual result notifications for the search results as an output of the visual search results model 202 of FIG. A corresponding enhanced overlay on the user interface may be received.
より詳細には、コンピューティングシステムは、前に取得された視覚クエリをクエリ処理システム内に入力し得る。たとえば、コンピューティングシステムは、視覚クエリ入力データ204をクエリ処理システム202内に入力し得る。視覚クエリを入力するのに先立って、コンピューティングシステムは、エッジ検出アルゴリズムにアクセスし得る。より詳細には、収集された画像は、エッジ検出アルゴリズム(たとえば、勾配フィルタ)を用いてフィルタリングされ、それにより、得られる画像を取得することができ、この画像は、バイナリ行列内の画像に含まれるオブジェクトの位置を判定する水平および垂直方向で測定され得るバイナリ行列を表す。 More particularly, the computing system may input a previously obtained visual query into a query processing system. For example, a computing system may input visual query input data 204 into query processing system 202. Prior to inputting a visual query, the computing system may access an edge detection algorithm. More specifically, the collected images can be filtered using an edge detection algorithm (e.g., a gradient filter) to obtain a resulting image, which contains the images in a binary matrix. represents a binary matrix that can be measured in the horizontal and vertical directions to determine the position of the object being displayed.
1006において、コンピューティングシステムは、ユーザ中心の視覚的関心グラフを活用して、観測されたユーザの視覚的関心に基づいて、前に取得された複数の候補検索結果および対応する検索結果通知オーバーレイを選択および/またはフィルタリングし得る。ユーザ中心の視覚的関心グラフは、クエリ処理システム、たとえば、クエリ処理システム202、に含まれ得る。 At 1006, the computing system leverages the user-centered visual interest graph to generate a plurality of previously obtained candidate search results and corresponding search result notification overlays based on the observed user visual interests. May be selected and/or filtered. A user-centered visual interest graph may be included in a query processing system, e.g., query processing system 202.
1008において、コンピューティングシステムは、複数の候補検索結果のランク付けを生成し得る。たとえば、コンピューティングシステムは、ランク付けシステム、たとえば、ランク付けシステム402、の出力として、候補検索結果の現在のランク付けおよび対応する検索結果通知オーバーレイを受信し得る。 At 1008, the computing system may generate a ranking of the plurality of candidate search results. For example, a computing system may receive a current ranking of candidate search results and a corresponding search result notification overlay as an output of a ranking system, e.g., ranking system 402.
より詳細には、ランク付けシステムは、クエリ処理システム内に含まれたユーザに関連するユーザ固有のユーザの関心データに対する複数の候補検索結果の比較に少なくとも部分的に基づいて、ランク付けを生成し得る。たとえば、候補検索結果に関連する初期検索スコアを修正または再重み付けするために、項目に対する重みが加えられてよい。 More particularly, the ranking system generates a ranking based at least in part on a comparison of the plurality of candidate search results against user-specific user interest data associated with a user included within the query processing system. obtain. For example, weights may be added to items to modify or reweight initial search scores associated with candidate search results.
いくつかの実装形態では、視覚検索システムは、2つ以上の同じオブジェクトを含むとして識別され得る画像内の重複通知オーバーレイを明らかにし得る。視覚検索システムは、同じ検索結果を提供する複数の潜在的な候補検索結果通知オーバーレイの中から1つの潜在的な候補検索結果通知オーバーレイのみを出力し得る。 In some implementations, the visual search system may reveal duplicate notification overlays within images that may be identified as containing two or more of the same object. The visual search system may output only one potential candidate search result notification overlay among multiple potential candidate search result notification overlays that provide the same search result.
1010において、コンピューティングシステムは、少なくとも1つの選択された検索結果として、複数の候補検索結果のうちの少なくとも1つ、たとえば、出力データ404、を選択し得る。より詳細には、視覚検索システム400は、ランク付けに少なくとも部分的に基づいて、少なくとも1つの選択された検索結果として、複数の候補検索結果のうちの少なくとも1つを選択し、次いで、選択された検索結果に関連する画像の特定のサブ部分上にオーバーレイするために、少なくとも1つの選択された検索結果にそれぞれ関連する少なくとも1つの選択された視覚結果通知を提供し得る。そのような様式で、ユーザの関心は、個人化された検索結果を提供し、ユーザインターフェース内のクラッターを低減するために使用され得る。 At 1010, the computing system may select at least one of the plurality of candidate search results, eg, output data 404, as the at least one selected search result. More particularly, visual search system 400 selects at least one of the plurality of candidate search results as the at least one selected search result based at least in part on the ranking, and then selects at least one of the plurality of candidate search results as the selected search result. At least one selected visual result notification, each associated with the at least one selected search result, may be provided for overlaying on a particular sub-portion of the image associated with the search result. In such a manner, user interests may be used to provide personalized search results and reduce clutter within the user interface.
1012において、コンピューティングシステムは、少なくとも1つの選択された視覚結果通知をユーザに提供し得る。たとえば、コンピューティングシステムは、クエリ処理システム202の出力に基づいて予測される結果を含む出力データ404をユーザに提供し得る。 At 1012, the computing system may provide at least one selected visual result notification to the user. For example, the computing system may provide output data 404 to the user that includes predicted results based on the output of the query processing system 202.
図5は、図9で説明した例示的な方法の利点を例示する。602は、図9で説明した方法を使用しない、例示的な拡張現実ユーザインターフェースを示す。ユーザインターフェース602は、図9で説明した方法を用いずに、インターフェース602が、見通すために使用不可能であり、さらに、通知オーバーレイ604を使用困難な状態にするように、インターフェース602が通知オーバーレイ604で過剰に混雑していることを例示する。 FIG. 5 illustrates the advantages of the example method described in FIG. 9. 602 depicts an example augmented reality user interface that does not use the method described in FIG. 9. User interface 602 can be configured to display notification overlay 604 without using the method described in FIG. This is an example of excessive congestion.
対照的に、インターフェース606は、図9で説明した方法を使用した、例示的な拡張現実ユーザインターフェースを示す。インターフェース606は、図9で説明した方法を用いて、ユーザが、選択通知オーバーレイ604を依然として見通すことができ、同様に、すべての選択された通知オーバーレイ604に容易にアクセスすることができるように、選択された通知オーバーレイ604のみが表示されることを示す。 In contrast, interface 606 depicts an example augmented reality user interface using the method described in FIG. The interface 606 uses the method described in FIG. Indicates that only selected notification overlays 604 are displayed.
図10は、本開示の例示的な実施形態による例示的な方法1100のフローチャート図を示す。図10は、ステップが例示および考察のために特定の順序で実行されることを示すが、本開示の方法は、特定の例示された順序または配置に限定されない。方法1100の様々なステップは、本開示の範囲から逸脱せずに、様々な方法で、省かれること、再配置されること、組み合わされること、かつ/または適応されることが可能である。 FIG. 10 shows a flowchart diagram of an example method 1100 according to an example embodiment of the present disclosure. Although FIG. 10 shows steps being performed in a particular order for illustration and discussion, the methods of this disclosure are not limited to the particular illustrated order or arrangement. Various steps of method 1100 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of this disclosure.
1102において、コンピューティングシステムは、視覚クエリを取得し得る。たとえば、コンピューティングシステム(たとえば、図1のユーザコンピューティングデバイス102および視覚検索システム104)は、ユーザから視覚クエリ入力データ204を取得し得る。 At 1102, a computing system may obtain a visual query. For example, a computing system (eg, user computing device 102 and visual search system 104 of FIG. 1) may obtain visual query input data 204 from a user.
1104において、コンピューティングシステムは、複数の異なる項目を記述するグラフにアクセスし得る。詳細には、コンテンツ(たとえば、製品レビューなど、ユーザ生成コンテンツ)のそれぞれのセットは、複数の異なる項目の各々に関連付けられる。 At 1104, the computing system may access a graph that describes a plurality of different items. In particular, each set of content (eg, user-generated content, such as product reviews) is associated with each of a plurality of different items.
より詳細には、コンピューティングシステムは、前に取得された視覚クエリをクエリ処理システム内に入力し得る。たとえば、コンピューティングシステムは、視覚クエリ入力データ204をクエリ処理システム202内に入力し得る。 More particularly, the computing system may input a previously obtained visual query into a query processing system. For example, a computing system may input visual query input data 204 into query processing system 202.
1106において、コンピューティングシステムは、グラフから複数の選択された項目を選択し得る。より詳細には、クエリ処理システム202は、複数の異なる項目の階層表現であり得るグラフを活用し得る。グラフから複数の選択された項目を選択することは、視覚検索クエリに基づいて、画像内に示されたオブジェクト(たとえば、画像内に示された特定の映画)に対応する、グラフ内の1次項目を識別することを含み得る。次に、視覚検索システムは、グラフの階層表現内の1次項目に関する、グラフ内の1つまたは複数の追加項目を識別し、複数の選択された項目として1次項目および1つまたは複数の追加項目を選択することができる。 At 1106, the computing system may select multiple selected items from the graph. More particularly, query processing system 202 may utilize graphs, which may be hierarchical representations of multiple different items. Selecting multiple selected items from a graph is based on a visual search query, selecting the first order in the graph that corresponds to the object shown in the image (for example, a particular movie shown in the image). may include identifying the item. The visual search system then identifies one or more additional items in the graph with respect to the primary item in the hierarchical representation of the graph, and identifies the primary item and one or more additional items as multiple selected items. Items can be selected.
1108において、コンピューティングシステムは、検索結果として、コンテンツの組み合わされたセットをユーザに提供し得る。たとえば、コンピューティングシステムは、視覚検索結果モデル202の出力に基づいて、予測される結果を含む出力データ404をユーザに提供し得る。 At 1108, the computing system may provide the combined set of content to the user as a search result. For example, the computing system may provide the user with output data 404 that includes predicted results based on the output of the visual search results model 202.
図6は、図10で説明した例示的な方法の利点を例示する。ユーザインターフェース702は、図10で説明した方法を用いない例示的な検索結果を示す。ユーザインターフェース702は、図10で説明した方法を用いずに、検索結果が視覚クエリとして使用されるまったく同じオブジェクトのみに関する結果を含むことを例示する。対照的に、ユーザインターフェース704は、図10で説明した方法を使用する例示的な検索結果を示す。ユーザインターフェース704は、図10で説明した方法を用いて、検索結果が拡張され、複数のカノニカルエンティティに関連する結果を含むことを例示する。 FIG. 6 illustrates the advantages of the example method described in FIG. 10. User interface 702 shows exemplary search results that do not use the method described in FIG. 10. User interface 702 illustrates that the search results include results only for the exact same object used as the visual query, without using the method described in FIG. 10. In contrast, user interface 704 shows example search results using the method described in FIG. 10. User interface 704 illustrates that the search results are expanded to include results related to multiple canonical entities using the method described in FIG. 10.
図11は、本開示の例示的な実施形態による例示的な方法1200のフローチャート図を示す。図11は、ステップが例示および考察のために特定の順序で実行されることを示すが、本開示の方法は、特定の例示された順序または配置に限定されない。方法1200の様々なステップは、本開示の範囲から逸脱せずに、様々な方法で、省かれること、再配置されること、組み合わされること、かつ/または適応されることが可能である。 FIG. 11 shows a flowchart diagram of an example method 1200 according to an example embodiment of the disclosure. Although FIG. 11 shows steps being performed in a particular order for purposes of illustration and discussion, the methods of this disclosure are not limited to the particular illustrated order or arrangement. Various steps of method 1200 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of this disclosure.
1202において、コンピューティングシステムは、視覚クエリを取得し得る。たとえば、コンピューティングシステム(たとえば、図1のユーザコンピューティングデバイス102および視覚検索システム104)は、ユーザから視覚クエリ入力データ204を取得し得る。 At 1202, a computing system may obtain a visual query. For example, a computing system (eg, user computing device 102 and visual search system 104 of FIG. 1) may obtain visual query input data 204 from a user.
1204において、コンピューティングシステムは、視覚クエリ画像の1つまたは複数の構成特性、詳細には、画像の様々な属性(たとえば、識別された1つまたは複数のオブジェクトまでの距離、オブジェクトの数、オブジェクトの相対的な類似性、角度方位など)を識別し得る。 At 1204, the computing system determines one or more constituent characteristics of the visual query image, in particular various attributes of the image (e.g., distance to one or more identified objects, number of objects, object relative similarity, angular orientation, etc.).
より詳細には、コンピューティングシステムは、前に取得された視覚クエリをクエリ処理システム内に入力し得る。たとえば、コンピューティングシステムは、視覚クエリ入力データ204をクエリ処理システム202内に入力し得る。視覚クエリを入力するのに先立って、コンピューティングシステムは、エッジ検出アルゴリズムにアクセスし得る。より詳細には、収集された画像は、エッジ検出アルゴリズム(たとえば、勾配フィルタ)を用いてフィルタリングされ、それにより、得られた画像を取得することができ、この画像は、バイナリ行列内の画像内に含まれるオブジェクトの位置を判定する水平および垂直方向で測定され得るバイナリ行列を表す。 More particularly, the computing system may input a previously obtained visual query into a query processing system. For example, a computing system may input visual query input data 204 into query processing system 202. Prior to inputting a visual query, the computing system may access an edge detection algorithm. More specifically, the collected images can be filtered using an edge detection algorithm (e.g., a gradient filter) to obtain the resulting image, which is divided into two parts within the image in a binary matrix. represents a binary matrix that can be measured in the horizontal and vertical directions to determine the position of objects contained in the .
1206において、コンピューティングシステムは、視覚検索クエリがオブジェクト固有であるかまたはカテゴリー別であるかを判定し得る。より詳細には、クエリ処理システム202は、識別された構成特性を活用して、視覚クエリが、検索結果の拡張されたコーパスが関連するカテゴリー別クエリであるか、または視覚クエリ内で識別される1つまたは複数のオブジェクトに特に関係するオブジェクト固有のクエリであるかを予測し得る。 At 1206, the computing system may determine whether the visual search query is object-specific or categorical. More particularly, the query processing system 202 leverages the identified compositional characteristics to determine whether the visual query is a categorical query to which the expanded corpus of search results is associated or identified within the visual query. It may be possible to predict which object-specific queries are specifically related to one or more objects.
1208において、コンピューティングシステムは、1つまたは複数のオブジェクト固有の検索結果をユーザに提供し得る。たとえば、コンピューティングシステムは、視覚検索結果モデル202の出力に基づいて予測された結果を含む出力データ404をユーザに提供し得る。 At 1208, the computing system may provide one or more object-specific search results to the user. For example, the computing system may provide output data 404 to the user that includes predicted results based on the output of the visual search results model 202.
1210において、コンピューティングシステムは、1つまたは複数のカテゴリー別検索結果をユーザに提供し得る。たとえば、コンピューティングシステムは、視覚検索結果モデル202の出力に基づいて予測された結果を含む出力データ404をユーザに提供し得る。図7は、図11で説明した例示的な方法の利点を例示する。画像802および804は、同じ付随テキストクエリ(たとえば、「どちらが最も高い食物繊維を有するか?」)を有し得る画像の変形態の2つの例である。2つの例示的な画像802および804に応じて、視覚検索システムは、同じ付随するテキストクエリにもかかわらず、2つの異なる結果を戻すことになる。ユーザインターフェース806は、画像802に応じて、構成特性(たとえば、シリアルボックスが焦点の中央に位置する、シリアルボックスがおよそ90度の角度にある、すなわち、シリアルボックスが斜めに置かれていない、画像内に含まれるすべてのシリアルボックスが特に識別可能である)に基づいて、視覚検索システムが画像内で識別されたシリアルの中から最も高い食物繊維含有量を有するシリアルの画像を戻すことができることを例示する。対照的に、ユーザインターフェース808は、画像804に応じて、構成特性(たとえば、通路全体がビュー内にあるように画像が撮影された、シリアルボックスにまったく焦点が合っていない、シリアルボックスが30度の角度にある、すなわち、より斜めに置かれている)に基づいて、視覚検索システムがすべてのシリアルの中から最も高い食物繊維含有率を有するシリアルの画像を戻すことができることを例示する。 At 1210, the computing system may provide one or more categorized search results to the user. For example, the computing system may provide output data 404 to the user that includes predicted results based on the output of the visual search results model 202. FIG. 7 illustrates the advantages of the example method described in FIG. 11. Images 802 and 804 are two examples of image variations that may have the same accompanying text query (eg, "which has the highest dietary fiber?"). In response to the two example images 802 and 804, the visual search system will return two different results despite the same accompanying text query. The user interface 806 displays configuration characteristics (e.g., the cereal box is centered in focus, the cereal box is at an approximately 90 degree angle, i.e., the cereal box is not placed at an angle, the image (all cereal boxes contained within are specifically identifiable), the visual search system can return the image of the cereal with the highest dietary fiber content among the cereals identified within the image. Illustrate. In contrast, the user interface 808 displays configuration characteristics (e.g., the image was taken such that the entire aisle was in view, the cereal box was not in focus at all, the cereal box was 30 degrees We illustrate that a visual search system can return the image of the cereal with the highest dietary fiber content among all cereals based on the angle of (i.e., more diagonally placed).
図12は、本開示の例示的な実施形態による例示的な方法1300のフローチャート図を示す。図12は、ステップが例示および考察のために特定の順序で実行されることを示すが、本開示の方法は、特定の例示された順序または配置に限定されない。方法1300の様々なステップは、本開示の範囲から逸脱せずに、様々な方法で、省かれること、再配置されること、組み合わされること、かつ/または適応されることが可能である。 FIG. 12 shows a flowchart diagram of an example method 1300 according to an example embodiment of the disclosure. Although FIG. 12 shows steps being performed in a particular order for purposes of illustration and discussion, the methods of this disclosure are not limited to the particular illustrated order or arrangement. Various steps of method 1300 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of this disclosure.
1302において、コンピューティングシステムは、視覚クエリを取得し得る。たとえば、コンピューティングシステム(たとえば、図1のユーザコンピューティングデバイス102および視覚検索システム104)は、ユーザから視覚クエリ入力データ204を取得し得る。 At 1302, a computing system may obtain a visual query. For example, a computing system (eg, user computing device 102 and visual search system 104 of FIG. 1) may obtain visual query input data 204 from a user.
1304において、コンピューティングシステムは、視覚クエリに関連する1つまたは複数の追加のエンティティを識別し得る。具体的には、コンピュータ視覚検索システムは、1つまたは複数のコンテキスト信号または情報に基づいて、視覚検索クエリに関連する1つまたは複数のエンティティを識別し得る。 At 1304, the computing system may identify one or more additional entities related to the visual query. Specifically, a computer visual search system may identify one or more entities related to a visual search query based on one or more contextual signals or information.
より詳細には、コンピューティングシステムは、前に取得された視覚クエリをクエリ処理システム内に入力し得る。たとえば、コンピューティングシステムは、視覚クエリ入力データ204をクエリ処理システム202内に入力し得る。視覚クエリを入力するのに先立って、コンピューティングシステムは、エッジ検出アルゴリズムにアクセスし得る。より詳細には、収集された画像は、エッジ検出アルゴリズム(たとえば、勾配フィルタ)を用いてフィルタリングされ、それにより、得られる画像を取得することができ、この画像は、バイナリ行列内の画像に含まれるオブジェクトの位置を判定する水平および垂直方向で測定され得るバイナリ行列を表す。 More particularly, the computing system may input a previously obtained visual query into a query processing system. For example, a computing system may input visual query input data 204 into query processing system 202. Prior to inputting a visual query, the computing system may access an edge detection algorithm. More specifically, the collected images can be filtered using an edge detection algorithm (e.g., a gradient filter) to obtain a resulting image, which contains the images in a binary matrix. represents a binary matrix that can be measured in the horizontal and vertical directions to determine the position of the object being displayed.
1306において、コンピューティングシステムは、第1のエンティティと1つまたは複数の追加のエンティティの組合せに関するコンテンツに対して構成されたクエリを判定し得る。より詳細には、クエリ処理システム202は、複数のエンティティを活用して、第1のエンティティと1つまたは複数の追加のエンティティの組合せに関するコンテンツに対して構成されたクエリを判定し得る。詳細には、エンティティは、人々、オブジェクト、および/または、イベントなど、抽象エンティティを含み得る。 At 1306, the computing system may determine a query configured for content regarding the combination of the first entity and one or more additional entities. More particularly, query processing system 202 may utilize multiple entities to determine queries constructed for content regarding a combination of a first entity and one or more additional entities. In particular, entities may include abstract entities such as people, objects, and/or events.
1308において、コンピューティングシステムは、第1のエンティティと1つまたは複数の追加のエンティティの組合せに関するコンテンツのセットをユーザに提供し得る。たとえば、コンピューティングシステムは、視覚検索結果モデル202の出力に基づいて予測された結果を含む出力データ404をユーザに提供し得る。 At 1308, the computing system may provide the user with a set of content regarding the combination of the first entity and one or more additional entities. For example, the computing system may provide output data 404 to the user that includes predicted results based on the output of the visual search results model 202.
図8は、図12で説明した例示的な方法の利点を例示する。ユーザインターフェース904は、例示的な視覚クエリ902に基づく、図12で説明した方法を使用しない例示的な検索結果を示す。ユーザインターフェース904は、図12で説明した方法を用いずに、現在の技術は視覚クエリが与えられた複数のエンティティのクエリを構成することができないことにより、検索結果が視覚クエリ画像内で認識された個々のオブジェクトのみに関する結果を含むことを示す。対照的に、ユーザインターフェース906は、図12で説明した方法を使用して、同じ例示的な視覚クエリ902に基づく例示的な検索結果を示す。この方法は、視覚クエリ内で識別されたすべての顔を考慮に入れ、どれが識別されたすべての顔を含む1次予備的検索結果であったかを示す、特定の授賞式の検索結果をもたらす、それらの顔またはそれらの顔に関するイベントのうちのいくつかまたはすべてを含むクエリを構成する。906は、図12で説明した方法を用いて、検索結果が拡張され、構成特性に基づいて関心のある複数のオブジェクトを考慮に入れることができることを例示する。 FIG. 8 illustrates the advantages of the example method described in FIG. 12. User interface 904 shows example search results based on example visual query 902 and without using the method described in FIG. The user interface 904 shows that without using the method described in FIG. Indicates that results are included only for individual objects. In contrast, user interface 906 shows example search results based on the same example visual query 902 using the method described in FIG. 12. The method takes into account all the faces identified in the visual query and yields search results for a particular awards ceremony indicating which were the primary preliminary search results that included all the faces identified. Construct a query that includes some or all of those faces or events about those faces. 906 illustrates that using the method described in FIG. 12, the search results can be expanded to take into account multiple objects of interest based on configuration characteristics.
追加の開示
本明細書で論じる技術は、サーバ、データベース、ソフトウェアアプリケーション、および他のコンピュータベースシステム、ならびに行われるアクションおよびそのようなシステム間で送られる情報を参照する。コンピュータベースシステムの固有の柔軟性は、構成要素同士の間のタスクおよび機能性の多種多様な考えられる構成、組合せ、および分割を可能にする。たとえば、本明細書で論じたプロセスは、単一のデバイスまたは構成要素または組み合わせて動作する複数のデバイスまたは構成要素を使用して実装され得る。データベースおよびアプリケーションは、単一システム上で実装されてよく、または複数のシステムにわたって分散されてもよい。分散された構成要素は、連続的にまたは並列に動作し得る。
Additional Disclosures The techniques discussed herein refer to servers, databases, software applications, and other computer-based systems, and the actions taken and information sent between such systems. The inherent flexibility of computer-based systems allows for a wide variety of possible configurations, combinations, and divisions of tasks and functionality between components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components operating in combination. Databases and applications may be implemented on a single system or distributed across multiple systems. Distributed components may operate serially or in parallel.
本主題は、その様々な特定の例示的な実施形態に関して詳細に説明されているが、各例は、本開示の限定ではなく、説明として提供される。当業者は、前述の理解を得ると、そのような実施形態に対する変更、変形態、および均等物を容易に作り出すことができる。したがって、本開示は、当業者に容易に明らかになるように、主題に対するそのような修正、変形体、およびまたは追加の包含を妨げない。たとえば、一実施形態の一部として例示または説明した特徴は、またさらなる実施形態をもたらすために別の実施形態とともに使用され得る。したがって、本開示はそのような変更、変形態、および均等物を包含とすることが意図される。 Although the present subject matter has been described in detail with respect to various specific exemplary embodiments thereof, each example is offered by way of illustration rather than limitation of the disclosure. Modifications, variations, and equivalents to such embodiments can be readily devised by those skilled in the art once armed with the foregoing understanding. Accordingly, this disclosure does not preclude the inclusion of such modifications, variations, and/or additions to the subject matter as would be readily apparent to those skilled in the art. For example, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Accordingly, this disclosure is intended to cover such modifications, variations, and equivalents.
100 コンピューティングシステム、システム
102 ユーザコンピューティングデバイス
104 視覚検索システム
112 プロセッサ
114 メモリ
116 データ
118 命令
122 ユーザ入力構成要素
124 カメラ
126 カメラアプリケーション
128 オブジェクト検出器
130 ユーザインターフェース生成器、コンテンツキャッシュ
132 オンデバイストラッカー
134 コンテンツキャッシュ
136 フロントエンドサーバ
138 コンテンツデータ記憶ユニット
140 バックエンドサーバ
142 プロセッサ
146 メモリ
148 データ
150 命令
152 オブジェクト認識器
154 クエリ処理システム
156 コンテンツランク付けシステム
158 ユーザインターフェース
180 ネットワーク
200 視覚検索システム、システム
202 クエリ処理システム、ユーザ関心システム、視覚検索結果モデル
204 視覚クエリ、入力データ、視覚クエリ入力データ、画像/視覚クエリ入力データ、画像/クエリ入力データ
206 出力データ、候補検索結果
400 視覚検索システム、システム
402 ランク付けシステム
404 出力データ、出力
500 視覚検索システム、システム
502 コンテキスト構成要素
504 コンテキスト情報
602 インターフェース
604 通知オーバーレイ
606 インターフェース
702 ユーザインターフェース
704 ユーザインターフェース
802 画像
804 画像
806 ユーザインターフェース
808 ユーザインターフェース
902 視覚クエリ
904 ユーザインターフェース
906 ユーザインターフェース
1000 方法
1100 方法
1200 方法
1300 方法
100 computing systems, systems
102 User Computing Device
104 Visual Search System
112 processor
114 Memory
116 data
118 Command
122 User Input Component
124 camera
126 Camera Application
128 Object Detector
130 User interface generator, content cache
132 On-device tracker
134 Content cache
136 Front-end server
138 Content data storage unit
140 backend server
142 processor
146 Memory
148 data
150 instructions
152 Object Recognizer
154 Query Processing System
156 Content Ranking System
158 User Interface
180 network
200 Visual Search System, System
202 Query processing systems, user interest systems, visual search result models
204 visual query, input data, visual query input data, image/visual query input data, image/query input data
206 Output data, candidate search results
400 Visual Search System, System
402 Ranking System
404 Output data, output
500 Visual Search System, System
502 Context Component
504 Context information
602 Interface
604 Notification Overlay
606 Interface
702 User Interface
704 User Interface
802 images
804 images
806 User Interface
808 User Interface
902 visual queries
904 User Interface
906 User Interface
1000 ways
1100 methods
1200 methods
1300 methods
Claims (38)
1つまたは複数のコンピューティングデバイスを含むコンピューティングシステムが、ユーザに関連する視覚検索クエリを取得するステップであって、前記視覚検索クエリが画像を含む、取得するステップと、
前記コンピューティングシステムが、前記視覚検索クエリに対する複数の候補検索結果を識別するステップであって、各候補検索結果が、前記画像の特定のサブ部分に関連付けられ、複数の候補視覚結果通知が、前記複数の候補検索結果にそれぞれ関連付けられる、識別するステップと、
前記コンピューティングシステムが、前記ユーザに関連し、かつ前記ユーザの視覚的関心を記述する、ユーザ固有のユーザの関心データにアクセスするステップと、
前記コンピューティングシステムが、前記ユーザに関連する前記ユーザ固有のユーザの関心データに対する前記複数の候補検索結果の比較に少なくとも部分的に基づいて、前記複数の候補検索結果のランク付けを生成するステップと、
前記コンピューティングシステムが、前記ランク付けに少なくとも部分的に基づいて、少なくとも1つの選択された検索結果として、前記複数の候補検索結果のうちの少なくとも1つを選択するステップと、
前記コンピューティングシステムが、前記選択された検索結果に関連する前記画像の前記特定のサブ部分上にオーバーレイするために、前記少なくとも1つの選択された検索結果にそれぞれ関連する少なくとも1つの選択された視覚結果通知を提供するステップと
を含む、コンピュータ実装方法。 A computer-implemented method for providing personalized visual search query result notifications in a user interface overlaid on a statue, the method comprising:
A computing system including one or more computing devices obtains a visual search query associated with a user, the visual search query including an image;
the computing system identifying a plurality of candidate search results for the visual search query, each candidate search result being associated with a particular sub-portion of the image; identifying, each associated with a plurality of candidate search results;
the computing system accessing user-specific user interest data related to the user and describing the user's visual interests;
the computing system generating a ranking of the plurality of candidate search results based at least in part on a comparison of the plurality of candidate search results to user interest data specific to the user associated with the user; ,
the computing system selecting at least one of the plurality of candidate search results as at least one selected search result based at least in part on the ranking;
at least one selected visual field each associated with the at least one selected search result for overlaying the particular sub-portion of the image associated with the selected search result. and providing results notification.
前記ユーザに対して識別された視覚的関心に対してオーバーレイされた可変加重関心バイアス
を含む、請求項1に記載のコンピュータ実装方法。 the user-specific interest data is based at least in part on images captured by the user, wherein the user-specific interest data is based at least in part on images captured by the user;
2. The computer-implemented method of claim 1, comprising a variable weighted interest bias overlaid on visual interests identified for the user.
請求項1に記載のコンピュータ実装方法。 2. The variable weighted interest bias assigned to the identified visual interest decays over time such that the user-specific interest data is based at least in part on the time frame of interest expressed. Computer implementation method.
1つまたは複数のプロセッサと、
前記1つまたは複数のプロセッサによって実行されると、前記コンピューティングシステムに動作を実行させる命令を記憶した、1つまたは複数の非一時的コンピュータ可読媒体と
を含み、前記動作が、
視覚検索クエリを取得することであって、前記視覚検索クエリが、オブジェクトを示す画像を含む、取得することと、
複数の異なる項目を記述するグラフにアクセスすることであって、コンテンツのそれぞれのセットが、前記複数の異なる項目の各々に関連付けられる、アクセスすることと、
前記視覚検索クエリに基づいて、前記画像が示す前記オブジェクトに対する前記グラフから複数の選択された項目を選択することと、
前記視覚検索クエリに応じて、検索結果として、コンテンツの組み合わされたセットを戻すことであって、コンテンツの前記組み合わされたセットが、前記複数の選択された項目の各々に関連するコンテンツの前記それぞれのセットの少なくとも一部分を含む、戻すことと
を含む
コンピューティングシステム。 A computing system that returns content about a plurality of canonical items in response to a visual search query, the computing system comprising:
one or more processors;
one or more non-transitory computer-readable media storing instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising:
Obtaining a visual search query, the visual search query including an image depicting an object;
accessing a graph describing a plurality of different items, a respective set of content being associated with each of the plurality of different items;
selecting a plurality of selected items from the graph for the object represented by the image based on the visual search query;
in response to the visual search query, returning a combined set of content as a search result, wherein the combined set of content includes the respective pieces of content associated with each of the plurality of selected items; and a computing system comprising at least a portion of a set of.
前記視覚検索クエリに基づいて、前記グラフから前記複数の選択された項目を選択することが、
前記視覚検索クエリに基づいて、前記画像内に示された前記オブジェクトに対応する、前記グラフ内の1次項目を識別することと、
前記グラフの前記階層表現内の前記1次項目に関する、前記グラフ内の1つまたは複数の追加項目を識別することと、
前記複数の選択された項目として、前記1次項目および前記1つまたは複数の追加項目を選択することと
を含む、請求項12に記載のコンピューティングシステム。 the graph describing the plurality of different items includes a hierarchical representation of the plurality of different items;
selecting the plurality of selected items from the graph based on the visual search query;
identifying a primary item in the graph that corresponds to the object shown in the image based on the visual search query;
identifying one or more additional items in the graph with respect to the primary item in the hierarchical representation of the graph;
13. The computing system of claim 12, comprising selecting the primary item and the one or more additional items as the plurality of selected items.
前記視覚検索クエリに基づいて、前記グラフから前記複数の選択された項目を選択することが、
前記複数のクラスタの1次クラスタを識別するためにエッジしきい値アルゴリズムを実行することと、
前記複数の選択された項目として、前記1次クラスタ内に含まれた前記ノードを選択することと
を含む、請求項16に記載のコンピューティングシステム。 the plurality of nodes of the graph are arranged into a plurality of clusters,
selecting the plurality of selected items from the graph based on the visual search query;
performing an edge thresholding algorithm to identify a primary cluster of the plurality of clusters;
17. The computing system of claim 16, further comprising selecting the nodes included in the primary cluster as the plurality of selected items.
前記画像が示した前記オブジェクトに視覚的に類似する複数の視覚的類似ノードを識別するためにエッジしきい値アルゴリズムを実行することと、
前記複数の選択された項目として、前記視覚的類似ノードを選択することと
を含む、請求項16に記載のコンピューティングシステム。 selecting the plurality of selected items from the graph based on the visual search query;
performing an edge thresholding algorithm to identify a plurality of visually similar nodes that are visually similar to the object depicted by the image;
and selecting the visually similar nodes as the plurality of selected items.
前記ユーザ生成コンテンツに対する視覚的記述性のレベルを判定するために、前記ユーザ生成コンテンツ内に含まれた語を意味的に解析すること
を含み、
検索結果としてコンテンツの前記組み合わされたセットを戻すことが、前記複数の選択された項目のうちの少なくとも1つに対して、視覚的記述性のその判定されたレベルに少なくとも基づいて、前記ユーザ生成コンテンツをコンテンツの前記組み合わされたセット内に含めるかどうかを判定することを含む
請求項13に記載のコンピューティングシステム。 The said operation is
semantically analyzing words included within the user-generated content to determine a level of visual descriptiveness for the user-generated content;
returning the combined set of content as a search result for at least one of the plurality of selected items, based at least on the determined level of visual descriptiveness of the user generated content; 14. The computing system of claim 13, comprising determining whether to include content within the combined set of content.
ユーザに関連し、前記ユーザの視覚的関心を記述する、ユーザ固有のユーザの関心データにアクセスすること
をさらに含み、
前記視覚検索クエリに基づいて、前記グラフから前記複数の選択された項目を選択することが、前記視覚検索クエリに基づいて、かつ前記ユーザ固有のユーザの関心データにさらに基づいて、前記グラフから前記複数の選択された項目を選択することを含む
請求項12に記載のコンピューティングシステム。 The said operation is
further comprising accessing user-specific user interest data related to the user and describing the visual interests of the user;
selecting the plurality of selected items from the graph based on the visual search query; and selecting the plurality of selected items from the graph based on the visual search query and further based on user interest data specific to the user. 13. The computing system of claim 12, comprising selecting a plurality of selected items.
1つまたは複数のプロセッサと、
前記1つまたは複数のプロセッサによって実行されると、前記コンピューティングシステムに動作を実行させる命令を記憶した、1つまたは複数の非一時的コンピュータ可読媒体と
を含み、前記動作が、
視覚検索クエリを取得することであって、前記視覚検索クエリが、1つまたは複数のオブジェクトを示す画像を含む、取得することと、
前記視覚検索クエリ内に含まれた前記画像の1つまたは複数の構成特性を識別することと、
前記視覚検索クエリ内に含まれた前記画像の前記1つまたは複数の構成特性に少なくとも部分的に基づいて、前記視覚検索クエリが、前記視覚検索クエリ内に含まれた前記画像内で識別された前記1つまたは複数のオブジェクトに特に関係するオブジェクト固有のクエリを含むかどうか、または前記視覚検索クエリが、前記視覚検索クエリ内に含まれた前記画像内で識別された前記1つまたは複数のオブジェクトの一般的なカテゴリーに関係するカテゴリー別クエリを含むかどうかを判定することと、
前記視覚検索クエリがオブジェクト固有のクエリを含むと判定されるとき、前記視覚検索クエリ内に含まれた前記画像内で識別された前記1つまたは複数のオブジェクトに特に関係する、1つまたは複数のオブジェクト固有の検索結果を戻すことと、
前記視覚検索クエリがカテゴリー別クエリを含むと判定されるとき、前記視覚検索クエリ内に含まれた前記画像内で識別された前記1つまたは複数のオブジェクトの前記一般的なカテゴリーに関係する、1つまたは複数のカテゴリー別検索結果を戻すことと
を含む
コンピューティングシステム。 A computing system for disambiguating between object-specific and categorical visual queries, comprising:
one or more processors;
one or more non-transitory computer-readable media storing instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising:
Obtaining a visual search query, the visual search query including an image depicting one or more objects;
identifying one or more constituent characteristics of the image included within the visual search query;
The visual search query was identified within the image included within the visual search query based at least in part on the one or more constituent characteristics of the image included within the visual search query. whether the visual search query includes an object-specific query specifically related to the one or more objects identified within the image included within the visual search query; determining whether the query includes a categorical query related to a general category of;
one or more objects specifically related to the one or more objects identified within the image included within the visual search query when the visual search query is determined to include an object-specific query. returning object-specific search results;
1 relating to the general category of the one or more objects identified in the images included in the visual search query when the visual search query is determined to include a categorical query; and returning one or more categorized search results.
前記画像内の前記1つまたは複数のオブジェクトのうちの少なくとも1つがその下で分類される、オブジェクトの個別のカテゴリーの収集物を生成することと、
前記視覚検索クエリに基づいて、オブジェクトの複数の選択された個別のカテゴリーを前記収集物から選択することと、
前記視覚検索クエリに応じて、検索結果としてコンテンツの組み合わされたセットを戻すことであって、コンテンツの前記組み合わされたセットが、オブジェクトの前記複数の選択された個別のカテゴリーの各々に関連する結果を含む、戻すことと
を含む、請求項22に記載のコンピューティングシステム。 returning the one or more categorical search results related to the general category of the one or more objects identified in the images included in the visual search query;
generating a collection of distinct categories of objects under which at least one of the one or more objects in the image is classified;
selecting a plurality of selected distinct categories of objects from the collection based on the visual search query;
in response to the visual search query, returning a combined set of content as a search result, the combined set of content relating to each of the plurality of selected distinct categories of objects; 23. The computing system of claim 22, comprising: reverting.
視覚検索クエリを取得するステップであって、前記視覚検索クエリが、第1のエンティティを示す画像を含む、取得するステップと、
1つまたは複数のコンテキスト信号に少なくとも部分的に基づいて、前記視覚検索クエリに関連する1つまたは複数の追加のエンティティを識別するステップと、
前記第1のエンティティと前記1つまたは複数の追加のエンティティの組合せに関するコンテンツに対して構成されたクエリを判定するステップと、
前記視覚検索クエリに応じて、コンテンツのセットを戻すステップであって、コンテンツの前記セットが、前記構成されたクエリに応じ、かつ前記第1のエンティティと前記1つまたは複数の追加のエンティティの前記組合せに関する、少なくとも1つのコンテンツ項目を含む、戻すステップと
を含む、コンピュータ実装方法。 A computer-implemented method for returning content about a plurality of configured entities to a visual search query, the method comprising:
Obtaining a visual search query, wherein the visual search query includes an image depicting a first entity;
identifying one or more additional entities related to the visual search query based at least in part on one or more context signals;
determining a query configured for content regarding the combination of the first entity and the one or more additional entities;
returning a set of content in response to the visual search query, wherein the set of content is responsive to the configured query and in response to the first entity and the one or more additional entities; and returning at least one content item regarding the combination.
前記1つまたは複数のコンテキスト信号に少なくとも部分的に基づいて、前記視覚検索クエリに関連する前記1つまたは複数の追加のエンティティを識別するステップが、前記画像以外の前記ウェブドキュメントの1つまたは複数の部分が参照する、前記1つまたは複数の追加のエンティティを識別するステップを含む
請求項31に記載のコンピュータ実装方法。 the image included in the visual search query is present in a web document;
identifying the one or more additional entities related to the visual search query based at least in part on the one or more context signals in one or more of the web documents other than the image; 32. The computer-implemented method of claim 31, comprising identifying the one or more additional entities to which a portion of .
前記視覚検索クエリに関連する前記画像の追加のインスタンスを含む1つまたは複数の追加のウェブドキュメントを識別するステップと、
前記1つまたは複数の追加のウェブドキュメントが参照する前記1つまたは複数の追加のエンティティを識別するステップと
を含む、請求項31に記載のコンピュータ実装方法。 identifying the one or more additional entities associated with the visual search query based at least in part on the one or more context signals;
identifying one or more additional web documents containing additional instances of the image related to the visual search query;
and identifying the one or more additional entities to which the one or more additional web documents refer.
前記視覚検索クエリ内に含まれた前記画像に関連するテキストメタデータにアクセスするステップであって、前記テキストメタデータが、前記画像のコンテンツを記述する、アクセスするステップと、
前記視覚検索クエリ内に含まれた前記画像に関連する前記テキストメタデータに基づいて、前記1つまたは複数の追加のエンティティを識別するステップと
を含む、請求項31に記載のコンピュータ実装方法。 identifying the one or more additional entities associated with the visual search query based at least in part on the one or more context signals;
accessing textual metadata associated with the image included in the visual search query, the textual metadata describing content of the image;
and identifying the one or more additional entities based on the textual metadata associated with the image included in the visual search query.
前記視覚検索クエリ内に含まれた前記画像に関連するロケーションまたは時間メタデータにアクセスするステップと、
前記視覚検索クエリ内に含まれた前記画像に関連する前記ロケーションまたは前記時間メタデータに基づいて、前記1つまたは複数の追加のエンティティを識別するステップと
を含む、請求項31に記載のコンピュータ実装方法。 identifying the one or more additional entities associated with the visual search query based at least in part on the one or more context signals;
accessing location or time metadata associated with the images included in the visual search query;
and identifying the one or more additional entities based on the location or the temporal metadata associated with the image included in the visual search query. Method.
前記視覚検索クエリに関連する前記画像内に示された1つまたは複数の2次エンティティを識別するステップと、
前記第1のエンティティと前記1つまたは複数の2次エンティティの予備的組合せに基づいて、予備的検索結果のセットを取得するステップと、
予備的検索結果の前記セットが参照する前記1つまたは複数の追加のエンティティを識別するステップと
を含む、請求項31に記載のコンピュータ実装方法。 identifying the one or more entities associated with the visual search query based at least in part on the one or more context signals;
identifying one or more secondary entities shown in the image that are relevant to the visual search query;
obtaining a preliminary set of search results based on the preliminary combination of the first entity and the one or more secondary entities;
and identifying the one or more additional entities to which the set of preliminary search results refers.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/025,435 US20220092105A1 (en) | 2020-09-18 | 2020-09-18 | Intelligent Systems and Methods for Visual Search Queries |
US17/025,435 | 2020-09-18 | ||
JP2021152254A JP7331054B2 (en) | 2020-09-18 | 2021-09-17 | Intelligent system and method for visual search queries |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021152254A Division JP7331054B2 (en) | 2020-09-18 | 2021-09-17 | Intelligent system and method for visual search queries |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2023162232A true JP2023162232A (en) | 2023-11-08 |
Family
ID=77838742
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021152254A Active JP7331054B2 (en) | 2020-09-18 | 2021-09-17 | Intelligent system and method for visual search queries |
JP2023129236A Pending JP2023162232A (en) | 2020-09-18 | 2023-08-08 | Intelligent systems and methods for visual search queries |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021152254A Active JP7331054B2 (en) | 2020-09-18 | 2021-09-17 | Intelligent system and method for visual search queries |
Country Status (5)
Country | Link |
---|---|
US (1) | US20220092105A1 (en) |
EP (3) | EP3971735A1 (en) |
JP (2) | JP7331054B2 (en) |
KR (1) | KR20220037996A (en) |
CN (1) | CN114329069A (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220300550A1 (en) * | 2021-03-19 | 2022-09-22 | Google Llc | Visual Search via Free-Form Visual Feature Selection |
US11899682B2 (en) * | 2021-09-08 | 2024-02-13 | Microsoft Technology Licensing, Llc | Generating and presenting a searchable graph based on a graph query |
Family Cites Families (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2010118019A (en) * | 2008-11-14 | 2010-05-27 | Sharp Corp | Terminal device, distribution device, control method of terminal device, control method of distribution device, control program, and recording medium |
US8670597B2 (en) * | 2009-08-07 | 2014-03-11 | Google Inc. | Facial recognition with social network aiding |
US9135277B2 (en) * | 2009-08-07 | 2015-09-15 | Google Inc. | Architecture for responding to a visual query |
JP5830784B2 (en) * | 2011-06-23 | 2015-12-09 | サイバーアイ・エンタテインメント株式会社 | Interest graph collection system by relevance search with image recognition system |
US20140143250A1 (en) * | 2012-03-30 | 2014-05-22 | Xen, Inc. | Centralized Tracking of User Interest Information from Distributed Information Sources |
US9066200B1 (en) * | 2012-05-10 | 2015-06-23 | Longsand Limited | User-generated content in a virtual reality environment |
US20170206276A1 (en) * | 2016-01-14 | 2017-07-20 | Iddo Gill | Large Scale Recommendation Engine Based on User Tastes |
US10489410B2 (en) * | 2016-04-18 | 2019-11-26 | Google Llc | Mapping images to search queries |
TWI617930B (en) * | 2016-09-23 | 2018-03-11 | 李雨暹 | Method and system for sorting a search result with space objects, and a computer-readable storage device |
US11004131B2 (en) * | 2016-10-16 | 2021-05-11 | Ebay Inc. | Intelligent online personal assistant with multi-turn dialog based on visual search |
US11126653B2 (en) * | 2017-09-22 | 2021-09-21 | Pinterest, Inc. | Mixed type image based search results |
US11609919B2 (en) * | 2019-07-30 | 2023-03-21 | Walmart Apollo, Llc | Methods and apparatus for automatically providing personalized search results |
-
2020
- 2020-09-18 US US17/025,435 patent/US20220092105A1/en active Pending
-
2021
- 2021-09-16 KR KR1020210123884A patent/KR20220037996A/en not_active Application Discontinuation
- 2021-09-17 EP EP21197571.9A patent/EP3971735A1/en active Pending
- 2021-09-17 JP JP2021152254A patent/JP7331054B2/en active Active
- 2021-09-17 EP EP23173793.3A patent/EP4224339A1/en active Pending
- 2021-09-17 EP EP23173794.1A patent/EP4224340A1/en active Pending
- 2021-09-22 CN CN202111106870.3A patent/CN114329069A/en active Pending
-
2023
- 2023-08-08 JP JP2023129236A patent/JP2023162232A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP7331054B2 (en) | 2023-08-22 |
EP4224340A1 (en) | 2023-08-09 |
JP2022051559A (en) | 2022-03-31 |
EP3971735A1 (en) | 2022-03-23 |
KR20220037996A (en) | 2022-03-25 |
CN114329069A (en) | 2022-04-12 |
EP4224339A1 (en) | 2023-08-09 |
US20220092105A1 (en) | 2022-03-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10043109B1 (en) | Attribute similarity-based search | |
CN108701118B (en) | Semantic category classification | |
US10956007B2 (en) | Electronic device and method for providing search result thereof | |
JP6759844B2 (en) | Systems, methods, programs and equipment that associate images with facilities | |
CN107209762B (en) | Visual interactive search | |
US9600499B2 (en) | System for collecting interest graph by relevance search incorporating image recognition system | |
US20180181569A1 (en) | Visual category representation with diverse ranking | |
US20200342320A1 (en) | Non-binary gender filter | |
US11036790B1 (en) | Identifying visual portions of visual media files responsive to visual portions of media files submitted as search queries | |
KR101017016B1 (en) | Method, system and computer-readable recording medium for providing information on goods based on image matching | |
JP2023162232A (en) | Intelligent systems and methods for visual search queries | |
US9830534B1 (en) | Object recognition | |
EP3304346A1 (en) | Task-focused search by image | |
CN113039539A (en) | Extending search engine capabilities using AI model recommendations | |
US9619519B1 (en) | Determining user interest from non-explicit cues | |
KR102586170B1 (en) | Electronic device and method for providing search result thereof | |
US8694529B1 (en) | Refinement surfacing for search hierarchies | |
Khanwalkar et al. | Exploration of large image corpuses in virtual reality | |
US20220300550A1 (en) | Visual Search via Free-Form Visual Feature Selection | |
US11500926B2 (en) | Cascaded multi-tier visual search system | |
KR20210127035A (en) | Method and system for providing search terms whose popularity increases rapidly | |
CN112020712A (en) | Digital supplemental association and retrieval for visual search | |
CN115795184B (en) | RPA-based scene get-on point recommendation method and device | |
US20240104150A1 (en) | Presenting Related Content while Browsing and Searching Content | |
US11830056B2 (en) | Providing local recommendations based on images of consumable items |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230906 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20230906 |