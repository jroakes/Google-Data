CN115917647A - Automatic non-linear editing style transfer - Google Patents
Automatic non-linear editing style transfer Download PDFInfo
- Publication number
- CN115917647A CN115917647A CN202080102660.4A CN202080102660A CN115917647A CN 115917647 A CN115917647 A CN 115917647A CN 202080102660 A CN202080102660 A CN 202080102660A CN 115917647 A CN115917647 A CN 115917647A
- Authority
- CN
- China
- Prior art keywords
- content
- computer
- source video
- editing
- video
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012546 transfer Methods 0.000 title claims abstract description 39
- 230000033001 locomotion Effects 0.000 claims abstract description 78
- 238000000034 method Methods 0.000 claims abstract description 48
- 238000001514 detection method Methods 0.000 claims abstract description 22
- 230000000007 visual effect Effects 0.000 claims description 30
- 230000007704 transition Effects 0.000 claims description 14
- 230000008859 change Effects 0.000 claims description 8
- 238000009432 framing Methods 0.000 claims description 6
- 230000000087 stabilizing effect Effects 0.000 claims 1
- 238000004590 computer program Methods 0.000 abstract description 6
- 238000003860 storage Methods 0.000 description 17
- 230000015654 memory Effects 0.000 description 13
- 238000004458 analytical method Methods 0.000 description 11
- 230000008569 process Effects 0.000 description 10
- 238000010586 diagram Methods 0.000 description 9
- 230000000694 effects Effects 0.000 description 8
- 238000012545 processing Methods 0.000 description 8
- 230000014509 gene expression Effects 0.000 description 6
- 238000005286 illumination Methods 0.000 description 5
- 230000008901 benefit Effects 0.000 description 4
- 230000009471 action Effects 0.000 description 3
- 238000010801 machine learning Methods 0.000 description 3
- 238000004519 manufacturing process Methods 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 230000006855 networking Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 238000004040 coloring Methods 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 230000008451 emotion Effects 0.000 description 2
- 230000004438 eyesight Effects 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 238000005259 measurement Methods 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 241001465754 Metazoa Species 0.000 description 1
- 238000012952 Resampling Methods 0.000 description 1
- BQCADISMDOOEFD-UHFFFAOYSA-N Silver Chemical compound [Ag] BQCADISMDOOEFD-UHFFFAOYSA-N 0.000 description 1
- 238000009825 accumulation Methods 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000004075 alteration Effects 0.000 description 1
- 238000010923 batch production Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 238000010411 cooking Methods 0.000 description 1
- 238000005520 cutting process Methods 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000005562 fading Methods 0.000 description 1
- 238000001914 filtration Methods 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 230000004313 glare Effects 0.000 description 1
- 230000009191 jumping Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 230000036651 mood Effects 0.000 description 1
- 238000013515 script Methods 0.000 description 1
- 230000035807 sensation Effects 0.000 description 1
- 229910052709 silver Inorganic materials 0.000 description 1
- 239000004332 silver Substances 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/02—Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers
- G11B27/031—Electronic editing of digitised analogue information signals, e.g. audio or video signals
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
- G11B27/28—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording
Abstract
The present disclosure provides systems, methods, and computer program products for performing automated non-linear editing style transfers. A computer-implemented method may include: determining one or more shot boundaries in the video; analyzing the identified content in each of the one or more shots in the video based on performing object detection; determining an editing style for a respective shot based at least in part on measuring motion across frames within each of one or more shots in a video; the method further includes determining a content snippet to adjust from the target set of content based on analyzing the target set of content in view of the identified content and the determined editing style of the footage from the video, and automatically adjusting the content snippet based at least in part on modifying the content snippet from the target set of content with the determined editing style of the footage from the video.
Description
Technical Field
The present disclosure relates generally to computer vision and automated processing of visual content. More particularly, the present disclosure relates to systems, methods, and computer program products for performing automatic nonlinear editing style transfers.
Background
Advances in computer vision and machine learning have improved the detection and classification of objects within visual content. For example, many such improvements have been directed to perceptual applications in the fields of autonomous vehicles, robotics, and safety. However, such applications typically focus on people and other objects in the scene, without particular regard to how the camera depicts the various scenes.
Many websites, including content sharing and social networking websites, allow users to store and share image and video content with other users. Furthermore, smart phones with one or more high quality digital cameras, large amounts of storage space, and mobile broadband have made it easy for users to capture and distribute visual content from anywhere. However, while the amount of newly generated visual content continues to increase over time, most users do not have professional editing skills, access to post-production editing software, or time to perform complex edits across large amounts of original visual content.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or may be learned from the description, or may be learned through practice of the embodiments.
One example aspect of the present disclosure is directed to a system for performing automatic nonlinear style transitions from source content to target content, for example, by determining one or more shot boundaries in a video based on analyzing the video, analyzing identified content in respective shots based on performing object detection on each of the one or more shots in the video, determining an editing style of the respective shot based at least in part on measuring motion across frames within each of the one or more shots in the video, determining a content segment to adjust from a target content set based on analyzing the target content set in view of the identified content and the determined editing style of the shots from the video, and automatically adjusting the content segment based at least in part on modifying the content segment from the target content set using the determined editing style of the shots from the video.
Other aspects of the disclosure are directed to various apparatuses, non-transitory computer-readable media, computer-implemented methods, user interfaces, and electronic devices. These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification with reference to the drawings, in which:
FIG. 1 depicts a block diagram of an example system for performing automatic nonlinear editing style transfer, according to an example embodiment of the present disclosure.
FIG. 2 depicts a flowchart of an example method for performing automatic nonlinear editing style transfer in accordance with an example embodiment of the present disclosure.
FIG. 3 depicts a flowchart of an example method for performing automatic non-linear editing style transfer from source content to user-provided target content according to an example embodiment of the present disclosure.
FIG. 4 depicts a diagram of an example match performed between source content and target content, according to an example embodiment of the present disclosure.
FIG. 5 depicts an illustration of example keypoint tracking for automatic nonlinear editing style transfer, according to an example embodiment of the present disclosure.
Fig. 6 depicts an illustration of example frame-to-frame keypoint tracking for measuring camera motion for automatic nonlinear editing style transfer, according to an example embodiment of the present disclosure.
Fig. 7 depicts a diagram depicting an example re-projection of a starting location of target content based on camera motion in source content, according to an example embodiment of the present disclosure.
Fig. 8 depicts a block diagram of an example computer system that may perform one or more example embodiments of the present disclosure.
Detailed Description
SUMMARY
In general, the present disclosure is directed to improving various forms of content, including but not limited to visual content such as images and video content, using automatic non-linear editing style transfer. Examples described in this disclosure enable automatic detection and measurement of editing styles in one or more segments of content, e.g., whether provided as part of the original content or introduced later using post-production software. In addition, the present disclosure also enables automatic transfer of editing styles from source content to other collections and segments of target content. Moreover, examples of the present disclosure provide improvements in automated processing and manipulation of visual content compared to existing approaches.
The editing styles used to capture and present visual content are important to convey emotions, messages, and/or other aspects of the storytelling process. For example, an edit property is generally considered a building block of an edit style. Example editing properties may include framing, camera motion, focus, translation, playback speed, audio, volume, color, lighting, and so forth. Such edit attributes, when combined together, can leave a significant stylistic impression to the viewer.
While raw footage may itself capture events and tell stories to deliver messages, non-linear editing may be used to alter or enhance such content. Non-linear editing generally refers to a form of post-production editing in which the original content is saved and an edit decision list is compiled to maintain a track of the edited audio, video, and/or image content. The edited content may then be reconstructed from the original content and from the associated edit decision list. Traditional non-linear editing is a manual, time-consuming process that requires some editing skills and access to specialized editing software. As a result, non-linear editing is typically not performed on user-generated content due to the associated complexity and expense. As such, most user-generated content remains unedited, at which time such content may otherwise be enhanced and improved.
The present disclosure provides examples of performing automatic detection, measurement, and transfer of a collection editing style from a segment of source content to one or more segments of target content, as an alternative to conventional approaches. In an example of the present disclosure, a computer system performs an automatic non-linear editing style transfer from source video content to a target content collection by: determining one or more shot boundaries in the video based on analyzing the video, analyzing the identified content in each of the one or more shots in the video based on performing object detection on the respective shot, determining an editing style of the respective shot based at least in part on measuring motion across frames within each of the one or more shots in the video, determining a content segment to adjust from the target content set based on analyzing the target content set in view of the identified content and the determined editing style of the shot from the video, and automatically adjusting the content segment based at least in part on modifying the content segment from the target content set using the determined editing style of the shot from the video.
The systems, methods, and computer program products described herein provide a number of technical effects and benefits. As one example, embodiments described in the present disclosure perform automatic analysis of source content and target content to automatically transfer editing styles from source content to target content more efficiently and with less computing resources (e.g., less processing power, less memory usage, less power consumption, etc.) than, for example, conventional and manual post-production editing.
Referring now to the drawings, example embodiments of the disclosure will be discussed in further detail.
Example System for performing automatic non-Linear Style transfer
FIG. 1 depicts a block diagram of a system 100 for performing automated nonlinear editing style transfer according to an example embodiment of the present disclosure.
The system 100 includes one or more user devices 102, one or more server machines 110, and a data store 170 communicatively coupled by a network 120.
The network 120 may be a public network (e.g., the internet), a private network (e.g., a Local Area Network (LAN) or a Wide Area Network (WAN)), or any combination thereof. In an example, the network 120 may include the internet, one or more intranets, wired networks, wireless networks, and/or other suitable types of communication networks. The network 120 may also include a wireless telecommunications network (e.g., a cellular network) adapted to communicate with other communication networks, such as the internet. Further, network 120 may include one or more short-range wireless networks.
The user device 102 may generally be a Personal Computer (PC), a laptop computer, a mobile phone, a tablet computer, a server computer, a wearable computing device, or any other type of computing device (i.e., client). The user device 102 may run an Operating System (OS) that manages hardware and software of the respective device. A browser application (not shown) may be running on user device 102. The browser application may be a Web browser capable of accessing content and services provided by the server machine 110 or a combination of the server machines 110. Other types of computer programs and scripts may also be run on the user device 102.
The applications 104 each may generally provide a user interface 106 that allows a user to submit user input and receive various textual, graphical, and audio outputs associated with the respective application 104 running on the user device 102. For example, a user may typically provide user input to the application 104 via a user input component of the respective user device 102, such as a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus), a virtual keyboard, a traditional keyboard, a mouse, a microphone, a traditional keyboard, or by other means in which a user may provide input to the application 104 via the device. The user may also receive output from the application 104 via one or more user interfaces 106 provided through a display, the user device 102, a computing system, or any other type of device.
The applications 104 may include, for example, social networks, photo sharing, video sharing, storage services, and/or any other type of "app" running on the user device 102. Further, such applications 104 may have corresponding websites, services, and/or APIs that may be used in association with or separate and apart from the applications 104. The data store 108 can generally include any removable or non-removable storage device associated with the user device 102, and the user device 102 can also generally have access to other various stores (e.g., data store 116, data store 170) via the network 120.
Server machines 110 may each include one or more Web servers 112 and/or one or more application servers 114. The Web server 112 may provide text, audio, images, video, or any other content from the server machine 110 or other sources (e.g., data stores 116, 170) and the user device 102. Web server 112 may also provide Web-based application services, business logic, and interfaces to server machine 110 and user devices. Web server 112 may also transmit and receive text, audio, video, and image content to and from user device 102, user device 102 may be stored or provided by, for example, data stores 108, 116, 170 for saving, sharing, and/or publishing of content 130.
In an example, one or more Web servers 112 are coupled to one or more application servers 114 that provide application services, data, business logic, and/or APIs to server machine 110 and user device 102. In some examples, application server 114 independently provides one or more such services without using Web server 112. In an example, the Web server 112 may provide the server machine 110 and the user device 102 with access to one or more application server 114 services associated with an automated nonlinear editing system (e.g., automated nonlinear editing system 140). Such functionality may also be provided as part of one or more different Web applications, standalone applications, systems, plug-ins, web browser extensions, and Application Programming Interfaces (APIs), among others. In some examples, the plug-ins and extensions may be referred to individually or collectively as "add-ons".
The server machine 110 includes a local data store 116 and has access to other data stores 170 to store and access various forms of content 130. In an example, the server machine 110 may provide, be associated with, or be used in conjunction with one or more cloud or Web-based services and applications, such as social networking sites, cloud storage providers, content sharing sites, image sharing sites, video sharing sites, and/or any other site, service, or application that stores, processes, and displays user-generated and/or other types of content 130. In various examples, such sites, services, and/or applications may be accessed by a user via one or more applications 104 running on respective user devices 102.
In an example, the user device 102 and the server machine 110 can store and access various forms of content 130 (e.g., source content, target content) from a data store (e.g., data stores 108, 116, 170). In an example, content may generally refer to any form or format of text data, audio data, visual data, graphical data, graphics, images, video, multimedia, and/or any other type of content used to represent or describe any dimension (e.g., 2D, 3D). In various examples, the content 130 may be raw or newly captured, pre-processed or partially edited, professionally edited, curated, and/or user generated.
The data store 170 generally refers to persistent storage capable of storing various types of content 130, such as text, audio, video, and images. In some examples, the data store 170 may include a network attached file server or cloud storage, while in other examples, the data store 170 may include other forms of persistent storage, such as object oriented databases, relational databases, and so forth. The data store 170 may include user-generated content 130 (e.g., user-generated images, videos, etc.) uploaded by the user device 102 and/or content 130 provided by one or more other parties. Data may be added to the data store 170, for example, as discrete files (e.g., moving Picture Experts Group (MPEG) files, windows Media Video (WMV) files, joint Photographic Experts Group (JPEG) files/Graphics Interchange Format (GIF) files, portable Network Graphics (PNG) files, etc.) or as components of a single compressed file (e.g., a zip file).
Content 130 may generally include visual content such as images and video. The image may generally include any visual or graphical representation, such as a photograph or screen shot captured by a camera, computing device, or other device. The image may include, for example, a portrait, a square, a panorama, and/or any other type of image.
Video generally refers to a set of consecutive image frames representing a scene in motion. For example, a series of successive images may be captured in succession or later reconstructed to produce the effect of a moving image, which may include camera motion and/or motion of content within a scene. Video content may be presented in a variety of formats including, but not limited to, analog, digital, two-dimensional, and three-dimensional video. Further, the video content may include movies, video clips, or any collection of animated images to be displayed sequentially. The video data may comprise digital video having a sequence of still image frames that may also be stored as image data. Thus, each image frame may represent a snapshot of the scene that has been captured according to the time interval.
In various examples, the user device 102 and/or the server machine 110 run, execute, or otherwise utilize the automated nonlinear editing system 140 and the associated source content editing template 150. For example, the user device 102 may include one or more applications 104 associated with services (e.g., an automated nonlinear editing system 140) provided by one or more server machines 110. For example, various types of computing devices (e.g., smartphones, smart televisions, tablet computers, smart wearable devices, smart home computer systems, etc.) may use specialized apps and/or APIs to access services provided by the server machine 110, issue commands to the server machine 110, and/or receive content from the server machine 110 without accessing or using web pages.
In an example, the functions performed by one or more server machines 110 may be performed in whole or in part by one or more other machines and/or user devices 102. The server machine 110 may be accessed as a service provided by a system or device via an appropriate Application Programming Interface (API) and data feed, and is thus not limited to use with websites. Further, the server machine 110 may be associated with and/or utilize one or more automated non-linear editing systems 140 services, e.g., independently operated or provided by different providers.
In various examples, the automated non-linear editing system 140 analyzes various types of visual content, such as image and video data, to detect, measure, and transfer an editing style from a set of one or more segments of source content to another set of one or more segments of target content. In an example, the automated non-linear editing system 140 performs various automated analyses on existing, edited video content to detect and measure editing styles present in the video content, and to automatically apply the editing styles to other edited and/or unedited content, such as a clip from one or more videos, an image, a sequence of images, and/or a collection of images in target content.
Editing style generally refers to a collection of one or more style attributes associated with content 130, which may include attributes present in the original content and/or style attributes applied as part of the editing process. In some examples, the content 130 may include one or more editing styles based on one or more combinations of attributes associated with the content and/or particular portions of the content, such as particular shots in video content. Editing style attributes may generally include, but are not limited to, framing, camera motion, focusing, zooming, transitioning, playback speed, color, lighting, audio, and/or text associated with the content 130 or a particular portion of the content 130.
In various examples, camera framing generally refers to the placement and location of a subject within a lens. Focusing generally refers to lens focusing during a lens, and may include, for example, deep focusing, shallow focusing, soft focusing, and/or other types of focusing. The camera motion types may generally include still camera, pan Tilt Zoom (PTZ) camera motion, and/or six degree of freedom camera motion. Zooming generally refers to the change in focal length of a zoom lens during a shot, which allows for changing from a close-up view to a wide/full/long view, or vice versa, during a shot. Transitions generally refer to the manner in which two or more shots of visual content are combined or joined together. Example transitions typically include, but are not limited to, dissolve, fade, wipe, and various forms of shear (e.g., L-shear, skip shear, cut in, cut out, match shear, etc.).
Color generally refers to a color attribute or coloring scheme applied to visual content. For example, visual content may be presented in full color or various types of coloring schemes (lively, warm, cool, etc.). Such content may also be presented in gray scale, black and white, silver tones, and the like. Illumination generally refers to illumination that is captured and/or applied to visual content. Various lighting aspects of visual content may include, but are not limited to, light fading, filtering, glare, brightness, contrast, and the like.
Playback speed generally refers to the playback speed of content, such as visual content and/or audible content. The playback speed may be adjusted, for example, from a local real-time value (e.g., 1 x) to a faster speed (e.g., 1.25x,1.5x,2x, etc.) or a slower speed (e.g., 0.75x,0.5x, etc.). In addition to fixed speed, speed may be dynamically adjusted in a rigid mode (e.g., speed ramp-up, piecewise linear transition) or a complex mode where playback speed varies irregularly.
Audio generally refers to audio content, such as, for example, background music, sound, or audio-based speech associated with the content 130. For example, audio present in the source content may or may not be transferred to the target content along with other editing style attributes. Similarly, text, such as a text overlay, may be associated with the content 130. Text associated with the source content may or may not also be transferred to target content having other editing style attributes. Various other editing style attributes may include, but are not limited to, content quality, content resizing, content orientation, resolution, and the like.
In various examples, the automated non-linear editing system 140 analyzes the content 130 to automatically detect and measure various editing style attributes present in or associated with the content. The automated non-linear editing system 140 may generally use data and metadata to represent detected and measured editing style attributes. For example, the automated nonlinear editing system 140 may generate one or more editing styles for a piece of content, where each editing style represents, for example, a different grouping of editing style attributes. The automated nonlinear editing system 140 may also generate a single editing style that includes some or all of the editing style attributes detected and measured in the source content. Source content generally refers to content 130 that serves as a model or source of a style to be transferred to a collection of one or more other segments of other content 130, which may be referred to as target content.
In various examples, the automated non-linear editing system 140 automatically detects and measures editing style attributes of source content and records the information in a source content editing template 150 that may be used and then reused to automatically transfer editing styles from the source content to various sets of target content from any number of users (1, 10, hundreds, thousands, millions, etc.). As such, editing style attributes may be automatically extracted and described as data and metadata that may later be analyzed and used at different times, and the editing styles are transferred from source content to various types of target content by various users. The source content editing template 150 may include, for example, global editing style attributes that are consistent within the source content and/or may include local editing style attributes associated with particular portions, segments, or time periods within the source content. In some examples, the source content editing template 150 may include information about shot boundaries detected in the video, information about one or more shots in the video, information about content identified within respective shots of the video, and/or editing style attributes determined at various times across various frames from a shot in the video.
Example method of performing automatic non-linear style transfer
FIG. 2 depicts a flowchart of an example method for performing automatic nonlinear editing style transfer in accordance with an example embodiment of the present disclosure.
Although fig. 2 depicts steps performed in a particular order as an example for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of the method 200 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 202, the computing system determines one or more shot boundaries in the video. In an example, a computing system receives one or more segments of source content to be used for automatic nonlinear editing style transfer. For example, a user may provide a set of one or more segments of original and/or edited content, such as an image, a burst of photographs, a video clip, a movie trailer, a montage, a music video, a high-energy moment video, an advertising video, or any other type of content. In various examples, a computing system receives a set of one or more segments of video content as source content to perform an automatic non-linear editing style transfer on the set of one or more segments of target content.
In an example, the automated non-linear editing system 140 of the computing system analyzes the source video content to detect one or more shot boundaries present in the source video content. For example, the computing system may perform shot boundary detection analysis based on detecting color changes and/or identifying keypoint matches across video frames to identify shot boundaries and associated shots. A shot may be generally referred to as a continuous view or a series of frames captured or recorded by a camera without cutting, jumping, or other interruptions. In some examples, shots may be grouped together into groups that may be referred to as shots or scenes depicted within the video content. In some examples, a shot may be constructed or reconstructed from various images that may or may not be associated with video content.
In an example, the automated non-linear editing system 140 performs color analysis across frames of video content and signals a shot change based on detecting a significant change in color across video frames. For example, color may be tracked across one or more video frames, and when the color of one or more frames differs from the color in one or more other comparison frames that exceed a threshold, the occurrence of a shot change may be signaled. In some examples, the automated non-linear editing system 140 calculates a color histogram for each of one or more frames, compares the color histogram information to previous frames, and signals a shot change when the distribution of frame colors changes at a rate different from the color representation along the sliding history window.
In an example, the automated nonlinear editing system 140 determines shots, shot groupings, scenes, and/or shot boundaries in the video content by performing keypoint matching in the respective frames and by matching the identified set of keypoints across the frames using an approximate nearest neighbor search and by applying weak geometric constraints in the form of a basis matrix to obtain a spatially consistent set of keypoint matches. In some examples, shot boundaries may be identified based on detecting significant changes in keypoint matches occurring between frames. For example, a shot change may be signaled using a significant change in keypoints across frames that exceed a threshold level.
In some examples, color analysis and keypoint matching may be used together to identify shots, detect shot boundaries, and/or group shots into scenes. One or more other types of shot boundaries and shot identifications may be used alone or in combination with color analysis and/or keypoint matching. For example, one or more machine learning models may be used to automatically detect, predict, or confirm transitions between frames in video content.
At 204, the computing system analyzes the identified content in the respective shot based on performing object detection on the respective shot in the video. In an example, the automated non-linear editing system 140 of the computing system analyzes frames of identified shots by performing object detection on the frames of the respective shots to classify the content and focus of the shots. In some examples, the automatic nonlinear editing system 140 performs object detection on one or more frames of the shot to identify a set of one or more objects present in the shot. The identified list of objects may then be analyzed, for example, from largest to smallest. In an example, the union of the boxes for each object may be iteratively computed until the frame coverage exceeds a threshold (e.g., 50%, 65%, 80%, or any percentage of the frame coverage). As such, the list of identified objects may be analyzed until the analyzed objects cover points that allow sufficient space in the sorted frames of the shot.
In some examples, automated nonlinear editing system 140 identifies, analyzes, and determines features and details about recognized faces and human gestures as part of object detection, including but not limited to emotions, expressions, and activities. In some examples, the automated non-linear editing system 140 may identify, analyze, and determine features and details about one or more objects, such as food, products, tools, animals, or other objects present in visual content. In various examples, one or more objects, features, expressions, activities, and/or other aspects of the identified shots via object detection and analysis may be used, for example, at least in part, to identify matching target content having similarity, consistency, and/or compatibility with the source content.
In an example, the automated nonlinear editing system 140 performs object detection and analysis on the visual source content and then identifies lens focus for the visual source content. In some examples, shots that include a small number of objects, e.g., as defined by one or more thresholds, may be classified or labeled as single focus shots. For example, a single focus lens may generally represent camera attention and/or motion that tends to focus on one or more particular objects in the lens or across the lens.
In some examples, lenses including a medium number of objects, e.g., as defined by one or more thresholds, may be classified or labeled as medium focus lenses. For example, a medium focus lens may generally include a group or text-covered arrangement of one or more persons and/or products. As such, a medium focus lens may generally focus on multiple objects in the lens, rather than on a particular object. In some examples, a shot may be marked or classified as having little or no particular object focus. For example, the shots of people, landscapes, and/or general behind-the-scenes footage may not have a particular focus on a particular object.
Fig. 4 depicts a diagram 400 showing example matching that may be performed between source content and target content according to an example embodiment of the present disclosure. For example, a first example of source content 402 may be processed using object detection and classified as focused on a single object on a small number of objects including a person's face. Then, the first instance of source content 402 may be determined to be compatible and match the first instance of target content 406, e.g., also processed using object detection, and classified as focused on a small number of objects including human faces. Similarly, it may be determined that the second instance of source content 404 focused on the hand's close-ups during eating is compatible and matches the second instance of target content 408 focused on the hand's close-ups during typing. In various examples, the results of the object detection and analysis performed on the source and target content may be used as one set of considerations and criteria in matching the source content to the target content when performing the automatic nonlinear editing style transfer.
At 206, the computing system determines an editing style for the respective shot based at least in part on measuring motion across frames of the respective shot in the video. In an example, an automated non-linear editing system 140 of a computing system analyzes shots from source visual content to determine one or more editing style attributes of an editing style used in the respective shot. In an example, the automated nonlinear editing system 140 detects and measures camera motion in a shot or across a group of shots in source visual content. For example, the automated nonlinear editing system 140 may detect and measure camera motion or how the camera moves across a set of frames in one or more shots.
In the exampleCamera motion can be represented as a frame-to-frame re-projection of an image as a projective transformation: h ref . In various examples, the automated non-linear editing system 140 calculates camera motion or movement, for example, using frame-to-frame keypoint matching and using a random sample consensus (RANSAC) homography solver or another type of homography solver, respectively, in each of the source content and/or the target content. In some examples, keypoints are tracked on at least a foreground object, a background object, and a human foreground object. In some examples, classifiers or other tools are used to reject keypoints detected on the face or body to remove foreground motion from the homography estimation. In various examples, camera motion or movement detected across frames may be determined based on measuring distances between respective keypoints across frames, where the current total re-projection is a left-product accumulation of previous projections: h ref (t)＝H ref (t)*H ref (t-1)*H ref (t-2)...*H ref (t start )。
FIG. 5 depicts an illustration 500 of example keypoint tracking for automatic nonlinear editing style transfer, according to an example embodiment of the present disclosure. In an example, keypoints may be tracked in content frames, such as video frames 502. In an example, keypoints 504 can be generally tracked on foreground and/or background objects, human foreground keypoints 506 can also be tracked, and matching keypoints 508 can be identified across frames of video content to detect and measure camera motion. As described above, in some examples, the human foreground keypoints 504 may not be considered when detecting and measuring camera motion.
Fig. 6 depicts a diagram 600 of example frame-to-frame keypoint tracking for measuring camera motion for automatic nonlinear editing style transfer, according to an example embodiment of the present disclosure. Frames 602, 604, and 606 illustrate keypoint tracking across a series of frames, where the camera spans a scene that includes two seated people. Using frame 602 as the starting position, the keypoints in frame 604 extend to lines showing frame-to-frame keypoint tracking for forming homographies, and frame 606 generally represents the position where camera movement ends in this example. Frames 608, 610, and 612 represent actual frames without keypoint tracking, where frame 608 corresponds to the starting position at frame 602, frame 610 corresponds to frame 604 showing movement from the starting position, and frame 612 corresponds to frame 606 after camera movement is complete.
In various examples, such movements measured in source content may be transferred to target content, for example, during a user session or as part of a batch process. In some examples, movement in the source content may be measured and stored in one or more source content editing templates 150, e.g., based on the timing, location, and/or identification of associated video frames and using various data describing the movement that occurs across frames. As such, for example, the source content editing template 150 may be used and reused to perform automatic non-linear editing style transfers onto different sets of one or more segments of target content at different times without the need to repeatedly re-analyze the source content for each request.
In various examples, automated nonlinear editing system 140 analyzes the content to determine one or more editing style attributes associated with the content. For example, the content may be analyzed, either directly or with the aid of one or more general or specific classifiers or models, to identify editing style attributes, such as framing, camera motion, focus, zoom, transition, playback speed, color, lighting, audio, and/or text, generally associated with and/or having particular portions of the content. Such information may include data and metadata describing editing style attributes and their relationship to the content or particular portions of the content (e.g., shots, frames, timing, sequences, etc.), which may be stored as part of the source content editing template 150.
At 208, the computing system determines a content segment to adjust from the target content set based on analyzing the target content set in view of the identified content and the determined editing style of the footage from the video. In an example, the automated non-linear editing system 140 of the computing system receives a set of one or more segments of target content that match the source content as part of performing an automated non-linear editing style transfer from the source content. (e.g., by transferring styles from edited source content pages to target content)
In an example, the automated non-linear editing system 140 receives a set of one or more segments of user-generated content, which may include visual content, such as one or more images and or one or more videos provided by a user. In some examples, the user may provide or select source content that is analyzed and processed by the automated non-linear editing system 140 to determine a source content editing style to be transferred to target content, such as one or more segments of content created by the user. The user may also select or provide a source content editing template 150 that includes a source content editing style to be transferred, e.g., the user's target content.
In an example, the automated nonlinear editing system 140 analyzes each of the one or more segments of content in the target content set to determine which portions of the target content are compatible with the source content for automated nonlinear style transfer. For example, the automated nonlinear editing system 140 may perform shot boundary detection on the target video content to identify shots, object detection and analysis on the shots and images to identify objects and shot focus, and identify one or more editing style attributes of the target content.
In an example, the automated non-linear editing system 140 compares the editing style attributes from each of the one or more portions of the source content to the information determined for each of the one or more portions of the target content. For example, the automated nonlinear editing system 140 may compare one or more of a shot length, object content, shot focus, and/or one or more editing style attributes of a portion of the source content to each of one or more segments or portions of content within the target content collection. Such a comparison may be performed, for example, to match source and target content for automatic non-linear style transitions based on one or more of compatibility, to allow style transitions to occur between the source and target content, or to match similar objects, scenes, focus, and/or other attributes across the source and target content that are not typically otherwise relevant. In an example, the source content and target content may be matched based on objects, mood, focus, movement, scene, editing style, and other considerations to allow the editing style of the source content to be applied to the target content while preserving the expression and meaning delivered in the source content when applied to the target content.
In an example, the model may be trained and used to analyze, compare, and match various portions of the source and target content. In some examples, each of the one or more portions or segments of the source content is compared to one or more portions or segments of the target content and scored. For example, shot lengths may be compared between shots of source content and shots of target content to determine that the shot lengths of the target content are sufficient to perform a style transition to the target content.
In an example, the source content is matched to the target content based on lens focus where lenses classified as single focus, medium focus, and/or little or no focus match in the source content and the target content. In some examples, objects, expressions, activities, and other content may be weighted more heavily when matching single focus lenses to determine appropriate target shot as compared to matching medium focus lenses, which may typically be weighted based on the global behavior of such lenses. Landscape footage in the source content may generally be identified as compatible and matching with target content that is also classified as having little or no focus.
In an example, the automated non-linear editing system 140 matches one or more portions or segments of the source content with different portions or segments in the target content. For example, a source content video with eight shots may have one or more global and/or shot-specific editing style attributes, each shot having similar or different timing. The automatic nonlinear editing system 140 may generally process each of the source video shots to match different segments of the target content (e.g., shots from one or more videos in the target content) to corresponding shots of the source content based on constraints (e.g., shot length, frame size), object recognition, shot focus, content similarity, and other aspects to identify compatible target content.
At 210, the computing system automatically adjusts a content segment from the target content based at least in part on modifying the content segment using the determined editing style of the shot from the first video. In an example, the automated non-linear editing system 140 adjusts shots of the target video content that have been matched to the shots of the source video content by transferring one or more editing style attributes extracted from the shots of the source video content to the matched shots of the target video content. In various examples, each of one or more segments of target content (e.g., a shot, an image, etc.) matches source content (e.g., a shot, an image, etc.). The editing style attributes of the segment source content are then automatically transferred to the corresponding segment that matches the target content, e.g., to generate and present the target content with the editing style attributes of the source content.
In an example, a set of one or more segments of source content has been matched with corresponding segments of target content. The editing style attributes detected in one or more segments of the source content are then transferred to corresponding segments of the target content. Such editing style attributes may include, but are not limited to, one or more information about framing, camera motion, focus, zoom, transition, playback speed, color, lighting, audio, or text. Additionally, such editing style attributes may be measured, extracted, stored, and provided for use in the source content editing template 150. Further, such editing style attributes may be automatically transferred from source content to target content to correctly apply the same sensations, streams, effects and/or impressions from the source content to different target content provided by the user.
In an example, camera movement or motion is transferred from source content to target content. For example, frame-to-frame camera motion may be detected and measured in each of the source content and the target content. In an example, by inverse projection based on the target content
In an example, the starting position H is determined in a segment of the target content based on camera motion or movement being transferred from the source content ref (t = start). For example, the starting location for re-projection onto the target content may include an initial scale and an offset. In an example, the starting position is determined based on analyzing the target content in view of camera motion or movement of the segment being transferred to the target content. For example, the starting position may be determined such that camera movement or motion projected onto the target content remains within the boundaries of the target content (e.g., remains within and no more than a frame). As such, the starting position may be determined based on moving the starting position to a particular location within the frame of the target content and/or at a particular zoom level to allow camera movement or motion from the source content to be performed correctly based on the target content. In some examples, the target location is determined and/or adjusted, e.g., to augment or maximize camera motion or movement being applied to the target content.
Fig. 7 depicts an illustration 700 for re-projecting a start position of target content based on camera motion of source content for automatic non-linear editing style transfer. In an example, images 702-706 represent the positioning of source content movement within the rectangular frame boundaries of the target content such that the movement being applied from the source content remains within the boundaries while maximizing the size and coverage of the shot content, as shown in images 708-712.
For example, the image 702 shows the determined starting position of the source content movement within the rectangular boundary of the target content. Image 708 shows the starting position as viewed in the frame of the target content. Image 704 shows the start of the movement applied from the source content within the rectangular frame boundary of the target content. Image 710 shows how the source content movement corresponds to and occurs within the frame of the target content. Image 706 shows how the applied source content movement continues and remains within the rectangular frame boundaries of the target content. Image 712 shows the corresponding movement from image 706 within the frame of the target content.
In an example, any one or more editing style attributes of the source content may be transferred to the target content. For example, the playback speed detected and measured in the source content may be transferred and applied to the target content at the same rate as in the source content or at a different rate (e.g., faster, slower). In some examples, the playback of the source content and the target content is calculated as a constant value. In addition, frame resampling can be used to transfer playback speed from source content to target content prior to reprojection
In an example, the illumination and brightness of the source content is transferred to the target content. In some examples, a brightness detector is used to detect and measure, for example, illumination or brightness over time in the source content. For example, the brightness detector may average the values of the illumination and/or pixels in the shot of the source content. Such values may then be used to adjust lighting and brightness in the target content, for example, to transfer stylistic lighting adjustments such as fades and transitions from the source content to the target content.
In various examples, the editing style attributes may be transferred from the same type of source content and target content and/or different types of source content and target content. For example, a shot from the target video content may be matched with and receive various editing style attributes from a shot of the source video content via automatic editing style transitions. Similarly, one or more target content images may be matched with and receive various editing style attributes from different forms of source content (e.g., one or more source content shots and/or images). In one example, the target content image matches a shot of the source video content. A video clip may then be generated from the target content image, for example, by repeating the image as a frame over a period of time. Editing style attributes such as the measured movement in the source video content (e.g., a shot) may then be transferred to the generated video clip for the matched target content image.
FIG. 3 depicts a flowchart of an example method for performing automatic non-linear editing style transfer from source content to user-provided target content according to an example embodiment of the present disclosure.
Although fig. 3 depicts steps performed in a particular order as an example for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of the method 300 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 302, the computing system generates one or more editing styles from the source content. In an example, the automated non-linear editing system 140 receives one or more segments or sections of content. For example, any user, such as an end user, creator, administrator, or other person, may provide one or more segments of source content. In some examples, the source content may be automatically selected or determined based on ratings, user feedback, machine learning analysis, content type, genre, user attributes, user preferences, and/or other ways of identifying source content for use by the automatic non-linear editing system 140.
In an example, the automated nonlinear editing system 140 generates one or more editing styles for each of one or more segments of source content. In various examples, the automated non-linear editing system 140 analyzes and processes source content to detect, measure, and record editing style attributes of the source content according to examples of the present disclosure. The editing style attributes may include, for example, camera motion, focus, zoom, transition, playback speed, color, lighting, audio, or text associated with a respective shot of the source video. In some examples, various aspects of the editing style may be determined and applied to the target content, for example, during a user session. In other examples, the source content editing template 150 may be created and stored with information about the editing style. Such information can then be used and reused across any number of users to automatically apply the editing styles from the source content to the various target content.
At 304, the computing system provides the user with one or more editing styles for selection. In an example, the automated nonlinear editing system 140 provides the user with an option to select one or more source content items having an editing style. For example, the user may select one or more available editing styles to apply to the user's target content. For example, a user may select a single source content editing style to apply to the target content. The user may also generate different versions of the target content, each corresponding to a different source content editing style (e.g., for comparison to determine favorites, etc.).
In various examples, the user is provided with an opportunity to preview the source content associated with the respective edit style, e.g., using a thumbnail preview or other method of providing access to the associated source content. In an example, the source content and the editing styles may be searchable. The source content and associated editing styles may also be grouped together for presentation to the user, e.g., based on topic, channel, influencer, creator, and/or category such as vacation, sports, hiking, bicycling, cooking, etc. For example, a user may search for source content related to vacations and be presented with several source content editing styles for selection. In some examples, users may also provide or select their own source content, including but not limited to video clips, music videos, original videos, pictures, photos, and the like. Such content may be provided, for example, from one or more sites or services, such as photo sharing, video sharing, social networking, and so forth.
At 306, the computing system receives target content to adjust based on the selected editing style. In an example, a user provides one or more segments of target content to be analyzed and matched to source content to perform an automatic editing style transfer. For example, a user may provide one or more segments of visual content, such as images and/or video. Such content may be user-generated content associated with a user account. In some examples, the user may specify one or more particular segments of the target content to be used. The user may also provide a location that includes a collection of content that the user wishes to use as target content. In one example, a user provides a user account for the user associated with a photo sharing service, a video sharing service, a storage service, social media, and/or one or more other services, which the automated non-linear editing system 140 may access and analyze to identify various content of the user to be used as target content.
At 308, the computing system analyzes the target content in view of the selected editing style. In an example, the automated non-linear editing system 140 of the computing system analyzes each of the one or more segments of the target content provided by or associated with the user. For example, data, metadata, images, and other aspects of the target content may be analyzed to identify and classify topics and objects associated within respective segments of the target content.
In some examples, shot boundary detection may be performed on target video content to identify shots and shot boundaries in the video content, as described in 202 and this disclosure. Similarly, as described in 204 and the present disclosure, object detection and classification may be performed on the target visual content to identify one or more of objects, expressions, activities, lens focus, and/or other aspects of the visual content. Such detection and classification of target visual content may be performed, for example, to aid in matching source content with target content when performing automatic editing style transfers.
In an example, the automated non-linear editing system 140 matches the analyzed target content with a corresponding segment of the source content. In one example, the source video content may include six shots that have been identified and analyzed for automatic editing style transfers based on examples of the present disclosure. For each of the six shots in the source video content, the automated non-linear editing system 140 compares various aspects of the shot, including but not limited to one or more of shot length, shot focus, object, expression, activity, etc., to various target content that has been analyzed. For example, the automated non-linear editing system 140 may compare and rank one or more segments or sections of target content that match the source content. The matching target content may then be selected, for example, based on one or more criteria, which may be the highest ranking matching segment or snippet of target content for each of the six shots in the source video content.
At 310, the computing system automatically adjusts the target content based on the selected editing style. In an example, the automated non-linear editing system 140 of the computing system modifies each of the one or more segments or sections of the target content to transfer the editing style from the source content to the target content. In some examples, the automated non-linear editing system 140 may modify the target content before transferring the editing style. For example, the length of a shot of the target video content may be adjusted, the target content orientation may be adjusted, and/or the target content quality may be automatically modified before performing the automatic editing style transfer.
At 312, the computing system provides the adjusted target content to the user. In an example, the automated non-linear editing system 140 of the computing system automatically transfers the editing style from each of the one or more segments or sections of the source content to the one or more matching segments or sections of the target content. In various examples, the automated non-linear editing system 140 may generate a new segment of content, such as a new video that includes target content with an editing style that is transferred from the source content.
In an example, one or more user interfaces 106 may be provided to a user to perform various operations involving generated output including target content having an editing style applied from source content. For example, a user may use the user interface 106 to store, modify, publish, regenerate, and/or discard associated output. In one example, the user interface 106 may include a simplified editing interface that allows a user to include, exclude, and/or rearrange each of the one or more shots in the visual output generated from the automated non-linear editing system 140. For example, if the user dislikes one shot or image among the generated output including twenty shots or images, the user may select a particular shot or image to be excluded and save the resulting output.
Example apparatus and System
Fig. 8 illustrates a diagram of an example machine in the form of a computer system 800 within which a set of instructions, for causing the machine to perform any one or more of the operations discussed herein, may be executed. In other examples, the machine may be connected (e.g., networked) to other machines in a LAN, an intranet, an extranet, or the internet. The machine may operate in the capacity of a server or a client machine in client-server network environment, or as a peer machine in a peer-to-peer (or distributed) network environment. The machine may be a Personal Computer (PC), a tablet PC, a set-top box (STB), a Personal Digital Assistant (PDA), a cellular telephone, a wearable computing device, a Web appliance, a server, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while only a single machine is illustrated, the term "machine" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the operations discussed herein.
Processor 802 represents one or more general-purpose processing devices such as a microprocessor, central processing unit, or the like. More specifically, the processor 802 may be a Complex Instruction Set Computing (CISC) microprocessor, reduced Instruction Set Computing (RISC) microprocessor, very Long Instruction Word (VLIW) microprocessor, or a processor implementing other instruction sets or processors implementing a combination of instruction sets. The processor 802 may also be one or more special-purpose processing devices such as an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Digital Signal Processor (DSP), network processor, or the like. The processor 802 is configured to execute instructions 822 for performing the operations discussed herein.
The computer system 800 may also include a network interface device 808. The computer system 800 may further include a video display unit 810 (e.g., a Liquid Crystal Display (LCD) or a Cathode Ray Tube (CRT)), an alphanumeric input device 812 (e.g., a keyboard), a cursor control device 814 (e.g., a mouse), and a signal generation device 816 (e.g., a speaker).
The data storage device 818 may include a computer-readable storage medium 828 on which is stored one or more sets of instructions 822 (e.g., software computer instructions), the one or more sets of instructions 822 embodying any one or more of the examples described herein. The instructions 822 may also reside, completely or at least partially, within the main memory 804 and/or within the processor 802 during execution thereof by the computer system 800, the main memory 804 and the processor 802 also constituting computer-readable storage media. The instructions 822 may be transmitted or received over the network 820 via the network interface device 808.
In one example, the instructions 822 include instructions for one or more modules of the automated nonlinear editing system 140 (e.g., the automated nonlinear editing system 140 of fig. 1) and/or a software library containing methods that invoke the automated nonlinear editing system 140. While the computer-readable storage medium 828 (machine-readable storage medium) is shown by way of example to be a single medium, the term "computer-readable storage medium" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions. The term "computer-readable storage medium" may also include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the operations of the present disclosure. The term "computer-readable storage medium" shall accordingly be taken to include, but not be limited to, solid-state memories, optical media, and magnetic media.
In the preceding description, numerous details have been set forth. However, it will be apparent to one of ordinary skill in the art having the benefit of the present disclosure that the present disclosure may be practiced without these specific details. In some instances, well-known structures and devices are shown in block diagram form, rather than in detail, in order to avoid obscuring the present disclosure.
Some portions of the detailed description have been presented in terms of processes and symbolic representations of operations on data bits within a computer memory. A process is here, and generally, conceived to be a self-consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like.
It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussions, it is appreciated that throughout the specification discussions utilizing terms such as "analyzing," "determining," "identifying," "adjusting," "transmitting," "receiving," "processing," or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulate and transform data represented as physical (e.g., electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices.
Certain examples of the present disclosure also relate to apparatus for performing the operations herein. The apparatus may be constructed for the intended purpose or it may comprise a computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, and magnetic-optical disks, read-only memories (ROMs), random Access Memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, or any type of media suitable for storing electronic instructions.
It is to be understood that the above description is intended to be illustrative, and not restrictive. Many other examples will be apparent to those of skill in the art upon reading and understanding the above description. The scope of the disclosure should, therefore, be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled.
Additional disclosure
The technology discussed herein relates to servers, databases, software applications, and other computer-based systems, and the actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between and among components. For example, the processes discussed herein may be implemented using a single device or component or a combination of multiple devices or components. Databases and applications may be implemented on a single system or distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Modifications, variations, and equivalents of such embodiments may readily occur to those skilled in the art upon a reading of the foregoing description. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Thus, the present disclosure is intended to cover such modifications, alterations, and equivalents.
Claims (22)
1. A computer-implemented method for performing non-linear editing style transfer from source video content to target content, comprising:
determining one or more shot boundaries in a source video based on analyzing the source video;
analyzing the identified content in the respective shot based on performing object detection on each of the one or more shots in the source video;
determining an editing style for each of the one or more shots in the source video based at least in part on measuring motion across frames within the respective shot;
analyzing a target content set based on the identified content and the determined editing style of the footage from the source video, determining a content segment to adjust from the target content set; and
automatically adjusting a content segment from the target content set based at least in part on modifying the content segment using the determined editing style of the shot from the source video.
2. The computer-implemented method of claim 1, wherein determining the shot boundaries is based at least in part on detecting a change in frame color across frames of the source video.
3. The computer-implemented method of any preceding claim, wherein determining the shot boundaries is based at least in part on analyzing keypoint matches across frames of the source video.
4. The computer-implemented method of any preceding claim, further comprising:
identifying the one or more shots in the source video based on at least one of the determined shot boundaries.
5. The computer-implemented method of any preceding claim, further comprising:
generating an edit template for the source video, the edit template reusable for automatically transferring the edit style of corresponding shots in the source video to other visual content; and
storing the generated edit template for the source video.
6. The computer-implemented method of claim 5, wherein the edit template comprises one or more of: information about the shot boundaries in the source video, one or more of the shots in the source video, the identified content in the respective shots in the source video, and at least one editing style determined for the respective shots in the source video.
7. The computer-implemented method of claim 6, wherein the content segments from the target set of content are determined based at least in part on the information from the editing template generated for the source video.
8. The computer-implemented method of claim 6 or 7, wherein the content segments from the target content collection are automatically adjusted based at least in part on the information from the editing template generated for the source video.
9. The computer-implemented method of any preceding claim, further comprising:
generating output content comprising adjusted content segments of the target content set that are adjusted based on the editing style of the footage from the source video.
10. The computer-implemented method of any preceding claim, further comprising:
determining a second piece of content to adjust from the set of target content based on analyzing the set of target content in view of the identified content and the determined editing style of the second shot from the source video; and
automatically adjusting the second segment of content based at least in part on modifying the second segment of content with the editing style of the second shot from the source video.
11. The computer-implemented method of claim 10, wherein the output content comprises an adjusted second content segment of the target content set adjusted based on the editing style of the second shot from the source video.
12. The computer-implemented method of any preceding claim, wherein the automatically adjusting comprises stabilizing camera motion of a respective content segment prior to modifying the respective content segment.
13. The computer-implemented method of any preceding claim, wherein modifying respective content segments from the target content collection comprises applying camera motion of respective shots from the source video.
14. The computer-implemented method of any preceding claim, wherein modifying the respective content segment from the target content collection comprises applying a zoom of the respective shot from the source video.
15. The computer-implemented method of any preceding claim, wherein modifying respective content segments from the target content collection comprises applying transitions associated with respective shots of the source video.
16. The computer-implemented method of any preceding claim, wherein the editing style comprises information about one or more of: framing, camera motion, focusing, zooming, transitioning, playback speed, color, lighting, audio, or text associated with the respective lens.
17. The computer-implemented method of any preceding claim, wherein the content segment is a collection of one or more images.
18. The computer-implemented method of any preceding claim, wherein the content clip is a video clip from a second video.
19. The computer-implemented method of claim 18, wherein the target set of content comprises the second video.
20. The computer-implemented method of any preceding claim, wherein the camera motion of the video is pan-tilt-zoom camera motion.
21. One or more non-transitory computer-readable media collectively storing computer-executable instructions that, when executed by one or more computer processors, will cause the one or more processors to perform the method of any preceding claim.
22. A computer system, comprising:
one or more processors; and
the one or more non-transitory computer-readable media of claim 21.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/060347 WO2022103397A1 (en) | 2020-11-13 | 2020-11-13 | Automatic non-linear editing style transfer |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115917647A true CN115917647A (en) | 2023-04-04 |
Family
ID=73790232
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080102660.4A Pending CN115917647A (en) | 2020-11-13 | 2020-11-13 | Automatic non-linear editing style transfer |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230419997A1 (en) |
EP (1) | EP4165635A1 (en) |
CN (1) | CN115917647A (en) |
WO (1) | WO2022103397A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11694018B2 (en) * | 2021-01-29 | 2023-07-04 | Salesforce, Inc. | Machine-learning based generation of text style variations for digital content items |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2014001607A1 (en) * | 2012-06-29 | 2014-01-03 | Nokia Corporation | Video remixing system |
-
2020
- 2020-11-13 CN CN202080102660.4A patent/CN115917647A/en active Pending
- 2020-11-13 EP EP20821490.8A patent/EP4165635A1/en active Pending
- 2020-11-13 WO PCT/US2020/060347 patent/WO2022103397A1/en active Application Filing
- 2020-11-13 US US18/251,838 patent/US20230419997A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230419997A1 (en) | 2023-12-28 |
EP4165635A1 (en) | 2023-04-19 |
WO2022103397A1 (en) | 2022-05-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9570107B2 (en) | System and method for semi-automatic video editing | |
US10685460B2 (en) | Method and apparatus for generating photo-story based on visual context analysis of digital content | |
US9554111B2 (en) | System and method for semi-automatic video editing | |
US9002175B1 (en) | Automated video trailer creation | |
US8692940B2 (en) | Method for producing a blended video sequence | |
US9189137B2 (en) | Method and system for browsing, searching and sharing of personal video by a non-parametric approach | |
US9047376B2 (en) | Augmenting video with facial recognition | |
US10452920B2 (en) | Systems and methods for generating a summary storyboard from a plurality of image frames | |
TWI253860B (en) | Method for generating a slide show of an image | |
CN108780654B (en) | Generating mobile thumbnails for video | |
CN113010703B (en) | Information recommendation method and device, electronic equipment and storage medium | |
US20210117471A1 (en) | Method and system for automatically generating a video from an online product representation | |
CN113348486A (en) | Image display with selective motion description | |
JP2010020781A (en) | Method and apparatus for producing animation | |
US11676382B2 (en) | Systems and methods for generating composite media using distributed networks | |
US11582519B1 (en) | Person replacement utilizing deferred neural rendering | |
US11581020B1 (en) | Facial synchronization utilizing deferred neural rendering | |
JP2023529380A (en) | Machine learning-based image compression settings that reflect user preferences | |
US20230419997A1 (en) | Automatic Non-Linear Editing Style Transfer | |
Soe et al. | A content-aware tool for converting videos to narrower aspect ratios | |
US20230066331A1 (en) | Method and system for automatically capturing and processing an image of a user | |
KR20140033667A (en) | Apparatus and method for video edit based on object | |
WO2016203469A1 (en) | A digital media reviewing system and methods thereof | |
Niu et al. | Real-time generation of personalized home video summaries on mobile devices | |
Souza et al. | Generating an Album with the Best Media Using Computer Vision |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |