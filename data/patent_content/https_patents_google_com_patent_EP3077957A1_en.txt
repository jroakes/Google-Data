EP3077957A1 - Local real-time facial recognition - Google Patents
Local real-time facial recognitionInfo
- Publication number
- EP3077957A1 EP3077957A1 EP14868544.9A EP14868544A EP3077957A1 EP 3077957 A1 EP3077957 A1 EP 3077957A1 EP 14868544 A EP14868544 A EP 14868544A EP 3077957 A1 EP3077957 A1 EP 3077957A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- image capture
- capture device
- data points
- mapping information
- image
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000001815 facial effect Effects 0.000 title claims abstract description 20
- 238000013507 mapping Methods 0.000 claims abstract description 146
- 238000000034 method Methods 0.000 claims abstract description 44
- 230000004044 response Effects 0.000 claims abstract description 18
- 238000004891 communication Methods 0.000 description 17
- 238000005516 engineering process Methods 0.000 description 17
- 230000008569 process Effects 0.000 description 14
- 238000012545 processing Methods 0.000 description 14
- 238000004590 computer program Methods 0.000 description 8
- 230000003993 interaction Effects 0.000 description 5
- 230000006855 networking Effects 0.000 description 5
- 238000013461 design Methods 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 239000011521 glass Substances 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 229920001690 polydopamine Polymers 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000013459 approach Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 239000002355 dual-layer Substances 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/765—Interface circuits between an apparatus for recording and another apparatus
- H04N5/77—Interface circuits between an apparatus for recording and another apparatus between a recording apparatus and a television camera
- H04N5/772—Interface circuits between an apparatus for recording and another apparatus between a recording apparatus and a television camera the recording apparatus and the television camera being placed in the same enclosure
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/94—Hardware or software architectures specially adapted for image or video understanding
- G06V10/95—Hardware or software architectures specially adapted for image or video understanding structured as a network, e.g. client-server architectures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/179—Human faces, e.g. facial parts, sketches or expressions metadata assisted face recognition
Definitions
- Image capture devices such as glasses or other devices with image capture capability, are becoming more prevalent. These image capture devices are able to capture images of their surrounding and allow the user to interact with the surrounding more intelligently based on the captured images. In some instances, the image capture devices are able to capture images of nearby users (e.g., in the field of view of the image capture device).
- a user operating the image capture device is typically associated with various services (e.g., social networking services) that maintain information regarding contacts of the user. The information can be used to assist the user operating the image capture device in interacting with nearby users.
- the disclosed subject matter relates to a machine-implemented method for real-time facial recognition at an image capture device, the method comprising capturing an image at an image capture device, the image containing at least a first person.
- the method may further include analyzing the image to identify one or more data points corresponding to the first person.
- the method may further include determining if mapping information corresponding to the one or more data points is locally available at the image capture device.
- the method may further include sending a request to a remote device for mapping information when it is determined that mapping information corresponding to the one or more data points is not locally available at the image capture device.
- the method may further include receiving mapping information corresponding to the one or more data points in response to the request.
- the method may further include locally storing the mapping information at the image capture device.
- the disclosed subject matter also relates to an image capture device configured to capture images, the image capture device comprising a local data store and a recognition module.
- the recognition module may be configured to analyze an image captured by the image capture device to identify one or more data points corresponding to a first person within the image.
- the recognition module may be configured to determine if mapping information corresponding to the one or more data points is locally available at the local data store.
- the recognition module may be configured to send a request to a remote device for mapping information when it is determined that mapping information corresponding to the one or more data points is not locally available at the image capture device.
- the recognition module may be configured to receive mapping information corresponding to the one or more data points in response to the request and locally store the mapping information at the local data store.
- the disclosed subject matter also relates to a machine-readable medium comprising instructions stored therein, which when executed by a machine, cause the machine to perform operations comprising analyzing an image captured at an image capture device.
- the operations may further include identify one or more data points corresponding to a first person based on the analyzing.
- the operations may further include determining if mapping information corresponding to the one or more data points is locally available at the image capture device, the mapping information comprising a mapping of a set of data points to a person.
- the operations may further include sending a request to a remote device for mapping information when it is determined that mapping information corresponding to the one or more data points is locally available at the image capture device and providing identifier information associated with the first person at the image capture device if mapping information corresponding to the one or more data points is locally available at the image capture device.
- FIG. 1 illustrates an example client-server network environment, which provides for real-time facial recognition of images captured using an image capture device.
- FIG. 2 illustrates a flow diagram of an example process 200 for real-time facial recognition of images captured using an image capture device.
- FIG. 3 conceptually illustrates an electronic system with which some implementations of the subject technology are implemented.
- the present disclosure provides a method and system for real-time facial recognition of images captured using an image capture device.
- the image capture device may comprise image capturing glasses, or other similar image capture devices, configured to capture images of items in the field of view of the image capture device.
- the image capture device may be a wearable portable image capture device.
- the image capture device captures images and sends the images to a server for processing to facilitate real-time and/or local facial recognition for one or more persons ("users") in the vicinity and/or field of view of the image capture device.
- the image capture device may scan its surrounding and capture images of its surroundings (e.g., objects within its field of view).
- the images may be captured periodically (e.g., according to a capture rate), continuously, and/or once an object is detected within the field of view of the image capture device.
- the image capture device is in communication with a local database and/or a remote server.
- the local database and/or the remote server e.g., at a remote database
- the image capture device upon capturing an image, compares the data points within the captured image to the mapping information in the local database.
- the data points extracted from the capture image are compared to the data points stored for each user. If, in response to the comparison, a mapping corresponding to the data points (e.g., a mapping associated with data points that match the captured image data points) is identified, the user associated with the mapping is identified as the user corresponding to the captured image (or the captured data points).
- a mapping corresponding to the data points e.g., a mapping associated with data points that match the captured image data points
- the data points are sent to the remote server for analysis.
- the remote server has access to a database of mapping information for one or more users.
- the mapping information for each user includes a mapping of data points (indexes) to a user (e.g., a set of data points, extracted from one or more images, mapped to the person which the data points represent).
- the database is built by accessing one or more data sources having image and tag data (e.g., one or more social networking services).
- image and tag data provides several images (e.g., photos, videos) and tag information indicating that an image corresponds to a specific user (e.g., identified by a user identifier).
- the images for each user can be scanned to build a set of data points corresponding to that user and may be stored within the database as a mapping of the data points and the user corresponding to the data points.
- user information may also be obtained through other sources such as emails, calendar information, maps or other services having user information (including image and tag data, or other user identifying information).
- the server upon receiving an image from the image capture device, compares the data points within the image to the mappings stored in the database to find mapping information corresponding to the data points within the captured image.
- the location of the image capture device may be compared with the location of a user to determine and/or confirm a mapping of the data points of the captured image and a user (e.g., if the user is close to the image capture device, it is more likely that the image captured corresponds to the user).
- the location of an identified user in response to comparing data points (e.g., the captured data points to data points associated with mappings), may be compared against the location of the image capture device to confirm that the correct user is identified.
- mapping information corresponding to the data points is identified (e.g., data points associated with the mapping match the data points of the captured image)
- the index containing the user e.g., the index corresponding to the mapping
- the image capture device then can access the local database to identify the user by accessing the mapping information stored in the local database (e.g., as long as the user is in the field of view of the image capture device).
- the information regarding the user may be provided for display at the image capture device. For example, a box may be placed around the user (e.g., while the user is still in view) and/or the identifier information for the user may be provided for display.
- identifier information about the user may also be provided in other manners including for example audio announcing the identifier information, or other signals or indication that the user is within the field of view of the image capture device.
- the remote server may index the image and store the index information along with some additional contextual information (e.g., in a second data base).
- the contextual information may, for example, include recency of the image, length of interaction, and frequency of interaction, or other information indicative of possible importance or relevance of the captured user with respect the user associated with the image capture device (e.g., user holding or wearing the image capture device).
- a score is calculated based on the contextual information. The calculated score may be compared to other image data (e.g., for one or more other captured images) stored in the database and/or to a threshold score.
- the index information for the captured image (e.g., user face) is stored if the score meets a pre-defined condition based on the comparison. Additional inputs of the image capture device (e.g., audio, user input, etc.) or other sources (e.g., social networking services, etc.), may be used in some examples, to identify the user corresponding to the unknown captured image.
- Index information for a user may also be provided in response to geolocation information indicating that a nearby user is likely to be in the vicinity of the image capture device.
- the proximity of a user e.g., a user other than the operator of the image capture device, contacts, etc.
- location information regarding various users including for example users (e.g., contacts) associated with the user operating the image capture device, may be determined and compared to the location of the image capture device (or the user operating the device).
- the server may provide mapping information (e.g., indexes) corresponding to the nearby user to the image capture device for storage at the local database.
- mapping information e.g., indexes
- the image capture device has the mapping information to recognize the nearby user in real-time (e.g., while the user is still in the field of view) once the user enters the field of view of the image capture device using local information stored at the local database.
- FIG. 1 illustrates an example client-server network environment, which provides for real-time facial recognition of images captured using an image capture device.
- a network environment 100 includes an image capture device 1 10 and a server 120 communicably connected by a network 130.
- Network 130 can be a public communication network (e.g., the Internet, cellular data network, dialup modems over a telephone network) or a private communications network (e.g., private LAN, leased lines). Further, network 130 can include, but is not limited to, any one or more of the following network topologies, including a bus network, a star network, a ring network, a mesh network, a star-bus network, a tree or hierarchical network, and the like.
- image capture device 1 10 can be computing devices such as glasses, laptop or desktop computers, smartphones, PDAs, portable media players, tablet computers, or other appropriate computing devices having image capturing capability.
- image capture device 110 includes a recognition module 1 12 and a local data store 1 14.
- Recognition module 1 12 may be configured to facilitate real-time facial recognition of images captured using the image capture device 1 10.
- Local data store 1 14 e.g., a cache
- mapping information for facilitating real-time facial recognition of images captured using the image capture device 1 10.
- Server 120 may be a processor-based computing device such as a computer server, or other computing device, including a recognition module 122, mapping module 124, geolocation module 126 and unknown user module 128.
- server 120 (including one or more of the modules 122, 124, 126 and 128) may have access to an index storage 123, a geolocation history storage 125 and an unknown index storage 127.
- recognition module 122 is in communication with the image capture device 1 10 (e.g., through network 130) and is configured to receive captured image information from the image capture device.
- the recognition module 122 is configured to determine a mapping of a user to the captured image and provide mapping information to the image capture device for facilitating real-time facial recognition of images captured using the image capture device.
- the mapping service 124 may be configured to receive data points corresponding to a captured image, or portion thereof (e.g., a portion representing a face), and determine a mapping of the data points within the captured image, or portion thereof, to a user.
- the index storage 123 includes mapping information.
- the mapping information defines a mapping from a set of data points to a user for a plurality of users.
- the information stored at the index storage 123 is generated from image and tag data stored at an image and tag information source 140.
- the image and tag information source 140 may include one or more services or data stores (e.g., social networking services) storing image and tag data corresponding to one or more users.
- server 120 e.g., at the mapping module 1214 is configured to receive image and tag data from the image and tag information source 140 and generate mapping information (e.g., indexes) for storage in the index storage 123.
- the geolocation module 126 may be configured to determine geographic matches between a geographic location of a user associated with (e.g., operating) the image capture device 1 10 and one or more other users in the vicinity. In one example, location information may be provided by the image capture device 110 and compared to data regarding location of one or more other users.
- geolocation history storage 125 may also store location information (e.g., geographic data generated by a location service, based on matches between an image and a user, or otherwise available to the server) for future use.
- geolocation module 126 may be configured to compare mappings generated by the mapping module 124 to confirm the mapping using geolocation data.
- geolocation module 126 may be configured to determine likely users to be in the vicinity of the image capture device 1 10, such that mapping information for the users can be sent to the image capture device 1 10.
- the unknown user module 128 is configured to determine and/or store information regarding images (e.g., users) captured by the image capture device 100 for which mapping information is not readily available (e.g., within the index storage). In some examples, the unknown user module 128 may be configured to store user information for one or more unknown users within the unknown index storage 127. In some examples, the unknown user module 128 may be configured to determine a mapping of unknown user data points to a user according to various criteria.
- server 120 can be a single computing device such as a computer server. In other implementations, server 120 can represent more than one computing device working together to perform the actions of a server computer (e.g., cloud computing). Server 120 may be coupled with various remote databases or storage services. While server 120 is displa yed as being remote from one another, it should be understood that the functions performed by these servers may be performed within a single server, or across multiple servers.
- Communications between the image capture device 1 10, server 120 and/or image and tag information source 130 may be facilitated through various communication protocols.
- one or more of the image capture device 1 10, server 120 and/or image and tag information source 140, and/or one or more remote services may communicate wirelessly through a communication interface (not shown), which may include digital signal processing circuitry where necessary.
- the communication interface may provide for communications under various modes or protocols, including Global System for Mobile communication (GSM) voice calls, Short Message Service (SMS), Enhanced Messaging Service (EMS), or Multimedia Messaging Service (MMS) messaging, Code Division Multiple Access (CDMA), Time Division Multiple Access (TDMA), Personal Digital Cellular (PDC), Wideband Code Division Multiple Access (WCDMA), CDMA2000, or General Packet Radio System (GPRS), among others.
- GSM Global System for Mobile communication
- SMS Short Message Service
- EMS Enhanced Messaging Service
- MMS Multimedia Messaging Service
- CDMA Code Division Multiple Access
- TDMA Time Division Multiple Access
- PDC Personal Digital Cellular
- WCDMA Wideband Code Division Multiple Access
- CDMA2000 Code Division Multiple Access 2000
- GPRS General Packet Radio System
- the network 130 can include, for example, any one or more of a personal area network (PAN), a local area network (LAN), a campus area network (CAN), a metropolitan area network (MAN), a wide area network (WAN), a broadband network (BBN), the Internet, and the like.
- PAN personal area network
- LAN local area network
- CAN campus area network
- MAN metropolitan area network
- WAN wide area network
- BBN broadband network
- the network 108 can include, but is not limited to, any one or more of the following network topologies, including a bus network, a star network, a ring network, a mesh network, a star-bus network, tree or hierarchical network, and the like.
- the image capture device 1 10 is configured to capture images of its surroundings (e.g., objects within its field of view). The images may be captured periodically (e.g., according to a capture rate), continuously, and/or once an object is detected within the field of view of the image capture device.
- the recognition module 112 may receive the captured image and may be facilitated to generate data points corresponding to an image of a user (e.g., data points corresponding to a human face within the captured image).
- the recognition module 112 may be further configured to compare the generated data points to mapping information stored in the local data store 1 14 to determine if the data points correspond to a mapping stored in the data store 1 14, for example, by comparing the data points to the data points for each mapping. If so, the mapping is retrieved and the user within the image is identified. [0032] Otherwise, if mapping information regarding the data points within the captured image are not available within the data store 1 14, the data points are sent to the server 120 for analysis.
- the recognition module 122 provides a request to the mapping module 124 for a mapping corresponding to the data points within the captured image.
- the mapping module 124 may access the index storage 123 to determine if a mapping having indexes matching the data points is available at the index storage 123.
- the location of the image capture device may be determined.
- the recognition service 122 may send a request to the geolocation service 126 for comparison of the determined location of the image capture device, or the user operating the device, with the location of a user to determine and/or confirm that he data points of the captured image match data points mapped to a user (e.g., if the user is close to the image capture device, it is more likely that the image captured corresponds to the user).
- mapping module 124 and/or geolocation module 126 if a mapping is identified by the mapping module 124 and/or geolocation module 126, the mapping is provided to the recognition module 122.
- the recognition module 122 sends mapping information (e.g., the index containing the user corresponding to the image) to the image capture device 1 10.
- the mapping information is received at the image capture device and stored at the local data store 1 14.
- the recognition module 1 12 can, in some examples, access the local data store 114 to identify the user using the mapping information (e.g., as long as the user is in the field of view of the image capture device 10).
- server 120 may provide mapping information for storage within local data store 1 14 in response to geolocation information indicating that a user is likely to be in the vicinity of the image capture device. That is, geographic location of the image capture device may be compared to the location of other users, for example at the geolocation module 126, to determine if one or more other users are in the vicinity of the image capture device 1 10, which increases the likelihood that the image capture device 1 10 will at some point in the near future capture an image of the nearby user and thus require mapping information corresponding to the user to recognize the user.
- the location information for the users and/or image capture device may be retrieved from geolocation history storage 125 and may include various information such as calendar data, or other similar data indicating a location of users.
- information regarding the location of the image capture device 110 and/or one or more users (or devices operated by the users) may be retrieved from one or more third party services (e.g., GPS services, user mobile device locating services, etc.).
- the geolocation module 126 may provide mapping information corresponding to the nearby user (e.g., via recognition module 122) to the image capture device 1 10 for storage at the local data store 1 14.
- the recognition module 1 12 is configured to provide information regarding the user at the image capture device 1 10 once the user is identified For example, a box may be placed around the user (e.g., while the user is still in view) and the identifier information for the user may be provided for display. In other example, the identifier information may be provided to the user operating the image capture device in various other ways (e.g., audio or other signals).
- the data points sent by the image capture device 1 10 to the server 120 may not correspond to any mapped indexes based on the findings of mapping module 124 and/or geolocation module 126
- the data points of the captured image may be provided to the unknown user module 126.
- the unknown user module 126 may index the image and/or data points and store the index information along with some additional contextual information at the unknown index storage 127.
- the unknown user module 126 may calculate a score based on the contextual information. The calculated score may be compared to scores for other images stored in the unknown index storage database 127 and/or to a threshold score.
- the indexed data from the captured image (e.g., user face) is stored in the unknown index storage 128 if the score meets a pre-defined condition based on the comparison.
- Additional input of the image capture device 110 e.g., audio
- FIG. 2 illustrates a flow diagram of an example process 200 for real-time facial recognition of images captured using an image capture device.
- an image is captured at an image capture device.
- the image capture device may scan its surrounding and capture images of its surroundings (e.g., objects within its field of view). The images may be captured periodically (e.g., according to a capture rate), continuously, and/or once an object is detected within the field of view of the image capture device.
- the captured image is analyzed to identify one or more data points corresponding to a first person within the image.
- various techniques may be used for recognizing faces of one or more persons within an image. Each recognized face may then be analyzed to determine one or more data points representing features of the face, such that facial recognition may be performed with respect to the recognized face image.
- mapping information corresponding to the data points is available locally.
- a local storage e.g., cache
- the image capture device upon capturing an image, compares the data points within the captured image to the data points of the mappings in the local database.
- the local database may include mapping information provided in response to prior requests send to a remote server storing mapping information as described in more detail below with respect to process 200.
- Mapping information for a user may also be provided for local storage, and thus available within the local database, in response to geolocation information indicating that a user is likely to be in the vicinity of the image capture device, which increases the likelihood that the image capture device will at some point in the near future capture an image of the user and thus require indexes corresponding to the user to recognize the user.
- step 204 information regarding the person captured in the image is retrieved according to the mapping information.
- the information may include an identifier of the person (e.g., name, id, etc.).
- the information may include other demographic information regarding the user.
- the information may further include a profile, account, page or other data corresponding to the user.
- the information is provided to the user at the image capture device.
- the information regarding the user may be provided for display at the image capture device.
- a box may be placed around the portion of the image including the face of the user (e.g., while the user is still in view) and the identifier information for the user may be provided for display.
- an audio notification may be provided at the image capture device indicating that the identified user is within the field of view of the image capture device.
- step 206 a request is sent to a remote server for mapping information corresponding to the data points.
- the request includes the data points identified in step 202.
- the captured image may be sent to the remote server and the remote server may analyze the image to identify the one or more data points.
- the remote server has access to a database of mapping information corresponding to a set of users.
- the mapping information for each user may include a set of data points mapped to the user which the data points represent.
- the database is built by accessing one or more data sources having image and tag data (e.g., one or more social networking services).
- image and tag data provides images (e.g., photos, videos) and tag information indicating that each image (or portion thereof) corresponds to a specific user (e.g., identified by a user identifier).
- the images for each user can be scanned to build a set of data points corresponding to each user and may be stored within the database.
- the server upon receiving an image and/or data points corresponding to a person from the image capture device, compares the data points within the image to the data points stored in the database to find a mapping corresponding to the data points within the captured image.
- the location of the image capture device may be compared with location of a user to determine and/or confirm a mapping of the data points to a user (e.g., if the user is close to the image capture device, it is more likely that the image captured corresponds to the user).
- mapping information corresponding to the captured data points is identified, in some examples, the identified mapping information (e.g., the index containing the user represented by the image) is sent to the image capture device. In some implementations, if the server determines that the data points sent by the server do not correspond to any mapping information in the server database, the server may index the image and store the index information along with some additional contextual information.
- step 207 it is determined if mapping information corresponding to the image and/or the data points within the image is received in response to the request. If so, in step 208, the mapping information is stored at the local database. The image capture device then can access the local database to identify the user corresponding to the data points (e.g., as long as the user is in the field of view of the image capture device). Otherwise, if no mapping information is received, the process ends in step 209. In some examples, a warning or other indication that the user cannot be recognized can be provided for display.
- the term "software” is meant to include firmware residing in read-only memory or applications stored in magnetic storage, which can be read into memory for processing by a processor.
- multiple software aspects of the subject disclosure can be implemented as sub-parts of a larger program while remaining distinct software aspects of the subject disclosure.
- multiple software aspects can also be implemented as separate programs.
- any combination of separate programs that together implement a software aspect described here is within the scope of the subject disclosure.
- the software programs when installed to operate on one or more electronic systems, define one or more specific machine implementations that execute and perform the operations of the software programs.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- FIG. 3 conceptually illustrates an electronic system with which some implementations of the subject technology are implemented.
- Electronic system 300 can be a server, computer, phone, PDA, laptop, tablet computer, television with one or more processors embedded therein or coupled thereto, or any other sort of electronic device.
- Such an electronic system includes various types of computer readable media and interfaces for various other types of computer readable media.
- Electronic system 300 includes a bus 308, processing unit(s) 312, a system memory 304, a read-only memory (ROM) 310, a permanent storage device 302, an input device interface 314, an output device interface 306, and a network interface 316.
- Bus 308 collectively represents all system, peripheral, and chipset buses that communicatively connect the numerous internal devices of electronic system 300. For instance, bus 308 communicatively connects processing unit(s) 312 with ROM 310, system memory 304, and permanent storage device 302.
- processing unit(s) 312 retrieves instructions to execute and data to process in order to execute the processes of the subject disclosure.
- the processing unit(s) can be a single processor or a multi-core processor in different implementations.
- ROM 310 stores static data and instructions that are needed by processing unit(s) 312 and other modules of the electronic system.
- Permanent storage device 302 is a read-and-write memory device. This device is a non-volatile memory unit that stores instructions and data even when electronic system 300 is off.
- Some implementations of the subject disclosure use a mass-storage device (such as a magnetic or optical disk and its corresponding disk drive) as permanent storage device 302.
- system memory 304 is a read-and-write memory device. However, unlike storage device 302, system memory 304 is a volatile read-and-write memory, such a random access memory.
- System memory 304 stores some of the instructions and data that the processor needs at runtime.
- the processes of the subject disclosure are stored in system memory 304, permanent storage device 302, and/or ROM 310.
- the various memory units include instructions for facilitating real-time facial recognition according to various embodiments. From these various memory units, processing unit(s) 312 retrieves instructions to execute and data to process in order to execute the processes of some implementations .
- Bus 308 also connects to input and output device interfaces 314 and 306.
- Input device interface 314 enables the user to communicate information and select commands to the electronic system.
- Input devices used with input device interface 314 include, for example, alphanumeric keyboards and pointing devices (also called “cursor control devices").
- Output device interfaces 306 enables, for example, the display of images generated by the electronic system 300.
- Output devices used with output device interface 306 include, for example, printers and display devices, such as cathode ray tubes (CRT) or liquid crystal displays (LCD). Some implementations include devices such as a touchscreen that functions as both input and output devices.
- CTR cathode ray tubes
- LCD liquid crystal displays
- bus 308 also couples electronic system 300 to a network (not shown) through a network interface 316.
- the computer can be a part of a network of computers (such as a local area network (“LAN”), a wide area network (“WAN”), or an Intranet, or a network of networks, such as the Internet. Any or all components of electronic system 300 can be used in conjunction with the subject disclosure.
- Some implementations include electronic components, such as microprocessors, storage and memory that store computer program instructions in a machine-readable or computer-readable medium (alternatively referred to as computer-readable storage media, machine-readable media, or machine-readable storage media).
- electronic components such as microprocessors, storage and memory that store computer program instructions in a machine-readable or computer-readable medium (alternatively referred to as computer-readable storage media, machine-readable media, or machine-readable storage media).
- Such computer-readable media include RAM, ROM, read-only compact discs (CD-ROM), recordable compact discs (CD-R), rewritable compact discs (CD-RW), read-only digital versatile discs (e.g., DVD-ROM, dual-layer DVD-ROM), a variety of recordable/rewritable DVDs (e.g., DVD-RAM, DVD-R W, DVD+RW, etc.), flash memory (e.g., SD cards, mini-SD cards, micro-SD cards, etc.), magnetic and/or solid state hard drives, read-only and recordable Blu-Ray® discs, ultra density optical discs, any other optical or magnetic media, and floppy disks.
- RAM random access memory
- ROM read-only compact discs
- CD-R recordable compact discs
- CD-RW rewritable compact discs
- read-only digital versatile discs e.g., DVD-ROM, dual-layer DVD-ROM
- flash memory e.g., SD cards, mini
- the computer-readable media can store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations.
- Examples of computer programs or computer code include machine code, such as is produced by a compiler, and files including higher-level code that are executed by a computer, an electronic component, or a microprocessor using an interpreter.
- ASICs application specific integrated circuits
- FPGAs field programmable gate arrays
- integrated circuits execute instructions that are stored on the circuit itself.
- the terms "computer”, “server”, “processor”, and “memory” all refer to electronic or other technological devices. These terms exclude people or groups of people.
- display or displaying means displaying on an electronic device.
- computer readable medium and “computer readable media” are entirely restricted to tangible, physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals, wired download signals, and any other ephemeral signals.
- implementations of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-to-peer networks.
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- client device e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device.
- Data generated at the client device e.g., a result of the user interaction
- any specific order or hierarchy of steps in the processes disclosed is an illustration of exemplary approaches. Based upon design preferences, it is understood that the specific order or hierarchy of steps in the processes may be rearranged, or that some illustrated steps may not be performed. Some of the steps may be performed simultaneously. For example, in certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- a phrase such as an "aspect” does not imply that such aspect is essential to the subject technology or that such aspect applies to all configurations of the subject technology.
- a disclosure relating to an aspect may apply to all configurations, or one or more configurations.
- a phrase such as an aspect may refer to one or more aspects and vice versa.
- a phrase such as a “configuration” does not imply that such configuration is essential to the subject technology or that such configuration applies to all configurations of the subject technology.
- a disclosure relating to a configuration may apply to all configurations, or one or more configurations.
- a phrase such as a configuration may refer to one or more configurations and vice versa.
Abstract
Description
Claims
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/099,781 US9569656B2 (en) | 2013-12-06 | 2013-12-06 | Local real-time facial recognition |
PCT/US2014/068918 WO2015085246A1 (en) | 2013-12-06 | 2014-12-05 | Local real-time facial recognition |
Publications (3)
Publication Number | Publication Date |
---|---|
EP3077957A1 true EP3077957A1 (en) | 2016-10-12 |
EP3077957A4 EP3077957A4 (en) | 2017-07-19 |
EP3077957B1 EP3077957B1 (en) | 2021-06-23 |
Family
ID=53272432
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP14868544.9A Active EP3077957B1 (en) | 2013-12-06 | 2014-12-05 | Local real-time facial recognition |
Country Status (4)
Country | Link |
---|---|
US (1) | US9569656B2 (en) |
EP (1) | EP3077957B1 (en) |
CN (1) | CN105814587B (en) |
WO (1) | WO2015085246A1 (en) |
Families Citing this family (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180285312A1 (en) * | 2014-03-04 | 2018-10-04 | Google Inc. | Methods, systems, and media for providing content based on a level of conversation and shared interests during a social event |
US10178301B1 (en) * | 2015-06-25 | 2019-01-08 | Amazon Technologies, Inc. | User identification based on voice and face |
US10094655B2 (en) | 2015-07-15 | 2018-10-09 | 15 Seconds of Fame, Inc. | Apparatus and methods for facial recognition and video analytics to identify individuals in contextual video streams |
EP3365838A4 (en) | 2015-10-21 | 2019-08-28 | 15 Seconds Of Fame, Inc. | Methods and apparatus for false positive minimization in facial recognition applications |
US10083720B2 (en) * | 2015-11-06 | 2018-09-25 | Aupera Technologies, Inc. | Method and system for video data stream storage |
CN106845353B (en) * | 2016-12-24 | 2018-06-29 | 深圳云天励飞技术有限公司 | A kind of image processing system |
US10715581B2 (en) * | 2017-01-25 | 2020-07-14 | International Business Machines Corporation | System and method to download file from common recipient devices in proximity |
US10936856B2 (en) | 2018-08-31 | 2021-03-02 | 15 Seconds of Fame, Inc. | Methods and apparatus for reducing false positives in facial recognition |
US11010596B2 (en) | 2019-03-07 | 2021-05-18 | 15 Seconds of Fame, Inc. | Apparatus and methods for facial recognition systems to identify proximity-based connections |
CN110598565A (en) * | 2019-08-16 | 2019-12-20 | 信利光电股份有限公司 | Far and near face recognition method and device and automatic door |
US11341351B2 (en) | 2020-01-03 | 2022-05-24 | 15 Seconds of Fame, Inc. | Methods and apparatus for facial recognition on a user device |
US11600111B2 (en) * | 2020-06-01 | 2023-03-07 | Paralaxiom Technologies Private Limited | System and method for face recognition |
CN112995148A (en) * | 2020-06-22 | 2021-06-18 | 威盛电子股份有限公司 | Driver login device, system and method |
Family Cites Families (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7130454B1 (en) | 1998-07-20 | 2006-10-31 | Viisage Technology, Inc. | Real-time facial recognition and verification system |
US20030161507A1 (en) | 2002-02-28 | 2003-08-28 | Spectra Systems Corporation | Method and apparatus for performing facial recognition with a hand-held imaging device |
US7751805B2 (en) | 2004-02-20 | 2010-07-06 | Google Inc. | Mobile image-based information retrieval system |
DE112005002101T5 (en) * | 2004-09-01 | 2007-08-23 | Creative Technology Ltd | System for operating multi-image capture devices |
US7796837B2 (en) | 2005-09-22 | 2010-09-14 | Google Inc. | Processing an image map for display on computing device |
US7689011B2 (en) * | 2006-09-26 | 2010-03-30 | Hewlett-Packard Development Company, L.P. | Extracting features from face regions and auxiliary identification regions of images for person recognition and other applications |
US8098904B2 (en) | 2008-03-31 | 2012-01-17 | Google Inc. | Automatic face detection and identity masking in images, and applications thereof |
JP2011516966A (en) | 2008-04-02 | 2011-05-26 | グーグル インコーポレイテッド | Method and apparatus for incorporating automatic face recognition in a digital image collection |
US8605956B2 (en) | 2009-11-18 | 2013-12-10 | Google Inc. | Automatically mining person models of celebrities for visual search applications |
US8611601B2 (en) * | 2011-03-08 | 2013-12-17 | Bank Of America Corporation | Dynamically indentifying individuals from a captured image |
US9317530B2 (en) | 2011-03-29 | 2016-04-19 | Facebook, Inc. | Face recognition based on spatial and temporal proximity |
US8261090B1 (en) | 2011-09-28 | 2012-09-04 | Google Inc. | Login to a computing device based on facial recognition |
KR101901591B1 (en) * | 2011-11-01 | 2018-09-28 | 삼성전자주식회사 | Face recognition apparatus and control method for the same |
US9087273B2 (en) | 2011-11-15 | 2015-07-21 | Facebook, Inc. | Facial recognition using social networking information |
US9471838B2 (en) * | 2012-09-05 | 2016-10-18 | Motorola Solutions, Inc. | Method, apparatus and system for performing facial recognition |
-
2013
- 2013-12-06 US US14/099,781 patent/US9569656B2/en active Active
-
2014
- 2014-12-05 WO PCT/US2014/068918 patent/WO2015085246A1/en active Application Filing
- 2014-12-05 CN CN201480066605.9A patent/CN105814587B/en active Active
- 2014-12-05 EP EP14868544.9A patent/EP3077957B1/en active Active
Also Published As
Publication number | Publication date |
---|---|
CN105814587B (en) | 2020-08-07 |
US9569656B2 (en) | 2017-02-14 |
EP3077957A4 (en) | 2017-07-19 |
CN105814587A (en) | 2016-07-27 |
US20150163448A1 (en) | 2015-06-11 |
WO2015085246A1 (en) | 2015-06-11 |
EP3077957B1 (en) | 2021-06-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9569656B2 (en) | Local real-time facial recognition | |
US11749266B2 (en) | Voice commands across devices | |
US20200342393A1 (en) | Inventorying items using image data | |
US10891106B2 (en) | Automatic batch voice commands | |
US11593348B2 (en) | Programmatically managing partial data ownership and access to record data objects stored in network accessible databases | |
US10275513B1 (en) | Providing application functionality | |
US20180173882A1 (en) | Privacy-based degradation of activity signals and automatic activation of privacy modes | |
US9336358B2 (en) | Granting permission in association with an application | |
US11792242B2 (en) | Sharing routine for suggesting applications to share content from host application | |
EP3030989B1 (en) | Providing information in association with a search field | |
KR20160048708A (en) | Recognition method and apparatus for communication message | |
EP3931736A1 (en) | Data privacy using a podium mechanism | |
US10628392B1 (en) | Customized rule-based collections | |
US9652442B1 (en) | Virtual photo wall | |
US9721032B2 (en) | Contextual URL suggestions | |
US10796384B2 (en) | Suggesting pre-created groups based on a user web identity and online interactions | |
US10095773B1 (en) | Processing a social endorsement for an item | |
US11244354B2 (en) | System and methods for providing recommendations | |
US20180316633A1 (en) | Automatic determination of data items for a coincident event | |
US10885136B1 (en) | Audience filtering system | |
US20160094713A1 (en) | Communication message recognition method and device thereof | |
CN106415652B (en) | Suggesting pre-created groups based on user WEB identity and online interactions | |
AU2016399752B2 (en) | Secondary computing device assistant | |
US10419555B2 (en) | Computing system and methods for associating computing devices | |
US9721012B1 (en) | Providing social presence information for content items |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20160622 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAX | Request for extension of the european patent (deleted) | ||
A4 | Supplementary search report drawn up and despatched |
Effective date: 20170621 |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06K 9/46 20060101ALI20170614BHEPIpc: G06K 9/20 20060101AFI20170614BHEPIpc: G06K 9/00 20060101ALN20170614BHEP |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20181123 |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06K 9/20 20060101AFI20200929BHEPIpc: G06K 9/00 20060101ALN20200929BHEPIpc: G06K 9/46 20060101ALI20200929BHEP |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06K 9/46 20060101ALI20201013BHEPIpc: G06K 9/20 20060101AFI20201013BHEPIpc: G06K 9/00 20060101ALN20201013BHEP |
|
INTG | Intention to grant announced |
Effective date: 20201029 |
|
GRAJ | Information related to disapproval of communication of intention to grant by the applicant or resumption of examination proceedings by the epo deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
INTC | Intention to grant announced (deleted) | ||
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06K 9/46 20060101ALI20201216BHEPIpc: G06K 9/00 20060101ALN20201216BHEPIpc: G06K 9/20 20060101AFI20201216BHEP |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06K 9/00 20060101ALN20201218BHEPIpc: G06K 9/46 20060101ALI20201218BHEPIpc: G06K 9/20 20060101AFI20201218BHEP |
|
INTG | Intention to grant announced |
Effective date: 20210121 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602014078355Country of ref document: DERef country code: ATRef legal event code: REFRef document number: 1404937Country of ref document: ATKind code of ref document: TEffective date: 20210715 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: FP |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG9D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210923 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1404937Country of ref document: ATKind code of ref document: TEffective date: 20210623 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R079Ref document number: 602014078355Country of ref document: DEFree format text: PREVIOUS MAIN CLASS: G06K0009200000Ipc: G06V0030140000 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210923Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210924Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211025Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602014078355Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
26N | No opposition filed |
Effective date: 20220324 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20211231 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211205Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211205 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211231 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211231Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211231 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20141205 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20221228Year of fee payment: 9 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230506 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20231227Year of fee payment: 10 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: NLPayment date: 20231226Year of fee payment: 10Ref country code: FRPayment date: 20231227Year of fee payment: 10 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210623 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20231229Year of fee payment: 10 |