US11860758B2 - System for adjusting application performance based on platform level benchmarking - Google Patents
System for adjusting application performance based on platform level benchmarking Download PDFInfo
- Publication number
- US11860758B2 US11860758B2 US16/978,574 US201816978574A US11860758B2 US 11860758 B2 US11860758 B2 US 11860758B2 US 201816978574 A US201816978574 A US 201816978574A US 11860758 B2 US11860758 B2 US 11860758B2
- Authority
- US
- United States
- Prior art keywords
- application
- applications
- performance
- fix
- computing system
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 238000000034 method Methods 0.000 claims description 39
- 238000012986 modification Methods 0.000 claims description 10
- 230000004048 modification Effects 0.000 claims description 10
- 230000004044 response Effects 0.000 claims description 8
- 238000004458 analytical method Methods 0.000 description 31
- 238000012937 correction Methods 0.000 description 18
- 230000006870 function Effects 0.000 description 15
- 230000015654 memory Effects 0.000 description 15
- 238000004891 communication Methods 0.000 description 14
- 230000006872 improvement Effects 0.000 description 10
- 238000011156 evaluation Methods 0.000 description 9
- 238000005516 engineering process Methods 0.000 description 7
- 238000009877 rendering Methods 0.000 description 6
- 230000009471 action Effects 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 208000036993 Frustration Diseases 0.000 description 3
- 230000008901 benefit Effects 0.000 description 3
- 230000007547 defect Effects 0.000 description 3
- 238000012545 processing Methods 0.000 description 3
- 230000001413 cellular effect Effects 0.000 description 2
- 239000002131 composite material Substances 0.000 description 2
- 238000013480 data collection Methods 0.000 description 2
- 238000011982 device technology Methods 0.000 description 2
- 239000000835 fiber Substances 0.000 description 2
- 230000007774 longterm Effects 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 238000005259 measurement Methods 0.000 description 2
- 239000000203 mixture Substances 0.000 description 2
- 230000001052 transient effect Effects 0.000 description 2
- 238000003491 array Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 230000007812 deficiency Effects 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 230000002068 genetic effect Effects 0.000 description 1
- 230000036541 health Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 230000035772 mutation Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 230000002265 prevention Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/34—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment
- G06F11/3409—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment for performance assessment
- G06F11/3428—Benchmarking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/07—Responding to the occurrence of a fault, e.g. fault tolerance
- G06F11/0703—Error or fault processing not based on redundancy, i.e. by taking additional measures to deal with the error or fault not making use of redundancy in operation, in hardware, or in data representation
- G06F11/0793—Remedial or corrective actions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/3051—Monitoring arrangements for monitoring the configuration of the computing system or of the computing system component, e.g. monitoring the presence of processing resources, peripherals, I/O links, software programs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/32—Monitoring with visual or acoustical indication of the functioning of the machine
- G06F11/324—Display of status information
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/34—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment
- G06F11/3452—Performance evaluation by statistical analysis
Definitions
- a software developer may analyze the performance data (e.g., data about application stability, rendering times, battery usage) to debug issues or otherwise make improvements to the application. For example, a developer may release an application update that fixes a periodic crash identified from performance data collected from a subset of devices that execute the application.
- performance data e.g., data about application stability, rendering times, battery usage
- an issue may not be apparent from performance data but may still be observable to a user or detectable in other ways.
- An application developer may therefore be unaware that a performance issue with their application exists.
- an issue may be attributed to a defect in a hardware component, an operating system or computing platform, or a computing service that is maintained outside an application developer's control.
- different software developers may duplicate efforts to resolve similar issues that appear during execution of their respective applications.
- the application developer may lack the knowledge to resolve the issue. As a result, an application may underperform, potentially causing frustration for end users.
- a computing system may obtain application performance data (e.g., data about application stability, rendering times, battery usage, permission denials, startup times, wireless radio scans, and other information indicative of an anomaly or performance metric) associated with a plurality of different applications that execute on computing devices that operate corresponding (or at least similar) computing platforms.
- the computing system may normalize the performance data for each application by deriving, from the performance data, one or more performance metrics for each application.
- the computing system may compare a performance metric of a particular application to a corresponding performance benchmark established by the computing system. Comparing the performance metrics of a particular application to the established performance benchmarks may indicate an application's performance relative to other, different applications that execute on the same or similar computing platform.
- the computing system may determine one or more ways the particular application can improve the performance metric to meet or exceed the performance benchmark.
- the computing system may output a recommended improvement that a software developer can optionally implement (e.g., in a patch or software update) or in some cases, the computing system may automatically (i.e., without user intervention) reconfigure an application executable or other application resource, to automatically implement the improvement.
- an example computing system can identify performance issues with an application before a developer even thinks to look for performance improvements.
- the example computing system may output information about a gap in performance to educate a developer to understand how the performance of his or her application compares to other applications that are available on the platform. A developer need not know what areas can be improved, or even how to improve these areas. Instead, the example computing system may automatically identify areas for improvement and suggest or automatically implement fixes to improve performance of an application to be more in-line with performance of other applications executing on the computing platform.
- the example computing system may provide information to the developer indicating how other software developers resolved a similar issue, thereby preventing a developer from performing unnecessary effort to find a solution. Accordingly, the example computing system may cause applications executing on a particular computing platform to perform better, operate more efficiently, and perhaps result in a more consistent user experience on the computing platform. The example computing system may therefore minimize frustrations users may otherwise experience from interactions with what would otherwise be slow or under-performing applications on a computing platform.
- a computing device and/or computing system may analyze information (e.g., application performance data and the like) only if the computing device and/or the computing system receives explicit permission from a user of the computing device and/or the computing system.
- information e.g., application performance data and the like
- individual users may be provided with an opportunity to provide input to control whether programs or features of the computing device and/or computing system (e.g., a developer service module) can collect and make use of the information.
- the individual users may further be provided with an opportunity to control what the programs or features can or cannot do with the information.
- information collected may be pre-treated in one or more ways before it is transferred, stored, or otherwise used by a computing device and/or computing system, so that personally-identifiable information is removed.
- the example computing system may pre-treat the performance data to ensure that any user identifying information or device identifying information embedded in the performance data is removed.
- the user may have control over whether information is collected about the user and user's device, and how such information, if collected, may be used by the computing device and/or computing system.
- a method includes obtaining, by a computing system, first performance data collected during execution of a first application at a first group of computing devices, determining, based on the first performance data, at least one metric for quantifying performance of the first application, and comparing, by the computing system, the at least one metric to a corresponding benchmark derived from second performance data collected during execution of one or more second applications at a second group of computing devices, each of the one or more second applications being different than the first application.
- the method further includes determining whether the at least one metric is within a threshold amount of the corresponding benchmark, determining, based at least in part on determining that the at least one metric is not within the threshold amount of the corresponding benchmark, a fix to the first application, and outputting, by the computing system, for presentation at a developer device, an indication of the fix to the first application.
- a computer-readable storage medium including instructions that, when executed, cause at least one processor to obtain first performance data collected during execution of a first application at a first group of computing devices, determine, based on the first performance data, at least one metric for quantifying performance of the first application, and compare the at least one metric to a corresponding benchmark derived from second performance data collected during execution of one or more second applications at a second group of computing devices, each of the one or more second applications being different than the first application.
- the instructions when executed, further cause the at least one processor to determine whether the at least one metric is within a threshold amount of the corresponding benchmark, determine, based at least in part on determining that the at least one metric is not within the threshold amount of the corresponding benchmark, a fix to the first application, and output, for presentation at a developer device, an indication of the fix to the first application.
- a computing system in another example, includes at least one processor configured to obtain first performance data collected during execution of a first application at a first group of computing devices, determine, based on the first performance data, at least one metric for quantifying performance of the first application, and compare the at least one metric to a corresponding benchmark derived from second performance data collected during execution of one or more second applications at a second group of computing devices, each of the one or more second applications being different than the first application.
- the at least one processor is further configured to determine whether the at least one metric is within a threshold amount of the corresponding benchmark, determine, based at least in part on determining that the at least one metric is not within the threshold amount of the corresponding benchmark, a fix to the first application, and output, for presentation at a developer device, an indication of the fix to the first application.
- a system including means for obtaining first performance data collected during execution of a first application at a first group of computing devices, means for determining, based on the first performance data, at least one metric for quantifying performance of the first application, and means for comparing the at least one metric to a corresponding benchmark derived from second performance data collected during execution of one or more second applications at a second group of computing devices, each of the one or more second applications being different than the first application.
- the system further includes means for determining whether the at least one metric is within a threshold amount of the corresponding benchmark, means for determining, based at least in part on determining that the at least one metric is not within the threshold amount of the corresponding benchmark, a fix to the first application, and means for outputting, for presentation at a developer device, an indication of the fix to the first application.
- a method in another example, includes outputting, by a computing device, to a computing system, first performance data collected during execution of a first application; receiving, by the computing device, from the computing system, instructions to execute a fix of the first application that improves performance of the first application relative to performance of one or more second applications executing at a group of computing devices, each of the one or more second applications being different than the first application; and executing, by the computing device, the instruction to execute the fix of the first application.
- a computer-readable storage medium including instructions that, when executed, cause at least one processor to output, to a computing system, first performance data collected during execution of a first application; receive, from the computing system, instructions to execute a fix of the first application that improves performance of the first application relative to performance of one or more second applications executing at a group of computing devices, each of the one or more second applications being different than the first application; and execute the instruction to execute the fix of the first application.
- a computing device in another example, includes at least one processor configured to output, to a computing system, first performance data collected during execution of a first application; receive, from the computing system, instructions to execute a fix of the first application that improves performance of the first application relative to performance of one or more second applications executing at a group of computing devices, each of the one or more second applications being different than the first application; and execute the instruction to execute the fix of the first application.
- a system including means for outputting, to a computing system, first performance data collected during execution of a first application; means for receiving, from the computing system, instructions to execute a fix of the first application that improves performance of the first application relative to performance of one or more second applications executing at a group of computing devices, each of the one or more second applications being different than the first application; and means for executing the instruction to execute the fix of the first application.
- a method in another example, includes receiving, by a computing device, from a computing system, a recommendation for a fix of a first application executing at a first group of computing devices that improves performance of the first application relative to performance of one or more second applications executing at a second group of computing devices, each of the one or more second applications being different than the first application; receiving, by the computing device, user input authorizing the fix of the first application; responsive to sending, to the computing system, an indication of the user input authorizing the fix of the first application, receiving, by the computing device, from the computing system, instructions to execute the fix of the first application; and executing, by the computing device, based on the instructions, the fix of the first application.
- a computer-readable storage medium including instructions that, when executed, cause at least one processor to receive, from a computing system, a recommendation for a fix of a first application executing at a first group of computing devices that improves performance of the first application relative to performance of one or more second applications executing at a second group of computing devices, each of the one or more second applications being different than the first application; receive user input authorizing the fix of the first application; responsive to sending, to the computing system, an indication of the user input authorizing the fix of the first application, receive, from the computing system, instructions to execute the fix of the first application; and execute, based on the instructions, the fix of the first application.
- a computing device in another example, includes at least one processor configured to receive, from a computing system, a recommendation for a fix of a first application executing at a first group of computing devices that improves performance of the first application relative to performance of one or more second applications executing at a second group of computing devices, each of the one or more second applications being different than the first application; receive user input authorizing the fix of the first application; responsive to sending, to the computing system, an indication of the user input authorizing the fix of the first application, receive, from the computing system, instructions to execute the fix of the first application; and execute, based on the instructions, the fix of the first application.
- a system including means for receiving, from a computing system, a recommendation for a fix of a first application executing at a first group of computing devices that improves performance of the first application relative to performance of one or more second applications executing at a second group of computing devices, each of the one or more second applications being different than the first application; means for receiving user input authorizing the fix of the first application; responsive to sending, to the computing system, an indication of the user input authorizing the fix of the first application, means for receiving, from the computing system, instructions to execute the fix of the first application; and means for executing, based on the instructions, the fix of the first application.
- FIG. 1 is a conceptual diagram illustrating an example computing system configured to identify performance issues with an application, relative to performance of other applications that execute on similar or corresponding computing platforms, and determine ways to improve performance of the application, in accordance with one or more aspects of the present disclosure.
- FIG. 2 is a block diagram illustrating an example computing system configured to identify performance issues with an application, relative to performance of other applications that execute on similar or corresponding computing platforms, and determine ways to improve performance of the application, in accordance with one or more aspects of the present disclosure.
- FIG. 3 is an example screen shot of a display screen of a computing device that is accessing an application performance evaluation service provided by an example computing system, in accordance with one or more aspects of the present disclosure.
- FIG. 4 is a flowchart illustrating example operations performed by an example computing system configured to identify performance issues with an application, relative to performance of other applications that execute on similar or corresponding computing platforms, and determine ways to improve performance of the application, in accordance with one or more aspects of the present disclosure.
- FIG. 1 is a conceptual diagram illustrating an example computing system configured to identify performance issues with an application, relative to performance of other applications that execute on similar or corresponding computing platforms, and determine ways to improve performance of the application, in accordance with one or more aspects of the present disclosure.
- System 100 includes computing system 160 in communication, via network 130 , with computing device 110 , computing devices 116 A- 116 N (collectively “computing devices 116 ”), and computing devices 118 A- 118 N (collectively “computing devices 118 ”).
- computing devices 118 A- 118 N
- FIG. 1 is a conceptual diagram illustrating an example computing system configured to identify performance issues with an application, relative to performance of other applications that execute on similar or corresponding computing platforms, and determine ways to improve performance of the application, in accordance with one or more aspects of the present disclosure.
- System 100 includes computing system 160 in communication, via network 130 , with computing device 110 , computing devices 116 A- 116 N (collectively “computing devices 116 ”), and computing devices 118 A-
- Computing system 160 includes developer service module 162 and application data store 164 .
- Computing device 110 includes developer client module 120 and further includes user interface component (“UIC”) 112 which is configured to output user interface 114 .
- UICC user interface component
- Each of computing devices 116 executes a respective instance of application 122 A and each of computing devices 118 executes a respective instance of application 122 B.
- Network 130 represents any public or private communications network, for instance, cellular, Wi-Fi, and/or other types of networks, for transmitting data between computing systems, servers, and computing devices.
- Network 130 may include one or more network hubs, network switches, network routers, or any other network equipment, that are operatively inter-coupled thereby providing for the exchange of information between computing system 160 and computing devices 110 , 116 , and 118 .
- Computing system 160 and computing devices 110 , 116 , and 118 may transmit and receive data across network 130 using any suitable communication techniques.
- Computing system 160 and computing devices 110 , 116 , and 118 may each be operatively coupled to network 130 using respective network links.
- the links coupling computing system 160 and computing devices 110 , 116 , and 118 to network 130 may be Ethernet, ATM or other types of network connections, and such connections may be wireless and/or wired connections.
- Computing system 160 represents any combination of one or more computers, mainframes, servers, blades, cloud computing systems, or other types of remote computing systems capable of exchanging information via network 130 as part of an application performance evaluation service. That is, computing system 160 may receive application performance data via network 130 and analyze the performance data to determine performance issues and ways to resolve the performance issues of applications executing at computing devices 116 , 118 . Computing system 160 may output recommendations to fix any identified performance issues uncovered during the analysis of the performance data. In some cases, computing system 160 outputs the recommendations and other information about its analysis to computing device 110 for subsequent presentation, e.g., via user interface 114 , to a developer or other user of computing device 110 . In some examples, computing system 160 may interface with computing devices 116 , 118 directly to implement fixes for improving performance of individual applications executing at computing devices 116 , 118 .
- Developer service module 162 controls computing system 160 to perform specific operations for implementing the application performance evaluation service provided by computing system 160 .
- Developer service module 162 may provide an interface between computing system 160 and client devices, such as computing device 110 , that access the performance evaluation service provided by computing system 160 , e.g., to obtain information about an application's performance and ways to improve the application's performance.
- Developer service module 162 may perform operations described herein using software, hardware, firmware, or a mixture of hardware, software, and firmware residing in and/or executing at computing system 160 .
- Computing system 160 may execute developer service module 162 with multiple processors or multiple devices, as virtual machines executing on underlying hardware, as one or more services of an operating system or computing platform, and/or as one or more executable programs at an application layer of a computing platform of computing system 160 .
- Developer service module 162 is configured to collect application performance data from different applications that execute on similar or corresponding computing platforms. Developer service module 162 analyzes application performance data to determine issues in a particular application's performance, relative to other, different applications that execute in a similar or corresponding execution environment. That is, unlike other debugging or analysis tools that might analyze application performance data for a single application to determine performance issues that might arise as the application executes on different computing architectures, operating systems, or computing platforms, developer service module 162 instead identifies performance discrepancies between individual applications and other, different applications that execute in similar or corresponding computing environments.
- Developer service module 162 collects and stores application performance data collected from network 130 at application data store 164 .
- the performance data (e.g., data about application stability, rendering times, battery usage, permission denials, startup times, wireless radio scans, and other information indicative of an anomaly or performance metric) contained in application data store 164 may be organized and searchable according to an application identifier (e.g., name, publisher, etc.), application category (e.g., travel, leisure, entertainment, etc.), application genre (e.g., navigation, transportation, game, etc.), or other searchable criteria.
- application data store 164 may store first performance data collected from computing devices 116 during their respective executions of application 122 A separately from second performance data collected from computing devices 118 during their respective executions of application 122 B.
- performance data stored in application data store 164 may be a used as a proxy indicator or indirect indicator for performance.
- the data collected and maintained at data store 164 may be low-level performance details that by themselves do not provide insight into a particular issue but when aggregated together, are a sign of performance.
- radio scans and location querying tend to consume a lot of battery power and therefore, a computing platform may request that applications minimize performing such functions unless necessary or only if done with a limited frequency.
- An application therefore that performs more radio scans or locations queries than other applications may indicate that the application is a poor performer, relative to the other applications, for battery consumption.
- Developer service module 162 may make further determinations about things like “rendering performance” or other categories of performance by looking at multiple types of performance data concurrently.
- developer service module 162 may take precautions to ensure that user privacy is preserved. That is, developer service module 162 may only collect, store, and analyze performance data if developer service module 162 receives explicit permission from a user of the computing device from which the performance data originated. For example, in situations discussed below in which developer service module 162 may collect information about performance of applications executing at computing devices 116 , 118 , individual users of computing devices 116 , 118 may be provided with an opportunity to provide input to computing devices 116 , 118 and/or computing system 160 to control whether developer service module 162 can collect and make use of their information. The individual users may further be provided with an opportunity to control what developer service module 162 can or cannot do with the information.
- Application performance data may be pre-treated in one or more ways before it is transferred to, stored by, or otherwise used by developer service module 162 , so that personally-identifiable information is removed. For example, before developer service module 162 collects performance data associated with application 122 A while executing at computing device 116 A, computing device 116 A may pre-treat the performance data to ensure that any user identifying information or device identifying information embedded in the performance data is removed before being transferred to computing system 160 . In other examples, developer service module 162 may pre-treat the performance data upon receipt and before storing the performance data at data store 164 . In either case, the user may have control over whether the performance data is collected, and how such information, if collected, may be used by computing device 116 A and computing system 160 .
- Computing device 110 represents any suitable computing device or computing system capable of exchanging information via network 130 to access the performance evaluation service provided by computing system 160 for obtaining information about application performance and ways to improve application performance. That is, computing device 110 may be a software developer or designer workstation configured to access performance data stored at data store 164 , obtain analysis performed by developer service module 162 , and obtain performance-improvement recommendations generated by developer service module 162 .
- Developer client module 120 may provide the interface between computing device 110 and the service provided by computing system 160 .
- developer client module 120 may be a stand-alone application executing at computing device 110 , or in some examples, developer client module 120 may be a subroutine or internet application accessed from an internet browser executing at computing device 110 . In either case, developer client module 120 is configured to exchange information with computing system 160 to implement the performance evaluation service.
- Developer client module 120 is configured to receive, from computing system 160 and via network 130 , results of an analysis of application performance data and recommendations determined by computing system 160 to improve the results.
- Module 120 may perform operations described herein using software, hardware, firmware, or a mixture of hardware, software, and firmware residing in and/or executing at computing device 110 .
- Computing device 110 may execute module 120 with multiple processors or multiple devices, as virtual machines executing on underlying hardware, as one or more services of an operating system or computing platform, and/or as one or more executable programs at an application layer of a computing platform of computing device 110 .
- UIC 112 of computing device 110 may function as an input and/or output device for computing device 110 .
- UIC 112 may be implemented using various technologies. For instance, UIC 112 may function as an input device using presence-sensitive input screens, microphone technologies, infrared sensor technologies, or other input device technology for use in receiving user input.
- UIC 112 may function as output device configured to present output to a user using any one or more display devices, speaker technologies, haptic feedback technologies, or other output device technology for use in outputting information to a user.
- Developer client module 120 may cause UIC 112 to output a user interface associated with the service provided by computing system 160 .
- developer client module 120 may send instructions to UIC 112 that cause UIC 112 to display user interface 114 at a display screen of UIC 112 .
- User interface 114 may present information obtained from developer service module 162 in visual, audible, or haptic formats so that a developer can better understand how performance of a particular application compares to performance of other applications, executing on similar or corresponding computing platforms.
- Computing devices 116 , 118 each represent any suitable computing device or computing system capable of locally executing applications, such as applications 122 A and 122 B, and further capable of exchanging information via network 130 with computing system 160 about performance of the locally executing applications.
- Examples of computing devices 116 , 118 include mobile phones, tablet computers, laptop computers, desktop computers, servers, mainframes, blades, wearable devices (e.g., computerized watches etc.), home automation devices, assistant devices, gaming consoles and systems, media players, e-book readers, television platforms, automobile navigation or infotainment systems, or any other type of mobile, non-mobile, wearable, and non-wearable computing device configured to execute applications on a computing platform being analyzed by developer service module 162 .
- Each of computing devices 116 , 118 executes applications on similar or corresponding computing platforms, such as a common operating system.
- Applications 122 A and 122 B represent two different machine-readable, executables configured to operate at an application layer of similar or identical computing platforms that operate on each of computing devices 116 and 118 .
- Computing devices 116 may execute instructions associated with a particular computing platform in which application 122 A executes and computing devices 118 may execute similar instructions associated with the same computing platform as computing devices 116 , however, for executing application 122 B instead of application 122 A.
- applications 122 A and 122 B are too numerous to list.
- applications 122 A and 122 B may include business applications, developer tools, educational applications, entertainment applications, financial applications, game applications, graphic or design applications, health and fitness applications, lifestyle or assistant applications, medical applications, music applications, news applications, photography, video, and other multimedia applications, productivity applications, reference applications, social networking applications, sports applications, travel applications, utility applications, weather applications, communication applications, calendar applications, or any other category or type of application.
- Applications 122 A and 122 B are completely different applications and may be associated with different application developers, producers, versions, and the like.
- applications 122 A and 122 B are “peers” being that they are from a same category or genre of application (e.g., travel) and perform similar functions or provide similar features (e.g., both may enable transportation booking).
- applications 122 A and 122 B are from a same category or genre of application (e.g., travel) but are not “peers” as the two applications may perform different functions or provide different features (e.g., one may enable transportation booking and another may aid in navigation).
- applications 122 A and 122 B are merely from similar categories or genres, or whether applications 122 A and 122 B are peers, applications 122 A and 122 B are completely different applications from different application developers, producers, or the like.
- a user of computing device 110 who in the example of FIG. 1 is a software developer or designer of application 122 A, may wish to understand how performance of application 122 A compares to performance of other applications, such as application 122 B, executing on the same or similar computing platforms.
- the user may provide input at a presence-sensitive screen of UIC 112 at or near a location of UIC at which user interface 114 is displayed.
- UIC 112 may provide information about the input to developer client module 120 and in response to the information about the input, developer client module 120 may access the service provided by developer service module 162 for obtaining an analysis of the performance of application 122 A relative to performance of other applications executing on similar or corresponding computing platforms.
- developer service module 162 of computing system 160 may obtain performance data collected by computing devices 116 , 118 while computing devices 116 , 118 were executing respective instances of applications 122 A, 122 B. For example, when application 122 A executes at computing device 116 A, application 122 A may output performance data indicative of how application 122 A performs in an execution environment of computing device 116 A. Computing device 116 A may send, via network 130 , the performance data to computing system 160 where developer service module 162 may store the performance data at application performance data store 164 . Each of computing devices 116 , 118 may perform similar operations as computing device 116 A to collect and send performance data to computing system 160 that is indicative of how applications 122 A, 122 B perform in respective execution environments of computing devices 116 , 118 .
- Developer service module 162 may analyze the performance data of each of applications 122 A and 122 B to determine one or more performance metrics for each of applications 122 A and 122 B.
- the basis behind the performance metrics is described in greater detail with respect to the additional FIGS.
- a performance metric may be any quantitative and measurable value that can be derived from application performance data to indicate a level of performance associated with some aspect of an application.
- performance metrics include: battery statistics (e.g., consumption rates, etc.), stability measurements (e.g., a crash rate, etc.), rendering metrics (e.g., latency between frame renderings, etc.), timing metrics (e.g., start-up time, transition delay from one mode to another, delay in implementing an intent, delays for retrieving or uploading information, etc.), permission metrics (e.g., frequency of unsuccessful or user denied requests to make use of device component or user information, etc.).
- battery statistics e.g., consumption rates, etc.
- stability measurements e.g., a crash rate, etc.
- rendering metrics e.g., latency between frame renderings, etc.
- timing metrics e.g., start-up time, transition delay from one mode to another, delay in implementing an intent, delays for retrieving or uploading information, etc.
- permission metrics e.g., frequency of unsuccessful or user denied requests to make use of device component or user information, etc.
- developer service module 162 may establish one or more benchmarks to use in evaluating the performance metrics of individual applications; specifically, to determine an application's performance on a computing platform, relative to performance of other applications that run on the computing platform. Said differently, developer service module 162 may determine, based on a performance metric of one or more applications, a performance goal that other applications should operate towards so that a user interacting with applications on the computing platform has an enjoyable, consistent, and frustration free user experience no matter which application he or she is interacting with.
- developer service module 162 may rank the performance metrics of multiple applications to determine a highest performing application, for each particular metric.
- developer service module 162 may determine a composite metric (e.g., an average value, median value, etc.) based on two or more applications' metrics to use as a benchmark in evaluating performance of other applications.
- developer service module 162 may determine a composite metric based two or more highest ranking, two or more lowest ranking, or some other combination of applications' metrics to use as a benchmark in evaluating performance of other applications.
- Developer service module may evaluate an application by comparing a performance metric of the application to a corresponding benchmark. For instance, if performance data stored at data store 164 indicates that application 122 A has a performance metric that is within a threshold of a benchmark value, developer service module 162 may determine that application 122 A does not have a performance issue, as the performance of the application relates to that particular metric. Otherwise, developer service module 162 may determine that if application 122 A has a performance metric that is outside the threshold of the benchmark value, that application 122 A does have a performance issue, as the performance of application 122 A relates to that particular metric.
- developer service module 162 establishes a benchmark for a particular metric (e.g., start-up time) to be based on a corresponding metric associated with application 122 B. Developer service module 162 may subsequently evaluate start-up time of application 122 A by comparing the start-up time of application 122 A to the benchmark.
- a particular metric e.g., start-up time
- Developer service module 162 may determine whether the start-up time metric is within a threshold amount of the benchmark.
- the threshold amount may be a percentage (e.g., ten percent) or a value (e.g., two milliseconds).
- the threshold amount may be zero and satisfying the benchmark may require meeting or exceeding the benchmark performance.
- the threshold amount may be a percentile ranking of an application relative to other applications executing on the computing platform. That is, to satisfy a benchmark, developer service module may require an application's performance to be better than the bottom ‘x’ percentile (e.g., where ‘x’ is any percentile value) of other applications that have been rated against the benchmark.
- developer service module 162 may determine that the start-up time of application 122 A does not satisfy the start-up time benchmark as the start-up time of application 122 A (e.g., on average) exceeds the benchmark by more than a threshold amount of time (e.g., five milliseconds). Responsive to determining that a performance metric is not within a threshold amount of a corresponding benchmark, developer service module may determine a fix to the application to improve its performance.
- a threshold amount of time e.g., five milliseconds
- the fix to the application may include one or more modifications to source code or a configuration file associated with the application.
- the fix to the application may include disabling a library, function call, software development kit, application programming interface, or service utilized by the application.
- the fix to the application may include replacing a first library, function call, or service utilized by the application with an alternative library, function call, or service.
- developer service module 162 may maintain information about ways to improve application performance, relative to each specific metric.
- the information may be based on feedback obtained from other developers of applications that have metrics which form the basis for benchmarks.
- the information may be based on feedback obtained from other developers of applications that have found ways to improve their metrics relative to the benchmarks.
- the information may be based on work-arounds obtained from a developer of the computing platform who may have uncovered the fix after identifying ways that other similarly situated applications executing on the computing platform have improved their metrics relative to the benchmarks.
- the information about ways to improve application performance may be derived automatically by developer service module 162 (e.g., using a machine learning model, trial and error experimentation, or other ways which are described in more detail with respect to the additional FIGS.). For example, developer service module 162 may determine which application programming interfaces (APIs) or libraries an underperforming application uses, determine that other better performing applications on the computing platform use different APIs or libraries for the same functionality (e.g., a different third-party library for user authentication), and conclude that the underperforming application may improve its performance by replacing existing APIs or library calls with those used by the better performing applications.
- APIs application programming interfaces
- libraries an underperforming application uses
- developer service module 162 may determine that the start-up time issue identified with application 122 A could be attributed to a slow-performing application programming interface (API) relied on by application 122 A during start-up which has been observed by developer service module 162 to cause slow start-up times in other applications.
- developer service module 162 may determine a fix to the start-up time issue with application 122 A is to: use a more responsive API, relocate the API call outside the start-up thread, or adjust a start-up sequence so that the slow API does not get used until other resources of application 122 A are otherwise up and running.
- API application programming interface
- developer service module 162 may share information about the fix with developer client module 120 , for subsequent presentation to the user developer.
- Developer service module 162 may output, for presentation at computing device 110 , an indication of the fix to application 122 A.
- developer service module 162 may send data via network 130 that is interpreted by developer client module 120 as an instruction to update user interface 114 with information about the fix.
- Developer client module 120 in response to receiving instruction to update user interface 114 with information about the fix, may cause UIC 112 to output user interface 114 . That is, developer client module 120 may cause UIC 112 to output, for display a graphical indication of any determined performance anomalies or issues and/or any fix identified to resolve the determined performance anomalies or issues.
- a computing system that operates in accordance to the described techniques may output an indication of a performance issue and a potential fix which a software developer, designer, or other user of the computing system, can elect to implement or not implement, so as to improve performance of an application, relative to performance of other applications executing in a similar or corresponding computing platform.
- an example computing system can identify performance issues with an application before a developer even thinks to look for performance improvements.
- the example computing system may output information about a gap in performance to educate a developer to understand how the performance of his or her application compares to other applications that are available on the platform.
- a developer need not know what areas need improving, what areas can be improved, or even how to improve these areas.
- the example computing system may automatically identify areas for improvement and suggest or automatically implement fixes to improve performance of an application to be more in-line with performance of other applications executing on the computing platform. Accordingly, the example computing system may cause applications executing on a particular computing platform to operate more efficiently and perhaps, provide a more consistent user experience as a user interacts with different applications on the computing platform.
- FIG. 2 is a block diagram illustrating an example computing system configured to identify performance issues with an application and determine ways to improve performance of the application, relative to performance of other applications that execute on similar or corresponding computing platforms, in accordance with one or more aspects of the present disclosure.
- Computing system 260 of FIG. 2 is described below as an example of computing system 160 of FIG. 1 .
- FIG. 2 illustrates only one particular example of computing system 260 , and many other examples of computing system 260 may be used in other instances and may include a subset of the components included in computing system 260 or may include additional components not shown in FIG. 2 .
- computing system 260 includes one or more processors 270 , one or more communication units 272 , and one or more storage components 276 communicatively coupled via communication channel 274 .
- Storage components 276 includes developer service module 262 and application performance data store 264 .
- Developer service module 262 includes UI module 266 , analysis module 267 , and correction module 268 .
- Communication channels 274 may interconnect each of the components 266 , 270 , 272 , and 276 for inter-component communications (physically, communicatively, and/or operatively).
- communication channels 274 may include a system bus, a network connection, an inter-process communication data structure, or any other method for communicating data.
- One or more communication units 272 of computing system 260 may communicate with external devices (e.g., computing devices 110 , 116 , 118 of FIG. 1 ) via one or more wired and/or wireless networks by transmitting and/or receiving network signals on the one or more networks.
- Examples of communication units 272 include a network interface card (e.g. such as an Ethernet card), an optical transceiver, a radio frequency transceiver, a GPS receiver, or any other type of device that can send and/or receive information.
- Other examples of communication units 272 may include short wave radios, cellular data radios, wireless network radios, as well as universal serial bus (USB) controllers.
- USB universal serial bus
- One or more storage components 276 within computing system 260 may store information for processing during operation of computing system 260 (e.g., computing system 260 may store application performance data collected and accessed by modules 262 , 266 , 267 , and 268 , and data store 264 during execution at computing system 260 ).
- storage component 276 is a temporary memory, meaning that a primary purpose of storage component 276 is not long-term storage.
- Storage components 276 on computing system 260 may be configured for short-term storage of information as volatile memory and therefore not retain stored contents if powered off. Examples of volatile memories include random access memories (RAM), dynamic random-access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art.
- Storage components 276 also include one or more computer-readable storage media.
- Storage components 276 in some examples include one or more non-transitory computer-readable storage mediums.
- Storage components 276 may be configured to store larger amounts of information than typically stored by volatile memory.
- Storage components 276 may further be configured for long-term storage of information as non-volatile memory space and retain information after power on/off cycles. Examples of non-volatile memories include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories.
- EPROM electrically programmable memories
- EEPROM electrically erasable and programmable
- Storage components 276 may store program instructions and/or information (e.g., data) associated with modules 262 , 266 , 267 , and 268 , and data store 264 .
- Storage components 276 may include a memory configured to store data or other information associated with modules 262 , 266 , 267 , and 268 , and data store 264 .
- processors 270 may implement functionality and/or execute instructions associated with computing system 260 .
- Examples of processors 270 include application processors, display controllers, graphics processors, auxiliary processors, one or more sensor hubs, and any other hardware configure to function as a processor, a processing unit, or a processing device.
- Modules 262 , 266 , 267 , and 268 may be operable by processors 270 to perform various actions, operations, or functions of computing system 260 .
- processors 270 of computing system 260 may retrieve and execute instructions stored by storage components 276 that cause processors 270 to perform operations attributed to modules 262 , 266 , 267 , and 268 .
- the instructions when executed by processors 270 , may cause computing system 260 to store information within storage components 276 .
- Developer service module 262 may include some or all of the functionality of developer service module 162 of computing system 160 of FIG. 1 . Developer service module 262 may additionally include some or all of the functionality of client service module 120 of computing device 110 of FIG. 1 . Developer service module 262 may perform similar operations as modules 162 and 120 for providing an application performance evaluation service that identifies performance anomalies in applications, relative to other application executing on a computing platform, and recommends or implements fixes to improve the applications' performance on the computing platform.
- UI module 280 may provide a user interface associated with the service provided by developer service module 262 .
- UI module 280 may host a web interface from which a client, such as computing device 110 , can access the service provided by developer service module 262 .
- a user of computing device 110 may interact with a web browser or other application (e.g., developer client module 120 ) executing at or accessible from computing device 110 .
- UI module 280 may send information to the web browser or other application that causes the client to display a user interface, such as user interface 114 of FIG. 1 .
- Analysis module 267 may analyze application performance data stored at data store 264 . Analysis module 267 may determine performance metrics associated with individual applications executing at a computing platform, determine benchmarks for evaluating the performance metrics, and compare the performance metrics to the benchmarks to determine whether an individual application has a potential performance anomaly or issue, relative to other applications executing on the computing platform.
- analysis module 267 may determine a permissions denial rate associated with an application. For example, a permission may grant an application access to a user's calendar, a camera, a user's contact list, a user's or devices location, a microphone, a telephone, other sensors, messaging services, and storage. Applications often depend specific permissions to function properly, yet a developer may be unaware of what percentage of users have actually agreed to grant the application each permission and how the user behavior towards granting or not granting the application each permission compares with other applications executing on a computing platform. A computing platform may want to minimize a frequency of permission denials to improve user satisfaction with the computing platform. Analysis module 267 may define a permissions denial rate as an amount (e.g., a percentage) of daily permission sessions during which users did not grant or otherwise denied an application a permission.
- an amount e.g., a percentage
- analysis module 267 may determine a start-up time for an application.
- the start-up time may indicate an amount of time for an application to launch. If a user has a choice between two peer applications, he or she may more often choose to launch the peer with the shortest start-time.
- a computing platform may want all applications to launch as quickly as possible so as to provide the best user experience with the computing platform.
- Analysis module 267 may quantify a start-time based on a measurement of time (e.g., seconds, milliseconds, etc.). Analysis module 267 may quantify a start-time as being one of multiple different levels, e.g., slow, moderate, fast.
- Analysis module 267 may further quantify a start-time metric in one or more other ways. For example, analysis module 267 may assign an application a “slow cold start” label when a percentage of daily sessions during which users of an application experienced at least one cold startup time of more than five seconds. A cold start of an application is when an application launches from scratch (e.g., often displaying a splash screen, etc.). Whereas, analysis module 267 may assign an application a “slow warm start” label when a percentage of daily sessions during which users of an application experienced at least one hot startup time of more than one and a half seconds.
- a warm start of an application occurs when an application is brought into the foreground of a user interface, after previously executing in the background (e.g., often displaying a previously viewed screen that was last in view when the application was last viewed, etc.).
- Analysis module 267 may assign other types of labels to a start-up time metric that are specific to a particular computing platform.
- analysis module 267 may determine a failed wireless signal scan metric.
- some applications may cause a computing device to scan for available Bluetooth®, Wi-Fi®, near-field-communication (NFC), or other wireless signals. Such scanning is power intensive and often results in increased battery drain. Therefore, a computing platform may wish to encourage the prevention of failed wireless signal scans that result in wasted battery consumption.
- a failed wireless signal scan may indicate a percentage of battery sessions (i.e., periods between two full charges of a device) during which users of an application experienced at least one failed wireless signal scan that lasted more than thirty minutes or some other time duration.
- performance metrics generated by analysis module 267 may include an application non-responsive (ANR) rate that is defined as a percentage of daily sessions during which application users experienced at least one occurrence where an application was non-responsive and required restart or a crash rate that indicates a percentage of daily sessions during which users experienced at least one crash of an application.
- ANR application non-responsive
- Still other performance metric examples include a stuck wake lock rate that indicates a percentage of battery sessions during which users experienced at least one partial wake lock of more than one hour while the app was in the background.
- Correction module 268 may obtain information from analysis module 267 about a potential issue with an application executing on a computing platform and determine a fix or other action for computing system 260 to take for addressing the issue and improve the application's performance, relative to other applications executing on the same or similar computing platform. For example, correction module 268 may cause UI module 266 to output a recommendation for fixing an application so that it provides a detailed explanation for a permission request when the permission denial rate outside a suitable range for the computing platform.
- correction module 268 may alert computing system 160 to cause an underperforming application to be demoted in ratings in an application store. If an application has performance metrics that are far outside a threshold amount of a benchmark, correction module 268 may alert computing system 160 to cause the underperforming application to be undiscoverable or unavailable from the application store.
- Correction module 268 may maintain information about ways to improve application performance, relative to each specific issue identified by analysis module 267 , for each performance metric. For example, correction module 268 may obtain feedback from application developers that have metrics which form the basis for benchmarks or have otherwise found ways to improve their application's performance and use the feedback to provide a solution or a fix for a specific issue. Correction module 268 may obtain information from a computing platform developer of work-arounds that he or she may have uncovered after identifying ways that other similarly situated applications executing on the computing platform have improved their metrics relative to the benchmarks.
- correction module 268 may determine a fix for an issue automatically and without any assistance from a user, by performing automatic bug fixing techniques. For example, correction module 268 may generate one or more potential software patches that might improve performance of an application and through trial-and-error, try the different patches to see which if any result in a fix. Correction module 268 may perform search-based program mutation, machine learning, and/or genetic programming techniques to identify one or more suitable fixes. After determining a potential fix, correction module 268 may validate the fix by testing the fix or delivering the fix to an application developer so that he or she may validate the fix.
- FIG. 3 is an example screen shot of a display screen of a computing device accessing an application performance evaluation service provided by an example computing system, in accordance with one or more aspects of the present disclosure.
- FIG. 3 shows user interface 314 .
- FIG. 3 is described in the context of system 100 of FIG. 1 .
- Computing system 160 may cause computing device 110 to present user interface 314 at UIC 112 when developer client module 120 requests access to the application performance evaluation service provided by developer service module 162 .
- User interface 314 is just one example of such a user interface, many other examples may exist.
- the purpose of user interface 314 is to provide an application developer with insights into specific performance metrics obtained about a target application in order to understand how the target application is performing relative to the top performing applications executing on a computing platform.
- the relative performance of an application may be based on top performing applications in a similar category or genre as the target application.
- the relative performance of an application may be based on top performing applications that are considered peers.
- a benefits of user interface 314 includes causing observable or measurable improvements to an overall computing platform or application ecosystem, not just improvements to an individual application executing in that ecosystem. For example, developers can view user interface 314 to understand where an application's performance may be lagging relative to peers or other applications and therefore be motivated to improve the application's performance, which results in an overall performance improvement of the computing platform.
- User interface 314 is divided into several tabs or sections, with each tab being associated with a different performance metric or group of performance metrics. For example, as shown in FIG. 3 , the start-up metric tab is in the foreground of user interface 314 with metrics B-E and a group of vital metrics appearing on tabs that re hidden from view in the background of user interface 314 .
- Each tab may include a results section (e.g., shown as a graph, table, chart, or other data format) for providing information about an application's relative performance as compared to the benchmark and an acceptable threshold level within the benchmark.
- the start-up performance metric of the application in the example of FIG. 3 is outside an acceptable threshold amount of the benchmark.
- the results section includes information about where an application's performance is compared to other applications executing on the computing platform. For example, in the example of FIG. 3 , the application's start-up metric is in line with the bottom twenty-five percent of applications executing on the computing platform.
- User interface 314 may designate an application's performance using color or other formatting. For example, in cases where a performance metric is outside an acceptable threshold level of a benchmark, user interface 314 may use a red color font or line color to emphasize that the application may have a potential performance issue. In cases where the performance metric is inside the acceptable threshold level of the benchmark, user interface 314 may use a green color font or line color to emphasize that the application is performing similarly to other top performing applications on the computing platform.
- Each tab of user interface 314 may include a summary and resolution section where potential issues are clearly identified and recommended fixes for the potential issues are displayed.
- user interface 314 includes a detailed summary of the start-time issue along with potential fixes that a developer may implement to improve the start-time metric of the application.
- Also included in user interface 314 may be selectable elements that cause computing system 160 to perform various actions to further aid a developer in improving the performance of an application, relative to performance of other applications executing on the computing platform.
- a user may select a graphical element that causes computing system 160 to link to a source file editor for viewing highlighted sections of source code that computing system 160 suspects may be a cause of the slow start time.
- a user may select a graphical element that causes computing system 160 to automatically modify the suspect sections of source code automatically, without further user input.
- FIG. 4 is a flowchart illustrating example operations performed by an example computing system configured to identify performance issues with an application and determine ways to improve performance of the application, relative to performance of other applications that execute on similar or corresponding computing platforms, in accordance with one or more aspects of the present disclosure.
- Operations 400 - 490 may be performed by a computing system, such as computing systems 160 , 260 .
- a computing system may perform operations 400 - 490 in a different order than that shown in FIG. 4 .
- a computing system may perform additional or fewer operations than operations 400 - 490 .
- FIG. 4 is described in the context of computing system 260 of FIG. 2 .
- computing system 260 may obtain consent from user to make use of application performance data collected during execution of applications on their respective devices ( 400 ). For example, before computing system 260 or any of computing devices 116 , 118 stores or transmits application performance data, a user of computing devices 116 , 118 will be offered an opportunity to give or not give computing system 260 permission to collect and make use of such data. Only if a user clearly and unambiguously consents to such data collection will computing system 260 make use of application performance data from the user's device.
- Computing system 260 may obtain first performance data collected during execution of a first application at a first group of computing devices ( 410 ) and computing system 260 may obtain second performance data collected during execution of one or more second applications at a second group of computing devices ( 420 ).
- analysis module 267 may receive performance data being transmitted via network 130 from computing devices 116 , 118 .
- Analysis module 267 may store the performance data at data store 264 .
- each computing device from the first group of computing devices and the second group of computing devices executes a respective instance of a common computing platform.
- each of the first and second groups of computing devices may operate a common operating system or computing platform so that analysis module, when determining performance of a particular application, determines the application performance relative to other applications executing on the same or similar computing platform as the particular application.
- the first application and each of the one or more second applications may in some examples be associated with a common category or genre of application.
- the first and second applications may be travel category applications or navigation genre applications within the same travel category.
- the first application and each of the one or more second applications are peer applications, and the first application differs from each of the one or more second applications by at least one of: functionality, title, or developer.
- the first and second applications may be a specific type of game (e.g., first-person shooter, crossword puzzle, etc.) and while the applications may share some overlap in functionality, the applications are not the same and differ in functionality, title, game play, developer, appearance, or other feature.
- Computing system 260 may determine, based on the first performance data, at least one metric quantifying performance of the first application ( 430 ). For example, analysis module 267 may compute a permissions denial rate metric based on the performance data for application 122 A that indicates how often users prevent application 122 A from accessing a particular permission of computing devices 116 (e.g., camera, microphone, etc.).
- a permissions denial rate metric based on the performance data for application 122 A that indicates how often users prevent application 122 A from accessing a particular permission of computing devices 116 (e.g., camera, microphone, etc.).
- Computing system 260 may determine one or more benchmarks based on the second performance data ( 440 ). For example, analysis module 267 may determine an average permissions denial rate for one or more other applications executing on computing devices 116 , 118 (e.g., application 122 B). Analysis module 267 may use the average permissions denial rate as a benchmark for determining whether an application has too high of a permissions denial rate relative to other applications executing on the computing platform.
- Computing system 260 may compare the at least one metric to a corresponding benchmark derived from the second performance data ( 450 ). For example, analysis module 267 may compare the permissions denial rate for application 122 A to the benchmark established during operation 440 .
- computing system 260 may determine that the first application does not have a performance issue with that particular metric ( 460 , YES branch). Conversely, if the metric is not within the threshold amount of the benchmark, computing system 260 may determine that the first application has a performance issue with that particular metric ( 460 , NO branch). Analysis module 267 may determine that the permissions denial rate for application 122 A exceeds the benchmark established during operation 440 by ten percent or some other amount that exceeds a tolerable threshold.
- Computing system 260 may determine a fix to the first application ( 470 ). For example, analysis module 267 may trigger correction module 268 to determine a way for application 122 A to improve the permission denial rate metric such that overall performance of application 122 A on the computing platform is closer to the performance of other top performing applications on the computing platform.
- Correction module 268 may inspect source files or other attributes of application 122 A and determine that when application 122 A requests permission to use a device location or camera, no explanation or reason is provided to the user for the requests. Correction module 268 may determine that the permission denial rate may be improved if application 122 A included such information when making future requests.
- Computing system 260 may output an indication of the fix ( 480 ).
- correction module 268 may send a command to UI module 266 that causes a user interface, such as user interface 114 or 314 , to provide information to a developer user of computing system 260 that he or she may wish to modify application 122 A to provide more information when making a permission request.
- Computing system 260 may implement the fix ( 490 ).
- UI module 266 may receive, from computing device 110 or other developer device, an indication of user input that authorizes the fix to be automatically implemented by computing system 260 .
- correction module 268 may output to computing devices 116 instructions for automatically implementing the fix to application 122 A.
- the instructions may include updated source files, an updated executable, or an updated configuration file generated by correction module 268 and that are used during execution of application 122 A to cause the fix.
- computing system 260 may take other actions in response to determining a performance deficiency in an application relative to performance of other application on a computing platform. For example, a cause of a performance issue may not be readily apparent from existing application performance data.
- Computing system 260 may send instructions via network 130 that cause computing devices 116 to start collecting more detailed performance data when applications 122 A executes at computing devices 116 than what is normally collected. For instance, computing system 260 may cause computing devices' 116 performance data collection rate to increase or may cause computing devices 116 to collect additional data beyond the normal data collected.
- a method comprising: obtaining, by a computing system, first performance data collected during execution of a first application at a first group of computing devices; determining, based on the first performance data, at least one metric for quantifying performance of the first application; comparing, by the computing system, the at least one metric to a corresponding benchmark derived from second performance data collected during execution of one or more second applications at a second group of computing devices, each of the one or more second applications being different than the first application; determining whether the at least one metric is within a threshold amount of the corresponding benchmark; responsive to determining that the at least one metric is not within the threshold amount of the corresponding benchmark, determining, a fix to the first application; and outputting, by the computing system, for presentation at a developer device, an indication of the fix to the first application.
- Clause 2 The method of clause 1, wherein each computing device from the first group of computing devices and the second group of computing devices executes a respective instance of a common computing platform.
- Clause 3 The method of any of clauses 1 or 2, wherein the first application and each of the one or more second applications are associated with a common category or genre of application.
- Clause 4 The method of clause 3, wherein the first application and each of the one or more second applications are peer applications, and the first application differs from each of the one or more second applications by at least one of: functionality, title, or developer.
- Clause 5 The method of any of clauses 1-4, further comprising: receiving, from the developer device, an indication of user input that authorizes the fix to be automatically implemented by the computing system; and in response to receiving the indication of user input, outputting, by the computing system, to one or more computing devices from the first group of computing device, instructions for automatically implementing the fix to the first application.
- Clause 9 The method of any of clauses 1-8, wherein the first group of computing devices and the second group of computing devices comprise a single computing device.
- Clause 10 A computing system comprising at least one processor configured to perform any one of the methods of clauses 1-9.
- Clause 11 A computing system comprising means for performing any one of the methods of clauses 1-9.
- Clause 12 A computer-readable storage medium comprising instructions that, when executed, cause at least one processor to perform any one of the methods of clauses 1-9.
- a method comprising: outputting, by a computing device, to a computing system, first performance data collected during execution of a first application; receiving, by the computing device, from the computing system, instructions to execute a fix of the first application that improves performance of the first application relative to performance of one or more second applications executing at a group of computing devices, each of the one or more second applications being different than the first application; and executing, by the computing device, the instruction to execute the fix of the first application.
- a method comprising: receiving, by a computing device, from a computing system, a recommendation for a fix of a first application executing at a first group of computing devices that improves performance of the first application relative to performance of one or more second applications executing at a second group of computing devices, each of the one or more second applications being different than the first application; receiving, by the computing device, user input authorizing the fix of the first application; responsive to sending, to the computing system, an indication of the user input authorizing the fix of the first application, receiving, by the computing device, from the computing system, instructions to execute the fix of the first application; and executing, by the computing device, based on the instructions, the fix of the first application.
- Clause 15 A computing device comprising at least one processor configured to perform any of the methods of clauses 13 or 14.
- Clause 16 A computing system comprising means for performing any of the methods of clauses 13 or 14.
- Clause 17 A computer-readable storage medium comprising instructions that, when executed, cause at least one processor to perform any of the methods of clauses 13 or 14.
- such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other storage medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer.
- any connection is properly termed a computer-readable medium.
- a computer-readable medium For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium.
- DSL digital subscriber line
- Disk and disc includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and Blu-ray disc, where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable medium.
- processors such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry.
- DSPs digital signal processors
- ASICs application specific integrated circuits
- FPGAs field programmable logic arrays
- processors may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein.
- the functionality described herein may be provided within dedicated hardware and/or software modules. Also, the techniques could be fully implemented in one or more circuits or logic elements.
- the techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set).
- IC integrated circuit
- a set of ICs e.g., a chip set.
- Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined in a hardware unit or provided by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.
Abstract
Description
Claims (17)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/978,574 US11860758B2 (en) | 2018-05-07 | 2018-06-20 | System for adjusting application performance based on platform level benchmarking |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862667974P | 2018-05-07 | 2018-05-07 | |
US16/978,574 US11860758B2 (en) | 2018-05-07 | 2018-06-20 | System for adjusting application performance based on platform level benchmarking |
PCT/US2018/038435 WO2019216926A1 (en) | 2018-05-07 | 2018-06-20 | System for adjusting application performance based on platform level benchmarking |
Publications (2)
Publication Number | Publication Date |
---|---|
US20210019247A1 US20210019247A1 (en) | 2021-01-21 |
US11860758B2 true US11860758B2 (en) | 2024-01-02 |
Family
ID=62981319
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/978,574 Active 2039-12-18 US11860758B2 (en) | 2018-05-07 | 2018-06-20 | System for adjusting application performance based on platform level benchmarking |
Country Status (4)
Country | Link |
---|---|
US (1) | US11860758B2 (en) |
EP (1) | EP3762828A1 (en) |
CN (1) | CN112041818A (en) |
WO (1) | WO2019216926A1 (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11770311B2 (en) * | 2019-04-05 | 2023-09-26 | Palo Alto Networks, Inc. | Automatic and dynamic performance benchmarking and scoring of applications based on crowdsourced traffic data |
JP6859495B1 (en) * | 2020-01-06 | 2021-04-14 | ＩｎｓｕＲＴＡＰ株式会社 | Processing equipment, processing methods and programs |
US11836040B2 (en) * | 2021-10-29 | 2023-12-05 | Fidelity Information Services, Llc | Software application development tool for automation of maturity advancement |
Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040003266A1 (en) | 2000-09-22 | 2004-01-01 | Patchlink Corporation | Non-invasive automatic offsite patch fingerprinting and updating system and method |
US20050278703A1 (en) * | 2004-06-15 | 2005-12-15 | K5 Systems Inc. | Method for using statistical analysis to monitor and analyze performance of new network infrastructure or software applications for deployment thereof |
US7424706B2 (en) | 2003-07-16 | 2008-09-09 | Microsoft Corporation | Automatic detection and patching of vulnerable files |
US20090144584A1 (en) * | 2007-11-30 | 2009-06-04 | Iolo Technologies, Llc | System and method for performance monitoring and repair of computers |
US20120290870A1 (en) * | 2010-11-05 | 2012-11-15 | Interdigital Patent Holdings, Inc. | Device validation, distress indication, and remediation |
US20140282422A1 (en) | 2013-03-12 | 2014-09-18 | Netflix, Inc. | Using canary instances for software analysis |
US20150227412A1 (en) | 2014-02-07 | 2015-08-13 | AppDynamics, Inc. | Server performance correction using remote server actions |
US20160254943A1 (en) * | 2013-10-30 | 2016-09-01 | Hewlett-Packard Development Company, L.P. | Monitoring a cloud service modeled as a topology |
US20170004155A1 (en) | 2015-06-30 | 2017-01-05 | International Business Machines Corporation | Extensible indexing system evaluation and recommendation |
US20170315897A1 (en) | 2016-04-29 | 2017-11-02 | International Business Machines Corporation | Server health checking |
US20180314576A1 (en) * | 2017-04-29 | 2018-11-01 | Appdynamics Llc | Automatic application repair by network device agent |
-
2018
- 2018-06-20 US US16/978,574 patent/US11860758B2/en active Active
- 2018-06-20 WO PCT/US2018/038435 patent/WO2019216926A1/en unknown
- 2018-06-20 CN CN201880092343.1A patent/CN112041818A/en active Pending
- 2018-06-20 EP EP18743622.5A patent/EP3762828A1/en active Pending
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040003266A1 (en) | 2000-09-22 | 2004-01-01 | Patchlink Corporation | Non-invasive automatic offsite patch fingerprinting and updating system and method |
US7424706B2 (en) | 2003-07-16 | 2008-09-09 | Microsoft Corporation | Automatic detection and patching of vulnerable files |
US20050278703A1 (en) * | 2004-06-15 | 2005-12-15 | K5 Systems Inc. | Method for using statistical analysis to monitor and analyze performance of new network infrastructure or software applications for deployment thereof |
US20090144584A1 (en) * | 2007-11-30 | 2009-06-04 | Iolo Technologies, Llc | System and method for performance monitoring and repair of computers |
US20120290870A1 (en) * | 2010-11-05 | 2012-11-15 | Interdigital Patent Holdings, Inc. | Device validation, distress indication, and remediation |
US20140282422A1 (en) | 2013-03-12 | 2014-09-18 | Netflix, Inc. | Using canary instances for software analysis |
US20160254943A1 (en) * | 2013-10-30 | 2016-09-01 | Hewlett-Packard Development Company, L.P. | Monitoring a cloud service modeled as a topology |
US20150227412A1 (en) | 2014-02-07 | 2015-08-13 | AppDynamics, Inc. | Server performance correction using remote server actions |
US20170004155A1 (en) | 2015-06-30 | 2017-01-05 | International Business Machines Corporation | Extensible indexing system evaluation and recommendation |
US20170315897A1 (en) | 2016-04-29 | 2017-11-02 | International Business Machines Corporation | Server health checking |
US20180314576A1 (en) * | 2017-04-29 | 2018-11-01 | Appdynamics Llc | Automatic application repair by network device agent |
Non-Patent Citations (18)
Title |
---|
A. Pandey, L. Vu, V. Puthiyaveettil, H. Sivaraman, U. Kurkure and A. Bappanadu, "An Automation Framework for Benchmarking and Optimizing Performance of Remote Desktops in the Cloud," 2017 International Conference on High Performance Computing & Simulation (HPCS), 2017, pp. 745-752 (Year: 2017). * |
A. Papaioannou and K. Magoutis, "An Architecture for Evaluating Distributed Application Deployments in Multi-clouds," 2013 IEEE 5th International Conference on Cloud Computing Technology and Science, 2013, pp. 547-554, doi: 10.1109/CloudCom.2013.79. (Year: 2013). * |
A. Yamashita, "Experiences from performing software quality evaluations via combining benchmark-based metrics analysis, software visualization, and expert assessment," 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME), 2015, pp. 421-428, doi: 10.1109/ICSM.2015.7332493. (Year: 2015). * |
Fuad, Mohammad Muztaba, Debzani Deb, and Jinsuk Baek. "Self-healing by means of runtime execution profiling." In 14th International Conference on Computer and Information Technology (ICCIT 2011), pp. 202-207. IEEE, 2011. (Year: 2011). * |
G. A. D. Vale and E. M. L. Figueiredo, "A Method to Derive Metric Thresholds for Software Product Lines," 2015 29th Brazilian Symposium on Software Engineering, 2015, pp. 110-119, doi: 10.1109/SBES.2015.9. (Year: 2015). * |
G. Cong et al., "A Systematic Approach toward Automated Performance Analysis and Tuning," in IEEE Transactions on Parallel and Distributed Systems, vol. 23, No. 3, pp. 426-435, Mar. 2012, doi: 10.1109/TPDS.2011.189. (Year: 2012). * |
H. Zhu, Y. Zou and L. Zha, "VegaBench: A Benchmark Tool for Grid System Software," 2006 Fifth International Conference on Grid and Cooperative Computing Workshops, 2006, pp. 543-548, doi: 10.1109/GCCW.2006.99. (Year: 2006). * |
International Preliminary Report on Patentability from International Application No. PCT/US2018/038435, dated Nov. 19, 2020, 8 pp. |
International Search Report and Written Opinion of International Application No. PCT/US2018/038435, dated Mar. 1, 2019, 14 pp. |
Le Goues et al., "GenProg: Evolutionary Program Repair", 2018, accessed on GitHub on May 1, 2018, 3pp. |
M. Fagan, M. M. H. Khan and B. Wang, "Leveraging Cloud Infrastructure for Troubleshooting Edge Computing Systems," 2012 IEEE 18th International Conference on Parallel and Distributed Systems, Singapore, 2012, pp. 440-447, doi: 10.1109/ICPADS.2012.67. (Year: 2012). * |
M. Sahasrabudhe, M. Panwar and S. Chaudhari, "Application performance monitoring and prediction," 2013 IEEE International Conference on Signal Processing, Computing and Control (ISPCC), 2013, pp. 1-6, doi: 10.1109/ISPCC.2013.6663466. (Year: 2013). * |
P. Oliveira, M. T. Valente, A. Bergel and A. Serebrenik, "Validating metric thresholds with developers: An early result," 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME), 2015, pp. 546-550, doi: 10.1109/ICSM.2015.7332511. (Year: 2015). * |
R. Gioiosa, G. Kestor and D. J. Kerbyson, "Online Monitoring System for Performance Fault Detection," 2014 IEEE International Parallel & Distributed Processing Symposium Workshops, 2014, pp. 1475-1484, doi: 10.1109/IPDPSW.2014.165. (Year: 2014). * |
Response to Communication pursuant to Rule 137(4) EPC dated Aug. 12, 2022, from counterpart European Application No. 18743622.5 filed Sep. 8, 2022, 2 pp. |
Response to Communication Pursuant to Rules 161(1) and 162 EPC from European Patent Application No. 18743622.5, dated Oct. 20, 2020, filed Mar. 30, 2021, 14 pp. |
V. Soundararajan, B. Agrawal, B. Herndon, P. Sethuraman and R. Taheri, "Benchmarking a virtualization platform," 2014 IEEE International Symposium on Workload Characterization (IISWC), 2014, pp. 99-109, doi: 10.1109/IISWC.2014.6983049. (Year: 2014). * |
Wikipedia, "Automatic Bug Fixing", Last edited Feb. 6, 2018, accessed May 1, 2018, 7pp. |
Also Published As
Publication number | Publication date |
---|---|
EP3762828A1 (en) | 2021-01-13 |
US20210019247A1 (en) | 2021-01-21 |
CN112041818A (en) | 2020-12-04 |
WO2019216926A1 (en) | 2019-11-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9449042B1 (en) | Recommending improvements to and detecting defects within applications | |
US20110209128A1 (en) | Systems, methods and apparatuses for facilitating targeted compilation of source code | |
US9607332B1 (en) | Embedded web application gallery | |
US11860758B2 (en) | System for adjusting application performance based on platform level benchmarking | |
US10339034B2 (en) | Dynamically generated device test pool for staged rollouts of software applications | |
US9219719B1 (en) | Automatic dynamic vetting of browser extensions and web applications | |
US9563545B2 (en) | Autonomous propagation of system updates | |
US20160171589A1 (en) | Personalized application recommendations | |
US11205254B2 (en) | System and method for identifying and obscuring objectionable content | |
US20140244762A1 (en) | Application distribution platform for rating and recommending applications | |
US20220334951A1 (en) | Page simulation system | |
US20130081008A1 (en) | Detection and installation of software on a per-user basis | |
US20230030604A1 (en) | Feature Switching Kits | |
US11037190B2 (en) | Web page performance improvement system | |
US10990679B2 (en) | Methods, systems, articles of manufacture and apparatus to verify application permission safety | |
CN111330280A (en) | Data processing method and device in game, storage medium and electronic equipment | |
WO2016099447A1 (en) | Personalized application recommendations | |
CN111859077A (en) | Data processing method, device, system and computer readable storage medium | |
US20160352817A1 (en) | Predictive Peer Determination For Peer-to-Peer Digital Content Download | |
US20190007412A1 (en) | Customized device identification | |
US20170169024A1 (en) | Searching and Accessing Software Application Functionality Using Application Connections | |
US20170171100A1 (en) | Classified Network Bandwidth Management Based on Customer Performance in Social Communities | |
US10241882B2 (en) | System and method for dynamic adjustment of logging | |
US9965744B1 (en) | Automatic dynamic vetting of browser extensions and web applications | |
US11616882B2 (en) | Accelerating pre-production feature usage |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HURLEY, FERGUS GERARD;HUGHES, DINO DEREK;GAILLARD, OLIVIER BENOIT;AND OTHERS;SIGNING DATES FROM 20180608 TO 20180618;REEL/FRAME:053698/0992 |
|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: APPLICATION DISPATCHED FROM PREEXAM, NOT YET DOCKETED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |