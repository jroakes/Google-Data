CN112119388A - Training image embedding model and text embedding model - Google Patents
Training image embedding model and text embedding model Download PDFInfo
- Publication number
- CN112119388A CN112119388A CN201980028937.0A CN201980028937A CN112119388A CN 112119388 A CN112119388 A CN 112119388A CN 201980028937 A CN201980028937 A CN 201980028937A CN 112119388 A CN112119388 A CN 112119388A
- Authority
- CN
- China
- Prior art keywords
- image
- training
- embedding
- embedding model
- search
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/55—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/242—Query formulation
- G06F16/2425—Iterative querying; Query formulation based on the results of a preceding query
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/22—Matching criteria, e.g. proximity measures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for jointly training an image embedding model and a text embedding model. In one aspect, a method comprises: processing data from a historical query log of a search system to generate a candidate set of training examples, wherein each training example comprises: (i) a search query comprising a sequence of one or more words, (ii) an image, and (iii) selection data characterizing a frequency with which a user selects an image in response to the image being identified by search results of the search query; selecting a plurality of training examples from a candidate set of training examples; and jointly training the image embedding model and the text embedding model using the training data.
Description
Technical Field
This specification relates to processing data using machine learning models.
Background
The machine learning model receives inputs and generates outputs, such as predicted outputs, based on the received inputs. Some machine learning models are parametric models, and generate an output based on received inputs and parameter values of the model.
Some machine learning models are depth models that employ a multi-layer model to generate output for received input. For example, a deep neural network is a deep machine learning model that includes an output layer and one or more hidden layers, each applying a nonlinear transformation to a received input to produce an output.
Disclosure of Invention
This specification describes a training system implemented as a computer program on one or more computers in one or more locations that trains image embedding models and text embedding models using training data derived from historical query logs of a search system.
According to a first aspect, there is provided a method performed by one or more data processing apparatus, the method comprising: processing data from a historical query log of a search system to generate a candidate set of training examples, wherein each training example comprises: (i) a search query comprising a sequence of one or more words, (ii) an image, and (iii) selection data characterizing a frequency with which a user selects an image in response to the image being identified by search results of the search query; selecting a plurality of training examples from a candidate set of training examples for joint training based at least in part on selection data for the training examples: (i) an image embedding model having a plurality of image embedding model parameters, and (ii) a text embedding model having a plurality of text embedding model parameters; and jointly training the image embedding model and the text embedding model using the training data, wherein for each selected training example, the training comprises: processing the images of the training examples using an image embedding model to generate an embedding of the images; processing a representation of the search query of the training example using a text embedding model to generate an embedding of the search query; determining a similarity measure between the embedding of the image and the embedding of the search query; and adjusting the image embedding model parameters and the text embedding model parameters based at least in part on a similarity metric between the embedding of the image and the embedding of the search query.
In some embodiments, the training data is generated using historical query logs of the web search system.
In some implementations, the selection data for each training example indicates a score of a number of times the user selected an image of the training example in response to the image of the training example being identified by search results of a search query of the training example.
In some embodiments, selecting the plurality of training examples from the candidate set of training examples comprises: a plurality of training examples are selected for which images of the training examples are most frequently selected by a user in response to the images being identified by search results of a search query of the training examples.
In some embodiments, the image embedding model and the text embedding model include one or more neural networks.
In some embodiments, adjusting the image embedding model parameters and the text embedding model parameters comprises: determining a gradient of a loss function, the loss function dependent on a similarity measure between the embedding of the image and the embedding of the search query; and adjusting image embedding model parameters and text embedding model parameters using the gradient.
In some embodiments, the loss function depends on the selection data of the training examples.
In some embodiments, the loss function is a classification loss function or a triplet loss function.
In some implementations, the embedding of the image has the same dimensionality as the embedding of the search query.
In some implementations, determining a similarity metric between the embedding of the image and the embedding of the search query includes: a euclidian distance between the embedding of the image and the embedding of the search query is determined.
In some embodiments, the loss function includes one or more regularization terms, wherein each regularization term depends on: (i) a similarity metric between the embedding of the images of the training examples and the embedding of the respective additional images, and (ii) a co-click rate of the embedding of the images of the training examples and the respective additional images, a similar image click rate of the embedding of the images of the training examples and the respective additional images, or both.
According to another aspect (or in an embodiment of the preceding aspect) there is provided a method comprising receiving a search query comprising a sequence of one or more words, and processing the search query using a trained text embedding model obtained by a method according to any aspect or embodiment described herein to generate an embedding of the search query. The embedding of the search query may be used to determine a respective relevance score for each of a plurality of images in the search index. To determine a relevance score for an image in a search index, the image may be processed using a trained image embedding model obtained by a method according to any aspect or embodiment described herein to generate an embedding of the image. The relevance score for an image may be determined based on a similarity metric (e.g., a euclidean similarity metric) between the embedding of the image and the embedding of the sequence of words in the search query. A ranking of image search results for the search query may be determined based at least in part on relevance scores determined using the trained text embedding model and image embedding model.
According to another aspect (or in an embodiment of the aforementioned aspect) there is provided a method comprising receiving a search query comprising an image, and processing the search query using a trained image embedding model obtained by a method according to any aspect or embodiment described herein to generate an embedding of the search query. The embedding of the search query may be used to determine a respective relevance score for each of a plurality of images in the search index. To determine the relevance scores for the images in the search index, the images may be processed using a trained image embedding model to generate an embedding of the images. The relevance score for an image may be determined based on a similarity metric (e.g., a euclidean similarity metric) between the embedding of the image and the embedding of the search query. A ranking of image search results for a search query may be determined based at least in part on relevance scores determined using a trained image embedding model.
According to a second aspect, there is provided a system comprising: one or more computers; and one or more storage devices communicatively coupled to the one or more computers, wherein the one or more storage devices store instructions that, when executed by the one or more computers, cause the one or more computers to perform operations comprising the operations of the methods previously described.
According to a third aspect, one or more computer storage media (which may or may not be non-transitory storage media) storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising the operations of the methods previously described are provided.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
The training system described in this specification can generate large amounts of training data (e.g., tens or hundreds of millions of training examples) for training image-embedded models and text-embedded models by processing data from historical query logs of a search system. The large amount of training data that can be efficiently derived from historical query logs (e.g., of a web search system) enables a training system to train out highly efficient image-embedded models and text-embedded models. Such scalability (e.g., for training examples that may potentially include hundreds of millions of search queries) is a technological improvement in the field of model training. For example, this scalability enables training of image embedding models that generate image embedding that implicitly characterizes concepts (e.g., food, scenes, landmarks, artifacts, etc.) over a large range. In contrast, some conventional image embedding models generate image embeddings that only implicitly characterize concepts within a small range (e.g., only food or only landmarks).
The training system may process the historical query logs to generate a "query-image" training example that associates the text search query with relevant images (e.g., images that the user frequently selects when they are identified by search results of the text search query). In particular, the query-image training example may associate a highly specific textual search query (e.g., "red 2014 ford wildhorse") with a relevant image (e.g., depicting an object specified by the textual search query). By jointly training the image embedding model and the text embedding model using query-image training examples derived from historical query logs, the training system may cause the image embedding model to generate image embeddings that implicitly represent highly specific concepts. For example, the trained image embedding model may process the images to generate an image embedding that implicitly represents the color, make, and model of the car depicted in the images. This is a technical improvement in the field of model training, in contrast, for example, training an image-embedded model and a text-embedded model using training examples that associate images with generic labels (e.g., "cars") as in some conventional training data sets may result in the image-embedded model generating relatively uninformative embedding.
In some implementations, the training system can generate query-image training examples that include search queries expressed in a number of different natural languages (e.g., english, french, german, etc.). By jointly training the image embedding model and the text embedding model using the multilingual query-image training examples, the training system can train the text embedding model to generate information-containing text embedding independent of the text language. For example, the training system may train the text embedding model to generate similar embedding of the text based on the similarity of images associated with search queries that include the text "young Elizabeth Queen" (English) and "young Elizabeth Queen" (French). This is another technical improvement in the field of model training,
the training system may train the image embedding model and the text embedding model based on selection data that characterizes, for example, the frequency with which two images are "co-clicked" or the frequency with which a given image is selected when it is identified by the search results of a search query (i.e., by the "image-to-image" training example). The selection data may be determined by a user-derived signal (e.g., a click) that aggregates millions of users and enables the training system to more efficiently train the image-embedded model and the text-embedded model.
Generating training data using conventional methods lacks many of the advantages of generating training data by processing historical query logs of a search system. For example, manually generating training data (e.g., text labels for images manually specified by a person) is time consuming and difficult, and typically only a relatively small amount of training data may be generated in this manner. As another example, generating training data by correlating images drawn from a social network (or other source) with explanatory text may yield less and lower quality training data than generating training data from historical query logs, e.g., because the explanatory text may inaccurately characterize the content of the images. Producing more and/or higher quality training data and using the training data to train the machine learning model will improve the accuracy of the trainable machine learning model (thus providing a machine learning model that is less error-making than machine learning models trained on training data generated using conventional methods). Additionally or alternatively, by reducing the time required to obtain labeled training data, a larger set of training data may be established over a given period of time, and this will also improve the accuracy of the trainable machine learning model. Furthermore, improving the accuracy of the trainable machine learning model may enable the use of simpler machine learning models (e.g., with fewer hidden layers), and this may result in a reduction of the computational resources required to implement the machine learning model. In a complementary manner, data may need to be stored for a shorter period of time, and thus may require overall less memory storage.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example image embedding model.
FIG. 2 illustrates an example text embedding model.
FIG. 3 illustrates an example training system for training image embedding models and text embedding models using training data derived from historical query logs of a search system.
FIG. 4 illustrates an example search results page provided by a search system that includes image search results for a search query that includes a sequence of one or more terms.
FIG. 5 illustrates an example search results page provided by a search system that includes image search results for a search query that includes an image.
FIG. 6 illustrates an example process for jointly training an image embedding model and a text embedding model using a query-image training example.
FIG. 7A illustrates an example process for training an image embedding model using an image-to-image training example.
FIG. 7B illustrates an example process for jointly training an image embedding model and a text embedding model using a query-image training example and an image-image training example.
FIG. 8 is a flow diagram of an example process for jointly training an image embedding model and a text embedding model using query-image training examples derived from historical query logs of a search system.
FIG. 9 is a flow diagram of an example process for training an image embedding model using image-to-image training examples derived from historical query logs of a search system.
FIG. 10 illustrates an example of a portion of a graphical representation of a query-image training example and an image-image training example.
FIG. 11 illustrates an example search system.
FIG. 12 illustrates an example computer system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This specification describes a training system for training image embedding models and text embedding models using training data derived from historical query logs of a search system. The training data derived from the historical query logs may include query-image training examples, image-image training examples, or both.
The query-image training examples include: (i) a text search query, (ii) an image, and (iii) selection data characterizing a frequency with which the user selects the image in response to the image being identified by search results of the text search query. If the selection data indicates that the user frequently selects an image when the image is identified by search results of a search query, the training system may jointly train the image embedding model and the text embedding model to generate a similar embedding of the text search query and the image.
The image-image training example includes an image pair (including a first image and a second image) and selection data indicating: (i) a co-click rate of the image pair, (ii) a similar image click rate of the image pair, or (iii) both. The common click rate of the image pair characterizes a frequency with which a user selects both the first image and the second image in response to both being simultaneously identified by search results of the search query. The similar image click-through rates for the image pairs characterize the frequency with which a user selects the first image in response to the first image being identified by search results of a search query that includes the second image, and vice versa. The training system may train the image embedding model to generate a similar embedding of the first image and the second image when a co-click rate, a similar image click rate, or both of the first image and the second image indicate that they are related.
In some implementations, the training system can use query-image training examples and image-image training examples derived from historical query logs to jointly train the image embedding model and the query embedding model with a loss function that is graphical regularization. In particular, the query-image training examples and the image-image training examples may be represented as graph structures, and the training system may jointly train the image embedding model and the query embedding model using a loss function based on the graph structures.
These and other features are described in more detail below.
FIG. 1 illustrates an example image embedding model 100. The image embedding model 100 is configured to process an image 102 according to current values of a set of image embedding model parameters to generate an embedding 104 of the image 102. The embedding 104 is a representation of the image 102 as an ordered set of values (e.g., as a vector or matrix). As will be described in more detail below, the image embedding model 100 may be trained using machine learning techniques to generate an embedding 104 of the image 102, the embedding 104 implicitly representing semantic content of the image 102 (e.g., an object depicted by the image 102).
The image embedding model 100 may be configured to process an image 102 represented in any suitable format. For example, the image embedding model 100 may be configured to process an image 102 represented in a red-green-blue (RGB) color format (i.e., a format in which an image is represented by associating respective red, green, and blue values with each pixel of the image). As another example, the image embedding model 100 may be configured to process a feature representation of the image 102, such as a Histogram of Oriented Gradient (HOG) representation, a scale-invariant feature transform (SIFT) representation, or a Speeded Up Robust Features (SURF) representation of the image 102. Other feature representations may also be used for training.
The image embedding model 100 may be a neural network model implemented by a computer program on one or more computers in one or more locations. For example, the image embedding model 100 may be a convolutional neural network with an architecture derived from an inclusion neural network or a ResNet neural network.
FIG. 2 illustrates an example text embedding model 200. The text embedding model 200 is configured to process a representation of a sequence of one or more words in a natural language (i.e., text 202) according to current values of a set of text embedding model parameters to generate an embedding 204 of the text 202. The embedding 204 is a representation of the text 202 as an ordered set of values (e.g., as a vector or matrix). As will be described in more detail below, the text embedding model 200 may be trained using machine learning techniques to generate an embedding 204 of text 202, the embedding 204 implicitly representing semantic content of the text 202 (e.g., an object described by the text 202).
The text embedding model 200 may be configured to process text 202 represented in any suitable format. For example, the text embedding model 200 may be configured to process text 202 represented as a series of "one-hot" vectors, where each one-hot vector represents a respective character (or word) of the text 202. As another example, the text embedding model 200 may be configured to process text 202 represented by the output of the Word2vec model.
The text embedding model 200 may be a neural network model implemented by a computer program on one or more computers in one or more locations. For example, the text embedding model 200 may be a convolutional neural network having an architecture that includes a plurality of one-dimensional (1D) convolutional layers. As another example, the text embedding model 200 may be a lookup-based mapping from text 202 to embedding 204. As another example, the text embedding model 200 may be a series of fully connected layers configured to process n-gram text tokens. As another example, the text embedding model may be a recurrent neural network model (e.g., LSTM) configured to sequentially process representations of characters of the text 202.
FIG. 3 illustrates an example training system 300 for training the image embedding model 100 and the text embedding model 200 using training data 302 derived from historical query logs 304 of a search system 306. The training system 300 trains the image embedding model 100 and the text embedding model 200 by determining the values of the image embedding model parameters 308 and the text embedding model parameters 310. For example, when the image embedding model 100 and the text embedding model 200 are implemented as respective neural networks, the training system 300 may iteratively adjust parameters of the neural networks using gradients of the loss function, as will be described in more detail below. The training system 300 may be implemented by a computer program on one or more computers in one or more locations. In some embodiments, the training system 300 uses one or more tensor processing units (TPU-Application Specific Integrated Circuits (ASICs) designed for machine learning) during the training of the image embedding model 100 and the text embedding model 200.
Historical query log 304 of search system 306 indexes a large number (e.g., millions) of search queries previously processed by search system 306. In particular, historical query log 304 may index search queries by maintaining data that includes: (i) a search query, and (ii) data specifying one or more search results selected by a user of the device that sent the search query. A user may "select" a search result by expressing an interest in the search result via any kind of interaction with the search result. For example, a user may select a search result by clicking on a hypertext link included in the search result or by hovering a cursor over the search result for a predetermined period of time to generate a request for an electronic document (e.g., an image) identified by the search result.
The training system 300 may process data from the historical query logs 304 to generate query-image training examples and image-image training examples for training the image embedding model 100 and the text embedding model 200.
The query-image training examples include: (i) a text search query, (ii) an image, and (iii) selection data characterizing a frequency with which a user selects an image in response to the image being identified by search results of the text search query. If the selection data indicates that the user frequently selects an image when the image is identified by search results of a search query, the training system 300 may jointly train the image embedding model 100 and the text embedding model 200 to generate a similar embedding of the search query and the image. The similarity between the embeddings may be determined using any suitable numerical similarity metric (e.g., using euclidean distance if the image embedding model 100 and the text embedding model 200 are configured to generate embeddings of the same dimension).
The image-image training example includes an image pair (which includes a first image and a second image) and selection data indicating: (i) a co-click rate of the image pair, (ii) a similar image click rate of the image pair, or (iii) both. The common click rate of the image pair characterizes a frequency with which a user selects both the first image and the second image in response to both being simultaneously identified by search results of the search query. The similar image click-through rates for the image pairs characterize the frequency with which a user selects the first image in response to the first image being identified by search results of a search query that includes the second image, and vice versa. The training system 300 may train the image embedding model 100 to generate a similar embedding of the first image and the second image when the co-click rate, similar image click rate, or both of the first image and the second image indicate that they are related.
FIG. 4 shows an example search results page 400 provided by search system 306, the example search results page 400 including image search results for a search query that includes a sequence of one or more words. In particular, search results page 400 displays search query 408: search results 402, 404, and 406 for "red ford wildhorse".
FIG. 5 shows an example search results page 500 provided by search system 306, the example search results page 500 including image search results for a search query that includes an image. In particular, the search results page 500 displays search results 502, 504, and 506 of a search query 508, the search query 508 including an image depicting a truck. In response to receiving a search query that includes a query image, search system 306 may be configured to provide search results that identify images similar to the query image. In this example, each of the search results 502, 504, and 506 identifies images that are similar to the query image.
If the user selects a search result from the same set of search results that identifies the first image and the second image, respectively, the user is said to "co-click" on the first image and the second image. For example, the user may co-click on the image identified by search result 402 and the image identified by search result 404 by selecting two search results (e.g., one after the other) on search results page 400. As another example, the user may co-click on the image identified by search result 504 and the image identified by search result 506 by selecting two search results (e.g., one after the other) on search results page 500. If the user selects three or more search results from the same set of search results, the images identified by each pair of selected search results may be considered co-clicked. For example, if the user selects search result A, B and C from the same set of search results, the image pairs identified by search results { A, B }, { A, C } and { B, C } may all be considered co-clicked.
FIG. 6 illustrates an example process for jointly training the image embedding model 100 and the text embedding model 200 using a query-image training example 600. The query-image training example 600 includes a search query 602 and an image 604, the search query 602 including a sequence of one or more words. The training system 300 uses the image embedding model 100 and processes the image 604 according to the current values of the image embedding model parameters 308 to generate an embedding 606 of the image 604. The training system 300 processes the search query 602 using the text embedding model 200 and based on the current values of the text embedding model parameters 310 to generate an embedding 608 of the search query 602.
The training system 300 determines a similarity metric 610 between the embedding 606 of the image 604 and the embedding 608 of the search query 602, and determines a model parameter adjustment 612 based on the similarity metric 610. Thereafter, the training system 300 uses the model parameter adjustments 612 to adjust the values of the image embedding model parameters 308 and the text embedding model parameters 310. In some implementations, in determining the model parameter adjustments 612, the training system 300 uses selection data that characterizes the frequency with which a user selects an image 604 in response to the image 604 being identified by search results of the search query 602. An example process for jointly training an image embedding model and a text embedding model using a query-image training example is described in more detail with reference to FIG. 8.
FIG. 7A illustrates an example process for training the image embedding model 100 using an image-to-image training example 700 that includes a first image 702 and a second image 704. The training system 300 uses the image embedding model 100 and processes the first image 702 according to the current values of the image embedding model parameters 308 to generate an embedding 706 of the first image 702. Similarly, the training system 300 uses the image embedding model 100 and processes the second image 704 according to the current values of the image embedding model parameters 308 to generate an embedding 708 of the second image 704.
The training system 300 determines a similarity metric 710 between the embedding 706 of the first image 702 and the embedding 708 of the second image 704 and determines a model parameter adjustment 712 based on the similarity metric 710. In some embodiments, in determining the model parameter adjustments 712, the training system 300 uses selection data that characterizes the co-click rate, similar image click rate, or both of the first image 702 and the second image 704. An example process for training an image embedding model using an image-to-image training example is described in more detail with reference to FIG. 9.
FIG. 7B illustrates an example process for jointly training an image embedding model and a text embedding model using a query-image training example and an image-image training example. In particular, in each of a plurality of training iterations, one or more query-image training examples 600 and one or more image-image training examples 700 may be processed by an image embedding model to generate respective embeddings (as described with reference to fig. 6 and 7A). The training system 300 may determine the corresponding model parameter adjustments 714 based on the query-image training example 600 (as described with reference to fig. 6) and the image-image training example (as described with reference to fig. 7A). Thereafter, the training system 300 may use the model parameter adjustments 714 to adjust the current values of the image embedding model parameters 308 and the text embedding model parameters 310. The model parameter adjustments determined based on the query-image training examples may be weighted more heavily or less heavily (e.g., using a gradient scaling factor) than the model parameter adjustments determined based on the image-image training examples. The weights applied to model parameter adjustment may be adjustable system hyper-parameters.
FIG. 8 is a flow diagram of an example process 800 for jointly training an image embedding model and a text embedding model using query-image training examples derived from historical query logs of a search system. For convenience, process 800 will be described as being performed by a system of one or more computers located at one or more locations. For example, a training system (e.g., training system 300 of FIG. 3) suitably programmed in accordance with the subject specification can perform process 800.
The system processes data from the historical query logs of the search system to generate a candidate set of query-image training examples (802). Each query-image training example includes: (i) a search query comprising a sequence of one or more words, (ii) an image, and (iii) selection data characterizing a frequency with which a user selects an image in response to the image being identified by search results of the search query. The selection data may indicate a score for a number of times the image is selected in response to the image being identified by search results of the search query.
The system selects a query-image training example from a candidate set of training examples for jointly training the image embedding model and the text embedding model based at least in part on selection data for the training example (804). For example, if the image of a particular training example is selected most frequently by the user in response to the image being identified by the search results of the search query of the particular query-image training example, the system may select the particular query-image training example. As another example, the system may select a particular query-image training example if the images of the particular query-image training example are among the top N images most frequently selected by the user after being identified by the search results of the search query of the particular query-image training example. The system may use any of a variety of other suitable criteria in selecting query-image training examples for jointly training the image embedding model and the text embedding model. For example, the system may limit the number of selected query-image training examples that include a search query specifying the name of a particular person and corresponding images depicting the particular person. In this example, because the appearance of the same person may vary significantly between images (e.g., because the person wears different clothing, shoes, glasses, etc.), including a large number of query-image training examples corresponding to a particular person may reduce the effectiveness of the training process.
Step 806-. For convenience, step 806 and 812 describe steps that may be performed for a given query-image training example. More generally, any suitable method may be used to jointly train the image embedding model and the text embedding model. For example, the image embedding model and the text embedding model may be jointly trained using a stochastic gradient descent method, where step 806-. In this example, the system may determine that training is complete when the training termination criteria are met. For example, the training termination criteria may be that a predetermined number of iterations of step 806 and 812 have been performed. As another example, the training termination criteria may be that the values of the parameters of the image embedding model and the text embedding model vary below a predetermined threshold between iterations of step 806 and 812.
The system processes the images of the given query-image training example using the image embedding model and according to the current values of the image embedding model parameters to generate an embedding of the images (806). For example, if the image embedding model is a neural network model, the system processes the image using a series of neural network layers defined by the architecture of the neural network model.
The system processes a representation of the search query for the given query-image training example using the text embedding model and according to current values of text embedding model parameters to generate an embedding of the search query (808). For example, if the text embedding model is a neural network model, the system processes the representation of the search query using a series of neural network layers defined by the architecture of the neural network model.
The system determines a similarity metric between the embedding of the image and the embedding of the search query for the given query-image training example (810). For example, the embedding of the image and the embedding of the search query may have the same dimensions, and the system may determine the similarity metric by determining a euclidean distance or a cosine similarity metric between the respective embeddings.
The system adjusts image embedding model parameters and text embedding model parameters based at least in part on a similarity metric between the embedding of the image and the embedding of the search query for the given query-image training example (812). For example, when the image embedding model and the text embedding model are respective neural network models, the system may determine a gradient of the loss function and use the gradient of the loss function to adjust image embedding model parameters and text embedding model parameters. The system may determine the gradient of the loss function using any suitable method (e.g., back propagation). The loss function may be any suitable loss function that depends on a similarity measure between the embedding of the images and the embedding of the search query for a given query-image training example. The following are some examples.
In some embodiments, the loss function may be a categorical loss function. In these embodiments, the search query for a given query-image training example is considered to identify a "positive" label for the image of the given query-image training example. The search queries of the other query-image training examples are considered to identify the corresponding "negative" label for the image of the given query-image training example. More specifically, the system may determine a similarity metric between the embedding of the image of the given query-image training example and the embedding of the search query of the given query-image training example as a "positive" score. The system may determine a respective "negative" score for each other training example as a similarity measure between the embedding of the image of the given query-image training example and the embedding of the search query of that other training example. The system may process the positive and negative scores using a soft-max (or sampled soft-max) function and provide the output of the soft-max (or sampled soft-max) function to a cross entropy loss function (or any other suitable classification loss function).
In some embodiments, the loss function may be a triplet loss function. In these implementations, the system may determine the embedding of the image for a given query-image training example as "anchor," determine the embedding of the search query for the given query-image training example as "positive," and determine the embedding of the search query for another query-image training example as "negative.
Alternatively, the loss function may depend on selection data for a given query-image training example that characterizes a frequency with which an image is selected by a user in response to recognition of the image by search results of a search query of the given query-image training example. For example, the loss function may include a multiple scaling factor that is based on a score of the number of times the user selects an image in response to the image being identified by the search results of the search query of the given query-image training example.
FIG. 9 is a flow diagram of an example process 900 for training an image embedding model using image-to-image training examples derived from historical query logs of a search system. For convenience, process 900 will be described as being performed by a system of one or more computers located at one or more locations. For example, a training system (e.g., training system 300 of FIG. 3) suitably programmed in accordance with the present description may perform process 900.
The system processes data from the historical query logs of the search system to generate an image-to-image training example (902). Each image-image training example includes: (i) a pair of images comprising the first image and the second image, (ii) selection data indicative of a co-click rate of the pair of images, a similar image click rate of the pair of images, or both.
The common click rate of the image pair characterizes a frequency with which a user selects both the first image and the second image in response to both being simultaneously identified by search results of the search query. For example, the co-click rate of an image pair may indicate a score of a number of times for selecting both the first image and the second image in response to both being identified by search results of the search query simultaneously.
The similar image click-through rates for the image pairs characterize the frequency with which a user selects the first image in response to the first image being identified by search results of a search query that includes the second image, and vice versa. For example, a similar image click rate for an image pair may indicate a fraction of the number of times a user selects a first image in response to the first image being identified by search results of a search query that includes a second image, and vice versa.
Step 904-. For convenience, step 904-908 describes steps that may be performed for each image-image training example. More generally, any suitable method may be used to train the image embedding model. For example, the image embedding model may be trained using a random gradient descent method, wherein steps 904-908 are iteratively repeated for a "batch" (i.e., set) of selected training examples. In this example, the system may determine that training is complete when the training termination criteria are met. For example, the training termination criteria may be that a predetermined number of iterations of steps 904-908 have been performed. As another example, the training termination criteria may be that the values of the parameters of the image embedding model vary below a predetermined threshold between iterations of steps 904 and 908. As will be described in more detail with reference to fig. 10, the image-image training example may also be used in conjunction with the query-image training example to jointly train the image embedding model and the text embedding model using a loss function of graphical regularization.
The system processes the first image and the second image of the training example using the image embedding model and according to current values of image embedding model parameters to generate respective embeddings of the first image and the second image of the training example (904). For example, if the image embedding model is a neural network model, the system processes the first image and the second image (e.g., one after the other) using a series of neural network layers defined by the architecture of the neural network model.
The system determines a similarity metric between the embedding of the first image and the embedding of the second image (906). For example, the system may determine the similarity metric by determining a euclidean distance or a cosine similarity metric between the respective embeddings of the first image and the second image.
The system adjusts image embedding model parameters based at least in part on: (i) a similarity measure between the respective embeddings of the first image and the second image, and (ii) selection data (i.e., a co-click rate, a similar image click rate, or both) (910). For example, when the image embedding model is a neural network, the system may determine a gradient of the loss function and use the gradient of the loss function to adjust the image embedding model parameters. The system may determine the gradient of the loss function using any suitable method (e.g., back propagation). The loss function may be any suitable loss function depending on a similarity measure between the respective embedded and selected data. For example, the loss function may be given by:
w·D(hθ(I1),hθ(I2)) (1)
wherein h isθ(I1) Embedding of a first image representing an image-image training example, hθ(I2) Embedding of a second image representing an image-image training example, D (·,) is a similarity metric (e.g., a Euclidean similarity metric), and the scaling factor w may be determined in any suitable manner using the co-click rate, similar image click rate, or both of the image-image training examples. For example, the scaling strength w may be determined as a linear combination of the co-click rate and the similar image click rate of the image-to-image training example (e.g., using a predetermined weighting factor).
In some implementations, the training system 300 can use the query-image training example and the image-image training example to jointly train the image embedding model 100 and the text embedding model 200 with a loss function that is graph regularized. The query-image training examples and the image-image training examples may be understood as representing graph structures, where each node of the graph corresponds to a query-image training example and each edge corresponds to an image-image training example. More specifically, edges in the graph connecting the first and second nodes corresponding to the first and second query-image training examples, respectively, may be defined by the image-image training examples that include pairs of images specified by the first and second query-image training examples. In particular, the "strength" of an edge connecting a first node and a second node may be defined based on a co-click rate, a similar image click rate, or both, specified by the image-image training example corresponding to the edge. FIG. 10 shows an example of a portion of a graphical representation in which nodes 1002 and 1004 represent a query-image training example that includes respective images 1006 and 1008 of a car. In this example, an edge 1010 connecting nodes 1002 and 1004 is associated with a common click rate X (where X may be a real number) and a similar image click rate Y (where Y may be a real number) for the image pairs 1006 and 1008. More generally, some or all of the nodes of the graph may correspond to images included in image-image training examples, where images are not included in any query-image training examples (i.e., are not associated with corresponding text search queries).
In one example, the loss function of the graph regularization may have the form:
where I indexes the nodes in the graphical representation, N is the total number of nodes, IiImages representing an image associated with the ith node (e.g., an image of the image-query training example (if any) corresponding to the ith node), QiSearch queries, L, representing image-query training examples (if any) corresponding to the ith node1(Ii,Qi) Representing a penalty function (e.g., a classification penalty or a triplet penalty as described with reference to 812) associated with the image-query training example corresponding to the ith node, N (i) representing a set of "neighbors" of node i in the graphical representation, wijRepresenting links in a graphical representationThe strength of the edge connecting node i and node j, hθ(Ii) Representing an image I associated with the ith nodeiIs embedded, hθ(Ij) Representing an image I associated with the jth nodejD (·,) is a similarity metric (e.g., a euclidean similarity metric). Two nodes in the graph representation are called neighbors if they are connected by an edge. The strength w of the edge connecting nodes i and j may be determined in any suitable manner using the co-click rate, similar image click rate, or both of the image-image training examples defining the edgeij. For example, the strength w of the edge connecting nodes i and j can be determinedijDetermined as a linear combination of the co-click rate and the similar image click rate (e.g., using a predetermined weighting factor). For nodes associated with pictures rather than related to a textual search query, the lossy L defined by equation 2 may be removed1(Ii,Qi) And (4) components.
The training system 300 may jointly train the image embedding model 100 and the text embedding model 200 using a graph-regularized loss function (e.g., as described in equation 2) using any suitable machine learning training technique. For example, the training system 300 may jointly train the image embedding model 100 and the text embedding model 200 by stochastic gradient descent using an alternative representation of the loss function in equation 2:
where i and j index the nodes of the graph representation, and if node i and node j are connected by an edge in the graph representation, then (i, j) ∈, | i | represents the number of edges incident on node i, | j | represents the number of edges incident on node j, and the remaining variables are defined in the same manner as equation 2. In this example, the training system 300 may perform a random gradient descent by sampling the edges from the graphical representation in each training iteration and using back propagation (or any other suitable technique) to perform the gradient of the loss function given by equations 3 and 4. The training system 300 may determine that training is complete when any suitable training termination criteria is met, for example, when a predetermined number of iterations of random gradient descent have been performed. An example method for training the image embedding model 100 and the text embedding model 200 Using a Graph regularized loss function is described with reference to t.d. bui, s.ravi, v.ramavajjala, "Neural Graph machinery: Learning Neural Networks Using Graphs", 2017, arXiv:1703.04818 vl.
After the training system 300 determines the values of the image embedding model parameters 308 and the text embedding model parameters 310, the trained image embedding model 100 and text embedding model 200 may be used for any of a variety of purposes. The following are some examples.
In one example, the trained image embedding model 100 can be used by the search system 306 to rank image search results in response to a search query that includes a query image. More specifically, the search system 306 may use the image embedding model 100 to generate a respective embedding for each image in a search index maintained by the search system (as described with reference to FIG. 11). Upon receiving a search query that includes a query image, the search system 306 may use an image embedding model to generate an embedding of the query image, and thereafter use the generated embedding to determine a respective relevance score for each of a plurality of images in the search index. The search system 306 may determine a relevance score for a given image in the search index based on a similarity metric (e.g., euclidean distance) between the embedding of the given image and the embedding of the query image. The search system 306 may determine a ranking of image search results for the search query based at least in part on the relevance scores determined using the embedding generated by the image embedding model 100. As one example, consider a situation in which an image of a piece of mechanical equipment is periodically acquired and used to determine when it is likely that a component may need to be replaced or serviced. The search index may contain images of components having various degrees of wear, and the search system may process the received component images to obtain an indication of the current state (e.g., current degree of wear) of the components embodied in the images. The system may further provide an output of the determined status and/or alert an operator if the component requires replacement or maintenance. As another example, the query image may be a medical image, and the images may be images of patients with various medical conditions or in various health states. The search system may process the received component images to obtain an indication of the current medical condition of the person embodied in the images, for example, as a first stage check to determine whether the medical image should be reviewed by a specialist.
In another example, both the trained text embedding model and the trained image embedding model may be used by search system 306 to rank image search results in response to a search query that includes a sequence of one or more words. More specifically, the search system 306 can use the image embedding model 100 to generate a respective embedding for each image in a search index maintained by the search system. After receiving a search query comprising a sequence of one or more words, the search system may generate embeddings of the sequence of words using a text embedding model, and thereafter determine a respective relevance score for each of a plurality of images in the search index using the generated embeddings. The search system may determine a relevance score for a given image in the search index based on a similarity metric (e.g., euclidean distance) between the embedding of the given image and the embedding of the word sequence of the search query. Search system 306 can determine a ranking of image search results for the search query based at least in part on relevance scores determined using the embeddings generated by the image embedding model and the text embedding model.
In another example, the trained text embedding model may be used to determine a "cluster" of similar keywords (or keyword sequences), i.e., a collection of keywords that express similar semantic content. In a specific example, the group of similar keywords may be: "shoes", "footwear", "boots", "spiked shoes", "high-heeled shoes", "slippers", "sneakers" and the like. A text embedding model may be used to generate a keyword cluster by determining a respective embedding for each keyword in a keyword corpus, and thereafter using a clustering algorithm to cluster the keywords based on their respective embedding. The clustering algorithm may be, for example, a k-means clustering algorithm or an expectation maximization clustering algorithm. Keyword clusters generated using the trained text embedding model may be used as distribution parameters that regulate the transmission of digital components (e.g., advertisements) for presentation with an electronic document (e.g., a web page).
In another example, both the trained text embedding model and the trained image embedding model may be used in an image classification system configured to process an image to generate an output associating the image with a label from a predetermined set of labels. For example, the labels may specify object categories (e.g., people, cats, vehicles, etc.), and the image classification system may be trained to associate the images with the labels of the objects depicted in the images. In this example, the image classification system may use the image embedding model 100 to generate an embedding of the input image, and may use the text embedding model to generate a respective embedding for each search query in the search query corpus. The image classification system may determine a respective similarity metric between the embedding of the input image and the respective embedding of each search query, after which the input image may be associated with the particular search query having the highest similarity metric. The image classification system may determine the label associated with the input image based on: (i) visual features derived from the input image, and (ii) semantic features derived from the particular search query. An example of an image classification system that may use the text embedding model 200 and the image embedding model 100 is described with reference to U.S. patent application No. 62/768,701.
FIG. 11 illustrates an example search system 100. The search system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations in which the systems, components, and techniques described below are implemented.
The search system 1100 is configured to receive a search query 1102 from a user device 1104, process the search query 1102 to determine one or more search results 1106 in response to the search query 1102, and provide the search results 1106 to the search device 1104. Search query 1102 may include search terms expressed in natural language (e.g., English), images, audio data, or any other suitable form of data. Search results 1106 identify electronic document 1108 from website 1110 that is responsive to search query 1102 and includes a link to electronic document 1108. Electronic documents 1108 may include, for example, images, HTML web pages, word processing documents, Portable Document Format (PDF) documents, and videos. Electronic document 1108 may include content such as words, phrases, images, and audio data, and may include embedded information (e.g., meta information and hyperlinks) and embedded instructions (e.g., scripts). Website 1110 is a collection of one or more electronic documents 1108 associated with a domain name and hosted by one or more servers. For example, website 1110 can be a collection of web pages formatted in hypertext markup language (HTML) that can contain text, images, multimedia content, and programming elements (e.g., scripts).
In a particular example, the search query 1102 may include the search term "Apollodenum," and the search system 1100 may be configured to perform an image search, i.e., provide search results 1106 identifying respective images responsive to the search query 1102. In particular, the search system 1100 may provide search results 1106, each search result including: (i) a title of the web page, (ii) a representation of the image extracted from the web page, and (iii) a hypertext link (e.g., specifying a Uniform Resource Locator (URL)) that points to the web page or the image itself. In this example, the search system 1100 may provide search results 1106, which include: (i) the title "apolipronto month" of a web page, (ii) a reduced-size representation (i.e., thumbnail) of an image of an apolipronto spacecraft included in the web page, and (iii) a hypertext link pointing to the image.
A computer network 1112, such as a Local Area Network (LAN), Wide Area Network (WAN), the internet, a mobile phone network, or a combination thereof, connects the website 1110, the user device 1104, and the search system 1100 (i.e., enables them to send and receive data over the network 1112). Generally, the network 1112 may connect the search system 1100 to thousands of websites 1110 and user devices 1104.
The user device 1104 is an electronic device that is under user control and is capable of sending and receiving data (including electronic documents 1108) over the network 1112. Example user devices 1104 include personal computers, mobile communication devices, and other devices that can send and receive data over the network 1112. The user device 1104 typically includes a user application (e.g., a web browser) that facilitates the sending and receiving of data over the network 1112. In particular, a user application included in the user device 1104 enables the user device 1104 to send a search query 1102 to the search system 1100 over a network 1112 and receive search results 1106 provided by the search system 1100 in response to the search query 1102.
A user application included in the user device 1104 may present the search results 1106 received from the search system 1100 to a user of the user device (e.g., by rendering a search results page that displays an ordered list of the search results 1106). The user may select one of the search results 1106 presented by the user device 1104 (e.g., by clicking on a hypertext link included in the search results 1106), which may cause the user device 1104 to generate a request for an electronic document 1108 identified by the search results 1106. A request for an electronic document 1108 identified by the search results 1106 is sent over the network 1112 to a website 1110 that hosts the electronic document 1108. In response to receiving the request for the electronic document 1108, the website 1110 hosting the electronic document 1108 may send the electronic document 1108 to the user device 1104.
The search system 1100 processes the search query 1102 using the ranking engine 1114 to determine search results 1106 that are responsive to the search query 1102.
Search system 1100 uses index engine 1120 to generate and maintain a search index 1116 via electronic documents 1108 of a "crawling" (i.e., systematically browsing) website 1110. For each of a large number (e.g., millions) of electronic documents 1108, search index 1116 suggests an index for the electronic documents by maintaining (i) data that identifies electronic document 1108 (e.g., by a link to electronic document 1108) and (ii) data that characterizes electronic document 1108. The data maintained by search index 1116 that characterizes the electronic documents may include, for example, data that specifies a type of electronic document (e.g., image, video, PDF document, etc.), a quality of the electronic document (e.g., a resolution of the electronic document (when the electronic document is an image or video)), keywords associated with the electronic document, cached copies of the electronic document, or a combination thereof.
The search system 1100 may store the search index 1116 in a data store that may include thousands of data storage devices. The indexing engine 1120 may maintain the search index 1116 by continually updating the search index 1116 (e.g., by indexing new electronic documents 1108 and deleting electronic documents 1108 that are no longer available from the search index 1116).
The search system 1100 uses the query logging engine 1122 to generate and maintain historical query logs 1118 (as previously described). The search system 1100 may store the historical query logs 1118 in a data store that may include thousands of data storage devices. The query logging engine 1122 may maintain the historical query logs 1118 by continually updating the historical query logs 1118 (e.g., by indexing new search queries as they are processed by the search system 1100).
FIG. 12 is a block diagram of an example computer system 1200 that may be used to perform the operations described above. The system 1200 includes a processor 1210, a memory 1220, a storage device 1230, and an input/output device 1240. Each of the components 1210, 1220, 1230, and 1240 may be interconnected, for example, using a system bus 1250. Processor 1210 is capable of processing instructions for execution within system 1200. In one implementation, the processor 1210 is a single-threaded processor. In another implementation, the processor 1210 is a multi-threaded processor. Processor 1210 is capable of processing instructions stored in memory 1220 or storage device 1230.
The memory 1220 stores information within the system 1200. In one implementation, the memory 1220 is a computer-readable medium. In one implementation, the memory 1220 is a volatile memory unit or units. In another implementation, the memory 1220 is a non-volatile memory unit or units.
The storage device 1230 is capable of providing mass storage for the system 1200. In one implementation, the storage device 1230 is a computer-readable medium. In various different implementations, the storage device 1230 may include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices (e.g., cloud storage) over a network, or some other mass storage device.
Input/output device 1240 provides input/output operations for system 1200. In one embodiment, input/output devices 1240 may include one or more network interface devices, such as an Ethernet card, a serial communication device (e.g., an RS-232 port), and/or a wireless interface device (e.g., an 802.11 card). In another embodiment, the input/output devices may include driver devices configured to receive input data and transmit output data to other input/output devices, such as a keyboard, a printer, and a display device 1260. However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and so forth.
Although an example processing system has been described in fig. 12, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
The term "configured" is used herein in connection with system and computer program components. For a system of one or more computers to be configured to perform a particular operation or action, it is meant that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operation or action. For one or more computer programs to be configured to perform specific operations or actions, it is meant that the one or more programs include instructions which, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions may be encoded on an artificially generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses various apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or can further comprise, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates a run-time environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be run on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Generally, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or on the same multiple computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in combination with, special purpose logic circuitry, e.g., an FPGA or an ASIC.
A computer suitable for running a computer program may be based on a general purpose microprocessor, or a special purpose microprocessor, or both, or on any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Further, the computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive).
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
To support interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices may also be used to support interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending documents to and receiving documents from the device used by the user; for example, the user is interacted with through a web browser that sends a web page to the user device in response to a request received from the web browser. In addition, the computer may interact with the user in reply by sending a text message or other form of message to a personal device (e.g., a smartphone running a messaging application) and receiving a response message from the user.
The data processing apparatus for implementing the machine learning model may also comprise, for example, a dedicated hardware accelerator unit for processing a general purpose and computationally intensive portion of the machine learning training or production (i.e., reasoning) workload.
The machine learning model may be implemented and deployed using a machine learning framework (e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework).
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface, a web browser, or an application through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the Internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data, such as HTML pages, to the user device, for example, for the purpose of displaying data to the user interacting with the device acting as a client and receiving user input from the user. Data generated at the user device, such as a result of the user interaction, may be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular inventions of particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings and described in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (20)
1. A method performed by one or more data processing apparatus, the method comprising:
processing data from a historical query log of a search system to generate a candidate set of training examples, wherein each training example comprises: (i) a search query comprising a sequence of one or more words, (ii) an image, and (iii) selection data characterizing a frequency with which a user selects an image in response to the image being identified by search results of the search query;
selecting a plurality of training examples from a candidate set of training examples for joint training based at least in part on selection data for the training examples: (i) an image embedding model having a plurality of image embedding model parameters, and (ii) a text embedding model having a plurality of text embedding model parameters; and
jointly training an image embedding model and a text embedding model using training data, wherein for each selected training example, the training comprises:
processing the images of the training examples using an image embedding model to generate an embedding of the images;
processing a representation of the search query of the training example using a text embedding model to generate an embedding of the search query;
determining a similarity measure between the embedding of the image and the embedding of the search query; and
the image embedding model parameters and the text embedding model parameters are adjusted based at least in part on a similarity metric between the embedding of the image and the embedding of the search query.
2. The method of claim 1, wherein the training data is generated using historical query logs of the web search system.
3. The method of claim 1 or 2, wherein the selection data for each training example indicates a score of a number of times the user selected an image of the training example in response to the image of the training example being identified by search results of the search query of the training example.
4. The method of claim 1, 2 or 3, wherein selecting a plurality of training examples from a candidate set of training examples comprises:
a plurality of training examples are selected for which images of the training examples are most frequently selected by a user in response to the images being identified by search results of a search query of the training examples.
5. The method of any preceding claim, wherein the image embedding model and the text embedding model comprise one or more neural networks.
6. The method of claim 5, wherein adjusting image embedding model parameters and text embedding model parameters comprises:
determining a gradient of a loss function, the loss function dependent on a similarity measure between the embedding of the image and the embedding of the search query; and
the gradient is used to adjust image embedding model parameters and text embedding model parameters.
7. The method of claim 6, wherein the loss function depends on the selection data of the training examples.
8. The method of claim 6, wherein the loss function is a classification loss function or a triplet loss function.
9. The method of any preceding claim, wherein the embedding of the image has the same dimensionality as the embedding of the search query.
10. The method of any preceding claim, wherein determining a similarity measure between the embedding of the image and the embedding of the search query comprises:
a euclidian distance between the embedding of the image and the embedding of the search query is determined.
11. A system, comprising:
one or more computers; and
one or more storage devices communicatively coupled to the one or more computers, wherein the one or more storage devices store instructions that, when executed by the one or more computers, cause the one or more computers to perform operations comprising:
processing data from a historical query log of a search system to generate a candidate set of training examples, wherein each training example comprises: (i) a search query comprising a sequence of one or more words, (ii) an image, and (iii) selection data characterizing a frequency with which a user selects an image in response to the image being identified by search results of the search query;
selecting a plurality of training examples from a candidate set of training examples for joint training based at least in part on selection data for the training examples: (i) an image embedding model having a plurality of image embedding model parameters, and (ii) a text embedding model having a plurality of text embedding model parameters; and
jointly training the image embedding model and the text embedding model using the training data, wherein for each selected training example, the training comprises:
processing the images of the training examples using an image embedding model to generate an embedding of the images;
processing a representation of the search query of the training example using a text embedding model to generate an embedding of the search query;
determining a similarity measure between the embedding of the image and the embedding of the search query; and
the image embedding model parameters and the text embedding model parameters are adjusted based at least in part on a similarity metric between the embedding of the image and the embedding of the search query.
12. The system of claim 11, wherein the training data is generated using historical query logs of the web search system.
13. The system of claim 11 or 12, wherein the selection data for each training example indicates a score of a number of times the user selected an image of the training example in response to the image of the training example being identified by search results of the search query of the training example.
14. The system of claim 11, 12 or 13, wherein selecting a plurality of training examples from the candidate set of training examples comprises:
a plurality of training examples are selected for which images of the training examples are most frequently selected by a user in response to the images being identified by search results of a search query of the training examples.
15. The system of claim 11, 12, 13 or 14, wherein the image embedding model and the text embedding model comprise one or more neural networks.
16. One or more non-transitory computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:
processing data from a historical query log of a search system to generate a candidate set of training examples, wherein each training example comprises: (i) a search query comprising a sequence of one or more words, (ii) an image, and (iii) selection data characterizing a frequency with which a user selects an image in response to the image being identified by search results of the search query;
selecting a plurality of training examples from a candidate set of training examples for joint training based at least in part on selection data for the training examples: (i) an image embedding model having a plurality of image embedding model parameters, and (ii) a text embedding model having a plurality of text embedding model parameters; and
jointly training the image embedding model and the text embedding model using the training data, wherein for each selected training example, the training comprises:
processing the images of the training examples using an image embedding model to generate an embedding of the images;
processing a representation of the search query of the training example using a text embedding model to generate an embedding of the search query;
determining a similarity measure between the embedding of the image and the embedding of the search query; and
the image embedding model parameters and the text embedding model parameters are adjusted based at least in part on a similarity metric between the embedding of the image and the embedding of the search query.
17. The non-transitory computer storage medium of claim 16, wherein the training data is generated using historical query logs of the web search system.
18. The non-transitory computer storage medium of claim 16 or 17, wherein the selection data for each training example indicates a score of a number of times the user selected an image of the training example in response to the image of the training example being identified by a search structure of a search query of the training example.
19. The non-transitory computer storage medium of claim 16, 17, or 18, wherein selecting a plurality of training examples from a candidate set of training examples comprises:
a plurality of training examples are selected for which images of the training examples are most frequently selected by a user in response to the images being identified by search results of a search query of the training examples.
20. The non-transitory computer storage medium of claim 16, 17, 18, or 19, wherein the image embedding model and the text embedding model comprise one or more neural networks.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/265,811 US20200250538A1 (en) | 2019-02-01 | 2019-02-01 | Training image and text embedding models |
US16/265,811 | 2019-02-01 | ||
PCT/US2019/057841 WO2020159593A1 (en) | 2019-02-01 | 2019-10-24 | Training image and text embedding models |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112119388A true CN112119388A (en) | 2020-12-22 |
Family
ID=68610305
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980028937.0A Pending CN112119388A (en) | 2019-02-01 | 2019-10-24 | Training image embedding model and text embedding model |
Country Status (4)
Country | Link |
---|---|
US (2) | US20200250538A1 (en) |
EP (1) | EP3743827A1 (en) |
CN (1) | CN112119388A (en) |
WO (1) | WO2020159593A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117556033A (en) * | 2024-01-11 | 2024-02-13 | 北京并行科技股份有限公司 | Method and device for determining embedded model parameters of question-answering system and computing equipment |
Families Citing this family (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11003856B2 (en) * | 2018-02-22 | 2021-05-11 | Google Llc | Processing text using neural networks |
US10740647B2 (en) | 2018-03-14 | 2020-08-11 | Adobe Inc. | Detecting objects using a weakly supervised model |
US11586927B2 (en) * | 2019-02-01 | 2023-02-21 | Google Llc | Training image and text embedding models |
US11631234B2 (en) * | 2019-07-22 | 2023-04-18 | Adobe, Inc. | Automatically detecting user-requested objects in images |
US11468550B2 (en) | 2019-07-22 | 2022-10-11 | Adobe Inc. | Utilizing object attribute detection models to automatically select instances of detected objects in images |
US11107219B2 (en) | 2019-07-22 | 2021-08-31 | Adobe Inc. | Utilizing object attribute detection models to automatically select instances of detected objects in images |
US11302033B2 (en) | 2019-07-22 | 2022-04-12 | Adobe Inc. | Classifying colors of objects in digital images |
US11423304B2 (en) * | 2020-01-15 | 2022-08-23 | Beijing Jingdong Shangke Information Technology Co., Ltd. | System and method for semantic analysis of multimedia data using attention-based fusion network |
US11468110B2 (en) | 2020-02-25 | 2022-10-11 | Adobe Inc. | Utilizing natural language processing and multiple object detection models to automatically select objects in images |
US11055566B1 (en) | 2020-03-12 | 2021-07-06 | Adobe Inc. | Utilizing a large-scale object detector to automatically select objects in digital images |
US11587234B2 (en) | 2021-01-15 | 2023-02-21 | Adobe Inc. | Generating class-agnostic object masks in digital images |
US11972569B2 (en) | 2021-01-26 | 2024-04-30 | Adobe Inc. | Segmenting objects in digital images utilizing a multi-object segmentation model framework |
US11830478B2 (en) | 2021-04-01 | 2023-11-28 | Nippon Telegraph And Telephone Corporation | Learning device, learning method, and learning program for images and sound which uses a similarity matrix |
CN113360537B (en) * | 2021-06-04 | 2024-01-12 | 北京百度网讯科技有限公司 | Information query method, device, electronic equipment and medium |
Family Cites Families (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120290575A1 (en) * | 2011-05-09 | 2012-11-15 | Microsoft Corporation | Mining intent of queries from search log data |
US10055493B2 (en) * | 2011-05-09 | 2018-08-21 | Google Llc | Generating a playlist |
US8855430B1 (en) * | 2012-05-30 | 2014-10-07 | Google Inc. | Refining image annotations |
US20140136540A1 (en) * | 2012-11-09 | 2014-05-15 | Ebay Inc. | Query diversity from demand based category distance |
US9710433B2 (en) * | 2012-11-30 | 2017-07-18 | Yahoo! Inc. | Dynamic content mapping |
US9633018B2 (en) * | 2013-01-14 | 2017-04-25 | Microsoft Technology Licensing, Llc | Generation of related content for social media posts |
US9582823B2 (en) * | 2013-06-17 | 2017-02-28 | Ebay Inc. | Metadata refinement using behavioral patterns |
US10181091B2 (en) * | 2014-06-20 | 2019-01-15 | Google Llc | Fine-grained image similarity |
US9836641B2 (en) * | 2014-12-17 | 2017-12-05 | Google Inc. | Generating numeric embeddings of images |
US10740596B2 (en) * | 2016-11-08 | 2020-08-11 | Nec Corporation | Video security system using a Siamese reconstruction convolutional neural network for pose-invariant face recognition |
US11625597B2 (en) * | 2017-11-15 | 2023-04-11 | Canon Medical Systems Corporation | Matching network for medical image analysis |
US10607118B2 (en) * | 2017-12-13 | 2020-03-31 | Microsoft Technology Licensing, Llc | Ensemble model for image recognition processing |
US11403690B2 (en) * | 2018-01-29 | 2022-08-02 | Walmart Apollo, Llc | Determining brand affinity of users |
US11507876B1 (en) * | 2018-12-21 | 2022-11-22 | Meta Platforms, Inc. | Systems and methods for training machine learning models to classify inappropriate material |
-
2019
- 2019-02-01 US US16/265,811 patent/US20200250538A1/en active Pending
- 2019-10-24 WO PCT/US2019/057841 patent/WO2020159593A1/en unknown
- 2019-10-24 CN CN201980028937.0A patent/CN112119388A/en active Pending
- 2019-10-24 EP EP19805802.6A patent/EP3743827A1/en active Pending
-
2023
- 2023-11-09 US US18/505,776 patent/US20240078258A1/en active Pending
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117556033A (en) * | 2024-01-11 | 2024-02-13 | 北京并行科技股份有限公司 | Method and device for determining embedded model parameters of question-answering system and computing equipment |
CN117556033B (en) * | 2024-01-11 | 2024-03-29 | 北京并行科技股份有限公司 | Method and device for determining embedded model parameters of question-answering system and computing equipment |
Also Published As
Publication number | Publication date |
---|---|
US20200250538A1 (en) | 2020-08-06 |
US20240078258A1 (en) | 2024-03-07 |
WO2020159593A1 (en) | 2020-08-06 |
EP3743827A1 (en) | 2020-12-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20240078258A1 (en) | Training Image and Text Embedding Models | |
US20220222920A1 (en) | Content processing method and apparatus, computer device, and storage medium | |
US20230205813A1 (en) | Training Image and Text Embedding Models | |
CN110162593B (en) | Search result processing and similarity model training method and device | |
CN111753060B (en) | Information retrieval method, apparatus, device and computer readable storage medium | |
WO2021159776A1 (en) | Artificial intelligence-based recommendation method and apparatus, electronic device, and storage medium | |
US9514405B2 (en) | Scoring concept terms using a deep network | |
US8515212B1 (en) | Image relevance model | |
CN111125422B (en) | Image classification method, device, electronic equipment and storage medium | |
US8374914B2 (en) | Advertising using image comparison | |
US20230409653A1 (en) | Embedding Based Retrieval for Image Search | |
US7831111B2 (en) | Method and mechanism for retrieving images | |
RU2711125C2 (en) | System and method of forming training set for machine learning algorithm | |
CN111382361B (en) | Information pushing method, device, storage medium and computer equipment | |
DE102016013487A1 (en) | Semantic vector space with natural language | |
US10606910B2 (en) | Ranking search results using machine learning based models | |
CN106354856B (en) | Artificial intelligence-based deep neural network enhanced search method and device | |
CN111625715A (en) | Information extraction method and device, electronic equipment and storage medium | |
US20070098257A1 (en) | Method and mechanism for analyzing the color of a digital image | |
CN114239730A (en) | Cross-modal retrieval method based on neighbor sorting relation | |
US20130332440A1 (en) | Refinements in Document Analysis | |
CN116956183A (en) | Multimedia resource recommendation method, model training method, device and storage medium | |
US20230237093A1 (en) | Video recommender system by knowledge based multi-modal graph neural networks | |
US11695645B2 (en) | Methods and systems for dynamic re-clustering of nodes in computer networks using machine learning models | |
CN117009621A (en) | Information searching method, device, electronic equipment, storage medium and program product |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |