CN105103089A - Systems and methods for generating accurate sensor corrections based on video input - Google Patents
Systems and methods for generating accurate sensor corrections based on video input Download PDFInfo
- Publication number
- CN105103089A CN105103089A CN201380074783.1A CN201380074783A CN105103089A CN 105103089 A CN105103089 A CN 105103089A CN 201380074783 A CN201380074783 A CN 201380074783A CN 105103089 A CN105103089 A CN 105103089A
- Authority
- CN
- China
- Prior art keywords
- references object
- portable set
- sensor
- processor
- video image
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
- G06T11/001—Texturing; Colouring; Generation of texture or colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/73—Determining position or orientation of objects or cameras using feature-based methods
- G06T7/75—Determining position or orientation of objects or cameras using feature-based methods involving models
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6812—Motion detection based on additional sensors, e.g. acceleration sensors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/80—Camera processing pipelines; Components thereof
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C25/00—Manufacturing, calibrating, cleaning, or repairing instruments or devices referred to in the other groups of this subclass
Abstract
A portable device includes a sensor, a video capture module, a processor, and a computer-readable memory that stores instructions. When executed on the processor, the instructions operate to cause the sensor to generate raw sensor data indicative of a physical quantity, cause the video capture module to capture video imagery of a reference object concurrently with the sensor generating raw sensor data when the portable device is moving relative to the reference object, and cause the processor to calculate correction parameters for the sensor based on the captured video imagery of the reference object and the raw sensor data.
Description
Technical field
The disclosure relates generally to the equipment being equipped with motion sensing module, and more specifically, relates to the precise calibration of development for the sensor used in such module.
Background technology
It is to present contextual object of the present disclosure generally that the background provided herein describes.The degree in this background parts is described in it, the work of the inventor of current nomination, and may each side of the qualified description as prior art in addition when submit applications, both indefinitely impliedly not admit as conflict prior art of the present disclosure yet.
In recent years, due to the progress in MEMS (micro electro mechanical system) (MEMS) field, such as the cost of the sensor of accelerometer, gyroscope and magnetometer and so on reduces.These cheap sensors are widely used in the mobile device of such as smart phone, flat computer etc. and so on, to be controlled or trigger software application by sensing relative motion (up and down, left and right, rolling, pitching (pitch), deflection (yaw) etc.).But compared with the sensor used in the business of such as unmanned plane or manufacturing machine people and so on or commercial Application, the sensor of the low cost used in mobile device has low accuracy.
Such as such as the sensor with the output of three-dimensional (3D) vector of accelerometer, magnetometer and gyroscope and so on is easy to occur sensor biased error, difference between its ideal that can be looked at as zero exports and to export with actual non-zero and intersecting axle mushing error, they are disturbed by the nonopiate of chip layout and mimic channel and cause.Usually, the error of sensor that motion sensing module uses can be classified into " drift " error and " intersecting axle " error.Drift error is defined as at True Data or the constant displacement between expection output with original sensor data.The sensor biased error of accelerometer is the example of drift error.Intersecting axle error is defined as the error (that is, error coupler is to multiple coordinate) of the component that inseparable one-tenth is associated with independent coordinate.The intersecting axle interference of magnetometer is the example of intersecting axle error.
In order to make great efforts the accuracy increasing motion sensing result, some motion sensing module with multiple sensor uses sensor fusion to carry out optimum results.In general, sensor fusion refers to the data of combination from multiple sensor, makes produced information have higher fiduciary level compared with the information produced from any one separated sensor.The data produced by multiple sensor may be redundancies, and the vicissitudinous fiduciary level of possibility tool, the data therefore from multiple sensor often have best of breed.Simple sensor fusion algorithm can use the weighted mean of the data from multiple sensor, to consider the fiduciary level changed, and more complicated sensor fusion algorithm can optimize the combination (such as, using Kalman filter or linear quadratic estimation) of sensing data in time.
In theory, sensor fusion techniques provides motion sensing result accurately, even if work as adopted separated sensor when having low fiduciary level.But in practice, for some combination of sensor, sensor fusion has some shortcoming.Such as, when the number of available sensors (that is, " feature set ") increases, the complicacy of sensor fusion algorithm acutely increases.Thus for using large quantity sensor and/or having the motion sensing module of sensor in complicated error source (such as, intersecting axle error), high assessing the cost makes sensor fusion thorny.On the other hand, a small amount of sensor seriously may limit any increase utilizing the measurement accuracy of sensor fusion.Therefore, the number of sensor affects the effectiveness of sensor fusion techniques greatly.In fact, available sensors is that in some sight of difference and incompatible type, sensor fusion techniques even may be completely impracticable wherein.Although some portable set realizes sensor fusion now, the technology realized compensates basic drift error at most, and does not compensate intersecting axle error.
Summary of the invention
According to an embodiment, portable set comprises sensor, Video Capture module, processor and stores the computer-readable memory of instruction.When performing on a processor, command operating is the original sensor data making sensor generate instruction physical quantity; When portable set moves relative to references object, Video Capture module is made to catch video image (videoimagery) the sensor generation simultaneously original sensor data of references object; And make processor calculate the correction parameter for sensor based on the video image of the references object of catching and original sensor data.
According to another embodiment, there is the method realization of the sensor error correction in the portable set of sensor and camera on the one or more processors for exploitation efficiently.Method comprises when portable set moves relative to references object, makes sensor generate the original sensor data of instruction physical quantity.Further, method comprises the multiple images sensor generation simultaneously original sensor data making camera catch references object.Again further, method comprises the geometric properties based on multiple image and references object, determines multiple locating and orientings of portable set; And use locating and orienting and original sensor data, calculate the correction parameter for sensor.
According to another embodiment, tangible computer computer-readable recording medium stores instruction.When executed by one or more processors, instruction makes one or more processor: receive the original sensor data generated by the sensor operated in a portable device; And receive the video image of the references object of being caught by the Video Capture module operated in a portable device.While portable set is relative to references object movement, catch original sensor data and video image concomitantly.Instruction makes one or more processor use video image and the original sensor data of the references object of catching further, calculates the correction parameter for sensor.
Accompanying drawing explanation
Fig. 1 illustrates sensor calibration developed by wherein portable set sample scenario based on the video image of the references object of catching.
Fig. 2 illustrate wherein portable set via sensor calibration routine to develop the example system of sensor calibration.
Fig. 3 is the process flow diagram of the exemplary method for generating sensor calibration based on the video image of catching.
Fig. 4 is the process flow diagram of the exemplary method for generating regular sensor calibration.
Fig. 5 is object for identifying in the video image of catching and mates the process flow diagram of the exemplary method of object and the references object identified.
Embodiment
The portable set of the specialized equipment that technology of the present disclosure can be utilized to input for such as smart phone, flat computer, process continuous videos etc. and so on, the video image based on the references object of catching develops sensor calibration.References object can be standard real world object, and wherein the correspondence of object represents that storage in a database as numerical data (such as, three-dimensional (3D) reconstruct of object).According to technology of the present disclosure, portable set is equipped with one or more sensor, catch the video image of references object and calculate as the accurate location of the function of time and/or orientation (geographic position of positioning mark portable set, and directed mark portable set is relative to the direction of the mass centre of portable set) based on the expression of the references object in this video image and references object database.The original sensor data (accelerometer data, gyro data etc.) concurrent with the video image of catching also collected by portable set.Based on location and/or directed and original sensor data, the exploitation of sensor calibration routine is for the correction parameter of the one or more sensors in the sensor comprised in portable set.These corrections can continuous application and regular update to improve sensing, thus calibrating sensors effectively.
Fig. 1 illustrates sensor calibration developed by wherein portable set 10 sample scenario based on the video image of the references object 20 of catching.Inter alia, portable set 10 comprises multiple sensors of such as motion sensor and so on.These sensors can be cheap MEMS sensor, such as such as accelerometer, magnetometer and gyroscope.In addition, portable set 10 is communicatively coupled to mobile network and/or wide area network by one or more wave point.The example embodiment of portable set 10 discusses in more detail with reference to Fig. 2.
Exemplary reference object 20 can be the landmark of such as such as Eiffel Tower or Empire State Building and so on.In some cases, corresponding to the digital 3D model storage of references object 20 in references object database.Numeral 3D model can represent the shape of references object with the combination of the point on 3D grid, simple shape (such as, polygon, right cylinder) etc., and represents the outward appearance of references object with color, one or more rest images etc.Further, the special properties of the references object database purchase such as references object in geometric proportion, measurement result, geographic position etc. and so on.References object database can be by the addressable such as Google in such as the Internet 3D
and so on the database of 3D model.
As indicated by path 25, when portable set 10 moves by 3d space, portable set 10 capturing video picture.Video image is made up of the unique successive images or frame comprising references object 20.When portable set 10 moves along path 25, portable set 10 changes relative to the position of references object 20 and/or direction, therefore at the video picture frame of catching along the difference place in path 25, the references object 20 from different points of view is shown.
In some embodiments, portable set 10 is from the 3D geometry of one or more two dimension (2D) video picture frame reconstructed reference object 20 of catching and outward appearance (such as, by structure from motion or SFM technology).Further, portable set 10 attempts the 3D geometry of references object 20 of coupling reconstruct and outward appearance (hereinafter referred to " 3D object reconstruction ") and the 3D model in references object database.Example matching process is with reference to Fig. 2 and discussing in detail with further reference to Fig. 5.
After mating the 3D geometry and/or outward appearance and suitable digital 3D model reconstructed, portable set 10 downloads the character of references object 20 from references object database.Such as, character can comprise with the height of such as references object 20 of suitable unit (such as, rice), the measurement result of width and the degree of depth and so on.Based on 3D object reconstruction and the character of references object 20, portable set 10 develops location and/or orientation accurately.The height of references object 20 in video picture frame and the height of the references object of measurement can indicate such as portable set 10 at a distance of the distance of references object 20.Location and/or orientation are corresponding to each time of catching one or more video picture frame.
Portable set 10 use accurately location and/or orientation to generate sensor calibration.Some sensor calibrations directly can calculate from location and/or orientation, and the exploitation of other sensor calibration can relate to the further conversion of location and/or orientation.Such as, the exploitation that accelerometer corrects can require intermediate computations, and wherein this intermediate computations relates to and such as calculates average acceleration based on multiple location.
After correction exploitation, the sensing routine application sensors of such as motion sensing routine and so on corrects to improve original sensor data.Such as, motion sensing routine can collect original sensor data, calculates observable (acceleration, direction etc.), and corrects to observable application sensors.By the further video image of the references object 20 or new references object of catching and analyze previous analysis, sensor calibration can upgrade in time.Thus the sensing of portable set 10 improves via sensor calibration, wherein sensor calibration is based on the video image of the references object of catching.
Fig. 2 illustrates the sensor calibration for one or more sensor 40 developed by wherein portable set 10 example system based on the video image of the such as references object of references object 20 and so on.Portable set 10 comprises the Video Image Capturing module 50 of the video image for catching references object.Such as, portable set 10 can triggering video image capture model 50 with the short time (such as, 5-10 second) IT video image, and perform sensor calibration routine 60 subsequently to develop sensor calibration based on the video image of catching, as discussed below.
Such as, Video Image Capturing module 50 can comprise CCD video camera, complementary metal oxide semiconductor (CMOS) (CMOS) imageing sensor or other suitable 2D Video Image Capturing equipment any.In certain embodiments, portable set 10 comprises such as secondary camera, light detects and range finding (LIDAR) sensor, laser instrument, radio detection and range finding (RADAR) sensor and so on 3D rendering capture device.Additionally, image capture module 50 can comprise the simulation of such as picture filter, polarization plates etc. and so on, optics or Digital Image Processing parts.
Being stored in sensor calibration routine 60 that is in computer-readable memory 55 and that performed by CPU65 uses the one or more video picture frames in video picture frame to generate one or more 3D object reconstruction (representing shape and the outward appearance of references object) of references object.Such as, sensor calibration routine 60 can select the frame of the predefine number in video image, and uses 3D reconfiguration technique to develop one or more 3D object reconstruction of references object based on selected frame.
3D object reconstruction can be developed with any suitable 3D model format known in the art, and 3D object reconstruction can be solid with reference to object encoding and/or be expressed as shell/border.Such as, 3D object reconstruction can with STereoLithography (STL), OBJ, 3DS, Polygon (PLY), Google
or
file layout.
Communication module 70 to send the one or more 3D object reconstruction in 3D object reconstruction to references object server 75 via mobile network 77 and wide area network 78.Subsequently, references object server 75 attempts other expression and the reference 3D model in the references object database 80 be stored on computer-readable recording medium of the one or more 3D object reconstruction of coupling and/or references object, and computer-readable recording medium can comprise volatibility and nonvolatile memory component.Various module can be used to mate the reference 3D model in 3D object reconstruction and references object database 80.Such as, 3D object reconstruction can be become the collection of part or distinguishing characteristics by references object server 75 with reference to 3D model decomposition, wherein mates to be defined as 3D object reconstruction and to have similar segment set with 3D model.Alternatively, such as, references object server 75 can compare sampled point on 3D dimension grid between the distribution (being called distribution of shapes) of distance, wherein coupling is defined as 3D object reconstruction and has similar distribution of shapes with 3D model.
In certain embodiments, communication module 70 sends the video image of catching of all or part to references object server 75.References object server 75 can match video picture itself and the reference 3D model in references object database 80.Such as, references object server 75 can analyze multiple frames that the video image of references object is shown from change viewpoint.Based on these viewpoints, references object server 75 can assign score at least some 3D model in the 3D model in references object database, and wherein score instruction 3D model and video image both represent the probability of same object.Such as, high score can define the coupling between 3D model and video image.According to an embodiment, portable set 10 provides the video image and original sensor data (together with sensor information with the type of mark sensor) of catching to the webserver of such as references object server 75 and so on.
After match video picture with reference 3D model, references object server 75 sends the instruction of the character of coupling references object to portable set 10.The sensor calibration routine 60 of portable set 10 uses one or more 3D object reconstruction of references object character (such as, the precise proportions of references object and measurement result) and references object to calculate location and/or directed accurately.Location and/or orientation can calculate according to any proper technology, such as the known technology in 3D reconstruct and augmented reality (AR) field.
Sensor calibration routine 60 is according to accurately location and/or orientation develop sensor calibration.In some embodiments, the exploitation of correction relates to simple directly operation, such as such as in the direct differential accurately between location and the raw data location exported by one or more sensor.In other cases, the exploitation of correction relates to the operation of multiple chain, such as coordinate transform, matrix inversion, numerical derivative etc.Such as, exploitation for the correction of gyro sensor can relate to positioned/oriented coordinate transform from from Cartesian coordinates to body-centered coordinate, time correlation rotation matrix (being associated with multiple orientation) numerical derivative, for deriving solving and matrix inversion for calculating suitable gyroscope correction parameter (such as, for the correction parameter of each Eulerian angle in three Eulerian angle) of the Line independent equation of accurate Eulerian angle.The exploitation that particular sensor corrects discusses in more detail with reference to Fig. 3.
Be stored in motion sensing routine 85 that is in storer 55 and that performed by CPU65 and apply the sensor calibration developed by sensor calibration routine 60, to improve sensing.Such as, sensor calibration parameter can be applied to the original sensor data exported from the one or more sensors sensor 40 by motion sensing routine 85.Motion sensing routine can process this calibrated sensing data further, to develop and to export the observable (with the acceleration of certain unit, in the direction of certain time, navigate and predict) of expectation.Expect that the exploitation of observable can relate to calibrated sensing data corresponding to the only sensor in sensor 40, or exploitation can relate to calibrated sensing data corresponding to the multiple sensors in sensor 40.
In certain embodiments, the 3D object reconstruction of portable set 10 to references object database 80 upload object and the character of calculating, be used as references object for by miscellaneous equipment.Such as, as discussed above, portable set 10 can improve sensing based on the video image of initial reference object, and portable set 10 can use through improve sensing to gather the character in such as ratio, geographic position etc. and so on of new real-world objects, wherein new real-world objects is not by the 3D model representation in references object database 80.In addition, portable set 10 can generate the 3D object reconstruction of new real-world objects based on the video image of catching.Then can upload character and the 3D object reconstruction of the new real-world objects of collection to references object database 80, thus increase the number of the available reference object in references object database 80.
And, the 3D object reconstruction of the references object frequently run into can be stored in local storage 55 by the exemplary portable equipment of such as portable set 10 and so on, and wherein storer 55 can be volatibility and/or the nonvolatile memory form of such as ROM (read-only memory) (ROM) and random access memory (RAM) and so on.These local 3D object reconstruction stored can be the 3D models downloaded from the references object database of such as references object database 80 and so on, or the local 3D object reconstruction stored can be the 3D object reconstruction of the new real-world objects based on the video image generation of catching.First portable set 10 can be attempted mating the references object in 3D object reconstruction and local storage 55, and then, if do not find suitable coupling, attempts the reference 3D model in coupling 3D object reconstruction and remote data base.In this way, portable set 10 by the 3D object reconstruction (instead of must exchange references object information with remote server) of the references object in the 3D object reconstruction of the current generation of coupling and local storage 55, can increase the efficiency of regular sensor calibration exploitation.
Its video image can be landmark with the references object that technology discussed above is caught, but references object is not limited to such terrestrial reference or is not even limited to common building.References object can be the object of any kind with corresponding reference information, and wherein reference information is used from exploitation sensor calibration together with video image one.Such as, chessboard, fast response (QR) code, bar code or other 2D object with known dimensions can be used as references object, for the sensor calibration developed for direction sensor, proximity transducer or other type sensor.
Next, Fig. 3 diagram is used for the exemplary method 110 generating portable device sensor correction based on the video image of catching.Method 110 can realize in the sensor calibration routine 60 such as shown in Fig. 2.
At frame 115 place, by the image capture module (such as, the image capture module 50 of portable set 10) of portable set in short time T IT video image.Time T can be the time of the predefined amount required for sensor calibration exploitation, or the nearest history that time T can be based on such as environmental baseline or sensor row is dynamically determined.Video image is made up of the one or more video picture frames comprising references object, and wherein video picture frame catches (that is, each frame catch dt of being separated by time) with frame rate 1/dt.The frame comprising references object can be included in the whole of references object in the boundary line of video picture frame or only part.Video image can comprise the 2D video image of being caught by 2D Video Image Capturing equipment and/or the 3D video image of being caught by 3D video capture device.
At frame 120 place, the references object in video image is mated with the expression of the references object in Local or Remote references object database.The expression of the object in references object database can comprise 3D model, ratio and measurement data, geographic position data etc.In certain embodiments, the mating to comprise and mate 3D model and/or 3D object reconstruction of video image and references object.In other embodiments, video image is mated by suitable 2D technology, such as such as analyzes the multiple 2D images corresponding to various viewpoint.
Next (frame 125), based on the coupling character of references object and the further process of video image, accurate location and/or orientation is calculated.Such as, the knowledge of references object ratio can be utilized to analyze 3D object reconstruction to infer location and/or orientation.Can for correspond to each video picture frame catch time (0, dt, 2dt ..., T) or the subset of these times come compute location and/or orientation.Such as, can calculate location and/or the orientation of predefine number (M), wherein M location and/or orientation are corresponding to the time (M<T/dt) of catching M frame.These times corresponding to frame subset can be equal or spaced apart unevenly in time.
3D location can be represented by three Cartesian coordinatess (x, y, z), and orientation can by three of the mass centre relative to a portable set Eulerian angle (
θ, ψ) represent.Coordinate (x, y, z) can define relative to the initial point x=y=z=0 of such as references object position, and Eulerian angle (
θ, ψ) can define relative to initial point φ=θ=ψ=0 at the horizontal direction place such as pointing to references object.
At frame 130 place, gather original sensor data for the one or more sensors in portable set.These sensors can export raw position data (x
raw, y
raw, z
raw) and its original orientation data (φ
raw, θ
raw, ψ
raw) or such as such as acceleration (a
x, raw, a
y, raw, a
z.raw) or ground magnetic vector (m
x, raw, m
y, raw, m
z.raw) and so on another three-component export.Sensor can also export the out of Memory of the component with any number in any format.Hereafter include the sample list that it can be implemented in the common sensor in portable set.This list is not intended to be exhaustive, and should be appreciated that technology of the present disclosure can be applied to the sensor of other type.
Type: | Original sensor data indicates: |
Accelerometer | Acceleration |
Barometer | Pressure |
Gyroscope | Object orientation |
Hygrometer | Humidity |
Infrared proximity transducer | To the distance of neighbouring object |
Infrared/laser radar sensor | Speed |
Magnetometer | The intensity in magnetic field and/or direction |
Photometer | Light intensity |
Alignment sensor | Geographic position |
Thermometer | Temperature |
Ultrasonic sensor | To the distance of neighbouring object |
At frame 135 place, exploitation sensor calibration parameter.These correction parameters can be derived from original sensor data with in the location that frame 125 generates and/or orientation.In order to the exploitation of sensor calibration is described, below describes and original sensor data is called x
raw=(x
raw, y
raw, z
raw) and true three-component character (such as, the physical location of portable set) is called x=(x, y, z).Should be appreciated that x
rawwith x can refer to such as direction vector, any three-component character of magnetic vector or other three-component character and so on.In addition, x
rawany of such as acceleration, speed, angular velocity etc. and so on can be referred to x and derive three-component character (that is, can derive from location and/or orientation).
The general structure that raw data exports can be expressed as x
raw=a+Cx, wherein vector a represents drift error, the yardstick ratio that Matrix C represents (x, y, z) diagonally and depart from cornerwise intersecting axle error, and vector x represents true three-component character (such as physical location, acceleration etc.).In the matrix representation of expansion, raw data exports and is:
EQUATION x
raw=a+Cx have expressed raw data with true/real three component character and exports, but equation can invert for raw data to express true three-component character, x=C
-1(x
raw– a).Thus, know C
-1offset drift rightly and intersecting axle error will be allowed with a.Therefore, C
-1with the expression that the unknown component of 12 in a can be sensor calibration parameter discussed above.
By using the video image of catching, three-component character x can estimate exactly for multiple position/orientation of portable set.Such as, can from the time 0, dt, 2dt ..., the T multiple video picture frames of catching calculate multiple location x (0), x (dt), x (2dt) ..., x (T).Further, three-component character can be derived from multiple location Calculation is multiple.Such as, by getting the numerical derivative (such as using finite difference method) of multiple location with time step dt, can calculate multiple acceleration a (0), a (dt), a (2dt) ..., a (T).Thus, by using the original sensor data of video image and the collection of catching, x
rawat least 12 values and at least 12 concurrent estimations (video image based on catching) of x combine, to estimate C
-1with the sensor calibration parameter of 12 in a.Further, if more than 12 (x
raw, x) to can be used for sensor calibration routine, then for C
-1refinement or optimization can be carried out about supplementary data with the estimation of a.Such as, refinement can be carried out for C by least square method or random sample consensus (RANSAC) method
-1with the estimation of a.
Fig. 4 diagram is for developing with regular update sensor calibration to improve the exemplary method 160 of the motion sensing in portable set.Method 160 can be implemented in the portable set 10 such as shown in Fig. 2.
At frame 165 place, capturing video picture, wherein video image comprises references object.At frame 170 place, based on this video image, sensor calibration routine exploitation sensor calibration.Then at frame 175 place, these sensor calibrations are applied to improve motion sensing.Such as, the motion sensing of improvement can be utilized in navigation, direction, range finding or other based drive application.
Next (frame 180), method 160 determines whether portable set should terminate the need of using motion sensing or motion sensing further.Such as, navigation application can be stopped, to trigger the end of the motion sensing of improvement.Under these circumstances, method 160 terminates, and when the Another Application in portable set needs to use the motion sensing improved, method 160 can restart.But if the application on portable set needs to use motion sensing further, then flow process proceeds to frame 185.
At frame 185 place, method 160 has determined whether the time since the last time exploitation that sensor corrects is greater than threshold value.Such as, portable set can pass through regular update sensor calibration (such as, per minute, every ten minutes, every day etc. upgrade sensor calibration) and improve sensing continuously, and in this case, threshold value corrects equaling required/preferred sensor the cycle upgraded.If the time since self-correcting exploitation is less than threshold value, then flow process turns back to frame 175, and current sensor corrects for improving further motion sensing.But if the time since self-correcting exploitation is greater than threshold value, then flow process turns back to frame 165, develops new sensor calibration there based on the video image of newly catching.
In certain embodiments, the time (that is, threshold value) between sensor calibration exploitation is dynamically determined.Such as, under some condition and/or geographic position, sensor is exposed to more or less error.Under these circumstances, definite threshold can be carried out based on location (such as, geo-positioning system or GPS location).Alternatively, based on the statistics behavior using one or more sensors of deduction in the past from one or more sensor, dynamically definite threshold can be carried out.
Fig. 5 illustrates the 3D object for identifying in video image and mates the method 220 of the references object in 3D object and references object database.Method 220 can be implemented in the portable set 10 such as shown in Fig. 2.
At frame 225 place, image capture module capturing video picture, wherein video image can comprise one or more references object.Video image with any video image form, can such as move photographic experts group (MPEG) 4, Audio Video Interleaved (AVI), Flash video (FLV) etc.Further, video image can have any suitable frame rate (24p, 25p, 30p etc.) and pixel resolution (1024x768,1920x1080 etc.).
At frame 230 place, via the object in 3D reconstruct or other suitable technology identification video image any.Such as, the image capture device of such as CCD camera and so on can catch multiple image by different points of view, and to infer the 3D structure of object, or multiple image capture device can catch stereo pairs, and uses superimposed images to infer 3D structure.In certain embodiments, the 3D structure of single object or multiple object can be inferred from video image.
At frame 235 place, carry out attempting the expression of the such as 3D model of the references object in the 3D structure of matching identification object and references object database and so on.References object database can be local data base (that is, being stored in the local storage of portable set) or by portable set via mobile network and/or the addressable remote reference object database of wide area network.
If the structure of the 3D structure matching references object of identification of object, then flow process proceeds to frame 240, and the object-based video image of portable set and the information about references object calculate location and/or orientation accurately there.But if the 3D structure of identification of object does not mate the structure of references object, then flow process proceeds to frame 245.
In certain embodiments, the geographic position (such as, survey location, GPS location) of references object is stored in references object database.Portable set can use this geographical location information to sort to references object, and geographically near references object was analyzed as potential coupling before the object being in remote geographic position.Such as, portable set can generate approximate location via GPS or other alignment sensor, and carries out rank according to the distance at a distance of approximate location to references object.In certain embodiments, all references object in database are all regarded as potential coupling, and in other embodiments, only the contiguous references object of predefine number is regarded as potential coupling.
At frame 245 place, determine whether up to the present identify the time spent with match objects is greater than threshold value.If the time spent so far is greater than threshold value, then method 220 terminates.Otherwise flow process turns back to frame 230, new or different object can be identified and mate with references object potentially there.
additional consideration
Following additional consideration is applicable to aforementioned discussion.Run through this instructions, Multi-instance can realize being described as the parts of single instance, operation or structure.Although the independent operation of one or more method is illustrated and is described as separate operations, the one or more operations separately in operation can perform concomitantly, and do not require that operation performs with illustrated order.The 26S Proteasome Structure and Function being rendered as the discrete parts in example arrangement may be implemented as structure or the parts of combination.Similarly, the 26S Proteasome Structure and Function being rendered as single parts can be implemented as discrete parts.These and other changes, amendment, interpolation and improvement drop in the scope of theme of the present disclosure.
Additionally, some embodiment is described as comprising the some parts of logical OR, module or mechanism in this article.Module can form arbitrary software module (code such as, machine readable media stored) or hardware module.Hardware module is the tangible unit that can perform some operation, and can configure in certain mode or arrange.In the exemplary embodiment, one or more computer system (such as, independently client or server computer system) or computer system one or more hardware modules (such as, processor or processor group) hardware module that software (such as, application or applying portion) is configured to be operating as execution some operation as described in this article can be passed through.
In various embodiments, hardware module can mechanically or electronically realize.Such as, hardware module can comprise special circuit or logic, and it is operated to perform some by permanent configuration (such as, as application specific processor, such as field programmable gate array (FPGA) or special IC (ASIC)).Hardware module can also comprise FPGA (Field Programmable Gate Array) or circuit (such as, as in general processor or other programmable processor comprise), it is temporarily configured to perform some by software and operates, be to be understood that, hardware module is mechanically realized the driving that the decision in the circuit of special and permanent configuration or in the circuit (such as, passing through software merit rating) of temporarily configuration can be considered by cost and time.
Therefore, term hardware should be understood to comprise tangible entity, as following entity, it by physique, permanent configuration (such as, pass through hardwire) or temporarily configure (such as, by programming) to operate or to perform some operation described herein in some way.Consider that wherein hardware module is by the embodiment of temporarily configuration (such as, by programming), each hardware module in hardware module does not need to be configured or instantiation when any one example of time.Such as, when hardware module comprises the general processor using software merit rating, general processor can be configured to corresponding different hardware module at different time.Software can correspondingly configuration processor, such as, for forming specific hardware module when an example of time, and forms different hardware module when the different instances of time.
Hardware and software module can provide information to other hardware and/or software module and receive the information from other hardware and/or software module.Accordingly, described hardware module can be regarded as communicative couplings.In multiple hardware in such hardware or software module or the simultaneous situation of software module, communication can be realized by (such as, on proper circuit and the bus) Signal transmissions connecting hardware or software module.Wherein multiple hardware module or software different time be configured or instantiation embodiment in, the storage of the information in the memory construction that the communication between such hardware or software module can such as can be accessed by multiple hardware or software module and obtaining realizes.Such as, a hardware or software module can executable operations, and the output of this operation are stored in its memory devices be communicatively coupled to.Then in the time after a while, other hardware or software module can access memory devices, to obtain and to process the output of storage.Hardware and software module can also be initiated and the communication inputing or outputing equipment, and can operate in resource (such as, the collection of information).
The various operations of exemplary method described herein can be performed by one or more processor at least in part, and this one or more processor temporarily configures (such as, passing through software) or is forever configured to perform associative operation.No matter temporarily configuration or permanent configuration, such processor can form the module that it is operating as the processor realization performing one or more operation or function.In some example embodiments, the module related to herein can comprise the module that processor realizes.
Similarly, method described herein or routine can be that processor realizes at least in part.Such as, the hardware module that at least some operation in the operation of method can be realized by one or more processor or processor perform.The performance of some operation can be distributed between one or more processor, not only resides in individual machine, but disposes across some machines.In some example embodiments, one or more processor can be positioned at single position (such as, in home environment, office environment or as server farm), and in other embodiments, processor can across some position distribution.
One or more processor can also be operating as the support " performance of the associative operation in cloud computing environment or as SaaS.Such as, as indicated above, at least some operation in operation can perform by calculating unit (example of machine as comprising processor), these operations via network (such as, the Internet) and may have access to via one or more suitable interface (such as, API).
The performance of some operation can be distributed between one or more processor, not only resides in individual machine, but disposes across some machines.In some example embodiments, the module that one or more processor or processor realize can be positioned at single geographic position (such as, in home environment, office environment or server farm).In other example embodiment, the module that one or more processor or processor realize can across some location distribution.
The some parts of this instructions represents with the algorithm of the operation for data or symbol and presents, and data are stored as bit or binary digital signal in machine memory (such as, computer memory).These algorithms or symbol represent it is that the those of ordinary skill of data processing field is for passing on the example of the technology of their work essence to others skilled in the art.As used in this article, " algorithm " or " routine " be operation be certainly in harmony sequence or cause the similar process of expected result.In this context, algorithm, routine and operation relate to the physical manipulation to physical quantity.Usually, but non-must, such amount can take the form of electric signal, magnetic signal or optical signalling, and they can be stored, access, transmit, combine, compare or otherwise be handled by machine.The reason of main term is out of habit use such as the word of " data ", " content ", " bit ", " value ", " element ", " symbol ", " char ", " item ", " number ", " numeral " etc. and so on to mention such signal sometimes easily.But these words are only mark easily, and to be associated with suitable physical quantity.
Unless otherwise expressly specified, use the discussion of the word of such as " process ", " calculating ", " determination ", " presenting ", " display " etc. and so on can refer to the machine of manipulation or transform data (such as herein, computing machine) action or process, data are represented as one or more storer (such as, volatile memory, nonvolatile memory or its combination), register or receive, store, physics (such as, electronics, magnetic or optics) amount in other machine part of transmission or display information.
As used in this article, any of " embodiment " or " embodiment " is mentioned that particular element, feature, structure or the characteristic meaning in conjunction with the embodiments to describe is included at least one embodiment.The appearance in phrase " in one embodiment " each place in the description not necessarily all refers to same embodiment.
Some embodiments can use expression " coupling " to be described with " connection " derivative words together with them.Such as, some embodiments can use term " coupling " to describe, and are in direct physical or electrical contact to indicate two or more elements.But term " coupling " can also mean two or more elements and directly not contact each other, but still coordination with one another or mutual.Embodiment is not limited to this context.
As used in this article, term " comprises ", " comprising ", " having " or its other variant any be intended to contain nonexcludability and comprise.Such as, comprise the process of element list, method, article or device and be not necessarily only limitted to those elements, but can comprise and clearly not listing or other element that such process, method, article or device are intrinsic.Further, unless clearly there is contrary explanation, "or" refer to inclusive or, instead of exclusiveness or.Such as, any one in following item satisfy condition A or B:A for true (or existence) and B be false (or not existing), A for false (or not existing) and B be true (or existence) for true (or existence) and A and B.
In addition, the use of "a" or "an" is adopted to element and the parts of description embodiment herein.This is only used to facilitate and the general sense providing description is carried out.This description should be read as and comprise one or at least one, and odd number also comprises plural number, unless obviously meant other side.
Upon reading this disclosure, it will be appreciated by those of skill in the art that additional alternative structural and the Functional Design of the correction parameter still had for being used video input exploitation sensor by principle disclosed herein.Thus, although specific embodiment and application have been illustrated and have described, should be appreciated that the disclosed embodiments are not limited to precise arrangements disclosed herein and parts.Various amendment, change and change (it will be easy understand to those skilled in the art) can be made in the layout of the methods and apparatus disclosed, operation and details aspect in this article, and not depart from the spirit and scope limited in appended claims.
Claims (24)
1. a portable set, comprising:
Sensor;
Video Capture module;
Processor; And
Computer-readable memory, stores instruction thereon, and wherein said instruction is operating as when being performed by described processor:
Described sensor is made to generate the original sensor data of instruction physical quantity,
When described portable set moves relative to references object, described Video Capture module is made to catch the video image described sensor generation original sensor data simultaneously of described references object, and
Described processor is made to calculate the correction parameter for described sensor based on the described video image of the described references object of catching and described original sensor data.
2. portable set according to claim 1, wherein said instruction is further operable to when being performed by described processor the standard real world object described references object being designated and having known geometric properties.
3. portable set according to claim 2, wherein said standard real world is to the 2D image liked on two dimension (2D) surface.
4. portable set according to claim 1, wherein said instruction is further operable to the digital 3D model mating described video image and the described references object of catching when being performed by described processor, wherein:
Described digital 3D model is stored in a database, and described portable set is coupled to described database via communication network, and
The geometric properties of described references object specified by described digital 3D model.
5. portable set according to claim 4, wherein in order to mate the described digital 3D model of described video image and the described references object of catching, described command operating is transmit the described video image of catching at least partially to the references object server being coupled to described database via described communication network.
6. portable set according to claim 4, wherein said instruction is further operable to the approximate location generated for the described portable set mated with the geographic position data of described digital 3D model when being performed by described processor.
7. portable set according to claim 1, wherein said sensor is one of following item:
(i) accelerometer,
(ii) gyroscope, or
(iii) magnetometer.
8. portable set according to claim 1, wherein said instruction makes described processor described correction parameter is applied to described sensor further follow-up original sensor data when being performed by described processor exports.
9. portable set according to claim 1, wherein in order to calculate described correction parameter, described command operating is:
Obtain the geometric properties of described references object,
The described geometric properties of described references object is used 3D reconfiguration technique to be applied to the described video image of catching, and
The multiple locating and orientings at the described portable set of corresponding time are calculated based on the described video image of catching.
10. portable set according to claim 9, wherein in order to calculate described correction parameter, described command operating is for determining x
rawvector a in=a+Cx and Matrix C, wherein:
Vector x
rawrepresent original sensor data,
Described vector a represents drift error,
Described Matrix C represents intersecting axle error, and
X represents calibrated original sensor data;
Wherein said command operating is use described multiple locating and orienting of described portable set to determine described vector a and described Matrix C.
11. portable sets according to claim 1, wherein said instruction is further operable to when being performed by described processor with correction parameter described in regular intervals regular update.
12. portable sets according to claim 1, wherein said Video Capture module to be configured to while described portable set operates capturing video picture continuously.
13. 1 kinds of methods realized on the one or more processors, the sensor error had in the portable set of sensor and camera for exploitation efficiently corrects, and described method comprises:
When described portable set moves relative to references object, described sensor is made to generate the original sensor data of instruction physical quantity;
The multiple images making described camera catch described references object simultaneously described sensor generate described original sensor data;
Based on the geometric properties of described multiple image and described references object, determine multiple locating and orientings of described portable set; And
Use described multiple locating and orienting and described original sensor data, calculate the correction parameter for described sensor.
14. methods according to claim 13, described method comprises via communication network to the described multiple image of references object server transmission further, multiple image and described references object described in wherein said references object server matches.
15. methods according to claim 14, described method comprises further transmits described original sensor data and sensor information to described references object server.
16. methods according to claim 13, comprise the standard real world object described references object being designated and having known geometric properties further.
17. methods according to claim 13, comprise the digital 3D model of the described multiple image of coupling and described references object further, wherein said digital 3D model is stored in a database.
18. methods according to claim 13, the digital 3D model wherein mating described multiple image and described references object comprises:
Generate one or more approximate location collection of described portable set,
Via communication network to the described one or more approximate location collection of references object server transmission, and
Receive the geo-location numeral 3D model of described references object from described references object server, wherein said geo-location numeral 3D model indicates the geometric properties of described references object.
19. 1 kinds of tangible computer computer-readable recording mediums, store instruction thereon, and described instruction makes described one or more processor when executed by one or more processors:
Receive the original sensor data generated by the sensor operated in a portable device;
Receive the video image of the references object of being caught by the Video Capture module operated in described portable set, wherein while described portable set is relative to described references object movement, catch described original sensor data and described video image concomitantly;
Use the described video image of the described references object of catching and described original sensor data, calculate the correction parameter for described sensor.
20. computer-readable mediums according to claim 19, wherein in order to calculate described correction parameter, described instruction makes described one or more processor:
Determine the geometric properties of described references object,
Based on described geometric properties and the described video image of described references object, determine the locating and orienting of described portable set,
Based on the described locating and orienting determined, determine the correct sensing data corresponding to described original sensor data, and
Based on the difference of described correct sensing data and described original sensor data, calculate described correction parameter.
21. computer-readable mediums according to claim 20, wherein:
Described sensor is accelerometer, and
In order to calculate described correction parameter, described instruction makes described one or more processor based on described multiple location Calculation average acceleration.
22. computer-readable mediums according to claim 20, wherein:
Described sensor is gyroscope, and
In order to calculate described correction parameter, described instruction makes described one or more processor calculate the numerical derivative of the time correlation rotation matrix be associated with described multiple orientation.
23. computer-readable mediums according to claim 20, wherein in order to determine the described locating and orienting of described portable set, described instruction makes described one or more processor application 3D reconstruct.
24. computer-readable mediums according to claim 19, wherein said portable set comprises at least one change relative to described references object in position and direction relative to the described movement of described references object.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/CN2013/078296 WO2014205757A1 (en) | 2013-06-28 | 2013-06-28 | Systems and methods for generating accurate sensor corrections based on video input |
Publications (2)
Publication Number | Publication Date |
---|---|
CN105103089A true CN105103089A (en) | 2015-11-25 |
CN105103089B CN105103089B (en) | 2021-11-09 |
Family
ID=52115222
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201380074783.1A Active CN105103089B (en) | 2013-06-28 | 2013-06-28 | System and method for generating accurate sensor corrections based on video input |
Country Status (3)
Country | Link |
---|---|
US (1) | US20150002663A1 (en) |
CN (1) | CN105103089B (en) |
WO (1) | WO2014205757A1 (en) |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2018214778A1 (en) * | 2017-05-25 | 2018-11-29 | 阿里巴巴集团控股有限公司 | Method and device for presenting virtual object |
CN109643125A (en) * | 2016-06-28 | 2019-04-16 | 柯尼亚塔有限公司 | For training the 3D virtual world true to nature of automated driving system to create and simulation |
CN110633576A (en) * | 2018-06-22 | 2019-12-31 | 顶级公司 | Data processing |
CN111885296A (en) * | 2020-06-16 | 2020-11-03 | 联想企业解决方案(新加坡)有限公司 | Dynamic processing method of visual data and electronic equipment |
CN113228070A (en) * | 2018-10-22 | 2021-08-06 | 罗伯特·博世有限公司 | Method and system for automatic repeat step and loop detection for manual assembly line operations |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160284122A1 (en) * | 2015-03-26 | 2016-09-29 | Intel Corporation | 3d model recognition apparatus and method |
US10220172B2 (en) | 2015-11-25 | 2019-03-05 | Resmed Limited | Methods and systems for providing interface components for respiratory therapy |
US10732989B2 (en) * | 2017-02-09 | 2020-08-04 | Yanir NULMAN | Method for managing data, imaging, and information computing in smart devices |
JP2018185182A (en) * | 2017-04-25 | 2018-11-22 | 東京電力ホールディングス株式会社 | Position specifying device |
US20200331061A1 (en) * | 2017-11-10 | 2020-10-22 | General Electric Company | Positioning system for an additive manufacturing machine |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090326850A1 (en) * | 2008-06-30 | 2009-12-31 | Nintendo Co., Ltd. | Coordinate calculation apparatus and storage medium having coordinate calculation program stored therein |
CN101795361A (en) * | 2009-01-30 | 2010-08-04 | 索尼公司 | Two-dimensional polynomial model for depth estimation based on two-picture matching |
CN102084398A (en) * | 2008-06-25 | 2011-06-01 | 微软公司 | Registration of street-level imagery to 3D building models |
CN102104767A (en) * | 2009-10-16 | 2011-06-22 | 苹果公司 | Facial pose improvement with perspective distortion correction |
US20110149094A1 (en) * | 2009-12-22 | 2011-06-23 | Apple Inc. | Image capture device having tilt and/or perspective correction |
US20110178708A1 (en) * | 2010-01-18 | 2011-07-21 | Qualcomm Incorporated | Using object to align and calibrate inertial navigation system |
CN102472609A (en) * | 2009-07-28 | 2012-05-23 | 佳能株式会社 | Position and orientation calibration method and apparatus |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6826477B2 (en) * | 2001-04-23 | 2004-11-30 | Ecole Polytechnique Federale De Lausanne (Epfl) | Pedestrian navigation method and apparatus operative in a dead reckoning mode |
JP4004899B2 (en) * | 2002-09-02 | 2007-11-07 | ファナック株式会社 | Article position / orientation detection apparatus and article removal apparatus |
US7251493B2 (en) * | 2004-02-13 | 2007-07-31 | Sony Ericsson Mobile Communications Ab | Mobile terminals and methods for determining a location based on acceleration information |
US7800652B2 (en) * | 2007-12-12 | 2010-09-21 | Cyberlink Corp. | Reducing video shaking |
CN101246023A (en) * | 2008-03-21 | 2008-08-20 | 哈尔滨工程大学 | Closed-loop calibration method of micro-mechanical gyroscope inertial measuring component |
JP5328252B2 (en) * | 2008-07-30 | 2013-10-30 | アルパイン株式会社 | Position detection apparatus and position detection method for navigation system |
US20100157061A1 (en) * | 2008-12-24 | 2010-06-24 | Igor Katsman | Device and method for handheld device based vehicle monitoring and driver assistance |
US9106879B2 (en) * | 2011-10-04 | 2015-08-11 | Samsung Electronics Co., Ltd. | Apparatus and method for automatic white balance with supplementary sensors |
US10068157B2 (en) * | 2012-05-10 | 2018-09-04 | Apple Inc. | Automatic detection of noteworthy locations |
-
2013
- 2013-06-28 CN CN201380074783.1A patent/CN105103089B/en active Active
- 2013-06-28 WO PCT/CN2013/078296 patent/WO2014205757A1/en active Application Filing
-
2014
- 2014-04-10 US US14/250,193 patent/US20150002663A1/en not_active Abandoned
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102084398A (en) * | 2008-06-25 | 2011-06-01 | 微软公司 | Registration of street-level imagery to 3D building models |
US20090326850A1 (en) * | 2008-06-30 | 2009-12-31 | Nintendo Co., Ltd. | Coordinate calculation apparatus and storage medium having coordinate calculation program stored therein |
CN101795361A (en) * | 2009-01-30 | 2010-08-04 | 索尼公司 | Two-dimensional polynomial model for depth estimation based on two-picture matching |
CN102472609A (en) * | 2009-07-28 | 2012-05-23 | 佳能株式会社 | Position and orientation calibration method and apparatus |
CN102104767A (en) * | 2009-10-16 | 2011-06-22 | 苹果公司 | Facial pose improvement with perspective distortion correction |
US20110149094A1 (en) * | 2009-12-22 | 2011-06-23 | Apple Inc. | Image capture device having tilt and/or perspective correction |
US20110178708A1 (en) * | 2010-01-18 | 2011-07-21 | Qualcomm Incorporated | Using object to align and calibrate inertial navigation system |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109643125A (en) * | 2016-06-28 | 2019-04-16 | 柯尼亚塔有限公司 | For training the 3D virtual world true to nature of automated driving system to create and simulation |
US11417057B2 (en) | 2016-06-28 | 2022-08-16 | Cognata Ltd. | Realistic 3D virtual world creation and simulation for training automated driving systems |
WO2018214778A1 (en) * | 2017-05-25 | 2018-11-29 | 阿里巴巴集团控股有限公司 | Method and device for presenting virtual object |
CN108958462A (en) * | 2017-05-25 | 2018-12-07 | 阿里巴巴集团控股有限公司 | A kind of methods of exhibiting and device of virtual objects |
CN110633576A (en) * | 2018-06-22 | 2019-12-31 | 顶级公司 | Data processing |
CN113228070A (en) * | 2018-10-22 | 2021-08-06 | 罗伯特·博世有限公司 | Method and system for automatic repeat step and loop detection for manual assembly line operations |
CN111885296A (en) * | 2020-06-16 | 2020-11-03 | 联想企业解决方案(新加坡)有限公司 | Dynamic processing method of visual data and electronic equipment |
CN111885296B (en) * | 2020-06-16 | 2023-06-16 | 联想企业解决方案(新加坡)有限公司 | Dynamic processing method of visual data and electronic equipment |
Also Published As
Publication number | Publication date |
---|---|
US20150002663A1 (en) | 2015-01-01 |
WO2014205757A1 (en) | 2014-12-31 |
CN105103089B (en) | 2021-11-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN105103089A (en) | Systems and methods for generating accurate sensor corrections based on video input | |
US9243916B2 (en) | Observability-constrained vision-aided inertial navigation | |
Jin et al. | FPGA design and implementation of a real-time stereo vision system | |
US8437501B1 (en) | Using image and laser constraints to obtain consistent and improved pose estimates in vehicle pose databases | |
Wendel et al. | Natural landmark-based monocular localization for MAVs | |
US20140198976A1 (en) | Method and system for fast dense stereoscopic ranging | |
Panek et al. | Meshloc: Mesh-based visual localization | |
KR20200075727A (en) | Method and apparatus for calculating depth map | |
US11069071B1 (en) | System and method for egomotion estimation | |
Cheng et al. | Extracting three-dimensional (3D) spatial information from sequential oblique unmanned aerial system (UAS) imagery for digital surface modeling | |
Kondermann et al. | Stereo ground truth with error bars | |
AliAkbarpour et al. | Parallax-tolerant aerial image georegistration and efficient camera pose refinement—without piecewise homographies | |
Liu et al. | Graphcspn: Geometry-aware depth completion via dynamic gcns | |
Guizilini et al. | Semi-parametric learning for visual odometry | |
Michelini et al. | Structure from motion for complex image sets | |
Bao et al. | Robust tightly-coupled visual-inertial odometry with pre-built maps in high latency situations | |
Mayer | Efficient hierarchical triplet merging for camera pose estimation | |
Pirvu et al. | Depth distillation: unsupervised metric depth estimation for UAVs by finding consensus between kinematics, optical flow and deep learning | |
EP4163873A1 (en) | Method and apparatus with global localization | |
CN117132649A (en) | Ship video positioning method and device for artificial intelligent Beidou satellite navigation fusion | |
Zhang et al. | Data association between event streams and intensity frames under diverse baselines | |
Ruano et al. | Aerial video georegistration using terrain models from dense and coherent stereo matching | |
Ramachandran et al. | A fast bilinear structure from motion algorithm using a video sequence and inertial sensors | |
Loquercio et al. | Learning depth with very sparse supervision | |
CN114863201A (en) | Training method and device of three-dimensional detection model, computer equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyApplicant after: Pan WeibinApplicant after: Hu LiangAddress before: American CaliforniaApplicant before: Google Inc.Applicant before: Pan WeibinApplicant before: Hu Liang |
|
CB02 | Change of applicant information | ||
GR01 | Patent grant | ||
GR01 | Patent grant |