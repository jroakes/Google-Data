US9563683B2 - Efficient data replication - Google Patents
Efficient data replication Download PDFInfo
- Publication number
- US9563683B2 US9563683B2 US14/120,340 US201414120340A US9563683B2 US 9563683 B2 US9563683 B2 US 9563683B2 US 201414120340 A US201414120340 A US 201414120340A US 9563683 B2 US9563683 B2 US 9563683B2
- Authority
- US
- United States
- Prior art keywords
- data
- source
- remote server
- storage
- pool
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 230000010076 replication Effects 0.000 title claims abstract description 72
- 238000000034 method Methods 0.000 claims abstract description 151
- 230000002123 temporal effect Effects 0.000 claims description 71
- 230000003362 replicative effect Effects 0.000 claims description 11
- 238000003860 storage Methods 0.000 description 308
- 238000013523 data management Methods 0.000 description 73
- 230000008569 process Effects 0.000 description 45
- 238000013500 data storage Methods 0.000 description 40
- 230000006870 function Effects 0.000 description 34
- 230000000875 corresponding effect Effects 0.000 description 33
- 238000007726 management method Methods 0.000 description 32
- 238000010586 diagram Methods 0.000 description 26
- 230000002085 persistent effect Effects 0.000 description 22
- 230000008859 change Effects 0.000 description 19
- 238000004422 calculation algorithm Methods 0.000 description 17
- 230000014759 maintenance of location Effects 0.000 description 16
- 238000011084 recovery Methods 0.000 description 16
- 230000007246 mechanism Effects 0.000 description 15
- 238000012546 transfer Methods 0.000 description 13
- 238000013507 mapping Methods 0.000 description 11
- 230000000717 retained effect Effects 0.000 description 10
- 238000004891 communication Methods 0.000 description 9
- 230000004048 modification Effects 0.000 description 9
- 238000012986 modification Methods 0.000 description 9
- 238000004364 calculation method Methods 0.000 description 8
- 238000004590 computer program Methods 0.000 description 8
- 230000000694 effects Effects 0.000 description 8
- 230000007774 longterm Effects 0.000 description 8
- 230000008901 benefit Effects 0.000 description 7
- 239000000470 constituent Substances 0.000 description 7
- 238000010276 construction Methods 0.000 description 7
- 238000005516 engineering process Methods 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 7
- 238000007906 compression Methods 0.000 description 6
- 239000000835 fiber Substances 0.000 description 6
- 230000003993 interaction Effects 0.000 description 6
- 230000009471 action Effects 0.000 description 5
- 230000006835 compression Effects 0.000 description 5
- 238000012217 deletion Methods 0.000 description 5
- 230000037430 deletion Effects 0.000 description 5
- 238000010899 nucleation Methods 0.000 description 5
- 239000002131 composite material Substances 0.000 description 4
- 238000011161 development Methods 0.000 description 4
- 238000012423 maintenance Methods 0.000 description 4
- 238000012545 processing Methods 0.000 description 4
- 230000004044 response Effects 0.000 description 4
- 238000012360 testing method Methods 0.000 description 4
- 230000003442 weekly effect Effects 0.000 description 4
- 230000004913 activation Effects 0.000 description 3
- 230000009286 beneficial effect Effects 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 3
- 230000001427 coherent effect Effects 0.000 description 3
- 230000002596 correlated effect Effects 0.000 description 3
- 230000006378 damage Effects 0.000 description 3
- 238000013499 data model Methods 0.000 description 3
- 238000013461 design Methods 0.000 description 3
- 238000011010 flushing procedure Methods 0.000 description 3
- 230000037406 food intake Effects 0.000 description 3
- 238000005457 optimization Methods 0.000 description 3
- 238000005192 partition Methods 0.000 description 3
- 230000001360 synchronised effect Effects 0.000 description 3
- 230000009466 transformation Effects 0.000 description 3
- 238000000844 transformation Methods 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 238000003491 array Methods 0.000 description 2
- 230000001276 controlling effect Effects 0.000 description 2
- 238000013506 data mapping Methods 0.000 description 2
- 230000007423 decrease Effects 0.000 description 2
- 230000008014 freezing Effects 0.000 description 2
- 238000007710 freezing Methods 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 238000009434 installation Methods 0.000 description 2
- 230000000670 limiting effect Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 238000002360 preparation method Methods 0.000 description 2
- 238000012552 review Methods 0.000 description 2
- 230000007958 sleep Effects 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 238000013519 translation Methods 0.000 description 2
- 230000003466 anti-cipated effect Effects 0.000 description 1
- 230000001174 ascending effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000002457 bidirectional effect Effects 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 239000000969 carrier Substances 0.000 description 1
- 239000003054 catalyst Substances 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 230000000295 complement effect Effects 0.000 description 1
- 230000001010 compromised effect Effects 0.000 description 1
- 230000001186 cumulative effect Effects 0.000 description 1
- 238000013144 data compression Methods 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000003090 exacerbative effect Effects 0.000 description 1
- 239000004744 fabric Substances 0.000 description 1
- 238000009396 hybridization Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- WCJNRJDOHCANAL-UHFFFAOYSA-N n-(4-chloro-2-methylphenyl)-4,5-dihydro-1h-imidazol-2-amine Chemical compound CC1=CC(Cl)=CC=C1NC1=NCCN1 WCJNRJDOHCANAL-UHFFFAOYSA-N 0.000 description 1
- 238000013439 planning Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000002829 reductive effect Effects 0.000 description 1
- 238000013468 resource allocation Methods 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 238000013515 script Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 230000007480 spreading Effects 0.000 description 1
- 238000003892 spreading Methods 0.000 description 1
- 238000003786 synthesis reaction Methods 0.000 description 1
- 238000010257 thawing Methods 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
- 238000009827 uniform distribution Methods 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 230000002747 voluntary effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/27—Replication, distribution or synchronisation of data between databases or within a distributed database system; Distributed database system architectures therefor
-
- G06F17/30575—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/0223—User address space allocation, e.g. contiguous or non contiguous base addressing
- G06F12/023—Free address space management
- G06F12/0253—Garbage collection, i.e. reclamation of unreferenced memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/07—Responding to the occurrence of a fault, e.g. fault tolerance
- G06F11/14—Error detection or correction of the data by redundancy in operation
- G06F11/1402—Saving, restoring, recovering or retrying
- G06F11/1446—Point-in-time backing up or restoration of persistent data
- G06F11/1448—Management of the data involved in backup or backup restore
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/10—File systems; File servers
- G06F16/13—File access structures, e.g. distributed indices
- G06F16/137—Hash-based
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/10—File systems; File servers
- G06F16/17—Details of further file system functions
- G06F16/1727—Details of free space management performed by the file system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/10—File systems; File servers
- G06F16/17—Details of further file system functions
- G06F16/174—Redundancy elimination performed by the file system
- G06F16/1748—De-duplication implemented within the file system, e.g. based on file segments
- G06F16/1752—De-duplication implemented within the file system, e.g. based on file segments based on file chunks
-
- G06F17/30097—
-
- G06F17/30138—
-
- G06F17/30159—
Definitions
- This invention relates generally to data management, data protection, disaster recovery and business continuity. More specifically, this invention relates to a system and method for efficient data replication.
- FIG. 1 shows a typical set of data management operations that would be applied to the data of an application such as a database underlying a business service such as payroll management.
- application 102 requires primary data storage 122 with some contracted level of reliability and availability.
- Backups 104 are made to guard against corruption or the primary data storage through hardware or software failure or human error. Typically backups may be made daily or weekly to local disk or tape 124 , and moved less frequently (weekly or monthly) to a remote physically secure location 125 .
- Compliance with legal or voluntary policies 108 may require that some data be retained for safely future access for some number of years; usually data is copied regularly (say, monthly) to a long-term archiving system 128 .
- Disaster Recovery services 110 guard against catastrophic loss of data if systems providing primary business services fail due to some physical disaster.
- Primary data is copied 130 to a physically distinct location as frequently as is feasible given other constraints (such as cost). In the event of a disaster the primary site can be reconstructed and data moved back from the safe copy.
- Business Continuity services 112 provide a facility for ensuring continued business services should the primary site become compromised. Usually this requires a hot copy 132 of the primary data that is in near-lockstep with the primary data, as well as duplicate systems and applications and mechanisms for switching incoming requests to the Business Continuity servers.
- any near neighbor object no matter the degree of difference between it and the object being replicated, can increase the replication efficiency to a degree proportional to the percentage of its constituent data chunks that are already present on the remote system, as lookups on these data hashes in the index will not be required.
- each Depth-1 hash might represent 2 MiB of data as 512 Depth-0 hashes, each of which represents 4 KiB of data. Given this example, a successful lookup of a Depth-1 hash would reduce the number of index lookup operations required by 511.
- Also described herein are systems and methods to estimate the potential payoff for execution of a garbage collection process e.g., a mark/sweep process
- a garbage collection process e.g., a mark/sweep process
- This enables decisions to be made by executive processes or by end-users as to whether it is advisable to invest time into a garbage collection mark and sweep process.
- Characteristics of the deduplicating store that provide for this can include the temporal tree of objects, the differencing, and/or the sparse copy-in process.
- systems, methods, and non-transitory computer-readable media are provided for using synthetic near neighbors for replication and for history tree-based statistics for predicting garbage collection effectiveness.
- the disclosed subject matter includes a method for creating a data object for replication.
- a computing device creates an empty data object for replication on both a source data store and a target data store.
- the computing device determines a set of hash values for the source data object to be replicated on the source data store.
- the computing device transmits the set of hash values to the remote data store.
- the computing device receives response data from the remote data store, the response data comprising first data indicative of which hash values from the set of hash values are present at the remote data store, and second data indicative of which hash values from the set of hash values are not present at the remote data store.
- the computing device generates a complete data object for replication based on the first data, the second data, and the empty data object for replication.
- the disclosed subject matter includes a method for predicting an amount of storage that can be reclaimed from a storage device.
- the computing device calculates an accumulated difference of differences between a base object and one or more newer generation objects of the base object stored on a storage device.
- the computing device calculates a divested difference based on one or more of the newer generation objects being removed from the storage device.
- the computing device calculates an amount of storage that can be reclaimed from the storage device based on the accumulated difference and the divested difference.
- the techniques described herein provide for a computerized method of replicating an object from a source local deduplication store to a remote server based on data already stored on the remote server to reduce the amount of duplicate data sent from the local deduplication store to the remote server.
- the method includes sending, by the source local deduplication store to the remote server, a set of hashes for a source object to be replicated from the source local deduplication store to the remote server.
- the method includes receiving, by the source local deduplication store, data from the remote server including: a set of object hashes representative of data in the source object that is already present on the remote server, and data indicative of source object hashes that are not present on the remote server.
- the method include identifying, by the source local deduplication store, portions of the source object that are not already present on the remote server based on the received data.
- the method includes transmitting, by the source local deduplication store, the identified portions of the source object to the remote server to replicate the source object on the remote server so that only the portions of the source object not already present on the remote server are copied to the remote server to replicate the source object.
- the techniques described herein provide for a computerized method of replicating an object from a source local deduplication store to a remote server based on data already stored on the remote server to reduce the amount of duplicate data sent from the local deduplication store to the remote server.
- the method includes receiving, by the remote server, a set of hashes for a source object to be replicated from the source local deduplication store to the remote server.
- the method includes generating, by the remote server, data including: a set of object hashes representative of data in the source object that is already present on the remote server, and data indicative of source object hashes that are not present on the remote server.
- the method includes sending, by the remoter server, the generated data to the source local deduplication store so that the source local deduplication store can identify data from the source object that is already present on the remote server so that only remaining data of the source object not already present on the remote server is copied to the remote server to replicate the source object.
- the techniques described herein provide for a computerized method of replicating an object from a source local deduplication store to a remote server based on data already stored on the remote server to reduce the amount of duplicate data sent from the local deduplication store to the remote server.
- the method includes sending, by the source local deduplication store to the remote server, a set of hashes for a source object to be replicated from the source local deduplication store to the remote server.
- the method includes generating, by the remote server, data including: a set of object hashes representative of data in the source object that is already present on the remote server, and data indicative of source object hashes that are not present on the remote server.
- the method includes transmitting, by the remote server, the generated data to the source local deduplication store.
- the method includes identifying, by the source local deduplication store, portions of the source object that are not already present on the remote server based on the received data.
- the method includes transmitting, by the source local deduplication store, the identified portions of the source object to the remote server to replicate the source object on the remote server so that only the portions of the source object not already present on the remote server are copied to the remote server to replicate the source object.
- the method includes assembling, by the remote server, a remote object, the remote object comprising the set of object hashes representative of data in the source object that is already present on the remote server and the identified portions, wherein the remote object is identical to the source object.
- the techniques described herein provide for a computerized method of maintaining running information of ingestion and deletion of file system data for a deduplicated data store to predict the outcome of garbage collection operation on the deduplicated data store without performing the garbage collection operation.
- the method includes maintaining, by a computer device, a temporal graph, the temporal graph including nodes, the nodes including hash references to objects.
- the method includes updating, by the computer device, an accumulated difference count when a node is added to the temporal graph, the accumulated difference count including a number of hash differences between a parent node and its children nodes in the temporal graph.
- the method includes updating, by the computer device, a divested difference count when a node is removed from the temporal graph, the divested difference count including a number of hash differences referenced by the removed node but not by either a parent node of the removed node or any child nodes of the removed node.
- the method includes predicting, by the computer device, the outcome of the garbage collection based on at least one of the accumulated difference count and the divested difference count.
- the techniques described herein provide for a computerized system for maintaining running information of ingestion and deletion of file system data for a deduplicated data store to predict the outcome of garbage collection operation on the deduplicated data store without performing the garbage collection operation, comprising a processor configured to run a module stored in memory that is configured to cause the processor to maintain a temporal graph, the temporal graph including nodes, the nodes including hash references to objects.
- the module stored in memory is configured to cause the processor to update an accumulated difference count when a node is added to the temporal graph, the accumulated difference count including a number of hash differences between a parent node and its children nodes in the temporal graph.
- the module stored in memory is configured to cause the processor to update a divested difference count when a node is removed from the temporal graph, the divested difference count including a number of hash differences referenced by the removed node but not by either a parent node of the removed node or any child nodes of the removed node.
- the module stored in memory is configured to cause the processor to predict the outcome of the garbage collection based on at least one of the accumulated difference count and the divested difference count.
- the techniques described herein provide for a non-transitory computer readable medium having executable instructions operable to cause an apparatus to maintain a temporal graph, the temporal graph including nodes, the nodes including hash references to objects.
- the executable instructions are operable to cause the apparatus to update an accumulated difference count when a node is added to the temporal graph, the accumulated difference count including a number of hash differences between a parent node and its children nodes in the temporal graph.
- the executable instructions are operable to cause the apparatus to update a divested difference count when a node is removed from the temporal graph, the divested difference count including a number of hash differences referenced by the removed node but not by either a parent node of the removed node or any child nodes of the removed node.
- the executable instructions are operable to cause the apparatus to predict the outcome of the garbage collection based on at least one of the accumulated difference count and the divested difference count.
- FIG. 1 is a simplified diagram of current methods deployed to manage the data lifecycle for a business service.
- FIG. 2 is an overview of the management of data throughout its lifecycle by a single Data Management Virtualization System.
- FIG. 3 is a simplified block diagram of the Data Management Virtualization system.
- FIG. 4 is a view of the Data Management Virtualization Engine.
- FIG. 5 illustrates the Object Management and Data Movement Engine.
- FIG. 6 shows the Storage Pool Manager.
- FIG. 7 shows the decomposition of the Service Level Agreement.
- FIG. 8 illustrates the Application Specific Module.
- FIG. 9 shows the Service Policy Manager.
- FIG. 10 is a flowchart of the Service Policy Scheduler.
- FIG. 11 is a block diagram of the Content Addressable Storage (CAS) provider.
- CAS Content Addressable Storage
- FIG. 12 shows the definition of an object handle within the CAS system.
- FIG. 13 shows the data model and operations for the temporal relationship graph stored for objects within the CAS.
- FIG. 14 is a diagram representing the operation of a garbage collection algorithm in the CAS.
- FIG. 15 is a flowchart for the operation of copying an object into the CAS.
- FIG. 16 is a system diagram of a typical deployment of the Data Management Virtualization system.
- FIG. 17 is a schematic diagram of a characteristic physical server device for use with the Data Management Virtualization system.
- FIG. 18 is a schematic diagram showing the data model for a data fingerprint to be used in conjunction with certain embodiments of the invention.
- FIG. 19 is a system architecture diagram of a deployment of the Data Management Virtualization system that incorporates data fingerprinting.
- FIG. 20 is a process diagram for the operation of copying an object using a hybrid seeding algorithm.
- FIG. 21 is a process diagram for the operation of a Data Management Virtualization system that provides replication for business continuity.
- FIG. 22 is an exemplary table defining new statistics that can be calculated, according to some embodiments.
- FIG. 23A illustrates an exemplary diagram of a temporal tree structure, according to some embodiments.
- FIG. 23B is an exemplary diagram of the logical accumulated differences statistic, according to some embodiments.
- FIG. 24 is an exemplary diagram of calculating logical accumulated differences statistic, according to some embodiments.
- FIG. 25A is an exemplary diagram of removing an object from a temporal tree structure, according to some embodiments.
- FIG. 25B is an exemplary diagram of a logical divested differences statistic, according to some embodiments.
- FIG. 26A is an exemplary diagram of a physical space used statistic, according to some embodiments.
- FIG. 26B is an exemplary diagram of a tree structure used to calculate the physical space used statistic in FIG. 26A , according to some embodiments.
- FIG. 27 illustrates a method for creating a near neighbor object for replication synthetically where no existing near neighbor can otherwise be determined.
- FIG. 28 illustrates a method for creating a synthetic near neighbor for replication synthetically where no existing near neighbor can otherwise be determined.
- FIG. 29 illustrates a method for performing a full restore phase for replication synthetically where no existing near neighbor can otherwise be determined.
- This disclosure pertains to generating a data fingerprint for an object stored in a virtual storage pool that may be used to compare two objects over the life of those data objects.
- This disclosure also pertains to a method for improved incremental copy performance using hybrid seeding to perform copies and differencing operations using different virtual storage pools.
- This disclosure also pertains to a mechanism for data replication for disaster recovery and business continuity using a pipeline of storage pools.
- a user defines business requirements with regard to the lifecycle of the data, and the Data Management Virtualization System performs these operations automatically.
- a snapshot is taken from primary storage to secondary storage; this snapshot is then used for a backup operation to other secondary storage. Essentially an arbitrary number of these backups may be made, providing a level of data protection specified by a Service Level Agreement.
- the data management engine is operable to execute a sequence of snapshot operations to create point-in-time images of application data on a first storage pool, each successive point-in-time image corresponding to a specific, successive time-state of the application data, and each snapshot operation creating difference information indicating which application data has changed and the content of the changed application data for the corresponding time state.
- the data management engine is also operable to execute at least one back-up function for the application data that is scheduled for execution at non-consecutive time-states, and is also full of maintain history information having time-state information indicating the time-state of the last back-up function performed on the application data for a corresponding back-up copy of data.
- the data management engine creates composite difference information from the difference information for each time-state between the time-state of the last back-up function performed on the application data and the time-state of the currently-scheduled back-up function to be performed on the application data, and sends the composite difference information to a second storage pool to be compiled with the back-up copy of data at the last time-state to create a back-up copy of data for the current time-state.
- Data Management Virtualization technology is based on an architecture and implementation based on the following guiding principles.
- SLA Service Level Agreement
- RTO Retention and Recovery Time Objective
- the Data Management Virtualization system achieves these improvements by leveraging extended capabilities of modern storage systems by tracking the portions of the data that have changed over time and by data deduplication and compression algorithms that reduce the amount of data that needs to be copied and moved.
- the Data Management Virtualization System leverages a single copy of the data for multiple purposes. A single instance of the data maintained by the system may serve as the source, from which each data management function may make additional copies as needed. This contrasts with requiring application data to be copied multiple times by multiple independent data management applications in the traditional approach.
- the Data Management Virtualization system allows the user to classify and aggregate different storage media into storage pools, for example, a Quick Recovery Pool, which may include high speed disks, and a Cost Efficient Long-term Storage Pool, which may be a deduplicated store on high capacity disks, or a tape library.
- the Data Management Virtualization System can move data amongst these pools to take advantage of the unique characteristics of each storage medium.
- the abstraction of Storage Pools provides access independent of the type, physical location or underlying storage technology.
- the Data Management Virtualization System discovers the capabilities of the storage systems that include the Storage Pools, and takes advantage of these capabilities to move data efficiently. If the Storage System is a disk array that supports the capability of creating a snapshot or clone of a data volume, the Data Management Virtualization System will take advantage of this capability and use a snapshot to make a copy of the data rather than reading the data from one place and writing it to another. Similarly, if a storage system supports change tracking, the Data Management Virtualization System will update an older copy with just the changes to efficiently create a new copy. When moving data across a network, the Data Management Virtualization system uses a deduplication and compression algorithm that avoids sending data that is already available on the other side of the network.
- a copy of an application that is made today will, in general, have a lot of similarities to the copy of the same application that was made yesterday.
- today's copy of the data could be represented as yesterday's copy with a series of delta transformations, where the size of the delta transformations themselves are usually much smaller than all of the data in the copy itself.
- the Data Management Virtualization system captures and records these transformations in the form of bitmaps or extent lists.
- the underlying storage resources a disk array or server virtualization system—are capable of tracking the changes made to a volume or file; in these environments, the Data Management Virtualization system queries the storage resources to obtain these change lists, and saves them with the data being protected.
- the Data Management Virtualization system there is a mechanism for eavesdropping on the primary data access path of the application, which enables the Data Management Virtualization system to observe which parts of the application data are modified, and to generate its own bitmap of modified data. If, for example, the application modifies blocks 100 , 200 and 300 during a particular period, the Data Management Virtualization system will eavesdrop on these events, and create a bitmap that indicates that these particular blocks were modified. When processing the next copy of application data, the Data Management Virtualization system will only process blocks 100 , 200 and 300 since it knows that these were the only blocks that were modified.
- the Data Management Virtualization system takes advantage of a point-in-time snapshot capability of an underlying storage device to make the initial copy of the data.
- This virtual copy mechanism is a fast, efficient and low-impact technique of creating the initial copy that does not guarantee that all the bits will be copied, or stored together.
- virtual copies are constructed by maintaining metadata and data structures, such as copy-on-write volume bitmaps or extents, that allow the copies to be reconstructed at access time. The copy has a lightweight impact on the application and on the primary storage device.
- the Data Management Virtualization system uses the similar virtual-machine-snapshot capability that is built into the Server Virtualization systems.
- the Data Management Virtualization System may include its own built-in snapshot mechanism.
- the snapshot is possible to use as a data primitive underlying all of the data management functions supported by the system. Because it is lightweight, the snapshot can be used as an internal operation even when the requested operation is not a snapshot per se; it is created to enable and facilitate other operations.
- the preparatory operations may include application quiescence, which includes flushing data caches and freezing the state of the application; it may also include other operations known in the art and other operations useful for retaining a complete image, such as collecting metadata information from the application to be stored with the image.
- FIG. 2 illustrates one way that a Virtualized Data Management system can address the data lifecycle requirements described earlier in accordance with these principles.
- a sequence of efficient snapshots are made within local high-availability storage 202 . Some of these snapshots are used to serve development/test requirements without making another copy.
- a copy is made efficiently into long-term local storage 204 , which in this implementation uses deduplication to reduce repeated copying.
- the copies within long-term storage may be accessed as backups or treated as an archive, depending on the retention policy applied by the SLA.
- a copy of the data is made to remote storage 206 in order to satisfy requirements for remote backup and business continuity—again a single set of copies suffices both purposes.
- a further copy of the data may be made efficiently to a repository 208 hosted by a commercial or private cloud storage provider.
- FIG. 3 illustrates the high level components of the Data Management Virtualization System that implements the above principles.
- the system includes these basic functional components further described below.
- Application 300 creates and owns the data. This is the software system that has been deployed by the user, as for example, an email system, a database system, or financial reporting system, in order to satisfy some computational need.
- the Application typically runs on a server and utilizes storage. For illustrative purposes, only one application has been indicated. In reality there may be hundreds or even thousands of applications that are managed by a single Data Management Virtualization System.
- Storage Resources 302 is where application data is stored through its lifecycle.
- the Storage Resources are the physical storage assets, including internal disk drives, disk arrays, optical and tape storage libraries and cloud-based storage systems that the user has acquired to address data storage requirements.
- the storage resources include Primary Storage 310 , where the online, active copy of the application data is stored, and Secondary Storage 312 where additional copies of the application data are stored for the purposes such as backup, disaster recovery, archiving, indexing, reporting and other uses.
- Secondary storage resources may include additional storage within the same enclosure as the primary storage, as well as storage based on similar or different storage technologies within the same data center, another location or across the internet.
- One or more Management Workstations 308 allow the user to specify a Service Level Agreement (SLA) 304 that defines the lifecycle for the application data.
- SLA Service Level Agreement
- a Management workstation is a desktop or laptop computer or a mobile computing device that is used to configure, monitor and control the Data Management Virtualization System.
- a Service Level Agreement is a detailed specification that captures the detailed business requirements related to the creation, retention and deletion of secondary copies of the application data.
- the SLA is more than the simple RTO and RPO that are used in traditional data management applications to represent the frequency of copies and the anticipated restore time for a single class of secondary storage.
- the SLA captures the multiple stages in the data lifecycle specification, and allows for non-uniform frequency and retention specifications within each class of secondary storage. The SLA is described in greater detail in FIG. 7 .
- Data Management Virtualization Engine 306 manages all of the lifecycle of the application data as specified in SLA. It manages potentially a large number of SLAs for a large number of applications.
- the Data Management Virtualization Engine takes inputs from the user through the Management Workstation and interacts with the applications to discover the applications primary storage resources.
- the Data Management Virtualization Engine makes decisions regarding what data needs to be protected and what secondary storage resources best fulfill the protection needs. For example, if an enterprise designates its accounting data as requiring copies to be made at very short intervals for business continuity purposes as well as for backup purposes, the Engine may decide to create copies of the accounting data at a short interval to a first storage pool, and to also create backup copies of the accounting data to a second storage pool at a longer interval, according to an appropriate set of SLAs. This is determined by the business requirements of the storage application.
- the Engine then makes copies of application data using advanced capabilities of the storage resources as available.
- the Engine may schedule the short-interval business continuity copy using a storage appliance's built-in virtual copy or snapshot capabilities.
- the Data Management Virtualization Engine moves the application data amongst the storage resources in order to satisfy the business requirements that are captured in the SLA.
- the Data Management Virtualization Engine is described in greater detail in FIG. 4 .
- the Data Management Virtualization System as a whole may be deployed within a single host computer system or appliance, or it may be one logical entity but physically distributed across a network of general-purpose and purpose-built systems. Certain components of the system may also be deployed within a computing or storage cloud.
- the Data Management Virtualization Engine largely runs as multiple processes on a fault tolerant, redundant pair of computers. Certain components of the Data Management Virtualization Engine may run close to the application within the application servers. Some other components may run close to the primary and secondary storage, within the storage fabric or in the storage systems themselves.
- the Management stations are typically desktop and laptop computers and mobile devices that connect over a secure network to the Engine.
- FIG. 4 illustrates an architectural overview of the Data Management Virtualization Engine 306 according to certain embodiments of the invention.
- the 306 Engine includes the following modules:
- Application Specific Module 402 This module is responsible for controlling and collecting metadata from the application 300 .
- Application metadata includes information about the application such as the type of application, details about its configuration, location of its datastores, its current operating state. Controlling the operation of the application includes actions such as flushing cached data to disk, freezing and thawing application I/O, rotating or truncating log files, and shutting down and restarting applications.
- the Application Specific module performs these operations and sends and receives metadata in responses to commands from the Service Level Policy Engine 406 , described below.
- the Application Specific Module is described in more detail in connection with FIG. 8 .
- Service Level Policy Engine 406 This module acts on the SLA 304 provided by the user to make decisions regarding the creation, movement and deletion of copies of the application data.
- Each SLA describes the business requirements related to protection of one application.
- the Service Level Policy Engine analyzes each SLA and arrives at a series of actions each of which involve the copying of application data from one storage location to another. The Service Level Policy Engine then reviews these actions to determine priorities and dependencies, and schedules and initiates the data movement jobs.
- the Service Level Policy Engine is described in more detail in connection with FIG. 9 .
- This module creates a composite object consisting of the Application data, the Application Metadata and the SLA which it moves through different storage pools per instruction from the Policy Engine.
- the Object Manager receives instructions from the Service Policy Engine 406 in the form of a command to create a copy of application data in a particular pool based on the live primary data 413 belonging to the application 300 , or from an existing copy, e.g., 415 , in another pool.
- the copy of the composite object that is created by the Object Manager and the Data Movement Engine is self contained and self describing in that it contains not only application data, but also application metadata and the SLA for the application.
- the Object Manager and Data Movement Engine are described in more detail in connection with FIG. 5 .
- Storage Pool Manager 412 This module is a component that adapts and abstracts the underlying physical storage resources 302 and presents them as virtual storage pools 418 .
- the physical storage resources are the actual storage assets, such as disk arrays and tape libraries that the user has deployed for the purpose of supporting the lifecycle of the data of the user's applications. These storage resources might be based on different storage technologies such as disk, tape, flash memory or optical storage. The storage resources may also have different geographic locations, cost and speed attributes, and may support different protocols.
- the role of the Storage Pool Manager is to combine and aggregate the storage resources, and mask the differences between their programming interfaces.
- the Storage Pool Manager presents the physical storage resources to the Object Manager 410 as a set of storage pools that have characteristics that make these pools suitable for particular stages in the lifecycle of application data. The Storage Pool Manager is described in more detail in connection with FIG. 6 .
- FIG. 5 illustrates the Object Manager and Data Movement Engine 410 .
- the Object Manager and Data Movement Engine discovers and uses Virtual Storage Resources 510 presented to it by the Pool Managers 504 . It accepts requests from the Service Level Policy Engine 406 to create and maintain Data Storage Object instances from the resources in a Virtual Storage Pool, and it copies application data among instances of storage objects from the Virtual Storage Pools according to the instructions from the Service Level Policy Engine.
- the target pool selected for the copy implicitly designates the business operation being selected, e.g. backup, replication or restore.
- the Service Level Policy Engine resides either locally to the Object Manager (on the same system) or remotely, and communicates using a protocol over standard networking communication. TCP/IP may be used in a preferred embodiment, as it is well understood, widely available, and allows the Service Level Policy Engine to be located locally to the Object Manager or remotely with little modification.
- the system may deploy the Service Level Policy Engine on the same computer system as the Object Manager for ease of implementation.
- the system may employ multiple systems, each hosting a subset of the components if beneficial or convenient for an application, without changing the design.
- the Object Manager 501 and the Storage Pool Managers 504 are software components that may reside on the computer system platform that interconnects the storage resources and the computer systems that use those storage resources, where the user's application resides.
- the placement of these software components on the interconnect platform is designated as a preferred embodiment, and may provide the ability to connect customer systems to storage via communication protocols widely used for such applications (e.g. Fibre Channel, iSCSI, etc.), and may also provide ease of deployment of the various software components.
- the Object Manager 501 and Storage Pool Manager 504 communicate with the underlying storage virtualization platform via the Application Programming Interfaces made available by the platform. These interfaces allow the software components to query and control the behavior of the computer system and how it interconnects the storage resources and the computer system where the user's Application resides.
- the components apply modularity techniques as is common within the practice to allow replacement of the intercommunication code particular to a given platform.
- the Object Manager and Storage Pool Managers communicate via a protocol. These are transmitted over standard networking protocols, e.g. TCP/IP, or standard Interprocess Communication (IPC) mechanisms typically available on the computer system. This allows comparable communication between the components if they reside on the same computer platform or on multiple computer platforms connected by a network, depending on the particular computer platform.
- TCP/IP standard networking protocols
- IPC Interprocess Communication
- the current configuration has all of the local software components residing on the same computer system for ease of deployment. This is not a strict requirement of the design, as described above, and can be reconfigured in the future as needed.
- Object Manager 501 is a software component for maintaining Data Storage Objects, and provides a set of protocol operations to control it.
- the operations include creation, destruction, duplication, and copying of data among the objects, maintaining access to objects, and in particular allow the specification of the storage pool used to create copies.
- the pools may be remote or local.
- the storage pools are classified according to various criteria, including means by which a user may make a business decision, e.g. cost per gigabyte of storage.
- the particular storage device from which the storage is drawn may be a consideration, as equipment is allocated for different business purposes, along with associated cost and other practical considerations. Some devices may not even be actual hardware but capacity provided as a service, and selection of such a resource can be done for practical business purposes.
- the network topological “proximity” is considered, as near storage is typically connected by low-latency, inexpensive network resources, while distant storage may be connected by high-latency, bandwidth limited expensive network resources; conversely, the distance of a storage pool relative to the source may be beneficial when geographic diversity protects against a physical disaster affecting local resources.
- the amount of storage used and the amount available in a given pool are considered, as there may be benefit to either concentrating or spreading the storage capacity used.
- the Service Level Policy Engine described below, combines the SLA provided by the user with the classification criteria to determine how and when to maintain the application data, and from which storage pools to draw the needed resources to meet the Service Level Agreement (SLA).
- SLA Service Level Agreement
- the object manager 501 creates, maintains and employs a history mechanism to track the series of operations performed on a data object within the performance pools, and to correlate those operations with others that move the object to other storage pools, in particular capacity-optimized ones.
- This series of records for each data object is maintained at the object manager for all data objects in the primary pool, initially correlated by primary data object, then correlated by operation order: a time line for each object and a list of all such time lines.
- Each operation performed exploits underlying virtualization primitives to capture the state of the data object at a given point in time.
- the underlying storage virtualization appliance may be modified to expose and allow retrieval of internal data structures, such as bitmaps, that indicate the modification of portions of the data within the data object.
- data structures are exploited to capture the state of a data object at a point in time: e.g., a snapshot of the data object, and to provide differences between snapshots taken at a specific time, and thereby enables optimal backup and restore. While the particular implementations and data structures may vary among different appliances from different vendors, a data structure is employed to track changes to the data object, and storage is employed to retain the original state of those portions of the object that have changed: indications in the data structure correspond to data retained in the storage.
- a typical data structure employed is a bitmap, where each bit corresponds to a section of the data object. Setting the bit indicates that section has been modified after the point in time of the snapshot operation.
- the underlying snapshot primitive mechanism maintains this for as long as the snapshot object exists.
- the time line described above maintains a list of the snapshot operations against a given primary data object, including the time an operation is started, the time it is stopped (if at all), a reference to the snapshot object, and a reference to the internal data structure (e.g. bitmaps or extent lists), so that it can be obtained from the underlying system. Also maintained is a reference to the result of copying the state of the data object at any given point in time into another pool—as an example, copying the state of a data object into a capacity-optimized pool 407 using content addressing results in an object handle. That object handle corresponds to a given snapshot and is stored with the snapshot operation in the time line. This correlation is used to identify suitable starting points.
- Optimal backup and restore consult the list of operations from a desired starting point to an end point.
- a time ordered list of operations and their corresponding data structures are constructed such that a continuous time series from start to finish is realized: there is no gap between start times of the operations in the series. This ensures that all changes to the data object are represented by the corresponding bitmap data structures. It is not necessary to retrieve all operations from start to finish; simultaneously existing data objects and underlying snapshots overlap in time; it is only necessary that there are no gaps in time where a change might have occurred that was not tracked. As bitmaps indicate that a certain block of storage has changed but not what the change is, the bitmaps may be added or composed together to realize a set of all changes that occurred in the time interval.
- the system instead of using this data structure to access the state at a point in time, the system instead exploits the fact that the data structure represents data modified as time marches forward. Rather, the end state of the data object is accessed at the indicated areas, thus returning the set of changes to the given data object from the given start time to the end time.
- the backup operation exploits this time line, the correlated references, and access to the internal data structures to realize our backup operation. Similarly, it uses the system in a complementary fashion to accomplish our restore operation. The specific steps are described below in the section for “Optimal Backup/Restore.”
- FIG. 5 illustrates several representative storage pool types. Although one primary storage pool and two secondary storage pools are depicted in the figure, many more may be configured in some embodiments.
- Primary Storage Pool 507 contains the storage resources used to create the data objects in which the user Application stores its data. This is in contrast to the other storage pools, which exist to primarily fulfill the operation of the Data Management Virtualization Engine.
- Performance Optimized Pool 508 a virtual storage pool able to provide high performance backup (i.e. point in time duplication, described below) as well as rapid access to the backup image by the user Application
- Capacity Optimized Pool 509 a virtual storage pool that chiefly provides storage of a data object in a highly space-efficient manner by use of deduplication techniques described below.
- the virtual storage pool provides access to the copy of the data object, but does not do so with high performance as its chief aim, in contrast to the Performance Optimized pool above.
- the initial deployments contain storage pools as described above, as a minimal operational set.
- the design fully expects multiple Pools of a variety of types, representing various combinations of the criteria illustrated above, and multiple Pool Managers as is convenient to represent all of the storage in future deployments.
- the tradeoffs illustrated above are typical of computer data storage systems.
- these three pools represent a preferred embodiment, addressing most users requirements in a very simple way. Most users will find that if they have one pool of storage for urgent restore needs, which affords quick recovery, and one other pool that is low cost, so that a large number of images can be retained for a large period of time, almost all of the business requirements for data protection can be met with little compromise.
- the format of data in each pool is dictated by the objectives and technology used within the pool.
- the quick recovery pool is maintained in the form very similar to the original data to minimize the translation required and to improve the speed of recovery.
- the long-term storage pool uses deduplication and compression to reduce the size of the data and thus reduce the cost of storage.
- the Object Manager 501 creates and maintains instances of Data Storage Objects 503 from the Virtual Storage Pools 418 according to the instructions sent to it by the Service Level Policy Engine 406 .
- the Object Manager provides data object operations in five major areas: point-in-time duplication or copying (commonly referred to as “snapshots”), standard copying, object maintenance, mapping and access maintenance, and collections.
- Object Management operations also include a series of Resource Discovery operations for maintaining Virtual Storage Pools themselves and retrieving information about them.
- the Pool Manager 504 ultimately supplies the functionality for these.
- Snapshot operations create a data object instance representing an initial object instance at a specific point in time. More specifically, a snapshot operation creates a complete virtual copy of the members of a collection using the resources of a specified Virtual Storage Pool. This is called a Data Storage Object. Multiple states of a Data Storage Object are maintained over time, such that the state of a Data Storage Object as it existed at a point in time is available.
- a virtual copy is a copy implemented using an underlying storage virtualization API that allows a copy to be created in a lightweight fashion, using copy-on-write or other in-band technologies instead of copying and storing all bits of duplicate data to disk.
- This may be implemented using software modules written to access the capabilities of an off-the-shelf underlying storage virtualization system such as provided by EMC, vmware or IBM in some embodiments. Where such underlying virtualizations are not available, the described system may provide its own virtualization layer for interfacing with unintelligent hardware.
- Snapshot operations require the application to freeze the state of the data to a specific point so that the image data is coherent, and so that the snapshot may later be used to restore the state of the application at the time of the snapshot. Other preparatory steps may also be required. These are handled by the Application-Specific Module 302 , which is described in a subsequent section. For live applications, therefore, the most lightweight operations are desired.
- Snapshot operations are used as the data primitive for all higher-level operations in the system. In effect, they provide access to the state of the data at a particular point in time. As well, since snapshots are typically implemented using copy-on-write techniques that distinguish what has changed from what is resident on disk, these snapshots provide differences that can also be composed or added together to efficiently copy data throughout the system.
- the format of the snapshot may be the format of data that is copied by Data Mover 502 , which is described below.
- a standard copy operation copies all or a subset of a source data object in one storage pool to a data object in another storage pool.
- the result is two distinct objects.
- One type of standard copy operation that may be used is an initial “baseline” copy. This is typically done when data is initially copied from one Virtual Storage Pool into another, such as from a performance-optimized pool to a capacity-optimized storage pool.
- Another type of standard copy operation may be used wherein only changed data or differences are copied to a target storage pool to update the target object. This would occur after an initial baseline copy has previously been performed.
- a complete exhaustive version of an object need not be preserved in the system each time a copy is made, even though a baseline copy is needed when the Data Virtualization System is first initialized. This is because each virtual copy provides access to a complete copy. Any delta or difference can be expressed in relation to a virtual copy instead of in relation to a baseline. This has the positive side effect of virtually eliminating the common step of walking through a series of change lists.
- Standard copy operations are initiated by a series of instructions or requests supplied by the Pool Manager and received by the Data Mover to cause the movement of data among the Data Storage Objects, and to maintain the Data Storage Objects themselves.
- the copy operations allow the creation of copies of the specified Data Storage Objects using the resources of a specified Virtual Storage Pool. The result is a copy of the source Data Object in a target Data Object in the storage pool.
- the Snapshot and Copy operations are each structured with a preparation operation and an activation operation.
- the two steps of prepare and activate allow the long-running resource allocation operations, typical of the prepare phase, to be decoupled from the actuation. This is required by applications that can only be paused for a short while to fulfill the point-in-time characteristics of a snapshot operation, which in reality takes a finite but non-zero amount of time to accomplish.
- this two-step preparation and activation structure allows the Policy Engine to proceed with an operation only if resources for all of the collection members can be allocated.
- Object Maintenance operations are a series of operations for maintaining data objects, including creation, destruction, and duplication.
- the Object Manager and Data Mover use functionality provided by a Pool Request Broker (more below) to implement these operations.
- the data objects may be maintained at a global level, at each Storage Pool, or preferably both.
- Collection operations are auxiliary functions. Collections are abstract software concepts, lists maintained in memory by the object manager. They allow the Policy Engine 206 to request a series of operations over all of the members in a collection, allowing a consistent application of a request to all members.
- the use of collections allows for simultaneous activation of the point-in-time snapshot so that multiple Data Storage Objects are all captured at precisely the same point in time, as this is typically required by the application for a logically correct restore.
- the use of collections allows for convenient request of a copy operation across all members of a collection, where an application would use multiple storage objects as a logical whole.
- the Object Manager discovers Virtual Storage Pools by issuing Object Management Operations 505 to the Pool Manager 504 , and uses the information obtained about each of the pools to select one that meets the required criteria for a given request, or in the case where none match, a default pool is selected, and the Object Manager can then create a data storage object using resources from the selected Virtual Storage Pool.
- the Object Manager also provides sets of Object Management operations to allow and maintain the availability of these objects to external Applications.
- the first set is operations for registering and unregistering the computers where the user's Applications reside.
- the computers are registered by the identities typical to the storage network in use (e.g. Fibre Channel WWPN, iSCSI identity, etc.).
- the second set is “mapping” operations, and when permitted by the storage pool from which an object is created, the Data Storage Object can be “mapped,” that is, made available for use to a computer on which a user Application resides.
- This availability takes a form appropriate to the storage, e.g. a block device presented on a SAN as a Fibre Channel disk or iSCSI device on a network, a filesystem on a file sharing network, etc. and is usable by the operating system on the Application computer.
- an “unmapping” operation reverses the availability of the virtual storage device on the network to a user Application. In this way, data stored for one Application, i.e. a backup, can be made available to another Application on another computer at a later time, i.e. a restore.
- the Data Mover 502 is a software component within the Object Manager and Data Mover that reads and writes data among the various Data Storage Objects 503 according to instructions received from the Object Manager for Snapshot (Point in Time) Copy requests and standard copy requests.
- the Data Mover provides operations for reading and writing data among instances of data objects throughout the system.
- the Data Mover also provides operations that allow querying and maintaining the state of long running operations that the Object Manager has requested for it to perform.
- the Data Mover uses functionality from the Pool Functionality Providers (see FIG. 6 ) to accomplish its operation.
- the Snapshot functionality provider 608 allows creation of a data object instance representing an initial object instance at a specific point in time.
- the Difference Engine functionality provider 614 is used to request a description of the differences between two data objects that are related in a temporal chain. For data objects stored on content-addressable pools, a special functionality is provided that can provide differences between any two arbitrary data objects. This functionality is also provided for performance-optimized pools, in some cases by an underlying storage virtualization system, and in other cases by a module that implements this on top of commodity storage.
- the Data Mover 502 uses the information about the differences to select the set of data that it copies between instances of data objects 503 .
- the Difference Engine Provider provides a specific representation of the differences between two states of a Data Storage Object over time.
- the changes between two points in time are recorded as writes to a given part of the Data Storage Object.
- the difference is represented as a bitmap where each bit corresponds to an ordered list of the Data Object areas, starting at the first and ascending in order to the last, where a set bit indicates a modified area. This bitmap is derived from the copy-on-write bitmaps used by the underlying storage virtualization system.
- the difference may be represented as a list of extents corresponding to changed areas of data.
- the representation is described below, and is used to determine efficiently the parts of two Content Addressable Data Objects that differ.
- the Data Mover uses this information to copy only those sections that differ, so that a new version of a Data Object can be created from an existing version by first duplicating it, obtaining the list of differences, and then moving only the data corresponding to those differences in the list.
- the Data Mover 502 traverses the list of differences, moving the indicated areas from the source Data Object to the target Data Object. (See Optimal Way for Data Backup and Restore.)
- the Object Manager 501 instructs the Data Mover 502 through a series of operations to copy data among the data objects in the Virtual Storage Pools 418 .
- the procedure includes the following steps, starting at the reception of instructions:
- the collection name from above is used as well as the name of the source Data Object that is to be copied and the name of two antecedents: a Data Object against which differences are to be taken in the source Storage Resource Pool, and a corresponding Data Object in the target Storage Resource Pool. This step is repeated for each source Data Object to be operated on in this set.
- the collection name is supplied as well as a Storage Resource Pool to act as a target.
- the prepare command instructs the Object Manager to contact the Storage Pool Manager to create the necessary target Data Objects, corresponding to each of the sources in the collection.
- the prepare command also supplies the corresponding Data Object in the target Storage Resource Pool to be duplicated, so the Provider can duplicate the provided object and use that as a target object.
- a reference name for the copy request is returned.
- the reference name for the copy request returned above is supplied.
- the Data Mover is instructed to copy a given source object to its corresponding target object.
- Each request includes a reference name as well as a sequence number to describe the overall job (the entire set of source target pairs) as well as a sequence number to describe each individual source-target pair.
- the names of the corresponding antecedents are supplied as part of the Copy instruction.
- the Copy Engine uses the name of the Data Object in the source pool to obtain the differences between the antecedent and the source from the Difference Engine at the source.
- the indicated differences are then transmitted from the source to the target.
- these differences are transmitted as bitmaps and data.
- these differences are transmitted as extent lists and data.
- Data Storage Objects are software constructs that permit the storage and retrieval of Application data using idioms and methods familiar to computer data processing equipment and software. In practice these currently take the form of a SCSI block device on a storage network, e.g. a SCSI LUN, or a content-addressable container, where a designator for the content is constructed from and uniquely identifies the data therein.
- Data Storage Objects are created and maintained by issuing instructions to the Pool Manager. The actual storage for persisting the Application data is drawn from the Virtual Storage Pool from which the Data Storage Object is created.
- the structure of the data storage object varies depending on the storage pool from which it is created.
- the data structure for a given block device Data Object implements a mapping between the Logical Block Address (LBA) of each of the blocks within the Data Object to the device identifier and LBA of the actual storage location.
- LBA Logical Block Address
- the identifier of the Data Object is used to identify the set of mappings to be used.
- the current embodiment relies on the services provided by the underlying physical computer platform to implement this mapping, and relies on its internal data structures, such as bitmaps or extent lists.
- the content signature is used as the identifier, and the Data Object is stored as is described below in the section about deduplication.
- a Pool Manager 504 is a software component for managing virtual storage resources and the associated functionality and characteristics as described below.
- the Object manager 501 and Data Movement Engine 502 communicate with one or more Pool Managers 504 to maintain Data Storage Objects 503 .
- Virtual Storage Resources 510 are various kinds of storage made available to the Pool Manager for implementing storage pool functions, as described below.
- a storage virtualizer is used to present various external Fibre Channel or iSCSI storage LUNs as virtualized storage to the Pool Manager 504 .
- FIG. 6 further illustrates the Storage Pool Manager 504 .
- the purpose of the storage pool manager is to present underlying virtual storage resources to the Object Manager/Data Mover as Storage Resource Pools, which are abstractions of storage and data management functionality with common interfaces that are utilized by other components of the system. These common interfaces typically include a mechanism for identifying and addressing data objects associated with a specific temporal state, and a mechanism for producing differences between data objects in the form of bitmaps or extents.
- the pool manager presents a Primary Storage Pool, a Performance Optimized Pool, and a Capacity Optimized Pool.
- the common interfaces allow the object manager to create and delete Data Storage objects in these pools, either as copies of other data storage objects or as new objects, and the data mover can move data between data storage objects, and can use the results of data object differencing operations.
- the storage pool manager has a typical architecture for implementing a common interface to diverse implementations of similar functionality, where some functionality is provided by “smart” underlying resources, and other functionality must be implemented on top of less functional underlying resources.
- Pool request broker 602 and pool functionality providers 604 are software modules executing in either the same process as the Object Manager/Data Mover, or in another process communicating via a local or network protocol such as TCP.
- the providers include a Primary Storage provider 606 , Snapshot provider 608 , Content Addressable provider 610 , and Difference Engine provider 614 , and these are further described below.
- the set of providers may be a superset of those shown here.
- Virtual Storage Resources 510 are the different kinds of storage made available to the Pool Manager for implementing storage pool functions.
- the virtual storage resources include sets of SCSI logical units from a storage virtualization system that runs on the same hardware as the pool manager, and accessible (for both data and management operations) through a programmatic interface: in addition to standard block storage functionality additional capabilities are available including creating and deleting snapshots, and tracking changed portions of volumes.
- the virtual resources can be from an external storage system that exposes similar capabilities, or may differ in interface (for example accessed through a file-system, or through a network interface such as CIFS, iSCSI or CDMI), in capability (for example, whether the resource supports an operation to make a copy-on-write snapshot), or in non-functional aspects (for example, high-speed/limited-capacity such as Solid State Disk versus low-speed/high-capacity such as SATA disk).
- interface for example accessed through a file-system, or through a network interface such as CIFS, iSCSI or CDMI
- capability for example, whether the resource supports an operation to make a copy-on-write snapshot
- non-functional aspects for example, high-speed/limited-capacity such as Solid State Disk versus low-speed/high-capacity such as SATA disk).
- the capabilities and interface available determine which providers can consume the virtual storage resources, and which pool functionality needs to be implemented within the pool manager by one or more providers: for example, this implementation of a content addressable storage provider only requires “dumb” storage, and the implementation is entirely within content addressable provider 610 ; an underlying content addressable virtual storage resource could be used instead with a simpler “pass-through” provider. Conversely, this implementation of a snapshot provider is mostly “pass-through” and requires storage that exposes a quick point-in-time copy operation.
- Pool Request Broker 602 is a simple software component that services requests for storage pool specific functions by executing an appropriate set of pool functionality providers against the configured virtual storage resource 510 .
- the requests that can be serviced include, but are not limited to, creating an object in a pool; deleting an object from a pool; writing data to an object; reading data from an object; copying an object within a pool; copying an object between pools; requesting a summary of the differences between two objects in a pool.
- Primary storage provider 606 enables management interfaces (for example, creating and deleting snapshots, and tracking changed portions of files) to a virtual storage resource that is also exposed directly to applications via an interface such as fibre channel, iSCSI, NFS or CIFS.
- management interfaces for example, creating and deleting snapshots, and tracking changed portions of files
- a virtual storage resource that is also exposed directly to applications via an interface such as fibre channel, iSCSI, NFS or CIFS.
- Snapshot provider 608 implements the function of making a point-in-time copy of data from a Primary resource pool. This creates the abstraction of another resource pool populated with snapshots. As implemented, the point-in-time copy is a copy-on-write snapshot of the object from the primary resource pool, consuming a second virtual storage resource to accommodate the copy-on-write copies, since this management functionality is exposed by the virtual storage resources used for primary storage and for the snapshot provider.
- Difference engine provider 614 can satisfy a request for two objects in a pool to be compared that are connected in a temporal chain.
- the difference sections between the two objects are identified and summarized in a provider-specific way, e.g. using bitmaps or extents.
- the difference sections might be represented as a bitmap where each set bit denotes a fixed size region where the two objects differ; or the differences might be represented procedurally as a series of function calls or callbacks.
- a difference engine may produce a result efficiently in various ways.
- a difference engine acting on a pool implemented via a snapshot provider uses the copy-on-write nature of the snapshot provider to track changes to objects that have had snapshots made. Consecutive snapshots of a single changing primary object thus have a record of the differences that is stored alongside them by the snapshot provider, and the difference engine for snapshot pools simply retrieves this record of change.
- a difference engine acting on a pool implemented via a Content Addressable provider uses the efficient tree structure (see below, FIG. 12 ) of the content addressable implementation to do rapid comparisons between objects on demand.
- Content addressable provider 610 implements a write-once content addressable interface to the virtual storage resource it consumes. It satisfies read, write, duplicate and delete operations. Each written or copied object is identified by a unique handle that is derived from its content. The content addressable provider is described further below ( FIG. 11 ).
- the pool request broker 502 accepts requests for data manipulation operations such as copy, snapshot, or delete on a pool or object.
- the request broker determines which provider code from pool 504 to execute by looking at the name or reference to the pool or object.
- the broker then translates the incoming service request into a form that can be handled by the specific pool functionality provider, and invokes the appropriate sequence of provider operations.
- an incoming request could ask to make a snapshot from a volume in a primary storage pool, into a snapshot pool.
- the incoming request identifies the object (volume) in the primary storage pool by name, and the combination of name and operation (snapshot) determines that the snapshot provider should be invoked which can make point-in-time snapshots from the primary pool using the underlying snapshot capability.
- This snapshot provider will translate the request into the exact form required by the native copy-on-write function performed by the underlying storage virtualization appliance, such as bitmaps or extents, and it will translate the result of the native copy-on-write function to a storage volume handle that can be returned to the object manager and used in future requests to the pool manager.
- Optimal Way for Data Backup is a series of operations to make successive versions of Application Data objects over time, while minimizing the amount of data that must be copied by using bitmaps, extents and other temporal difference information stored at the Object Mover. It stores the application data in a data storage object and associates with it the metadata that relates the various changes to the application data over time, such that changes over time can be readily identified.
- the procedure includes the following steps:
- the mechanism provides an initial reference state, e.g. T0, of the Application Data within a Data Storage Object.
- Each successive version e.g. T4, T5 uses the Difference Engine Provider for the Virtual Storage Pool to obtain the difference between it and the instance created prior to it, so that T5 is stored as a reference to T4 and a set of differences between T5 and T4. 4.
- the Copy Engine receives a request to copy data from one data object (the source) to another data object (the destination). 5. If the Virtual Storage Pool in which the destination object will be created contains no other objects created from prior versions of the source data object, then a new object is created in the destination Virtual Storage Pool and the entire contents of the source data object are copied to the destination object; the procedure is complete. Otherwise the next steps are followed. 6.
- a recently created prior version in the destination Virtual Storage Pool is selected for which there exists a corresponding prior version in the Virtual Storage Pool of the source data object. For example, if a copy of T5 is initiated from a snapshot pool, and an object created at time T3 is the most recent version available at the target, T3 is selected as the prior version. 7. Construct a time-ordered list of the versions of the source data object, beginning with an initial version identified in the previous step, and ending with the source data object that is about to be copied. In the above example, at the snapshot pool, all states of the object are available, but only the states including and following T3 are of interest: T3, T4, T5. 8.
- Each data object within the destination Virtual Storage Pool is complete; that is, it represents the entire data object and allows access to the all of the Application Data at the point in time without requiring external reference to state or representations at other points in time.
- the object is accessible without replaying all deltas from a baseline state to the present state.
- the duplication of initial and subsequent versions of the data object in the destination Virtual Storage Pool does not require exhaustive duplication of the Application Data contents therein.
- to arrive at second and subsequent states requires only the transmission of the changes tracked and maintained, as described above, without exhaustive traversal, transmission or replication of the contents of the data storage object.
- the operation of the Optimal Way for Data Restore is the converse of the Optimal Way for Data Backup.
- the procedure to recreate the desired state of a data object in a destination Virtual Storage Pool at a given point in time includes the following steps:
- Step 2 If no version of the data object is identified in Step 2, then create a new destination object in the destination Virtual Storage Pool and copy the data from the source data object to the destination data object. The procedure is complete. Otherwise, proceed with the following steps. 4. If a version of the data object is identified in Step 2, then identify a data object in the source Virtual Storage Pool corresponding to the data object identified in Step 2. 5. If no data object is identified in Step 4, then create a new destination object in the destination Virtual Storage Pool and copy the data from the source data object to the destination data object. The procedure is complete. Otherwise, proceed with the following steps. 6. Create a new destination data object in the Destination Virtual Storage Pool by duplicating the data object identified in Step 2. 7. Employ the Difference Engine Provider for the source Virtual Storage Pool to obtain the set of differences between the data object identified in Step 1 and the data object identified in Step 4. 8. Copy the data identified by the list created in Step 7 from the source data object to the destination data object. The procedure is complete.
- Access to the desired state is complete: it does not require external reference to other containers or other states. Establishing the desired state given a reference state requires neither exhaustive traversal nor exhaustive transmission, only the retrieved changes indicated by the provided representations within the source Virtual Storage Pool.
- FIG. 7 illustrates the Service Level Agreement.
- the Service Level Agreement captures the detailed business requirements with respect to secondary copies of the application data.
- the business requirements define when and how often copies are created, how long they are retained and in what type of storage pools these copies reside. This simplistic description does not capture several aspects of the business requirements.
- the frequency of copy creation for a given type of pool may not be uniform across all hours of the day or across all days of a week. Certain hours of the day, or certain days of a week or month may represent more (or less) critical periods in the application data, and thus may call for more (or less) frequent copies.
- all copies of application data in a particular pool may not be required to be retained for the same length of time. For example, a copy of the application data created at the end of monthly processing may need to be retained for a longer period of time than a copy in the same storage pool created in the middle of a month.
- the Service Level Agreement 304 of certain embodiments has been designed to represent all of these complexities that exist in the business requirements.
- the Service Level Agreement has four primary parts: the name, the description, the housekeeping attributes and a collection of Service Level Policies. As mentioned above, there is one SLA per application.
- the name attribute 701 allows each Service Level Agreement to have a unique name.
- the description attribute 702 is where the user can assign a helpful description for the Service Level Agreement.
- the Service Level agreement also has a number of housekeeping attributes 703 that enable it to be maintained and revised. These attributes include but are not limited to the owner's identity, the dates and times of creation, modification and access, priority, enable/disable flags.
- the Service Level Agreement also contains a plurality of Service Level Policies 705 .
- Some Service level Agreements may have just a single Service Level Policy. More typically, a single SLA may contain tens of policies.
- Each Service Level Policy includes at least the following, in certain embodiments: the source storage pool location 706 and type 708 ; the target storage pool location 710 and type 712 ; the frequency for the creation of copies 714 , expressed as a period of time; the length of retention of the copy 716 , expressed as a period of time; the hours of operation 718 during the day for this particular Service Level Policy; and the days of the week, month or year 720 on which this Service Level Policy applies.
- Each Service Level Policy specifies a source and target storage pool, and the frequency of copies of application data that are desired between those storage pools. Furthermore, the Service Level Policy specifies its hours of operation and days on which it is applicable. Each Service Level Policy is the representation of one single statement in the business requirements for the protection of application data. For example, if a particular application has a business requirement for an archive copy to be created each month after the monthly close and retained for three years, this might translate to a Service level Policy that requires a copy from the Local Backup Storage Pool into the Long-term Archive Storage Pool at midnight on the last day of the month, with a retention of three years.
- All of the Service Level Policies with a particular combination of source and destination pool and location say for example, source Primary Storage pool and destination local Snapshot pool, when taken together, specify the business requirements for creating copies into that particular destination pool.
- Business requirements may dictate for example that snapshot copies be created every hour during regular working hours, but only once every four hours outside of these times.
- Two Service Level Policies with the same source and target storage pools will effectively capture these requirements in a form that can be put into practice by the Service Policy Engine.
- This form of a Service Level Agreement allows the representation of the schedule of daily, weekly and monthly business activities, and thus captures business requirements for protecting and managing application data much more accurately than traditional RPO and RPO based schemes. By allowing hour of operation and days, weeks, and months of the year, scheduling can occur on a “calendar basis.”
- a combination of Service Level Policies may require a large number of snapshots to be preserved for a short time, such as 10 minutes, and a lesser number of snapshots to be preserved for a longer time, such as 8 hours; this allows a small amount of information that has been accidentally deleted can be reverted to a state not more than 10 minutes before, while still providing substantial data protection at longer time horizons without requiring the storage overhead of storing all snapshots taken every ten minutes.
- the backup data protection function may be given one Policy that operates with one frequency during the work week, and another frequency during the weekend.
- Service Level Policies for all of the different classes of source and destination storage are included, the Service Level Agreement fully captures all of the data protection requirements for the entire application, including local snapshots, local long duration stores, off-site storage, archives, etc.
- a collection of policies within a SLA is capable of expressing when a given function should be performed, and is capable of expressing multiple data management functions that should be performed on a given source of data.
- Service Level Agreements are created and modified by the user through a user interface on a management workstation. These agreements are electronic documents stored by the Service Policy Engine in a structured SQL database or other repository that it manages. The policies are retrieved, electronically analyzed, and acted upon by the Service Policy Engine through its normal scheduling algorithm as described below.
- FIG. 8 illustrates the Application Specific Module 402 .
- the Application Specific module runs close to the Application 300 (as described above), and interacts with the Application and its operating environment to gather metadata and to query and control the Application as required for data management operations.
- the Application Specific Module interacts with various components of the application and its operating environment including Application Service Processes and Daemons 801 , Application Configuration Data 802 , Operating System Storage Services 803 (such as VSS and VDS on Windows), Logical Volume Management and Filesystem Services 804 , and Operating System Drivers and Modules 805 .
- the Application Specific Module performs these operations in response to control commands from the Service Policy Engine 406 .
- Metadata Collection and Application Consistency.
- Metadata Collection is the process by which the Application Specific Module collects metadata about the application.
- metadata includes information such as: configuration parameters for the application; state and status of the application; control files and startup/shutdown scripts for the application; location of the datafiles, journal and transaction logs for the application; and symbolic links, filesystem mount points, logical volume names, and other such entities that can affect the access to application data.
- Metadata is collected and saved along with application data and SLA information. This guarantees that each copy of application data within the system is self contained and includes all of the details required to rebuild the application data.
- Application Consistency is the set of actions that ensure that when a copy of the application data is created, the copy is valid, and can be restored into a valid instance of the application. This is critical when the business requirements dictate that the application be protected while it is live, in its online, operational state. The application may have interdependent data relations within its data stores, and if these are not copied in a consistent state will not provide a valid restorable image.
- the exact process of achieving application consistency varies from application to application. Some applications have a simple flush command that forces cached data to disk. Some applications support a hot backup mode where the application ensures that its operations are journaled in a manner that guarantees consistency even as application data is changing. Some applications require interactions with operating system storage services such as VSS and VDS to ensure consistency.
- the Application Specific Module is purpose-built to work with a particular application and to ensure the consistency of that application. The Application Specific Module interacts with the underlying storage virtualization device and the Object Manager to provide consistent snapshots of application data.
- the preferred embodiment of the Application Specific Module 402 is to run on the same server as Application 300 . This assures the minimum latency in the interactions with the application, and provides access to storage services and filesystems on the application host.
- the application host is typically considered primary storage, which is then snapshotted to a performance-optimized store.
- the Application Specific Module is only triggered to make a snapshot when access to application data is required at a specific time, and when a snapshot for that time does not exist elsewhere in the system, as tracked by the Object Manager.
- the Object Manager is able to fulfill subsequent data requests from the performance-optimized data store, including for satisfying multiple requests for backup and replication which may issue from secondary, capacity-optimized pools.
- the Object Manager may be able to provide object handles to the snapshot in the performance-optimized store, and may direct the performance-optimized store in a native format that is specific to the format of the snapshot, which is dependent on the underlying storage appliance.
- this format may be application data combined with one or more LUN bitmaps indicating which blocks have changed; in other embodiments it may be specific extents.
- the format used for data transfer is thus able to transfer only a delta or difference between two snapshots using bitmaps or extents.
- Metadata such as the version number of the application, may also be stored for each application along with the snapshot.
- application metadata is read and used for the policy. This metadata is stored along with the data objects.
- application metadata will only be read once during the lightweight snapshot operation, and preparatory operations which occur at that time such as flushing caches will only be performed once during the lightweight snapshot operation, even though this copy of application data along with its metadata may be used for multiple data management functions.
- FIG. 9 illustrates the Service Policy Engine 406 .
- the Service Policy Engine contains the Service Policy Scheduler 902 , which examines all of the Service Level Agreements configured by the user and makes scheduling decisions to satisfy Service Level Agreements. It relies on several data stores to capture information and persist it over time, including, in some embodiments, a SLA Store 904 , where configured Service Level Agreements are persisted and updated; a Resource Profile Store 906 , storing Resource Profiles that provide a mapping between logical storage pool names and actual storage pools; Protection Catalog Store 908 , where information is cataloged about previous successful copies created in various pools that have not yet expired; and centralized History Store 910 .
- History Store 910 is where historical information about past activities is saved for the use of all data management applications, including the timestamp, order and hierarchy of previous copies of each application into various storage pools. For example, a snapshot copy from a primary data store to a capacity-optimized data store that is initiated at 1 P.M. and is scheduled to expire at 9 P.M. will be recorded in History Store 910 in a temporal data store that also includes linked object data for snapshots for the same source and target that have taken place at 11 A.M. and 12 P.M.
- These stores are managed by the Service Policy Engine. For example, when the user, through the Management workstation creates a Service Level Agreement, or modifies one of the policies within it, it is the Service Policy Engine that persists this new SLA in its store, and reacts to this modification by scheduling copies as dictated by the SLA. Similarly, when the Service Policy Engine successfully completes a data movement job that results in a new copy of an application in a Storage Pool, the Storage Policy Engine updates the History Store, so that this copy will be factored into future decisions.
- the preferred embodiment of the various stores used by the Service Policy Engine is in the form of tables in a relational database management system in close proximity to the Service Policy Engine. This ensures consistent transactional semantics when querying and updating the stores, and allows for flexibility in retrieving interdependent data.
- the scheduling algorithm for the Service Policy Scheduler 902 is illustrated in FIG. 10 .
- the Service Policy Scheduler decides it needs to make a copy of application data from one storage pool to another, it initiates a Data Movement Requestor and Monitor task, 912 .
- These tasks are not recurring tasks and terminate when they are completed.
- a plurality of these requestors might be operational at the same time.
- the Service Policy Scheduler considers the priorities of Service Level Agreements when determining which additional tasks to undertake. For example, if one Service Level Agreement has a high priority because it specifies the protection for a mission-critical application, whereas another SLA has a lower priority because it specifies the protection for a test database, then the Service Policy Engine may choose to run only the protection for the mission-critical application, and may postpone or even entirely skip the protection for the lower priority application. This is accomplished by the Service Policy Engine scheduling a higher priority SLA ahead of a lower priority SLA. In the preferred embodiment, in such a situation, for auditing purposes, the Service Policy Engine will also trigger a notification event to the management workstation.
- FIG. 10 illustrates the flowchart of the Policy Schedule Engine.
- the Policy Schedule Engine continuously cycles through all the SLAs defined. When it gets to the end of all of the SLAs, it sleeps for a short while, e.g. 10 seconds, and resumes looking through the SLAs again.
- Each SLA encapsulates the complete data protection business requirements for one application; thus all of the SLAs represent all of the applications.
- the schedule engine collects together all of the Service Level Policies that have the same source pool and destination pool 1004 the process state at 1000 and iterates to the next SLA in the set of SLAs in 1002 . Taken together, this subset of the Service Level Policies represent all of the requirements for a copy from that source storage pool to that particular destination storage pool.
- the Service Policy Scheduler discards the policies that are not applicable to today, or are outside their hours of operation. Among the policies that are left, find the policy that has the shortest frequency 1006 , and based on the history data and in history store 910 , the one with the longest retention that needs to be run next 1008 .
- the Scheduler moves to the next Source and Destination pool combination for the same Service Level agreement 1018 . If there are no more distinct combinations, the Scheduler moves on to the next Service Level Agreement 1020 .
- Service Policy Scheduler After the Service Policy Scheduler has been through all source/destination pool combinations of all Service Level Agreements, it pauses for a short period and then resumes the cycle.
- a simple example system with a snapshot store and a backup store, with only 2 policies defined, would interact with the Service Policy Scheduler as follows. Given two policies, one stating “backup every hour, the backup to be kept for 4 hours” and another stating “backup every 2 hours, the backup to be kept for 8 hours,” the result would be a single snapshot taken each hour, the snapshots each being copied to the backup store but retained a different amount of time at both the snapshot store and the backup store. The “backup every 2 hours” policy is scheduled to go into effect at 12:00 P.M by the system administrator.
- the Service Policy Scheduler begins operating at step 1000 , it finds the two policies at step 1002 . (Both policies apply because a multiple of two hours has elapsed since 12:00 P.M.) There is only one source and destination pool combination at step 1004 . There are two frequencies at step 1006 , and the system selects the 1-hour frequency because it is shorter than the 2-hour frequency. There are two operations with different retentions at step 1008 , and the system selects the operation with the 8-hour retention, as it has the longer retention value. Instead of one copy being made to satisfy the 4-hour requirement and another copy being made to satisfy the 8-hour requirement, the two requirements are coalesced into the longer 8-hour requirement, and are satisfied by a single snapshot copy operation.
- the system determines that a copy is due at step 1010 , and checks the relevant objects at the History Store 910 to determine if the copy has already been made at the target (at step 912 ) and at the source (at step 914 ). If these checks are passed, the system initiates the copy at step 916 , and in the process triggers a snapshot to be made and saved at the snapshot store. The snapshot is then copied from the snapshot store to the backup store. The system then goes to sleep 1022 and wakes up again after a short period, such as 10 seconds. The result is a copy at the backup store and a copy at the snapshot store, where every even-hour snapshot lasts for 8 hours, and every odd-hour snapshot lasts 4 hours. The even-hour snapshots at the backup store and the snapshot store are both tagged with the retention period of 8 hours, and will be automatically deleted from the system by another process at that time.
- FIG. 11 is a block diagram of the modules implementing the content addressable store for the Content Addressable Provider 510 .
- the content addressable store 510 implementation provides a storage resource pool that is optimized for capacity rather than for copy-in or copy-out speed, as would be the case for the performance-optimized pool implemented through snapshots, described earlier, and thus is typically used for offline backup, replication and remote backup.
- Content addressable storage provides a way of storing common subsets of different objects only once, where those common subsets may be of varying sizes but typically as small as 4 KiBytes.
- the storage overhead of a content addressable store is low compared to a snapshot store, though the access time is usually higher.
- a content addressable store has no intrinsic relationship to one another, even though they may share a large percentage of their content, though in this implementation a history relationship is also maintained, which is an enabler of various optimizations to be described.
- the content addressable store will store only one copy of a data subset that is repeated multiple times within a single object, whereas a snapshot-based store will store at least one full-copy of any object.
- the content addressable store 510 is a software module that executes on the same system as the pool manager, either in the same process or in a separate process communicating via a local transport such as TCP.
- the content addressable store module runs in a separate process so as to minimize impact of software failures from different components.
- This module's purpose is to allow storage of Data Storage Objects 403 in a highly space-efficient manner by deduplicating content (i.e., ensuring repeated content within single or multiple data objects is stored only once).
- the content addressable store module provides services to the pool manager via a programmatic API. These services include the following:
- Object to Handle mapping 1102 an object can be created by writing data into the store via an API; once the data is written completely the API returns an object handle determined by the content of the object. Conversely, data may be read as a stream of bytes from an offset within an object by providing the handle. Details of how the handle is constructed are explained in connection with the description of FIG. 12 .
- Temporal Tree Management 1104 tracks parent/child relationships between data objects stored.
- an API When a data object is written into the store 510 , an API allows it to be linked as a child to a parent object already in the store. This indicates to the content addressable store that the child object is a modification of the parent.
- a single parent may have multiple children with different modifications, as might be the case for example if an application's data were saved into the store regularly for some while; then an early copy were restored and used as a new starting point for subsequent modifications.
- Temporal tree management operations and data models are described in more detail below.
- Difference Engine 1106 can generate a summary of difference regions between two arbitrary objects in the store.
- the differencing operation is invoked via an API specifying the handles of two objects to be compared, and the form of the difference summary is a sequence of callbacks with the offset and size of sequential difference sections.
- the difference is calculated by comparing two hashed representations of the objects in parallel.
- Garbage Collector 1108 is a service that analyzes the store to find saved data that is not referenced by any object handle, and to reclaim the storage space committed to this data. It is the nature of the content addressable store that much data is referenced by multiple object handles, i.e., the data is shared between data objects; some data will be referenced by a single object handle; but data that is referenced by no object handles (as might be the case if an object handle has been deleted from the content addressable system) can be safely overwritten by new data.
- Object Replicator 1110 is a service to duplicate data objects between two different content addressable stores. Multiple content addressable stores may be used to satisfy additional business requirements, such as offline backup or remote backup.
- the Data Hash module 1112 generates fixed length keys for data chunks up to a fixed size limit. For example, in this embodiment the maximum size of chunk that the hash generator will make a key for is 64 KiB.
- the fixed length key is either a hash, tagged to indicate the hashing scheme used, or a non-lossy algorithmic encoding.
- the hashing scheme used in this embodiment is SHA-1, which generates a secure cryptographic hash with a uniform distribution and a probability of hash collision near enough zero that no facility need be incorporated into this system to detect and deal with collisions.
- the Data Handle Cache 1114 is a software module managing an in-memory database that provides ephemeral storage for data and for handle-to-data mappings.
- the Persistent Handle Management Index 1104 is a reliable persistent database of CAH-to-data mappings.
- this embodiment is implemented as a B-tree, mapping hashes from the hash generator to pages in the persistent data store 1118 that contain the data for this hash. Since the full B-tree cannot be held in memory at one time, for efficiency, this embodiment also uses an in-memory bloom filter to avoid expensive B-tree searches for hashes known not to be present.
- the Persistent Data Storage module 1118 stores data and handles to long-term persistent storage, returning a token indicating where the data is stored.
- the handle/token pair is subsequently used to retrieve the data.
- data is written to persistent storage, it passes through a layer of lossless data compression 1120 , in this embodiment implemented using zlib, and a layer of optional reversible encryption 1122 , which is not enabled in this embodiment.
- copying a data object into the content addressable store is an operation provided by the object/handle mapper service, since an incoming object will be stored and a handle will be returned to the requestor.
- the object/handle mapper reads the incoming object, requests hashes to be generated by the Data Hash Generator, stores the data to Persistent Data Storage and the handle to the Persistent Handle Management Index.
- the Data Handle Cache is kept updated for future quick lookups of data for the handle.
- Data stored to Persistent Data Storage is compressed and (optionally) encrypted before being written to disk.
- a request to copy in a data object will also invoke the temporal tree management service to make a history record for the object, and this is also persisted via Persistent Data Storage.
- copying a data object out of the content addressable store given its handle is another operation provided by the object/handle mapper service.
- the handle is looked up in the Data Handle Cache to locate the corresponding data; if the data is missing in the cache the persistent index is used; once the data is located on disk, it is retrieved via persistent data storage module (which decrypts and decompresses the disk data) and then reconstituted to return to the requestor.
- FIG. 12 shows how the handle for a content addressed object is generated.
- the data object manager references all content addressable objects with a content addressable handle.
- This handle is made up of three parts.
- the first part 1201 is the size of the underlying data object the handle immediately points to.
- the second part 1202 is the depth of object it points to.
- the third 1203 is a hash of the object it points to.
- Field 1203 optionally includes a tag indicating that the hash is a non-lossy encoding of the underlying data.
- the tag indicates the encoding scheme used, such as a form of run-length encoding (RLE) of data used as an algorithmic encoding if the data chunk can be fully represented as a short enough RLE. If the underlying data object is too large to be represented as a non-lossy encoding, a mapping from the hash to a pointer or reference to the data is stored separately in the persistent handle management index 1104 .
- RLE run-length
- the data for a content addressable object is broken up into chunks 1204 .
- the size of each chunk must be addressable by one content addressable handle 1205 .
- the data is hashed by the data hash module 1102 , and the hash of the chunk is used to make the handle. If the data of the object fits in one chunk, then the handle created is the final handle of the object. If not, then the handles themselves are grouped together into chunks 1206 and a hash is generated for each group of handles. This grouping of handles continues 1207 until there is only one handle 1208 produced which is then the handle for the object.
- the top level content handle is dereferenced to obtain a list of next-level content handles. These are dereferenced in turn to obtain further lists of content handles until depth-0 handles are obtained. These are expanded to data, either by looking up the handle in the handle management index or cache, or (in the case of an algorithmic hash such as run-length encoding) expanding deterministically to the full content.
- FIG. 13 illustrates the temporal tree relationship created for data objects stored within the content addressable store. This particular data structure is utilized only within the content addressable store.
- the temporal tree management module maintains data structures 1302 in the persistent store that associate each content-addressed data object to a parent (which may be null, to indicate the first in a sequence of revisions).
- the individual nodes of the tree contain a single hash value. This hash value references a chunk of data, if the hash is a depth-0 hash, or a list of other hashes, if the hash is a depth-1 or higher hash.
- the references mapped to a hash value is contained in the Persistent Handle Management Index 1104 .
- the edges of the tree may have weights or lengths, which may be used in an algorithm for finding neighbors.
- the “Add” operation may be used whenever an object is copied-in to the CAS from an external pool. If the copy-in is via the Optimal Way for Data Backup, or if the object is originating in a different CAS pool, then it is required that a predecessor object be specified, and the Add operation is invoked to record this predecessor/successor relationship.
- the “Remove” operation is invoked by the object manager when the policy manager determines that an object's retention period has expired. This may lead to data stored in the CAS having no object in the temporal tree referring to it, and therefore a subsequent garbage collection pass can free up the storage space for that data as available for re-use.
- Different CAS pools may be used to accomplish different business objectives such as providing disaster recovery in a remote location.
- the copy may be sent as hashes and offsets, to take advantage of the native deduplication capabilities of the target CAS.
- the underlying data pointed to by any new hashes is also sent on an as-needed basis.
- the temporal tree structure is read or navigated as part of the implementation of various services:
- the CAS difference engine 1106 compares two objects identified by hash values or handles as in FIGS. 11 and 12 , and produces a sequence of offsets and extents within the objects where the object data is known to differ. This sequence is achieved by traversing the two object trees in parallel in the hash data structure of FIG. 12 .
- the tree traversal is a standard depth- or breadth-first traversal. During traversal, the hashes at the current depth are compared. Where the hash of a node is identical between both sides, there is no need to descend the tree further, so the traversal may be pruned. If the hash of a node is not identical, the traversal continues descending into the next lowest level of the tree.
- the traversal reaches a depth-0 hash that is not identical to its counterpart, then the absolute offset into the data object being compared where the non-identical data occurs, together with the data length, is emitted into the output sequence. If one object is smaller in size than another, then its traversal will complete earlier, and all subsequent offsets encountered in the traversal of the other are emitted as differences.
- Garbage Collector is a service that analyzes a particular CAS store to find saved data that is not referenced by any object handle in the CAS store temporal data structure, and to reclaim the storage space committed to this data.
- Garbage collection uses a standard “Mark and Sweep” approach. Since the “mark” phase may be quite expensive, the algorithm used for the mark phase attempts to minimize marking the same data multiple times, even though it may be referenced many times; however the mark phase must be complete, ensuring that no referenced data is left unmarked, as this would result in data loss from the store as, after a sweep phase, unmarked data would later be overwritten by new data.
- the algorithm employed for marking referenced data uses the fact that objects in the CAS are arranged in graphs with temporal relationships using the data structure depicted in FIG. 13 . It is likely that objects that share an edge in these graphs differ in only a small subset of their data, and it is also rare that any new data chunk that appears when an object is created from a predecessor should appear again between any two other objects. Thus, the mark phase of garbage collection processes each connected component of the temporal graph.
- FIG. 14 is an example of garbage collection using temporal relationships in certain embodiments.
- a depth-first search is made, represented by arrows 1402 , of a data structure containing temporal relationships. Take a starting node 1404 from which to begin the tree traversal. Node 1404 is the tree root and references no objects. Node 1406 contains references to objects H1 and H2, denoting a hash value for object 1 and a hash value for object 2. All depth-0, depth-1 and higher data objects that are referenced by node 1406 , here H1 and H2, are enumerated and marked as referenced.
- node 1408 is processed. As it shares an edge with node 1406 , which has been marked, the difference engine is applied to the difference between the object referenced by 1406 and the object referenced by 1408 , obtaining a set of depth-0, depth-1 and higher hashes that exist in the unmarked object but not in the marked object. In the figure, the hash that exists in node 1408 but not in node 1406 is H3, so H3 is marked as referenced. This procedure is continued until all edges are exhausted.
- a comparison of the results produced by a prior art algorithm 1418 and the present embodiment 1420 shows that when node 1408 is processed by the prior art algorithm, previously-seen hashes H1 and H2 are emitted into the output stream along with new hash H3.
- Present embodiment 1420 does not emit previously seen hashes into the output stream, resulting in only new hashes H3, H4, H5, H6, H7 being emitted into the output stream, with a corresponding improvement in performance. Note that this method does not guarantee that data will not be marked more than once. For example, if hash value H4 occurs independently in node 1416 , it will be independently marked a second time.
- Copying an object from another pool into the CAS uses the software modules described in FIG. 11 to produce a data structure referenced by an object handle as in FIG. 12 .
- the input to the process is (a) a sequence of chunks of data at specified offsets, sized appropriately for making depth-0 handles, and optionally (b) a previous version of the same object. Implicitly, the new object will be identical to the previous version except where the input data is provided and itself differs from the previous version.
- the algorithm for the copy-in operation is illustrated in a flowchart at FIG. 15 .
- the sequence (a) may be a sparse set of changes from (b).
- this can greatly reduce the amount of data that needs to be copied in, and therefore reduce the computation and i/o activity required. This is the case, for example, when the object is to be copied in via the optimal way for data backup described previously.
- sequence (a) includes sections that are largely unchanged from a predecessor, identifying the predecessor (b) allows the copy-in procedure to do quick checks as to whether the data has indeed changed and therefore to avoid data duplication at a finer level of granularity than might be possible for the difference engine in some other storage pool providing input to a CAS.
- the new object will be identical to the previous version except where the input data is provided and itself differs from the previous version.
- the algorithm for the copy-in operation is illustrated in a flowchart at FIG. 15 .
- the process starts at step 1500 as an arbitrarily-sized data object in the temporal store is provided, and proceeds to 1502 , which enumerates any and all hashes (depth-0 through the highest level) referenced by the hash value in the predecessor object, if such is provided. This will be used as a quick check to avoid storing data that is already contained in the predecessor.
- step 1504 if a predecessor is input, create a reference to a clone of it in the content-addressable data store temporal data structure. This clone will be updated to become the new object. Thus the new object will become a copy of the predecessor modified by the differences copied into the CAS from the copying source pool.
- the Data Mover 502 pushes the data into the CAS.
- the data is accompanied by an object reference and an offset, which is the target location for the data.
- the data may be sparse, as only the differences from the predecessor need to be moved into the new object.
- the incoming data is broken into depth-0 chunks sized small enough that each can be represented by a single depth-0 hash.
- the data hash module generates a hash for each depth-0 chunk.
- step 1512 read the predecessor hash at the same offset. If the hash of the data matches the hash of the predecessor at the same offset, then no data needs to be stored and the depth-1 and higher objects do not need to be updated for this depth-0 chunk. In this case, return to accept the next depth-0 chunk of data.
- This achieves temporal deduplication without having to do expensive global lookups.
- the source system is ideally sending only the differences from the data that has previously been stored in the CAS, this check may be necessary if the source system is performing differencing at a different level of granularity, or if the data is marked as changed but has been changed back to its previously-stored value. Differencing may be performed at a different level of granularity if, for example, the source system is a snapshot pool which creates deltas on a 32 KiB boundary and the CAS store creates hashes on 4 KiB chunks.
- the data may be hashed and stored. Data is written starting at the provided offset and ending once the new data has been exhausted. Once the data has been stored, at step 1516 , if the offset is still contained within the same depth-1 object, then depth-1, depth-2 and all higher objects 1518 are updated, generating new hashes at each level, and the depth-0, depth-1 and all higher objects are stored at step 1514 to a local cache.
- step 1520 if the amount of data to be stored exceeds the depth-1 chunk size and the offset is to be contained in a new depth-1 object, the current depth-1 must be flushed to the store, unless it is determined to be stored there already. First look it up in the global index 1116 . If it is found there, remove the depth-1 and all associated depth-0 objects from the local cache and proceed with the new chunk 1522 .
- step 1524 as a quick check to avoid visiting the global index, for each depth-0, depth-1 and higher object in the local cache, lookup its hash in the local store established in 1502 . Discard any that match.
- step 1526 for each depth-0, depth-1 and higher object in the local cache, lookup its hash in the global index 1116 . Discard any that match. This ensures that data is deduplicated globally.
- step 1528 store all remaining content from the local cache into the persistent store, then continue to process the new chunk.
- Reading an object out of the CAS is a simpler process and is common across many implementations of CAS.
- the handle for the object is mapped to a persistent data object via the global index, and the offset required is read from within this persistent data. In some cases it may be necessary to recurse through several depths in the object handle tree.
- the Replicator 1110 is a service to duplicate data objects between two different content addressable stores.
- the process of replication could be achieved through reading out of one store and writing back into another, but this architecture allows more efficient replication over a limited bandwidth connection such as a local- or wide-area network.
- a replicating system operating on each CAS store uses the difference engine service described above together with the temporal relationship structure as described in FIG. 13 , and additionally stores on a per-object basis in the temporal data structure used by the CAS store a record of what remote store the object has been replicated to. This provides definitive knowledge of object presence at a certain data store.
- the system uses the temporal data structure to determine which objects exist on which data stores. This information is leveraged by the Data Mover and Difference Engine to determine a minimal subset of data to be sent over the network during a copy operation to bring a target data store up to date. For example, if data object O has been copied at time T3 from a server in Boston to a remote server in Seattle, Protection Catalog Store 908 will store that object O at time T3 exists both in Boston and Seattle. At time T5, during a subsequent copy from Boston to Seattle, the temporal data structure will be consulted to determine the previous state of object O in Seattle that should be used for differencing on the source server in Boston. The Boston server will then take the difference of T5 and T3, and send that difference to the Seattle server.
- the process to replicate an object A is then as follows: Identify an object A0 that is recorded as having already been replicated to the target store and a near neighbor of A in the local store. If no such object A0 exists then send A to the remote store and record it locally as having been sent.
- a typical method as embodied here is: send all the hashes and offsets of data chunks within the object; query the remote store as to which hashes represent data that is not present remotely; send the required data to the remote store (sending the data and hashes is implemented in this embodiment by encapsulating them in a TCP data stream).
- FIG. 16 shows the software and hardware components in one embodiment of the Data Management Virtualization (DMV) system.
- the software in the system executes as three distributed components:
- the Host Agent software 1602 a , 1602 b , 1602 c implements some of the application-specific module described above. It executes on the same servers 1610 a , 1610 b , 1610 c as the application whose data is under management.
- the DMV server software 1604 a , 1604 b implements the remainder of the system as described here. It runs on a set of Linux servers 1612 , 1614 that also provide highly available virtualized storage services.
- Management Client software 1606 that runs on a desktop or laptop computer 1620 .
- Data Management Virtualization systems communicate with one another between primary site 1622 and data replication (DR) site 1624 over an IP network such as a public internet backbone.
- DR data replication
- the DMV systems at primary and DR sites access one or more SAN storage systems 1616 , 1618 via a fibre-channel network 1626 .
- the servers running primary applications access the storage virtualized by the DMV systems access the storage via fibre-channel over the fibre-channel network, or iSCSI over the IP network.
- the DMV system at the remote DR site runs a parallel instance of DMV server software 1604 c on Linux server 1628 .
- Linux server 1628 may also be an Amazon Web Services EC2 instance or other similar cloud computational resource.
- FIG. 17 is a diagram that depicts the various components of a computerized system upon which certain elements may be implemented, according to certain embodiments of the invention.
- the logical modules described may be implemented on a host computer 1701 that contains volatile memory 1702 , a persistent storage device such as a hard drive, 1708 , a processor, 1703 , and a network interface, 1704 .
- the system computer can interact with storage pools 1705 , 1706 over a SAN or Fibre Channel device, among other embodiments.
- FIG. 17 illustrates a system in which the system computer is separate from the various storage pools, some or all of the storage pools may be housed within the host computer, eliminating the need for a network interface.
- the programmatic processes may be executed on a single host, as shown in FIG. 17 , or they may be distributed across multiple hosts.
- the host computer shown in FIG. 17 may serve as an administrative workstation, or may implement the application and Application Specific Agent 402 , or may implement any and all logical modules described in this specification, including the Data Virtualization System itself, or may serve as a storage controller for exposing storage pools of physical media to the system.
- Workstations may be connected to a graphical display device, 1707 , and to input devices such as a mouse 1709 and a keyboard 1710 .
- the active user's workstation may include a handheld device.
- FIG. 18 illustrates a method for generating a data fingerprint for an object stored in a virtual storage pool, according to certain embodiments of the invention.
- a data fingerprint is a short binary digest of a data object that may be generated independently regardless of how the data object is stored, and is identical when generated multiple times against identical input data with identical parameters.
- Useful properties for the fingerprint are that it be of fixed size, that it be fast to generate for data objects in all storage pools, and that it be unlikely that different data objects have identical fingerprints.
- a data fingerprint is different from a checksum or a hash. For example, a fingerprint is taken for only a sample of the object, not the whole object. Obtaining a binary digest of a small percentage of the data object is sufficient to provide a fingerprint for the whole data object. Since a data fingerprint only requires reads and computes on a small percentage of data, such fingerprints are computationally cheap or efficient compared to a checksum or hash.
- data fingerprints are also different in that a single data object may have multiple fingerprints. Over the life of a data object, multiple fingerprints are stored with the object as metadata, one per generation of the data object. The multiple fingerprints persist over multiple copies and generations of the data object.
- Data fingerprints may be used to compare two objects to determine whether they are the same data object. If the data fingerprints for two objects differ, the two objects can definitively be said to be different. As with checksums, data fingerprints may thus be used to provide a measure or test of data integrity between copied or stored versions of a data object. Two data objects with the same data fingerprint may not necessarily be the same object.
- data fingerprints may be used to compare two objects with increasing reliability.
- a fingerprint match on a subsequent revision increases confidence that all the previous copies were accurate. If a fingerprint does not match, this indicates that either this copy or previous copies were not accurate.
- a new fingerprint may be computed and validated against the corresponding fingerprint for that generation or revision.
- taking a data fingerprint may include taking a checksum or binary digest of a portion of each image. Comparing the two data objects based on a single portion of each image would not necessarily indicate that they are the same image. However, if multiple portions of the two images are identical, it is possible to conclude with increased certainty that the two images are the same image.
- the calculation of a data fingerprint may require a selection function, which may be dynamic, that selects a subset or portion of the input data object. Any such function may be used; one specific example is described below in connection with certain embodiments.
- the function may select small portions of the data object that are spread out throughout the entirety of the data object. This strategy for selecting portions of data is useful for typical storage workloads, in which large chunks of data are often modified at one time; by selecting a relatively large number of non-contiguous portions or extents of data that are widely distributed within the data object, the selection function increases the probability that a large contiguous change in the data object may be detected.
- the function may change over time or may base its output on various inputs or parameters.
- the choice of a selection function should ideally be done with an awareness of the content of a data object. Portions of the data object that are likely to change from generation to generation should be included in the fingerprint computation. Portions of the data object that are static, or tend to be identical for similar objects should not be included in the fingerprint. For example, disk labels and partition tables, which tend to be static should not generally be included in the fingerprint, since these would match across many generations of the same object. The tail end of a volume containing filesystems often tend to be unused space; this area should not be used in the computation of the fingerprint, as it will add computational and IO cost to the fingerprint, without increasing its discriminating value.
- the selection function may balance the goal of increased probability of detecting changes with the goal of providing a consistently-fast fingerprinting time. This tradeoff is expressly permitted, as the disclosed system allows for multiple data fingerprints to be taken of the same data object. Multiple fingerprints can provide the increased error-checking probability as well, as when the number of fingerprints becomes large, the number of un-checked bytes in the data object decreases to zero.
- a data fingerprinting function may operate as follows, in some embodiments.
- a data object, 1810 is any file stored within any virtual storage pool, for example a disk image stored as part of a data protection or archiving workflow.
- Start, 1820 is a number representing an offset or location within the file.
- Period, 1830 is a number representing a distance between offsets within the file.
- Data Sample, 1840 is a subset of data from within the data object.
- Chunk checksums, 1850 are the result of specific arithmetic checksum operations applied to specific data within the file.
- the data fingerprint, 1860 is a single numerical value derived deterministically from the content of the data object 1810 and the parameters start 1820 and period 1830 . Other parameters and other parametrized functions may be used in certain embodiments.
- the data samples 1840 are broken into fixed length chunks, in this illustration 4 KB.
- a chunk checksum 1850 is calculated for the data stream, where the checksum includes the data in the chunk and the SHA-1 hash of the data in the chunk.
- One checksum algorithm used is the fletcher-32 method (http://en.wikipedia.org/wiki/Fletcher's_checksum). These chunk checksums are then added together modulo 2 64 , and the arithmetic sum of the chunk checksums is the data fingerprint 1860 , parameterized by Start and Period.
- Other methods for combining the plurality of hash values or checksums into a single hash value may be contemplated in certain embodiments of the invention.
- a single hash value is preferred for simplicity. It is not necessary for the single hash value to reveal which data subsets were used in producing the chunk checksums.
- a data fingerprint may be performed using other functions that focus on interesting sections of a data object, where certain sections are determined to be interesting using various means.
- Interesting sections may be sections that are determined to change frequently, or that are likely to change frequently.
- a priori information about the content of the data object or the frequency of change of parts of the data object may be used. For example, when the system detects that a data object is a disk image, the system may ignore the volume partition map, as the partition map rarely changes. As another example, if the system knows that it is storing a Microsoft Word document, and that the headers of the document are unlikely to change, it may designate the body and text areas of the document as “interesting,” and may choose to fingerprint those areas.
- Fingerprinting an “interesting” area may be performed in a manner similar to FIG. 18 , in some embodiments, where the data samples are chosen by first identifying interesting data areas and then identifying areas to sample within the interesting data areas using an algorithm that generates a sparse subset of the interesting data areas.
- the described fingerprinting algorithm has a very small overhead, and thus fingerprinting may be performed often. However, in cases such as when a pool includes offline tapes, fingerprinting all data may not have a reasonable overhead.
- FIG. 19 illustrates how the data fingerprint is used for assurance of accuracy in copy operations, according to certain embodiments of the invention.
- an additional operation is defined: that of generating a fingerprint for a data object, given a set of parameters (operation 1930 ). Every data object that is cataloged is fingerprinted and the fingerprint is stored with all other metadata.
- Object Manager 501 may make a request for a fingerprint on a data object to each pool.
- the first fingerprint is generated at the first storage-optimized pool or snapshot pool and stored in the catalog store.
- the data movement requestor 912 After a data object is first copied into the Performance Optimized Pool 508 using the lightweight snapshot operation, the data movement requestor 912 generates a set of parameters for a fingerprint, and uses them to request a fingerprint (operation 1910 ) from the object manager 501 .
- the object manager requests a fingerprint from the performance optimized pool (operation 1940 ).
- the performance optimized pool is capable of generating the fingerprint.
- every pool managed by pool manager 504 is capable of generating a fingerprint.
- the new fingerprint is stored into the protection catalog store 908 , along with other metadata for the object as described above (operation 1930 ).
- the fingerprint is requested from the target pool for the target object (operation 1930 , operation 1960 ). Once generated, the stored fingerprint is then passed on to each subsequent pool, where the newly calculated fingerprint is then verified against the stored fingerprint to assure that copying errors have not occurred. Each subsequent pool may calculate the fingerprint again and validate the calculated fingerprint against the stored fingerprint.
- the data object 1810 is sampled at regular intervals defined by Start 1820 and Period 1830 parameters. Each sample is a fixed size, in this illustration 64 KB.
- the parameter Period is chosen such that it is approximately 1/1000 of the size of the data object, and Start is chosen between 0 and Period according to a pseudo-random number generator.
- the start parameter may be modified, resulting in a data fingerprint of a different region of the data object.
- the object size changes only in certain circumstances. If the object size stays constant the period stays constant. If the object size changes the period will change as well. A period of 1/1000 (0.001) or another small fraction may be selected to ensure that calculating a fingerprint will take a small time and/or a constant time. Note that depending on the function used to generate the subset of the data object used for the data fingerprinting operation, other parameters may be modified instead of the start parameter. The result is to cause the data fingerprint to be generated from a different region of the data object, such that cumulative data fingerprints result in fingerprinting of an increasing proportion of the data object over time.
- Multiple generations of a data object may be created as a result of interactions with service level agreements (SLAs), as described elsewhere in the present disclosure. For example, given a SLA that schedules a snapshot operation once every hour, an additional generation of a data object will be created every hour. For each additional generation, a new data fingerprint is created and sent. If the data object has not changed from the previous generation to the current generation, the data itself need not be sent, but a fingerprint is sent to the target data pool regardless, to incrementally increase the probability that the sparse data fingerprinting operation has captured all changes to the data throughout the data object.
- SLAs service level agreements
- the fingerprint operation may be supported by one or more storage pools in the system.
- the pools are brokered by the operation manager such as Pool Request Broker 602 .
- the fingerprint operation is supported by all pools.
- Fingerprinting remains with the metadata for the lifetime of the data object. This allows fingerprinting to also be used during restore as well as during copy or other phases of data storage, access and recovery, which provides true end-to-end metadata from a data perspective. Fingerprinting during restore is performed as follows. When a restore operation is requested by Object Manager 501 , a fingerprint operation may take place on the restored data. This fingerprint operation may take place before or after the restore operation. By using the fingerprint operation, all previously-stored revisions of the data object are used to verify the currently-restored copy of the data, according to the fingerprint verification method described above. This leverages incremental knowledge in a way different from that of typical I/O path CRC protection.
- each copy of an object between virtual storage pools is incremental, transferring only data from the source object known to be absent in the target pool. It follows from this that any errors in copying in one generation of an object will still be present in subsequent generations. Indeed such errors may be compounded.
- the use of a data fingerprint provides a check that copies of an object in different virtual storage pools have the same data content.
- the choice of data fingerprint method also controls the level of confidence in the check: as the Period ( 1830 ) is made smaller, the cost of generating the fingerprint goes up, as more data needs to be read from the pool, but the chance of generating a matching fingerprint despite the data containing copying errors decreases.
- FIG. 20 illustrates an improved method for data backup and restore using the Object Manager and Data Mover in some embodiments, where copies of the object to be moved already exist in more than one virtual storage pool.
- hybrid seeding The procedure described below is called hybrid seeding, and is named according to the fact that the data is copied, or “seeded,” to a plurality of storage pools, which are later used to copy the data to another pool.
- the greatest value is when there is a large amount of data to be transferred.
- the cost for performing this method is low, so this method may be used for both large and small transfers.
- Performance Optimized Pool 2010 is a virtual storage pool with the property that retrieving data and metadata from an object in the pool is quick, for example a snapshot pool.
- Capacity Optimized Pool 2020 is a virtual storage pool with slower data- and metadata retrieval characteristics, for example a content addressable store.
- Target Pool 2030 is a virtual storage pool that is a target for copying data from pool 2020 , for example, a content addressable store on a remote system.
- Each pool has different performance characteristics: the capacity optimized pool performs well at performing differences and the performance optimized pool performs well at retrieving bulk data. If one pool performs better than another pool at a particular task, it may be said to have relatively high performance at that task relative to the other pool.
- Object A2, 2040 is a data object that has previously been copied into storage pool 2020 .
- Object A3, 2050 is the result of a previous copy of object A2 into storage pool 2030 .
- Object B1 2060 is a data object in storage pool 2010 that is the result of changes to an object A, that is, it is a newer generation or version of A.
- Object B2, 2070 is the result of a previous copy of object B1 into storage pool 2020 .
- Object B3, 2080 is the intended result of a copy of object B2 into target pool 2030 .
- Object Manager 501 acts as a controller to direct a command to copy data objects to the data stores which contain the data objects.
- the copy operation for Object B2, 2070 is optimized using hybrid seeding.
- the Difference Engine 614 is instructed by the Object Manager to compute differences between objects A2 and B2 prior to copying the differences to target pool 2030 to be applied to an object there.
- This logic may reside in storage pool 2020 , or may also reside in Object Manager 501 , or may reside elsewhere in the storage virtualization system. Since A2 has previously been copied to storage pool 2030 as A3, and since B2 has previously been copied from storage pool 2010 to pool 2020 as B1, the result of operation 2100 is also a delta of object A3 and B1.
- This delta or difference set may be characterized as a type of differences specification.
- the delta of A3 and B1 (not shown) is requested by the Object Manager to be copied from storage pool 2010 to storage pool 2030 , to be applied to object A3 in accordance with differences computed previously at storage pool 2020 .
- the copy may be performed via direct connection between storage pool 2010 and storage pool 2030 , in some embodiments.
- This division of the differencing operation and the bulk data copy operation results in a higher-performance differencing operation at capacity-optimized pool 2020 than would have been possible at performance-optimized pool 2010 .
- the division also results in a higher-performance copy from performance-optimized pool 2010 than would have been possible had the copy been performed from capacity-optimized pool 2020 .
- the logic for each storage pool may be provided by a single centralized controller, in which case the messaging described here may occur within the controller; in other embodiments, logic for one or more storage modules may be executed on computing resources at the storage pools themselves. Copy operations may be requested by the Data Mover or by capacity-optimized pool 2020 or other pools in some embodiments.
- This method is applicable at least under the following preconditions: Object A has identical copies in two pools, copy A2 ( 2040 ) in a capacity optimized pool 2020 and copy A3 ( 2050 ) in another target capacity optimized pool 2030 . These may for example be the result of previously copying an object A from storage pool 2010 via pool 2020 to pool 2030 .
- Object B is a newer version of A, and has identical copies in two pools, copy B1 in the first performance optimized pool 2010 and copy B2 in the capacity optimized pool 2020 .
- B has been copied already from pool 2010 to pool 2020 as a backup.
- Object B is to be copied from the first capacity optimized pool 2020 to the second, the target pool 2030 .
- the retrieval time for data from the first pool 2010 is much better than that for the second pool 2020 .
- pool 2010 is based on enterprise-class primary storage, while pool 2020 is a lower cost or higher latency device more suited to archiving or backup.
- the copy is executed by invoking the differencing engine in pool 2020 to provide a set of differences between A2 and B2.
- the differences generated by the difference engine are a description of the changed sections of the object, not the data themselves.
- the differences may include a set of (offset, length) pairs describing extents within the data object that have changed.
- the differences between sections would be generated by reading from object B2 and object B3, and then applying the differences to object B3 to achieve the required copy.
- the differences are generated by the difference engine in pool 2020 , but the data are read from the object in pool 2010 , that is, the sections are read from object B1 and B2 and then applied to object B3 to achieve the required copy.
- a similar method may also be used during restore operations.
- differencing and bulk data copy may be separated, and bulk copying may be performed from the data store that is fastest, rather than from the store that performs the differencing operation.
- this separation of operations by pool constitutes a hybridization of operations. Different operations are performed at different storage pools, and these operations are combined into a single operation by the virtualization layer. Further such applications may exist. For example, comparing two objects in the performance pool may be a difficult task, but the comparison operations may be performed at two content-addressable pools containing the same data objects.
- FIG. 21 illustrates a mechanism for data replication for Disaster Recovery and Business Continuity according to some embodiments.
- Replication and failover are well-understood operations in which business logic and data are maintained at a hot backup at a remote site. Failover transfers operation over to the remote site. Replication transfers data over to the remote site.
- a new replication method is described below by which a pipeline of storage pools is used in conjunction with data management virtualization to reduce the amount of data that gets transferred. This method also enables bidirectional continuous deduplicated replication.
- Business continuity and disaster recovery are well established practices in the IT industry. For operation they depend on having data from a primary location replicated to a secondary location regularly.
- the replica at the secondary location must be consistent, that is, it must represent a state of the data as it was at some moment in time at the primary location such that it can be used to start a secondary application server at the secondary location.
- the replica needs to be quick to access, so that a secondary application server can be started up using the data very quickly in the event of the primary application server becoming unavailable.
- the replica needs to be low-latency, i.e., when data is made available to an application server at the secondary location it should represent a consistency point at a time on the primary server that is as recent as possible, typically measured in minutes.
- Sync-back is the operation that is performed when data is transferred back from the backup site to the main site. In other words, the direction of the data copy arrow points in the opposite direction.
- Sync-back supports and enables fail-back, which is an operation that reverses the fail-over and transfers operation back to the primary server.
- Primary Location 2100 is a location where a business supplies application or data services.
- Primary Application Server 2101 is a server at location 2100 , representative of one or more such servers, delivering business services that may be consumed locally or remotely over some network interface.
- Standby Location 2110 is a location where the business may alternatively supply application or data services, in some event that causes disruption to the availability of service at the primary location. For example, a power outage at the primary location might cause a web server to be unavailable, in which case a web server at the backup location could be configured to respond to the requests that would normally be directed to the primary location.
- Primary Data A, 2120 is the live data being read and written for the operation of the primary application server. This might for example include a database that is servicing transactions for a web application interface. In a preferred embodiment, this is a LUN exported to a the primary application server over a storage network. In another preferred embodiment, this is a disk image for a virtual machine.
- Primary Pool 2130 is the primary storage resource pool from which storage for the operation of the primary application server is allocated. This would typically be an enterprise class SAN storage array.
- Performance Optimized Pool, 2131 is a storage pool for data protection as described previously which supports the lightweight snapshot operation and differencing. In a preferred implementation this is a snapshot pool based on low cost networked storage.
- Capacity Optimized Pool, 2132 is a storage pool that supports the differencing operation. In a preferred implementation this is a deduplicating content addressable store.
- Capacity Optimized Pool, 2133 is a storage pool at the standby location, that in turn supports the differencing operation. Again, in a preferred implementation this is a deduplicating content addressable store.
- Performance Optimized Pool, 2134 is a storage pool at the standby location that has faster access times than is typically the case for a content addressable store. In a preferred implementation, this is a snapshot pool based on low cost networked storage.
- Primary Pool 2135 is a storage pool from which storage can be allocated for execution of a standby application server. This could, for example, be an enterprise class SAN storage array.
- Copies A1 2121 , A2 2122 , A3 2123 , A4 2124 , A5 2125 of the primary data object A are exact data copies to be created within each of the storage pools as described.
- the sequential copy operations 2140 will be described below in greater detail. These operations are issued by the Service Policy Engine 501 and brokered to each storage pool by the Object Manager 501 as described previously.
- Logic implementing the Service Policy Engine, Object Manager, and other controllers may be implemented in a centralized server or may be distributed across the network.
- the purpose of this method of combining operations and components as described previously is to meet these goals for business continuity, while reducing the load on a network connection, and not requiring any dedicated network bandwidth for business continuity in addition to replication for data protection. Stated differently, the purpose may be to provide the effect of asynchronous mirroring of data from a primary location to a second location.
- the Service Policy Engine is responsible for marshalling operations in sequence such that virtual copies of Primary Data A are created in each pool in the sequence. As the Service Policy Engine issues copy requests to the Object Manager, the Object Manager brokers these requests to lightweight snapshot or efficient incremental copy operations between pairs of pools.
- the first operation executed is to make a lightweight snapshot of the current state of Primary Data A;
- the second operation executed is to copy just the changed extents within Snapshot A1 into the Capacity Optimized Pool, generating a new content-addressed object A2;
- the third operation is to use efficient replication between content addressable stores to generate a new content addressed object A3 with minimized data transfer due to data deduplication in the first capacity-optimized pool;
- the fourth operation is to apply just the changes between A3 and its previous revision in order to update a previous object in the second Performance Optimized Pool to A4.
- capacity-optimized pool 2133 at the standby location may receive metadata from capacity-optimized pool 2132 at the primary location, and data from performance-optimized pool 2131 at the primary location, thus providing faster throughput for the bulk data transfer.
- capacity-optimized pool 2133 may receive data from the primary location and may immediately send it to performance-optimized pool 2134 .
- one or both of capacity-optimized pool 2132 or performance-optimized pool 2131 at the primary location may send the metadata and data to the remote location, with both capacity-optimized pool 2133 and performance-optimized pool 2134 as the destination.
- Designating performance-optimized pool 2134 as the destination and immediately copying the data to performance-optimized pool 2134 provides updated data to the remote performance pool in as short a time as possible, reducing the time window between backups during which data loss can occur, and allowing the remote failover location to resume operation with as little lost data as possible.
- the remote performance-optimized pool Since the remote performance-optimized pool always has at least one complete older copy of the data on the original system, access to data on the remote pool can be provided near-instantaneously.
- the data on the remote site does not depend on the data at the local site, and is stored in a native format that is readily usable by the business application.
- the data on the remote performance-optimized pool In the event of a failover, the data on the remote performance-optimized pool is available within a finite length of time, and the length of time is independent of the size of data stored and of the latency or availability of the data link between the local site and the remote site. Since data is stored in a native format at the remote site, it is possible to copy data directly in a native format between the local site and the remote site, in some embodiments.
- these virtual copy operations are scheduled to execute successively or serially with a delay or with no delay between them; the entire sequence to be initiated at a regular interval which may be selected by an operator to vary from minutes to hours.
- the entire sequence is programmed to recommence as soon as it completes.
- the operations are pipelined tightly so operations near the start of the sequence overlap operations near the end of the sequence. This reduces latency while incurring the greatest resource consumption.
- system operates in a parallel fashion, so that multiple operations may occur simultaneously.
- the standby application server In the event that the Primary Application Server becomes damaged or otherwise unavailable, the standby application server must be brought into operation with known-good data from the primary side that is as recent as possible.
- a virtual copy is made of the most recent data object in the standby Performance Optimized Pool, to obtain a data object which can once again be used as a disk image or logical unit number (LUN) that may be referenced and modified by the standby application server.
- the virtual copy is made into a standby primary pool using the lightweight copy operation.
- the standby site is not as well provisioned with resources or connectivity as the primary site, so once the primary site is available once more it is preferred that the business service in question be once again provided at the primary site. However any changes made to data while the secondary site was providing the business service must themselves be replicated back to the primary site.
- B4 a space efficient snapshot of B5, the modified version of A5, is made in the standby performance optimized pool 2134 .
- B2 a copy of B3, is made in the primary capacity optimized pool by minimal transfer of deduplicated data.
- Bi in the primary performance optimized pool is generated efficiently by efficient copy using the difference engine.
- Bi is then available for an operator to restore to the primary application server in a variety of ways.
- a virtual copy of B ⁇ is made using a lightweight snapshot operation and exposed as a LUN or disk image to the primary application server.
- the data from B ⁇ is fully copied over LUN or disk image A.
- a virtual copy of B ⁇ is made using a lightweight snapshot operation and exposed as a LUN or disk image to a new primary application server which may be a physical server or virtual machine.
- This method effectively allows the failover/replication site primary pool 2135 to be a high-performance data store providing instant access or very short access time and efficient and recent transfer of primary pool 2130 data, while still providing the benefits of the data management virtualization system.
- the optimization is enabled by the use of the intelligent deduplication and other methods described above.
- the efficient object replication between two Content-Addressable Stores ( 610 ) can be facilitated by the choice of a near neighbor object that is specified to the difference engine provider ( 614 ) along with the object to be replicated.
- the near neighbor if it exists, lies within the same temporal tree as does the object being replicated.
- the least efficient replication workflow is often one in which no near neighbor object has been presented for differencing yet some or most of the data in the object being replicated are present on the remote system.
- This workflow occurs in at least two practical scenarios.
- the first scenario is one in which a given object has been deduplicated remote and is having “Replication for Business Continuity” (see, e.g., FIG. 21 ) applied to it for the first time.
- the system creates a new temporal tree when performing “Replication for Business Continuity,” a tree which may in turn refer to some or all of the same hashes referred to by a temporal tree. There is no near neighbor in this scenario.
- the second scenario is one in which the appropriate near neighbor object has been expired on the remote deduplication store, but some or all of the near neighbor object's data is still present on the remote deduplication store.
- the near neighbor may have been mistakenly erased, or erased by policy (see, e.g., the service level policy shown in FIG. 7 ) in order to conserve disk storage.
- some or all of the underlying hashes could still be present on the remote system.
- both the mark and sweep phases of garbage collection can be time consuming (perhaps many days or weeks). Since in most cases no storage is returned for use until the sweep phase begins, it is useful to know prior to executing the “mark” phase of garbage collection, approximately what the payoff would be if a mark and sweep process were to be executed (e.g., how much storage space is currently consumed by unreferenced data stored in the deduplicating store).
- Embodiments of the techniques disclosed herein can consist of (a) a set of statistics pertinent to a content-addressable deduplicating store (b) a set of operations for calculating and maintaining those statistics (c) a set of formulae that can be applied to the statistics to yield particular business value.
- a set of statistics pertinent to a content-addressable deduplicating store (b) a set of operations for calculating and maintaining those statistics
- a set of statistics pertinent to a content-addressable deduplicating store (b) a set of operations for calculating and maintaining those statistics (c) a set of formulae that can be applied to the statistics to yield particular business value.
- a set of statistics pertinent to a content-addressable deduplicating store (b) a set of operations for calculating and maintaining those statistics
- c) a set of formulae that can be applied to the statistics to yield particular business value for example
- FIG. 22 is an exemplary table defining new statistics that can be calculated according to some embodiments.
- the table includes 40001 LOGICAL_ACCUMULATED_DIFFS (or “LAD”), 40002 LOGICAL_DIVESTED_DIFFS (or “LDD”), 40003 TOTAL_CHUNKS_INDEXED (or “TCI”), and 40004 PHYSICAL_SPACE_USED (or “PSU”).
- These statistics can be numerical values that may be stored in a persistent database, or in a system memory, and may be queried at any time to make calculations about the cost and value of a potential garbage collection (“GC”) mark and sweep phase.
- GC potential garbage collection
- these statistics are stored as persistent metadata on the same storage medium as the deduplicating store itself, and are updated transactionally whenever a change is committed to the store.
- LAD LOGICAL_ACCUMULATED_DIFFS
- LOGICAL_DIVESTED_DIFFS is the amount by which LOGICAL_ACCUMULATED_DIFFS is known to have been reduced by operations that remove one or more objects from the deduplicating store.
- LDD LOGICAL_DIVESTED_DIFFS
- PSU PHYSICAL_SPACE_USED
- FIG. 23A illustrates an exemplary diagram of a temporal tree structure, according to some embodiments.
- FIG. 23A shows the temporal tree structure by which generations of objects in the deduplicating store are tracked.
- the temporal tree structure includes the empty or null object 41001 , object 41002 (A), object 41003 (B), object 41004 (C), object 41005 (D), and object 41006 (E).
- object 41002 A
- object 41003 B
- object 41004 C
- object 41005 D
- object 41006 E
- FIG. 23A illustrates an exemplary diagram of a temporal tree structure, according to some embodiments.
- FIG. 23A shows the temporal tree structure by which generations of objects in the deduplicating store are tracked.
- the temporal tree structure includes the empty or null object 41001 , object 41002 (A), object 41003 (B), object 41004 (C), object 41005 (D), and object 41006 (E).
- LAD is the sum of the count of chunks that are different.
- 41001 is the empty or null object, it contains no data chunks.
- 41006 (E) object contains chunks described as H 1 , H 2 , H 3 , H 6 . It shares parent (B) with 41004 (C). The set of corresponding differences from its parent (B) is ⁇ H 6 ⁇ and the count of differences is 1.
- FIG. 23B is a diagram of statistic 40001 LAD, according to some embodiments.
- LAD is an approximation of the total data stored into the deduplicating store. It is not an underestimate, since it counts every unique data chunk at least once. In fact it is generally an overestimate, since many data chunks are counted twice; for example if the same data chunk is used in two unrelated objects; or if the same data chunk is used at two different locations in the same, or subsequent, generations of a particular object; or if a data chunk is used in one generation of an object, not used in a subsequent generation, then reused in a later generation. It is a reasonable approximation to suppose that the overestimation is a constant factor, and can be denoted ⁇ .
- FIG. 24 is an exemplary diagram of calculating LOGICAL ACCUMULATED DIFFS, according to some embodiments.
- FIG. 24 builds object 42002 (F) onto the temporal tree structure in FIG. 23A , and the resulting accumulated differences 42003 from adding object 42002 (F).
- LAD may be evaluated at any time by walking the temporal object tree and using a differencing engine to compute the differences between subsequent generation of objects. However on a large store such a calculation may be comparable in time to a GC mark phase. More practical, and as implemented in some embodiments, the LAD statistic may be calculated once and updated as objects are added or removed from the deduplicating store.
- 42001 adds object F ( 42002 ) as a child of object E ( 41006 ).
- object F is a newer generation of object E, and contains a small amount of change: H 2 becomes H 5 at the second offset, and H 4 becomes H 6 at the fourth offset.
- 42003 depicts accumulated differences computed as object F is created. H 5 and H 6 are identified as new content during the creating of object F, so the accumulated difference count is 2. As a result, the LAD from FIG. 23B of 7 is incremented by 2, resulting in a LAD of 9.
- the LAD statistic can be incremented by 1 each time the step 1512 does not find a match (e.g., each time the edge of the graph between 1512 and 1516 is traversed).
- FIG. 25A is an exemplary diagram of removing an object from a temporal tree structure, according to some embodiments. Note that the process shown in FIG. 25A can be adapted if there are different numbers of hash values in each node; for example, if F only has H1, H2, and G has H1, H2, H3 and H4, and H has H3, H6. If there are different numbers of hash values, in some embodiments the nodes with a smaller set of hash values can be padded with additional hash values with a predetermined value (e.g., zeros) to increase the number of hash values to equal the number of hash values in the larger sets. Continuing with the example above, F and H can be padded with zeros to match the length of G.
- a predetermined value e.g., zeros
- object F is an object in a deduplicating store, in this example made up of data chunks H 1 , H 5 , H 3 , H 6 .
- object G is an immediate successor to object F, which differs from object F in having data chunk H 7 at offset 3 and H 8 at offset 4.
- object H is an immediate successor to object G, which differs from object G in having data chunk H 2 at offset 2 and H 9 at offset 4.
- object G is removed at 43008 from being between object F and object H, and LDD is updated appropriately.
- FIG. 25B is an exemplary diagram for calculating the LOGICAL DIVESTED DIFFS statistic, according to some embodiments.
- LDD counts the difference between the monotonically increasing LAD value (e.g., calculated as described in FIG. 24 ), vs. the definition of LAD from FIG. 23B .
- the impact of LAD is computed, and rather than decreasing LAD immediately, the negative change is accumulated in LDD.
- G-F is a difference list, that is a list of data chunks from G that are different from data chunks at the same location in F. This list shows that G-F consists of data chunk H 7 at offset 3 and H 8 at offset 4.
- H-G is a difference list, that is a list of data chunks from H that are different from data chunks at the corresponding location in G. This list shows that H-G consists of data chunk H 2 at offset 2 and H 9 at offset 4.
- 43014 is a list of divested differences, that is, data chunks that were at a certain offset in G but in neither F nor H at that offset. It is computed by comparing the two difference lists G-F and H-G, and keeping those chunks from G-F only where there is a different chunk at the same offset in H-G. The computation of LDD for operation “Remove B” is therefore the number of diffs captured in 43014 . Therefore, if the value changed both times, as with changing to H8 in 43010 to H9 in 43012 , then it is divested.
- the divested diff calculation count can be computed by assuming that the shorter objects be padded to the length of the longer object using a series of repetitions of some fixed content data chunk, such as a chunk containing all zeros.
- FIG. 26A is an exemplary diagram of a calculation of the PHYSICAL SPACE USED (“PSU”) statistic for the tree shown in FIG. 26B according to some embodiments.
- objects A thru H have the same content as discussed in previous figures.
- Each object may contain a mixture of data chunks that are unique or are shared with other objects in the temporal tree, either near neighbors or distant.
- the data chunk corresponding to each hash value (H 1 thru H 9 ) is written only once to a storage medium. Also, as the data is written to a storage medium, it may be compressed using any of several standard or well-known compression tools.
- the statistic PHYSICAL SPACE USED is the sum of the post-compression sizes of the data chunks corresponding to hashes H 1 thru H 9 . Note that in the example shown in FIGS. 26A-26B , PSU incorporates the size of data for H 8 even though object G is marked as deleted. The physical space for H8 will not be reclaimed until a full garbage collection operation, so until that operation H8 still contributes to PSU.
- S reclaimable is the amount of storage that is used but is not referenced.
- LDD is the statistic LOGICAL_DIVESTED_DIFFS as described above.
- LAD is the statistic LOGICAL_ACCUMULATED_DIFFS as described above.
- PSU is the statistic PHYSICAL_SPACE_USED as described above.
- the formula shown above for S reclaimable can be used as a good instantaneous estimate of the amount of storage that could be reclaimed by executing both mark and sweep phases of Garbage Collection.
- each of the nine data chunks H 1 -H 9 can be a terabyte (TB) in size.
- the system may be configured to wait until three (3) or more TBs can be reclaimed before executing the GC operation.
- the error between the correct value and the estimated value may be even larger, but the cost of analyzing the full store to get an accurate value may be so much greater than that of calculating the estimate as to make it impractical.
- the S reclaimable estimate can be used as a threshold to trigger a garbage collection operation.
- the threshold can be evaluated periodically and a garbage collection is then commenced if the threshold exceeds a given fixed value, or a given percentage of PSU.
- the threshold can be set to wait until S reclaimable is greater than three (3) (e.g., such that three or more TBs can be reclaimed before executing the GC operation).
- the growth of (LDD/LAD)*PSU can be used to indicate the growth rate of the raw storage needed to store multiple generations of a set of objects in a deduplicating store, and therefore can be factored into long term plans for making additional storage available to such a system.
- the originating server can provide a reference to a near neighbor object.
- the Synthetic Near Neighbor process creates such a reference object by looking up large portions at a time of the data on the remote server at a time, then creating this reference object artificially on both the originating and destination servers. Once this near neighbor has been created, the replication can proceed much more efficiently, sending only the data that wasn't found during the previous looking up of data. This can greatly reduce the time to replicate where no near-neighbor object has been specified.
- FIG. 27 illustrates a method for creating a near neighbor object for replication synthetically where no existing near neighbor can otherwise be determined.
- this workflow achieved during a discovery message exchange between the local dedup store 2450 and the remote deduplication store 2456 , creates a near neighbor object O SA 2400 for the object to be replicated O R 2401 stored in local dedup store 2450 that consists of greater-depth objects known to be part of the object being replicated and to be present on the remote deduplication store 2456 for the remote device 2458 , at their appropriate offsets from the beginning of the object, interspersed with runs of zero-fill mapping to those extents where the appropriate higher-depth hash was not found on the remote target.
- This method can be configured such that an object consisting of the chosen higher-depth-size (for example, 2 MiB) of zeroes O z 2402 be available on all systems, to serve as a basis for the creation of the zero-fill portions of the synthetic near neighbor object.
- O z can be created on each Content-Addressable Store during its initialization.
- O z can be configured with the same preferred higher-depth size on all installations, and requires no special consideration in its construction, as it is only populated with zeroes.
- the system can be configured to construct a near neighbor with a size that is rounded down to the nearest multiple of the size of O z . This can avoid the need to have pre-created and to continually manage the existence of variable-length higher-depth objects whose lengths are between 1 and the size of the chosen higher-depth object, in order to match hashes at the end of an object whose length is not a whole multiple of the size of the chosen higher-depth object.
- O z is created on each Content-Addressable Store during its installation and initialization (e.g., local dedup store, and remote dedup store in FIG. 27 ).
- the Synthetic Ancestry workflow is invoked.
- the resultant synthetic near neighbor O SA 2400 is substituted into the remote deduplication workflow as if it were either specified as a near neighbor or had been arrived at by negotiation of common near neighbor as described earlier.
- a connection has been already established between the source local dedup store 2450 and the target remote dedup store 2456 (e.g., via the local business process engine 2452 and the remote business process engine 2454 ).
- the local business process engine 2452 creates an empty object on the local dedup store 2450
- the remote business process engine 2454 creates an empty object on the remote dedup store 2456 ; this empty object will be grown in stages to produce the synthetic near neighbor.
- FIG. 28 illustrates a method for creating a synthetic near neighbor for replication synthetically where no existing near neighbor can otherwise be determined.
- the source object is scanned for its constituent greater-depth hashes, which are then grouped into network messages M SA 2403 that are sent to the remote cluster.
- the local business process engine 2452 uses the network messages 2403 to check the existence of hashes with the remote business process engine 2454 at 2405 .
- the remote business process engine 2454 determines for each of the higher-depth hashes sent whether or not it is present on the remote cluster at 2403 . For each hash in the message that is not found the hash of zeroes is substituted in the message.
- the remote device 2458 uses the resulting message to grow the synthetic near neighbor O SA 2400 under construction on the remote dedup store 2456 by the amount (for example, 2 GiB) of data it represents 2501 (shown in FIG. 27 at 2406 ).
- the remote business process engine 2454 then sends the message (the assembled page of found hashes 2407 ) back to the source local dedup store 2450 , where at step 2503 it is similarly used to grow the synthetic near neighbor O SA 2400 under construction on the local dedup store (shown in FIG. 27 at 2408 ).
- step 2409 steps 2403 , 2405 , 2406 , 2407 and 2408 repeat until there are no further hashes.
- the remote replication workflow described above is continued as if the near neighbor (referred to therein as A0) had been discovered rather than synthesized (steps 2410 and 2411 shown in FIG. 27 ).
- the differences between the object O R 2401 being replicated and the synthetic near neighbor are thus transmitted to the remote Content Addressable Store, and the object O R 2401 is assembled thereupon, such that when the flow as described above is completed, object O R 2401 now exists on the remote Content-Addressable Store.
- FIG. 29 illustrates a method for performing a full restore phase for replication synthetically where no existing near neighbor can otherwise be determined.
- the object is read in pieces of a reasonable size 2510 (in one preferred implementation, 64 KiB) with these pieces being written to the target device D OUT 2404 in an efficient manner 2511 .
- a check is made to determine whether it is time to update the progress reporting for this operation. If so, progress is sent from the remote Content-Addressable Store to the local Content-Addressable Store 2512 . Once there is no more data to copy, the operation is complete.
- This method can also incorporate a technique to represent progress.
- Progress reporting can be made available to the user interface so that the user can be aware of it. It is always expressed as a number between 0 and the length of the object being replicated.
- Progress can be represented through two phases of replication at an approximately constant rate.
- the two phases are characterized as synthetic near neighbor construction, and remote replication.
- Progress reporting is often a difficult problem for network communication workflows in which the set of data to be sent is not known in advance. For example, with enterprise customers, for whom replication can take days to complete, the ability to provide meaningful progress reports helps the customer plan their IT operations.
- the techniques described herein can be used to provide accurate progress reporting.
- Q is the size of the object O being replicated.
- L1 f is the number of higher-level hashes found from the replication object O.
- L1 t is the total number of higher-level hashes contained within the replication object O.
- the traditional replication phase is undertaken, with the synthetic near neighbor object being used as a near neighbor to the object being replicated.
- P SA is the full progress previously calculated during the synthetic ancestry phase as described above.
- P merge is the linear progress (from 0 to the length of the replicated object) of the replication phase.
- Q is the size of the object O being replicated.
- O size is the size in bytes of the object to be replicated.
- P RST is the progress reported (between 50% and 100% of the size of the replicated object) of the restore to target device, as calculated by:
- P RR is the linear progress of the restore to remote device, as described above.
- the subject matter described herein can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structural means disclosed in this specification and structural equivalents thereof, or in combinations of them.
- the subject matter described herein can be implemented as one or more computer program products, such as one or more computer programs tangibly embodied in an information carrier (e.g., in a machine readable storage device), or embodied in a propagated signal, for execution by, or to control the operation of, data processing apparatus (e.g., a programmable processor, a computer, or multiple computers).
- a computer program (also known as a program, software, software application, or code) can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file.
- a program can be stored in a portion of a file that holds other programs or data, in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processor of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- Information carriers suitable for embodying computer program instructions and data include all forms of nonvolatile memory, including by way of example semiconductor memory devices, (e.g., EPROM, EEPROM, and flash memory devices); magnetic disks, (e.g., internal hard disks or removable disks); magneto optical disks; and optical disks (e.g., CD and DVD disks).
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD and DVD disks
- optical disks e.g., CD and DVD disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- the subject matter described herein can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, (e.g., a mouse or a trackball), by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well.
- feedback provided to the user can be any form of sensory feedback, (e.g., visual feedback, auditory feedback, or tactile feedback), and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the subject matter described herein can be implemented in a computing system that includes a back end component (e.g., a data server), a middleware component (e.g., an application server), or a front end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described herein), or any combination of such back end, middleware, and front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
Abstract
Description
4. The Copy Engine receives a request to copy data from one data object (the source) to another data object (the destination).
5. If the Virtual Storage Pool in which the destination object will be created contains no other objects created from prior versions of the source data object, then a new object is created in the destination Virtual Storage Pool and the entire contents of the source data object are copied to the destination object; the procedure is complete. Otherwise the next steps are followed.
6. If the Virtual Storage Pool in which the destination object is created contains objects created from prior versions of the source data object, a recently created prior version in the destination Virtual Storage Pool is selected for which there exists a corresponding prior version in the Virtual Storage Pool of the source data object. For example, if a copy of T5 is initiated from a snapshot pool, and an object created at time T3 is the most recent version available at the target, T3 is selected as the prior version.
7. Construct a time-ordered list of the versions of the source data object, beginning with an initial version identified in the previous step, and ending with the source data object that is about to be copied. In the above example, at the snapshot pool, all states of the object are available, but only the states including and following T3 are of interest: T3, T4, T5.
8. Construct a corresponding list of the differences between each successive version in the list such that all of the differences, from the beginning version of the list to the end are represented. Difference both, identify which portion of data has changed and includes the new data for the corresponding time. This creates a set of differences from the target version to the source version, e.g. the difference between T3 and T5.
9. Create the destination object by duplicating the prior version of the object identified in
10. Copy the set of differences identified in the list created in
4. If a version of the data object is identified in
5. If no data object is identified in Step 4, then create a new destination object in the destination Virtual Storage Pool and copy the data from the source data object to the destination data object. The procedure is complete. Otherwise, proceed with the following steps.
6. Create a new destination data object in the Destination Virtual Storage Pool by duplicating the data object identified in
7. Employ the Difference Engine Provider for the source Virtual Storage Pool to obtain the set of differences between the data object identified in
8. Copy the data identified by the list created in Step 7 from the source data object to the destination data object. The procedure is complete.
-
- Garbage Collection navigates the tree in order to reduce the cost of the “mark” phase, as described below
- Replication to a different CAS pool finds a set of near-neighbors in the temporal tree that are also known to have been transferred already to the other CAS pool, so that only a small set of differences need to be transferred additionally
- Optimal-Way for data restore uses the temporal tree to find a predecessor that can be used as a basis for the restore operation. In the CAS temporal tree data structure, children are subsequent versions, e.g., as dictated by archive policy. Multiple children are supported on the same parent node; this case may arise when a parent node is changed, then used as the basis for a restore, and subsequently changed again.
CAS Difference Engine
Claims (13)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/120,368 US9646067B2 (en) | 2013-05-14 | 2014-05-14 | Garbage collection predictions |
US14/120,340 US9563683B2 (en) | 2013-05-14 | 2014-05-14 | Efficient data replication |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361823210P | 2013-05-14 | 2013-05-14 | |
US14/120,368 US9646067B2 (en) | 2013-05-14 | 2014-05-14 | Garbage collection predictions |
US14/120,340 US9563683B2 (en) | 2013-05-14 | 2014-05-14 | Efficient data replication |
Publications (2)
Publication Number | Publication Date |
---|---|
US20140351214A1 US20140351214A1 (en) | 2014-11-27 |
US9563683B2 true US9563683B2 (en) | 2017-02-07 |
Family
ID=50977039
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/120,368 Active 2034-11-16 US9646067B2 (en) | 2013-05-14 | 2014-05-14 | Garbage collection predictions |
US14/120,340 Active 2034-11-09 US9563683B2 (en) | 2013-05-14 | 2014-05-14 | Efficient data replication |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/120,368 Active 2034-11-16 US9646067B2 (en) | 2013-05-14 | 2014-05-14 | Garbage collection predictions |
Country Status (6)
Country | Link |
---|---|
US (2) | US9646067B2 (en) |
EP (1) | EP2997501A2 (en) |
JP (1) | JP2016524220A (en) |
AU (1) | AU2014265979A1 (en) |
CA (1) | CA2912394A1 (en) |
WO (1) | WO2014185974A2 (en) |
Cited By (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9826030B1 (en) * | 2015-06-04 | 2017-11-21 | Amazon Technologies, Inc. | Placement of volume partition replica pairs |
US20180060179A1 (en) * | 2016-08-26 | 2018-03-01 | International Business Machines Corporation | Advanced object replication using reduced metadata in object storage environments |
US10073747B2 (en) | 2016-09-19 | 2018-09-11 | International Business Machines Corporation | Reducing recovery time in disaster recovery/replication setup with multitier backend storage |
US10291699B2 (en) * | 2008-12-22 | 2019-05-14 | Google Llc | Asynchronous distributed de-duplication for replicated content addressable storage clusters |
US10311083B2 (en) * | 2015-02-17 | 2019-06-04 | Cohesity, Inc. | Search and analytics for storage systems |
US10437817B2 (en) | 2016-04-19 | 2019-10-08 | Huawei Technologies Co., Ltd. | Concurrent segmentation using vector processing |
US10452641B1 (en) * | 2015-06-30 | 2019-10-22 | EMC IP Holding Company LLC | Snapshot conscious internal file data modification for network-attached storage |
US10459961B2 (en) | 2016-04-19 | 2019-10-29 | Huawei Technologies Co., Ltd. | Vector processing for segmentation hash values calculation |
US10545863B2 (en) | 2017-09-21 | 2020-01-28 | Toshiba Memory Corporation | Memory system and method for controlling nonvolatile memory |
US10565058B1 (en) * | 2016-03-30 | 2020-02-18 | EMC IP Holding Company LLC | Adaptive hash-based data replication in a storage system |
US10795861B2 (en) | 2018-06-20 | 2020-10-06 | International Business Machines Corporation | Online measurement of potential deduplication efficiency |
US10802922B2 (en) | 2016-08-26 | 2020-10-13 | International Business Machines Corporation | Accelerated deduplication block replication |
US10922280B2 (en) | 2018-04-10 | 2021-02-16 | Nutanix, Inc. | Policy-based data deduplication |
US11176097B2 (en) | 2016-08-26 | 2021-11-16 | International Business Machines Corporation | Accelerated deduplication block replication |
US11269531B2 (en) | 2017-10-25 | 2022-03-08 | International Business Machines Corporation | Performance of dispersed location-based deduplication |
US11469881B2 (en) * | 2018-12-26 | 2022-10-11 | Korea Institute Of Science And Technology | Apparatus and method for forgery prevention of digital information |
US11556562B1 (en) | 2021-07-29 | 2023-01-17 | Kyndryl, Inc. | Multi-destination probabilistic data replication |
US11960504B2 (en) | 2021-09-02 | 2024-04-16 | Bank Of America Corporation | Data replication over low-latency network |
Families Citing this family (62)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8671265B2 (en) | 2010-03-05 | 2014-03-11 | Solidfire, Inc. | Distributed data storage system providing de-duplication of data using block identifiers |
US9054992B2 (en) | 2011-12-27 | 2015-06-09 | Solidfire, Inc. | Quality of service policy sets |
US9838269B2 (en) | 2011-12-27 | 2017-12-05 | Netapp, Inc. | Proportional quality of service based on client usage and system metrics |
US9053216B1 (en) | 2013-08-09 | 2015-06-09 | Datto, Inc. | CPU register assisted virtual machine screenshot capture timing apparatuses, methods and systems |
US20150244795A1 (en) | 2014-02-21 | 2015-08-27 | Solidfire, Inc. | Data syncing in a distributed system |
US10656864B2 (en) * | 2014-03-20 | 2020-05-19 | Pure Storage, Inc. | Data replication within a flash storage array |
US9823842B2 (en) | 2014-05-12 | 2017-11-21 | The Research Foundation For The State University Of New York | Gang migration of virtual machines using cluster-wide deduplication |
US9672165B1 (en) * | 2014-05-21 | 2017-06-06 | Veritas Technologies Llc | Data management tier coupling primary storage and secondary storage |
US9594636B2 (en) | 2014-05-30 | 2017-03-14 | Datto, Inc. | Management of data replication and storage apparatuses, methods and systems |
US9542119B2 (en) * | 2014-07-09 | 2017-01-10 | Toshiba Corporation | Solid-state mass storage media having data volumes with different service levels for different data types |
JP2016057876A (en) * | 2014-09-10 | 2016-04-21 | 富士通株式会社 | Information processing apparatus, input/output control program, and input/output control method |
US9753955B2 (en) | 2014-09-16 | 2017-09-05 | Commvault Systems, Inc. | Fast deduplication data verification |
US10621143B2 (en) * | 2015-02-06 | 2020-04-14 | Ashish Govind Khurange | Methods and systems of a dedupe file-system garbage collection |
US11836369B1 (en) * | 2015-02-27 | 2023-12-05 | Pure Storage, Inc. | Storing data in an expanded storage pool of a vast storage network |
US9639274B2 (en) | 2015-04-14 | 2017-05-02 | Commvault Systems, Inc. | Efficient deduplication database validation |
US9817599B2 (en) | 2015-05-11 | 2017-11-14 | Hewlett Packard Enterprise Development Lp | Storing indicators of unreferenced memory addresses in volatile memory |
US10769212B2 (en) * | 2015-07-31 | 2020-09-08 | Netapp Inc. | Extensible and elastic data management services engine external to a storage domain |
RU2015139057A (en) | 2015-09-14 | 2017-03-17 | ИЭмСи КОРПОРЕЙШН | METHOD AND SYSTEM OF DISTRIBUTED DATA STORAGE |
US10133770B2 (en) | 2015-12-16 | 2018-11-20 | EMC IP Holding Company LLC | Copying garbage collector for B+ trees under multi-version concurrency control |
US10061697B2 (en) * | 2015-12-16 | 2018-08-28 | EMC IP Holding Company LLC | Garbage collection scope detection for distributed storage |
US11182344B2 (en) * | 2016-03-14 | 2021-11-23 | Vmware, Inc. | File granular data de-duplication effectiveness metric for data de-duplication |
US10372683B1 (en) * | 2016-03-29 | 2019-08-06 | EMC IP Holding Company LLC | Method to determine a base file relationship between a current generation of files and a last replicated generation of files |
CN107852349B (en) * | 2016-03-31 | 2020-12-01 | 慧与发展有限责任合伙企业 | System, method, and storage medium for transaction management for multi-node cluster |
US10929022B2 (en) * | 2016-04-25 | 2021-02-23 | Netapp. Inc. | Space savings reporting for storage system supporting snapshot and clones |
US10642763B2 (en) | 2016-09-20 | 2020-05-05 | Netapp, Inc. | Quality of service policy sets |
US10320907B2 (en) * | 2016-09-26 | 2019-06-11 | Netapp, Inc. | Multi-stage prefetching to exploit long-term future data access sequence knowledge |
US9690701B1 (en) * | 2016-09-27 | 2017-06-27 | International Business Machines Corporation | Probabilistic, parallel collection of memory no longer in use |
CN108228083A (en) * | 2016-12-21 | 2018-06-29 | 伊姆西Ip控股有限责任公司 | For the method and apparatus of data deduplication |
US10372605B2 (en) * | 2016-12-27 | 2019-08-06 | EMC IP Holding Company LLC | Generational garbage collector for trees under multi-version concurrency control |
US10168948B2 (en) | 2017-02-20 | 2019-01-01 | International Business Machines Corporation | Replicating data in a data storage system |
US10331350B1 (en) * | 2017-04-27 | 2019-06-25 | EMC IP Holding Company LLC | Capacity determination for content-based storage |
US10324806B1 (en) * | 2017-04-27 | 2019-06-18 | EMC IP Holding Company LLC | Snapshot visualization for content-based storage |
US11429587B1 (en) | 2017-06-29 | 2022-08-30 | Seagate Technology Llc | Multiple duration deduplication entries |
US10706082B1 (en) * | 2017-06-29 | 2020-07-07 | Seagate Technology Llc | Deduplication database management |
US10620843B2 (en) * | 2017-07-26 | 2020-04-14 | Netapp, Inc. | Methods for managing distributed snapshot for low latency storage and devices thereof |
US10496674B2 (en) | 2017-08-07 | 2019-12-03 | International Business Machines Corporation | Self-describing volume ancestry for data synchronization |
US10555278B2 (en) * | 2017-11-03 | 2020-02-04 | Comptel Oy | Method and arrangement for allocating communication resources in a communication network |
US10572184B2 (en) | 2018-01-11 | 2020-02-25 | International Business Machines Corporation | Garbage collection in data storage systems |
US10445022B1 (en) | 2018-04-26 | 2019-10-15 | Alibaba Group Holding Limited | Optimization of log-structured merge (LSM) tree-based databases using object solid state drive (SSD) devices |
US10592363B2 (en) * | 2018-06-04 | 2020-03-17 | International Business Machines Corporation | Asynchronous remote mirror cloud archival |
KR102567111B1 (en) | 2018-08-02 | 2023-08-14 | 삼성전자주식회사 | Storage device and method for operating storage device |
US10783022B2 (en) | 2018-08-03 | 2020-09-22 | EMC IP Holding Company LLC | Immediate replication for dedicated data blocks |
US11120147B2 (en) * | 2018-09-11 | 2021-09-14 | International Business Machines Corporation | Operating system garbage-collection with integrated clearing of sensitive data |
KR102620731B1 (en) * | 2018-09-27 | 2024-01-05 | 에스케이하이닉스 주식회사 | Memory system and operating method thereof |
US11860743B1 (en) | 2018-11-27 | 2024-01-02 | Amazon Technologies, Inc. | Database operational compatibility safeguards |
US11372991B1 (en) * | 2018-11-27 | 2022-06-28 | Amazon Technologies, Inc. | Database operational continuity |
US11099942B2 (en) * | 2019-03-21 | 2021-08-24 | International Business Machines Corporation | Archival to cloud storage while performing remote backup of data |
CN111858143A (en) * | 2019-04-24 | 2020-10-30 | 伊姆西Ip控股有限责任公司 | Method, apparatus, and computer-readable storage medium for managing storage system |
US11294871B2 (en) | 2019-07-19 | 2022-04-05 | Commvault Systems, Inc. | Deduplication system without reference counting |
US11507422B2 (en) * | 2019-08-01 | 2022-11-22 | EMC IP Holding Company LLC | Method and system for intelligently provisioning resources in storage systems |
US11232094B2 (en) * | 2019-12-16 | 2022-01-25 | Vast Data Ltd. | Techniques for determining ancestry in directed acyclic graphs |
US11360866B2 (en) * | 2020-04-14 | 2022-06-14 | International Business Machines Corporation | Updating stateful system in server cluster |
US11550712B2 (en) | 2020-06-11 | 2023-01-10 | Google Llc | Optimizing garbage collection based on survivor lifetime prediction |
CN111831483A (en) * | 2020-07-20 | 2020-10-27 | 北京百度网讯科技有限公司 | Backup management method, system, electronic device, and medium |
US11809910B2 (en) | 2020-10-14 | 2023-11-07 | Bank Of America Corporation | System and method for dynamically resizing computational infrastructure to accommodate unexpected demands |
US11782641B2 (en) * | 2021-06-09 | 2023-10-10 | International Business Machines Corporation | Backend aware virtualized storage |
JP7387679B2 (en) | 2021-07-12 | 2023-11-28 | 株式会社日立製作所 | Backup system and method |
US11698858B2 (en) | 2021-08-19 | 2023-07-11 | Micron Technology, Inc. | Prediction based garbage collection |
US11687416B2 (en) | 2021-09-27 | 2023-06-27 | Kyndryl, Inc. | Data backup optimization |
US20230205641A1 (en) * | 2021-12-27 | 2023-06-29 | Dell Products L.P. | Disaster recovery drills based on checksum validations |
US11809713B1 (en) * | 2022-07-12 | 2023-11-07 | Silicon Motion, Inc. | Method and apparatus for performing data access management of memory device with aid of randomness-property control |
WO2024085681A1 (en) * | 2022-10-20 | 2024-04-25 | 삼성전자 주식회사 | Method for performing garbage collection, and electronic device supporting same |
Citations (86)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4626829A (en) | 1985-08-19 | 1986-12-02 | Intelligent Storage Inc. | Data compression using run length encoding and statistical encoding |
US5381545A (en) | 1991-06-04 | 1995-01-10 | International Business Machines Corporation | Data backup and recovery in a data processing system |
US5410667A (en) | 1992-04-17 | 1995-04-25 | Storage Technology Corporation | Data record copy system for a disk drive array data storage subsystem |
US5497483A (en) | 1992-09-23 | 1996-03-05 | International Business Machines Corporation | Method and system for track transfer control during concurrent copy operations in a data processing storage subsystem |
US5535381A (en) | 1993-07-22 | 1996-07-09 | Data General Corporation | Apparatus and method for copying and restoring disk files |
US5819292A (en) | 1993-06-03 | 1998-10-06 | Network Appliance, Inc. | Method for maintaining consistent states of a file system and for creating user-accessible read-only copies of a file system |
US5828876A (en) | 1996-07-31 | 1998-10-27 | Ncr Corporation | File system for a clustered processing system |
US5857208A (en) | 1996-05-31 | 1999-01-05 | Emc Corporation | Method and apparatus for performing point in time backup operation in a computer system |
US5963962A (en) | 1995-05-31 | 1999-10-05 | Network Appliance, Inc. | Write anywhere file-system layout |
US6065018A (en) | 1998-03-04 | 2000-05-16 | International Business Machines Corporation | Synchronizing recovery log having time stamp to a remote site for disaster recovery of a primary database having related hierarchial and relational databases |
US6081875A (en) | 1997-05-19 | 2000-06-27 | Emc Corporation | Apparatus and method for backup of a disk storage system |
US6119208A (en) | 1997-04-18 | 2000-09-12 | Storage Technology Corporation | MVS device backup system for a data processor using a data storage subsystem snapshot copy capability |
US6131148A (en) | 1998-01-26 | 2000-10-10 | International Business Machines Corporation | Snapshot copy of a secondary volume of a PPRC pair |
US6148412A (en) | 1996-05-23 | 2000-11-14 | International Business Machines Corporation | Availability and recovery of files using copy storage pools |
US6163856A (en) | 1998-05-29 | 2000-12-19 | Sun Microsystems, Inc. | Method and apparatus for file system disaster recovery |
US6192444B1 (en) | 1998-01-05 | 2001-02-20 | International Business Machines Corporation | Method and system for providing additional addressable functional space on a disk for use with a virtual data storage subsystem |
US6199146B1 (en) | 1998-03-12 | 2001-03-06 | International Business Machines Corporation | Storage management system and method for increasing capacity utilization of nonvolatile storage devices using partially filled substitute storage devices for continuing write operations |
US6202071B1 (en) | 1998-03-24 | 2001-03-13 | International Business Machines Corporation | Method and system for improved database disaster recovery |
US6212531B1 (en) | 1998-01-13 | 2001-04-03 | International Business Machines Corporation | Method for implementing point-in-time copy using a snapshot function |
US6226759B1 (en) | 1998-09-28 | 2001-05-01 | International Business Machines Corporation | Method and apparatus for immediate data backup by duplicating pointers and freezing pointer/data counterparts |
US6269431B1 (en) | 1998-08-13 | 2001-07-31 | Emc Corporation | Virtual storage and block level direct access of secondary storage for recovery of backup data |
US6269381B1 (en) | 1998-06-30 | 2001-07-31 | Emc Corporation | Method and apparatus for backing up data before updating the data and for restoring from the backups |
US6324548B1 (en) | 1999-07-22 | 2001-11-27 | Unisys Corporation | Database backup and recovery using separate history files for database backup and audit backup |
US6330614B1 (en) | 1998-03-20 | 2001-12-11 | Nexabit Networks Llc | Internet and related networks, a method of and system for substitute use of checksum field space in information processing datagram headers for obviating processing speed and addressing space limitations and providing other features |
US6434681B1 (en) | 1999-12-02 | 2002-08-13 | Emc Corporation | Snapshot copy facility for a data storage system permitting continued host read/write access |
US20020129214A1 (en) | 2001-03-09 | 2002-09-12 | Prasenjit Sarkar | System and method for minimizing message transactions for fault-tolerant snapshots in a dual-controller environment |
US6460055B1 (en) | 1999-12-16 | 2002-10-01 | Livevault Corporation | Systems and methods for backing up data files |
US6484186B1 (en) | 2000-02-15 | 2002-11-19 | Novell, Inc. | Method for backing up consistent versions of open files |
US6487561B1 (en) | 1998-12-31 | 2002-11-26 | Emc Corporation | Apparatus and methods for copying, backing up, and restoring data using a backup segment size larger than the storage block size |
US6557089B1 (en) | 2000-11-28 | 2003-04-29 | International Business Machines Corporation | Backup by ID-suppressed instant virtual copy then physical backup copy with ID reintroduced |
US20030101321A1 (en) | 2001-11-29 | 2003-05-29 | Ohran Richard S. | Preserving a snapshot of selected data of a mass storage system |
US20030140070A1 (en) | 2002-01-22 | 2003-07-24 | Kaczmarski Michael Allen | Copy method supplementing outboard data copy with previously instituted copy-on-write logical snapshot to create duplicate consistent with source data as of designated time |
US6625704B2 (en) | 2001-08-08 | 2003-09-23 | Sangate Systems, Inc. | Data backup method and system using snapshot and virtual tape |
US6654912B1 (en) | 2000-10-04 | 2003-11-25 | Network Appliance, Inc. | Recovery of file system data in file servers mirrored file system volumes |
US6654772B1 (en) | 1999-04-28 | 2003-11-25 | Emc Corporation | Multi-volume extent based file system |
US6665815B1 (en) | 2000-06-22 | 2003-12-16 | Hewlett-Packard Development Company, L.P. | Physical incremental backup using snapshots |
US6668264B1 (en) | 2001-04-03 | 2003-12-23 | Network Appliance, Inc. | Resynchronization of a target volume with a source volume |
US6772302B1 (en) | 1999-11-12 | 2004-08-03 | International Business Machines Corporation | Virtual copy method for data spanning storage boundaries |
US6779094B2 (en) | 2000-06-19 | 2004-08-17 | Storage Technology Corporation | Apparatus and method for instant copy of data by writing new data to an additional physical storage area |
US20040199570A1 (en) | 2003-02-14 | 2004-10-07 | Fuji Xerox Co., Ltd. | Information processing system |
US6823336B1 (en) | 2000-09-26 | 2004-11-23 | Emc Corporation | Data storage system and method for uninterrupted read-only access to a consistent dataset by one host processor concurrent with read-write access by another host processor |
US6823436B2 (en) | 2001-10-02 | 2004-11-23 | International Business Machines Corporation | System for conserving metadata about data snapshots |
US20050004954A1 (en) | 2003-07-01 | 2005-01-06 | Hand Held Products, Inc. | Systems and methods for expedited data transfer in a communication system using hash segmentation |
US6850929B2 (en) | 2001-03-08 | 2005-02-01 | International Business Machines Corporation | System and method for managing file system extended attributes |
US20050066095A1 (en) | 2003-09-23 | 2005-03-24 | Sachin Mullick | Multi-threaded write interface and methods for increasing the single file read and write throughput of a file server |
US6898688B2 (en) | 2001-12-28 | 2005-05-24 | Storage Technology Corporation | Data management appliance |
US6915397B2 (en) | 2001-06-01 | 2005-07-05 | Hewlett-Packard Development Company, L.P. | System and method for generating point in time storage copy |
US20050165794A1 (en) | 2003-12-30 | 2005-07-28 | Microsoft Corporation | Index key normalization |
US6928526B1 (en) | 2002-12-20 | 2005-08-09 | Datadomain, Inc. | Efficient data storage system |
US6948039B2 (en) | 2001-12-14 | 2005-09-20 | Voom Technologies, Inc. | Data backup and restoration using dynamic virtual storage |
US6957362B2 (en) | 2002-08-06 | 2005-10-18 | Emc Corporation | Instantaneous restoration of a production copy from a snapshot copy in a data storage system |
US20060059207A1 (en) | 2004-09-15 | 2006-03-16 | Diligent Technologies Corporation | Systems and methods for searching of storage data with reduced bandwidth requirements |
US20060074945A1 (en) | 2004-09-22 | 2006-04-06 | Masashi Mori | File management program, data structure, and file management device |
US7072916B1 (en) | 2000-08-18 | 2006-07-04 | Network Appliance, Inc. | Instant snapshot |
US7143251B1 (en) | 2003-06-30 | 2006-11-28 | Data Domain, Inc. | Data storage using identifiers |
US7222194B2 (en) | 2001-12-26 | 2007-05-22 | Hitachi, Ltd. | Backup system |
US7325111B1 (en) | 2005-11-01 | 2008-01-29 | Network Appliance, Inc. | Method and system for single pass volume scanning for multiple destination mirroring |
US7346623B2 (en) | 2001-09-28 | 2008-03-18 | Commvault Systems, Inc. | System and method for generating and managing quick recovery volumes |
US7386695B2 (en) | 2004-12-23 | 2008-06-10 | International Business Machines Corporation | Storage system with multiple copy targeting |
US7428657B2 (en) | 2005-10-20 | 2008-09-23 | Hitachi, Ltd. | Method for rolling back from snapshot with log |
US20080270436A1 (en) | 2007-04-27 | 2008-10-30 | Fineberg Samuel A | Storing chunks within a file system |
US20090222496A1 (en) | 2005-06-24 | 2009-09-03 | Syncsort Incorporated | System and Method for Virtualizing Backup Images |
US7647355B2 (en) | 2003-10-30 | 2010-01-12 | International Business Machines Corporation | Method and apparatus for increasing efficiency of data storage in a file system |
US7689633B1 (en) | 2004-09-15 | 2010-03-30 | Data Domain, Inc. | Network file system-based data storage system |
US7707184B1 (en) | 2002-10-09 | 2010-04-27 | Netapp, Inc. | System and method for snapshot full backup and hard recovery of a database |
US20100235333A1 (en) * | 2009-03-16 | 2010-09-16 | International Business Machines Corporation | Apparatus and method to sequentially deduplicate data |
US7814128B2 (en) | 2003-05-30 | 2010-10-12 | Symantec Operating Corporation | Multi-volume file support |
US20110238635A1 (en) * | 2010-03-25 | 2011-09-29 | Quantum Corporation | Combining Hash-Based Duplication with Sub-Block Differencing to Deduplicate Data |
US8037032B2 (en) | 2008-08-25 | 2011-10-11 | Vmware, Inc. | Managing backups using virtual machines |
US20110258161A1 (en) | 2010-04-14 | 2011-10-20 | International Business Machines Corporation | Optimizing Data Transmission Bandwidth Consumption Over a Wide Area Network |
US8046326B2 (en) * | 2009-07-24 | 2011-10-25 | Sap Ag | Adaptive synchronization of business objects |
US8139575B2 (en) | 2007-06-29 | 2012-03-20 | International Business Machines Corporation | Device, system and method of modification of PCI express packet digest |
US8150808B2 (en) | 2009-10-21 | 2012-04-03 | Delphix Corp. | Virtual database system |
US8161077B2 (en) | 2009-10-21 | 2012-04-17 | Delphix Corp. | Datacenter workflow automation scenarios using virtual databases |
US8180742B2 (en) | 2004-07-01 | 2012-05-15 | Emc Corporation | Policy-based information management |
US8180740B1 (en) | 2009-08-12 | 2012-05-15 | Netapp, Inc. | System and method for eliminating duplicate data by generating data fingerprints using adaptive fixed-length windows |
US20120124046A1 (en) | 2010-11-16 | 2012-05-17 | Actifio, Inc. | System and method for managing deduplicated copies of data using temporal relationships among copies |
US20120166448A1 (en) | 2010-12-28 | 2012-06-28 | Microsoft Corporation | Adaptive Index for Data Deduplication |
US8299944B2 (en) | 2010-11-16 | 2012-10-30 | Actifio, Inc. | System and method for creating deduplicated copies of data storing non-lossy encodings of data directly in a content addressable store |
US8407191B1 (en) | 2010-06-29 | 2013-03-26 | Emc Corporation | Priority based data scrubbing on a deduplicated data store |
US8468174B1 (en) | 2010-11-30 | 2013-06-18 | Jedidiah Yueh | Interfacing with a virtual database system |
US8548944B2 (en) | 2010-07-15 | 2013-10-01 | Delphix Corp. | De-duplication based backup of file systems |
US8706833B1 (en) | 2006-12-08 | 2014-04-22 | Emc Corporation | Data storage server having common replication architecture for multiple storage object types |
US8788769B2 (en) | 2010-11-16 | 2014-07-22 | Actifio, Inc. | System and method for performing backup or restore operations utilizing difference information and timeline state information |
US9098432B1 (en) | 2008-04-08 | 2015-08-04 | Emc Corporation | System and method for data deduplication of backup images |
US9223793B1 (en) * | 2009-06-03 | 2015-12-29 | American Megatrends, Inc. | De-duplication of files for continuous data protection with remote storage |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8108447B2 (en) * | 2010-03-11 | 2012-01-31 | Symantec Corporation | Systems and methods for garbage collection in deduplicated data systems |
US8417904B2 (en) * | 2010-03-17 | 2013-04-09 | Seagate Technology Llc | Garbage collection management in a data storage device |
-
2014
- 2014-05-14 US US14/120,368 patent/US9646067B2/en active Active
- 2014-05-14 CA CA2912394A patent/CA2912394A1/en not_active Abandoned
- 2014-05-14 WO PCT/US2014/000111 patent/WO2014185974A2/en active Application Filing
- 2014-05-14 JP JP2016513944A patent/JP2016524220A/en active Pending
- 2014-05-14 AU AU2014265979A patent/AU2014265979A1/en not_active Abandoned
- 2014-05-14 US US14/120,340 patent/US9563683B2/en active Active
- 2014-05-14 EP EP14731405.8A patent/EP2997501A2/en not_active Withdrawn
Patent Citations (88)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4626829A (en) | 1985-08-19 | 1986-12-02 | Intelligent Storage Inc. | Data compression using run length encoding and statistical encoding |
US5381545A (en) | 1991-06-04 | 1995-01-10 | International Business Machines Corporation | Data backup and recovery in a data processing system |
US5410667A (en) | 1992-04-17 | 1995-04-25 | Storage Technology Corporation | Data record copy system for a disk drive array data storage subsystem |
US5497483A (en) | 1992-09-23 | 1996-03-05 | International Business Machines Corporation | Method and system for track transfer control during concurrent copy operations in a data processing storage subsystem |
US5819292A (en) | 1993-06-03 | 1998-10-06 | Network Appliance, Inc. | Method for maintaining consistent states of a file system and for creating user-accessible read-only copies of a file system |
US5535381A (en) | 1993-07-22 | 1996-07-09 | Data General Corporation | Apparatus and method for copying and restoring disk files |
US5963962A (en) | 1995-05-31 | 1999-10-05 | Network Appliance, Inc. | Write anywhere file-system layout |
US6148412A (en) | 1996-05-23 | 2000-11-14 | International Business Machines Corporation | Availability and recovery of files using copy storage pools |
US5857208A (en) | 1996-05-31 | 1999-01-05 | Emc Corporation | Method and apparatus for performing point in time backup operation in a computer system |
US5828876A (en) | 1996-07-31 | 1998-10-27 | Ncr Corporation | File system for a clustered processing system |
US6119208A (en) | 1997-04-18 | 2000-09-12 | Storage Technology Corporation | MVS device backup system for a data processor using a data storage subsystem snapshot copy capability |
US6081875A (en) | 1997-05-19 | 2000-06-27 | Emc Corporation | Apparatus and method for backup of a disk storage system |
US6192444B1 (en) | 1998-01-05 | 2001-02-20 | International Business Machines Corporation | Method and system for providing additional addressable functional space on a disk for use with a virtual data storage subsystem |
US6212531B1 (en) | 1998-01-13 | 2001-04-03 | International Business Machines Corporation | Method for implementing point-in-time copy using a snapshot function |
US6131148A (en) | 1998-01-26 | 2000-10-10 | International Business Machines Corporation | Snapshot copy of a secondary volume of a PPRC pair |
US6065018A (en) | 1998-03-04 | 2000-05-16 | International Business Machines Corporation | Synchronizing recovery log having time stamp to a remote site for disaster recovery of a primary database having related hierarchial and relational databases |
US6199146B1 (en) | 1998-03-12 | 2001-03-06 | International Business Machines Corporation | Storage management system and method for increasing capacity utilization of nonvolatile storage devices using partially filled substitute storage devices for continuing write operations |
US6330614B1 (en) | 1998-03-20 | 2001-12-11 | Nexabit Networks Llc | Internet and related networks, a method of and system for substitute use of checksum field space in information processing datagram headers for obviating processing speed and addressing space limitations and providing other features |
US6202071B1 (en) | 1998-03-24 | 2001-03-13 | International Business Machines Corporation | Method and system for improved database disaster recovery |
US6163856A (en) | 1998-05-29 | 2000-12-19 | Sun Microsystems, Inc. | Method and apparatus for file system disaster recovery |
US6269381B1 (en) | 1998-06-30 | 2001-07-31 | Emc Corporation | Method and apparatus for backing up data before updating the data and for restoring from the backups |
US6269431B1 (en) | 1998-08-13 | 2001-07-31 | Emc Corporation | Virtual storage and block level direct access of secondary storage for recovery of backup data |
US6226759B1 (en) | 1998-09-28 | 2001-05-01 | International Business Machines Corporation | Method and apparatus for immediate data backup by duplicating pointers and freezing pointer/data counterparts |
US6487561B1 (en) | 1998-12-31 | 2002-11-26 | Emc Corporation | Apparatus and methods for copying, backing up, and restoring data using a backup segment size larger than the storage block size |
US6654772B1 (en) | 1999-04-28 | 2003-11-25 | Emc Corporation | Multi-volume extent based file system |
US6324548B1 (en) | 1999-07-22 | 2001-11-27 | Unisys Corporation | Database backup and recovery using separate history files for database backup and audit backup |
US6772302B1 (en) | 1999-11-12 | 2004-08-03 | International Business Machines Corporation | Virtual copy method for data spanning storage boundaries |
US6434681B1 (en) | 1999-12-02 | 2002-08-13 | Emc Corporation | Snapshot copy facility for a data storage system permitting continued host read/write access |
US6460055B1 (en) | 1999-12-16 | 2002-10-01 | Livevault Corporation | Systems and methods for backing up data files |
US6484186B1 (en) | 2000-02-15 | 2002-11-19 | Novell, Inc. | Method for backing up consistent versions of open files |
US6779094B2 (en) | 2000-06-19 | 2004-08-17 | Storage Technology Corporation | Apparatus and method for instant copy of data by writing new data to an additional physical storage area |
US6665815B1 (en) | 2000-06-22 | 2003-12-16 | Hewlett-Packard Development Company, L.P. | Physical incremental backup using snapshots |
US7072916B1 (en) | 2000-08-18 | 2006-07-04 | Network Appliance, Inc. | Instant snapshot |
US6823336B1 (en) | 2000-09-26 | 2004-11-23 | Emc Corporation | Data storage system and method for uninterrupted read-only access to a consistent dataset by one host processor concurrent with read-write access by another host processor |
US6654912B1 (en) | 2000-10-04 | 2003-11-25 | Network Appliance, Inc. | Recovery of file system data in file servers mirrored file system volumes |
US6557089B1 (en) | 2000-11-28 | 2003-04-29 | International Business Machines Corporation | Backup by ID-suppressed instant virtual copy then physical backup copy with ID reintroduced |
US6850929B2 (en) | 2001-03-08 | 2005-02-01 | International Business Machines Corporation | System and method for managing file system extended attributes |
US20020129214A1 (en) | 2001-03-09 | 2002-09-12 | Prasenjit Sarkar | System and method for minimizing message transactions for fault-tolerant snapshots in a dual-controller environment |
US6668264B1 (en) | 2001-04-03 | 2003-12-23 | Network Appliance, Inc. | Resynchronization of a target volume with a source volume |
US6915397B2 (en) | 2001-06-01 | 2005-07-05 | Hewlett-Packard Development Company, L.P. | System and method for generating point in time storage copy |
US6625704B2 (en) | 2001-08-08 | 2003-09-23 | Sangate Systems, Inc. | Data backup method and system using snapshot and virtual tape |
US7346623B2 (en) | 2001-09-28 | 2008-03-18 | Commvault Systems, Inc. | System and method for generating and managing quick recovery volumes |
US6823436B2 (en) | 2001-10-02 | 2004-11-23 | International Business Machines Corporation | System for conserving metadata about data snapshots |
US20030101321A1 (en) | 2001-11-29 | 2003-05-29 | Ohran Richard S. | Preserving a snapshot of selected data of a mass storage system |
US6948039B2 (en) | 2001-12-14 | 2005-09-20 | Voom Technologies, Inc. | Data backup and restoration using dynamic virtual storage |
US7222194B2 (en) | 2001-12-26 | 2007-05-22 | Hitachi, Ltd. | Backup system |
US6898688B2 (en) | 2001-12-28 | 2005-05-24 | Storage Technology Corporation | Data management appliance |
US20030140070A1 (en) | 2002-01-22 | 2003-07-24 | Kaczmarski Michael Allen | Copy method supplementing outboard data copy with previously instituted copy-on-write logical snapshot to create duplicate consistent with source data as of designated time |
US6957362B2 (en) | 2002-08-06 | 2005-10-18 | Emc Corporation | Instantaneous restoration of a production copy from a snapshot copy in a data storage system |
US7707184B1 (en) | 2002-10-09 | 2010-04-27 | Netapp, Inc. | System and method for snapshot full backup and hard recovery of a database |
US6928526B1 (en) | 2002-12-20 | 2005-08-09 | Datadomain, Inc. | Efficient data storage system |
US20040199570A1 (en) | 2003-02-14 | 2004-10-07 | Fuji Xerox Co., Ltd. | Information processing system |
US7814128B2 (en) | 2003-05-30 | 2010-10-12 | Symantec Operating Corporation | Multi-volume file support |
US7143251B1 (en) | 2003-06-30 | 2006-11-28 | Data Domain, Inc. | Data storage using identifiers |
US20050004954A1 (en) | 2003-07-01 | 2005-01-06 | Hand Held Products, Inc. | Systems and methods for expedited data transfer in a communication system using hash segmentation |
US20050066095A1 (en) | 2003-09-23 | 2005-03-24 | Sachin Mullick | Multi-threaded write interface and methods for increasing the single file read and write throughput of a file server |
US7647355B2 (en) | 2003-10-30 | 2010-01-12 | International Business Machines Corporation | Method and apparatus for increasing efficiency of data storage in a file system |
US20050165794A1 (en) | 2003-12-30 | 2005-07-28 | Microsoft Corporation | Index key normalization |
US8180742B2 (en) | 2004-07-01 | 2012-05-15 | Emc Corporation | Policy-based information management |
US7689633B1 (en) | 2004-09-15 | 2010-03-30 | Data Domain, Inc. | Network file system-based data storage system |
US20060059207A1 (en) | 2004-09-15 | 2006-03-16 | Diligent Technologies Corporation | Systems and methods for searching of storage data with reduced bandwidth requirements |
US20060074945A1 (en) | 2004-09-22 | 2006-04-06 | Masashi Mori | File management program, data structure, and file management device |
US7386695B2 (en) | 2004-12-23 | 2008-06-10 | International Business Machines Corporation | Storage system with multiple copy targeting |
US7937547B2 (en) | 2005-06-24 | 2011-05-03 | Syncsort Incorporated | System and method for high performance enterprise data protection |
US20090222496A1 (en) | 2005-06-24 | 2009-09-03 | Syncsort Incorporated | System and Method for Virtualizing Backup Images |
US7428657B2 (en) | 2005-10-20 | 2008-09-23 | Hitachi, Ltd. | Method for rolling back from snapshot with log |
US7325111B1 (en) | 2005-11-01 | 2008-01-29 | Network Appliance, Inc. | Method and system for single pass volume scanning for multiple destination mirroring |
US8706833B1 (en) | 2006-12-08 | 2014-04-22 | Emc Corporation | Data storage server having common replication architecture for multiple storage object types |
US20080270436A1 (en) | 2007-04-27 | 2008-10-30 | Fineberg Samuel A | Storing chunks within a file system |
US8139575B2 (en) | 2007-06-29 | 2012-03-20 | International Business Machines Corporation | Device, system and method of modification of PCI express packet digest |
US9098432B1 (en) | 2008-04-08 | 2015-08-04 | Emc Corporation | System and method for data deduplication of backup images |
US8037032B2 (en) | 2008-08-25 | 2011-10-11 | Vmware, Inc. | Managing backups using virtual machines |
US20100235333A1 (en) * | 2009-03-16 | 2010-09-16 | International Business Machines Corporation | Apparatus and method to sequentially deduplicate data |
US9223793B1 (en) * | 2009-06-03 | 2015-12-29 | American Megatrends, Inc. | De-duplication of files for continuous data protection with remote storage |
US8046326B2 (en) * | 2009-07-24 | 2011-10-25 | Sap Ag | Adaptive synchronization of business objects |
US8180740B1 (en) | 2009-08-12 | 2012-05-15 | Netapp, Inc. | System and method for eliminating duplicate data by generating data fingerprints using adaptive fixed-length windows |
US8566361B2 (en) | 2009-10-21 | 2013-10-22 | Delphix Corp. | Datacenter workflow automation scenarios using virtual databases |
US8161077B2 (en) | 2009-10-21 | 2012-04-17 | Delphix Corp. | Datacenter workflow automation scenarios using virtual databases |
US8150808B2 (en) | 2009-10-21 | 2012-04-03 | Delphix Corp. | Virtual database system |
US20110238635A1 (en) * | 2010-03-25 | 2011-09-29 | Quantum Corporation | Combining Hash-Based Duplication with Sub-Block Differencing to Deduplicate Data |
US20110258161A1 (en) | 2010-04-14 | 2011-10-20 | International Business Machines Corporation | Optimizing Data Transmission Bandwidth Consumption Over a Wide Area Network |
US8407191B1 (en) | 2010-06-29 | 2013-03-26 | Emc Corporation | Priority based data scrubbing on a deduplicated data store |
US8548944B2 (en) | 2010-07-15 | 2013-10-01 | Delphix Corp. | De-duplication based backup of file systems |
US8299944B2 (en) | 2010-11-16 | 2012-10-30 | Actifio, Inc. | System and method for creating deduplicated copies of data storing non-lossy encodings of data directly in a content addressable store |
US8788769B2 (en) | 2010-11-16 | 2014-07-22 | Actifio, Inc. | System and method for performing backup or restore operations utilizing difference information and timeline state information |
US20120124046A1 (en) | 2010-11-16 | 2012-05-17 | Actifio, Inc. | System and method for managing deduplicated copies of data using temporal relationships among copies |
US8468174B1 (en) | 2010-11-30 | 2013-06-18 | Jedidiah Yueh | Interfacing with a virtual database system |
US20120166448A1 (en) | 2010-12-28 | 2012-06-28 | Microsoft Corporation | Adaptive Index for Data Deduplication |
Non-Patent Citations (155)
Title |
---|
Alapati, "NetApp Technical Report: SnapMirror Sync and SnapMirror Semi-Sync Overview and Design Considerations," NetApp, Jul. 2010 (24 pages). |
American Megatrends, Inc., "StorTrends/ManageTrends (Version 2.7) User's Guide for the StorTends 1300 Storage Appliance" Mar. 23, 2009 (378 pages). |
Arrell et al., "Using RVA and SnapShot for Business Intelligence Applications with OS/390 and DB2," IBM, Redbooks, Aug. 1998 (70 pages). |
Baird, "Virtual Storage Architecture Guide (VSAG)," IEEE, No Month Listed 1995 (pp. 312-326). |
Baker, "Disk-Based Mirroring is a Lot More Than Just Safe," Computer Technology Review, No Month Listed 2000 (pp. 55-57). |
Cederqvist et al., "Version Management with CVS," No Month Listed 1992 (122 pages). |
Chang et al., "Performance Analysis of Two Frozen Image Based Backup/Restore Methods," IEEE International Conference on Electron Information Technology 2005, May 22-25, 2005 (7 pages). |
Chapman et al., "SnapMirror® Best Practices Guide," Network Appliance, Inc., Apr. 2006 (63 pages). |
Chatterjee et al., "Efficient Replication Leveraging Information Lifecycle Management in Data Storage Systems," Provisional Patent Application No. Not Available, Feb. 9, 2009 (25 pages). |
Chervenak et al., "Protecting File Systems: A Survey of Backup Techniques," Sixth Goddard Conference on Mass Storage Systems and Technologies, Fifteenth IEEE Symposium on Mass Storage Systems, College Park, Maryland, Mar. 23-26, 1998 (17 pages). |
Chutani et al., "The Episode File System," Usenix Association, Proceedings of the Winter 1992 Usenix Conference, San Francisco, California, Jan. 20-24, 1992 (19 pages). |
CommVault, "CommVault® Simpana® Quick Recovery® Software for Critical Data Center Applications and File Systems," No Month Listed 2010 (35 pages). |
Dantz Development Corporation, "Retrospect® User's Guide," No Month Listed 2002 (262 pages). |
Degwekar, "Using SnapMirror® with SnapDrive® for Unix®," No Month Listed 2007 (11 pages). |
Delphix Corp.'s Invalidity Contentions Under Patent L.R. 3-3, Jul. 24, 2014 (27 pages). |
Edwards et al., "FlexVol: Flexible, Efficient File Volume Virtualization in WAFL," Usenix Association, Usenix '08: 2008 Usenix Annual Technical Conference, No Month Listed 2008 (pp. 129-142). |
EMC, "Backup of Oracle in EMC Symmetrix Environments with EMC NetWorker PowerSnap," Aug. 2008 (25 pages). |
EMC, "EMC Celerra Version 5.6 Technical Primer: SLA-Driven Replication with Celerra Replicator (V2): Technology Concepts and Business Considerations," Jul. 2008 (20 pages). |
EMC, "EMC DL1500 and DL3000 with EMC NetWorker: Best Practices Planning," Jul. 2009 (36 pages). |
EMC, "Next-Generation Backup-to-Disk: Building the Foundation for Effective Operational Recovery Management," Jan. 31, 2005 (9 pages). |
EMC, "Unified Backup and Recovery with EMC NetWorker," Mar. 2010 (16 pages). |
Exhibit 1004 IPR2015-01689, Declaration of Ellie Young, Aug. 5, 2015 (24 pages). |
Exhibit 1006 IPR2015-01678, Pfaffenberger, Webster's New World Computer Dictionary Ninth Edition, Hungry Minds, Inc., New York, New York, No Month Listed 2001 (4 pages). |
Exhibit 1006 IPR2015-01689, File History for U.S. Appl. No. 12/947,393, Apr. 14, 2015 (108 pages). |
Exhibit 1007 IPR2015-01678, Microsoft Computer Dictionary Fourth Edition, Microsoft Press, Redmond, Washington, No Month Listed 1999 (3 pages). |
Exhibit 1007 IPR2015-01689, Declaration of Prashant Shenoy, Ph.D. in Support of Petition for Inter Partes Review, Aug. 5, 2015 (82 pages). |
Exhibit 1008 IPR2015-01678, File History for U.S. Appl. No. 12/947,438, Apr. 14, 2015 (100 pages). |
Exhibit 1009 IPR2015-01678, Declaration of Prashant Shenoy, Ph.D. in Support of Petition for Inter Partes Review, Aug. 5, 2015 (58 pages). |
Exhibit 1010 IPR2015-01678, Rivest, "The MD5 Message-Digest Algorithm," retrieved online at [URL:<<https://www.ietf.org/rfc/rfc1321.txt>>] Apr. 1992 (20 pages). |
Exhibit 1010 IPR2015-01678, Rivest, "The MD5 Message-Digest Algorithm," retrieved online at [URL:>] Apr. 1992 (20 pages). |
Exhibit 1011 IPR2015-01678, "Secure Hash Standard," U.S. Department of Commerce, Technology Administration, National Institute of Standards and Technology, FIPS PUB 180-1, Federal Information Processing Standards Publication, Apr. 17, 1995 (24 pages). |
Exhibit A-01 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Data Domain ("Data Domain") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (7 pages). |
Exhibit A-02 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Symantec NetBackup/Veritas NetBackup ("NetBackup") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (7 pages). |
Exhibit A-03 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Hitachi ShadowImage ("ShadowImage") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (7 pages). |
Exhibit A-04 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,269,431 as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (27 pages). |
Exhibit A-05 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,915,397 ("the '397 patent") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (44 pages). |
Exhibit A-06 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Hutchinson as Claim Chart for U.S. Pat. No. 6,732,244," Jul. 3, 2015 (64 pages). |
Exhibit A-07 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Petal: Distributed Virtual Disks (Lee) as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (35 pages). |
Exhibit A-08 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "EMC TimeFinder as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (51 pages). |
Exhibit A-09 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,434,681 as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (19 pages). |
Exhibit A-10 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 7,072,916 ("the '916 patent") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (65 pages). |
Exhibit A-11 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,898,688 as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (35 pages). |
Exhibit A-12 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Chervenak as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (21 pages). |
Exhibit A-13 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "IBM ADSTAR Distributed Storage Manager ("ADSM")/Tivoli Storage Manager as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (52 pages). |
Exhibit A-14 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Green, Designing a Fast On-line Backup System for a Log-structured File System as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (80 pages). |
Exhibit A-15 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 5,535,381 as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (53 pages). |
Exhibit A-16 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "IBM RAMAC Virtual Array ("RAMAC" or "RVA") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (68 pages). |
Exhibit A-17 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "The Episode File System ("Episode") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (21 pages). |
Exhibit A-18 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Informix Storage Manager and Database Servers ("Informix") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (34 pages). |
Exhibit A-19 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,779,094 ("the '094 patent") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (44 pages). |
Exhibit A-20 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "EMC NetWorker/Legato NetWorker ("NetWorker") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (59 pages). |
Exhibit A-21 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "EMC TimeFinder as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (51 pages). |
Exhibit A-22 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Retrospect as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (12 pages). |
Exhibit A-23 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,119,208 to White et al. ("White") as Prior Art to U.S. Pat. No. 6,732,244," Jul. 3, 2015 (25 pages). |
Exhibit B-01 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Data Domain ("Data Domain") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (10 pages). |
Exhibit B-02 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Hitachi ShadowImage ("ShadowImage") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (10 pages). |
Exhibit B-03 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,269,431 as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (42 pages). |
Exhibit B-04 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Green, Designing a Fast On-line Backup System for a Log-structured File System as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (104 pages). |
Exhibit B-05 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 5,535,381 as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (84 pages). |
Exhibit B-06 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Hutchinson as Claim Chart for U.S. Pat. No. 6,959,369," Jul. 3, 2015 (80 pages). |
Exhibit B-07 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Patent Application No. 2003/0140070 ("the '070 application") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (67 pages). |
Exhibit B-08 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,898,688 as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (53 pages). |
Exhibit B-09 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,434,681 as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (44 pages). |
Exhibit B-10 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 7,072,916 ("the '916 patent") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (59 pages). |
Exhibit B-11 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Chervenak as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (69 pages). |
Exhibit B-12 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "IBM ADSTAR Distributed Storage Manager ("ADSM")/Tivoli Storage Manager as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (54 pages). |
Exhibit B-13 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Petal: Distributed Virtual Disks (Lee) as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (51 pages). |
Exhibit B-14 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,915,397 ("the '397 patent") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (94 pages). |
Exhibit B-15 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "IBM RAMAC Virtual Array ("RAMAC" or "RVA") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (89 pages). |
Exhibit B-16 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Symantec NetBackup/Veritas NetBackup ("NetBackup") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (65 pages). |
Exhibit B-17 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "EMC NetWorker/Legato NetWorker ("NetWorker") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (92 pages). |
Exhibit B-18 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Retrospect as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (35 pages). |
Exhibit B-19 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "EMC TimeFinder as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (90 pages). |
Exhibit B-20 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "Informix Storage Manager and Database Servers ("Informix") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (70 pages). |
Exhibit B-21 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "NetApp Data Protection Solution ("NetApp") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (58 pages). |
Exhibit B-22 to Delphix Corp.'s Preliminary Invalidity and Non-Infringement Contentions, "U.S. Pat. No. 6,119,208 to White et al. ("White") as Prior Art to U.S. Pat. No. 6,959,369," Jul. 3, 2015 (43 pages). |
Friess et al., "Windows NT Backup and Recovery with ADSM," IBM, Redbooks, May 1998 (190 pages). |
Galli, "Journal File Systems in Linux," Upgrade The European Online Magazine for the IT Professional, vol. 2, No. 6, Dec. 2001 (8 pages). |
Garrett et al., "Syncsort Backup Express and NetApp: Advances Data Protection and Disaster Recovery," Enterprise Strategy Group, Jan. 2009 (19 pages). |
Gordon, "High Noon-Backup and Recovery: What Works, What Doesn't and Why," Enterprise Systems Journal, vol. 15, No. 9, Sep. 2000 (5 pages). |
Green et al., "Designing a Fast, On-Line Backup System for a Log-Structured File System," Digital Technical Journal, vol. 8, No. 2, No Month Listed 1996 (pp. 32-45). |
Gu et al., "DB2 UDB Backup and Recovery with ESS Copy Services," IBM, Redbooks, Aug. 2002 (144 pages). |
Hendricks et al., "Improving Small File Performance in Object-Based Storage," Parallel Data Laboratory, Carnegie Mellon University, Pittsburgh, Pennsylvania, May 2006 (21 pages). |
Herrin et al., "The Viva File System," retrieved online at [URL:<<http.://www.cs.wisc.edu/˜shankar/Viva/viva.html>>] Jun. 14, 1997 (26 pages). |
Herrin et al., "The Viva File System," retrieved online at [URL:>] Jun. 14, 1997 (26 pages). |
Heyt et al., "Tivoli Storage Manager Version 3.7: Technical Guide," IBM, Redbooks, Dec. 1999 (248 pages). |
Hitz et al., "File System Design for an NFS File Server Appliance," Network Appliance, Jan. 19, 1994 (23 pages). |
Holton et al., "XFS: A Next Generation Journalled 64-Bit Filesystem with Guaranteed Rate I/O," retrieved online at [URL:<<http://www.sgi.com/Technology/xfs-whitepaper.html>>] Jun. 5, 1997 (15 pages). |
Holton et al., "XFS: A Next Generation Journalled 64-Bit Filesystem with Guaranteed Rate I/O," retrieved online at [URL:>] Jun. 5, 1997 (15 pages). |
Hutchinson, "Logical vs. Physical File System Backup," Usenix Association, Proceedings of the 3rd Symposium on Operating Systems Design and Implementation, New Orleans, Louisiana, Feb. 1999 (12 pages). |
IBM, "IBM RAMAC Virtual Array," IBM, Redbooks, Jul. 1997, (490 pages). |
IBM, "Setting Up and Implementing ADSTAR Distributed Storage Manager/400," IBM, Redbooks, Mar. 1995 (350 pages). |
Informix Corporation, "Informix Backup and Restore Guide," Dec. 1999 (280 pages). |
Informix Corporation, "Informix Storage Manager: Administrators Guide," Dec. 1999 (166 pages). |
International Search Report and Written Opinion issued by the European Patent Office as International Searching Authority for International Application No. PCT/US14/00111 mailed Feb. 18, 2015 (16 pgs.). |
Isilon Systems, "Backup and Recovery with Isilon IQ Clustered Storage," Aug. 2007 (19 pages). |
Kara, "Ext4, btrfs and the others," Linux-Kongress, The International Linux System Technology Conference, Oct. 30, 2009 (15 pages). |
Keeton et al., "A Framework for Evaluating Storage System Dependability," Proceedings of the 2004 International Conference on Dependable Systems and Networks, No Month Listed 2004 (10 pages). |
Kim et al., "Volume Management in SAN Environment," IEEE, No Month Listed 2001 (pp. 500-505). |
Klivansky, "A Thorough Introduction to FlexClone™ Volumes," Network Appliance, Inc., Oct. 2004 (35 pages). |
Klosterman, "Delayed Instantiation Bulk Operations for Management of Distributed, Object-Based Storage Systems," Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, Pennsylvania, Aug. 2009 (255 pages). |
Kulkarni et al., "Redundancy Elimination Within Large Collections of Files," Usenix Association, Proceedings of the General Track: 2004 Usenix Annual Technical Conference, Boston, MA, Jun. 27-Jul. 2, 2004 (15 pages). |
Lee et al., "A Comparison of Two Distributed Disk Systems," Systems Research Center, Palo Alto, California, Apr. 30, 1998 (25 pages). |
Lee et al., "Petal: Distributed Virtual Disks," ACM, No Month Listed 1996 (pp. 84-92). |
Legato, "Legato NetWorker Release 6.1 UNIX Version: Administrators Guide," No Month Listed 2001 (638 pages). |
Leins et al., "Tivoli Storage Manager Version 3.7.3 & 4.1: Technical Guide," IBM, Redbooks, Sep. 2000 (368 pages). |
Linett et al., "The Real Problems of Backup," Fourth NASA Goddard Conference on Mass Storage Systems and Technologies, College Park, Maryland, Mar. 28-30, 1995 (13 pages). |
Mesnier et al., "Object-Based Storage," IEEE Communications Magazine, Aug. 2003 (pp. 84-90). |
Mesnier et al., "Object-Based Storage," IEEE Potentials, Apr./May 2005 (pp. 31-34). |
Milligan et al., "Simplifying Management of Complex Business Operations (A Study of Mainline Storage Virtualization)," CMG 2001 Proceedings, vol. 2, Dec. 2-7, 2001 (13 pages). |
Mortimer et al., "ADSM Version 3 Technical Guide," IBM, Redbooks, Dec. 1998 (384 pages). |
Mortimer et al., "Using ADSM to Back Up Databases," IBM, Redbooks, Jul. 1998 (618 pages). |
Mullender et al., "Immediate Files," Software-Practice and Experience, vol. 14, No. 4, Apr. 1984 (pp. 365-368). |
Muller et al., "A High Performance Multi-Structured File System Design," ACM, No Month Listed 1991 (pp. 56-67). |
Mushran, "OCFS2: A Cluster File System for Linux: User's Guide for Release 1.4," Jul. 2008 (44 pages). |
Muthitacharoen et al., "A Low-Bandwidth Network File System," ACM, No Month Listed 2001 (pp. 174-187). |
NetApp, Inc., "Data ONTAP® 7.3 Data Protection: Online Backup and Recovery Guide," Jun. 2008 (405 pages). |
NetApp, Inc., "Data ONTAP® 7.3 System Administration Guide," Nov. 2010 (349 pages). |
Network Appliance Inc., "Data ONTAP 10.0: Architecture Basic Concepts Guide," Nov. 2006 (18 pages). |
Network Appliance Inc., "SnapManager® 2.1 for Oracle® Installation and Administration Guide," Apr. 2007 (272 pages). |
Network Appliance, Inc., "Data ONTAP™ 6.3 Command Reference," Network Appliance, Inc., Sunnyvale, California, Aug. 2002 (452 pages). |
Network Appliance, Inc., "Network Appliance™ SnapMirror® Software," No Month Listed 2006 (2 pages). |
Osuna et al., "Data Protection Strategies in IBM System Storage N Series," IBM, Redbooks, Jun. 2008 (90 pages). |
Osuna et al., "IBM System Storage N Series SnapMirror," IBM, Redbooks, Jul. 2006 (124 pages). |
Pate et al., "Implementing SnapShot," IBM, Redbooks, Jul. 1999 (214 pages). |
Pate et al., "RAMAC Virtual Array, Peer-to-Peer Remote Copy, and IXFP/SnapShot FOR VSE/ESA," IBM, Redbooks, Jan. 1999 (84 pages). |
Pate et al., "RAMAC Virtual Array: Implementing Peer-to-Peer Remote Copy," IBM, Redbooks, Dec. 1998 (140 pages). |
Patterson et al., "SnapMirror®: File System Based Asynchronous Mirroring for Disaster Recovery," Usenix Association, Proceedings of the FAST 2002 Conference on File and Storage Technologies, Monterey, California, Jan. 28-30, 2002 (14 pages). |
Petition for Inter Partes Review of U.S. Pat. No. 8,299,944 Under 35 U.S.C. §§ 311-319 and 37 C.F.R. §§ 42.1-.80, 42.100-.123, Aug. 6, 2015 (43 pages). |
Petition for Inter Partes Review of U.S. Pat. No. 8,788,769 Under 35 U.S.C. §§ 311-319 and 37 C.F.R. §§ 42.1-.80, 42.100-.123, Aug. 7, 2015 (71 pages). |
Phillips, "Zumastor Linux Storage Server," Proceedings of the Linux Symposium, vol. 2, Ottawa, Ontario, Canada, Jun. 27-30, 2007 (14 pages). |
Prahlad et al., "Method for Managing SnapShots Generated by an Operating System or Other Application," U.S. Appl. No. 60/326,021, Sep. 28, 2001 (16 pages). |
Quinlan et al., "Venti: A New Approach to Archival Storage," Usenix Association, Proceedings of the FAST 2002 Conference on File and Storage Technologies, Monterey, California, Jan. 28-30, 2002 (14 pages). |
Sadagopan et al., "NetApp Technical Report: Oracle Fusion Middleware DR Solution Using NetApp Storage," NetApp., May 2008 (38 pages). |
Sarkar, "Instant Image: Transitive and Cyclical Snapshots in Distributed Storage Volumes," Euro-Par 2000, No Month Listed 2000 (pp. 1284-1291). |
Schuettinger et al., "Helping DBAs Become More Efficient: NetApp Efficiency and Manageability Advantages," NetApp, Inc., Jul. 2009 (12 pages). |
Solid et al., "Network Appliance Adds SnapRestore, SnapMirror to OnTap," Computergram International, Apr. 26, 1999 (2 pages). |
Solter et al., "OpenSolaris™ Bible," Wiley Publishing, Inc. Indianapolis, Indiana, No Month Listed 2009 (9 pages). |
Sweeney, "xFS In-core Inode Management," retrieved online at [URL:<<http://linux-xfs.sgi.com/projects/xfs/design-docs/>>] Nov. 29, 1993 (10 pages). |
Symantec Corporation, "Symantec Backup Exec Quick Recovery & Off-Host Backup Solutions for Microsoft Exchange Server 2003 & Microsoft SQL Server," No Month Listed 2005 (9 pages). |
Syncsort Incorporated, "Near-Instant Oracle Cloning with Syncsort AdvancedClient Technologies," No Month Listed 2007 (12 pages). |
Syncsort Incorporated, "Syncsort Backup Express Advanced Recovery for NetApp," No Month Listed 2008 (12 pages). |
Tate et al., "Implementing the IBM System Storage SAN Volume Controller V4.3," IBM, Redbooks, Oct. 2008 (970 pages). |
Thekkath et al., "Frangipani: A Scalable Distributed File System," Proceeding SOSP '97, Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles, No Month Listed 1997 (25 pages). |
Tretau et al., "IBM TotalStorage NAS Backup and Recovery Solutions," IBM, Redbooks, Jul. 2002 (226 pages). |
Veritas Software Corporation, "Veritas File System 4.1 Administrator's Guide," May 2005 (270 pages). |
Veritas Software Corporation, "Veritas FlashSnap Point-in-Time Copy Solutions, Administrator's Guide 4.1," Apr. 2006 (102 pages). |
Veritas Software Corporation, "Veritas NetBackup 4.5 Business Server™: Getting Started Guide," Mar. 2002 (91 pages). |
Veritas Software Corporation, "Veritas NetBackup™ 4.5 for Informix: System Administrator's Guide," Mar. 2002 (94 pages). |
Veritas Software Corporation, "Veritas NetBackup™ 4.5: User's Guide for UNIX," Mar. 2002 (212 pages). |
Vmware, "Vmware Consolidated Backup: Best Practices and Deployment Considerations for SAN Environments," retrieved online at [URL:<<https://web.archive.org/web/20080804070141/http://www.vmware.com/files/pdf/vcb-best-practices>>] No Month Listed 2007 (39 pages). |
Wolf, "VM Backup Bliss? The State of VM Data Protection in the Enterprise," Burton Group, Midvale, Utah, Aug. 1, 2007 (45 pages). |
You et al., "Deep Store: An Archival Storage System Architecture," Proceedings of the 21st International Conference on Data Engineering, No Month Listed 2005 (12 pages). |
Zhang et al., "yFS: A Journaling File System Design for Handling Large Data Sets with Reduced Seeking," Usenix Association, Proceedings of FAST '03: 2nd Usenix Conference on File and Storage Technologies, San Francisco, California, Mar. 31-Apr. 2, 2003 (15 pages). |
Zhu et al., "Avoiding the Disk Bottleneck in the Data Domain Deduplication File System," Proceedings of the 6th Usenix Conference on File and Storage Technologies FAST 2008, San Jose, California, No Month Listed 2008 (14 pages). |
Cited By (24)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11943290B2 (en) | 2008-12-22 | 2024-03-26 | Google Llc | Asynchronous distributed de-duplication for replicated content addressable storage clusters |
US10291699B2 (en) * | 2008-12-22 | 2019-05-14 | Google Llc | Asynchronous distributed de-duplication for replicated content addressable storage clusters |
US11663236B2 (en) | 2015-02-17 | 2023-05-30 | Cohesity, Inc. | Search and analytics for storage systems |
US10311083B2 (en) * | 2015-02-17 | 2019-06-04 | Cohesity, Inc. | Search and analytics for storage systems |
US11176165B2 (en) | 2015-02-17 | 2021-11-16 | Cohesity, Inc. | Search and analytics for storage systems |
US9826030B1 (en) * | 2015-06-04 | 2017-11-21 | Amazon Technologies, Inc. | Placement of volume partition replica pairs |
US11514025B2 (en) * | 2015-06-30 | 2022-11-29 | EMC IP Holding Company LLC | Snapshot conscious internal file data modification for network-attached storage |
US10452641B1 (en) * | 2015-06-30 | 2019-10-22 | EMC IP Holding Company LLC | Snapshot conscious internal file data modification for network-attached storage |
US10565058B1 (en) * | 2016-03-30 | 2020-02-18 | EMC IP Holding Company LLC | Adaptive hash-based data replication in a storage system |
US10437817B2 (en) | 2016-04-19 | 2019-10-08 | Huawei Technologies Co., Ltd. | Concurrent segmentation using vector processing |
US10459961B2 (en) | 2016-04-19 | 2019-10-29 | Huawei Technologies Co., Ltd. | Vector processing for segmentation hash values calculation |
US11630735B2 (en) * | 2016-08-26 | 2023-04-18 | International Business Machines Corporation | Advanced object replication using reduced metadata in object storage environments |
US20180060179A1 (en) * | 2016-08-26 | 2018-03-01 | International Business Machines Corporation | Advanced object replication using reduced metadata in object storage environments |
US10802922B2 (en) | 2016-08-26 | 2020-10-13 | International Business Machines Corporation | Accelerated deduplication block replication |
US11176097B2 (en) | 2016-08-26 | 2021-11-16 | International Business Machines Corporation | Accelerated deduplication block replication |
US10073747B2 (en) | 2016-09-19 | 2018-09-11 | International Business Machines Corporation | Reducing recovery time in disaster recovery/replication setup with multitier backend storage |
US10169167B2 (en) | 2016-09-19 | 2019-01-01 | International Business Machines Corporation | Reduced recovery time in disaster recovery/replication setup with multitier backend storage |
US10545863B2 (en) | 2017-09-21 | 2020-01-28 | Toshiba Memory Corporation | Memory system and method for controlling nonvolatile memory |
US11269531B2 (en) | 2017-10-25 | 2022-03-08 | International Business Machines Corporation | Performance of dispersed location-based deduplication |
US10922280B2 (en) | 2018-04-10 | 2021-02-16 | Nutanix, Inc. | Policy-based data deduplication |
US10795861B2 (en) | 2018-06-20 | 2020-10-06 | International Business Machines Corporation | Online measurement of potential deduplication efficiency |
US11469881B2 (en) * | 2018-12-26 | 2022-10-11 | Korea Institute Of Science And Technology | Apparatus and method for forgery prevention of digital information |
US11556562B1 (en) | 2021-07-29 | 2023-01-17 | Kyndryl, Inc. | Multi-destination probabilistic data replication |
US11960504B2 (en) | 2021-09-02 | 2024-04-16 | Bank Of America Corporation | Data replication over low-latency network |
Also Published As
Publication number | Publication date |
---|---|
US20140344216A1 (en) | 2014-11-20 |
AU2014265979A1 (en) | 2015-12-10 |
CA2912394A1 (en) | 2014-11-20 |
JP2016524220A (en) | 2016-08-12 |
EP2997501A2 (en) | 2016-03-23 |
US9646067B2 (en) | 2017-05-09 |
US20140351214A1 (en) | 2014-11-27 |
WO2014185974A2 (en) | 2014-11-20 |
WO2014185974A3 (en) | 2015-04-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9563683B2 (en) | Efficient data replication | |
US10037154B2 (en) | Incremental copy performance between data stores | |
US9772916B2 (en) | Resiliency director | |
US9384207B2 (en) | System and method for creating deduplicated copies of data by tracking temporal relationships among copies using higher-level hash structures | |
US9372758B2 (en) | System and method for performing a plurality of prescribed data management functions in a manner that reduces redundant access operations to primary storage | |
US10275474B2 (en) | System and method for managing deduplicated copies of data using temporal relationships among copies | |
US9372866B2 (en) | System and method for creating deduplicated copies of data by sending difference data between near-neighbor temporal states | |
US8396905B2 (en) | System and method for improved garbage collection operations in a deduplicated store by tracking temporal relationships among copies | |
US20150106580A1 (en) | System and method for performing backup or restore operations utilizing difference information and timeline state information | |
US20120124013A1 (en) | System and method for creating deduplicated copies of data storing non-lossy encodings of data directly in a content addressable store | |
CA2817592A1 (en) | Systems and methods for data management virtualization |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: HERCULES TECHNOLOGY GROWTH CAPITAL, INC., AS AGENT, CALIFORNIAFree format text: SECURITY INTEREST;ASSIGNOR:ACTIFIO, INC.;REEL/FRAME:036106/0004Effective date: 20150714Owner name: HERCULES TECHNOLOGY GROWTH CAPITAL, INC., AS AGENTFree format text: SECURITY INTEREST;ASSIGNOR:ACTIFIO, INC.;REEL/FRAME:036106/0004Effective date: 20150714 |
|
AS | Assignment |
Owner name: SILICON VALLEY BANK, MASSACHUSETTSFree format text: SECURITY AGREEMENT;ASSIGNOR:ACTIFIO, INC.;REEL/FRAME:036113/0970Effective date: 20150714 |
|
AS | Assignment |
Owner name: ACTIFIO, INC., MASSACHUSETTSFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:PROVENZANO, CHRISTOPHER A;GOLDBERG, KEITH;ABERCROMBIE, PHILIP J.;SIGNING DATES FROM 20151103 TO 20151115;REEL/FRAME:037086/0107 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: ACTIFIO, INC., MASSACHUSETTSFree format text: RELEASE BY SECURED PARTY;ASSIGNOR:SILICON VALLEY BANK;REEL/FRAME:042415/0317Effective date: 20170505Owner name: ACTIFIO, INC., MASSACHUSETTSFree format text: RELEASE BY SECURED PARTY;ASSIGNOR:HERCULES CAPITAL, INC. (FORMERLY KNOWN AS HERCULES TECHNOLOGY GROWTH CAPITAL, INC.);REEL/FRAME:042415/0395Effective date: 20170505 |
|
AS | Assignment |
Owner name: CRESTLINE DIRECT FINANCE, L.P., AS ADMINISTRATIVE AGENT, COLLATERAL AGENT AND SOLE LEAD ARRANGER, TEXASFree format text: SECURITY INTEREST;ASSIGNOR:ACTIFIO, INC.;REEL/FRAME:046702/0543Effective date: 20180802Owner name: CRESTLINE DIRECT FINANCE, L.P., AS ADMINISTRATIVEFree format text: SECURITY INTEREST;ASSIGNOR:ACTIFIO, INC.;REEL/FRAME:046702/0543Effective date: 20180802 |
|
AS | Assignment |
Owner name: ACTIFIO, INC., MASSACHUSETTSFree format text: RELEASE BY SECURED PARTY;ASSIGNOR:CRESTLINE DIRECT FINANCE, L.P., AS COLLATERAL AGENT;REEL/FRAME:053483/0084Effective date: 20200522 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: SMALL ENTITY |
|
FEPP | Fee payment procedure |
Free format text: SURCHARGE FOR LATE PAYMENT, SMALL ENTITY (ORIGINAL EVENT CODE: M2554); ENTITY STATUS OF PATENT OWNER: SMALL ENTITY |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YR, SMALL ENTITY (ORIGINAL EVENT CODE: M2551); ENTITY STATUS OF PATENT OWNER: SMALL ENTITYYear of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:ACTIFIO, INC.;REEL/FRAME:056911/0517Effective date: 20210316 |
|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |