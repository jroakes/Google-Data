CN108573701A - Inquiry based on lip detecting is endpoint formatting - Google Patents
Inquiry based on lip detecting is endpoint formatting Download PDFInfo
- Publication number
- CN108573701A CN108573701A CN201711049276.9A CN201711049276A CN108573701A CN 108573701 A CN108573701 A CN 108573701A CN 201711049276 A CN201711049276 A CN 201711049276A CN 108573701 A CN108573701 A CN 108573701A
- Authority
- CN
- China
- Prior art keywords
- video data
- frame
- audio data
- face
- lip
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
- G06V40/166—Detection; Localisation; Normalisation using acquisition arrangements
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/04—Segmentation; Word boundary detection
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/24—Speech recognition using non-acoustical features
- G10L15/25—Speech recognition using non-acoustical features using position of the lips, movement of the lips or face analysis
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0316—Speech enhancement, e.g. noise reduction or echo cancellation by changing the amplitude
- G10L21/0356—Speech enhancement, e.g. noise reduction or echo cancellation by changing the amplitude for synchronising with other signals, e.g. video signals
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/20—Speech recognition techniques specially adapted for robustness in adverse environments, e.g. in noise, of stress induced speech
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/225—Feedback of the input speech
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/227—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of the speaker; Human-factor methodology
Abstract
The system and method for describing the end-point detection for improving the speech inquiry submitted by user.In some embodiments, synchronous video data and audio data are received.Determination includes the frame sequence for the video data that corresponding image is moved with the lip in face.The first audio data based on the first frame corresponding to frame sequence and corresponding to the last frame of frame sequence second audio data come endpoint formatting audio data.The recording of endpoint formatting audio data is generated by automatic speech recognizer.Then the recording generated is provided for exporting.
Description
Technical field
This specification is usually directed to automatic speech recognition (ASR).
Background technology
Automatic speech recognition (ASR) is commonly used in the customer-furnished task of promotion and/or the completion of order.For example, intelligence
It is specified dynamic to be executed in response to the order that energy personal assistant (PA) system identifies the verbal order of user commonly using ASR
Make.PAS can be based on user's input, location aware and/or from various online sources (such as weather or traffic, news, stock
Admission fee lattice, subscriber calendar, retail price etc.) ability of access information executes action.
Invention content
Many PA systems usually only audio data coding speech (utterance) of processing user is to identify and record
(transcribe) it is inquired by the speech that user submits.However, if the audio data received includes for example due to background sound
The high noise level of sound is then likely difficult to accurately speech of endpoint formatting (endpointing) audio data to identify Yu be submitted
Language inquires the phonological component of corresponding audio.As a result, many PA systems often mistakenly identify or recorded audio data, including
Such as including do not correspond to user speech inquire audio section audio data, for example the speech of user inquiry before or it
Ambient noise afterwards.
In some embodiments, system can improve the end-point detection for the speech inquiry submitted by user.For example, system
Can initially obtain the audio data for encoding submitted speech inquiry and with include user when submitting speech inquiry
The video data that the audio data of the image of face obtained synchronizes.Then, which is distinguished using technology corresponding to language
The other parts (such as ambient noise) of the part of the audio data of sound input and the speech inquiry inputted corresponding to non-voice.Make
For example, system initially determines that the sequence of frames of video of the image including user face.Then, system identification includes detecting
The sequence of frames of video of the image of lip movement.In some embodiments, system determine sequence first frame and last frame and
Their corresponding time points.Then, in system identification audio data have with first and last frame of sequence of frames of video when
Between put the audio section of corresponding start and end time point.The endpoint formatting audio data of system provides audio to extract audio section
Section is to be output to ASR for recording.
The endpoint formatting technology described in the whole text can be used for providing various advantages for PA systems.For example, being regarded due to the use of synchronous
Frequency is according to verifying the phonological component of audio data, it is possible to using endpoint formatting technology for example reduce false positive (false
Positive) speech inquiry detection, miss (missed) for reducing specified PA systems activation term or phrase are detected, Huo Zheshi
The generation of multiple speech orders not in the audio data for the inquiry that coding receives.In addition, in some embodiments, inspection
The lip mobile data measured may be used as generating incorrect recording hypothesis by the speech recognition of ASR system individual authentication to reduce
(hypotheses) possibility.For example, the lip detected that can be directed to term and/or phrase described in instruction user moves
Dynamic data come verify based on speech recognition technology is generated applied to audio data baseline recording it is assumed that with identification and/or
Correct the term of wrong identification.
In one aspect, a method of computer implementation may include：Receive synchronous video data and audio data；
Determine that the frame sequence of video data includes image corresponding with the lip movement in face；Based on first corresponding to frame sequence
The second audio data of first audio data of frame and the last frame corresponding to frame sequence, endpoint formatting audio data；By certainly
Dynamic speech recognition device generates the recording of endpoint formatting audio data；And generated recording is provided to export.
One or more embodiments may include following optional feature.For example, in some embodiments, determining video
The frame sequence of data includes that image corresponding with the lip movement in face includes：Identification is opposite with the lip movement in face
The one or more features for the image answered count；And determine that one or more characteristic statistics identified include being confirmed as table
Show the characteristic statistics of lip movement related voice.
In some embodiments, this method further includes：Determine that video data includes user movement；And in response to determination
Video data includes user action, determines that the frame sequence of video data includes the image of face.
In some embodiments, synchronous video data and audio data are received from smart phone；The video of the synchronization
Data are captured by the front video of smart phone.
In some embodiments, endpoint formatting audio data corresponds to the audio for the speech inquiry that coding is submitted by user
A part for data.
In some embodiments, this method further includes：In response to determining that the frame sequence of video data includes the figure of face
Picture, activation personal assistant system are inquired with the speech for handling user's submission.
In some embodiments, determine that the frame sequence of video data includes figure corresponding with the lip movement in face
As including：Frame sequence is obtained from video data；And handle frame sequence, deep neural network configuration using deep neural network
For：Receive each frame in frame sequence；And the confidence of each frame in frame sequence is calculated, which indicates
The frame includes the possibility that corresponding image is moved with the lip in the face.
In some embodiments, this method further includes：Determine that the subset of the frame of video data includes the image of face, frame
Sequence includes the subset of frame, wherein determining that the frame sequence of video data includes image packet corresponding with the movement of the lip of face
It includes：In response to determining that the subset of the frame of video data includes the image of face, determine that the frame sequence of video data includes and face
On lip move corresponding image.
In some embodiments, determine that the subset of the frame of video data includes that the image of face includes：From video data
Obtain the subset of frame；The subset of frame is handled using deep neural network, deep neural network is configured as：The subset of receiving frame
Interior each frame；And the confidence of each frame in the subset of frame is calculated, which indicates that the frame includes being somebody's turn to do
The possibility of the image of face.
In some embodiments, endpoint formatting audio data includes：Identification corresponds to the first of the frame sequence of video data
First audio data of frame；Second audio data of the identification corresponding to the last frame of the frame sequence of video data；And it blocks
Audio data before the first audio data and after second audio data.
Other versions include corresponding system and computer program, are configured as executing and be compiled on computer memory device
The action of the method for code.
The details of one or more embodiments is elaborated in attached drawing and following specification.Other potential features and
Advantage will become apparent from specification, drawings and the claims.
Other embodiment in terms of these includes being configured as executing the method encoded on computer memory device
Correspondence system, device and the computer program of action.
Description of the drawings
Fig. 1, which is shown, to be shown come the system for determining endpoint that speech inquires section using the lip mobile data that detects
Example.
Fig. 2 shows can selectively handle the example for the system that speech is inquired using recording technology is substituted.
Fig. 3 shows the example for the system that can be used for training Fig. 1 and system shown in Fig. 2.
Fig. 4 shows the lip mobile data based on detecting to determine that speech inquires the example of the processing of the endpoint of section.
Fig. 5 shows the example for the computing device that can realize process described herein or part thereof on it.
In the accompanying drawings, identical reference numeral always shows corresponding part.
Specific implementation mode
In general, system can improve the end-point detection for the speech inquiry for submitting to personal assistant equipment by user.For example, being
System can initially obtain the audio data for encoding submitted speech inquiry and be included in user when submitting speech inquiry
The synchronizing video data of face image.System distinguishes part and the correspondence of the audio data inputted corresponding to voice using technology
In the other parts (such as ambient noise) of the speech inquiry of non-voice input.As an example, it includes detecting that system, which determines,
The sequence of the video frame of lip movement.The first frame and last frame of system identification sequence and its corresponding time point.The system
It identifies the following audio section of audio data, there is open corresponding with the time point of the first frame of sequence of frames of video and last frame
Begin and end time point.System provides audio section and is used for being output to ASR by extracting audio section come endpoint formatting audio data
Recording.
As described, " endpoint " may refer to the starting point or terminating point of audio section.For example, if using single endpoint
Carry out endpoint formatting single audio file, then two audio sections is generated, for example, from the beginning of audio file to the endpoint as terminating point
An audio section and from the endpoint as starting point to second audio section at the end of audio file.
Fig. 1 shows the PA systems that the endpoint of speech inquiry section can be determined using the lip mobile data detected
100 example.In this example, system 100 includes client device 110, face detection module 122, lip mobile module
124, endpoint module 126, ASR 128 and inquiry response generator 129 are inquired.
In general, system 100 can indicate any kind of intelligent personal assistants software, it is able to carry out such as voice friendship
Mutually, music playback, making do list, setting alarm clock, streaming audio data, offer information are (for example, weather traffic or reality
When information) task.In some embodiments, system 100 can also additionally be able to be used as home automation hub
(hub).System 100 can be set in such as client device 110 (such as passing through mobile application), by local network and client
The standby 110 another electronic equipments (such as supporting the personal assistant equipment of Wi-Fi) communicated are transported on client device 110
It is capable using associated server, or combinations thereof equipment on locally realize.
Client device 110 can be any kind of personal electric computing device for supporting network.For example, client is set
Standby 110 can be smart phone, lap-top computing devices, tablet computing device, intelligent personal assistants equipment, intelligence is wearable sets
One or more of the equipment of Internet of Things (IOT) ability of standby or any other type.
Face detection module 122, lip mobile module 124 and inquiry endpoint module 126, ASR 128 and inquiry response life
Grow up to be a useful person 129 can be software module in the system 100 that can be realized on application hardware element.For example, example shown in Fig. 1
In, module 122,124,126,128 and 129 is each associated with system 100, in service associated with client device 110
Device (for example, web server, application server or any other type can application server) on execute.
In other embodiments, face detection module 122, lip mobile module 124, inquiry endpoint module 126, ASR
128 and inquiry response generator 129 be software module, each of which is real on different hardware elements, for example different servers
It is existing.As an example, face detection module 122, lip mobile module 124, inquiry endpoint module 126 and inquiry response generator
129 can realize on the personal assistant service device that the order submitted in response to user obtains information, and ASR 128 can profit
It is realized with another different speech recognition server of identification and the speech inquiry submitted by user of recording.
In operation, user 102 initially submits speech 104 on client device 110, in client device 110
On handled and be encoded as audio data 104a.In response to receiving speech 104, client device 110 can be to user 102
Show user interface.For example, user interface can provide the instruction for seeing the front video to client device 110.Other
In example, user interface can provide information associated with speech 104 to alarm clock or notice, for example, in response to receiving words
The recording of the speech of sound 104.In some embodiments, user interface can provide the user executed by client device 110
The list of selectable device action.
The video data 106a of video camera capture user 102 associated with client device 110.In discribed example
In, the front video of client device 110 he/her say activation system 100 activation term or phrase (be known as " hot word
(hot word) "), the video data 106a of user 102 is captured after the phrase " good, computer " in such as speech 104.
In other examples, once user 102 provides input, such as speech input, text input, icon/button press, him/her is indicated
Speech 104 will be submitted, then video camera associated with client device 110 can be with automatic capture video.
Video data 106a is captured by video camera associated with client device 110, the video data 106a phases of capture
It is handled about audio data 104a.For example, the video frame in video data 106a and the audio section in audio data 104a are same
Step so that just provided with user 102 video frame of speech 104 associated time point with corresponding to the sound with audio data 104a
The associated corresponding time point alignment of frequency range.Then video data 106a and synchronous audio data 104b are sent to face's inspection
Survey module 122.
Face detection module 122 handles video data 106a to identify the face of user 102.Face detection module 122 can
Determine that the image of the face of user whether there is in the video frame of video data 106a to use face detection.Face
The example for the face detection that detection module 122 uses is by Zhu and Ramanan in entitled " Face Detection, Pose
Technology described in the research publication of Estimation, and Landmark Localization in the Wild ", can
In https：//www.Ics.uci.edu/~xzhu/paper/face-cvpr12.pdf is obtained.
In the example depicted in fig. 1, the identification of face detection module 122 is shot by the front video of client device 110
Video data 106a in the face 108 detected.Video data 106b includes the image for including the face 108 detected
Sequence of frames of video, and then synchronous audio data 104b is sent to lip mobile module 124.
Lip mobile module 124 handles video data 106b to identify that in video data 106b include corresponding to detect
Lip movement image sequence of frames of video.For example, lip mobile module 124 can iteratively compare the video of user face
Successive video frames in data 106b, to determine whether the lip of user moves between frames.In the example depicted in figure 1, lip
Portion's mobile module 124 determines the subset of the video frame for the image for including the face 108 and lip mobile data 109 that detect, example
Such as wherein detect the subset for the video data 106b that the lip of user moves between continuous video frame.
Lip mobile module 124 determines the lip detected in the sequence of frames of video identified in video data 106b
Whether mobile 109 is associated or corresponding with the voice of user.These video frame can be referred to as the voice of video data 106b
Part, and their corresponding part in audio data 104b can be referred to as the phonological component of audio data 104b.
Lip mobile module 124 can calculate characteristic statistics, instruction detection lip movement using various statistical techniques
Whether 109 part is associated with voice.For example, as it is following be described in detail in figure 3, lip mobile module 124 can answer
The characteristic statistics of video data 106b are calculated with various machine learning techniques, are then used it for differentiation and are inputted not phase with voice
The lip movement detected closed and the lip detected inputted corresponding to voice move.As an example, lip mobile module
124 can use such as deep neural network (deep neural network, DNN) or long short-term memory (long short-
Term memory, LSTM) network neural network framework come automatic distinguishing it is associated with voice detect lip movement
(or " being moved to the relevant lip that detects of voice ") and unrelated with voice lip movement (or " the non-language detected
The relevant lip movement of sound ").
Then, lip mobile module 124 extract be confirmed as in video data 106b include and voice is relevant detects
Lip movement video frame to generate video data 106c.Video data 106c including the sequence of frames of video and synchronous sound
Frequency may be sent to that inquiry endpoint module 126 according to 104b.
Endpoint module 126 is inquired based on processing video data 104c to identify the phonological component of synchronous audio data 104b
Carry out endpoint formatting audio data 104b.If video data 106c includes single sequence of frames of video, for example, the collection of successive video frames
It closes, then inquires endpoint module 126 based on the video frame with earliest time point in identification video data 106c (for example, starting
Frame) and video data 106c in the video frame (such as abort frame) with latest time point, come endpoint formatting audio data 104b.
Then inquiry endpoint module 126 identifies the time point corresponding to start frame and abort frame in synchronous audio data 104b respectively
Time point.Multiple audio sections can be generated based on endpoint formatting audio data 104b.
In the example depicted in fig. 1, inquiry 126 endpoint formatting audio data 104b of endpoint module from audio data 104b to give birth to
At three audio sections.In the audio section generated at three, inquiry endpoint module 126 is based on having in video data 106c and detect
With its associated video frame of voice relevant lip movement, determine that audio section 104c corresponds to phonological component.Show at this
In example, audio section 104c corresponds to the part that user 102 in audio data 104a says speech 104.It is true to inquire endpoint module 126
Fixed other two audio sections (such as the audio section labeled as " (1) " and audio section labeled as " (3) ") do not indicate that audio data
124 phonological component.This is because the video frame corresponding to audio section in video data 106a does not include the lip detected
It is mobile, or including the lip movement detected unrelated with voice as described above.For example, labeled as the audio segment table of " (1) "
Show that PAS activates phrase, such as " good, computer (OKAY CPMPUTER) ", and be labeled as the audio section expression of " (3) " with
Submit the residual sound that inquiry is collected later in family 102.
After audio sections of the endpoint formatting audio data 104b to generate audio data 104b, inquiry endpoint module 126 is right
Voice related audio section is sent afterwards for being output to ASR.In the example depicted in figure 1, inquiry endpoint module 126 sends audio section
104c is for being output to ASR 128.
128 recorded audio section 104c of ASR, and provide recording 104d to inquiry response generator 129.Inquiry response generates
Recording 104d is as the order for executing specific action for the processing of device 129.In discribed example, inquiry response generator 129 solves
Analysis recording in term, and determine the order be to provide Weather information for export to user 102.Then, inquiry response generates
Device 129 obtains the real-time weather information of associated with user 102 position, and generates response 112 for offer with to client
End equipment 110 exports.As indicated, be then responding to 112 as to recording 104d in include order response and be provided with
It exports to user 102.
In some embodiments, recording 104d additionally or alternatively can be sent to search engine by ASR 128, this is searched
Index, which is held up, for example to be scanned for based on the speech recorded in recording 104d.In such an embodiment, it is provided by user 102
Speech can be used for executing search, such as Webpage search or pass through locally applied search.
Above description is related to the exemplary realization of system 100.In other embodiments, system 100 can be configured
To provide one or more optional features.In some embodiments, the video camera of capture video data 106a can be with client
End equipment 110 is different.For example, if client device 110 is desk-top computing device or lap-top computing devices, the video camera
Can be user oriented individual web camera, because he/her uses client device 110.In other embodiments,
Video camera can be the equipment being placed in designated position so that when user 102 submits speech 104, user 102 regards at it
In open country.For example, in such an embodiment, video camera can be that the user in the specified region for example monitored in its visual field lives
Dynamic safety camera, television camera or some other types of fixed video cameras.In these embodiments each
In, the video data 106a of capture can be sent by connecting the local network of client device 110 and video camera.
It in some embodiments, can be by associated with client device 110 (including client device 110) multiple
Equipment captures video data 106.For example, can by client device 110 front video, capture attribute region peace
Group shot camera etc. collects video.In this embodiment, the video data collected by multiple devices can be sewn to one
It rises, is then communicated to face detection module 122 to be handled.
In some embodiments, for example, when the video camera for collecting video data 106a is the component of client device 110
When, video data 106a and audio data the 104a local synchronization on client device 110.Alternatively, if video camera and client
End equipment 110 is different, can be by video camera or another processing equipment (for example, face detection module 122 and/or lip move
The server that module 124 operates on it) execute synchronization.
In addition, system 100 can execute simultaneously operating in the Each point in time for handling data associated with speech 104.
For example, in the example depicted in fig. 1, system 100 the face for determining whether to detect user 102 in video data 106a it
Before, synchronizing video data 106a and audio data 104a.In this example, synchronize can be based on for example compare with by with client
110 associated video camera of equipment capture the associated audios of video data 106a and by associated with client device 110
Microphones capture audio data 104a.
Alternatively, in other embodiments, any time point before execution inquiry is endpoint formatting, such as in face detection
Later, but before lip movement detection, or after lip movement detection, but before inquiry is endpoint formatting, system 100 is held
Row simultaneously operating.It in this embodiment, can be by face detection module 122, lip detecting mobile module 124, inquiry end
Point module 126 or combinations thereof executes simultaneously operating.
In some embodiments, after the face for detecting user 102, face detection module 122 can handle video
Data 106a.The example of the processing operation executed by face detection module 122 includes：The visual field of video data 106a is reduced with right
The face that Ying Yu is detected, the perceptual property of adjustment video data 106a is to improve feature detection, such as brightness, contrast, face
Face feature (such as eyes, lip, nose in the visual field of color ratio, tone, saturation degree etc. and/or marking video data 106a
Son) position.
In some embodiments, the lip mobile data 109 detected can be used for independently verifying the dialogue of system 100
The identification and/or recording of sound 104.For example, lip mobile module 124 can identify in the lip movement 109 detected
Lip Move Mode, then determination be pre term associated with the lip Move Mode identified and/or phrase.Lip
Then portion's mobile module 124 identifies the term and/or phrase described in user 102 using the technology in speech 104.In this way
Embodiment in, lip mobile module 124 can be identified by the term and/or phrase described in user 102, without the use of acoustics
Model.In this respect, system 100 can generate recording for speech 104 based on the lip movement 109 detected it is assumed that
Also, such as by using acoustic model and language model, determine that the recording hypothesis based on detected lip movement 109 is
It is no consistent with the recognition hypotheses of speech 104 of audio data 104a are based only upon.In this respect, above-mentioned lip mobile detection technology
Can be used for reduce for example due to ambient noise, vacation or miss hot word detection and/or mistakenly multiple sequential queries are recorded
The possibility of incorrect recording is generated for single query.
In some embodiments, lip mobile module 124 and/or inquiry endpoint module 126 can use above-mentioned lip
It is mobile to detect and inquire multiple recording that the recording of larger inquiry is resolved to subquery by endpoint formatting technology.For example, larger
Inquiry can be that " good, computer, how is weather outsideI goes to work and to be late (OKAY COMPUTER, WHAT IS
THE WEATHER OUTSIDEI’M LATE FOR WORK)”.After the recording for generating entire phonological component, system 100 can
To determine that speech 104 includes three subqueries：" good, computer (OKAY COMPUTER) ", " temperature how (WHAT IS
THE TEMPERATURE) " and " I goes to work and to be late (I ' M LATE FOR WORK) ".The determination can be based on having and language
The video counts of the associated lip movement (such as limited lip movement between phrase) detected of transformation in sound phrase
According to the identification of the video frame in 106a.
Fig. 2 shows the personal assistant systems (PAS) 200 that inquiry can be selectively handled using replacement recording technology
Example.In this example, system 200 includes client device 210, mobile detection module 222, face detection module 224, lip
Mobile module 226, inquiry endpoint module 228 and speech act detection module 232.
In some embodiments, the component of system 200 executes the operation substantially similar with the component of system 100.For example,
By client device 210, face detection module 224, lip mobile module 226, inquiry endpoint module 228 and speech act detection
The function that module 232 executes and client device 110, face detection module 122, lip mobile module 124, inquiry endpoint module
126 and speech act detection module 128 execution function it is substantially similar.
In addition, in some embodiments, system can be incorporated to the combination of the feature of system 100 and system 200.In this way
Embodiment in, system can be inquired using face recognition and lip mobile data to handle the user that receives, such as
Whether can detect described in Fig. 1, and based on the face of user when receiving user's inquiry and/or the movement of user
Come use selectivity recording treatment technology, as described in below with reference to Fig. 2.
In general, system 200 handles the speech submitted with user using the transmission path of replacement using different technologies
204 associated data.For example, in the transmission path defined by path " A1 " and " B1 ", 200 use of system detects
Lip mobile data, such as lip mobile data 109, by similar to as above about the technology described in Fig. 2 in a manner of improve words
The speech recognition of sound 204.
Alternatively, in another transmission path defined by path " A1 " and " B1 ", system 200 is examined using only speech act
Survey the audio data 204a that module 232 handles encoded voice 204a as Default sound identification module.In the transmission path,
System 200 does not handle video data, because cannot detect the face of user when submitting speech 204.Fixed by path " A2 "
In another transmission path of justice, audio data 204a is similarly processed using speech act detection module 232 in system 200.
In the transmission path, system 200 does not handle video data, because near the client device 210 for receiving audio data 204a
It does not detect movement, indicates the face that can not possibly detect user in the video data of any collection.If such as video
The pixel of threshold number in data changes between being determined to be in successive frame, then can detect movement.In other examples
In, it can be based on using Object identifying and/or detection technique and being tracked and detecting in video in the visual field of video
Object is associated, the reference point in video detects movement.In some embodiments, video data can be handled to distinguish
Certain form of movement, such as move the associated movement detected with lip and move incoherent detection with lip
The movement arrived.
Referring now to example shown in Fig. 2, user 102 initially submits inquiry 204 to client device 210, in visitor
Audio data 204a is encoded as in family end equipment 210.Client device 210 receives audio data 204a, mobile detection module
222 determine whether detect movement near client device 210.For example, mobile detection module 222 can work as user 102
The motion sensor being placed on when submitting speech speech 204 in the region of the attribute where client device 210.
Mobile detection module 222 detects the movement near client device 210, to determine that the capture video of user 102 is
The no face that may include user 102, it is such as described referring to Fig.1 above.As shown in Fig. 2, if mobile detection module 222 is examined
The movement near client device 210 is measured, then system 200 is continued with transmission path " A1 ", and generation makes to set with client
The instruction of standby 210 associated video camera capture video data 206a.Video camera can with for capture video data 106a
Fig. 1 described in the similar mode of technology capture video data 206a.In this example, motion detection block 222 can be used
In the video capture of selectively triggering video camera, for example to be retained by not requiring video camera constantly to capture video data
The battery life of video camera and the power consumption for reducing video camera.
Alternatively, if mobile detection module 122 cannot detect that the movement near client device 210, system 200 are used
Transmission path " A2 " continues, and audio data 204a is sent to speech act detection module 232, without indicating video camera such as
It is upper to collect video data describedly.The details of operation about speech act detection module 232 is provided in further detail below.
In some embodiments, system 200 does not include mobile detection module 222.In such an embodiment, client
Audio data 204a and the video data 206a of capture are sent directly to face detection module 224 by end equipment 210.For example, one
Denier client device 210 receives audio data 204a, and video camera associated with client device 210 initially collects video
Data 206a.Then face detection module 224 handles the video data 206a of capture to use the face described above for Fig. 1
Identification technology determines whether to detect the face of user 102 in the visual field of the video data 206a of capture.
Face detection module 224 determines whether captured video data 206a associated with speech 204 includes user
102 face.As shown in Fig. 2, if the determination of face detection module 224 detects user in the video data 206a of capture
102 face, then system 200 is continued with transmission path " B1 ", and carries out the above-mentioned operation about described in Fig. 1.For example, face examines
It surveys module 224 and sends video data 206b and audio data 204a to lip mobile module 226, then make video data and sound
Frequency is according to synchronization, and the lip mobile data that recognition detection arrives, such as lip mobile data 109, as described above.Then, it looks into
It askes endpoint module 228 to be segmented synchronous audio data based on the lip mobile data detected, and such as the example of Fig. 1
It is shown, generate the recording 208a for audio section.
Alternatively, if face detection module 224 cannot detect the face of the user 102 in video data 206a, it is
System 200 is continued with transmission path " B2 ", and audio data 204a is sent to speech act detection module 232, without executing Fig. 1
Example shown in video processing technique.
Once system 200 enters any of transmission path " A2 " or " B2 ", audio data 204a is sent to speech
Activity detection module 232.As described in the whole text, speech act detection module 232 can for example use acoustic model and associated
Language model to pronounce (phonetically) record such as speech 204 speech inquiry ASR.For example, speech act
The pronunciation attribute that detection module 232 includes in audio data 204a based on processing generates the recording 208b for speech 204.
In some embodiments, system 200 is performed in parallel transmission path " B1 " and " B2 ".In this realization,
The data collected in one transmission path can be used for improving and/or supplement the data processing substituted in transmission path.For example, if
The face of user disappears during the part of video in video data 206a, then is generated by speech act detection module 232
Data can be used for through lip mobile module 226 as described above and inquiry 228 supplement process of endpoint module operation.
Fig. 3 shows that the example of training system 300, training system 300 can be used for training Fig. 1 and system shown in Fig. 2
100 and 200.System 300 includes the machine learning module 310 for the various assemblies that can be used for training system 100 and 200.Engineering
Video data that such as face detection module 122 and 224 collected with automatic detection (such as video counts can be trained by practising module 310
According to 106a and 206a) in face, training lip mobile module 124 and 226 to detect video data automatically (for example, video counts
According to 106b and 206b) in lip mobile data (such as lip mobile data 109) or training detection close to client device 210
Near movement motion detection block 222.
Machine learning module 310 can also be predicts one or more from one or more input using multilayer operation
Any suitable machine learning model of a output.For example, machine learning model 310 may include being located at input layer and output layer
Between one or more hidden layers.Then can by each layer output be used as network in another layer (such as next layer or
Output layer) input.In some embodiments, machine learning module 310 may include such as convolutional neural networks
(convolutional neural network, CNN), shot and long term remember (LSTM) network or combinations thereof.
For the various assemblies of training system 100 and 200, machine learning module 310 can use various statistical classification skills
Art determines the video data received in various processing stages, the video data 106a for example captured or video data 106b
Whether include being pre feature associated with the video frame of one group of manual classification.In the example depicted in fig. 3, machine learning
The access of module 310 includes the tranining database 312 of non-talking video frame 314a and the video frame 314b that speaks.
Non-talking video frame 314a corresponds to the video of the user for the phonological component for being confirmed as not corresponding to user's inquiry
Frame.For example, non-talking video frame 314a may include without the video frame or detection for detecting that the lip of user moves
It is unrelated with voice (for example, since the lip of user just on the feed is mobile and user to the lip movement that lip is mobile but detects
Cough associated lip movement etc.) video frame.It is determined to correspond to user on the contrary, the video frame 314b that speaks corresponds to and looks into
The video frame of the user of the phonological component of inquiry.
In some embodiments, each in video frame 314b can with when collect user video frame when user say
Word and/or phrase it is associated.For example, include in the video for providing the user that speech inquires " hello (HELLO) " regards
Frequency frame can be associated with term " hello (HELLO) ".In such an embodiment, machine learning module 310 can train
Lip mobile module 124 and 226 does not determine the phonological component of inquiry using only above-mentioned technology, but also executes speech recognition skill
Art, with mode-matching technique associated based on the lip mobile data for using with detecting identify term described in user or
Phrase.As an example, including and phrase " good, calculating if lip mobile module 124 determines the frame sequence of video data 106b
The associated lip Move Mode of machine (OKAY COMPUTER) ", then lip mobile module 124 can be independently of audio data 104a
To determine, table says phrase " good, computer (OKAY to user during the time series corresponding to frame sequence
COMPUTER)”。
Fig. 4 is shown for determining that speech inquires the processing 400 of the endpoint of section based on the lip mobile data detected
Example.In brief, handling 400 may include：Synchronous video data and audio data (410) are received, determines video counts
According to frame sequence include image (420) corresponding with the movement of the lip of face, endpoint formatting audio data (430) generates endpoint
The recording (440) of the audio data of change, and recording is provided to export (450).
In more detail, processing 400 may include receiving synchronous video data and audio data (410).For example, face examines
The video data 106a synchronous with video data 106a and audio data 104a can be received by surveying module 122.It audio data and regards
Frequency according to can for example on client device 110 local synchronization, or on the server using face detection module 122 it is remote
Journey synchronizes.As described above, synchronization process is related to the correspondence time point in identification audio 104a and video data 106b or is based on
Such as the audio of the audio and video data 104b of alignment audio data 104a.
In some embodiments, processing 400 can also comprise the image that determining sequence of frames of video includes face.At this
In the embodiment of sample, face detection module 122 determines that the sequence of the frame of video data 106a includes detecting for user 102
The image of face 108.As described above, face detection module 122 can determine video data using various face detections
Whether the frame of 106a includes feature associated with the face 108 detected.
Processing 400 can include determining that the frame sequence of video data includes the figure moved corresponding to the lip of face (541)
Picture.For example, in response to determining that the frame sequence of video data 106a includes the image for the face 108 of user 102 detected, lip
Mobile module 124 determines video data 106b comprising the frame with the image of the face detected includes comprising detecting
The frame sequence of lip movement 109.Then, lip mobile module 124 classifies to the lip movement 109 detected, with identification
Frame with the video data 106b with voice relevant lip movement, for example, video data 106b phonological component.Institute as above
It states, the phonological component of video data 106b typically refers to the wherein user 102 in video data 106b and provides certain type of mouth
Head input, the video frame such as saying hot word, speech inquiry is provided.
Processing 400 may include endpoint formatting audio data (430).For example, inquiry endpoint module 126 is based on such as audio section
The starting point and ending point of the audio section of 104c carrys out endpoint formatting audio data 104b.As shown in the example of figure 1, endpoint module is inquired
126 endpoint formatting audio data 104b are to generate three audio sections.In this example, audio section 104c corresponds in step 430
The frame sequence of the determining lip movement including voice association.Audio section 104c corresponds to the inquiry submitted by user 102, and two
A other audio sections indicate PAS activation commands (such as " good, computer (OKAY COMPUTER) ") or other types of non-language
Sound audio (such as ambient noise).In this example, although the lip of user is in the audio section phase corresponding with PAS activation commands
Between move, but the section is not still queried the processing of endpoint module 126, because its inquiry of the submission with user is unrelated.
Processing 400 may include generating the recording (440) of endpoint formatting audio data.For example, ASR 128, which is generated, indicates end
Reveal the recording 104d of the audio section 104c of audio data.As described by the example of figure 1, audio section 104c is selected for recording,
Because it is confirmed as indicating the phonological component of the audio data 104a for inquiring 104.
Processing 400 may include providing recording to export (450).It is used for example, automatic speech recognizer provides recording 104d
In being output to inquiry response generator 129.In the example depicted in fig. 1, then inquiry response generator 129 generates client and sets
Standby 110 are supplied to the response 112 of user 102 as output.
Fig. 5 can be used for the system and method described in this document being used as client computer or a server or multiple clothes
The block diagram of the computing device 500,550 of business device.Computing device 500 is intended to indicate that various forms of digital computers, such as above-knee
Type computer, desktop computer, work station, personal digital assistant, server, blade server, mainframe (mainframe) and
Other suitable computers.Computing device 550 is intended to indicate that various forms of mobile devices, such as personal digital assistant, honeycomb
Phone, smart phone and other similar computing devices.In addition, computing device 500 or 550 may include universal serial bus
(USB) flash drive.USB flash drive can store an operating system and other application program.USB flash drive can be with
Including input output assembly, such as it can be inserted into the wireless transmitter or USB connector of the USB port of another computing device.Here
Shown in component, their connection and relationship and their function be only exemplary, be not intended to limit this document
Described in and/or claimed invention embodiment.
Computing device 500 includes processor 502, memory 504, storage device 506, is connected to memory 504 and high speed
The high-speed interface 508 of ECP Extended Capabilities Port 510 and it is connected to low speed bus 514 and the low-speed interface 512 of storage device 506.Component
502, each in 504,506,508,510 and 512 uses various bus interconnections, and may be mounted on public mainboard or
Suitably install in other ways.Processor 502 can handle the instruction for being executed in computing device 500, including storage
Instruction in memory 504 or in storage device 506, (to be such as coupled to high-speed interface in external input/output device
508 display 516) on show the graphical information of GUI.In other embodiments, multiple processors can be suitably used
And/or multiple buses and multiple memories and multiple memorizers.Furthermore, it is possible to multiple computing devices 500 are connected, each equipment
The part of necessary operation is provided, such as server group, one group of blade server or multicomputer system.
Memory 504 is in 500 inner storag information of computing device.In one embodiment, memory 504 be (one or
It is multiple) volatile memory-elements.In another embodiment, memory 504 is (one or more) nonvolatile memory
Unit.Memory 504 can also be another form of computer-readable medium, such as disk or CD.
Storage device 506 can be that computing device 500 provides massive store.In one embodiment, storage device
506 can be or comprising computer-readable medium, such as floppy device, hard disc apparatus, compact disk equipment or tape unit, flash memory or
Other similar solid storage devices or the equipment array for including the equipment in storage area network or other configurations.Computer
Program product can be tangibly embodied in information carrier.Computer program product can also include instruction, upon being performed
Carry out one or more methods, method as escribed above.Information carrier is computer or machine-readable media, such as memory
504, the memory on storage device 506 or processor 502.
High-speed controller 508 manage computing device 500 bandwidth-intensive operations, and low speed controller 512 manage it is relatively low
Bandwidth-intensive operations.This distribution of function is merely exemplary.In one embodiment, high-speed controller 508 is for example
It is coupled to memory 504, display 516 by graphics processor or accelerator, and (does not show to acceptable various expansion cards
Go out) high-speed expansion ports 510.In this embodiment, low speed controller 512 is coupled to storage device 506 and low-speed expansion
Port 514.The low-speed expansion port that may include various communication port (such as USB, bluetooth, Ethernet, wireless ethernet) can
To be coupled to such as keyboard, indicating equipment, microphone/speaker to one or more input-output apparatus of, scanner or all
Such as the network equipment (such as passing through network adapter) of switch or router.Computing device 500 can be with a variety of different shapes
Formula realizes, as shown in the figure.For example, it may be implemented as standard server 520, or in server as one group it is more
Secondary realization.It can also be implemented as the part of frame server system 524.In addition, it can be in such as laptop computer
It is realized in 522 personal computer.Alternatively, the component from computing device 500 (can such as be set with mobile device (not shown)
It is combined for other components in 550).Each in these equipment can include one or more in computing device 500,550
It is a, and whole system can be made of the multiple computing devices 500,550 to communicate with one another.
Computing device 500 can realize with many different forms, as shown in the figure.For example, it may be implemented as marking
Quasi- server 520, or repeatedly realized in server as one group.It can also be implemented as frame server system
524 part.In addition, it can be realized in such as personal computer of laptop computer 522.Alternatively, from calculating
The component of equipment 500 can be combined with other component (not shown) (such as equipment 550) in mobile device.In these equipment
Each can include computing device 500, one or more of 550, and whole system can be multiple by what is communicated with one another
Computing device 500,550 forms.
Computing device 550 includes processor 552, memory 564 and such as input-output apparatus of display 554, leads to
Believe interface 666 and transceiver 568 etc..Equipment 550 may be provided with the storage device of such as microdrive or miscellaneous equipment,
To provide additional storage.Each in component 550,552,564,554,566 and 568 uses various bus interconnections, and
Multiple components can be in other ways installed on public mainboard or suitably.
Processor 552 can execute the instruction in computing device 550, including the instruction being stored in memory 564.Processing
Device may be implemented as including the individual chipset with the chips of multiple analog- and digital- processors.In addition, processor can be with
It is realized using any one of many frameworks.For example, processor 510 can be CISC (Complex Instruction Set Computer) processing
Device, RISC (Reduced Instruction Set Computer) processors or MISC (minimum instruction collection computer) processor.Processor can be such as
The coordination for providing other components of equipment 550, such as user interface, the application run by equipment 550 and is carried out by equipment 550
Wireless communication control.
Processor 552 can be carried out by the control interface 458 and display interface 456 for being coupled to display 554 with user
Communication.Display 554 can be such as TFT (Thin Film Transistor-LCD) displays or OLED (Organic Light Emitting Diode)
Display or other display technologies appropriate.Display interface device 556 may include for driving display 554 to be presented to user
The proper circuit of figure and other information.Control interface 558 can receive order from the user and will convert them to submit
To processor 552.It is furthermore possible to also provide the external interface 562 communicated with processor 552, so as to be carried out with miscellaneous equipment
The near region field communication of equipment 550.External interface 562 can provide such as wire communication in some embodiments, or at it
Wireless communication is provided in its embodiment, and multiple interfaces can also be used.
Memory 564 is in 550 inner storag information of computing device.Memory 564 may be implemented as computer-readable medium
Or one or more of medium, volatile memory-elements or Nonvolatile memery unit.Extended menory 574 can be with
Equipment 550 is provided and is connected to by expansion interface 572, and expansion interface 572 may include that such as SIMM (deposit by single row direct insert
Memory modules) card interface.This extended menory 574 can be that equipment 550 provides additional memory space, or can also deposit
Store up application or the other information of equipment 550.Specifically, extended menory 574 may include the finger for executing or supplementing above-mentioned processing
It enables, and can also include security information.Thus, for example, extended menory 574 may be provided as the peace for equipment 550
Full module, and can be programmed with the instruction of the safe handling of permission equipment 550.Furthermore, it is possible to since via SIMM cards
Security application and additional information are provided, such as identification information is placed on SIMM cards in a manner of not assailable.
Memory may include such as flash memory and or NVRAM memory, as described below.In one embodiment, it calculates
Machine program product is tangibly embodied in information carrier.Computer program product includes instruction, carries out one upon being performed
A or multiple methods, method as escribed above.Information carrier is such as memory 564, extended menory 574 or can for example lead to
Cross the computer or machine-readable media of the memory on the processor 552 that transceiver 568 or external interface 562 receive.
Equipment 550 can be carried out wireless communication by communication interface 566, and communication interface 566 can include number if necessary
Word signal processing circuit.Communication interface 666 can provide such as calling of GSM speeches, SMS, EMS or MMS message, CDMA, TDMA,
The various patterns of PDC, WCDMA, CDMA2000 or GPRS etc. or the communication of agreement.This communication can for example pass through radio-frequency receiving-transmitting
Machine 668 occurs.Furthermore it is possible to which short haul connection occurs, such as (do not shown using transceiver as bluetooth, Wi-Fi or other
Go out).In addition, GPS (global positioning system) receiver module 570 can be provided to equipment 550 it is additional navigation it is related to position
Wireless data, can be suitably used by the application run in equipment 550.
Equipment 550 can also audibly be communicated using audio codec 560, and audio codec 560 can be from user
It receives spoken message and is converted into available digital information.Audio codec 560 can equally be generated for user audible
The sound seen, such as example, by the loud speaker in the mobile phone of equipment 550.This sound may include being exhaled from speech phone
The sound cried may include the sound, such as verbal messages, music file etc. of record, and can also include by equipment 550
The sound that the application of upper operation generates.
Computing device 550 can realize with many different forms, as shown in the figure.For example, it may be implemented as bee
Cellular telephone 580.It can also be implemented as smart phone 582, personal digital assistant or other similar mobile devices a part.
The various realizations of system and method described herein can in Fundamental Digital Circuit, integrated circuit, specially design
ASIC (application-specific integrated circuit), computer hardware, firmware, software and/or these embodiments combination in realize.These are various
Embodiment may include the implementation in one or more computer programs executable and/or interpretable on programmable systems
Mode, the programmable system include at least one programmable processor that can be special or general purpose, are coupled to
It receives from it data and instruction and storage system, at least one input equipment and at least one output is arrived in transmission data and instruction
Equipment.
These computer programs (also referred to as program, software, software application or code) include for programmable processing
The machine instruction of device, and can be realized with advanced programs and/or object-oriented programming languages and/or assembly/machine language.
As it is used herein, term " machine readable media ", " computer-readable medium " refer to any computer program product, device
And/or equipment, such as disk, CD, memory, programmable logic device (PLD), for providing machine to programmable processor
Instruction and/or data, including receive the machine readable media of the machine instruction as machine-readable signal.Term is " machine readable
Signal " refers to any signal for providing machine instruction and/or data to programmable processor.
In order to provide the interaction with user, system and technology described herein can with display device (such as
The CRT (cathode-ray tube) or LCD (liquid crystal display) monitor of information are shown to user) and keyboard and indicating equipment (example
Such as mouse or trace ball, user can provide input by it to computer) computer on realize.Other classes can also be used
The equipment of type provides the interaction with user；For example, it can be any type of sense feedback to be supplied to the feedback of user, such as
Visual feedback, audio feedback or touch feedback；And input from the user, including sound, language can be received in any form
Sound or sense of touch.
System and technology described herein can be including aft-end assembly (for example, as data server) or including centres
Part component (for example, application server) or including front end assemblies (for example, the visitor with graphic user interface or Web browser
Family end computer, user can pass through the embodiment party of graphic user interface or Web browser and system described herein and technology
Formula or one or more this rear end, middleware or front end assemblies any combinations interact) computing system in realize.
The component of system can be interconnected by any form or medium of digital data communications, such as communication network.Communication network
Example includes LAN (" LAN ") and wide area network (" WAN "), such as internet.
Computing system may include client and server.Client and server is generally remote from each other, and usually logical
Communication network is crossed to interact.The relationship of client and server be due to running and having on each computer between
The computer program of client-server relation and generate.
Many embodiments have been described.It will be appreciated, however, that the case where not departing from the spirit and scope of the present invention
Under, various modifications can be carried out.In addition, logic flow shown in attached drawing do not need shown in particular order or sequence sequence
To realize desired result.Furthermore, it is possible to provide other steps from described flow, or can with removal process, and
Other components are added or removed to the system that can be described to it.Therefore, other embodiments are within the scope of the appended claims.
Claims (20)
1. a method of computer implementation, including：
Receive synchronous video data and audio data；
Determine that the frame sequence of the video data includes image corresponding with the lip movement in face；
Second audio of the first audio data based on the first frame corresponding to frame sequence and the last frame corresponding to frame sequence
Data carry out endpoint formatting audio data；
The recording of endpoint formatting audio data is generated by automatic speech recognizer；And
The recording generated is provided to export.
2. according to the method described in claim 1, wherein determining that the frame sequence of the video data includes and the lip in face
Moving corresponding image includes：
Identify that the one or more features that corresponding image is moved with the lip of face count；And
Determine that one or more of identified characteristic statistics include the feature for being confirmed as indicating lip movement related voice
Statistics.
3. according to the method described in claim 1, including：
Determine that video data includes user action；And
In response to determining that video data includes user action, determine that the frame sequence of video data includes the image of face.
4. according to the method described in claim 1, wherein：
Synchronous video data and audio data are received from smart phone；And
Synchronous video data is captured by the front video of smart phone.
5. according to the method described in claim 1, the wherein described endpoint formatting audio data corresponds to what coding was submitted by user
A part for the audio data of speech inquiry.
6. according to the method described in claim 5, including：
In response to determining that the frame sequence of video data includes the image of face, activation personal assistant system is submitted with handling by user
Speech inquiry.
7. according to the method described in claim 1, wherein determining that the frame sequence of the video data includes and the lip in face
Moving corresponding image includes：
Frame sequence is obtained from video data；And
Frame sequence is handled using deep neural network, the deep neural network is configured to：
Receive each frame in frame sequence；And
The confidence of each frame in frame sequence is calculated, the confidence indicates that frame includes being moved with the lip in face
Move the possibility of corresponding image.
8. according to the method described in claim 1, including：
Determining that the subset of the frame of the video data includes the image of the face, the frame sequence includes the subset of the frame,
Wherein determine that the frame sequence of the video data includes that image corresponding with the lip movement in the face includes：
In response to determining that the subset of the frame of video data includes the image of face, determine that the frame sequence of video data includes and face
On lip move corresponding image.
9. according to the method described in claim 8, wherein determining that the subset of the frame of the video data includes the image packet of face
It includes：
The subset of frame is obtained from video data；
The subset of frame is handled using deep neural network, the deep neural network is configured to：
Each frame in the subset of receiving frame；And
Calculate the confidence of each frame in the subset of frame, the confidence indicate frame include the image of face can
It can property.
10. according to the method described in claim 1, wherein, the endpoint formatting audio data includes：
First audio data of the identification corresponding to the first frame of the frame sequence of video data；
Second audio data of the identification corresponding to the last frame of the frame sequence of video data；And
Block the audio data before the first audio data and after second audio data.
11. a kind of system, including：
One or more computers；And
One or more storage devices of store instruction, described instruction make institute when being executed by one or more of computers
It states one or more computers and executes operation, the operation includes：
Receive synchronous video data and audio data；
Determine that the frame sequence of the video data includes image corresponding with the lip movement in face；
Second audio of the first audio data based on the first frame corresponding to frame sequence and the last frame corresponding to frame sequence
Data, endpoint formatting audio data；
The recording of endpoint formatting audio data is generated by automatic speech recognizer；And
The recording generated is provided to export.
12. system according to claim 11, wherein determining that the frame sequence of the video data includes and the lip in face
Move corresponding image in portion：
Identify that the one or more features that corresponding image is moved with the lip of face count；And
Determine that one or more of identified characteristic statistics include the feature for being confirmed as indicating lip movement related voice
Statistics.
13. system according to claim 11, including：
Determine that video data includes user action；And
In response to determining that video data includes user action, determine that the frame sequence of video data includes the image of face.
14. system according to claim 11, wherein：
Synchronous video data and audio data are received from smart phone；And
The video data of the synchronization is captured by the front video of smart phone.
15. system according to claim 11 is submitted wherein the endpoint formatting audio data corresponds to coding by user
Speech inquiry the audio data a part.
16. a kind of encoding the non-transitory computer readable storage devices for having computer program instructions, the computer program refers to
Order makes one or more of computers execute operation when being executed by one or more computers, and the operation includes：
Receive synchronous video data and audio data；
Determine that the frame sequence of the video data includes image corresponding with the lip movement in face；
Second audio of the first audio data based on the first frame corresponding to frame sequence and the last frame corresponding to frame sequence
Data, endpoint formatting audio data；
The recording of endpoint formatting audio data is generated by automatic speech recognizer；And
The recording generated is provided to export.
17. equipment according to claim 16, wherein determine that the frame sequence of the video data includes and the lip in face
Move corresponding image in portion：
Identify that the one or more features that corresponding image is moved with the lip of face count；And
Determine that one or more of identified characteristic statistics include the feature system for being determined as indicating lip movement related voice
Meter.
18. equipment according to claim 16, including：
Determine that video data includes user action；And
In response to determining that video data includes user action, determine that the frame sequence of video data includes the image of face.
19. equipment according to claim 16, wherein：
Synchronous video data and audio data are received from smart phone；And
The video data of the synchronization is captured by the front video of smart phone.
20. equipment according to claim 16 is submitted wherein the endpoint formatting audio data corresponds to coding by user
Speech inquiry the audio data a part.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202111366940.9A CN114141245A (en) | 2017-03-14 | 2017-10-31 | Query endpointing based on lip detection |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/458,214 | 2017-03-14 | ||
US15/458,214 US10332515B2 (en) | 2017-03-14 | 2017-03-14 | Query endpointing based on lip detection |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202111366940.9A Division CN114141245A (en) | 2017-03-14 | 2017-10-31 | Query endpointing based on lip detection |
Publications (2)
Publication Number | Publication Date |
---|---|
CN108573701A true CN108573701A (en) | 2018-09-25 |
CN108573701B CN108573701B (en) | 2021-11-30 |
Family
ID=60452748
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201711049276.9A Active CN108573701B (en) | 2017-03-14 | 2017-10-31 | Query endpointing based on lip detection |
CN202111366940.9A Pending CN114141245A (en) | 2017-03-14 | 2017-10-31 | Query endpointing based on lip detection |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202111366940.9A Pending CN114141245A (en) | 2017-03-14 | 2017-10-31 | Query endpointing based on lip detection |
Country Status (5)
Country | Link |
---|---|
US (4) | US10332515B2 (en) |
CN (2) | CN108573701B (en) |
DE (2) | DE202017106586U1 (en) |
GB (2) | GB2581886B (en) |
WO (1) | WO2018169568A1 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112397093A (en) * | 2020-12-04 | 2021-02-23 | 中国联合网络通信集团有限公司 | Voice detection method and device |
CN112567457A (en) * | 2019-12-13 | 2021-03-26 | 华为技术有限公司 | Voice detection method, prediction model training method, device, equipment and medium |
CN113129893A (en) * | 2019-12-30 | 2021-07-16 | Oppo（重庆）智能科技有限公司 | Voice recognition method, device, equipment and storage medium |
CN113380236A (en) * | 2021-06-07 | 2021-09-10 | 斑马网络技术有限公司 | Voice endpoint detection method and device based on lip, vehicle-mounted terminal and storage medium |
Families Citing this family (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10332515B2 (en) | 2017-03-14 | 2019-06-25 | Google Llc | Query endpointing based on lip detection |
WO2018169381A1 (en) * | 2017-03-17 | 2018-09-20 | Samsung Electronics Co., Ltd. | Method and system for automatically managing operations of electronic device |
US11335334B2 (en) * | 2017-11-02 | 2022-05-17 | Sony Corporation | Information processing device and information processing method |
US11348576B1 (en) * | 2017-12-06 | 2022-05-31 | Amazon Technologies, Inc. | Universal and user-specific command processing |
DE102018212902A1 (en) * | 2018-08-02 | 2020-02-06 | Bayerische Motoren Werke Aktiengesellschaft | Method for determining a digital assistant for performing a vehicle function from a multiplicity of digital assistants in a vehicle, computer-readable medium, system, and vehicle |
US10863971B2 (en) * | 2018-11-30 | 2020-12-15 | Fujifilm Sonosite, Inc. | Touchless input ultrasound control |
KR20200073733A (en) | 2018-12-14 | 2020-06-24 | 삼성전자주식회사 | Method for executing function and Electronic device using the same |
JP7442631B2 (en) * | 2019-10-18 | 2024-03-04 | グーグル エルエルシー | End-to-end multi-speaker audiovisual automatic speech recognition |
CN110827823A (en) * | 2019-11-13 | 2020-02-21 | 联想(北京)有限公司 | Voice auxiliary recognition method and device, storage medium and electronic equipment |
CN114730563A (en) * | 2019-11-18 | 2022-07-08 | 谷歌有限责任公司 | Re-scoring automatic speech recognition hypotheses using audio-visual matching |
CN112863496B (en) * | 2019-11-27 | 2024-04-02 | 阿里巴巴集团控股有限公司 | Voice endpoint detection method and device |
SE545310C2 (en) * | 2019-12-20 | 2023-06-27 | Tobii Ab | Improved turn-taking |
US11687778B2 (en) | 2020-01-06 | 2023-06-27 | The Research Foundation For The State University Of New York | Fakecatcher: detection of synthetic portrait videos using biological signals |
KR20210112726A (en) * | 2020-03-06 | 2021-09-15 | 엘지전자 주식회사 | Providing interactive assistant for each seat in the vehicle |
DE112021001301T5 (en) * | 2020-05-13 | 2023-04-06 | Nvidia Corporation | DIALOGUE-BASED AI PLATFORM WITH RENDERED GRAPHIC OUTPUT |
KR20220010259A (en) | 2020-07-17 | 2022-01-25 | 삼성전자주식회사 | Natural language processing method and apparatus |
KR20220059629A (en) * | 2020-11-03 | 2022-05-10 | 현대자동차주식회사 | Vehicle and method for controlling thereof |
US20220179615A1 (en) * | 2020-12-09 | 2022-06-09 | Cerence Operating Company | Automotive infotainment system with spatially-cognizant applications that interact with a speech interface |
US11659217B1 (en) * | 2021-03-29 | 2023-05-23 | Amazon Technologies, Inc. | Event based audio-video sync detection |
CN113223500B (en) * | 2021-04-12 | 2022-02-25 | 北京百度网讯科技有限公司 | Speech recognition method, method for training speech recognition model and corresponding device |
CN113345472B (en) * | 2021-05-08 | 2022-03-25 | 北京百度网讯科技有限公司 | Voice endpoint detection method and device, electronic equipment and storage medium |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5838678A (en) * | 1996-07-24 | 1998-11-17 | Davis; Joseph W. | Method and device for preprocessing streams of encoded data to facilitate decoding streams back-to back |
US20030171932A1 (en) * | 2002-03-07 | 2003-09-11 | Biing-Hwang Juang | Speech recognition |
US20070136071A1 (en) * | 2005-12-08 | 2007-06-14 | Lee Soo J | Apparatus and method for speech segment detection and system for speech recognition |
CN102279977A (en) * | 2010-06-14 | 2011-12-14 | 索尼公司 | Information processing apparatus, information processing method, and program |
Family Cites Families (42)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5621858A (en) | 1992-05-26 | 1997-04-15 | Ricoh Corporation | Neural network acoustic and visual speech recognition system training method and apparatus |
US6471420B1 (en) | 1994-05-13 | 2002-10-29 | Matsushita Electric Industrial Co., Ltd. | Voice selection apparatus voice response apparatus, and game apparatus using word tables from which selected words are output as voice selections |
US5586171A (en) | 1994-07-07 | 1996-12-17 | Bell Atlantic Network Services, Inc. | Selection of a voice recognition data base responsive to video data |
US5907351A (en) * | 1995-10-24 | 1999-05-25 | Lucent Technologies Inc. | Method and apparatus for cross-modal predictive coding for talking head sequences |
US6735566B1 (en) * | 1998-10-09 | 2004-05-11 | Mitsubishi Electric Research Laboratories, Inc. | Generating realistic facial animation from speech |
EP1039446B1 (en) * | 1998-10-09 | 2010-12-08 | Sony Corporation | Learning device and method, recognizing device and method, and recording medium |
US7219062B2 (en) | 2002-01-30 | 2007-05-15 | Koninklijke Philips Electronics N.V. | Speech activity detection using acoustic and facial characteristics in an automatic speech recognition system |
US7587318B2 (en) * | 2002-09-12 | 2009-09-08 | Broadcom Corporation | Correlating video images of lip movements with audio signals to improve speech recognition |
US7133535B2 (en) * | 2002-12-21 | 2006-11-07 | Microsoft Corp. | System and method for real time lip synchronization |
EP1443498B1 (en) | 2003-01-24 | 2008-03-19 | Sony Ericsson Mobile Communications AB | Noise reduction and audio-visual speech activity detection |
US7499104B2 (en) * | 2003-05-16 | 2009-03-03 | Pixel Instruments Corporation | Method and apparatus for determining relative timing of image and associated information |
US20040243416A1 (en) | 2003-06-02 | 2004-12-02 | Gardos Thomas R. | Speech recognition |
WO2005025224A1 (en) * | 2003-09-02 | 2005-03-17 | Sony Corporation | Content reception device, video/audio output timing control method, and content providing system |
JP2005101931A (en) * | 2003-09-25 | 2005-04-14 | Fuji Photo Film Co Ltd | Image printer |
US20050228673A1 (en) * | 2004-03-30 | 2005-10-13 | Nefian Ara V | Techniques for separating and evaluating audio and video source data |
JP4286860B2 (en) | 2004-05-21 | 2009-07-01 | 旭化成株式会社 | Operation content determination device |
JP2009218874A (en) * | 2008-03-11 | 2009-09-24 | Victor Co Of Japan Ltd | Recording/reproducing device |
EP2104105A1 (en) * | 2008-03-20 | 2009-09-23 | British Telecommunications Public Limited Company | Digital audio and video clip encoding |
KR101092820B1 (en) * | 2009-09-22 | 2011-12-12 | 현대자동차주식회사 | Lipreading and Voice recognition combination multimodal interface system |
US8629938B2 (en) | 2009-10-05 | 2014-01-14 | Sony Corporation | Multi-point television motion sensor system and method |
US8451312B2 (en) | 2010-01-06 | 2013-05-28 | Apple Inc. | Automatic video stream selection |
WO2012053867A1 (en) * | 2010-10-21 | 2012-04-26 | Samsung Electronics Co., Ltd. | Method and apparatus for recognizing an emotion of an individual based on facial action units |
US9251854B2 (en) * | 2011-02-18 | 2016-02-02 | Google Inc. | Facial detection, recognition and bookmarking in videos |
US9081571B2 (en) | 2012-11-29 | 2015-07-14 | Amazon Technologies, Inc. | Gesture detection management for an electronic device |
US20140320648A1 (en) * | 2013-04-23 | 2014-10-30 | Canary Connect, Inc. | Remote User Interface & Display For Events For A Monitored Location |
US20140333782A1 (en) * | 2013-05-07 | 2014-11-13 | Texas Instruments Incorporated | View-assisted image stabilization system and method |
US20150019206A1 (en) * | 2013-07-10 | 2015-01-15 | Datascription Llc | Metadata extraction of non-transcribed video and audio streams |
WO2015172157A1 (en) * | 2014-05-09 | 2015-11-12 | Lyve Minds, Inc. | Image organization by date |
CN208027742U (en) * | 2014-07-28 | 2018-10-30 | 菲力尔洛莱施公司 | Video concentration systems |
US10264175B2 (en) * | 2014-09-09 | 2019-04-16 | ProSports Technologies, LLC | Facial recognition for event venue cameras |
DE112014007265T5 (en) * | 2014-12-18 | 2017-09-07 | Mitsubishi Electric Corporation | Speech recognition device and speech recognition method |
US10109277B2 (en) * | 2015-04-27 | 2018-10-23 | Nuance Communications, Inc. | Methods and apparatus for speech recognition using visual information |
US20160342845A1 (en) * | 2015-04-28 | 2016-11-24 | Arcsoft Inc. | Detection zones |
TWI564791B (en) * | 2015-05-19 | 2017-01-01 | 卡訊電子股份有限公司 | Broadcast control system, method, computer program product and computer readable medium |
US9743042B1 (en) * | 2016-02-19 | 2017-08-22 | Microsoft Technology Licensing, Llc | Communication event |
CN105915798A (en) * | 2016-06-02 | 2016-08-31 | 北京小米移动软件有限公司 | Camera control method in video conference and control device thereof |
JP6789690B2 (en) * | 2016-06-23 | 2020-11-25 | キヤノン株式会社 | Signal processing equipment, signal processing methods, and programs |
US20180018970A1 (en) * | 2016-07-15 | 2018-01-18 | Google Inc. | Neural network for recognition of signals in multiple sensory domains |
DE112016007236T5 (en) * | 2016-09-16 | 2019-07-04 | Motorola Solutions, Inc. | System and method for the cooperation of a fixed camera and an unmanned mobile device to improve the identification security of an object |
US10652397B2 (en) * | 2016-10-07 | 2020-05-12 | Samsung Electronics Co., Ltd. | Terminal device and method for performing call function |
US10108849B2 (en) * | 2016-10-14 | 2018-10-23 | Bank Of America Corporation | Biometric facial recognition for accessing device and authorizing event processing |
US10332515B2 (en) | 2017-03-14 | 2019-06-25 | Google Llc | Query endpointing based on lip detection |
-
2017
- 2017-03-14 US US15/458,214 patent/US10332515B2/en active Active
- 2017-10-30 DE DE202017106586.3U patent/DE202017106586U1/en active Active
- 2017-10-30 GB GB2003401.3A patent/GB2581886B/en active Active
- 2017-10-30 GB GB1717843.5A patent/GB2560598B/en active Active
- 2017-10-30 DE DE102017125396.3A patent/DE102017125396B4/en active Active
- 2017-10-30 WO PCT/US2017/059037 patent/WO2018169568A1/en active Application Filing
- 2017-10-31 CN CN201711049276.9A patent/CN108573701B/en active Active
- 2017-10-31 CN CN202111366940.9A patent/CN114141245A/en active Pending
-
2019
- 2019-05-15 US US16/412,677 patent/US10755714B2/en active Active
-
2020
- 2020-07-23 US US16/936,948 patent/US11308963B2/en active Active
-
2022
- 2022-04-18 US US17/722,960 patent/US20220238112A1/en not_active Abandoned
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5838678A (en) * | 1996-07-24 | 1998-11-17 | Davis; Joseph W. | Method and device for preprocessing streams of encoded data to facilitate decoding streams back-to back |
US20030171932A1 (en) * | 2002-03-07 | 2003-09-11 | Biing-Hwang Juang | Speech recognition |
US20070136071A1 (en) * | 2005-12-08 | 2007-06-14 | Lee Soo J | Apparatus and method for speech segment detection and system for speech recognition |
CN102279977A (en) * | 2010-06-14 | 2011-12-14 | 索尼公司 | Information processing apparatus, information processing method, and program |
Non-Patent Citations (1)
Title |
---|
MASAKI AOKI等: "Voice Activity Detection by Lip Shape Tracking Using", 《ACM MM 2007》 * |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112567457A (en) * | 2019-12-13 | 2021-03-26 | 华为技术有限公司 | Voice detection method, prediction model training method, device, equipment and medium |
WO2021114224A1 (en) * | 2019-12-13 | 2021-06-17 | 华为技术有限公司 | Voice detection method, prediction model training method, apparatus, device, and medium |
CN112567457B (en) * | 2019-12-13 | 2021-12-10 | 华为技术有限公司 | Voice detection method, prediction model training method, device, equipment and medium |
CN113129893A (en) * | 2019-12-30 | 2021-07-16 | Oppo（重庆）智能科技有限公司 | Voice recognition method, device, equipment and storage medium |
CN113129893B (en) * | 2019-12-30 | 2022-09-02 | Oppo（重庆）智能科技有限公司 | Voice recognition method, device, equipment and storage medium |
CN112397093A (en) * | 2020-12-04 | 2021-02-23 | 中国联合网络通信集团有限公司 | Voice detection method and device |
CN112397093B (en) * | 2020-12-04 | 2024-02-27 | 中国联合网络通信集团有限公司 | Voice detection method and device |
CN113380236A (en) * | 2021-06-07 | 2021-09-10 | 斑马网络技术有限公司 | Voice endpoint detection method and device based on lip, vehicle-mounted terminal and storage medium |
Also Published As
Publication number | Publication date |
---|---|
CN108573701B (en) | 2021-11-30 |
US20220238112A1 (en) | 2022-07-28 |
US20180268812A1 (en) | 2018-09-20 |
US20200357401A1 (en) | 2020-11-12 |
US20190333507A1 (en) | 2019-10-31 |
DE102017125396A1 (en) | 2018-09-20 |
US10332515B2 (en) | 2019-06-25 |
WO2018169568A1 (en) | 2018-09-20 |
DE202017106586U1 (en) | 2018-06-18 |
US10755714B2 (en) | 2020-08-25 |
GB2581886A (en) | 2020-09-02 |
GB2560598A (en) | 2018-09-19 |
GB2581886B (en) | 2021-02-24 |
GB202003401D0 (en) | 2020-04-22 |
GB201717843D0 (en) | 2017-12-13 |
CN114141245A (en) | 2022-03-04 |
DE102017125396B4 (en) | 2022-05-05 |
US11308963B2 (en) | 2022-04-19 |
GB2560598B (en) | 2020-04-22 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108573701A (en) | Inquiry based on lip detecting is endpoint formatting | |
US10522136B2 (en) | Method and device for training acoustic model, computer device and storage medium | |
US11276407B2 (en) | Metadata-based diarization of teleconferences | |
US10878824B2 (en) | Speech-to-text generation using video-speech matching from a primary speaker | |
US20190341058A1 (en) | Joint neural network for speaker recognition | |
US10846522B2 (en) | Speaking classification using audio-visual data | |
CN112037791B (en) | Conference summary transcription method, apparatus and storage medium | |
CN111261162B (en) | Speech recognition method, speech recognition apparatus, and storage medium | |
CN108305618B (en) | Voice acquisition and search method, intelligent pen, search terminal and storage medium | |
US11100932B2 (en) | Robust start-end point detection algorithm using neural network | |
CN110570873A (en) | voiceprint wake-up method and device, computer equipment and storage medium | |
WO2021115268A1 (en) | Method and apparatus for determining running direction of metro train, and terminal and storage medium | |
US20210110815A1 (en) | Method and apparatus for determining semantic meaning of pronoun | |
CN111127699A (en) | Method, system, equipment and medium for automatically recording automobile defect data | |
JP6875819B2 (en) | Acoustic model input data normalization device and method, and voice recognition device | |
CN116246610A (en) | Conference record generation method and system based on multi-mode identification | |
CN110516083B (en) | Album management method, storage medium and electronic device | |
CN113129867A (en) | Training method of voice recognition model, voice recognition method, device and equipment | |
WO2020073839A1 (en) | Voice wake-up method, apparatus and system, and electronic device | |
CN107123420A (en) | Voice recognition system and interaction method thereof | |
CN112037772B (en) | Response obligation detection method, system and device based on multiple modes | |
US11929070B1 (en) | Machine learning label generation | |
CN116189680B (en) | Voice wake-up method of exhibition intelligent equipment | |
US11792365B1 (en) | Message data analysis for response recommendations | |
CN116844555A (en) | Method and device for vehicle voice interaction, vehicle, electronic equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |