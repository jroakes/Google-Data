CN116635821A - Proximity-based control on a second device - Google Patents
Proximity-based control on a second device Download PDFInfo
- Publication number
- CN116635821A CN116635821A CN202180084490.6A CN202180084490A CN116635821A CN 116635821 A CN116635821 A CN 116635821A CN 202180084490 A CN202180084490 A CN 202180084490A CN 116635821 A CN116635821 A CN 116635821A
- Authority
- CN
- China
- Prior art keywords
- assistant
- enabled
- enabled devices
- user
- proximity
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04847—Interaction techniques to control parameter settings, e.g. interaction with sliders or dials
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/165—Management of the audio stream, e.g. setting of volume, audio stream path
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04B—TRANSMISSION
- H04B17/00—Monitoring; Testing
- H04B17/30—Monitoring; Testing of propagation channels
- H04B17/309—Measuring or estimating channel quality parameters
- H04B17/318—Received signal strength
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W4/00—Services specially adapted for wireless communication networks; Facilities therefor
- H04W4/02—Services making use of location information
- H04W4/023—Services making use of location information using mutual or relative location information between multiple location based services [LBS] targets or of distance thresholds
Abstract
A method (500) includes obtaining proximity information (250) for each of a plurality of assistant-enabled devices (210) within an environment (32) of a user device (200). Each assistant-enabled device is controllable by the assistant application (106) to perform a respective set of available actions (120) associated with the assistant-enabled device. For each assistant-enabled device, the method further includes determining a proximity score based on proximity information indicating a proximity estimate of the corresponding assistant-enabled device relative to the user device (260). The method further comprises the steps of: a ranked list of candidate assistant-enabled devices is generated using the determined proximity scores for the assistant-enabled devices (310), and for each corresponding assistant-enabled device in the ranked list, a set of respective controls (220) for executing a respective set of actions associated with the corresponding assistant-enabled device is displayed in a Graphical User Interface (GUI) (400).
Description
Technical Field
The present disclosure relates to a proximity-based control on a second device.
Background
Individuals may have many connected devices within their environments that may be controlled by various applications. Such connected devices may include, for example, smart light bulbs, smart televisions, smart speakers, smart thermostats, smart security systems, or any smart appliance (e.g., a smart oven) configured to perform a corresponding set of actions. In some instances, a user may be provided with a particular application for controlling a particular connected device. However, in order to control a connected device, a user must identify an application from among numerous applications running on the user device, select an application to control a particular connected device, and then navigate within the particular application to control the connected device. Managing multiple controls for controlling each of the connected devices from a single user device can become confusing and cumbersome. Additionally, many connected devices may perform overlapping respective sets of actions, making it difficult to discern to which connected device the user's command is directed.
Disclosure of Invention
One aspect of the present disclosure provides a method for controlling an assistant-enabled device. The method includes obtaining, by data processing hardware of a user device, proximity information for each of a plurality of assistant-enabled devices within an environment of the user device. Each of the plurality of assistant-enabled devices is controllable by the assistant application to perform a respective set of available actions associated with the assistant-enabled device. For each of the plurality of assistant-enabled devices, the method further includes determining, by the data processing hardware, a proximity score based on the proximity information obtained for the corresponding assistant-enabled device. The proximity score indicates a proximity estimate of the corresponding assistant-enabled device relative to the user devices in the environment. The method further comprises the steps of: using the plurality of proximity scores determined for the plurality of assistant-enabled devices by the data processing hardware, generating a ranked list of candidate assistant-enabled devices from among the plurality of assistant-enabled devices, and for each of one or more corresponding assistant-enabled devices in the ranked list of candidate assistant-enabled devices, displaying, by the data processing hardware, a respective set of controls for performing a respective set of actions associated with the corresponding assistant-enabled devices in a Graphical User Interface (GUI) displayed on a screen in communication with the data processing hardware.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, the method further includes receiving, at the data processing hardware, a user request from a user of the user device to launch the helper application to execute on the data processing hardware. In these implementations, obtaining proximity information for each of a plurality of assistant-enabled devices occurs during execution of an assistant application on the data processing hardware. In additional embodiments, receiving the user request includes one of: receiving, in a GUI displayed on a screen, a user input indication indicating a selection of a graphical element representing an assistant application; receiving a voice input from a user, the voice input including a call command for launching a helper application for execution on data processing hardware; or detect a predefined movement/gesture of the user device configured to launch a helper application to execute on the data processing hardware.
In some examples, obtaining proximity information for at least one assistant-enabled device of a plurality of assistant-enabled devices within an environment of the user device includes: receiving, at a sensor of a user device, a wireless communication signal transmitted from at least one assistant-enabled device of a plurality of assistant-enabled devices; and determining proximity information for the at least one assistant-enabled device based on a signal strength of a wireless communication signal received at a sensor of the user device. In some implementations, the method further includes obtaining, by the data processing hardware, directional information for at least one of the plurality of assistant-enabled devices within the environment of the user device by: the method includes receiving, at each sensor in a sensor array of the user device, a wireless communication signal transmitted from at least one of the plurality of assistant-enabled devices, and determining directional information for the at least one assistant-enabled device based on respective signal strengths of the wireless communication signals received at each sensor in the sensor array of the user device relative to respective signal strengths of the wireless communication signals received at other sensors in the sensor array of the user device. Here, determining the proximity score for the at least one assistant-enabled device is further based on directionality information for the at least one assistant-enabled device.
In some examples, receiving proximity information for at least one assistant-enabled device of a plurality of assistant-enabled devices within an environment of a user device includes: the method includes receiving, at a user device, an audible or inaudible signal output from at least one of a plurality of assistant-enabled devices, and determining proximity information for the at least one assistant-enabled device based on energy and/or frequency of the audible or inaudible signal output from the at least one assistant-enabled device. In some embodiments, the method further comprises: at the data processing hardware, receiving, from each of the plurality of assistant-enabled devices, a respective set of available actions associated with the corresponding assistant-enabled device, and for each corresponding assistant-enabled device in the ranked list of candidate assistant-enabled devices, determining, by the data processing hardware, a respective set of controls for executing the respective set of actions associated with the corresponding assistant-enabled device. In these implementations, at least one available action in the available respective set of actions received from at least one of the plurality of assistant-enabled devices can include a suggested action for the corresponding assistant-enabled device to perform based on the current context. In additional examples, the method may include receiving, at the data processing hardware, device state information associated with a corresponding assistant-enabled device from at least one assistant-enabled device of the plurality of assistant-enabled devices. Here, determining the respective control sets for performing the respective sets of actions associated with the corresponding assistant-enabled devices is further based on device state information associated with the corresponding assistant-enabled devices.
In some implementations, generating a ranked list of candidate assistant-enabled devices from among the plurality of assistant-enabled devices includes ordering the assistant-enabled devices from the assistant-enabled device having the closest proximity to the user device to the assistant-enabled device having the farthest proximity to the user device. In some examples, generating a ranked list of candidate assistant-enabled devices from among the plurality of assistant-enabled devices includes discarding from the ranked list of candidate assistant-enabled devices any assistant-enabled devices that include a proximity score indicating a proximity estimate that meets a maximum distance threshold. Generating a ranked list of candidate assistant-enabled devices from among the plurality of assistant-enabled devices may additionally or alternatively include discarding from the ranked list of candidate assistant-enabled devices any assistant-enabled devices that include a proximity score indicating a proximity estimate that meets a minimum distance threshold.
In some examples, the method further comprises: at the data processing hardware, receiving device state information associated with at least one assistant-enabled device from at least one assistant-enabled device of the plurality of assistant-enabled devices, and after generating the ranked list of candidate assistant-enabled devices, re-ranking, by the data processing hardware, candidate assistant-enabled devices in the ranked list of candidate assistant-enabled devices using the device state information associated with the at least one assistant-enabled device. Here, displaying the respective control sets for performing the respective sets of actions associated with the corresponding assistant-enabled devices is based on a re-ranking of candidate assistant-enabled devices in a ranked list of candidate assistant-enabled devices.
The ranked list of candidate assistant-enabled devices from the plurality of assistant-enabled devices may include assistant-enabled devices selected from the plurality of assistant-enabled devices having N highest proximity scores among a plurality of proximity scores determined for the plurality of assistant-enabled devices. In some examples, displaying respective sets of controls for performing respective sets of actions associated with corresponding assistant-enabled devices includes: one of the assistant-enabled devices in the ranked list of candidate assistant-enabled devices displays a respective set of controls in the GUI differently than a respective set of controls displayed in the GUI for at least another one of the assistant-enabled devices in the ranked list of candidate assistant-enabled devices.
Another aspect of the present disclosure provides a user device for controlling an assistant-enabled device located within an environment of the user device. The user device includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that, when executed by the data processing hardware, cause the data processing hardware to perform operations comprising: proximity information is obtained for each of a plurality of assistant-enabled devices within an environment of a user device. Each of the plurality of assistant-enabled devices is controllable by the assistant application to perform a respective set of available actions associated with the assistant-enabled device. The operations also include, for each of the plurality of assistant-enabled devices, determining a proximity score based on proximity information obtained for the corresponding assistant-enabled device. Here, the proximity score indicates a proximity estimate of the corresponding assistant-enabled device relative to the user devices in the environment. The operations further comprise: using the plurality of proximity scores determined for the plurality of assistant-enabled devices, a ranked list of candidate assistant-enabled devices from among the plurality of assistant-enabled devices is generated, and for each of one or more corresponding assistant-enabled devices in the ranked list of candidate assistant-enabled devices, a respective set of controls for performing a respective set of actions associated with the corresponding assistant-enabled device is displayed in a Graphical User Interface (GUI) displayed on a screen in communication with the data processing hardware.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, the operations further include receiving a user request from a user of the user device requesting to launch the assistant application. In these implementations, obtaining proximity information for each of the plurality of assistant-enabled devices occurs during execution of the assistant application. In additional embodiments, receiving the user request includes one of: receiving, in a GUI displayed on a screen, a user input indication indicating a selection of a graphical element representing an assistant application; receiving a voice input from a user, the voice input including a call command for launching a helper application for execution on data processing hardware; or detect a predefined movement/gesture of the user device configured to launch a helper application to execute on the data processing hardware.
In some examples, obtaining proximity information for at least one assistant-enabled device of a plurality of assistant-enabled devices within an environment of the user device includes: receiving, at a sensor of a user device, a wireless communication signal transmitted from at least one assistant-enabled device of a plurality of assistant-enabled devices; and determining proximity information for the at least one assistant-enabled device based on a signal strength of a wireless communication signal received at a sensor of the user device. In some implementations, the operations further include obtaining directionality information for at least one assistant-enabled device of a plurality of assistant-enabled devices within an environment of the user device by: the method includes receiving, at each sensor in a sensor array of the user device, a wireless communication signal transmitted from at least one of the plurality of assistant-enabled devices, and determining directional information for the at least one assistant-enabled device based on respective signal strengths of the wireless communication signals received at each sensor in the sensor array of the user device relative to respective signal strengths of the wireless communication signals received at other sensors in the sensor array of the user device. Here, determining the proximity score for the at least one assistant-enabled device is further based on directionality information for the at least one assistant-enabled device.
In some examples, receiving proximity information for at least one assistant-enabled device of a plurality of assistant-enabled devices within an environment of a user device includes: receiving an audible or inaudible signal output from at least one assistant-enabled device of the plurality of assistant-enabled devices; and determining proximity information for the at least one assistant-enabled device based on energy and/or frequency of audible or inaudible signals output from the at least one assistant-enabled device. In some implementations, the operations further comprise: the method includes receiving, from each of a plurality of assistant-enabled devices, an available respective set of actions associated with the corresponding assistant-enabled device, and for each corresponding assistant-enabled device in a ranked list of candidate assistant-enabled devices, determining a respective set of controls for executing the respective set of actions associated with the corresponding assistant-enabled device. In these implementations, at least one available action in the available respective set of actions received from at least one of the plurality of assistant-enabled devices can include a suggested action for the corresponding assistant-enabled device to perform based on the current context. In additional examples, the operations may include receiving, from at least one of the plurality of assistant-enabled devices, device state information associated with the corresponding assistant-enabled device. Here, determining the respective control sets for performing the respective sets of actions associated with the corresponding assistant-enabled devices is further based on device state information associated with the corresponding assistant-enabled devices.
In some implementations, generating a ranked list of candidate assistant-enabled devices from among the plurality of assistant-enabled devices includes ordering the assistant-enabled devices from the assistant-enabled device having the closest proximity to the user device to the assistant-enabled device having the farthest proximity to the user device. In some examples, generating a ranked list of candidate assistant-enabled devices from among the plurality of assistant-enabled devices includes discarding from the ranked list of candidate assistant-enabled devices any assistant-enabled devices that include a proximity score indicating a proximity estimate that meets a maximum distance threshold. Generating a ranked list of candidate assistant-enabled devices from among the plurality of assistant-enabled devices may additionally or alternatively include discarding from the ranked list of candidate assistant-enabled devices any assistant-enabled devices that include a proximity score indicating a proximity estimate that meets a minimum distance threshold.
In some examples, the operations further comprise: the method includes receiving device state information associated with at least one assistant-enabled device from at least one assistant-enabled device of a plurality of assistant-enabled devices, and after generating a ranked list of candidate assistant-enabled devices, re-ranking candidate assistant-enabled devices in the ranked list of candidate assistant-enabled devices using the device state information associated with the at least one assistant-enabled device. Here, displaying the respective control sets for performing the respective sets of actions associated with the corresponding assistant-enabled devices is based on a re-ranking of candidate assistant-enabled devices in a ranked list of candidate assistant-enabled devices.
The ranked list of candidate assistant-enabled devices from the plurality of assistant-enabled devices may include assistant-enabled devices selected from the plurality of assistant-enabled devices having N highest proximity scores among a plurality of proximity scores determined for the plurality of assistant-enabled devices. In some examples, displaying respective sets of controls for performing respective sets of actions associated with corresponding assistant-enabled devices includes: one of the assistant-enabled devices in the ranked list of candidate assistant-enabled devices displays a respective set of controls in the GUI differently than a respective set of controls displayed in the GUI for at least another one of the assistant-enabled devices in the ranked list of candidate assistant-enabled devices.
The details of one or more embodiments of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
Drawings
FIG. 1 is a schematic diagram of an example system including controlling an assistant-enabled device located within an environment of a user device.
Fig. 2 is a schematic diagram of an example user device in communication with an assistant-enabled device.
Fig. 3A-3C are schematic diagrams 300a-C of a user device controlling an assistant-enabled device within an environment of the user device.
Fig. 4A-4C are example GUIs 400a-C rendered on a screen of a user device to display respective sets of controls for performing respective sets of actions associated with each corresponding assistant-enabled device.
Fig. 5 is a flow chart of an example arrangement of operations of a method for controlling an assistant-enabled device.
FIG. 6 is a schematic diagram of an example computing device that may be used to implement the systems and methods described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
The present disclosure relates generally to user devices executing an assistant application to control one or more assistant-enabled devices via an interface of a portable client device. In particular, the user device may automatically detect which assistant-enabled device the user of the user device most likely wishes to control from his or her user device among a plurality of assistant-enabled devices within the environment of the user device. The assistant-enabled devices may include any internet of things (IoT) device, such as, for example, a smart light, a smart television, a smart speaker, a smart thermostat, a smart security system, or any smart appliance (e.g., a smart oven) configured to perform a corresponding set of actions. That is, a user device (e.g., smart phone, tablet computer, smart watch, smart display, etc.) executing an assistant application may provide a respective set of controls for controlling the assistant-enabled device to perform one or more of the respective set of actions. For example, the user device may display in a graphical user interface a respective set of controls of any available actions that the assistant-enabled device is capable of performing at a given instance, wherein the graphical user interface may receive a user input indication (e.g., touch input and/or voice input) indicating selection of one of the controls to cause the assistant-enabled device to perform the respective action. Managing multiple controls for controlling each device from a single user device can become confusing and cumbersome as users interact with more and more assistant-enabled devices in their environment (e.g., home or office). Additionally, assistant-enabled devices may share overlapping capabilities and similar device names, creating a need to disambiguate corresponding controls between devices.
Embodiments herein relate to automatically detecting among a plurality of assistant-enabled devices which assistant-enabled device or devices a user is most likely to wish to control via an interface displayed on a screen of a user device associated with the user. The detection is based on the proximity of each assistant-enabled device relative to the user device. For example, each assistant-enabled device within the environment of the user device may broadcast proximity information receivable by the user device, which the user device may use to determine the proximity of each assistant-enabled device relative to the user device. The proximity information may include a wireless communication signal, such as WiFi, bluetooth, or ultrasound, wherein a signal strength of the wireless communication signal received at the user device may be related to a proximity (e.g., distance) of the assistant-enabled device relative to the user device. The proximity information may also include a user input indication that instructs the user to interact with one of the assistant-enabled devices, for example, by touching the device to adjust the volume. By utilizing proximity-based detection, the user can reach his or her intended control in fewer steps and in less time. The user device may obtain proximity information for each of the assistant-enabled devices in an environment of a user that may be controlled by the user device. That is, based on the proximity information obtained from each of these assistant-enabled devices, the user device may determine a respective proximity score for each assistant-enabled device that indicates a proximity estimate of the respective assistant-enabled device relative to the user device. Additionally, the user device may collect directionality information from the one or more assistant-enabled devices to indicate a direction/orientation of the user device relative to each of the one or more assistant-enabled devices. That is, the directionality information may indicate whether the user device is facing or pointing to a given assistant-enabled device to serve as an indicator that the user intends to use the user device to control the assistant-enabled device to perform the set of available corresponding actions. In these scenarios, the directionality information may be used in combination to determine the respective proximity score, or the directionality information may be used to bias the proximity score determined from the proximity information alone. After obtaining the proximity information and/or the directionality information, the user device may determine a proximity score for each of the assistant-enabled devices in the environment. After determining the proximity score for each of the assistant-enabled devices in the environment, the user device ranks the assistant-enabled devices (e.g., via execution of the assistant application) based on the proximity scores to prioritize the assistant-enabled devices that the user most likely intends to control at a given moment. Thereafter, the user device displays respective sets of controls for one or more assistant-enabled devices in a Graphical User Interface (GUI) displayed on a screen of the user device for controlling the one or more assistant-enabled devices to perform the respective actions currently available. For example, the control of the assistant-enabled device with the highest ranking proximity score may be displayed at the top of the screen in a larger font than other controls associated with other assistant-enabled devices in the user environment with lower ranking proximity scores. Additionally, because the user is unlikely to want to control an assistant-enabled device with a very low rank proximity score at a given moment, such a device may be omitted from display in the GUI.
Referring to FIG. 1, in some embodiments, a proximity-based controller system 100 includes a user device 200 that communicates via a network 10 with a plurality of assistant-enabled devices 210, 210a-n within an environment 32 of the user device 200. The environment 32 may include a house of the user 30 associated with the user device 200. In some examples, environment 32 includes a portion of a house of user 30, such as a floor or other section of the house. The network 10 may include a Local Area Network (LAN) (e.g., a home area network HAN) that facilitates communication and interoperability between user devices 200 within an environment 32, such as a user's home, school, or office. In the example shown, the assistant-enabled device 210 is located throughout the home 32 of the user 30 having a first floor and a second floor, with the smart speaker 210a, the smart light 210b, the smart television 210c, and the smart thermostat 210d located on the first floor, and the second smart television 210e and the second smart speaker 210f located on the second floor, for example, in a bedroom of the home 32 of the user 30. The user device 200 may communicate with each of the assistant-enabled devices 210 via a wireless connection using standard communication techniques and/or protocols. Thus, the network 10 may include wireless fidelity (WiFi) (e.g., 802.11), worldwide Interoperability for Microwave Access (WiMAX), 3G, 4G, long Term Evolution (LTE), 5G, digital Subscriber Line (DSL), bluetooth, near Field Communication (NFC), or any other wireless standard. House 32 may include one or more Access Points (APs) (not shown) configured to facilitate wireless communications between user device 200 and one or more assistant-enabled devices 210, 210 a-f.
User device 200 may be any computing device capable of wireless communication with assistant-enabled device 210. Although user device 200 comprises a tablet computer in the illustrated example, user device 200 may also comprise, but is not limited to, a smart phone, a smart watch, a laptop computer, a desktop computer, or a smart display. The user device 200 may use a variety of different operating systems 104. In the example where the user device 200 is a mobile device, the user device 200 may run an operating system including, but not limited to, those developed by google corporationDeveloped by apple Inc.)>Or WINDOWS +.>Thus, the operating system 104 running on the user device 200 may include, but is not limited to Or WINDOWS->One of them. In some examples, user device 200 may run an operating system including, but not limited to, MICROSOFT ++developed by Microsoft corporation>MAC developed by apple Corp->Or Linux. The user device 200 may communicate with the assistant-enabled devices 210, 210a-f while running an operating system 104 other than those operating systems 104 described above, whether currently available or developed in the future. The operating system 104 may execute one or more software applications 106.
Software application 106 may refer to computer software that, when executed by a computing device, causes the computing device to perform tasks. In some examples, the software application 106 is referred to as an "application," app, "or" program. Example software applications 106 include, but are not limited to, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and games. The application 106 may execute on a variety of different user devices 200. In some examples, the application 106 is installed on the user device 200 before the user 30 purchases the user device 200. In other examples, the user 30 downloads and installs the application 106 on the user device 200.
In some implementations, the user device 200 executes the assistant application 106, the assistant application 106 initiates communication with each assistant-enabled device 210, and provides a respective set of controls 220 for executing a respective set of actions 120 associated with each assistant-enabled device 210. That is, each assistant-enabled device 210 of the plurality of assistant-enabled devices 210 is controllable by the assistant application 106 to perform a set of available respective actions 120 associated with the assistant-enabled device 210. Advantageously, the assistant application 106 allows the user 30 to control and/or configure each assistant-enabled device 210 using an interface 400, such as a Graphical User Interface (GUI) 400 that the application 106 can render for display on a screen of the user device 200. In this manner, the user 30 does not have to rely on various separate applications to control the various devices in their home, but can execute the assistant application 106 to indicate to the user device 200 which assistant-enabled device 210 the user 30 wants to control. By having a single application 106 that provides a set of respective controls 220 for executing a set of available respective actions 120 associated with each of the plurality of assistant-enabled devices 210, memory and processing bandwidth is freed at the user device 200 because it is not necessary for the user 30 to install, open, or switch between each application provided by the manufacturer of the various assistant-enabled devices 210 in order to control the device 210 to execute its respective set of actions 120. As used herein, GUI 400 may receive user input indications via any one of a touch, voice, gesture, gaze, and/or input device (e.g., a mouse or stylus) for launching application 106 and controlling the functionality of application 106 executing on user device 200.
The assistant application 106 executing on the user device 200 may render to display on the GUI 400 a ranked list 310 of candidate assistant-enabled devices 210 that the user 30 may control via the user device 200. The ranked list 310 rendered on the GUI 400 may include, for each candidate assistant-enabled device 210, a respective graphic 402 identifying the candidate assistant-enabled device 210 and a respective set of controls 220 for performing a respective set of actions 120 associated with the corresponding assistant-enabled device 210. For example, the smart speakers 210a, 210f may each include the same respective set of actions 120 associated therewith, including a play operation for audibly playing media content (e.g., music), a stop/pause operation for stopping/pausing the audible output of the media content, and a volume action for increasing/decreasing the volume of the audio content output from the corresponding speakers 210a, 210 f. In this example, the assistant application 106 can display, for each of the smart speakers 210a, 210f displayed in the list 310, the same respective set of controls 220 for performing the respective set of actions 120 associated with the corresponding speaker 210a, 210 f. That is, the corresponding set of controls 220 may include: a control 220 for performing an action 120 associated with the play operation; a control 220 for performing an action 120 associated with a stop/pause operation; a control 220 for performing an action 120 associated with increasing the volume level; and control 220 or perform action 120 associated with reducing the volume level.
In some examples, the ranked list 310 of candidate assistant-enabled devices 210 rendered for display in the GUI 400 further includes graphics indicating respective device state information 215 for at least one of the assistant-enabled devices 210 in the ranked list 310. In the example shown, device status information 215 for the smart lamp 210b is displayed to indicate that the smart lamp 210b is currently off, and thus does not provide illumination. Likewise, device status information 215 for the intelligent thermostat 210d is displayed to indicate the current temperature setting of the intelligent thermostat 210d (e.g., the temperature is currently 70 degrees Fahrenheit).
The set of available respective actions 120 associated with each assistant-enabled device 210 may vary depending on the device state information 215 of the assistant-enabled device 210. For example, when the smart light 210b indicates the device status information 215 that is currently off and thus does not provide illumination, the only action 120 available to the smart light 210b to perform will be on, causing the assistant application 106 to only provide the corresponding control 220 for causing the smart light 210b to perform an operation to switch from the off state to the on state in which the smart light 210b will provide illumination. On the other hand, when the device status information 215 of the smart lamp 210b is currently on and thus provides illumination, the smart lamp 210b may perform at least the act 120 of turning off and optionally perform an operation of increasing/decreasing the illumination setting, wherein the illumination provided by the smart lamp 210b may be increased or decreased. In this scenario, the assistant application 106 may provide a set of controls 220, each control 220 allowing the user 30 to control the smart light 210b to perform a respective one of the actions 120 of switching from an on state to an off state, increasing a lighting setting, or decreasing a lighting setting. Thus, at least one available action 120 of the set of available respective actions 120 received from the at least one assistant-enabled device 210 may include suggested actions for the corresponding assistant-enabled device 210 to perform based on the current context. Here, the current context may include device state information 215 of the corresponding assistant-enabled device 210. The current context may additionally or alternatively include past behavior of the user 30 indicating which assistant-enabled device 210 the user 30 actually controlled under the same or similar context, including, but not limited to, the same or similar proximity information 250, directionality information 255, and/or device status information 215 associated with one or more assistant-enabled devices 210 in the environment 32 of the user 30.
The assistant application 106 may further provide settings to allow the user 30 of the user device 200 to customize the activity controls 220 for performing one or more specific actions 120 associated with each assistant-enabled device 210. The user 30 may provide a user request 108 requesting that the assistant application 106 be launched for execution on the user device 200. For example, a Graphical User Interface (GUI) 400 displayed on a screen of the user device 200 may receive the user request 108 as a user input indication indicating a selection of a graphical element representing the assistant application 106, or the user 30 may provide the user request 108 via a voice input including a call command for launching the assistant application 106 for execution on the user device 200. In other examples, the user device 200 receives the user request 108 in response to detecting a predefined movement/gesture of the user device 200 configured to launch the assistant application 106 to execute on the user device 200. For example, the user device 200 may include an accelerometer that detects that the user 30 has moved the user device from a first orientation to a second orientation.
As the number of assistant-enabled devices 210 in environment 32 increases, presenting each respective set of controls 220 in GUI 400 may become unmanageable. For example, simply displaying a respective set of controls 220 for executing a respective set of actions 120 associated with each of the plurality of assistant-enabled devices 210 would require the user 30 to navigate the GUI 400 to locate the target assistant-enabled device 210 that the user 30 wants to control. For obvious reasons, this is a tedious and burdensome task for the user 30 when there are numerous assistant-enabled devices 210 available for control. Using proximity information 250 for each of a plurality of assistant-enabled devices 210 within an environment 32 of a user 30, embodiments herein relate to assistant application 106 automatically detecting one or more candidate assistant-enabled devices 210 among the plurality of assistant-enabled devices 210 within the environment 32 as target assistant-enabled devices 210 that user 30 wants to control by commanding/indicating that the target assistant-enabled devices 210 perform respective actions 120. For example, the user 30 may provide a user input indication indicating selection of one of the controls 220 displayed in the GUI 400 that is associated with performing the respective action 120 associated with the target assistant-enabled device 210, thereby causing the assistant application 106 to transmit the corresponding command 80 to the target assistant-enabled device 210 to perform the respective action 120.
In the illustrated example, during execution of the assistant application 106, the user device 200 receives proximity information 250, 250a-f for each of a plurality of assistant-enabled devices 210, 210a-f within the environment 32. For each assistant-enabled device 210, the user device 200 (i.e., using the assistant application 106) determines a proximity score 260, 260a-f based on the proximity information 250 obtained for the corresponding assistant-enabled device 210. Here, each proximity score 260 indicates a proximity estimate of the corresponding assistant-enabled device 210 relative to the user devices 200 within the environment 32. As used herein, a proximity estimate may include a distance of the corresponding device 210 from the user device 200. The proximity estimate may further include a location of the corresponding device 210 relative to the location of the user device 200. Using the plurality of proximity scores 260 determined for the plurality of assistant-enabled devices 210, the user device 200 (i.e., using the assistant application 106) generates a ranked list 310 of candidate assistant-enabled devices 210 from the plurality of assistant-enabled devices 210 within the environment 32. Here, the ranked list 310 is displayed in the GUI 400, and for each assistant-enabled device 210 in the ranked list 310, the user device 200 displays in the GUI 400 a respective set of controls 220 for performing a respective set of actions 120 associated with the corresponding assistant-enabled device 210.
In some implementations, the assistant application 106 generates the ranked list 310 of candidate assistant-enabled devices 210 by ordering from the assistant-enabled device 210 having the closest proximity to the user device 200 to the assistant-enabled device 210 having the farthest proximity to the user device 200. For example, fig. 1 shows a ranked list 310 of candidate assistant-enabled devices 210 including smart speaker 210a, smart light 210b, smart television 210c, smart thermostat 210d, second smart television 210e, and second smart speaker 210f displayed from top to bottom in GUI 400. Here, the smart speaker 210a displayed at the top of the ranked list 310 in the GUI 400 includes the closest proximity to the user device 200, while the second smart speaker 210f displayed at the bottom of the ranked list 310 in the GUI 400 includes the furthest proximity to the user device 200.
In some examples, the assistant application 106 generates the ranked list 310 of candidate assistant-enabled devices 210 by selecting the assistant-enabled device 210 having the N highest proximity scores 260 among the plurality of proximity scores 260 determined for the plurality of assistant-enabled devices 210 within the environment 32 of the user device 200. In an additional example, the ranked list 310 of candidate assistant-enabled devices 210 includes only assistant-enabled devices 210 having a proximity score 260 that meets a maximum distance threshold. Here, the maximum distance threshold may be configurable and associated with a long distance between the assistant-enabled device 210 and the user device 200 to indicate that the user 30 is unlikely to intend to control the assistant-enabled device 210. In this manner, assistant-enabled devices 210 that are separated from user device 200 by more than a maximum distance threshold may be effectively filtered out of ranked list 310 of candidate assistant-enabled devices 210. For example, when the user device 200 is on a first floor, one or more of the assistant-enabled devices 210 located on a second floor may be excluded from the ranked list 310 of candidate assistant-enabled devices 210. Additionally or alternatively, the ranked list 310 of candidate assistant-enabled devices 210 may exclude any assistant-enabled devices 210 having a proximity score 260 indicating a proximity estimate that meets a minimum distance threshold. Here, the minimum distance threshold may be configurable and associated with a short distance between the assistant-enabled device 210 and the user device 200 to indicate that the user 30 is sufficiently close to directly manually control the assistant-enabled device 210 without using the application 106. In this manner, assistant-enabled devices 210 within reach of user 30 may be effectively filtered out of ranked list 310 of candidate assistant-enabled devices 210.
Fig. 2 illustrates an example user device 200 in communication with an assistant-enabled device (e.g., a smart speaker) 210. The user device 200 includes data processing hardware 202 and memory hardware 204 in communication with the data processing hardware 202 and storing instructions that, when executed on the data processing hardware 202, cause the data processing hardware 202 to execute the assistant application 106. The user device 200 may also include at least one sensor 206, such as an antenna, configured to receive wireless communication signals transmitted by the assistant-enabled device 210. The user device 200 may also include an array of one or more microphones 208. The assistant-enabled device 210 may include one of a plurality of assistant-enabled devices 210 within the environment 32 of the user device 200 that is controllable by the assistant application 106 to perform a set of available respective actions 120 associated with the assistant-enabled device 210. In some examples, assistant-enabled device 210 publishes a set of available corresponding actions 120 that assistant-enabled device 210 can perform. In the example shown, the set of available corresponding actions 120 includes a media player action 120 that includes a play operation, a volume control operation, a stop/pause operation, and a next track/last track operation. User device 200 may receive the set of available actions 120 from assistant-enabled device 210 directly or indirectly via an access point (not shown). The user device 200 may then determine a set of respective controls 220 for display in the GUI 400 based on the set of available respective actions 120 received from the assistant-enabled device 210. The user 30 may provide a user input indication indicating selection of one of the controls 220 displayed in the GUI 400 that is associated with performing a respective action 120 associated with the target assistant-enabled device 210 (e.g., speaker) that the user 30 actually wants to control, thereby causing the assistant application 106 to transmit a corresponding command 80 to the target assistant-enabled device 210 to perform the respective action 120.
The user device 200 may also receive device status information 215 from the assistant-enabled device 210. For example, the device status information 215 may indicate that the assistant-enabled device 210 is currently on and playing a music playlist. The user device 200 also receives proximity information 250 from the assistant-enabled device 210. The user device 200 may receive the proximity information 250 and optionally the device status information 215 from each of the assistant-enabled devices 210 within the environment 32 of the user device 200 continuously or at least during periodic intervals. As used herein, the proximity information 250 includes any information or data that the user device 200 may use to determine a proximity score 260 for the corresponding assistant-enabled device 210, where the proximity score 260 indicates a proximity estimate of the corresponding assistant-enabled device 210 relative to the user device 200 in the environment 32. Thus, while many examples include the user device 200 determining a proximity score 260 for a corresponding assistant-enabled device 210 based on proximity information 250 received from the assistant-enabled device 210, the user device 200 may receive proximity information 250 associated with the corresponding assistant-enabled device 210 from another device. For example, another device including an image capture device may provide image data indicating proximity information 250 associated with the assistant-enabled device 210 relative to the user device 200.
In some examples, the user device 200 receives the wireless communication signal transmitted by the assistant-enabled device 210 at the sensor 206 of the user device 200 and determines the proximity information 250 based on the signal strength of the wireless communication signal received at the sensor 206 of the user device 200. Here, the wireless communication signal may include, but is not limited to, a bluetooth signal, an infrared signal, an NFC signal, or an ultrasonic signal. In other examples, the user device 200 receives proximity information 250 from an access point (not shown), the proximity information 250 indicating a signal strength of a wireless communication signal received at the access point from the assistant-enabled device 210. In these examples, the user device 200 may determine the proximity score 260 indicative of the proximity estimate based on a signal strength of a wireless communication signal received from the user device 200 at the access point. In additional examples, the user device 200 receives audible or inaudible signals output from the assistant-enabled device 210 at an array of one or more microphones 208 and determines the proximity information 250 for at least one assistant-enabled device 210 based on the energy and/or frequency of the audible or inaudible signals output from the assistant-enabled device 210.
In some implementations, the user device 200 additionally obtains the directionality information 255 for the assistant-enabled device 210 by: wireless communication signals transmitted by the assistant-enabled device 210 are received at each microphone (e.g., sensor) in the array of microphones 208 and directivity information 255 is determined based on the respective signal strengths of the wireless communication signals received at each microphone (e.g., sensor) in the array of sensors 208 relative to the respective signal strengths of the wireless communication signals received at the other microphones in the array of microphones 208. In these implementations, the proximity score 260 determined for the assistant-enabled device 210 is further based on the directionality information 255. For example, the directionality information 255 may indicate that the user device 200 is not facing or pointing to the assistant-enabled device 210. This may serve as a strong indicator that the user 30 does not intend to control the assistant-enabled device 210. As such, the directionality information 255 may bias the proximity score 260 by increasing the proximity score 260 when the directionality information 255 indicates that the user device 200 is pointing away from the assistant-enabled device 210, or decreasing the proximity score 260 when the directionality information 255 indicates that the user device 200 is pointing away from the assistant-enabled device 210. As used herein, the proximity score 260 increases as the proximity to the user device 200 decreases and decreases as the proximity to the user device 200 increases. Thus, the magnitude of the proximity score 260 is inversely proportional to the proximity of the corresponding assistant-enabled device 210 relative to the user device 200.
In some implementations, the assistant application 106 trains the user-specific control model on historical data including, but not limited to, past proximity-directionality information 250, 255, resulting proximity scores 260, available sets of actions 120, device state information 215, and target assistant-enabled devices 210 associated with controls 220 that the user 30 actually controls to perform the respective actions 120 associated therewith. For example, the user 30 may provide a user input indication indicating one of the controls 220 displayed in the GUI 400 associated with performing the respective action 120 associated with the target assistant-enabled device 210, thereby causing the assistant application 106 to transmit the corresponding command 80 to the target assistant-enabled device 210 to perform the respective action 120.
Fig. 3A-3C illustrate schematic diagrams 300a-C of a user device 200 executing an assistant application 106 to control a plurality of assistant-enabled devices 210 within an environment (e.g., room 32) of the user device 200 as the user 30 moves through a room holding the user device 200. The room may include a home room on a first floor of the house 32 of fig. 1, with the assistant-enabled devices 210, 210a-d disposed throughout the home room. Specifically, the schematic diagrams of 300a-c illustrate a plurality of assistant-enabled devices 210 including a smart speaker 210a, a smart light 210b, a smart television 210c, and a smart thermostat 210 d. Fig. 4A-4C illustrate example GUIs 400a-C rendered on a screen of a user device 200 to display a respective set of controls 220 for performing a respective set of actions 120 associated with each corresponding assistant-enabled device 210. In particular, each GUI 400a-C depicts a ranked list 310 of candidate assistant-enabled devices 210 and a corresponding set of controls 220 for each device 210 based on the proximity information 250 obtained for each of the plurality of assistant-enabled devices 210 of fig. 3A-3C. Each candidate assistant-enabled device 210 in the ranked list 310 may be rendered in the GUI 400 as a respective graphic 402 representing the candidate assistant-enabled device 210. That is, GUI 400a of fig. 4A illustrates a ranked list 310 of candidate assistant-enabled devices 210 and corresponding controls 220 generated based on a proximity estimate of assistant-enabled devices 210 of fig. 3A relative to user devices 200 in environment 32. GUI 400B of fig. 4B illustrates a ranked list 310 of candidate assistant-enabled devices 210 and corresponding controls 220 generated based on proximity estimation of assistant-enabled devices 210 of fig. 3B relative to user devices 200 in environment 32. GUI 400C of fig. 4C illustrates ranked list 310 of candidate assistant-enabled devices 210 and corresponding controls 220 generated based on proximity estimation of assistant-enabled devices 210 of fig. 3C relative to user devices 200 in environment 32. As will become apparent, the ranked list 310 of candidate assistant-enabled devices 210 rendered in each of the respective GUIs 400a-C varies based on the proximity score 260 determined for each of the assistant-enabled devices 210 as the user device 200 moves to each of the different locations in the room 32 as shown in the schematics 300a-C of fig. 3A-3C.
Referring to fig. 3A and 4A, the user 30 is located at the far end of the room near a smart speaker 210a that is mounted on a desk, while other assistant-enabled devices 210, including a smart light 210b, a smart television 210c, and a smart thermostat 210d, are located at the opposite end of the room. Further, user device 200 and user 30 are directed to smart speaker 210a, indicating that user 30 may intend to control smart speaker 210a via assistant application 106. Additionally, the user device 200 and the user 30 are pointing away from the smart light 210b, the smart television 210c, and the smart thermostat 210d, indicating that the user 30 may not intend to control the smart light 210b, the smart television 210c, or the smart thermostat 210d via the assistant application 106.
As discussed above with reference to fig. 1 and 2, the user device 200 may continuously (or at periodic intervals) obtain proximity information 250 for each of the assistant-enabled devices 210 within the environment 32 to determine a corresponding proximity score 260, wherein each proximity score 260 indicates a proximity estimate of the corresponding assistant-enabled device 210 relative to the user device 200 within the environment 32. As shown in fig. 3A, the smart speaker 210a is in closest proximity to the user 30 holding the user device 200, resulting in a higher proximity score 260. The assistant-enabled device 210 that is the candidate with the highest proximity score 260 is a strong indicator that the user 30 wishes to control the intelligent speaker 210a. Instead, the smart light 210b, smart television 210c, and smart thermostat 210d are further away from the user 30 holding the user device 200, resulting in a lower proximity score 260. Thus, as a magnitude, the proximity score 260 may be used as a strong indicator as to whether the user 30 intends or does not intend to control the corresponding candidate assistant-enabled device 210. The user device 200 (i.e., using the assistant application 106) may identify the smart speaker 210a as having the highest proximity score 260 among the plurality of assistant-enabled devices 210a-d as the target assistant-enabled device 210 that the user 30 intends to control.
Additionally, assistant-enabled devices 210 located in other rooms on the second floor of the user's premises 32, such as the second smart television 210e and the second smart speaker 210f shown in fig. 1, may have even lower proximity scores 260 that cause the user device 200 (i.e., using the assistant application 106) to disqualify those assistant-enabled devices 210 from consideration as possible candidates for the user 30 to intend to control. As a result, the user device 200 may discard the second smart television 210e and the second smart speaker 210f from the ranked list 310 of candidate assistant-enabled devices 210 rendered in the GUIs 400a-C of fig. 4A-4C.
Referring to fig. 3A and 4A, after determining the proximity score 260 for each of the assistant-enabled devices 210, the user device 200 (i.e., using the assistant application 106) generates a ranked list 310 of candidate assistant-enabled devices 210 and corresponding controls 220 based on the proximity score 260 indicating a proximity estimate of the assistant-enabled devices 210 relative to the user device 200. More specifically, user device 200 generates ranked list 310 by ordering from assistant-enabled device 210a having a closest proximity to user device 200 to assistant-enabled device 210d having a farthest proximity to user device 200. In additional examples, the proximity score 260 is combined with additional context information, such as a confidence level for the available respective set of actions associated with the corresponding assistant-enabled device 210. For example, turning off the light later in the evening may have a higher confidence. In essence, such contextual information may be used to bias the proximity score 260 to influence how the assistant-enabled devices are placed in the ranked list 310 personalized for the user.
The GUI 400a of fig. 4A displays a ranked list 310 of candidate assistant-enabled devices 210 generated by the user device 200 using the determined proximity scores 260 for each of the assistant-enabled devices 210. As described above, the user device 200 may determine that the proximity score 260 associated with the second smart television 210e and the second smart speaker 210f located on the second floor of the user's premises 32 of fig. 1 may indicate a proximity estimate that meets the maximum distance threshold, thereby causing the user device 200 to discard these assistant-enabled devices 210e, 210f from the ranked list 310 of candidate assistant-enabled devices 210. With the highest proximity score 260, the user device 200 ranks the smart speaker 210a higher in the ranked list 310, resulting in the user device 200 rendering the corresponding graphical element 402 representing the smart speaker 210a, the corresponding control 220, and the device state information 215 in the GUI 400a more prominently than the graphical element 402 representing each of the lower ranked assistant-enabled devices 210b-d in the ranked list 310. In the example shown, the user device 200 renders/displays the graphical element 402 of the ranked list 310 of candidate assistant-enabled devices 210 in the GUI 400a and renders/displays the corresponding control 220 for the smart speaker 210a in a larger font at the top. The GUI 400a is non-limiting and in other examples, the user device 200 may render a graphical element 402 representing the smart speaker 210a associated with the highest proximity score 260 and/or display the graphical element 402 differently than the graphical element 402 representing the other assistant-enabled devices 210b-d in other locations of the GUI 400a, such as the middle. This display format makes it easier and faster for the user 30 to find a set of respective controls 220 for performing a set of respective actions 120 associated with the smart speaker 210 a.
In the example GUI 400a shown in fig. 4A, the set of respective controls 220 for the smart speaker 210a displayed in the GUI 400a includes controls 220 for causing the smart speaker 210a to perform any of the actions 120 from the set of respective actions 120 associated with the music playlist (e.g., from the streaming music service) currently available for the smart speaker 210a to perform. The set of corresponding controls 220 includes controls 220 that, when selected, cause the intelligent speaker 210a to perform a corresponding action 120 of reverting to a last song track in the playlist, playing the music playlist, jumping to a next song track in the playlist, and adjusting the volume level of the intelligent speaker 210 a. Continuing with this example, graphical element 402 representing smart speaker 210a located in the home room of user's house 32 also displays device status information 215 associated with smart speaker 210a indicating the battery level of smart speaker 210a and that the music playlist is currently paused, and thus, no music is currently being output from smart speaker 210 a. Notably, the control 220 for performing the action 120 of pausing the playlist is not displayed in the GUI 400a because the playlist is currently paused. Here, the user 30 may provide a user input indication indicating selection of the "Play (Play)" control 220 (e.g., by touching a graphical button in the GUI 400a that commonly represents "Play") to cause the smart speaker 210a to perform the corresponding action 120 of audibly playing the current song track in the playlist. Alternatively, the user input indication indicating selection of the "play" control 220 may include a voice input, such as the user 30 speaking the term "play" that may be captured by the microphone of the user device 200. Notably, the voice recognition capabilities for controlling the devices 210 in the ranked list 310 may be biased based on the proximity score 260. This follows the idea that a user may be more likely to issue voice-based commands. As such, a proximity score 260 that meets a threshold for a particular device 210 may cause a microphone to be turned on at the device 210 to capture and perform speech recognition of the captured speech. Additionally or alternatively, the voice recognition capabilities of a particular device 210 may be biased to recognize a set of available corresponding actions 120 for that device 210. As shown in fig. 3A, user input indicative of selection of the "play" control 220 displayed in GUI 400a indicates that user device 200 (i.e., assistant application 106) is to transmit a corresponding command 80 to smart speaker 210a to perform a corresponding action 120 of audibly playing a current song track in a playlist.
Referring back to GUI 400a of FIG. 4A, user device 200 further displays graphical element 402 in GUI 400a representing other assistant-enabled devices 210b-d in ranked list 310 of candidate assistant-enabled devices 210. Specifically, since the intelligent thermostat 210d has the second highest proximity score 260, the intelligent thermostat 210d is second in the ranked list 310, since the intelligent television 210c has the third highest proximity score 260, the intelligent television 210c is third in the ranked list 310, and since the intelligent light 210b has the lowest proximity score 260, the intelligent light 210b is last in the ranked list 310. In the example shown, the graphical element 402 representing the intelligent thermostat 210d provides a set of device state information 215 indicating a current temperature setting (e.g., 70 degrees) of the intelligent thermostat 210d and corresponding controls 220 for controlling the intelligent thermostat 210d to perform corresponding actions 120 that increase or decrease the current temperature setting. Similarly, the graphical element 402 representing the smart light 210b provides device status information 215 indicating that the smart light 210b is currently off and a single control 220 for controlling the smart light 210b to perform the corresponding action 120 of being on.
In some examples, for candidate assistant-enabled devices 210 in the ranked list 310 associated with lower ranked proximity scores 260, the gui 400a may display only the device state information 215 associated with those assistant-enabled devices 210 without rendering the respective controls 220. However, if the user 30 does not want to control any of these devices 210 to perform the respective action 120, the user 30 may provide an input indication that selects the corresponding graphical element 402 to cause the GUI 400a to render a set of respective controls 220 thereon. In the illustrated example, because the smart television 210c has a low proximity score 260 (e.g., fails to meet the minimum proximity score threshold), the user device 200 (i.e., using the application 106) suppresses display of the respective control 220 for the smart television 210c in the ranked list 310. This decision to suppress the display of control 220 may be based on corresponding directionality information 255 indicating that user device 200 is pointing away from smart television 210 c. Here, the directionality information 255 may bias the proximity score 260 by reducing the proximity score 260 for the smart tv 210 c. Further, the user device 200 may consider a context, such as the type of assistant-enabled device 210, when considering whether to bias the proximity score 260 based on the directionality information 255. In the current example, directional information 255 indicating that the user device 200 is pointing away from the smart television 210c may be used as a strong indicator that the user 30 does not intend to control the smart television 210c, indicating that the assistant-enabled device 210c is the smart television and that the user typically directs themselves to face the smart television's context when issuing a command. As a result, even if there are actions 120 available for the smart tv 210c to perform, the user device 200 may optionally suppress the display of the control 220 in the GUI 400a for the smart tv 210c to perform any actions 120. On the other hand, the directional information 255 associated with the smart light 210b may not be used to bias the proximity score 260 because the users 30 generally do not orient themselves in any particular way when controlling the operation of the smart light. Conversely, the directionality information 255 indicating that the user device 200 is facing the smart speaker 210a may further bias the proximity score 260 for the smart speaker 210a by increasing the proximity score 260. However, the amount of bias may be weighted less because users often control the smart speaker to perform operations even when facing other directions.
Referring to fig. 3B and 4B, in response to user 30 providing a user input indication indicating selection of the "play" control 220 displayed in GUI 400a of fig. 4A to cause user device 200 to transmit command 80 to smart speaker 210a to perform a corresponding "play" action 120 as shown in fig. 3A, smart speaker 210a is audibly playing the current song track in the music playlist. Additionally, the user device 200 and the user 30 are directed to other assistant-enabled devices 210 including a smart light 210b, a smart television 210c, and a smart thermostat 210d, indicating that the user 30 may intend to control one of the smart light 210b, the smart television 210c, or the smart thermostat 210d via the assistant application 106.
As shown in fig. 3B, the smart speaker 210a remains in closest proximity to the user 30 holding the user device 200, resulting in a higher proximity score 260. The assistant-enabled device 210 that is the candidate with the highest proximity score 260 is a strong indicator that the user 30 wishes to control the intelligent speaker 210 a. In contrast, smart light 210b, smart television 210c, and smart thermostat 210d remain farther from the user 30 holding the user device 200 than smart speaker 210a, resulting in these assistant-enabled devices 210b-d having a lower proximity score 260 than smart speaker 210 a.
Referring to fig. 3B and 4B, after determining the proximity score 260 for each of the assistant-enabled devices 210, the user device 200 (i.e., using the assistant application 106) generates a ranked list 310 of candidate assistant-enabled devices 210 and corresponding controls 220 based on the proximity score 260 indicating a proximity estimate of the assistant-enabled devices 210 relative to the user device 200. More specifically, user device 200 generates ranked list 310 by ordering from assistant-enabled device 210a having a closest proximity to user device 200 to assistant-enabled device 210d having a farthest proximity to user device 200. In some examples, the one or more proximity scores 260 are further determined or biased based on directionality information 255 associated with the one or more assistant-enabled devices 210.
GUI 400B of fig. 4B displays ranked list 310 of candidate assistant-enabled devices 210 generated by user device 200 using proximity scores 260 determined for each of assistant-enabled devices 210. As described above, the user device 200 may determine that the proximity score 260 associated with the second smart television 210e and the second smart speaker 210f located on the second floor of the user's premises 32 of fig. 1 may indicate a proximity estimate that still meets the maximum distance threshold, thereby causing the user device 200 to discard these assistant-enabled devices 210e, 210f from the ranked list 310 of candidate assistant-enabled devices 210. As a result of maintaining its highest proximity score 260, the user device 200 ranks the smart speaker 210a higher in the ranked list 310, resulting in the user device 200 rendering the corresponding graphical element 402 representing the smart speaker 210a, the corresponding control 220, and the device state information 215 in the GUI 400b more prominently than the graphical element 402 representing each of the lower ranked assistant-enabled devices 210b-d in the ranked list 310. Additionally, the corresponding control 220 of the assistant-enabled device 210 for the higher rank in the ranked list may be displayed more closely (e.g., larger, more controls, etc.). In the example shown, the user device 200 renders/displays the graphical element 402 of the ranked list 310 of candidate assistant-enabled devices 210 in GUI 400b and renders/displays the corresponding control 220 associated with smart speaker 210a on top in a larger font. GUI 400b is non-limiting and in other examples, user device 200 may render graphical element 402 representing smart speaker 210a associated with highest proximity score 260 in other locations of GUI 400b, such as the middle of GUI 400b, and/or display graphical element 402 differently than graphical element 402 representing other assistant-enabled devices 210 b-d. This display format makes it easier and faster for the user 30 to find a set of respective controls 220 for performing a set of respective actions 120 associated with the smart speaker 210 a.
With continued reference to the example GUI 400B of fig. 4B, the set of corresponding controls 220 for the smart speaker 210a displayed in the GUI 400B includes controls 220 that, when selected, cause the smart speaker 210a to perform the corresponding actions 120 of reverting to the last song track in the playlist, pausing the music playlist, jumping to the next song track in the playlist, and adjusting the volume level of the smart speaker 210 a. Thus, because the device status information 215 for the smart speaker 210a indicates that the smart speaker 210a is now currently playing music from the music playlist (i.e., in response to the transmission of the command 80 from the user device 200 to the smart speaker 210a to perform the corresponding "play" action 120 shown in fig. 3A), the action 120 of pausing the music playlist that was not available as the control 220 in the GUI 400a of fig. 4A is now available for selection in the GUI 400a of fig. 4B. Continuing with this example, graphical element 402 representing smart speaker 210a located in the home room of user's house 32 also displays device status information 215 associated with smart speaker 210a indicating the battery level of smart speaker 210a and the music playlist is currently playing, and thus music is currently being output from smart speaker 210 a. Notably, because the user 30 has provided an input indication indicating selection of the "play" control 220 in FIG. 3A, the control 220 for performing the action 120 of playing the playlist is not displayed in the GUI 400b because the playlist is currently playing.
Still referring to GUI 400B of FIG. 4B, user device 200 further displays a graphical element 402 in GUI 400B representing the other assistant-enabled devices 210B-d in ranked list 310 of candidate assistant-enabled devices 210. Specifically, since the intelligent thermostat 210d has the second highest proximity score 260, the intelligent thermostat 210d is second in the ranked list 310, since the intelligent television 210c has the third highest proximity score 260, the intelligent television 210c is third in the ranked list 310, and since the intelligent light 210b has the lowest proximity score 260, the intelligent light 210b is last in the ranked list 310. In the example shown, the graphical element 402 representing the intelligent thermostat 210d provides a set of device state information 215 indicating a current temperature setting (e.g., 70 degrees) of the intelligent thermostat 210d and corresponding controls 220 for controlling the intelligent thermostat 210d to perform corresponding actions 120 that increase or decrease the current temperature setting. Similarly, the graphical element 402 representing the smart light 210b provides device status information 215 indicating that the smart light 210b is currently off and a single control 220 for controlling the smart light 210b to perform the corresponding action 120 of being on.
In the example shown, the user device 200 (i.e., using the application 106) no longer suppresses the display of the corresponding control 220 for the smart television 210c in the ranked list 310. This decision to now render the display for the respective control 220 of the smart television 210c may be based on the corresponding directionality information 255 now indicating that the user device 200 has turned away from the smart speaker 210a to now face the smart television 210c, as shown in the schematic diagram 300B of fig. 3B. Here, the directionality information 255 indicating that the user device 200 is pointing to the smart tv 210c may cause the assistant application 106 to bias the proximity score 260 by increasing the proximity score 260 for the smart tv 210c and/or simply choose not to apply any bias that decreases the proximity score 260 as in fig. 3A when the directionality information 255 indicates that the user device 200 is pointing away from the smart tv 210 c. In the present example, the directional information 255 indicating that the user device 200 is facing the smart tv 210c may be used as a strong indicator that the user 30 intends to control the smart tv 210 c. Accordingly, the user device 200 may optionally render a display of the control 220 for "off" and "mute" in the GUI 400b to cause the smart television 210c to perform the actions 120 of powering off the smart television 210c and muting the audio output of the smart television 210 c. Further, the directional information 255 indicating that the user device 200 is pointing to the smart tv 210 may cause the smart tv 210c to provide an indication that the user device 200 is pointing to the smart tv and thereby act as a confirmation that the assistant application intends to control the tv. The indication may be a screen change color, a panel change color of the television 210c, an audible alarm, or any other indication confirming that the user device 200 is aligned with the television 200 c.
While many of the examples described herein discuss the assistant application 106 using the received directionality information 255 to bias the proximity score 260, other examples may include the assistant application 106 determining the proximity score 260 as a function of both the proximity information 250 and the directionality information 255. In these examples, the type of particular assistant-enabled device 210 that provides the information 250, 255 may be further considered when determining the corresponding proximity score 260. For example, the assistant application 106 may adjust the weight of the proximity information 250 and/or the weight of the directionality information 255 based on the type of the particular assistant-enabled device 210 to affect the degree to which the proximity information 250 and the directionality information 255 affect the magnitude of the resulting proximity score 260 for the particular assistant-enabled device 210. In some implementations, the assistant application 106 uses the proximity information 250 and the directionality information 255 that are continuously received over the duration to determine whether the user device 200 is moving. In these implementations, when the user device 200 is moving, the assistant application 106 can further derive a current heading for the user device 200 that indicates in what direction the user device 200 is moving, and thus to which assistant-enabled device 210 the user device 200 is moving. The current heading may bias the proximity score 260 determined for the one or more assistant-enabled devices 210, or the proximity score 260 determined for the one or more assistant-enabled devices 210 may be determined as a function of the current heading of the user device 200 derived from the proximity information 250 and the directionality information 255.
Additionally, when considering whether to include respective controls 220 in GUI 400 for performing available actions 120 associated with one or more assistant-enabled devices 210 in environment 32, user device 200 may consider a context in which user 30 provides an input indication indicating a selection of controls 220 for causing assistant-enabled devices 210 to perform respective actions 120 based on device state information 215 of one or more assistant-enabled devices 210. For example, if the user 30 immediately mutes or turns off the smart television 210c each time the user 30 issues a command 80 for the smart speaker 210a to play music while the device state information 215 of the smart television 210c indicates that the smart television 210c is currently on, the user device 200 may make a decision to render a display of the controls 220 for "off" and "mute" for the smart television 210c currently in the GUI 400b in response to issuing the command 80 to play music to the smart speaker 210a (fig. 3A) and the current device state information 215 for the smart television 210c indicating that the smart television 210c is currently on.
Referring to fig. 3B and 4B, the user 30 wants to switch the smart light 210B from off to On and uses the assistant application 106 to provide a corresponding user input indication (e.g., touch input, voice input, eye gaze, or gesture) indicating selection of the "On (On)" control 220 for the smart light 210B displayed in the GUI 400B. Here, the user device 200 receives a user input indication indicating selection of the "on" control 220 for the smart light 210b displayed in the GUI 400b, such that the user device 200 (i.e., the assistant application 106) transmits a corresponding command 80 to the smart light 210b to perform the respective action 120 of being on.
Referring to fig. 3C and 4C, the smart light 210B is now turned on and the user 30 has moved from the distal end of the room near the smart speaker 210a toward the opposite side of the room closest to the smart thermostat 210d, with the smart light 210B and the smart television 210C now in closer proximity to the user device 200 than the example depicted in fig. 3B. GUI 400c now shows that device status information 215 for smart light 210b indicates that smart light 210b is currently on, rendering only one control 220 for smart light 210b in GUI 400c to control smart light 210b to perform the only available action 120 that is off. Further, fig. 3C shows the user device 200 pointing to the intelligent thermostat 210d, which indicates that the user 30 may want to control the intelligent thermostat 210d via the assistant application 106. In contrast, the user device 200 is not pointed at either the smart light 210b or the smart television 210c, although in close proximity to the user device 200. The directionality information 255 indicating that the user device 200 is not pointing to the assistant-enabled device 210 may indicate that the user 30 may not want to control the assistant-enabled device 210, e.g., the smart light 210b, the smart television 210c, and the smart speaker 210a in the illustrated example.
As discussed above with reference to fig. 1 and 2, the user device 200 may continuously (or at periodic intervals) obtain the proximity information 250 and optionally the directionality information 255 for each of the assistant-enabled devices 210 to determine a proximity score 260 for each of the assistant-enabled devices 210 for generating a ranked list 310 of candidate assistant-enabled devices 210. As shown in fig. 3C, the intelligent thermostat 210d is in closest proximity to the user 30 holding the user device 200, while the intelligent speaker 210a has the furthest proximity to the user device 200, resulting in the user device 200 initially determining a higher proximity score 260 for the intelligent thermostat 210d than the intelligent speaker 210a. Since intelligent thermostat 210d is the candidate assistant-enabled device 210 in rank list 310 with the highest proximity score 260, there is initially a strong indication that user 30 intends to control intelligent thermostat 210 d. The smart light 210b and the smart television 210c, while closer to the user device 200 than the smart speaker 210a, include a further proximity to the user device 200 than the smart thermostat 210d, thereby causing the user device 200 to initially determine that the proximity score 260 for the smart television 210c and the smart light 210b is lower than the smart thermostat 210d but higher than the smart speaker 210a. Thus, the magnitude of the proximity score 260 may be used as a strong indicator as to whether the user 30 intends or does not intend to control the corresponding candidate assistant-enabled device 210. If only the proximity score 260 determined for the assistant-enabled devices 210a-d in the environment 32 is used, the user device 200 (i.e., using the assistant application 106) may identify the intelligent thermostat 210d having the highest proximity score 260 among the plurality of assistant-enabled devices 210a-d as the target assistant-enabled device 210 that the user 30 intends to control.
Additionally, the user device 200 (i.e., via the assistant application 106) may bias one or more proximity scores 260 based on potential suggestion confidence associated with the one or more assistant-enabled devices 210. For example, if the user 30 has recently interacted with the assistant-enabled device 210, there may be a high confidence of advice that the user 30 seeks to continue to control the assistant-enabled device 210. In this example, the user device 200 may apply the suggested confidence level by increasing the proximity score 260 associated with the corresponding assistant-enabled device 210 to bias the proximity score 260. Conversely, if the user 30 does not interact with the assistant-enabled device 210 within a threshold period of time, there may be a low suggested confidence that the user 30 seeks to control the assistant-enabled device 210, thereby causing the user device 200 to bias the proximity score 260 by reducing/lowering/decreasing the proximity score 260 associated with the corresponding assistant-enabled device 210. In the example shown, the user device 200 obtains a high confidence of advice associated with the smart speaker 210a indicating that the user 30 may seek to continue to control the smart speaker 210a, because the user device 200 has recently interacted with the smart speaker 210a in fig. 3A and 4A by selecting the "play" control 220 in the GUI 400a to issue a command 80 from the user device 200 for the smart speaker 210a to perform the corresponding action 120 to play the music playlist. Thus, although the smart speaker 210a has the furthest proximity to the user device 200 and the directional information 255 indicates that the user device 200 is pointing away from the smart speaker 210a, the user device 200 may apply a high recommendation confidence by increasing the proximity score 260 associated with the smart speaker 210 a. Here, the user device 200 (i.e., using the assistant application 106) maintains the set of intelligent speakers 210a and corresponding controls 220 highest in the ranked list 310 of candidate assistant-enabled devices rendered in the GUI 400C of fig. 4C. In other examples, the high suggested confidence that indicates that the user 30 intends to control the particular assistant-enabled device 210 independent of the proximity information 250 and the directionality information 255 of the particular assistant-enabled device 210 results in the assistant application 106 simply overwriting the proximity score 260 determined for the particular assistant-enabled device 210 such that the particular assistant-enabled device 210 ranks highest in the ranked list 310. Similarly, a low suggested confidence that the user 30 intends to control a particular assistant-enabled device 210 may simply cause the assistant application 106 to override the associated proximity score 260 of the particular assistant-enabled device 210 such that they are at least lower ranked in the ranked list 310 or rendered for less significant display in the GUI 400. In fact, low suggested confidence may result in the assistant-enabled device 210 simply being completely discarded from the ranked list 310 of candidate assistant-enabled devices 210.
Further, after determining that the proximity score 260 indicates a proximity estimate that meets (e.g., is less than) a minimum distance threshold, the user device 200 may discard any assistant-enabled devices 210. Here, the minimum distance threshold is configured to filter out any assistant-enabled devices 210 having a proximity score 260 that indicates that the assistant-enabled devices 210 are sufficiently close to the user device 200 that the user 30 can directly manually control the assistant-enabled devices 210 without using the application 106. In the example shown in fig. 3C, the proximity score 260 associated with the intelligent thermostat 210d indicates a proximity estimate that meets (e.g., is less than) the minimum distance threshold, thereby causing the user device 200 to discard the intelligent thermostat 210d from the ranked list 310 of candidate assistant-enabled devices 210 rendered in the GUI 400C of fig. 4C.
The GUI 400C of fig. 4C displays a ranked list 310 of candidate assistant-enabled devices 210 generated by the user device 200 using the determined proximity scores 260 for each of the assistant-enabled devices 210. As described above, the user device 200 may determine that the proximity score 260 associated with the intelligent thermostat 210d indicates a proximity estimate that fails to exceed the minimum distance threshold, thereby causing the user device 200 to discard the assistant-enabled device 210d from the ranked list 310 of candidate assistant-enabled devices 210. On the other hand, the most recent interaction with smart speaker 210a in fig. 3A may increase the proximity score 260 associated with smart speaker 210a to cause smart speaker 210a to have the highest ranking in ranking list 310. Thus, the user device 200 may continue to render the graphical element 402 representing the set of intelligent speakers 210a and corresponding controls 220 in the GUI 400c more prominently (e.g., at the top of the ranked list 310 and in a larger font) than the graphical elements 402 representing the other assistant-enabled devices 210 in the ranked list 310. The GUI 400c is non-limiting and in other examples, the user device 200 may render a graphical element 402 representing the smart speaker 210a associated with the highest proximity score 260 and/or display the graphical element 402 differently than the graphical element 402 representing the other assistant-enabled devices 210b, 210c in other locations of the GUI 400c, such as the middle. This display format makes it easier and faster for the user 30 to find a set of respective controls 220 for performing a set of respective actions 120 associated with the smart speaker 210 a.
The user device 200 also displays corresponding graphical elements 402 in the GUI 400c representing the other assistant-enabled devices 210b, 210c in the ranked list 310 of candidate assistant-enabled devices 210. Specifically, since the smart television 210c has the second highest proximity score 260 (e.g., after increasing the proximity score 260 associated with the smart speaker 210a and discarding the smart thermostat 210d from the ranked list 310), the smart television 210c is second in the ranked list 310. Since intelligent light 210b has the third highest proximity score 260, intelligent light 210b is third in ranked list 310. In the example GUI 400C of fig. 4C, the graphical element 402 representing the smart television 210C provides the device status information 215 as in the GUI 400B of fig. 4B by indicating that the smart television 210C is currently on, as well as the same set of respective controls 220 for controlling the smart television 210C to perform respective actions 120 that turn the smart television 210C off or mute.
Fig. 5 is a flow chart of an example arrangement of operations of a method 500 for controlling an assistant-enabled device. At operation 510, the method 500 includes obtaining, by the data processing hardware 202 of the user device 200, proximity information 250 for each of a plurality of assistant-enabled devices 210 within the environment 32 of the user device 200. Examples of the proximity information 250 for the at least one assistant-enabled device 210 include any information or data that the user device 200 may use to determine a proximity score 260 for the corresponding assistant-enabled device 210, where the proximity score 260 indicates a proximity estimate of the corresponding assistant-enabled device 210 relative to the user device 200 in the environment 32. Each assistant-enabled device 210 of the plurality of assistant-enabled devices 210 is controllable by the assistant application 106 to perform a set of available respective actions 120 associated with the assistant-enabled device 210. For example, the set of available corresponding actions 120 of intelligent speaker 210a may include one or more of the following: resume to the last song track in the playlist, play the music playlist, pause the playlist, skip to the next song track in the playlist, and adjust the volume level of the smart speaker 210 a.
At operation 520, the method 500 includes: for each assistant-enabled device 210 of the plurality of assistant-enabled devices 210, a proximity score 260 is determined by the data processing hardware 202 based on the proximity information 250 obtained for the corresponding assistant-enabled device 210. The proximity score 260 indicates a proximity estimate of the corresponding assistant-enabled device 210 relative to the user devices 200 in the environment 32. The proximity estimate may indicate a distance of the corresponding assistant-enabled device 210 from the user device 200 and/or a location including a location of the corresponding assistant-enabled device 210 relative to the user device 200.
At operation 530, the method 500 further includes generating, by the data processing hardware 202, a ranked list 310 of candidate assistant-enabled devices 210 from the plurality of assistant-enabled devices 210 using the plurality of proximity scores 260 determined for the plurality of assistant-enabled devices 210. In some examples, the method 500 generates the ranked list 310 of candidate assistant-enabled devices 210 by ordering from the assistant-enabled device 210 having the closest proximity to the user device 200 to the assistant-enabled device 210 having the farthest proximity to the user device 200. The ranked list 310 of candidate assistant-enabled devices 210 may include assistant-enabled devices 210 having the N highest proximity scores 260 from among a plurality of proximity scores 260 determined for the plurality of assistant-enabled devices 210. Additionally, generating the ranked list 310 of candidate assistant-enabled devices 210 may further include discarding from the ranked list 310 of candidate assistant-enabled devices 210 any assistant-enabled devices 210 that include a proximity score 260 that indicates a proximity estimate that satisfies a maximum distance threshold (i.e., is too far from the user device 200) and/or that satisfies a minimum distance threshold (i.e., is too close to the user device 200).
At operation 540, the method 500 further comprises: for each of one or more corresponding assistant-enabled devices 210 in the ranked list 310 of candidate assistant-enabled devices 210, a set of respective controls 220 for executing a set of respective actions 120 associated with the corresponding assistant-enabled device 210 is displayed by the data processing hardware 202 in a Graphical User Interface (GUI) 400 displayed on a screen of the user device 200 in communication with the data processing hardware 202. For each candidate assistant-enabled device 210, the user device 200 may render a corresponding graphical element 402 in the GUI 400 representing the candidate assistant-enabled device 210, its corresponding control 220, and/or device state information 215. Further, the method 500 of displaying in the GUI 400 a respective set of controls 220 for executing a respective set of actions 120 associated with a corresponding assistant-enabled device 210 may include: in response to receiving device state information 215 associated with at least one of the assistant-enabled devices 210, the candidate assistant-enabled devices 210 in the ranked list 310 of candidate assistant-enabled devices 210 are re-ranked. Additionally or alternatively, the method 500 for displaying a set of respective controls 220 for performing a set of respective actions 120 may include: for one assistant-enabled device 210, a respective set of controls 220 is displayed differently than the respective set of controls 220 displayed in GUI 400 for other assistant-enabled devices 210. For example, the assistant device 210 in the ranked list 310 with the highest proximity score 260 and highest ranking may be displayed in the GUI 400 more prominently than other assistant-enabled devices 210 with lower proximity scores 260.
A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform tasks. In some examples, a software application may be referred to as an "application," app, "or" program. Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
The non-transitory memory may be a physical device for temporarily or permanently storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by the computing device. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., commonly used for firmware such as a boot strap). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and magnetic disk or tape.
FIG. 6 is a schematic diagram of an example computing device 600 that may be used to implement the systems and methods described in this document. Computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
Computing device 600 includes a processor 610, memory 620, storage 630, high-speed interface/controller 640 connected to memory 620 and high-speed expansion ports 650, and low-speed interface/controller 660 connected to low-speed bus 670 and storage 630. Each of the components 610, 620, 630, 640, 650, and 660 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 610 may process instructions for execution within the computing device 600, including instructions stored in the memory 620 or on the storage device 630, to display graphical information for a Graphical User Interface (GUI) on an external input/output device, such as a display 680 coupled to the high-speed interface 640. In other embodiments, multiple processors and/or multiple buses, as well as multiple memories and memory types may be used, as appropriate. In addition, multiple computing devices 600 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a set of blade servers, or a multiprocessor system).
Memory 620 non-transitory stores information within computing device 600. Memory 620 may be a computer-readable medium, a volatile memory unit, or a non-volatile memory unit. Non-transitory memory 620 may be a physical device for temporarily or permanently storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by computing device 600. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., commonly used for firmware such as a boot strap). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and magnetic disk or tape.
The storage device 630 is capable of providing mass storage for the computing device 600. In some implementations, the storage device 630 is a computer-readable medium. In various different implementations, the storage device 630 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional embodiments, the computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 620, the storage device 630, or memory on processor 610.
The high-speed controller 640 manages bandwidth-intensive operations for the computing device 600, while the low-speed controller 660 manages lower bandwidth-intensive operations. This allocation of responsibilities is merely exemplary. In some implementations, the high-speed controller 640 is coupled to the memory 620, the display 680 (e.g., via a graphics processor or accelerator), and to the high-speed expansion port 650, which high-speed expansion port 650 may accept various expansion cards (not shown). In some implementations, a low-speed controller 660 is coupled to the storage device 630 and the low-speed expansion port 690. The low-speed expansion port 690, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router (e.g., through a network adapter).
Computing device 600 may be implemented in a number of different forms, as shown. For example, it may be implemented as a standard server 600a or multiple times in a group of such servers 600a, as a laptop computer 600b, or as part of a rack server system 600 c.
Various implementations of the systems and techniques described here can be implemented in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments may include embodiments in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, non-transitory computer-readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such a device. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, one or more aspects of the disclosure may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor or touch screen, for displaying information to the user and the computer optionally including a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Additionally, the computer may interact with the user by sending and receiving documents to and from devices used by the user; for example, by sending Web pages to a Web browser on a user's client device in response to requests received from the Web browser.
A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other embodiments are within the scope of the following claims.
Claims (30)
1. A method (500) comprising:
obtaining, by data processing hardware (202) of a user device (200), proximity information (250) for each of a plurality of assistant-enabled devices (210) within an environment (32) of the user device (200), each assistant-enabled device (210) of the plurality of assistant-enabled devices (210) controllable by an assistant application (106) to perform a respective set of available actions (120) associated with the assistant-enabled device (210);
for each assistant-enabled device (210) of the plurality of assistant-enabled devices (210), determining, by the data processing hardware (202), a proximity score (260) based on the proximity information (250) obtained for the corresponding assistant-enabled device (210), the proximity score (260) indicating a proximity estimate of the corresponding assistant-enabled device (210) relative to the user device (200) in the environment (32);
generating, by the data processing hardware (202), a ranked list (310) of candidate assistant-enabled devices (210) from among the plurality of assistant-enabled devices (210) using the plurality of proximity scores (260) determined for the plurality of assistant-enabled devices (210); and
For each of one or more corresponding assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210), a set of respective controls (220) for executing a set of respective actions (120) associated with the corresponding assistant-enabled device (210) is displayed by the data processing hardware (202) in an on-screen displayed graphical user interface GUI (400) in communication with the data processing hardware (202).
2. The method (500) of claim 1, further comprising:
receiving a user request (108) from a user of the user device (200) at the data processing hardware (202), the user request (108) for requesting to launch the assistant application (106) for execution on the data processing hardware (202),
wherein obtaining the proximity information (250) for each of the plurality of assistant-enabled devices (210) occurs during execution of the assistant application (106) on the data processing hardware (202).
3. The method (500) of claim 2, wherein receiving the user request (108) includes one of:
receiving, in the GUI (400) displayed on the screen, a user input indication indicating a selection of a graphical element (402) representing the assistant application (106);
Receiving a voice input from the user, the voice input comprising a call command (80), the call command (80) for launching the assistant application (106) for execution on the data processing hardware (202); or alternatively
A predefined movement/gesture of the user device (200) configured to launch the assistant application (106) to execute on the data processing hardware (202) is detected.
4. The method (500) of any of claims 1-3, wherein obtaining the proximity information (250) for at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) within the environment (32) of the user device (200) comprises:
receiving, at a sensor of the user device (200), a wireless communication signal transmitted from the at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210); and
the proximity information (250) for the at least one assistant-enabled device (210) is determined based on a signal strength of the wireless communication signal received at the sensor of the user device (200).
5. The method (500) of any of claims 1 to 4, further comprising: -obtaining, by the data processing hardware (202), directionality information (255) for at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) within the environment (32) of the user device (200) by:
Receiving, at each sensor in a sensor array of the user device (200), a wireless communication signal transmitted from the at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210); and
determining the directionality information (255) for the at least one assistant-enabled device (210) based on respective signal strengths of the wireless communication signals received at each sensor in the sensor array of the user device (200) relative to respective signal strengths of the wireless communication signals received at other sensors in the sensor array of the user device (200),
wherein determining the proximity score (260) for the at least one assistant-enabled device (210) is further based on the directionality information (255) for the at least one assistant-enabled device (210).
6. The method (500) of any of claims 1 to 5, wherein obtaining the proximity information (250) for at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) within the environment (32) of the user device (200) comprises:
receiving, at the user device (200), an audible or inaudible signal output from the at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210); and
The proximity information (250) for the at least one assistant-enabled device (210) is determined based on energy and/or frequency of the audible or inaudible signal output from the at least one assistant-enabled device (210).
7. The method (500) of any of claims 1 to 6, further comprising:
receiving, at the data processing hardware (202), from each assistant-enabled device (210) of the plurality of assistant-enabled devices (210), a respective set of available actions (120) associated with the corresponding assistant-enabled device (210); and
for each corresponding assistant-enabled device (210) in the ranked list (310) of candidate assistant-enabled devices (210), a set of respective controls (220) for executing a set of respective actions (120) associated with the corresponding assistant-enabled device (210) is determined by the data processing hardware (202).
8. The method (500) of claim 7, wherein at least one available action (120) of the respective set of available actions (120) received from at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) comprises a suggested action (120) for the corresponding assistant-enabled device (210) to perform based on a current context.
9. The method (500) of claim 7 or 8, further comprising:
at the data processing hardware (202), receiving device state information (215) associated with the corresponding assistant-enabled device (210) from at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210),
wherein determining a set of respective controls (220) for performing a set of respective actions (120) associated with the corresponding assistant-enabled device (210) is further based on the device state information (215) associated with the corresponding assistant-enabled device (210).
10. The method (500) of any of claims 1 to 9, wherein generating the ranked list (310) of candidate assistant-enabled devices (210) from among the plurality of assistant-enabled devices (210) comprises: the assistant-enabled devices (210) are ordered from the assistant-enabled device (210) having closest proximity to the user device (200) to the assistant-enabled device (210) having farthest proximity to the user device (200).
11. The method (500) of any of claims 1 to 10, wherein generating the ranked list (310) of candidate assistant-enabled devices (210) from among the plurality of assistant-enabled devices (210) comprises: any assistant-enabled devices (210) that include a proximity score (260) indicating a proximity estimate that meets a maximum distance threshold are discarded from the ranked list (310) of candidate assistant-enabled devices (210).
12. The method (500) of any of claims 1 to 11, wherein generating the ranked list (310) of candidate assistant-enabled devices (210) from among the plurality of assistant-enabled devices (210) comprises: any assistant-enabled devices (210) that include a proximity score (260) indicating a proximity estimate that meets a minimum distance threshold are discarded from the ranked list (310) of candidate assistant-enabled devices (210).
13. The method (500) of any of claims 1 to 12, further comprising:
receiving, at the data processing hardware (202), device state information (215) associated with at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) from the at least one assistant-enabled device (210); and
after generating the ranked list (310) of candidate assistant-enabled devices (210), re-ranking the candidate assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210) by the data processing hardware (202) using the device state information (215) associated with the at least one assistant-enabled device (210),
wherein displaying a set of respective controls (220) for performing a set of respective actions (120) associated with the corresponding assistant-enabled device (210) is based on the re-ranking of candidate assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210).
14. The method (500) of any of claims 1-13, wherein the ranked list (310) of candidate assistant-enabled devices (210) from the plurality of assistant-enabled devices (210) includes the assistant-enabled device (210) selected from the plurality of assistant-enabled devices (210) having the N highest proximity scores (260) among the plurality of proximity scores (260) determined for the plurality of assistant-enabled devices (210).
15. The method (500) of any of claims 1 to 14, wherein displaying the set of respective controls (220) for performing the set of respective actions (120) associated with the corresponding assistant-enabled device (210) comprises: one of the assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210) displays a set of respective controls (220) in the GUI (400) differently than a set of respective controls (220) displayed in the GUI (400) for at least another one of the assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210).
16. A user equipment (200), comprising:
Data processing hardware (202); and
memory hardware (204) in communication with the data processing hardware (202) and storing instructions that, when executed on the data processing hardware (202), cause the data processing hardware (202) to perform operations comprising:
obtaining proximity information (250) for each of a plurality of assistant-enabled devices (210) within an environment (32) of the user device (200), each assistant-enabled device (210) of the plurality of assistant-enabled devices (210) controllable by an assistant application (106) to perform a respective set of available actions (120) associated with the assistant-enabled device (210);
for each assistant-enabled device (210) of the plurality of assistant-enabled devices (210), determining a proximity score (260) based on the proximity information (250) obtained for the corresponding assistant-enabled device (210), the proximity score (260) indicating a proximity estimate of the corresponding assistant-enabled device (210) relative to the user devices (200) in the environment (32);
generating a ranked list (310) of candidate assistant-enabled devices (210) from among a plurality of assistant-enabled devices (210) using the plurality of proximity scores (260) determined for the plurality of assistant-enabled devices (210); and
For each of one or more corresponding assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210), a set of respective controls (220) for executing a set of respective actions (120) associated with the corresponding assistant-enabled device (210) is displayed in an on-screen displayed graphical user interface GUI (400) in communication with the data processing hardware (202).
17. The user equipment (200) of claim 16, wherein the operations further comprise:
receiving a user request (108) from a user of the user device (200), the user request (108) for requesting to launch the assistant application (106),
wherein obtaining the proximity information (250) for each of the plurality of assistant-enabled devices (210) occurs during execution of the assistant application (106).
18. The user equipment (200) of claim 17, wherein receiving the user request (108) comprises one of:
receiving, in the GUI (400) displayed on the screen, a user input indication indicating a selection of a graphical element (402) representing the assistant application (106);
receiving a voice input from the user, the voice input comprising a invoke command (80), the invoke command (80) for launching the assistant application (106); or alternatively
A predefined movement/gesture of the user device (200) configured to launch the assistant application (106) is detected.
19. The user device (200) of any of claims 16-18, wherein obtaining the proximity information (250) for at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) within the environment (32) of the user device (200) comprises:
receiving, at a sensor of the user device (200), a wireless communication signal transmitted from the at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210); and
the proximity information (250) for the at least one assistant-enabled device (210) is determined based on a signal strength of the wireless communication signal received at the sensor of the user device (200).
20. The user device (200) of any of claims 16-19, wherein the operations further comprise obtaining directionality information (255) for at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) within the environment (32) of the user device (200) by:
Receiving, at each sensor in a sensor array of the user device (200), a wireless communication signal transmitted from the at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210); and
determining the directionality information (255) for the at least one assistant-enabled device (210) based on respective signal strengths of the wireless communication signals received at each sensor in the sensor array of the user device (200) relative to respective signal strengths of the wireless communication signals received at other sensors in the sensor array of the user device (200),
wherein determining the proximity score (260) for the at least one assistant-enabled device (210) is further based on the directionality information (255) for the at least one assistant-enabled device (210).
21. The user device (200) of any of claims 16-20, wherein obtaining the proximity information (250) for at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) within the environment (32) of the user device (200) comprises:
receiving an audible or inaudible signal output from the at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210); and
The proximity information (250) for the at least one assistant-enabled device (210) is determined based on energy and/or frequency of the audible or inaudible signal output from the at least one assistant-enabled device (210).
22. The user equipment (200) of any of claims 16-21, wherein the operations further comprise:
receiving, from each assistant-enabled device (210) of the plurality of assistant-enabled devices (210), a respective set of available actions (120) associated with the corresponding assistant-enabled device (210); and
for each corresponding assistant-enabled device (210) in the ranked list (310) of candidate assistant-enabled devices (210), a set of respective controls (220) for executing a set of respective actions (120) associated with the corresponding assistant-enabled device (210) is determined.
23. The user device (200) of claim 22, wherein at least one available action (120) of the respective set of available actions (120) received from at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) comprises a suggested action (120) for the corresponding assistant-enabled device (210) to perform based on a current context.
24. The user equipment (200) of claim 22 or 23, wherein the operations further comprise:
receiving device state information (215) associated with the corresponding assistant-enabled device (210) from at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210),
wherein determining a set of respective controls (220) for performing a set of respective actions (120) associated with the corresponding assistant-enabled device (210) is further based on the device state information (215) associated with the corresponding assistant-enabled device (210).
25. The user device (200) of any of claims 16-24, wherein generating the ranked list (310) of candidate assistant-enabled devices (210) from among the plurality of assistant-enabled devices (210) comprises: the assistant-enabled devices (210) are ordered from the assistant-enabled device (210) having closest proximity to the user device (200) to the assistant-enabled device (210) having farthest proximity to the user device (200).
26. The user device (200) of any of claims 16-25, wherein generating the ranked list (310) of candidate assistant-enabled devices (210) from among the plurality of assistant-enabled devices (210) comprises: any assistant-enabled devices (210) that include a proximity score (260) indicating a proximity estimate that meets a maximum distance threshold are discarded from the ranked list (310) of candidate assistant-enabled devices (210).
27. The user device (200) of any of claims 16-26, wherein generating the ranked list (310) of candidate assistant-enabled devices (210) from among the plurality of assistant-enabled devices (210) comprises: any assistant-enabled devices (210) that include a proximity score (260) indicating a proximity estimate that meets a minimum distance threshold are discarded from the ranked list (310) of candidate assistant-enabled devices (210).
28. The user equipment (200) of any of claims 16-27, wherein the operations further comprise:
receiving device state information (215) associated with at least one assistant-enabled device (210) of the plurality of assistant-enabled devices (210) from the at least one assistant-enabled device (210); and
after generating the ranked list (310) of candidate assistant-enabled devices (210), using the device state information (215) associated with the at least one assistant-enabled device (210), the candidate assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210),
wherein displaying a set of respective controls (220) for performing a set of respective actions (120) associated with the corresponding assistant-enabled device (210) is based on the re-ranking of candidate assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210).
29. The user device (200) of any of claims 16-28, wherein the ranked list (310) of candidate assistant-enabled devices (210) from the plurality of assistant-enabled devices (210) includes the assistant-enabled device (210) selected from the plurality of assistant-enabled devices (210) having the N highest proximity scores (260) among the plurality of proximity scores (260) determined for the plurality of assistant-enabled devices (210).
30. The user device (200) of any of claims 16-29, wherein displaying the set of respective controls (220) for performing the set of respective actions (120) associated with the corresponding assistant-enabled device (210) comprises: one of the assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210) displays a set of respective controls (220) in the GUI (400) differently than a set of respective controls (220) displayed in the GUI (400) for at least another one of the assistant-enabled devices (210) in the ranked list (310) of candidate assistant-enabled devices (210).
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/951,967 US11402984B2 (en) | 2020-11-18 | 2020-11-18 | Proximity-based controls on a second device |
US16/951,967 | 2020-11-18 | ||
PCT/US2021/059586 WO2022108953A1 (en) | 2020-11-18 | 2021-11-16 | Proximity-based controls on a second device |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116635821A true CN116635821A (en) | 2023-08-22 |
Family
ID=78845106
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180084490.6A Pending CN116635821A (en) | 2020-11-18 | 2021-11-16 | Proximity-based control on a second device |
Country Status (5)
Country | Link |
---|---|
US (3) | US11402984B2 (en) |
EP (1) | EP4229504A1 (en) |
JP (1) | JP2023552704A (en) |
CN (1) | CN116635821A (en) |
WO (1) | WO2022108953A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP4047599A4 (en) * | 2020-12-23 | 2022-12-21 | Samsung Electronics Co., Ltd. | Method for providing voice-based content, and electronic device therefor |
US11631416B1 (en) * | 2021-12-09 | 2023-04-18 | Kyndryl, Inc. | Audio content validation via embedded inaudible sound signal |
Family Cites Families (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080177734A1 (en) * | 2006-02-10 | 2008-07-24 | Schwenke Derek L | Method for Presenting Result Sets for Probabilistic Queries |
US9275154B2 (en) * | 2010-06-18 | 2016-03-01 | Google Inc. | Context-sensitive point of interest retrieval |
US9942304B2 (en) * | 2011-01-08 | 2018-04-10 | N99 Llc | Remote control authority and authentication |
WO2013067526A1 (en) * | 2011-11-04 | 2013-05-10 | Remote TelePointer, LLC | Method and system for user interface for interactive devices using a mobile device |
US10185934B2 (en) * | 2013-07-09 | 2019-01-22 | Qualcomm Incorporated | Real-time context aware recommendation engine based on a user internet of things environment |
US10311694B2 (en) * | 2014-02-06 | 2019-06-04 | Empoweryu, Inc. | System and method for adaptive indirect monitoring of subject for well-being in unattended setting |
US9462108B2 (en) * | 2014-05-12 | 2016-10-04 | Lg Electronics Inc. | Mobile terminal and method for controlling the mobile terminal |
US10042445B1 (en) * | 2014-09-24 | 2018-08-07 | Amazon Technologies, Inc. | Adaptive display of user interface elements based on proximity sensing |
US20160117076A1 (en) * | 2014-10-22 | 2016-04-28 | Lg Electronics Inc. | Mobile terminal and control method thereof |
WO2016131696A1 (en) * | 2015-02-18 | 2016-08-25 | Koninklijke Philips N.V. | Monitoring activities of daily living of a person |
US20170064073A1 (en) * | 2015-09-01 | 2017-03-02 | Qualcomm Incorporated | Controlling one or more proximate devices via a mobile device based on one or more detected user actions while the mobile device operates in a low power mode |
US10104501B2 (en) * | 2016-04-12 | 2018-10-16 | Elliptic Laboratories As | Proximity detection |
US10587480B2 (en) * | 2016-11-14 | 2020-03-10 | WiSilica Inc. | User experience enhancement using proximity awareness |
US10992795B2 (en) * | 2017-05-16 | 2021-04-27 | Apple Inc. | Methods and interfaces for home media control |
US20190129607A1 (en) * | 2017-11-02 | 2019-05-02 | Samsung Electronics Co., Ltd. | Method and device for performing remote control |
US10455029B2 (en) * | 2017-12-29 | 2019-10-22 | Dish Network L.L.C. | Internet of things (IOT) device discovery platform |
US11323517B2 (en) * | 2018-04-30 | 2022-05-03 | Tracfone Wireless, Inc. | Internet of things connectivity aggregator system and process |
US11233671B2 (en) * | 2018-11-28 | 2022-01-25 | Motorola Mobility Llc | Smart internet of things menus with cameras |
US20200204673A1 (en) * | 2018-12-20 | 2020-06-25 | Arris Enterprises Llc | Internet of things user interface simplification |
US20200280800A1 (en) * | 2019-02-28 | 2020-09-03 | Sonos, Inc. | Playback Transitions |
-
2020
- 2020-11-18 US US16/951,967 patent/US11402984B2/en active Active
-
2021
- 2021-11-16 WO PCT/US2021/059586 patent/WO2022108953A1/en active Application Filing
- 2021-11-16 JP JP2023530045A patent/JP2023552704A/en active Pending
- 2021-11-16 EP EP21824197.4A patent/EP4229504A1/en active Pending
- 2021-11-16 CN CN202180084490.6A patent/CN116635821A/en active Pending
-
2022
- 2022-07-12 US US17/811,973 patent/US11880559B2/en active Active
-
2023
- 2023-12-20 US US18/391,583 patent/US20240143154A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP2023552704A (en) | 2023-12-19 |
US11880559B2 (en) | 2024-01-23 |
US20240143154A1 (en) | 2024-05-02 |
WO2022108953A1 (en) | 2022-05-27 |
US20220155946A1 (en) | 2022-05-19 |
US11402984B2 (en) | 2022-08-02 |
US20220342537A1 (en) | 2022-10-27 |
KR20230104288A (en) | 2023-07-07 |
EP4229504A1 (en) | 2023-08-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9851804B2 (en) | Environment-dependent dynamic range control for gesture recognition | |
US10534534B2 (en) | Method for controlling display, storage medium, and electronic device | |
TWI603258B (en) | Dynamic thresholds for always listening speech trigger | |
JP6318232B2 (en) | Voice management at the tab level for user notification and control | |
US20240143154A1 (en) | Proximity-Based Controls on a Second Device | |
AU2014201856B2 (en) | APP operating method and device and app output device supporting the same | |
KR101637813B1 (en) | Proximity detection of candidate companion display device in same room as primary display using wi-fi or bluetooth signal strength | |
US9727232B2 (en) | Methods, apparatuses, and computer program products for improving device behavior based on user interaction | |
US11157169B2 (en) | Operating modes that designate an interface modality for interacting with an automated assistant | |
US20190373038A1 (en) | Technologies for a seamless data streaming experience | |
WO2016067765A1 (en) | Information processing device, information processing method, and computer program | |
WO2019014189A1 (en) | Controlling visual indicators in an audio responsive device, and capturing and providing audio using an api | |
US10338882B2 (en) | Contextual based selection among multiple devices for content playback | |
JP2018021987A (en) | Conversation processing device and program | |
US11838582B1 (en) | Media arbitration | |
US20170220358A1 (en) | Identification and presentation of element at a first device to control a second device | |
KR102662777B1 (en) | Proximity-based controls for a second device | |
CN110955436A (en) | Application program installation method and electronic equipment | |
CN111656303A (en) | Gesture control of data processing apparatus | |
KR20240063190A (en) | Proximity-based controls on a second device | |
EP3930336A1 (en) | Display apparatus for providing content in connection with user terminal and method therefor | |
US10249265B2 (en) | Multi-device content presentation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |