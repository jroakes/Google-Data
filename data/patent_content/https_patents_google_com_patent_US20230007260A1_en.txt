US20230007260A1 - Probability Estimation for Video Coding - Google Patents
Probability Estimation for Video Coding Download PDFInfo
- Publication number
- US20230007260A1 US20230007260A1 US17/775,565 US202017775565A US2023007260A1 US 20230007260 A1 US20230007260 A1 US 20230007260A1 US 202017775565 A US202017775565 A US 202017775565A US 2023007260 A1 US2023007260 A1 US 2023007260A1
- Authority
- US
- United States
- Prior art keywords
- probability
- symbol
- estimation
- probability estimation
- model
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000003044 adaptive effect Effects 0.000 claims abstract description 11
- 238000000034 method Methods 0.000 claims description 58
- 238000009826 distribution Methods 0.000 claims description 15
- 230000003247 decreasing effect Effects 0.000 claims description 4
- 238000007476 Maximum Likelihood Methods 0.000 claims description 3
- 238000004422 calculation algorithm Methods 0.000 description 27
- 230000006870 function Effects 0.000 description 22
- 238000010586 diagram Methods 0.000 description 15
- 230000008569 process Effects 0.000 description 15
- 238000012545 processing Methods 0.000 description 13
- 238000004891 communication Methods 0.000 description 10
- 238000001914 filtration Methods 0.000 description 8
- 230000006835 compression Effects 0.000 description 7
- 238000007906 compression Methods 0.000 description 7
- 239000013598 vector Substances 0.000 description 6
- 238000013459 approach Methods 0.000 description 4
- 238000009795 derivation Methods 0.000 description 4
- 238000005457 optimization Methods 0.000 description 4
- 238000013139 quantization Methods 0.000 description 4
- 208000037170 Delayed Emergence from Anesthesia Diseases 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 3
- 230000000903 blocking effect Effects 0.000 description 3
- 238000004364 calculation method Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000004888 barrier function Effects 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000012360 testing method Methods 0.000 description 2
- 102100029272 5-demethoxyubiquinone hydroxylase, mitochondrial Human genes 0.000 description 1
- 238000012935 Averaging Methods 0.000 description 1
- 101100494773 Caenorhabditis elegans ctl-2 gene Proteins 0.000 description 1
- 102100035959 Cationic amino acid transporter 2 Human genes 0.000 description 1
- 102100021391 Cationic amino acid transporter 3 Human genes 0.000 description 1
- 102100021392 Cationic amino acid transporter 4 Human genes 0.000 description 1
- 101710195194 Cationic amino acid transporter 4 Proteins 0.000 description 1
- 101100112369 Fasciola hepatica Cat-1 gene Proteins 0.000 description 1
- 101000770593 Homo sapiens 5-demethoxyubiquinone hydroxylase, mitochondrial Proteins 0.000 description 1
- 101100005271 Neurospora crassa (strain ATCC 24698 / 74-OR23-1A / CBS 708.71 / DSM 1257 / FGSC 987) cat-1 gene Proteins 0.000 description 1
- 108091006231 SLC7A2 Proteins 0.000 description 1
- 108091006230 SLC7A3 Proteins 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 230000008867 communication pathway Effects 0.000 description 1
- 230000001143 conditioned effect Effects 0.000 description 1
- 230000006837 decompression Effects 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000003467 diminishing effect Effects 0.000 description 1
- 208000037265 diseases, disorders, signs and symptoms Diseases 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000004927 fusion Effects 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000001131 transforming effect Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/40—Conversion to or from variable length codes, e.g. Shannon-Fano code, Huffman code, Morse code
- H03M7/4006—Conversion to or from arithmetic code
- H03M7/4012—Binary arithmetic codes
- H03M7/4018—Context adapative binary arithmetic codes [CABAC]
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/40—Conversion to or from variable length codes, e.g. Shannon-Fano code, Huffman code, Morse code
- H03M7/4031—Fixed length to variable length coding
- H03M7/4037—Prefix coding
- H03M7/4043—Adaptive prefix coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/124—Quantisation
- H04N19/126—Details of normalisation or weighting functions, e.g. normalisation matrices or variable uniform quantisers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/13—Adaptive entropy coding, e.g. adaptive variable length coding [AVLC] or context adaptive binary arithmetic coding [CABAC]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/90—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using coding techniques not provided for in groups H04N19/10-H04N19/85, e.g. fractals
- H04N19/91—Entropy coding, e.g. variable length coding [VLC] or arithmetic coding
Definitions
- Digital video streams may represent video using a sequence of frames or still images.
- Digital video can be used for various applications including, for example, video conferencing, high-definition video entertainment, video advertisements, or sharing of user-generated videos.
- a digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data.
- Various approaches have been proposed to reduce the amount of data in video streams, including lossy and lossless compression techniques.
- Probability estimation is used for entropy coding, particularly with context-based entropy coding for lossless compression.
- a multimodal approach is described herein that uses multiple linear update models to accurately estimate probabilities.
- An aspect of the teachings herein is a method for entropy coding a sequence of symbols (i.e., multiple symbols).
- the method can include determining a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models, entropy coding at least one symbol of the sequence using a probability determined by the first probability model, after entropy coding a respective symbol of the sequence, determining a first probability estimation to update the probability using the first probability model, for a subsequent symbol relative to the at least one symbol of the sequence, determining a second probability estimation using a second probability model, and entropy coding the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.
- An aspect of the teachings herein is an apparatus for entropy coding a sequence of symbols including a processor.
- the processor is configured to determine a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models, entropy code at least one symbol of the sequence using a probability determined by the first probability model, after entropy coding a respective symbol of the sequence, determine a first probability estimation to update the probability using the first probability model, for a subsequent symbol relative to the at least one symbol of the sequence, determine a second probability estimation using a second probability model, and entropy code the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.
- FIG. 1 is a schematic of an example of a video encoding and decoding system.
- FIG. 2 is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station.
- FIG. 3 is a diagram of an example of a video stream to be encoded and subsequently decoded.
- FIG. 4 is a block diagram of an example of an encoder.
- FIG. 5 is a block diagram of an example of a decoder.
- FIG. 6 is a diagram illustrating quantized transform coefficients according to implementations of this disclosure.
- FIG. 7 is a diagram of a coefficient token tree that can be used to entropy code blocks into a video bitstream according to implementations of this disclosure.
- FIG. 8 is a diagram of an example of a tree for binarizing a quantized transform coefficient according to implementations of this disclosure.
- FIG. 9 is a flow chart of a method for entropy coding a sequence of symbols according to the teachings herein.
- Video compression schemes may include breaking respective images, or frames, into smaller portions, such as blocks, and generating an encoded bitstream using techniques to limit the information included for respective blocks thereof.
- the encoded bitstream can be decoded to re-create or reconstruct the source images from the limited information.
- the information may be limited by lossy coding, lossless coding, or some combination of lossy and lossless coding.
- entropy coding compresses a sequence in an informationally efficient way. That is, a lower bound of the length of the compressed sequence is the entropy of the original sequence.
- An efficient algorithm for entropy coding desirably generates a code (e.g., in bits) whose length approaches the entropy. For a sequence s with a length N, the entropy associated with binary codewords may be defined as equation (1), below:
- ⁇ t 1 N ⁇ log 2 ( p ( s t
- variable p represents the probability of an individual symbol
- variable p t represents the probability distribution of symbols at time t conditioned on the previously observed symbols.
- Arithmetic coding can use the probability to construct the codewords.
- a coder does not receive a streaming sequence of symbols together with the probability distribution for the symbols.
- probability estimation may be used in video codecs to implement entropy coding. That is, the probability distribution of the symbols may be estimated.
- the codelength approaches equation (2) below:
- entropy coding may rely upon probability estimation models (also called probability models herein) that model the distribution of values occurring in an encoded bitstream.
- probability estimation models also called probability models herein
- entropy coding can reduce the number of bits required to represent the input data to close to a theoretical minimum (i.e., the lower bound).
- the actual reduction in the number of bits required to represent video data can be a function of the accuracy of the probability model, the number of bits over which the coding is performed, and the computational accuracy of the (e.g., fixed-point) arithmetic used to perform the coding.
- a significant difficulty in the estimation is that the probability is time variant, which means that p t cannot be replaced by a single value p.
- probability estimation is described herein that combines a probability estimation model, which is a first-order linear system, with another model to form a higher-order linear system. While the teachings herein may be used in either a one-pass or a two-pass coding system, the estimation of probability herein may be referred to as online estimation of probability because it is capable of use in a one-pass system with high efficiency.
- the available probability estimation models may be two or more models including a context-adaptive binary arithmetic coding (CABAC) model, an AV1 model, a counting model, or any other probability estimation model or algorithm.
- CABAC context-adaptive binary arithmetic coding
- Implementations according to this disclosure can efficiently perform probability estimation for entropy coding, particularly with context-based entropy coding for lossless compression, by more accurately modeling the conditional probability of streaming symbols.
- the probability estimation contributes to efficient compression, reducing the number of bits required to represent video data.
- the probability estimation may be used in any probability estimation of a sequence of symbols but may be particularly effective for online probability estimation of such a sequence (e.g., real-time or delay sensitive applications of video coding).
- FIG. 1 is a schematic of an example of a video encoding and decoding system 100 .
- a transmitting station 102 can be, for example, a computer having an internal configuration of hardware such as that described in FIG. 2 .
- the processing of the transmitting station 102 can be distributed among multiple devices.
- a network 104 can connect the transmitting station 102 and a receiving station 106 for encoding and decoding of the video stream.
- the video stream can be encoded in the transmitting station 102
- the encoded video stream can be decoded in the receiving station 106 .
- the network 104 can be, for example, the Internet.
- the network 104 can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmitting station 102 to, in this example, the receiving station 106 .
- LAN local area network
- WAN wide area network
- VPN virtual private network
- the receiving station 106 in one example, can be a computer having an internal configuration of hardware such as that described in FIG. 2 . However, other suitable implementations of the receiving station 106 are possible. For example, the processing of the receiving station 106 can be distributed among multiple devices.
- an implementation can omit the network 104 .
- a video stream can be encoded and then stored for later transmission to the receiving station 106 or any other device having memory.
- the receiving station 106 receives (e.g., via the network 104 , a computer bus, and/or some communication pathway) the encoded video stream and stores the video stream for later decoding.
- a real-time transport protocol RTP
- a transport protocol other than RTP may be used, e.g., a Hypertext Transfer Protocol-based (HTTP-based) video streaming protocol.
- the transmitting station 102 and/or the receiving station 106 may include the ability to both encode and decode a video stream as described below.
- the receiving station 106 could be a video conference participant who receives an encoded video bitstream from a video conference server (e.g., the transmitting station 102 ) to decode and view and further encodes and transmits his or her own video bitstream to the video conference server for decoding and viewing by other participants.
- the video encoding and decoding system 100 may instead be used to encode and decode data other than video data.
- the video encoding and decoding system 100 can be used to process image data.
- the image data may include a block of data from an image.
- the transmitting station 102 may be used to encode the image data and the receiving station 106 may be used to decode the image data.
- the receiving station 106 can represent a computing device that stores the encoded image data for later use, such as after receiving the encoded or pre-encoded image data from the transmitting station 102 .
- the transmitting station 102 can represent a computing device that decodes the image data, such as prior to transmitting the decoded image data to the receiving station 106 for display.
- FIG. 2 is a block diagram of an example of a computing device 200 that can implement a transmitting station or a receiving station.
- the computing device 200 can implement one or both of the transmitting station 102 and the receiving station 106 of FIG. 1 .
- the computing device 200 can be in the form of a computing system including multiple computing devices, or in the form of one computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like.
- a processor 202 in the computing device 200 can be a conventional central processing unit.
- the processor 202 can be another type of device, or multiple devices, capable of manipulating or processing information now existing or hereafter developed.
- the disclosed implementations can be practiced with one processor as shown (e.g., the processor 202 ), advantages in speed and efficiency can be achieved by using more than one processor.
- a memory 204 in computing device 200 can be a read only memory (ROM) device or a random-access memory (RAM) device in an implementation. However, other suitable types of storage device can be used as the memory 204 .
- the memory 204 can include code and data 206 that is accessed by the processor 202 using a bus 212 .
- the memory 204 can further include an operating system 208 and application programs 210 , the application programs 210 including at least one program that permits the processor 202 to perform the techniques described herein.
- the application programs 210 can include applications 1 through N, which further include a video coding application that performs the techniques described herein.
- the computing device 200 can also include a secondary storage 214 , which can, for example, be a memory card used with a mobile computing device. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- the computing device 200 can also include one or more output devices, such as a display 218 .
- the display 218 may be, in one example, a touch sensitive display that combines a display with a touch sensitive element that is operable to sense touch inputs.
- the display 218 can be coupled to the processor 202 via the bus 212 .
- Other output devices that permit a user to program or otherwise use the computing device 200 can be provided in addition to or as an alternative to the display 218 .
- the output device is or includes a display
- the display can be implemented in various ways, including by a liquid crystal display (LCD), a cathode-ray tube (CRT) display, or a light emitting diode (LED) display, such as an organic LED (OLED) display.
- LCD liquid crystal display
- CRT cathode-ray tube
- LED light emitting diode
- OLED organic LED
- the computing device 200 can also include or be in communication with an image-sensing device 220 , for example, a camera, or any other image-sensing device 220 now existing or hereafter developed that can sense an image such as the image of a user operating the computing device 200 .
- the image-sensing device 220 can be positioned such that it is directed toward the user operating the computing device 200 .
- the position and optical axis of the image-sensing device 220 can be configured such that the field of vision includes an area that is directly adjacent to the display 218 and from which the display 218 is visible.
- the computing device 200 can also include or be in communication with a sound-sensing device 222 , for example, a microphone, or any other sound-sensing device now existing or hereafter developed that can sense sounds near the computing device 200 .
- the sound-sensing device 222 can be positioned such that it is directed toward the user operating the computing device 200 and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates the computing device 200 .
- FIG. 2 depicts the processor 202 and the memory 204 of the computing device 200 as being integrated into a single unit, other configurations can be utilized.
- the operations of the processor 202 can be distributed across multiple machines (wherein individual machines can have one or more processors) that can be coupled directly or across a local area or other network.
- the memory 204 can be distributed across multiple machines such as a network-based memory or memory in multiple machines performing the operations of the computing device 200 .
- the bus 212 of the computing device 200 can be composed of multiple buses.
- the secondary storage 214 can be directly coupled to the other components of the computing device 200 or can be accessed via a network and can comprise an integrated unit such as a memory card or multiple units such as multiple memory cards.
- the computing device 200 can thus be implemented in a wide variety of configurations.
- FIG. 3 is a diagram of an example of a video stream 300 to be encoded and subsequently decoded.
- the video stream 300 includes a video sequence 302 .
- the video sequence 302 includes several adjacent frames 304 . While three frames are depicted as the adjacent frames 304 , the video sequence 302 can include any number of adjacent frames 304 .
- the adjacent frames 304 can then be further subdivided into individual frames, for example, a frame 306 .
- the frame 306 can be divided into a series of planes or segments 308 .
- the segments 308 can be subsets of frames that permit parallel processing, for example.
- the segments 308 can also be subsets of frames that can separate the video data into separate colors.
- a frame 306 of color video data can include a luminance plane and two chrominance planes.
- the segments 308 may be sampled at different resolutions.
- the frame 306 may be further subdivided into blocks 310 , which can contain data corresponding to, for example, 16 ⁇ 16 pixels in the frame 306 .
- the blocks 310 can also be arranged to include data from one or more segments 308 of pixel data.
- the blocks 310 can also be of any other suitable size such as 4 ⁇ 4 pixels, 8 ⁇ 8 pixels, 16 ⁇ 8 pixels, 8 ⁇ 16 pixels, 16 ⁇ 16 pixels, or larger. Unless otherwise noted, the terms block and macroblock are used interchangeably herein.
- FIG. 4 is a block diagram of an example of an encoder 400 .
- the encoder 400 can be implemented, as described above, in the transmitting station 102 , such as by providing a computer software program stored in memory, for example, the memory 204 .
- the computer software program can include machine instructions that, when executed by a processor such as the processor 202 , cause the transmitting station 102 to encode video data in the manner described in FIG. 4 .
- the encoder 400 can also be implemented as specialized hardware included in, for example, the transmitting station 102 . In one particularly desirable implementation, the encoder 400 is a hardware encoder.
- the encoder 400 has the following stages to perform the various functions in a forward path (shown by the solid connection lines) to produce an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter prediction stage 402 , a transform stage 404 , a quantization stage 406 , and an entropy encoding stage 408 .
- the encoder 400 may also include a reconstruction path (shown by the dotted connection lines) to reconstruct a frame for encoding of future blocks.
- the encoder 400 has the following stages to perform the various functions in the reconstruction path: a dequantization stage 410 , an inverse transform stage 412 , a reconstruction stage 414 , and a loop filtering stage 416 .
- Other structural variations of the encoder 400 can be used to encode the video stream 300 .
- respective adjacent frames 304 can be processed in units of blocks.
- respective blocks can be encoded using intra-frame prediction (also called intra-prediction) or inter-frame prediction (also called inter-prediction).
- intra-frame prediction also called intra-prediction
- inter-frame prediction also called inter-prediction
- a prediction block can be formed.
- intra-prediction a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed.
- inter-prediction a prediction block may be formed from samples in one or more previously constructed reference frames.
- the prediction block can be subtracted from the current block at the intra/inter prediction stage 402 to produce a residual block (also called a residual).
- the transform stage 404 transforms the residual into transform coefficients in, for example, the frequency domain using block-based transforms.
- the quantization stage 406 converts the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients, using a quantizer value or a quantization level. For example, the transform coefficients may be divided by the quantizer value and truncated.
- the quantized transform coefficients are then entropy encoded by the entropy encoding stage 408 .
- the entropy-encoded coefficients, together with other information used to decode the block (which may include, for example, syntax elements such as used to indicate the type of prediction used, transform type, motion vectors, a quantizer value, or the like), are then output to the compressed bitstream 420 .
- the compressed bitstream 420 can be formatted using various techniques, such as variable length coding (VLC) or arithmetic coding.
- VLC variable length coding
- the compressed bitstream 420 can also be referred to as an encoded video stream or encoded video bitstream, and the terms will be used interchangeably herein.
- the reconstruction path (shown by the dotted connection lines) can be used to ensure that the encoder 400 and a decoder 500 (described below with respect to FIG. 5 ) use the same reference frames to decode the compressed bitstream 420 .
- the reconstruction path performs functions that are similar to functions that take place during the decoding process (described below with respect to FIG. 5 ), including dequantizing the quantized transform coefficients at the dequantization stage 410 and inverse transforming the dequantized transform coefficients at the inverse transform stage 412 to produce a derivative residual block (also called a derivative residual).
- the prediction block that was predicted at the intra/inter prediction stage 402 can be added to the derivative residual to create a reconstructed block.
- the loop filtering stage 416 can be applied to the reconstructed block to reduce distortion such as blocking artifacts.
- a non-transform-based encoder can quantize the residual signal directly without the transform stage 404 for certain blocks or frames.
- an encoder can have the quantization stage 406 and the dequantization stage 410 combined in a common stage.
- FIG. 5 is a block diagram of an example of a decoder 500 .
- the decoder 500 can be implemented in the receiving station 106 , for example, by providing a computer software program stored in the memory 204 .
- the computer software program can include machine instructions that, when executed by a processor such as the processor 202 , cause the receiving station 106 to decode video data in the manner described in FIG. 5 .
- the decoder 500 can also be implemented in hardware included in, for example, the transmitting station 102 or the receiving station 106 .
- the decoder 500 like the reconstruction path of the encoder 400 discussed above, includes in one example the following stages to perform various functions to produce an output video stream 516 from the compressed bitstream 420 : an entropy decoding stage 502 , a dequantization stage 504 , an inverse transform stage 506 , an intra/inter prediction stage 508 , a reconstruction stage 510 , a loop filtering stage 512 , and a deblocking filtering stage 514 .
- Other structural variations of the decoder 500 can be used to decode the compressed bitstream 420 .
- the data elements within the compressed bitstream 420 can be decoded by the entropy decoding stage 502 to produce a set of quantized transform coefficients.
- the dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage 506 inverse transforms the dequantized transform coefficients to produce a derivative residual that can be identical to that created by the inverse transform stage 412 in the encoder 400 .
- the decoder 500 can use the intra/inter prediction stage 508 to create the same prediction block as was created in the encoder 400 (e.g., at the intra/inter prediction stage 402 ).
- the prediction block can be added to the derivative residual to create a reconstructed block.
- the loop filtering stage 512 can be applied to the reconstructed block to reduce blocking artifacts. Other filtering can be applied to the reconstructed block.
- the deblocking filtering stage 514 is applied to the reconstructed block to reduce blocking distortion, and the result is output as the output video stream 516 .
- the output video stream 516 can also be referred to as a decoded video stream, and the terms will be used interchangeably herein.
- Other variations of the decoder 500 can be used to decode the compressed bitstream 420 . In some implementations, the decoder 500 can produce the output video stream 516 without the deblocking filtering stage 514 .
- bits are generally used for one of two things in an encoded video bitstream: either content prediction (e.g., inter mode/motion vector coding, intra prediction mode coding, etc.) or residual or coefficient coding (e.g., transform coefficients).
- Encoders may use techniques to decrease the bits spent on coefficient coding.
- a coefficient token tree (which may also be referred to as a binary token tree) specifies the scope of the value, with forward-adaptive probabilities for each branch in this token tree. The token base value is subtracted from the value to be coded to form a residual then the block is coded with fixed probabilities.
- a similar scheme with minor variations including backward-adaptivity is also possible.
- Adaptive techniques can alter the probability models as the video stream is being encoded to adapt to changing characteristics of the data.
- a decoder is informed of (or has available) the probability model used to encode an entropy-coded video bitstream in order to decode the video bitstream.
- FIG. 6 is a diagram 600 illustrating quantized transform coefficients according to implementations of this disclosure.
- the diagram 600 depicts a current block 620 , a scan order 602 , a quantized transform block 604 , a non-zero map 606 , an end-of-block map 622 , and a sign map 626 .
- the current block 620 is illustrated as a 4 ⁇ 4 block. However, any block size is possible.
- the current block can have a size (i.e., dimensions) of 4 ⁇ 4, 8 ⁇ 8, 16 ⁇ 16, 32 ⁇ 32, or any other square or rectangular block size.
- the current block 620 can be a block of a current frame.
- the current frame may be partitioned into segments (such as the segments 308 of FIG. 3 ), tiles, or the like, each including a collection of blocks, where the current block is a block of the partition.
- the quantized transform block 604 can be a block of a size similar to the size of the current block 620 .
- the quantized transform block 604 includes non-zero coefficients (e.g., a coefficient 608 ) and zero coefficients (e.g., a coefficient 610 ).
- the quantized transform block 604 contains quantized transform coefficients for the residual block corresponding to the current block 620 .
- the quantized transform coefficients are entropy coded by an entropy-coding phase, such as the entropy coding stage 408 of FIG. 4 .
- Entropy coding a quantized transform coefficient can involve the selection of a context model (also referred to as probability context model, probability model, model, and context) which provides estimates of conditional probabilities for coding the binary symbols of a binarized transform coefficient as described below with respect to FIG. 7 .
- a context model also referred to as probability context model, probability model, model, and context
- additional information may be used as the context for selecting a context model. For example, the magnitudes of the previously coded transform coefficients can be used, at least partially, for determining a probability model.
- a video coding system may traverse the transform block in a scan order and encode (e.g., entropy encode) the quantized transform coefficients as the quantized transform coefficients are respectively traversed (i.e., visited).
- encode e.g., entropy encode
- the top left corner of the transform block also known as the DC coefficient
- the next coefficient in the scan order i.e., the transform coefficient corresponding to the location labeled “1” is traversed and encoded, and so on.
- some quantized transform coefficients above and to the left of a current quantized transform coefficient are traversed first.
- Other scan orders are possible.
- a one-dimensional structure (e.g., an array) of quantized transform coefficients can result from the traversal of the two-dimensional quantized transform block using the scan order.
- encoding the quantized transform block 604 can include determining the non-zero map 606 , which indicates which quantized transform coefficients of the quantized transform block 604 are zero and which are non-zero.
- a non-zero coefficient and a zero coefficient can be indicated with values one (1) and zero (0), respectively, in the non-zero map.
- the non-zero map 606 includes a non-zero 607 at Cartesian location (0, 0) corresponding to the coefficient 608 and a zero 608 at Cartesian location (2, 0) corresponding to the coefficient 610 .
- encoding the quantized transform block 604 can include generating and encoding the end-of-block map 622 .
- the end-of-block map indicates whether a non-zero quantized transform coefficient of the quantized transform block 604 is the last non-zero coefficient with respect to a given scan order. If a non-zero coefficient is not the last non-zero coefficient in the transform block, then it can be indicated with the binary bit zero (0) in the end-of-block map. If, on the other hand, a non-zero coefficient is the last non-zero coefficient in the transform block, then it can be indicated with the binary value one (1) in the end-of-block map.
- the quantized transform coefficient corresponding to the scan location 11 i.e., the last non-zero quantized transform coefficient 628
- the end-of-block value 624 of one (1) all other non-zero transform coefficients are indicated with a zero.
- encoding the quantized transform block 604 can include generating and encoding the sign map 626 .
- the sign map 626 indicates which non-zero quantized transform coefficients of the quantized transform block 604 have positive values and which quantized transform coefficients have negative values. Transform coefficients that are zero need not be indicated in the sign map.
- the sign map 626 illustrates the sign map for the quantized transform block 604 . In the sign map, negative quantized transform coefficients can be indicated with a ⁇ 1 and positive quantized transform coefficients can be indicated with a one (1).
- FIG. 7 is a diagram of a coefficient token tree 700 that can be used to entropy code blocks into a video bitstream according to implementations of this disclosure.
- the coefficient token tree 700 is referred to as a binary tree because, at each node of the tree, one of two branches must be taken (i.e., traversed).
- the coefficient token tree 700 includes a root node 701 and a node 703 corresponding, respectively, to the nodes labeled A and B.
- EOB end-of-block
- a binary decision determining whether (or not) a current token is equal to the EOB token of the current block is coded immediately after an nonzero coefficient is decoded or at the first scan position (DC).
- DC first scan position
- M and N can take values, such as the values 2, 4, 8, 16, 32, and 64.
- the binary decision corresponds to the coding of a “1” bit corresponding to the decision to move from the root node 701 to the node 703 in the coefficient token tree 700 .
- coding a bit can mean the outputting or generating of a bit in the codeword representing a transform coefficient being encoded.
- decoding a bit can mean the reading (such as from an encoded bitstream) of a bit of the codeword corresponding to a quantized transform coefficient being decoded such that the bit corresponds to a branch being traversed in the coefficient token tree.
- a string of binary digits is generated for a quantized coefficient (e.g., the coefficients 608 , 610 of FIG. 6 ) of the quantized transform block (such as the quantized transform block 604 of FIG. 6 ).
- the quantized coefficients in an N ⁇ N block are organized into a 1D (one-dimensional) array (herein, an array u) following a prescribed scan order (e.g., the scan order 602 of FIG. 6 ).
- N can be 4, 8, 16, 32, or any other value.
- the starting position of the last run of zeroes in u[i], . . . , u[N*N ⁇ 1] can be denoted as eob.
- the eob can be set to the value N*N. That is, if the last coefficient of the 1D array u is not zero, then eob can be set to the value N*N.
- the values at each of the u[i]s is a quantized transform coefficient.
- the quantized transform coefficients of the 1D array u may also be referred herein simply as “coefficients” or “transform coefficients.”
- the eob is equal to 12 because there are no non-zero coefficients after the zero coefficient at position 12 of the 1D array u.
- the token t[i], for i ⁇ eob, can be indicative of the size and/or size range of the corresponding quantized transform coefficient at u[i].
- Table I below provides a listing of an example of token values, excluding the EOB_TOKEN, and their corresponding names according to an implementation of this disclosure.
- quantized coefficient values are taken to be signed 12-bit integers.
- the range of 12-bit signed values can be divided into 11 tokens (the tokens 0-10 in Table I) plus the end of block token (EOB_TOKEN).
- EOB_TOKEN the end of block token
- the coefficient token tree 700 can be traversed.
- the result (i.e., the bit string) of traversing the tree can then be encoded into a bitstream (such as the bitstream 420 of FIG. 4 ) by an encoder as described with respect to the entropy encoding stage 408 of FIG. 4 .
- the coefficient token tree 700 includes the tokens EOB_TOKEN (token 702 ), ZERO_TOKEN (token 704 ), ONE_TOKEN (token 706 ), TWO_TOKEN (token 708 ), THREE_TOKEN (token 710 ), FOUR_TOKEN (token 712 ), CAT1 (token 714 that is DCT_VAL_CAT1 in Table I), CAT2 (token 716 that is DCT_VAL_CAT2 in Table I), CAT3 (token 718 that is DCT_VAL_CAT3 in Table I), CAT4 (token 720 that is DCT_VAL_CAT4 in Table I), CAT5 (token 722 that is DCT_VAL_CAT5 in Table I) and CAT6 (token 724 that is DCT_VAL_CAT6 in Table I).
- EOB_TOKEN token 702
- ZERO_TOKEN token 704
- ONE_TOKEN token 706
- the coefficient token tree maps a single quantized coefficient value into a single token, such as one of the tokens 704, 706, 708, 710 and 712.
- Other tokens such as the tokens 714, 716, 718, 720, 722 and 724, represent ranges of quantized coefficient values.
- a quantized transform coefficient with a value of 37 can be represented by the token DCT_VAL_CAT5—the token 722 in FIG. 7 .
- the base value for a token is defined as the smallest number in its range.
- the base value for the token 720 is 19.
- Entropy coding identifies a token for each quantized coefficient and, if the token represents a range, can form a residual by subtracting the base value from the quantized coefficient.
- a quantized transform coefficient with a value of 20 can be represented by including the token 720 and a residual value of 1 (i.e., 20 minus 19) in the encoded video bitstream to permit a decoder to reconstruct the original quantized transform coefficient.
- the end of block token i.e., the token 702 ) signals that no further non-zero quantized coefficients remain in the transformed block data.
- the coefficient token tree 700 can be used.
- the coefficient token tree 700 is traversed starting at the root node 701 (i.e., the node labeled A). Traversing the coefficient token tree generates a bit string (a codeword) that will be encoded into the bitstream using, for example, binary arithmetic coding.
- the bit string is a representation of the current coefficient (i.e., the quantized transform coefficient being encoded).
- the token 702 (i.e., the EOB_TOKEN) is added into the bitstream. This is the case, for example, for the transform coefficient at scan order position 12 of FIG. 6 .
- the current coefficient is non-zero, or if there are non-zero values among any remaining coefficients of the current block, a “1” bit is added to the codeword and traversal passes to the node 703 (i.e., the node labeled B). At node B, the current coefficient is tested to see if it is equal to zero.
- the left-hand branch is taken such that token 704 representing the value ZERO_TOKEN and a bit “0” is added to the codeword. If not, a bit “1” is added to the codeword and traversal passes to node C. At node C, the current coefficient is tested to see if it is greater than 1. If the current coefficient is equal to one (1), the left-hand branch is taken and token 706 representing the value ONE_TOKEN is added to the bitstream (i.e., a “0” bit is added to the codeword). If the current coefficient is greater than one (1), traversal passes to node D to check the value of the current coefficient as compared to the value 4.
- a “0” bit is added to the codeword upon traversal to a left child node and a “1” bit is added to the codeword upon traversal to a right child node.
- a similar process is undertaken by a decoder when decoding a codeword from a compressed bitstream. The decoder reads a bit from bit stream. If the bit is a “1,” the coefficient token tree is traversed to the right and if the bit is a “0,” the tree is traversed to the left. The decoder reads then a next bit and repeats the process until traversal of the tree reaches a leaf node (i.e., a token).
- a binary string of 111010 is encoded.
- decoding the codeword 11100 results in the token TWO_TOKEN.
- a decoder can infer that the first bit must be 1.
- the first bit has to be 1 since, in traversing the tree, for a transform coefficient (e.g., transform coefficient at the zigzag scan order location 2 of FIG. 6 ) following a zero transform coefficient (e.g., transform coefficient at the zigzag scan order location 1 of FIG. 6 ), the traversal necessarily moves from the root node 701 to the node 703 .
- a binary flag checkEob can be used to instruct the encoder and the decoder to skip encoding and decoding the first bit leading from the root node in the coefficient token tree 700 .
- the binary flag checkEob is 0 (i.e., indicating that the root node should not be checked)
- the root node 701 of the coefficient token tree 700 is skipped and the node 703 becomes the first node of coefficient token tree 700 to be visited for traversal. That is, when the root node 701 is skipped, the encoder can skip encoding and the decoder can skip decoding and can infer a first bit (i.e., a binary bit “1”) of the encoded string.
- the binary flag checkEob can be initialized to 1 (i.e., indicating that the root node should be checked).
- the following steps illustrate an example process for decoding quantized transform coefficients in an N ⁇ N block.
- a token t[i] is decoded by using either (1) the full coefficient token tree (i.e., starting at the root node 701 of the coefficient token tree 700 ) if the binary flag checkEob is equal to 1; or (2) using the partial tree (e.g., starting at the node 703 ) where the EOB_TOKEN is skipped, if checkEob is equal to 0.
- decoding a token t[i] can include the steps of determining a context ctx, determining a binary probability distribution (i.e., a model) from the context ctx, and using a boolean arithmetic code to decode a path from the root node of the coefficient token tree 700 to a leaf node by using the determined probability distributions.
- the context ctx can be determined using a method of context derivation.
- the method of context derivation can use one or more of the block size, plane type (i.e., luminance or chrominance), the position i, and previously decoded tokens t [0], . . .
- the probability used to encode or decode a token t[i] given a context ctx may be fixed and does not adapt in a picture (i.e., a frame).
- the probability may be either a default value that is defined for the given context ctx or the probability may be coded (i.e., signaled) as part of the frame header for that frame. Coding the probability for every context in coding a frame can be costly.
- an encoder may analyze, for each context, whether it is beneficial to code the context's associated probability in the frame header and signal its decision to the decoder by using a binary flag.
- coding the probability for a context may use prediction to reduce cost (e.g., in bit rate) where the prediction may be derived from the probability of the same context in a previously decoded frame.
- each token can be associated with a value that is coded.
- an alphabet of symbols that includes more than two symbols is used for coding transform coefficients.
- the alphabet includes 12 symbols, namely ⁇ EOB_TOKEN, ZERO_TOKEN, ONE_TOKEN, TWO_TOKEN, THREE_TOKEN, FOUR_TOKEN, DCT_VAL_CAT1, DCT_VAL_CAT2, DCT_VAL_CAT3, DCT_VAL_CAT4, DCT_VAL_CAT5, DCT_VAL_CAT6 ⁇ .
- the alphabet for coding transform coefficients includes 12 symbols, which are also referred to as tokens. Other token alphabets that include more, fewer, or different tokens are possible.
- An alphabet that includes only the symbols ⁇ 0, 1 ⁇ is referred to herein as a binary alphabet.
- An alphabet that includes symbols other than and/or in addition to the symbols ⁇ 0, 1 ⁇ is referred to herein as a non-binary alphabet.
- Each of the tokens can be associated with a value.
- the EOB_TOKEN can have a value of 255.
- Each of the other tokens can each be associated with a different value.
- FIG. 8 is a diagram of an example of a tree 800 for binarizing a quantized transform coefficient according to implementations of this disclosure.
- the tree 800 is a binary tree that can be used for binarizing quantized transform coefficients in some video coding systems.
- the tree 800 can be used by a video coding system that uses the steps of binarization, context modelling, and binary arithmetic coding for encoding and decoding of quantized transform coefficients.
- the process may be referred to as context-adaptive binary arithmetic coding (CABAC).
- CABAC context-adaptive binary arithmetic coding
- the coding system may perform the following steps.
- the quantized transform coefficient x can be any of the coefficients (e.g., the coefficient 608 ) of the quantized transform block 604 of FIG. 6 .
- a coefficient x is first binarized into a binary string by using the tree 800 .
- the binarization process may binarize the unsigned value of the coefficient x. For example, binarizing the coefficient 628 (i.e., the value ⁇ 1), binarizes the value 1. This results in traversing the tree 800 and generating the binary string 10 .
- Each of the bits of the binary string 10 is referred to as a bin.
- a context is derived for each bin to be coded.
- a context can be derived from information such as one or more of the block size, plane type (i.e., luminance or chrominance), block position of the coefficient x, and previously decoded coefficients (e.g., a left and/or above neighboring coefficients, if available). Other information can be used to derive the context.
- a bin is coded by using, e.g., a binary arithmetic coding engine into a binary codeword together with a probability value associated with the context.
- the steps of coding a transform coefficient can include a step that is referred as context update.
- the context update step after a bin is coded, the probability associated with the context is updated to reflect the value of the bin.
- entropy coding a sequence of symbols may be achieved by using a probability model to determine a probability p for the sequence. Then, binary arithmetic coding may be used to map the sequence to a binary codeword at the encoder and to decode that sequence from the binary codeword at the decoder.
- the length (i.e., number of bits) of the codeword or string is given by equation (2) above. As the length is an integer number, however, the length is the smallest integer that is greater than the value calculated by equation (2).
- the efficiency of entropy coding can be directly related to the probability model.
- a subscript of t refers to the symbol at position t in the sequence.
- s is a sequence of five (5) binary symbols, such as 11010
- s 5 refers to the symbol in the 5 th position, such as the last 0 in the sequence 11010.
- sequence s can be expressed as s 1 , s 2 , . . . , s N .
- a symbol can refer to a token that is selected from a non-binary token alphabet that includes more than two tokens.
- the symbol i.e., token
- the token can be a token that is used to code, and is indicative of, a transform coefficient.
- “a sequence of symbols s” refers to the list of tokens s 1 , s 2 , . . . , s N used to code the transform coefficients at scan positions 1 , 2 , . . . , N, respectively, in a scan order.
- probability values such as the probability ⁇ circumflex over (p) ⁇ t (s t ) of a current symbol s t
- probability values can have either floating-point or fixed-point representations. Accordingly, operations applied to these values may use either floating-point arithmetic or fixed-point arithmetic.
- a probability estimation model which is a first-order linear system, is derived generally from equation (3) below, which estimates the probabilities that a symbol at t+1 is either 0 or 1 based on a weighted combination of the probabilities for the prior symbol at t and a conditional probability.
- the weighted combination uses a fixed weight or a variable weight as described below.
- the probability model may be from the probability estimation module in the CABAC framework used in H.264/AVC, such as described in Section III.C. of D. Marpe et al., “Context-based adaptive binary arithmetic coding in the H. 264/AVC video compression standard,” IEEE Transactions on Circuits and Systems for Video Technology, Vol. 13, No. 7, pp. 620-636 (2003).
- the value ⁇ is a constant of almost 0.95.
- the probability model may be from the probability estimation module in AV1, such as described in P. de Rivaz and J. Haughton, “AV1 bitstream & decoding process specification,” The Alliance for Open Media, p. 182 (2018) or Y.
- the probability update would use an adaptive a in terms of time (i.e., current symbol index) and number of symbols.
- p barrier there may be a barrier value p barrier such that if ⁇ circumflex over (p) ⁇ (0) or ⁇ circumflex over (p) ⁇ (1) is too small (i.e., is too close to 0 or 1 as indicated by a defined criteria), the value ⁇ is raised to p barrier .
- p barrier prevents the probability estimation from being equal to 0.
- p barrier is referred to as p 62 .
- Equation (3) is considered an update rule that corresponds to a linear dynamic system that is used for prediction of sequential data. It is a first-order linear system that may be even more generalized to be written as equation (4) below, where the observed outcome u of the random system at time t is treated as an input.
- a probability model may include an update algorithm that uses conditions other than those of a baseline probability model in its update rule. For example, instead of using ⁇ circumflex over (p) ⁇ (s t ), an estimate of the conditional probability over r symbols ⁇ circumflex over (p) ⁇ (s t s t ⁇ 1 . . . s t ⁇ ) may be used. In this estimation, a list may be used to apply multiple probability updates. In a possible technique, a weighted average of models may be used to create a higher-order linear system. In a possible technique, update rates may be self-adaptive as described in more detail below.
- the sequence of symbols input into the entropy coding and update algorithm may comprise a sequence s of N symbols.
- the sequence may correspond to the binarization of symbols representing any portion of a frame, such as the frame, a segment, a slice, a block, or some other portion of the frame, such as the data described with regards to FIGS. 6 - 8 .
- FIG. 9 is a flow chart of a method 900 for entropy coding a sequence of symbols according to the teachings herein.
- a sequence of symbols is received.
- the sequence is a sequence s of N binary symbols, where s ⁇ 0,1 ⁇ N, is entropy coded.
- the next step is to select a symbol at 904 .
- the current symbol may be a first symbol in the sequence.
- the current symbol is entropy coded using a probability.
- the probability may be the probability determined by a first, or baseline, probability model of a plurality of probability models.
- the probability may be an updated probability that uses a combination of estimations of the probability determined using respective probability models. In either event, the probability for the next symbol may be updated at 908 .
- the probability of the baseline and any other probability models may be updated, and a combination of these estimations may be used to update the probability at 908 .
- the combination is a second-order linear system different from each of the first-order linear systems represented by the models. This method 900 proceeds to check for remaining symbols at 910 and repeats until no symbols remain to be entropy coded.
- the method 900 is next described with certain examples. First described is an implementation where a fixed probability estimation is used to update the probability for entropy coding symbols of the sequence. The first example is followed by a second example that uses an adaptive probability estimation.
- the probability p is used to entropy code the current symbol
- the probability ⁇ circumflex over (p) ⁇ inf is a first probability estimate from a first probability model based on counting as described below
- the probability ⁇ circumflex over (p) ⁇ CABAC is a second probability estimation from a second probability model based on ⁇ circumflex over (p) ⁇ CABAC .
- a parameter mode is selected from the set comprising 0 and 1 (mode ⁇ 0,1 ⁇ ).
- a weight w used to combine the probability estimate of the first probability model with a conditional probability is set to a value of 0.5 in this fixed probability estimation, but a variable or adaptive weighting can be used in other examples of the teachings herein.
- a variable r and a variable t thres are set.
- the variable r is set to 5
- the variable r may be equal to a different value.
- the variable r may be set equal to 8.
- One use of the variable r is to define the size L of a list for storing probability values, which list is used to determine the conditional probability.
- the variable t thres also described below, is set equal to 25, but it could be set to a lower or higher value.
- the value ⁇ described with regards to equations (3) and (4) may depend upon the particular codec used for the encoding and decoding operations as described above.
- the value ⁇ may be a constant or may be adaptive in terms of time and number of symbols.
- a barrier value p barrier also referred to as p 62
- the index (time) t is initialized to 1 to indicate that processing starts with the first symbol s 1 in the sequence. While t remains less than or equal to the total number of symbols N, the processing receives the symbol s t , codes the symbol s t by p, which may also be described herein as ⁇ circumflex over (p) ⁇ t or ⁇ circumflex over (p) ⁇ , and then updates the probability as described below.
- the index t is updated to proceed to the next symbol s t+1 in the sequence, if any.
- the symbol s t+1 is entropy coded by the updated probability p. This process continues until all symbols in the sequence s are entropy coded (i.e., entropy encoded or entropy decoded).
- the function ProbUpdate is called after s t is entropy coded.
- the function ProbUpdate receives as input the probability p inf , the probability ⁇ circumflex over (p) ⁇ CABAC , the parameter ⁇ , the values of the symbols in the range s t ⁇ to s t , the index t of the current symbol s t , the weight w, the variable t thres , List, and the parameter mode.
- the function ProbUpdate returns the probability p, the probability p inf , the probability ⁇ circumflex over (p) ⁇ CABAC , and the entries in the List. More generally, the function ProbUpdate updates the probability p for coding the next symbol in the sequence of symbols.
- the probability estimate updates may incorporate two probability estimation models—the CABAC model previously described (and represented by ⁇ circumflex over (p) ⁇ CABAC ) as well as the maximum likelihood estimate (MLE) for an independent identical distribution (i.i.d) sequence of symbols based on counting (represented by ⁇ circumflex over (p) ⁇ inf ).
- the MLE for i.i.d sequence may be explained using a binary sequence for simplicity. Assume s 1 . . . s t is i.i.d Bernoulli (i.e., a Bernoulli distribution) where 0 happens with probability p, and there is no preference of p, i.e., the prior of p is U[0, 1]. From observation of the sequence, if 0 occurs k times and 1 occurs 1 times, the estimator that satisfies equation (5) below
- an updated probability may be determined using an estimate of conditional probability ⁇ circumflex over (p) ⁇ (s t
- the List is used with an adjustable size 2 ⁇ that stores all possible context sequences s t ⁇ 1 :s t ⁇ .
- the List functions as a hashtable for conditions to store the conditional probability.
- the baseline estimation ( ⁇ circumflex over (p) ⁇ inf or ⁇ circumflex over (p) ⁇ CABAC ) may be output as the probability p.
- the estimation may not be accurate.
- the condition has the length ⁇ (which, as described above, varies with the number of symbols).
- the history of shorter lengths ⁇ 1, ⁇ 2, etc. may be considered. This involves taking unions of counts in multiple dictionary items. Whenever the count over this union reaches the threshold t thres , this probability estimation is recorded. For example, this may result in merging 00000 and 00001 as 0000.
- the baseline estimation ⁇ circumflex over (p) ⁇ inf , or ⁇ circumflex over (p) ⁇ CABAC ) may be output as the probability p.
- a weighted average of probability update may be considered as follows.
- This second order system cannot be trivially reduced to a first order system only involving p t+1 , p t , and u t .
- the probability update described above used a fixed (e.g., a linear) combination of update algorithms for context-based probability estimation.
- a function ProbUpdate that implements the second order system described above is shown by the following pseudocode.
- the probability estimation models available as the baseline model are used to generate a respective estimated probability.
- the probability estimation models available as the baseline model are used to generate a respective estimated probability ( ⁇ circumflex over (p) ⁇ inf and ⁇ circumflex over (p) ⁇ CABAC in this example).
- t tmp which is used for collecting counts in the dictionary, is initialized to 0.
- the algorithm next counts and merges probabilities among the dictionary as described above, where i represents each possible outcome of the random symbol, and the summation calculates how many outcomes have been observed within the condition window (s t ⁇ t tmp +1:s t ). This counting and merging ends at the second “end if”.
- the baseline estimation ⁇ circumflex over (p) ⁇ CABAC or ⁇ circumflex over (p) ⁇ inf is selected given the value of mode for use as the updated probability.
- the function ProbUpdateCABAC called by the function ProbUpdate described above may be represented by the following pseudocode.
- This pseudocode represents the CABAC update described above, where p is the vector ⁇ circumflex over (p) ⁇ CABAC of the probability distribution, namely [p(1 ⁇ ), p ( ⁇ )].
- the function ProbUpdateCount called by the function ProbUpdate described above may be represented by the following pseudocode.
- This pseudocode represents the MLE calculation described above, where p is the vector ⁇ circumflex over (p) ⁇ inf , of the probability distribution for the given outcome value s t .
- update algorithms for context-based probability estimation are possible.
- additional algorithms may include a data-driven method that describes learning a linear combination, as opposed to using the fixed combination described above.
- An implementation for this entropy coding and adaptive probability estimation (as compared to fixed probability estimation) is next described.
- the different first-order linear models described above are desirably used as kernels to output a linear combination through actively learning the linear combination. While these three models are used in this example, any probability estimation algorithm may be used.
- n p as the number of kernels, ⁇ circumflex over (p) ⁇ n p ⁇ 2 where each row is a probability estimation, and w ⁇ n p is the weight/parameter of a linear combination.
- a weighted average of simple (i.e., first-order) probability estimations is used as the result for entropy coding the next symbol as follows.
- Each row of ⁇ circumflex over (p) ⁇ is updated by a probability update algorithm, and p(1, :) is fixed as an AV1 output.
- a stochastic gradient descent (SGD) is used to update w. For each s t , an entropy is incurred as follows.
- a gradient is taken with respect to w as follows.
- a constrained optimization step may be included. Solving such a step may be slow.
- a batch version the above algorithm may be used. At each epoch, a batch with increasing size 1, 4, 9, 16 . . . , may be taken, and the gradient of the batch is averaged. gradient in this batch.
- the update of w occurs only at the end of each batch, with a fixed step size ⁇ 0 . Both theoretically and empirically, the convergence rate of the SGD and batch versions are similar.
- the problem may be defined as the following equation.
- Optimality may be obtained from the Lagrangian according to the following equation.
- the Karush-Kuhn-Tucker (KKT) condition is represented by the following.
- the optimal value for x is represented by the following equation.
- the above data-driven method that describes learning a linear combination may be represented by the following pseudocode, where the input is a sequence of binary symbols as described with respect to the previous implementation.
- the first step is initialization.
- the variable a is initialized according to 0.99 ⁇ 2 ⁇ [0:n p ⁇ 2]/4(n p ⁇ 2) ⁇ n p ⁇ 2 .
- parameters or variables used for the entropy coding and probability of estimation are defined, initialized, or otherwise determined, either before, after, or concurrent with receipt of the sequence at 902 . Then, the remaining steps of the method 900 are performed according to the following pseudocode starting with receiving the first symbol s 1 and entropy coding the first symbol s 1 . Then, the probability estimations are updated using the respective models.
- the functions ProbUpdateCount and ProbUpdateCABAC have been discussed above.
- the function ProbUpdateAV1 is described below. Once the probability estimations are updated, they are combined using the selected mode.
- the function ProbUpdateAV1 may be represented by the following pseudocode. This pseudocode represents the AV1 calculation described above, where p is the vector ⁇ circumflex over (p) ⁇ (1, :) of the probability distribution for the given outcome value s t .
- NumOfSyms (the number of symbols) is 2 in this example, but it could be a higher number. Also note that a is used as an input to ProbUpdateCABAC. While it is a constant in these examples, this would allow the value to be adaptive.
- the table compares six techniques using nine different test sequences.
- the conventional CABAC and AV1 models/algorithms are baselines, against which different proposed models/algorithms are compared.
- the models for comparison are the SGD processing without SGD batch processing, the SGD batch processing, the fixed combination update algorithm for context-based probability estimation described with the parameters/variables above, and the fixed combination update algorithm for context-based probability estimation described with the parameters/variables above except that mode is set equal to 1 instead of 0.
- the proposed algorithms perform better than the baselines in most conditions.
- the differences generally relate to the parameter p 62 in CABAC. Too sparse a dataset results in worse entropy when using this parameter.
- a model can be, or can be a parameter in, lossless (entropy) coding.
- a model can be any parameter or method that affects probability estimation for the purpose of entropy coding.
- a model can define the probability to be used to encode and decode the decision at an internal node in a token tree (such as described with respect to FIG. 7 ).
- the two-pass process to learn the probabilities for a current frame may be simplified to a single-pass process by modifying a baseline model for probability estimation as described herein.
- a model may define a certain context derivation method. In such a case, implementations according to this disclosure can be used to combing probability estimations generated by a multitude of such methods.
- a model may define a completely new lossless coding algorithm.
- the probability update algorithm for entropy coding described herein may incorporate an average of different models with fast and slow update rates.
- An MLE estimator based on counting may be incorporated.
- Conditional probability and dictionary searching are options.
- the implementations also allow for adaptive fusion of models.
- encoding and decoding illustrate some examples of encoding and decoding techniques. However, it is to be understood that encoding and decoding, as those terms are used in the claims, could mean compression, decompression, transformation, or any other processing or change of data.
- example is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” is not necessarily to be construed as being preferred or advantageous over other aspects or designs. Rather, use of the word “example” is intended to present concepts in a concrete fashion.
- the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise or clearly indicated otherwise by the context, the statement “X includes A or B” is intended to mean any of the natural inclusive permutations thereof. That is, if X includes A; X includes B; or X includes both A and B, then “X includes A or B” is satisfied under any of the foregoing instances.
- Implementations of the transmitting station 102 and/or the receiving station 106 can be realized in hardware, software, or any combination thereof.
- the hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit.
- IP intellectual property
- ASICs application-specific integrated circuits
- programmable logic arrays optical processors
- programmable logic controllers programmable logic controllers
- microcode microcontrollers
- servers microprocessors, digital signal processors, or any other suitable circuit.
- signal processors should be understood as encompassing any of the foregoing hardware, either singly or in combination.
- signals and “data” are used interchangeably. Further, portions of the transmitting station 102 and the receiving station 106 do not necessarily have to be implemented in the same manner.
- the transmitting station 102 or the receiving station 106 can be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein.
- a special purpose computer/processor can be utilized which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein.
- the transmitting station 102 and the receiving station 106 can, for example, be implemented on computers in a video conferencing system.
- the transmitting station 102 can be implemented on a server, and the receiving station 106 can be implemented on a device separate from the server, such as a handheld communications device.
- the transmitting station 102 using an encoder 400 , can encode content into an encoded video signal and transmit the encoded video signal to the communications device.
- the communications device can then decode the encoded video signal using a decoder 500 .
- the communications device can decode content stored locally on the communications device, for example, content that was not transmitted by the transmitting station 102 .
- Other suitable transmitting and receiving implementation schemes are available.
- the receiving station 106 can be a generally stationary personal computer rather than a portable communications device, and/or a device including an encoder 400 may also include a decoder 500 .
- implementations of this disclosure can take the form of a computer program product accessible from, for example, a computer-usable or computer-readable medium.
- a computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor.
- the medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.
Abstract
Entropy coding a sequence of symbols is described. A first probability model for entropy coding is selected. At least one symbol of the sequence is coded using a probability determined using the first probability model. The probability according to the first probability model is updated with an estimation of a second probability model to entropy code a subsequent symbol. The combination may be a fixed or adaptive combination.
Description
- This application claims priority to U.S. Provisional Patent Application No. 62/932,508, filed Nov. 8, 2019, the entire content of which is incorporated wherein in its entirety by reference.
- Digital video streams may represent video using a sequence of frames or still images. Digital video can be used for various applications including, for example, video conferencing, high-definition video entertainment, video advertisements, or sharing of user-generated videos. A digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data. Various approaches have been proposed to reduce the amount of data in video streams, including lossy and lossless compression techniques.
- Probability estimation is used for entropy coding, particularly with context-based entropy coding for lossless compression. A multimodal approach is described herein that uses multiple linear update models to accurately estimate probabilities.
- An aspect of the teachings herein is a method for entropy coding a sequence of symbols (i.e., multiple symbols). The method can include determining a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models, entropy coding at least one symbol of the sequence using a probability determined by the first probability model, after entropy coding a respective symbol of the sequence, determining a first probability estimation to update the probability using the first probability model, for a subsequent symbol relative to the at least one symbol of the sequence, determining a second probability estimation using a second probability model, and entropy coding the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.
- An aspect of the teachings herein is an apparatus for entropy coding a sequence of symbols including a processor. The processor is configured to determine a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models, entropy code at least one symbol of the sequence using a probability determined by the first probability model, after entropy coding a respective symbol of the sequence, determine a first probability estimation to update the probability using the first probability model, for a subsequent symbol relative to the at least one symbol of the sequence, determine a second probability estimation using a second probability model, and entropy code the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.
- Aspects of this disclosure are disclosed in the following detailed description of the implementations, the appended claims, and the accompanying figures.
- The description herein makes reference to the accompanying drawings described below, wherein like reference numerals refer to like parts throughout the several views.
-
FIG. 1 is a schematic of an example of a video encoding and decoding system. -
FIG. 2 is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station. -
FIG. 3 is a diagram of an example of a video stream to be encoded and subsequently decoded. -
FIG. 4 is a block diagram of an example of an encoder. -
FIG. 5 is a block diagram of an example of a decoder. -
FIG. 6 is a diagram illustrating quantized transform coefficients according to implementations of this disclosure. -
FIG. 7 is a diagram of a coefficient token tree that can be used to entropy code blocks into a video bitstream according to implementations of this disclosure. -
FIG. 8 is a diagram of an example of a tree for binarizing a quantized transform coefficient according to implementations of this disclosure. -
FIG. 9 is a flow chart of a method for entropy coding a sequence of symbols according to the teachings herein. - Video compression schemes may include breaking respective images, or frames, into smaller portions, such as blocks, and generating an encoded bitstream using techniques to limit the information included for respective blocks thereof. The encoded bitstream can be decoded to re-create or reconstruct the source images from the limited information. The information may be limited by lossy coding, lossless coding, or some combination of lossy and lossless coding.
- One type of lossless coding is entropy coding, where entropy is generally considered the degree of disorder or randomness in a system. Entropy coding compresses a sequence in an informationally efficient way. That is, a lower bound of the length of the compressed sequence is the entropy of the original sequence. An efficient algorithm for entropy coding desirably generates a code (e.g., in bits) whose length approaches the entropy. For a sequence s with a length N, the entropy associated with binary codewords may be defined as equation (1), below:
-
Σt=1 N−log2(p(s t |s t−1, . . . ,1)):=Σt=1 N−log2(p t(s t)) (1) - The variable p represents the probability of an individual symbol, and the variable pt represents the probability distribution of symbols at time t conditioned on the previously observed symbols. Arithmetic coding can use the probability to construct the codewords.
- However, a coder does not receive a streaming sequence of symbols together with the probability distribution for the symbols. Instead, probability estimation may be used in video codecs to implement entropy coding. That is, the probability distribution of the symbols may be estimated. Where the estimation is {circumflex over (p)}t, the codelength approaches equation (2) below:
-
Σt=1 N−log2({circumflex over (p)} t(s t)) (2) - Stated differently, entropy coding may rely upon probability estimation models (also called probability models herein) that model the distribution of values occurring in an encoded bitstream. By using probability models based on a measured or estimated distribution of values so that {circumflex over (p)}t is close to pt, entropy coding can reduce the number of bits required to represent the input data to close to a theoretical minimum (i.e., the lower bound).
- In practice, the actual reduction in the number of bits required to represent video data can be a function of the accuracy of the probability model, the number of bits over which the coding is performed, and the computational accuracy of the (e.g., fixed-point) arithmetic used to perform the coding. A significant difficulty in the estimation is that the probability is time variant, which means that pt cannot be replaced by a single value p.
- To address the time-variant nature of the probability, probability estimation is described herein that combines a probability estimation model, which is a first-order linear system, with another model to form a higher-order linear system. While the teachings herein may be used in either a one-pass or a two-pass coding system, the estimation of probability herein may be referred to as online estimation of probability because it is capable of use in a one-pass system with high efficiency. The available probability estimation models may be two or more models including a context-adaptive binary arithmetic coding (CABAC) model, an AV1 model, a counting model, or any other probability estimation model or algorithm.
- Implementations according to this disclosure can efficiently perform probability estimation for entropy coding, particularly with context-based entropy coding for lossless compression, by more accurately modeling the conditional probability of streaming symbols. The probability estimation contributes to efficient compression, reducing the number of bits required to represent video data. The probability estimation may be used in any probability estimation of a sequence of symbols but may be particularly effective for online probability estimation of such a sequence (e.g., real-time or delay sensitive applications of video coding).
- Further details of estimating the probability for entropy coding symbols are described herein first with reference to a system in which the teachings may be incorporated.
-
FIG. 1 is a schematic of an example of a video encoding anddecoding system 100. A transmittingstation 102 can be, for example, a computer having an internal configuration of hardware such as that described inFIG. 2 . However, other implementations of the transmittingstation 102 are possible. For example, the processing of the transmittingstation 102 can be distributed among multiple devices. - A
network 104 can connect the transmittingstation 102 and areceiving station 106 for encoding and decoding of the video stream. Specifically, the video stream can be encoded in thetransmitting station 102, and the encoded video stream can be decoded in thereceiving station 106. Thenetwork 104 can be, for example, the Internet. Thenetwork 104 can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmittingstation 102 to, in this example, thereceiving station 106. - The
receiving station 106, in one example, can be a computer having an internal configuration of hardware such as that described inFIG. 2 . However, other suitable implementations of thereceiving station 106 are possible. For example, the processing of thereceiving station 106 can be distributed among multiple devices. - Other implementations of the video encoding and
decoding system 100 are possible. For example, an implementation can omit thenetwork 104. In another implementation, a video stream can be encoded and then stored for later transmission to the receivingstation 106 or any other device having memory. In one implementation, the receivingstation 106 receives (e.g., via thenetwork 104, a computer bus, and/or some communication pathway) the encoded video stream and stores the video stream for later decoding. In an example implementation, a real-time transport protocol (RTP) is used for transmission of the encoded video over thenetwork 104. In another implementation, a transport protocol other than RTP may be used, e.g., a Hypertext Transfer Protocol-based (HTTP-based) video streaming protocol. - When used in a video conferencing system, for example, the transmitting
station 102 and/or the receivingstation 106 may include the ability to both encode and decode a video stream as described below. For example, the receivingstation 106 could be a video conference participant who receives an encoded video bitstream from a video conference server (e.g., the transmitting station 102) to decode and view and further encodes and transmits his or her own video bitstream to the video conference server for decoding and viewing by other participants. - In some implementations, the video encoding and
decoding system 100 may instead be used to encode and decode data other than video data. For example, the video encoding anddecoding system 100 can be used to process image data. The image data may include a block of data from an image. In such an implementation, the transmittingstation 102 may be used to encode the image data and the receivingstation 106 may be used to decode the image data. Alternatively, the receivingstation 106 can represent a computing device that stores the encoded image data for later use, such as after receiving the encoded or pre-encoded image data from the transmittingstation 102. As a further alternative, the transmittingstation 102 can represent a computing device that decodes the image data, such as prior to transmitting the decoded image data to the receivingstation 106 for display. -
FIG. 2 is a block diagram of an example of acomputing device 200 that can implement a transmitting station or a receiving station. For example, thecomputing device 200 can implement one or both of the transmittingstation 102 and the receivingstation 106 ofFIG. 1 . Thecomputing device 200 can be in the form of a computing system including multiple computing devices, or in the form of one computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like. - A
processor 202 in thecomputing device 200 can be a conventional central processing unit. Alternatively, theprocessor 202 can be another type of device, or multiple devices, capable of manipulating or processing information now existing or hereafter developed. For example, although the disclosed implementations can be practiced with one processor as shown (e.g., the processor 202), advantages in speed and efficiency can be achieved by using more than one processor. - A
memory 204 incomputing device 200 can be a read only memory (ROM) device or a random-access memory (RAM) device in an implementation. However, other suitable types of storage device can be used as thememory 204. Thememory 204 can include code anddata 206 that is accessed by theprocessor 202 using abus 212. Thememory 204 can further include anoperating system 208 andapplication programs 210, theapplication programs 210 including at least one program that permits theprocessor 202 to perform the techniques described herein. For example, theapplication programs 210 can includeapplications 1 through N, which further include a video coding application that performs the techniques described herein. Thecomputing device 200 can also include asecondary storage 214, which can, for example, be a memory card used with a mobile computing device. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in thesecondary storage 214 and loaded into thememory 204 as needed for processing. - The
computing device 200 can also include one or more output devices, such as adisplay 218. Thedisplay 218 may be, in one example, a touch sensitive display that combines a display with a touch sensitive element that is operable to sense touch inputs. Thedisplay 218 can be coupled to theprocessor 202 via thebus 212. Other output devices that permit a user to program or otherwise use thecomputing device 200 can be provided in addition to or as an alternative to thedisplay 218. When the output device is or includes a display, the display can be implemented in various ways, including by a liquid crystal display (LCD), a cathode-ray tube (CRT) display, or a light emitting diode (LED) display, such as an organic LED (OLED) display. - The
computing device 200 can also include or be in communication with an image-sensingdevice 220, for example, a camera, or any other image-sensingdevice 220 now existing or hereafter developed that can sense an image such as the image of a user operating thecomputing device 200. The image-sensingdevice 220 can be positioned such that it is directed toward the user operating thecomputing device 200. In an example, the position and optical axis of the image-sensingdevice 220 can be configured such that the field of vision includes an area that is directly adjacent to thedisplay 218 and from which thedisplay 218 is visible. - The
computing device 200 can also include or be in communication with a sound-sensing device 222, for example, a microphone, or any other sound-sensing device now existing or hereafter developed that can sense sounds near thecomputing device 200. The sound-sensing device 222 can be positioned such that it is directed toward the user operating thecomputing device 200 and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates thecomputing device 200. - Although
FIG. 2 depicts theprocessor 202 and thememory 204 of thecomputing device 200 as being integrated into a single unit, other configurations can be utilized. The operations of theprocessor 202 can be distributed across multiple machines (wherein individual machines can have one or more processors) that can be coupled directly or across a local area or other network. Thememory 204 can be distributed across multiple machines such as a network-based memory or memory in multiple machines performing the operations of thecomputing device 200. Although depicted here as one bus, thebus 212 of thecomputing device 200 can be composed of multiple buses. Further, thesecondary storage 214 can be directly coupled to the other components of thecomputing device 200 or can be accessed via a network and can comprise an integrated unit such as a memory card or multiple units such as multiple memory cards. Thecomputing device 200 can thus be implemented in a wide variety of configurations. -
FIG. 3 is a diagram of an example of avideo stream 300 to be encoded and subsequently decoded. Thevideo stream 300 includes avideo sequence 302. At the next level, thevideo sequence 302 includes severaladjacent frames 304. While three frames are depicted as theadjacent frames 304, thevideo sequence 302 can include any number ofadjacent frames 304. Theadjacent frames 304 can then be further subdivided into individual frames, for example, aframe 306. At the next level, theframe 306 can be divided into a series of planes orsegments 308. Thesegments 308 can be subsets of frames that permit parallel processing, for example. Thesegments 308 can also be subsets of frames that can separate the video data into separate colors. For example, aframe 306 of color video data can include a luminance plane and two chrominance planes. Thesegments 308 may be sampled at different resolutions. - Whether or not the
frame 306 is divided intosegments 308, theframe 306 may be further subdivided intoblocks 310, which can contain data corresponding to, for example, 16×16 pixels in theframe 306. Theblocks 310 can also be arranged to include data from one ormore segments 308 of pixel data. Theblocks 310 can also be of any other suitable size such as 4×4 pixels, 8×8 pixels, 16×8 pixels, 8×16 pixels, 16×16 pixels, or larger. Unless otherwise noted, the terms block and macroblock are used interchangeably herein. -
FIG. 4 is a block diagram of an example of anencoder 400. Theencoder 400 can be implemented, as described above, in the transmittingstation 102, such as by providing a computer software program stored in memory, for example, thememory 204. The computer software program can include machine instructions that, when executed by a processor such as theprocessor 202, cause the transmittingstation 102 to encode video data in the manner described inFIG. 4 . Theencoder 400 can also be implemented as specialized hardware included in, for example, the transmittingstation 102. In one particularly desirable implementation, theencoder 400 is a hardware encoder. - The
encoder 400 has the following stages to perform the various functions in a forward path (shown by the solid connection lines) to produce an encoded orcompressed bitstream 420 using thevideo stream 300 as input: an intra/inter prediction stage 402, atransform stage 404, aquantization stage 406, and anentropy encoding stage 408. Theencoder 400 may also include a reconstruction path (shown by the dotted connection lines) to reconstruct a frame for encoding of future blocks. InFIG. 4 , theencoder 400 has the following stages to perform the various functions in the reconstruction path: a dequantization stage 410, aninverse transform stage 412, areconstruction stage 414, and aloop filtering stage 416. Other structural variations of theencoder 400 can be used to encode thevideo stream 300. - When the
video stream 300 is presented for encoding, respectiveadjacent frames 304, such as theframe 306, can be processed in units of blocks. At the intra/inter prediction stage 402, respective blocks can be encoded using intra-frame prediction (also called intra-prediction) or inter-frame prediction (also called inter-prediction). In any case, a prediction block can be formed. In the case of intra-prediction, a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed. In the case of inter-prediction, a prediction block may be formed from samples in one or more previously constructed reference frames. - Next, the prediction block can be subtracted from the current block at the intra/
inter prediction stage 402 to produce a residual block (also called a residual). Thetransform stage 404 transforms the residual into transform coefficients in, for example, the frequency domain using block-based transforms. Thequantization stage 406 converts the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients, using a quantizer value or a quantization level. For example, the transform coefficients may be divided by the quantizer value and truncated. - The quantized transform coefficients are then entropy encoded by the
entropy encoding stage 408. The entropy-encoded coefficients, together with other information used to decode the block (which may include, for example, syntax elements such as used to indicate the type of prediction used, transform type, motion vectors, a quantizer value, or the like), are then output to thecompressed bitstream 420. Thecompressed bitstream 420 can be formatted using various techniques, such as variable length coding (VLC) or arithmetic coding. Thecompressed bitstream 420 can also be referred to as an encoded video stream or encoded video bitstream, and the terms will be used interchangeably herein. - The reconstruction path (shown by the dotted connection lines) can be used to ensure that the
encoder 400 and a decoder 500 (described below with respect toFIG. 5 ) use the same reference frames to decode thecompressed bitstream 420. The reconstruction path performs functions that are similar to functions that take place during the decoding process (described below with respect toFIG. 5 ), including dequantizing the quantized transform coefficients at the dequantization stage 410 and inverse transforming the dequantized transform coefficients at theinverse transform stage 412 to produce a derivative residual block (also called a derivative residual). At thereconstruction stage 414, the prediction block that was predicted at the intra/inter prediction stage 402 can be added to the derivative residual to create a reconstructed block. Theloop filtering stage 416 can be applied to the reconstructed block to reduce distortion such as blocking artifacts. - Other variations of the
encoder 400 can be used to encode thecompressed bitstream 420. In some implementations, a non-transform-based encoder can quantize the residual signal directly without thetransform stage 404 for certain blocks or frames. In some implementations, an encoder can have thequantization stage 406 and the dequantization stage 410 combined in a common stage. -
FIG. 5 is a block diagram of an example of adecoder 500. Thedecoder 500 can be implemented in the receivingstation 106, for example, by providing a computer software program stored in thememory 204. The computer software program can include machine instructions that, when executed by a processor such as theprocessor 202, cause the receivingstation 106 to decode video data in the manner described inFIG. 5 . Thedecoder 500 can also be implemented in hardware included in, for example, the transmittingstation 102 or the receivingstation 106. - The
decoder 500, like the reconstruction path of theencoder 400 discussed above, includes in one example the following stages to perform various functions to produce anoutput video stream 516 from the compressed bitstream 420: anentropy decoding stage 502, adequantization stage 504, aninverse transform stage 506, an intra/inter prediction stage 508, areconstruction stage 510, aloop filtering stage 512, and adeblocking filtering stage 514. Other structural variations of thedecoder 500 can be used to decode thecompressed bitstream 420. - When the
compressed bitstream 420 is presented for decoding, the data elements within thecompressed bitstream 420 can be decoded by theentropy decoding stage 502 to produce a set of quantized transform coefficients. Thedequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and theinverse transform stage 506 inverse transforms the dequantized transform coefficients to produce a derivative residual that can be identical to that created by theinverse transform stage 412 in theencoder 400. Using header information decoded from thecompressed bitstream 420, thedecoder 500 can use the intra/inter prediction stage 508 to create the same prediction block as was created in the encoder 400 (e.g., at the intra/inter prediction stage 402). - At the
reconstruction stage 510, the prediction block can be added to the derivative residual to create a reconstructed block. Theloop filtering stage 512 can be applied to the reconstructed block to reduce blocking artifacts. Other filtering can be applied to the reconstructed block. In this example, thedeblocking filtering stage 514 is applied to the reconstructed block to reduce blocking distortion, and the result is output as theoutput video stream 516. Theoutput video stream 516 can also be referred to as a decoded video stream, and the terms will be used interchangeably herein. Other variations of thedecoder 500 can be used to decode thecompressed bitstream 420. In some implementations, thedecoder 500 can produce theoutput video stream 516 without thedeblocking filtering stage 514. - As can be discerned from the description of the
encoder 400 and the decoder above, bits are generally used for one of two things in an encoded video bitstream: either content prediction (e.g., inter mode/motion vector coding, intra prediction mode coding, etc.) or residual or coefficient coding (e.g., transform coefficients). Encoders may use techniques to decrease the bits spent on coefficient coding. For example, a coefficient token tree (which may also be referred to as a binary token tree) specifies the scope of the value, with forward-adaptive probabilities for each branch in this token tree. The token base value is subtracted from the value to be coded to form a residual then the block is coded with fixed probabilities. A similar scheme with minor variations including backward-adaptivity is also possible. Adaptive techniques can alter the probability models as the video stream is being encoded to adapt to changing characteristics of the data. In any event, a decoder is informed of (or has available) the probability model used to encode an entropy-coded video bitstream in order to decode the video bitstream. - Before describing updating of the probability estimation for a sequence of symbols, the development of the sequence of symbols is described starting with
FIG. 6 . -
FIG. 6 is a diagram 600 illustrating quantized transform coefficients according to implementations of this disclosure. The diagram 600 depicts acurrent block 620, ascan order 602, a quantizedtransform block 604, anon-zero map 606, an end-of-block map 622, and asign map 626. Thecurrent block 620 is illustrated as a 4×4 block. However, any block size is possible. For example, the current block can have a size (i.e., dimensions) of 4×4, 8×8, 16×16, 32×32, or any other square or rectangular block size. Thecurrent block 620 can be a block of a current frame. In another example, the current frame may be partitioned into segments (such as thesegments 308 ofFIG. 3 ), tiles, or the like, each including a collection of blocks, where the current block is a block of the partition. - The quantized
transform block 604 can be a block of a size similar to the size of thecurrent block 620. The quantizedtransform block 604 includes non-zero coefficients (e.g., a coefficient 608) and zero coefficients (e.g., a coefficient 610). As described above, the quantizedtransform block 604 contains quantized transform coefficients for the residual block corresponding to thecurrent block 620. Also as described above, the quantized transform coefficients are entropy coded by an entropy-coding phase, such as theentropy coding stage 408 ofFIG. 4 . - Entropy coding a quantized transform coefficient can involve the selection of a context model (also referred to as probability context model, probability model, model, and context) which provides estimates of conditional probabilities for coding the binary symbols of a binarized transform coefficient as described below with respect to
FIG. 7 . When entropy coding a quantized transform coefficient, additional information may be used as the context for selecting a context model. For example, the magnitudes of the previously coded transform coefficients can be used, at least partially, for determining a probability model. - To encode a transform block, a video coding system may traverse the transform block in a scan order and encode (e.g., entropy encode) the quantized transform coefficients as the quantized transform coefficients are respectively traversed (i.e., visited). In a zigzag scan order, such as the
scan order 602, the top left corner of the transform block (also known as the DC coefficient) is first traversed and encoded, the next coefficient in the scan order (i.e., the transform coefficient corresponding to the location labeled “1”) is traversed and encoded, and so on. In the zigzag scan order (i.e., scan order 602), some quantized transform coefficients above and to the left of a current quantized transform coefficient (e.g., a to-be-encoded transform coefficient) are traversed first. Other scan orders are possible. A one-dimensional structure (e.g., an array) of quantized transform coefficients can result from the traversal of the two-dimensional quantized transform block using the scan order. - In some examples, encoding the quantized
transform block 604 can include determining thenon-zero map 606, which indicates which quantized transform coefficients of the quantizedtransform block 604 are zero and which are non-zero. A non-zero coefficient and a zero coefficient can be indicated with values one (1) and zero (0), respectively, in the non-zero map. For example, thenon-zero map 606 includes a non-zero 607 at Cartesian location (0, 0) corresponding to thecoefficient 608 and a zero 608 at Cartesian location (2, 0) corresponding to thecoefficient 610. - In some examples, encoding the quantized
transform block 604 can include generating and encoding the end-of-block map 622. The end-of-block map indicates whether a non-zero quantized transform coefficient of the quantizedtransform block 604 is the last non-zero coefficient with respect to a given scan order. If a non-zero coefficient is not the last non-zero coefficient in the transform block, then it can be indicated with the binary bit zero (0) in the end-of-block map. If, on the other hand, a non-zero coefficient is the last non-zero coefficient in the transform block, then it can be indicated with the binary value one (1) in the end-of-block map. For example, as the quantized transform coefficient corresponding to the scan location 11 (i.e., the last non-zero quantized transform coefficient 628) is the last non-zero coefficient of the quantizedtransform block 604, it is indicated with the end-of-block value 624 of one (1); all other non-zero transform coefficients are indicated with a zero. - In some examples, encoding the quantized
transform block 604 can include generating and encoding thesign map 626. Thesign map 626 indicates which non-zero quantized transform coefficients of the quantizedtransform block 604 have positive values and which quantized transform coefficients have negative values. Transform coefficients that are zero need not be indicated in the sign map. Thesign map 626 illustrates the sign map for the quantizedtransform block 604. In the sign map, negative quantized transform coefficients can be indicated with a −1 and positive quantized transform coefficients can be indicated with a one (1). -
FIG. 7 is a diagram of a coefficienttoken tree 700 that can be used to entropy code blocks into a video bitstream according to implementations of this disclosure. The coefficienttoken tree 700 is referred to as a binary tree because, at each node of the tree, one of two branches must be taken (i.e., traversed). The coefficienttoken tree 700 includes aroot node 701 and anode 703 corresponding, respectively, to the nodes labeled A and B. - As described above with respect to
FIG. 6 , when an end-of-block (EOB) token is detected for a block, coding of coefficients in the current block can terminate and the remaining coefficients in the block can be inferred to be zero. As such, the coding of EOB positions can be an essential part of coefficient in a video coding system. - In some video coding systems, a binary decision determining whether (or not) a current token is equal to the EOB token of the current block is coded immediately after an nonzero coefficient is decoded or at the first scan position (DC). In an example, for a transform block of size M×N, where M denotes the number of columns and N denotes the number of rows in the transform block, the maximum number of times of coding whether a current token is equal to the EOB token is equal to M×N. M and N can take values, such as the
values root node 701 to thenode 703 in the coefficienttoken tree 700. Herein, “coding a bit” can mean the outputting or generating of a bit in the codeword representing a transform coefficient being encoded. Similarly, “decoding a bit” can mean the reading (such as from an encoded bitstream) of a bit of the codeword corresponding to a quantized transform coefficient being decoded such that the bit corresponds to a branch being traversed in the coefficient token tree. - Using the coefficient
token tree 700, a string of binary digits is generated for a quantized coefficient (e.g., thecoefficients FIG. 6 ) of the quantized transform block (such as the quantizedtransform block 604 ofFIG. 6 ). - In an example, the quantized coefficients in an N×N block (e.g., quantized transform block 604) are organized into a 1D (one-dimensional) array (herein, an array u) following a prescribed scan order (e.g., the
scan order 602 ofFIG. 6 ). N can be 4, 8, 16, 32, or any other value. The quantized coefficient at the ith position of the 1D array can be referred as u[i], where i=0, . . . , N*N− 1. The starting position of the last run of zeroes in u[i], . . . , u[N*N−1] can be denoted as eob. In the case where when u[N*N−1] is not zero, the eob can be set to the value N*N. That is, if the last coefficient of the 1D array u is not zero, then eob can be set to the value N*N. Using the examples ofFIG. 6 , the 1D array u can have the entries u[ ]=[−6, 0, −1, 0, 2, 4, 1, 0, 0, 1, 0, −1, 0, 0, 0, 0]. The values at each of the u[i]s is a quantized transform coefficient. The quantized transform coefficients of the 1D array u may also be referred herein simply as “coefficients” or “transform coefficients.” The coefficient at position i=0 (i.e., u[0]=—6) corresponds to the DC coefficient. In this example, the eob is equal to 12 because there are no non-zero coefficients after the zero coefficient atposition 12 of the 1D array u. - To encode and decode the coefficients u[i], . . . , u[N*N−1], for i=0 to N*N−1, a token t[i] is generated at each position i<=eob. The token t[i], for i<eob, can be indicative of the size and/or size range of the corresponding quantized transform coefficient at u[i]. The token for the quantized transform coefficient at eob can be an EOB_TOKEN, which is a token that indicates that the 1D array u contains no non-zero coefficients following the eob position (inclusive). That is, t[eob]=EOB_TOKEN indicates the EOB position of the current block. Table I below provides a listing of an example of token values, excluding the EOB_TOKEN, and their corresponding names according to an implementation of this disclosure.
-
TABLE I Token Name of Token 0 ZERO_TOKEN 1 ONE_TOKEN 2 TWO_TOKEN 3 THREE_TOKEN 4 FOUR_TOKEN 5 DCT_VAL_CAT1 (5, 6) 6 DCT_VAL_CAT2 (7-10) 7 DCT_VAL_CAT3 (11-18) 8 DCT_VAL_CAT4 (19-34) 9 DCT_VAL_CAT5 (35-66) 10 DCT_VAL_CAT6 (67-2048) - In an example, quantized coefficient values are taken to be signed 12-bit integers. To represent a quantized coefficient value, the range of 12-bit signed values can be divided into 11 tokens (the tokens 0-10 in Table I) plus the end of block token (EOB_TOKEN). To generate a token to represent a quantized coefficient value, the coefficient
token tree 700 can be traversed. The result (i.e., the bit string) of traversing the tree can then be encoded into a bitstream (such as thebitstream 420 ofFIG. 4 ) by an encoder as described with respect to theentropy encoding stage 408 ofFIG. 4 . - The coefficient
token tree 700 includes the tokens EOB_TOKEN (token 702), ZERO_TOKEN (token 704), ONE_TOKEN (token 706), TWO_TOKEN (token 708), THREE_TOKEN (token 710), FOUR_TOKEN (token 712), CAT1 (token 714 that is DCT_VAL_CAT1 in Table I), CAT2 (token 716 that is DCT_VAL_CAT2 in Table I), CAT3 (token 718 that is DCT_VAL_CAT3 in Table I), CAT4 (token 720 that is DCT_VAL_CAT4 in Table I), CAT5 (token 722 that is DCT_VAL_CAT5 in Table I) and CAT6 (token 724 that is DCT_VAL_CAT6 in Table I). As can be seen, the coefficient token tree maps a single quantized coefficient value into a single token, such as one of thetokens tokens FIG. 7 . - The base value for a token is defined as the smallest number in its range. For example, the base value for the token 720 is 19. Entropy coding identifies a token for each quantized coefficient and, if the token represents a range, can form a residual by subtracting the base value from the quantized coefficient. For example, a quantized transform coefficient with a value of 20 can be represented by including the token 720 and a residual value of 1 (i.e., 20 minus 19) in the encoded video bitstream to permit a decoder to reconstruct the original quantized transform coefficient. The end of block token (i.e., the token 702) signals that no further non-zero quantized coefficients remain in the transformed block data.
- To encode or decode a token t[i] by using a binary arithmetic coding engine (such as by the
entropy encoding stage 408 ofFIG. 4 ), the coefficienttoken tree 700 can be used. The coefficienttoken tree 700 is traversed starting at the root node 701 (i.e., the node labeled A). Traversing the coefficient token tree generates a bit string (a codeword) that will be encoded into the bitstream using, for example, binary arithmetic coding. The bit string is a representation of the current coefficient (i.e., the quantized transform coefficient being encoded). - If a current coefficient is zero, and there are no more non-zero values for the remaining transform coefficients, the token 702 (i.e., the EOB_TOKEN) is added into the bitstream. This is the case, for example, for the transform coefficient at
scan order position 12 ofFIG. 6 . On the other hand, if the current coefficient is non-zero, or if there are non-zero values among any remaining coefficients of the current block, a “1” bit is added to the codeword and traversal passes to the node 703 (i.e., the node labeled B). At node B, the current coefficient is tested to see if it is equal to zero. If so, the left-hand branch is taken such that token 704 representing the value ZERO_TOKEN and a bit “0” is added to the codeword. If not, a bit “1” is added to the codeword and traversal passes to node C. At node C, the current coefficient is tested to see if it is greater than 1. If the current coefficient is equal to one (1), the left-hand branch is taken and token 706 representing the value ONE_TOKEN is added to the bitstream (i.e., a “0” bit is added to the codeword). If the current coefficient is greater than one (1), traversal passes to node D to check the value of the current coefficient as compared to the value 4. If the current coefficient is less than or equal to 4, traversal passes to node E and a “0” bit is added to the codeword. At node E, a test for equality to the value “2” may be made. If true, token 706 representing the value “2” is added to the bitstream (i.e., a bit “0” is added to the codeword). Otherwise, at node F, the current coefficient is tested against either the value “3” or the value “4” and either token 710 (i.e., bit “0” is added to the codeword) or token 712 (i.e., bit “1” is added to the codeword) to the bitstream as appropriate; and so on. - In brief, a “0” bit is added to the codeword upon traversal to a left child node and a “1” bit is added to the codeword upon traversal to a right child node. A similar process is undertaken by a decoder when decoding a codeword from a compressed bitstream. The decoder reads a bit from bit stream. If the bit is a “1,” the coefficient token tree is traversed to the right and if the bit is a “0,” the tree is traversed to the left. The decoder reads then a next bit and repeats the process until traversal of the tree reaches a leaf node (i.e., a token). As an example, to encode a token t[i]=THREE_TOKEN, starting from the root node (i.e., the root node 701), a binary string of 111010 is encoded. As another example, decoding the codeword 11100 results in the token TWO_TOKEN.
- Note that the correspondence between “0” and “1” bits to left and right child nodes is merely a convention used to describe the encoding and decoding processes. In some implementations, a different convention, for example, in one where “1” corresponds to the left child node, and “0” corresponds to the right child node, can be used. As long as both the encoder and the decoder adopt the same convention, the processes described herein can be applied.
- Since an EOB_TOKEN is only possible after a nonzero coefficient, when u[i−1] is zero (that is, when the quantized transform coefficient at location i−1 of the 1D array u is equal to zero), a decoder can infer that the first bit must be 1. The first bit has to be 1 since, in traversing the tree, for a transform coefficient (e.g., transform coefficient at the zigzag
scan order location 2 ofFIG. 6 ) following a zero transform coefficient (e.g., transform coefficient at the zigzagscan order location 1 ofFIG. 6 ), the traversal necessarily moves from theroot node 701 to thenode 703. - As such, a binary flag checkEob can be used to instruct the encoder and the decoder to skip encoding and decoding the first bit leading from the root node in the coefficient
token tree 700. In effect, when the binary flag checkEob is 0 (i.e., indicating that the root node should not be checked), theroot node 701 of the coefficienttoken tree 700 is skipped and thenode 703 becomes the first node of coefficienttoken tree 700 to be visited for traversal. That is, when theroot node 701 is skipped, the encoder can skip encoding and the decoder can skip decoding and can infer a first bit (i.e., a binary bit “1”) of the encoded string. - At the start of encoding or decoding a block, the binary flag checkEob can be initialized to 1 (i.e., indicating that the root node should be checked). The following steps illustrate an example process for decoding quantized transform coefficients in an N×N block.
- At
step 1, the binary flag checkEob is set to zero (i.e., checkEob=0) and an index i is also set to zero (i.e., i=0). - At
step 2, a token t[i] is decoded by using either (1) the full coefficient token tree (i.e., starting at theroot node 701 of the coefficient token tree 700) if the binary flag checkEob is equal to 1; or (2) using the partial tree (e.g., starting at the node 703) where the EOB_TOKEN is skipped, if checkEob is equal to 0. - At
step 3, If the token t[i]=EOB_TOKEN, then the quantized transform coefficients u[i], . . . , u[N*N— 1] are all to zero and the decoding process terminates; otherwise, extra bits can be decoded if necessary (i.e., when t[i] is not equal to the ZERO_TOKEN) and reconstruct u[i]. - At step 4, the binary flag checkEob is set to 1 if u[i] is equal to zero, otherwise checkEob is set to 0. That is, checkEob can be set to the value (u[i]!=0).
- At
step 5, the index i is incremented (i.e., i=i+1). - At
step 6, the steps 2-5 are repeated until all quantized transform coefficients have been decoded (i.e., until the index i=N*N) or until the EOB_TOKEN is decoded. - At
step 2 above, decoding a token t[i] can include the steps of determining a context ctx, determining a binary probability distribution (i.e., a model) from the context ctx, and using a boolean arithmetic code to decode a path from the root node of the coefficienttoken tree 700 to a leaf node by using the determined probability distributions. The context ctx can be determined using a method of context derivation. The method of context derivation can use one or more of the block size, plane type (i.e., luminance or chrominance), the position i, and previously decoded tokens t [0], . . . , t[i−1] to determine the context ctx. Other criteria can be used to determine the context ctx. The binary probability distribution can be determined for any internal node of the coefficienttoken tree 700 starting from theroot node 701 when checkEOB=1 or from thenode 703 when checkEOB=0. - In some coding systems, the probability used to encode or decode a token t[i] given a context ctx may be fixed and does not adapt in a picture (i.e., a frame). For example, the probability may be either a default value that is defined for the given context ctx or the probability may be coded (i.e., signaled) as part of the frame header for that frame. Coding the probability for every context in coding a frame can be costly. As such, an encoder may analyze, for each context, whether it is beneficial to code the context's associated probability in the frame header and signal its decision to the decoder by using a binary flag. Furthermore, coding the probability for a context may use prediction to reduce cost (e.g., in bit rate) where the prediction may be derived from the probability of the same context in a previously decoded frame.
- In some coding systems, instead of traversing a coefficient token tree, such as the coefficient
token tree 700, to code a transform coefficient, each token can be associated with a value that is coded. As such, instead of coding binary symbols (i.e., selected from an alphabet comprised of the symbols {0, 1}), an alphabet of symbols that includes more than two symbols is used for coding transform coefficients. In an example, the alphabet includes 12 symbols, namely {EOB_TOKEN, ZERO_TOKEN, ONE_TOKEN, TWO_TOKEN, THREE_TOKEN, FOUR_TOKEN, DCT_VAL_CAT1, DCT_VAL_CAT2, DCT_VAL_CAT3, DCT_VAL_CAT4, DCT_VAL_CAT5, DCT_VAL_CAT6}. As such, the alphabet for coding transform coefficients includes 12 symbols, which are also referred to as tokens. Other token alphabets that include more, fewer, or different tokens are possible. An alphabet that includes only the symbols {0, 1} is referred to herein as a binary alphabet. An alphabet that includes symbols other than and/or in addition to the symbols {0, 1} is referred to herein as a non-binary alphabet. Each of the tokens can be associated with a value. In an example, the EOB_TOKEN can have a value of 255. Each of the other tokens can each be associated with a different value. -
FIG. 8 is a diagram of an example of atree 800 for binarizing a quantized transform coefficient according to implementations of this disclosure. Thetree 800 is a binary tree that can be used for binarizing quantized transform coefficients in some video coding systems. Thetree 800 can be used by a video coding system that uses the steps of binarization, context modelling, and binary arithmetic coding for encoding and decoding of quantized transform coefficients. The process may be referred to as context-adaptive binary arithmetic coding (CABAC). For example, to code a quantized transform coefficient x, the coding system may perform the following steps. The quantized transform coefficient x can be any of the coefficients (e.g., the coefficient 608) of the quantizedtransform block 604 ofFIG. 6 . - In the binarization step, a coefficient x is first binarized into a binary string by using the
tree 800. The binarization process may binarize the unsigned value of the coefficient x. For example, binarizing the coefficient 628 (i.e., the value −1), binarizes thevalue 1. This results in traversing thetree 800 and generating thebinary string 10. Each of the bits of thebinary string 10 is referred to as a bin. - In the context derivation step, for each bin to be coded, a context is derived. A context can be derived from information such as one or more of the block size, plane type (i.e., luminance or chrominance), block position of the coefficient x, and previously decoded coefficients (e.g., a left and/or above neighboring coefficients, if available). Other information can be used to derive the context.
- In the binary arithmetic coding step, given a context, a bin is coded by using, e.g., a binary arithmetic coding engine into a binary codeword together with a probability value associated with the context.
- The steps of coding a transform coefficient can include a step that is referred as context update. In the context update step, after a bin is coded, the probability associated with the context is updated to reflect the value of the bin.
- As described briefly above, entropy coding a sequence of symbols may be achieved by using a probability model to determine a probability p for the sequence. Then, binary arithmetic coding may be used to map the sequence to a binary codeword at the encoder and to decode that sequence from the binary codeword at the decoder. The length (i.e., number of bits) of the codeword or string is given by equation (2) above. As the length is an integer number, however, the length is the smallest integer that is greater than the value calculated by equation (2). The efficiency of entropy coding can be directly related to the probability model.
- In the following description, when referring to a sequence s of N symbols, a subscript of t refers to the symbol at position t in the sequence. For example, where s is a sequence of five (5) binary symbols, such as 11010, s5 refers to the symbol in the 5th position, such as the last 0 in the sequence 11010. As such the sequence s can be expressed as s1, s2, . . . , sN.
- In some implementations, a symbol can refer to a token that is selected from a non-binary token alphabet that includes more than two tokens. As such, the symbol (i.e., token) can have one of the available values. The token can be a token that is used to code, and is indicative of, a transform coefficient. In such cases, “a sequence of symbols s” refers to the list of tokens s1, s2, . . . , sN used to code the transform coefficients at
scan positions - As used herein, probability values, such as the probability {circumflex over (p)}t(st) of a current symbol st, can have either floating-point or fixed-point representations. Accordingly, operations applied to these values may use either floating-point arithmetic or fixed-point arithmetic.
- Given two estimated probabilities for the same symbol {circumflex over (p)}t1(st) and {circumflex over (p)}t2 (st) such that {circumflex over (p)}t1(st)<{circumflex over (p)}t2(st), the probability {umlaut over (p)}t2 (st) results in a codeword that is no shorter than the probability {umlaut over (p)}t1(st). That is, a smaller probability typically produces a longer codeword than a larger probability.
- A probability estimation model, which is a first-order linear system, is derived generally from equation (3) below, which estimates the probabilities that a symbol at t+1 is either 0 or 1 based on a weighted combination of the probabilities for the prior symbol at t and a conditional probability. The weighted combination uses a fixed weight or a variable weight as described below.
-
- This estimate is based on part on understanding that, where pt ∈
- Equation (3) is considered an update rule that corresponds to a linear dynamic system that is used for prediction of sequential data. It is a first-order linear system that may be even more generalized to be written as equation (4) below, where the observed outcome u of the random system at time t is treated as an input.
-
p t+1 =αp t+(1−α)u (4) - If another model in combination (e.g., generating the observed outcome u), the probability estimation model used for entropy coding may instead correspond to a higher-order linear system that produces more accurate results (e.g., lower entropy). In a possible technique, a probability model may include an update algorithm that uses conditions other than those of a baseline probability model in its update rule. For example, instead of using {circumflex over (p)}(st), an estimate of the conditional probability over r symbols {circumflex over (p)}(st st−1 . . . st−τ) may be used. In this estimation, a list may be used to apply multiple probability updates. In a possible technique, a weighted average of models may be used to create a higher-order linear system. In a possible technique, update rates may be self-adaptive as described in more detail below.
- Each of these techniques may be used separately or in combination for the probability estimation, that is, to entropy code a sequence of symbols. In the examples described below, the sequence of symbols input into the entropy coding and update algorithm may comprise a sequence s of N symbols. The sequence may correspond to the binarization of symbols representing any portion of a frame, such as the frame, a segment, a slice, a block, or some other portion of the frame, such as the data described with regards to
FIGS. 6-8 . -
FIG. 9 is a flow chart of amethod 900 for entropy coding a sequence of symbols according to the teachings herein. At 902, a sequence of symbols is received. In the examples herein, the sequence is a sequence s of N binary symbols, where s∈{0,1}N, is entropy coded. The next step is to select a symbol at 904. For example, the current symbol may be a first symbol in the sequence. At 906, the current symbol is entropy coded using a probability. In some implementations, the probability may be the probability determined by a first, or baseline, probability model of a plurality of probability models. In other implementations, the probability may be an updated probability that uses a combination of estimations of the probability determined using respective probability models. In either event, the probability for the next symbol may be updated at 908. The probability of the baseline and any other probability models may be updated, and a combination of these estimations may be used to update the probability at 908. The combination is a second-order linear system different from each of the first-order linear systems represented by the models. Thismethod 900 proceeds to check for remaining symbols at 910 and repeats until no symbols remain to be entropy coded. - The
method 900 is next described with certain examples. First described is an implementation where a fixed probability estimation is used to update the probability for entropy coding symbols of the sequence. The first example is followed by a second example that uses an adaptive probability estimation. - Parameters or variables used for the entropy coding and probability of estimation are defined, initialized, or otherwise determined, either before, after, or concurrent with receipt of the sequence at 902. Because this example uses binary symbols, probability values may be initialized so that the probability that the current symbol is 0 or is 1 is {circumflex over (p)}inf, ={circumflex over (p)}CABAC=p=[0.5,0.5]. That is, the probability that the first symbol is 0 or is 1 is set to be equal at the start of the sequence s. In this example, multiple probability models may be available for probability estimation, while two are shown. The probability p is used to entropy code the current symbol, the probability {circumflex over (p)}inf, is a first probability estimate from a first probability model based on counting as described below, and the probability {circumflex over (p)}CABAC is a second probability estimation from a second probability model based on {circumflex over (p)}CABAC. A parameter mode is selected from the set comprising 0 and 1 (mode ∈{0,1}). The parameter mode indicates which of the first probability model or the second probability model is a baseline model. In an example described herein, mode=0 such that the baseline model comprises a CABAC model.
- A weight w used to combine the probability estimate of the first probability model with a conditional probability is set to a value of 0.5 in this fixed probability estimation, but a variable or adaptive weighting can be used in other examples of the teachings herein. For reasons described in additional detail below, a variable r and a variable tthres are set. For binary entropy coding, the variable r is set to 5, but the variable r may be equal to a different value. For example, when performing multi-symbol entropy coding, the variable r may be set equal to 8. One use of the variable r is to define the size L of a list for storing probability values, which list is used to determine the conditional probability. Entries of probability values within the list are initialized as follows: List=[[0,0]]L, where L=2τ. The variable tthres, also described below, is set equal to 25, but it could be set to a lower or higher value.
- The value α described with regards to equations (3) and (4) may depend upon the particular codec used for the encoding and decoding operations as described above. In some examples, the value α may be a constant or may be adaptive in terms of time and number of symbols. In the following example, the value α is fixed such that α=(0.01875/0.5) 1/63 (approximately equal to 0.95), which is consistent with the CABAC model. As mentioned above, a barrier value pbarrier (also referred to as p62) may be used to limit p to a minimum value. In this example, p62=0.5α62.
- The index (time) t is initialized to 1 to indicate that processing starts with the first symbol s1 in the sequence. While t remains less than or equal to the total number of symbols N, the processing receives the symbol st, codes the symbol st by p, which may also be described herein as {circumflex over (p)}t or {circumflex over (p)}, and then updates the probability as described below. The index t is updated to proceed to the next symbol st+1 in the sequence, if any. The symbol st+1 is entropy coded by the updated probability p. This process continues until all symbols in the sequence s are entropy coded (i.e., entropy encoded or entropy decoded).
- Pseudocode that represents this outer loop of the entropy coding and probability estimation is shown below.
-
while t < N do Receive symbol st. Entropy code st by p. p, {circumflex over (p)}inf, {circumflex over (p)}CABAC, List ← ProbUpdate({circumflex over (p)}inf, {circumflex over (p)}CABAC, τ, st-τ: st, t, w, tthres, List, mode). t ← t + 1 end while - As can be seen from the above pseudocode, the function ProbUpdate is called after st is entropy coded. The function ProbUpdate receives as input the probability pinf, the probability {circumflex over (p)}CABAC, the parameter τ, the values of the symbols in the range st−τ to st, the index t of the current symbol st, the weight w, the variable tthres, List, and the parameter mode. The function ProbUpdate returns the probability p, the probability pinf, the probability {circumflex over (p)}CABAC, and the entries in the List. More generally, the function ProbUpdate updates the probability p for coding the next symbol in the sequence of symbols.
- In an implementation of the teachings herein, the probability estimate updates may incorporate two probability estimation models—the CABAC model previously described (and represented by {circumflex over (p)}CABAC) as well as the maximum likelihood estimate (MLE) for an independent identical distribution (i.i.d) sequence of symbols based on counting (represented by {circumflex over (p)}inf). The MLE for i.i.d sequence may be explained using a binary sequence for simplicity. Assume s1 . . . st is i.i.d Bernoulli (i.e., a Bernoulli distribution) where 0 happens with probability p, and there is no preference of p, i.e., the prior of p is U[0, 1]. From observation of the sequence, if 0 occurs k times and 1 occurs 1 times, the estimator that satisfies equation (5) below
-
- corresponds to equation (6) below for the estimated probability {circumflex over (p)}.
-
- These models, and the others herein, may be referred to as a first probability model, a second probability model, a third probability model, etc., to distinguish one from another without any regard to the sequence of performance. Whether mode indicates that the baseline model is the CABAC model or the MLE model, an updated probability may be determined using an estimate of conditional probability {circumflex over (p)}(st|st−1 . . . st−τ) over the previous symbols. To obtain the estimate of conditional probability pcond, the List is used with an
adjustable size 2τ that stores all possible context sequences st−1:st−τ. The List functions as a hashtable for conditions to store the conditional probability. When a symbol arrives, its previous τ symbols are taken as the context. Then, the corresponding context in the list is accessed, and the count is updated. The probability estimation is the frequency. Until the number of symbols coded is greater than τ (i.e., t>τ), the baseline estimation ({circumflex over (p)}inf or {circumflex over (p)}CABAC) may be output as the probability p. - When the corresponding list item has too few counts, the estimation may not be accurate. There are at least two possible solutions. First, the condition has the length τ (which, as described above, varies with the number of symbols). When the list item has few counts, the history of shorter lengths τ−1, τ−2, etc., may be considered. This involves taking unions of counts in multiple dictionary items. Whenever the count over this union reaches the threshold tthres, this probability estimation is recorded. For example, this may result in merging 00000 and 00001 as 0000. Second, if the total list is not large enough, the baseline estimation ({circumflex over (p)}inf, or {circumflex over (p)}CABAC) may be output as the probability p.
- The mode input mode lets a user decide whether to use a function ProbUpdateCABAC (corresponding to the CABAC model) or ProbUpdateCount (corresponding to the MLE model) to produce the baseline probability estimation and take its average (because weight w=0.5) with the conditional probability estimation (pcond) to provide a stable version of an output. Taking the average is non-trivial compared to changing an update rate (analogous to a in CABAC). This is because an average of two fixed rate update algorithms results in a second-order linear dynamic essentially different from a first-order update.
- That is, referring back to equation (4), a weighted average of probability update may be considered as follows.
-
q t+1 =aq t+(1−a)u t -
r t+i =ar t+(1−b)u t -
p t =wq t+(1−w)r t - Substituting in equation (4) and solving by canceling q and r results in equation (7) below, which is a second order system that covers CABAC when a=b=0.95.
-
p t+i=(a+b)p t −abp t−1+(w(1−a)+(1−w)(1−b))u t+(ab−(1−w)a−wb)u t−1 (7) - This second order system cannot be trivially reduced to a first order system only involving pt+1, pt, and ut.
- The probability update described above used a fixed (e.g., a linear) combination of update algorithms for context-based probability estimation. One example of a function ProbUpdate that implements the second order system described above is shown by the following pseudocode. In brief, when the function ProbUpdate is called after st is entropy coded by p in the outer loop, the probability estimation models available as the baseline model are used to generate a respective estimated probability.
-
DefProbUpdate({circumflex over (p)}inf, {circumflex over (p)}CABAC, τ, st-τ: st, t, w, tthres, List, mode): {circumflex over (p)}inf ← ProbUpdateCount({circumflex over (p)}inf, st, t). {circumflex over (p)}CABAC ← ProbUpdateCABAC({circumflex over (p)}CABAC) st, α). ttmp = 0. if t > τ then List(st; st-τ : st-1) ← List(st; st-τ : st-1) + 1. ttmp ← τ while ttmp > 0 and Σi List (i; st-t tmp + 1: st) < tthres dottmp ← ttmp − 1. end while if ttmp > 0 then end if end if if ttmp > 0 then if mode then p ← w{circumflex over (p)}inf + (1 − w)pcond. else p ← w{circumflex over (p)}CABAC + (1 − w)pcond. end if else if mode then p ← {circumflex over (p)}inf. else p ← {circumflex over (p)}CABAC. end if end if return p, {circumflex over (p)}inf, {circumflex over (p)}CABAC, List. - In brief, when the function ProbUpdate is called after st is entropy coded by p in the outer loop, the probability estimation models available as the baseline model are used to generate a respective estimated probability ({circumflex over (p)}inf and {circumflex over (p)}CABAC in this example). Thereafter, ttmp, which is used for collecting counts in the dictionary, is initialized to 0. The algorithm next counts and merges probabilities among the dictionary as described above, where i represents each possible outcome of the random symbol, and the summation calculates how many outcomes have been observed within the condition window (st−t
tmp +1:st). This counting and merging ends at the second “end if”. The next portion of code queries whether the dictionary is large enough (i.e., if ttmp>0), and if so, updates the probability estimation based on which baseline model is selected given the value of mode to update the probability according to one of two calculations. For example, if mode=0, the updated probability p takes on the value w{circumflex over (p)}CABAC (1−w)pcond If mode=0, the updated probability p takes on the value w{circumflex over (p)}inf+(1−w)pcond. If instead the dictionary is not large enough (i.e., the response to ttmp>0 is no), the baseline estimation {circumflex over (p)}CABAC or {circumflex over (p)}inf is selected given the value of mode for use as the updated probability. - Thereafter, p, {circumflex over (p)}inf, {circumflex over (p)}CABAC, and List are returned so that p can be used to entropy code the next symbol st, {circumflex over (p)}inf and {circumflex over (p)}CABAC are available to update the baseline estimation after the next symbol st is entropy coded, and List is available to optionally generate the conditional probability pcond after the next symbol st is entropy coded.
- The function ProbUpdateCABAC called by the function ProbUpdate described above may be represented by the following pseudocode. This pseudocode represents the CABAC update described above, where p is the vector {circumflex over (p)}CABAC of the probability distribution, namely [p(1−σ), p (σ)].
-
Def ProbUpdateCABAC(p, st, α): Find LPS: σ = argmini∈{0,1}p(i). if st = σ then p(σ) ← max(αp(σ), p62). else p(σ) ← αp(σ) + 1 − α. end if p(1 − σ) ← 1 − p(σ). return p - The function ProbUpdateCount called by the function ProbUpdate described above may be represented by the following pseudocode. This pseudocode represents the MLE calculation described above, where p is the vector {circumflex over (p)}inf, of the probability distribution for the given outcome value st.
-
Def ProbUpdateCount(p, st, t): return p - Other update algorithms for context-based probability estimation are possible. For example, additional algorithms may include a data-driven method that describes learning a linear combination, as opposed to using the fixed combination described above. An implementation for this entropy coding and adaptive probability estimation (as compared to fixed probability estimation) is next described.
- In this implementation, instead of using the conditional probability estimated by the previous symbols using List to make a higher-order linear system, the different first-order linear models described above (CABAC, counting (MLE), AV1, etc.) are desirably used as kernels to output a linear combination through actively learning the linear combination. While these three models are used in this example, any probability estimation algorithm may be used. Denote np as the number of kernels, {circumflex over (p)}∈
p ×2 where each row is a probability estimation, and w∈p is the weight/parameter of a linear combination. In other words, a weighted average of simple (i.e., first-order) probability estimations is used as the result for entropy coding the next symbol as follows. -
w T {circumflex over (p)}=Σw i {circumflex over (p)}(i,:) - Each row of {circumflex over (p)} is updated by a probability update algorithm, and p(1, :) is fixed as an AV1 output. In this way, the AV1 model/algorithm corresponds to the case when w0=1, wi=0, ∀i≥2. This may be the initialization of linear weights in the pseudocode described below. For this reason, the AV1 model may be referred to as the baseline model.
- Thereafter, w is updated. Because it is expected that all update algorithms chosen as kernels should result in an improvement to the output wT{circumflex over (p)}, w may be constrained so that w≥0. This also guarantees that the probability estimation is non-negative. Further, 1Tw=1 is applied to guarantee that the sum of probability is 1. A stochastic gradient descent (SGD) is used to update w. For each st, an entropy is incurred as follows.
-
f(w,{circumflex over (p)};s t)=−log2((w T {circumflex over (p)})(s t)) - A gradient is taken with respect to w as follows.
-
- At time t, a step size ηt=η0/t is used, which is standard for SGD, ηt=η0/tr r∈(0,1) are allowed, and stochastic approximation defines r∈(1/2, 1). Then, w is updated by the following gradient step.
-
- Alternatively, a fixed step size η=η0 can be used to get an inner loop argument that iterates {tilde over (w)}t and plug in for final probability estimation wt, which satisfies wt=(Σi=1 t{tilde over (w)}i)/t. This cancels out the noise in SGD so that a special diminishing step size or averaging the gradient may be used. A linear dynamic is also proposed for variable iterates wt+1=βwt+(1−β){tilde over (w)}t as a faster update. This is the process presented in the pseudocode below.
- To update weight w, a constrained optimization step may be included. Solving such a step may be slow. To reduce the number of calls for the step, a batch version the above algorithm may be used. At each epoch, a batch with increasing
size - A fast algorithm is next proposed that approximately solves the optimization problem. Namely, the problem may be defined as the following equation.
-
- Simplifying the notation results in the following equation.
-
- Optimality may be obtained from the Lagrangian according to the following equation.
-
- The Karush-Kuhn-Tucker (KKT) condition is represented by the following.
-
∇x L(x,μ)=x−y−λ+μ1=0; -
Δ≥0;x≥0;λi x i=0,∀i. - The optimal value for x is represented by the following equation.
-
x i*=max(γi−μ,0). - Thus, the following equation may be solved to get μ* and x*=max(γ−u*1, 0).
-
- Note that the above equation is a one-dimensional convex optimization that can be solved by binary search.
- The above data-driven method that describes learning a linear combination may be represented by the following pseudocode, where the input is a sequence of binary symbols as described with respect to the previous implementation. As with the previous implementation, the first step is initialization. During initialization, a variable np is set equal to 18, and the
probability 3 is set equal to {circumflex over (p)}=1np [0.5,0.5]. Further, the variable w={tilde over (w)}=[1,0, . . . , 0]T∈p . The variable a is initialized according to 0.99·2−[0:np −2]/4(np −2)∈p −2. Other variables are initialized as follows: η0=5, r=1, b_=b=0, g=0np , ß=0.95, and αmin=0.84, r∈(1/2, 1). The algorithm chooses the mode from SGD decreasing step size, SGD average argument, SGD dynamic argument, or SGD batch. It is worth noting that when the mode is SGD decreasing step size, it can be solved by the fast projected optimization algorithm described above. - As with the fixed probability estimation described above, parameters or variables used for the entropy coding and probability of estimation are defined, initialized, or otherwise determined, either before, after, or concurrent with receipt of the sequence at 902. Then, the remaining steps of the
method 900 are performed according to the following pseudocode starting with receiving the first symbol s1 and entropy coding the first symbol s1. Then, the probability estimations are updated using the respective models. The functions ProbUpdateCount and ProbUpdateCABAC have been discussed above. The function ProbUpdateAV1 is described below. Once the probability estimations are updated, they are combined using the selected mode. -
while t ≤ N do Receive symbol st. Entropy code st by wT{circumflex over (p)}. {circumflex over (p)}(1, :) ← ProbUpdateAV1({circumflex over (p)}(1, :), st, t, NumOfSyms). {circumflex over (p)}(2, :) ← ProbUpdateCount({circumflex over (p)}(2, :), st, t). {circumflex over (p)}(i, :) ← ProbUpdateCABAC({circumflex over (p)}(i, :), st, αi-2) for all 3 ≤ i ≤ np. if SGD decreasing step size then else if SGD average argument then else if SGD dynamic argument then w ← βw + (1 − β){tilde over (w)}. else if SGD batch then if t ≤ b then \\ Batch from b_ to b, size 16, . . . end if if t = b then g ← 0n p .b+ ← b + ({square root over (b − b_ + 1)} + 1)2, b_ ← b + 1, b ← b+. \\ Update batch. end if end if t ← t + 1.end while - The function ProbUpdateAV1 may be represented by the following pseudocode. This pseudocode represents the AV1 calculation described above, where p is the vector {circumflex over (p)}(1, :) of the probability distribution for the given outcome value st.
-
Def ProbUpdateAV1(p, s, t, NumOfSyms) : p0 ← 0.0076. r ← 3 + (t > 15) + (t > 31) + (NumOfSyms > 2) + (NumOfSyms > 4) p ← max((1 − 2−r)p,p0). p(s) ← p(s) + 1 − Σi=1 NumOfSymsp(i) return p - Note that NumOfSyms (the number of symbols) is 2 in this example, but it could be a higher number. Also note that a is used as an input to ProbUpdateCABAC. While it is a constant in these examples, this would allow the value to be adaptive.
- Below is a table of the entropy resulting for various binary sequences using different context-based probability estimation techniques described herein. The table compares six techniques using nine different test sequences. The conventional CABAC and AV1 models/algorithms are baselines, against which different proposed models/algorithms are compared. As can be seen from the left-most column, the models for comparison are the SGD processing without SGD batch processing, the SGD batch processing, the fixed combination update algorithm for context-based probability estimation described with the parameters/variables above, and the fixed combination update algorithm for context-based probability estimation described with the parameters/variables above except that mode is set equal to 1 instead of 0. The proposed algorithms perform better than the baselines in most conditions. The differences generally relate to the parameter p62 in CABAC. Too sparse a dataset results in worse entropy when using this parameter.
-
allzbk allzbk zblk zblk allzbk allzbk zblk allzbk allzbk 16.0 8.0 4.0 8.0 16.1 8.1 4.1 16.2 8.2 SGD 2423 3160 1299 757.9/ 245.0 504.2 96.71 352.8 748.3 689.1 Batch 2410 3160 1311 730.1/ 243.8 503.5 98.28 353.3 747.1 703.1 Fixed 2404 3165 1312 779.0/ 246.6 508.1 95.90 354.6 749.7 689.1 Modified 2414 3184 1410 712.5 248.2 500.2 96.04 354.3 744.4 Fixed CABAC 2457 3209 1311 792.3 247.2 510.1 96.05 356.0 754.6 AV1 2479 3177 1322 692.5 248.2 505.6 97.11 354.0 751.3 - The underlying probability model from which symbols are emitted in video coding is typically unknown and/or is likely too complex to be fully described. As such, designing a good model for use in entropy coding can be a challenging problem in video coding. For example, a model that works well for one sequence may perform poorly for another sequence. A model, as used herein, can be, or can be a parameter in, lossless (entropy) coding. A model can be any parameter or method that affects probability estimation for the purpose of entropy coding. For example, a model can define the probability to be used to encode and decode the decision at an internal node in a token tree (such as described with respect to
FIG. 7 ). In such a case, the two-pass process to learn the probabilities for a current frame may be simplified to a single-pass process by modifying a baseline model for probability estimation as described herein. In another example, a model may define a certain context derivation method. In such a case, implementations according to this disclosure can be used to combing probability estimations generated by a multitude of such methods. In yet another example, a model may define a completely new lossless coding algorithm. - The probability update algorithm for entropy coding described herein may incorporate an average of different models with fast and slow update rates. An MLE estimator based on counting may be incorporated. Conditional probability and dictionary searching are options. The implementations also allow for adaptive fusion of models.
- For simplicity of explanation, the techniques herein are each depicted and described as a series of blocks, steps, or operations. However, the blocks, steps, or operations in accordance with this disclosure can occur in various orders and/or concurrently. Additionally, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a technique in accordance with the disclosed subject matter.
- The aspects of encoding and decoding described above illustrate some examples of encoding and decoding techniques. However, it is to be understood that encoding and decoding, as those terms are used in the claims, could mean compression, decompression, transformation, or any other processing or change of data.
- The word “example” is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” is not necessarily to be construed as being preferred or advantageous over other aspects or designs. Rather, use of the word “example” is intended to present concepts in a concrete fashion. As used in this application, the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise or clearly indicated otherwise by the context, the statement “X includes A or B” is intended to mean any of the natural inclusive permutations thereof. That is, if X includes A; X includes B; or X includes both A and B, then “X includes A or B” is satisfied under any of the foregoing instances. In addition, the articles “a” and “an” as used in this application and the appended claims should generally be construed to mean “one or more,” unless specified otherwise or clearly indicated by the context to be directed to a singular form. Moreover, use of the term “an implementation” or the term “one implementation” throughout this disclosure is not intended to mean the same implementation unless described as such.
- Implementations of the transmitting
station 102 and/or the receiving station 106 (and the algorithms, methods, instructions, etc., stored thereon and/or executed thereby, including by theencoder 400 and the decoder 500) can be realized in hardware, software, or any combination thereof. The hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit. In the claims, the term “processor” should be understood as encompassing any of the foregoing hardware, either singly or in combination. The terms “signal” and “data” are used interchangeably. Further, portions of the transmittingstation 102 and the receivingstation 106 do not necessarily have to be implemented in the same manner. - Further, in one aspect, for example, the transmitting
station 102 or the receivingstation 106 can be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein. In addition, or alternatively, for example, a special purpose computer/processor can be utilized which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein. - The transmitting
station 102 and the receivingstation 106 can, for example, be implemented on computers in a video conferencing system. Alternatively, the transmittingstation 102 can be implemented on a server, and the receivingstation 106 can be implemented on a device separate from the server, such as a handheld communications device. In this instance, the transmittingstation 102, using anencoder 400, can encode content into an encoded video signal and transmit the encoded video signal to the communications device. In turn, the communications device can then decode the encoded video signal using adecoder 500. Alternatively, the communications device can decode content stored locally on the communications device, for example, content that was not transmitted by the transmittingstation 102. Other suitable transmitting and receiving implementation schemes are available. For example, the receivingstation 106 can be a generally stationary personal computer rather than a portable communications device, and/or a device including anencoder 400 may also include adecoder 500. - Further, all or a portion of implementations of this disclosure can take the form of a computer program product accessible from, for example, a computer-usable or computer-readable medium. A computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.
- The above-described implementations and other aspects have been described to facilitate easy understanding of this disclosure and do not limit this disclosure. On the contrary, this disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation as is permitted under the law to encompass all such modifications and equivalent arrangements.
Claims (26)
1. A method for entropy coding a sequence of symbols, comprising:
determining a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models;
entropy coding at least one symbol of the sequence using a probability determined by the first probability model;
after entropy coding a respective symbol of the sequence, determining a first probability estimation to update the probability using the first probability model;
for a subsequent symbol relative to the at least one symbol of the sequence, determining a second probability estimation using a second probability model; and
entropy coding the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.
2. The method of claim 1 , wherein the first probability model comprises a context-adaptive binary arithmetic coding (CABAC) model or an AV1 model.
3. The method of claim 1 , wherein the first probability model comprises a Maximum Likelihood Estimate of a Bernoulli distribution.
4. The method of claim 1 , wherein the at least one symbol comprises multiple symbols up to a minimum number of symbols.
5. The method of claim 1 , further comprising:
forming the combination as a linear combination of the first probability estimation and the second probability estimation.
6. The method of claim 1 , wherein:
the combination is a weighted combination of the first probability estimation and the second probability estimation.
7. The method of claim 6 , wherein the weighted combination uses a fixed weight.
8. The method of claim 6 , wherein the weighted combination uses a variable weight.
9. (canceled)
10. (canceled)
11. (canceled)
12. (canceled)
13. The method of claim 1 , wherein the at least one symbol comprises a first symbol, the method comprising:
entropy coding each symbol after the first symbol using the probability used for entropy coding a previous symbol updated using a combination of the first probability estimation and the second probability estimation.
14. (canceled)
15. (canceled)
16. An apparatus, comprising:
a processor configured to:
determine a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models;
entropy code at least one symbol of the sequence using a probability determined by the first probability model;
after entropy coding a respective symbol of the sequence, determine a first probability estimation to update the probability using the first probability model;
for a subsequent symbol relative to the at least one symbol of the sequence, determine a second probability estimation using a second probability model; and
entropy code the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.
17. The apparatus of claim 16 , wherein the first probability model comprises a Maximum Likelihood Estimate of a Bernoulli distribution.
18. The apparatus of claim 16 , wherein the at least one symbol comprises multiple symbols up to a minimum number of symbols.
19. The apparatus of claim 1 , wherein the processor is configured to:
form the combination as a linear combination of the first probability estimation and the second probability estimation.
20. The apparatus of claim 1 , wherein:
the combination is a weighted combination of the first probability estimation and the second probability estimation, and the weighted combination uses one of a fixed weight or a variable weight.
21. The apparatus of claim 16 , wherein the processor is configured to:
determine, using a third probability model for entropy coding, a third probability estimation for the subsequent symbol, wherein the combination comprises a combination of the first probability estimation, the second probability estimation, and the third probability estimation.
22. The apparatus of claim 21 , wherein the combination of the first probability estimation, the second probability estimation, and the third probability estimation is a linear combination using a weighted average of the first probability estimation, the second probability estimation, and the third probability estimation.
23. The apparatus of claim 22 , wherein a weight used for the weighted average is updated using a stochastic gradient descent (SGD).
24. The apparatus of claim 23 , wherein the first probability model comprises an SGD decreasing step size, an SGD average argument, an SGD dynamic argument, or a SGD batch.
25. The apparatus of claim 16 , wherein the at least one symbol comprises a first symbol, and the processor is configured to:
entropy code each symbol after the first symbol using the probability used for entropy coding the previous symbol updated using a combination of the first probability estimation and the second probability estimation.
26. The apparatus of claim 17 , wherein the combination uses an adaptive weighting of the first probability estimation and the second probability estimation.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/775,565 US20230007260A1 (en) | 2019-11-08 | 2020-11-09 | Probability Estimation for Video Coding |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962932508P | 2019-11-08 | 2019-11-08 | |
PCT/US2020/059594 WO2021092531A1 (en) | 2019-11-08 | 2020-11-09 | Probability estimation for entropy coding |
US17/775,565 US20230007260A1 (en) | 2019-11-08 | 2020-11-09 | Probability Estimation for Video Coding |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230007260A1 true US20230007260A1 (en) | 2023-01-05 |
Family
ID=75849312
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/775,565 Pending US20230007260A1 (en) | 2019-11-08 | 2020-11-09 | Probability Estimation for Video Coding |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230007260A1 (en) |
EP (1) | EP4029259A1 (en) |
CN (1) | CN114556790A (en) |
WO (1) | WO2021092531A1 (en) |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200365229A1 (en) * | 2019-05-13 | 2020-11-19 | Grail, Inc. | Model-based featurization and classification |
US20210126650A1 (en) * | 2018-07-06 | 2021-04-29 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Arithmetic Encoders, Arithmetic Decoders, Video Encoder, Video Decoder, Methods for Encoding, Methods for Decoding and Computer Program |
US20230247201A1 (en) * | 2015-10-13 | 2023-08-03 | Samsung Electronics Co., Ltd. | Method and device for encoding or decoding image |
-
2020
- 2020-11-09 EP EP20816823.7A patent/EP4029259A1/en active Pending
- 2020-11-09 CN CN202080071975.7A patent/CN114556790A/en active Pending
- 2020-11-09 WO PCT/US2020/059594 patent/WO2021092531A1/en unknown
- 2020-11-09 US US17/775,565 patent/US20230007260A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230247201A1 (en) * | 2015-10-13 | 2023-08-03 | Samsung Electronics Co., Ltd. | Method and device for encoding or decoding image |
US20210126650A1 (en) * | 2018-07-06 | 2021-04-29 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Arithmetic Encoders, Arithmetic Decoders, Video Encoder, Video Decoder, Methods for Encoding, Methods for Decoding and Computer Program |
US20200365229A1 (en) * | 2019-05-13 | 2020-11-19 | Grail, Inc. | Model-based featurization and classification |
Non-Patent Citations (1)
Title |
---|
Alshina et al. "Non-CE1: Accurate Initialization and Conditional Probability for CABAC Performance Improvement" February 2012 (Year: 2012) * |
Also Published As
Publication number | Publication date |
---|---|
CN114556790A (en) | 2022-05-27 |
EP4029259A1 (en) | 2022-07-20 |
WO2021092531A1 (en) | 2021-05-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11405618B2 (en) | Using multiple models for entropy coding in video compression | |
US10484695B2 (en) | Refined entropy coding for level maps | |
US10735767B2 (en) | Transform coefficient coding using level maps | |
US11102477B2 (en) | DC coefficient sign coding scheme | |
US10506258B2 (en) | Coding video syntax elements using a context tree | |
US11917156B2 (en) | Adaptation of scan order for entropy coding | |
US10194153B1 (en) | Bin string coding based on a most probable symbol | |
US10225562B1 (en) | Embedding information about EOB positions | |
US10284854B2 (en) | Adaptive stochastic entropy coding | |
US20230007260A1 (en) | Probability Estimation for Video Coding | |
WO2023163807A1 (en) | Time-variant multi-hypothesis probability model update for entropy coding | |
WO2023163808A1 (en) | Regularization of a probability model for entropy coding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HAN, JINGNING;SUN, YUE;XU, YAOWU;SIGNING DATES FROM 20220509 TO 20220524;REEL/FRAME:060056/0524 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |