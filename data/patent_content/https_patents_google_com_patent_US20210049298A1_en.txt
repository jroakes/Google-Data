US20210049298A1 - Privacy preserving machine learning model training - Google Patents
Privacy preserving machine learning model training Download PDFInfo
- Publication number
- US20210049298A1 US20210049298A1 US16/994,396 US202016994396A US2021049298A1 US 20210049298 A1 US20210049298 A1 US 20210049298A1 US 202016994396 A US202016994396 A US 202016994396A US 2021049298 A1 US2021049298 A1 US 2021049298A1
- Authority
- US
- United States
- Prior art keywords
- model parameter
- noisy
- gradient
- model
- estimate
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012549 training Methods 0.000 title claims abstract description 106
- 238000010801 machine learning Methods 0.000 title claims abstract description 69
- 238000000034 method Methods 0.000 claims abstract description 45
- 238000003860 storage Methods 0.000 claims abstract description 10
- 230000009466 transformation Effects 0.000 claims description 19
- 230000006870 function Effects 0.000 claims description 11
- 230000001131 transforming effect Effects 0.000 claims 3
- 238000004590 computer program Methods 0.000 abstract description 15
- 230000008569 process Effects 0.000 description 23
- 238000012545 processing Methods 0.000 description 16
- 238000013528 artificial neural network Methods 0.000 description 9
- 230000009471 action Effects 0.000 description 7
- 230000003044 adaptive effect Effects 0.000 description 7
- 230000003993 interaction Effects 0.000 description 7
- 238000004891 communication Methods 0.000 description 6
- 239000003795 chemical substances by application Substances 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- 230000036541 health Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000009826 distribution Methods 0.000 description 3
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 241000009334 Singa Species 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000003745 diagnosis Methods 0.000 description 1
- 238000003709 image segmentation Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6218—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database
- G06F21/6245—Protecting personal data, e.g. for financial or medical purposes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/17—Function evaluation by approximation methods, e.g. inter- or extrapolation, smoothing, least mean square method
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
- G06F18/2148—Generating training patterns; Bootstrap methods, e.g. bagging or boosting characterised by the process organisation or structure, e.g. boosting cascade
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/217—Validation; Performance evaluation; Active pattern learning techniques
- G06F18/2193—Validation; Performance evaluation; Active pattern learning techniques based on specific statistical tests
-
- G06K9/6257—
-
- G06K9/6265—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/774—Generating sets of training patterns; Bootstrap methods, e.g. bagging or boosting
- G06V10/7747—Organisation of the process, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/778—Active pattern-learning, e.g. online learning of image or video features
- G06V10/7796—Active pattern-learning, e.g. online learning of image or video features based on specific statistical tests
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
Definitions
- This specification relates to training machine learning models, e.g., neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input.
- Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer.
- Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that trains a machine learning model, e.g., a neural network, to perform a particular task.
- a machine learning model e.g., a neural network
- the system trains the machine learning model on training data that includes user data from multiple users in a manner that preserves the privacy of the users.
- the system can train a machine learning model to perform well on any of a variety of machine learning tasks while maintaining the privacy of the users from whom the training data was derived.
- the system can train the machine learning model to have improved performance on any of a variety of machine learning tasks relative to other training mechanisms that preserve the privacy of user data while still maintaining privacy equally as well or better than the conventional techniques.
- the system By determining privacy preserving noisy gradients for each model parameter using coordinate-wise, i.e., per-parameter, adaptive clipping, the system is able to add less noise to the gradient for each model parameter while still maintaining user privacy. Adding less noise to gradients during training results in the model performing better after training. Additionally, adding less noise to gradients can result in faster convergence of the model parameters, decreasing the amount of time and computational resources required to train the model. More specifically, the system achieves this adaptive clipping by, for each parameter, modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter. These estimates are repeatedly updated during training, resulting in a scheme that is both adaptive and coordinate-wise.
- FIG. 1 shows an example model training system.
- FIG. 2 is a flow diagram of an example process for training the machine learning model.
- FIG. 3 is a flow diagram of an example process for determining a privacy preserving noisy gradient for a given model parameter.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that trains a machine learning model that is configured to perform a particular machine learning task.
- the machine learning model can be trained to perform any kind of machine learning task, i.e., can be configured to receive any kind of digital data input and to generate any kind of score, classification, or regression output based on the input.
- the machine learning model is a neural network that is configured to perform an image processing task, i.e., receive an input image and to process the input image to generate a model output for the input image.
- the task may be image classification and the output generated by the machine learning model for a given image may be scores for each of a set of object categories, with each score representing an estimated likelihood that the image contains an image of an object belonging to the category.
- the task can be image embedding generation and the output generated by the machine learning model can be a numeric embedding of the input image.
- the task can be object detection and the output generated by the machine learning model can identify locations in the input image at which particular types of objects are depicted.
- the task can be image segmentation and the output generated by the machine learning model can assign each pixel of the input image to a category from a set of categories.
- the task can be to classify the resource or document, i.e., the output generated by the machine learning model for a given Internet resource, document, or portion of a document may be a score for each of a set of topics, with each score representing an estimated likelihood that the Internet resource, document, or document portion is about the topic.
- the output generated by the machine learning model for a given Internet resource, document, or portion of a document may be a score for each of a set of topics, with each score representing an estimated likelihood that the Internet resource, document, or document portion is about the topic.
- the output generated by the machine learning model may be a score that represents an estimated likelihood that the particular advertisement will be clicked on.
- the output generated by the machine learning model may be a score for each of a set of content items, with each score representing an estimated likelihood that the user will respond favorably to being recommended the content item.
- the output generated by the machine learning model may be a score for each of a set of pieces of text in another language, with each score representing an estimated likelihood that the piece of text in the other language is a proper translation of the input text into the other language.
- the task may be an audio processing task.
- the output generated by the machine learning model may be a score for each of a set of pieces of text, each score representing an estimated likelihood that the piece of text is the correct transcript for the utterance.
- the task may be a keyword spotting task where, if the input to the machine learning model is a sequence representing a spoken utterance, the output generated by the machine learning model can indicate whether a particular word or phrase (“hotword”) was spoken in the utterance.
- hotword a particular word or phrase
- the output generated by the machine learning model can identify the natural language in which the utterance was spoken.
- the task can be a natural language processing or understanding task, e.g., an entailment task, a paraphrase task, a textual similarity task, a sentiment task, a sentence completion task, a grammaticality task, and so on, that operates on a sequence of text in some natural language.
- a natural language processing or understanding task e.g., an entailment task, a paraphrase task, a textual similarity task, a sentiment task, a sentence completion task, a grammaticality task, and so on, that operates on a sequence of text in some natural language.
- the task can be a text to speech task, where the input is text in a natural language or features of text in a natural language and the model output is a spectrogram or other data defining audio of the text being spoken in the natural language.
- the task can be a health prediction task, where the input is electronic health record data for a patient and the output is a prediction that is relevant to the future health of the patient, e.g., a predicted treatment that should be prescribed to the patient, the likelihood that an adverse health event will occur to the patient, or a predicted diagnosis for the patient.
- a prediction that is relevant to the future health of the patient, e.g., a predicted treatment that should be prescribed to the patient, the likelihood that an adverse health event will occur to the patient, or a predicted diagnosis for the patient.
- the task can be an agent control task, where the input is an observation characterizing the state of an environment and the output defines an action to be performed by the agent in response to the observation.
- the agent can be, e.g., a real-world or simulated robot, a control system for an industrial facility, or a control system that controls a different kind of agent.
- user data i.e., data that is provided by a user, e.g., a label for an image that is provided by the user, data that is specific to a particular user, e.g., text written by a particular user or a medical record of a user, or that is generated as a result of user interaction with a system, e.g., search queries submitted by the user to a search engine or an interaction or selection history for a user that is based on interactions with content items by the user.
- user data i.e., data that is provided by a user, e.g., a label for an image that is provided by the user, data that is specific to a particular user, e.g., text written by a particular user or a medical record of a user, or that is generated as a result of user interaction with a system, e.g., search queries submitted by the user to a search engine or an interaction or selection history for a user that is based on interactions with content items by the user.
- This user data can include sensitive information that users may not wish to make public, e.g., data from typing histories, social networks, financial records, or medical records, and that users may not wish to be made transparent through predictions made by the model.
- the system trains the machine learning model in a manner that preserves the privacy of each of the users from which the training data for the model is derived.
- the system trains the machine learning model so that the outputs generated by the trained machine learning model are, to a specified degree, indistinguishable whether or not any given user's training data is included in the training data for the model. That is, the system performs the training such that, for any given user associated with any data in the training data set, outputs generated by the trained model are, to a specified degree, indistinguishable between the model being trained on (i) the entire training data set minus the data associated with the user or (ii) the entire training data set.
- the system accomplishes this by, for each batch of training examples that is used during training, determining respective privacy preserving gradients for each model parameter and using these privacy preserving gradients to update the model parameters.
- user data may be treated in one or more ways before it is stored or used for training the machine learning model, so that personally identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.
- FIG. 1 shows an example model training system 100 .
- the model training system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.
- the model training system 100 is a system that obtains training data 102 for training a machine learning model 110 to perform a particular task.
- the training data 102 includes user data, i.e., at least some of the training examples in the training data 102 are derived from users, i.e., are generated from data that is provided by a user, data that is specific to a particular user, or data that is generated as a result of user interaction with a system.
- the training data 102 includes a set of training examples, with each training example including a training input and, for each training input, a respective target output that should be generated by the machine learning model to perform the particular task.
- the system 100 can receive the training data 102 in any of a variety of ways.
- the system 100 can receive training data as uploads from users of the system over a data communication network, e.g., using an application programming interface (API) made available by the system 100 .
- API application programming interface
- the system 100 can receive an input from a user specifying which data that is already maintained by the system 100 should be used for training the machine learning model.
- the machine learning model 110 is a model, e.g., a neural network, having a set of parameters (“model parameters”) and that is configured to process model inputs in accordance with the model parameters to generate an output for the particular task.
- model parameters e.g., a neural network
- the machine learning model 110 can have any appropriate architecture that allows the model 110 to receive model inputs of the type required by the particular task and to generate model outputs of the form required for the particular task.
- Examples of machine learning models 110 that can be trained by the system 100 include fully-connected neural networks, convolutional neural networks, recurrent neural networks, attention-based neural networks, e.g., Transformers, and so on.
- a training engine 120 within the system 100 trains the machine learning model 110 on the training data 102 over multiple training steps using an iterative gradient descent training technique.
- the training engine 120 would compute respective gradients 122 of an objective function with respect to each of the model parameters and on a batch of training examples selected from the training data 102 , apply a learning rate to the computed gradients 122 to determine a parameter value update, and then apply the parameter value update to the current values of the model parameters of the machine learning model, i.e., by subtracting or adding the parameter value update with the current parameter values.
- training the machine learning model in this manner may not result in the privacy of the users that provided the user data being preserved.
- information related to a given user may be recoverable from the trained model because the model output for any given model input may be different depending on whether the given user's data was included in the training data or not.
- a privacy preserving engine 130 within the system 100 first applies coordinate-wise, i.e., per-parameter, adaptive clipping to each gradient 122 to determine respective privacy preserving noisy gradients 132 for each of the model parameters.
- the training engine 120 then updates the model parameters, i.e., determines updated model parameters 124 , based on the privacy preserving noisy gradients 132 instead of directly using the gradients 122 .
- the training engine 120 repeatedly updates the values of the model parameters to improve the performance of the machine learning model 110 on the particular task.
- the manner in which the training engine 120 determines the parameter value update i.e., how the training engine 120 applies the learning rate to any given privacy preserving noisy gradient 132 , is dependent on the optimizer that is being used in the training.
- the update is a product of the learning rate and the gradient.
- the update is a product of the learning rate and an exponentially decayed average of past gradients.
- the system first adapts the learning rate per weight, i.e., per model parameter, based on the sums of the squares of the gradients and then computes, for each model parameter, a product of the gradient with respect to the parameter and the adapted learning rate.
- the system 100 deploys the trained machine learning model and then uses the trained model to process requests received from users, e.g., through the API provided by the system.
- the system uses the trained machine learning model 110 to generate new model outputs for new model inputs.
- the system 100 can provide data specifying the final model parameter values to a user who submitted a request to train the machine learning model or to the users from whom the training data was derived, e.g., through the API.
- FIG. 2 is a flow diagram of an example process 200 for training the machine learning model.
- the process 200 will be described as being performed by a system of one or more computers located in one or more locations.
- a model training system e.g., the model training system 100 of FIG. 1 , appropriately programmed, can perform the process 200 .
- the system can repeatedly perform the process 200 on different batches of training data to determine trained values of the model parameters, i.e., by repeatedly updating the current values of the model parameters. For example, the system can continue performing the process 200 until a threshold number of iterations of the process have been performed, until a threshold amount of time has elapsed, or until the values of the model parameters have converged.
- the system Prior to performing the process 200 , the system initializes the model parameters and, for each model parameter, a respective mean noisy gradient estimate and a respective standard deviation noisy gradient estimate.
- the system can initialize the model parameters using a machine learning parameter initialization technique, e.g., random initialization.
- a machine learning parameter initialization technique e.g., random initialization.
- the system will use the mean noisy gradient estimates and the standard deviation noisy gradient estimates when adaptively clipping gradients to update the model parameters during training of the model.
- the system can initialize the mean noisy gradient estimate for each of the model parameters to a fixed value, e.g., zero.
- the system can initialize the standard deviation noisy gradient estimate for each of the model parameters based on predetermined maximum and minimum values for the standard deviation of the noisy gradients for the model parameters. For example, the system can initialize the standard deviation noisy gradient estimate for each of the model parameters to be equal to the square root of the product of the predetermined maximum value and the predetermined minimum value. These predetermined maximum and minimum values will then be used when updating the standard deviation noisy gradient estimate to ensure that the estimate stays remains within the range defined by these values.
- the system obtains a batch of training examples, i.e., a plurality of training examples that include user data from multiple different users (step 202 ). For example, each training example in the batch can be derived from a different user.
- the system computes, for each of the training examples and for each of the model parameters, a respective gradient of an objective function with respect to the model parameter (step 204 ).
- the objective function can be any appropriate objective function for the particular task that the model is being trained to perform and generally measures errors between an output generated by the machine learning model for a training input and a target output that should have been generated by the machine learning model for the training input. Examples of objective functions that may be appropriate for various tasks include cross-entropy losses, mean squared error losses, L2 distance losses, log likelihood objectives, and so on.
- the system computes a respective gradient for each training example in the batch.
- the system determines, for each of the training examples and for each of the model parameters, a respective privacy preserving noisy gradient (step 206 ).
- the system modifies the respective gradient for the model parameter based on (i) the mean noisy gradient estimate for the model parameter and (ii) the standard deviation noisy gradient estimate for the model parameter to generate a privacy preserving noisy gradient.
- the system adaptively clips the respective gradient for the model parameter based on (i) the mean noisy gradient estimate for the model parameter and (ii) the standard deviation noisy gradient estimate for the model parameter and then adds noise to the adaptively clipped gradient.
- the system determines, for each of the model parameters, a respective privacy preserving update for the model parameter from the privacy preserving noisy gradients for the model parameter for the plurality of training examples (step 208 ).
- the system computes an average noisy gradient for the given model parameter by averaging the privacy preserving noisy gradients for the model parameter for the plurality of training examples.
- the system For each model parameter, the system then applies a current learning rate to the average noisy gradient to generate the privacy preserving update for the model parameter.
- a current learning rate to the average noisy gradient to generate the privacy preserving update for the model parameter.
- the manner in which the system generates the privacy preserving update will depend on the optimizer that is being used for the training.
- the system can multiply the gradient by the current learning rate to generate the update.
- the system can determine a modified gradient based on the current gradient and one or more recently computed gradients and then multiply the modified gradient by the current learning rate to generate the update.
- the system updates the model parameters based on the privacy preserving updates (step 210 ). For example, the system can, for each model parameter, subtract the privacy preserving update for the model parameter from the model parameter to generate an updated model parameter.
- the system updates the mean noisy gradient estimate for each of the plurality of model parameters (step 212 ).
- the system updates the mean noisy gradient estimate for the model parameter based on the privacy preserving noisy gradients for the model parameter by interpolating between the mean noisy gradient estimate and the average privacy preserving noisy gradient for the model parameter for the plurality of training examples in accordance with a mean interpolation weight hyperparameter.
- the updated mean noisy gradient estimate is an interpolation between (i) the currently maintained mean noisy gradient estimate and (ii) the average privacy preserving noisy gradient for the model parameter for the plurality of training examples, where the weight assigned to (i) and (ii) in the interpolation is governed by a mean interpolation weight hyperparameter that defines how quickly the estimate changes during the training.
- the system updates the standard deviation noisy gradient estimate for each of the plurality of model parameters (step 214 ).
- the system updates the standard deviation noisy gradient estimate for any given model parameter based on the mean estimate for the given model parameter and the privacy preserving noisy gradients for the given model parameter. More specifically, the system computes an exponential moving average using these quantities in order to determine the updated standard deviation noisy gradient estimate for the given model parameter.
- the system To update the standard deviation noisy gradient estimate, the system first computes an initial updated estimate for the given model parameter that is bounded below by the predetermined minimum value for the standard deviation and bounded above by the predetermined maximum value for the standard deviation.
- the system computes the difference between (i) a squared distance between the mean estimate for the given model parameter and the average privacy preserving noisy gradient for the given model parameter and (ii) the product of an auxiliary transformation parameter for the given model parameter (that is generated from the current noisy standard deviation estimate for the model parameter) used when clipping gradients and a noise scale used to apply noise to the clipped gradients.
- auxiliary transformation parameter for the given model parameter and the noise scale will be described in more detail below with reference to FIG. 3 .
- the system sets the initial updated estimate equal to the difference.
- the system sets the initial updated estimate equal to the predetermined minimum value.
- the system sets the initial updated estimate equal to the predetermined maximum value.
- the system then computes the exponential moving average by interpolating between the square of the current standard deviation noisy gradient estimate and the initial updated estimate for the given model parameter, with the interpolation weights being governed by a standard deviation interpolation weight hyperparameter that defines how quickly the estimate changes during the training.
- the system By repeatedly performing the process 200 , the system repeatedly updates the model parameters, resulting in trained machine learning model that both preserves user privacy and performs well on the particular task for which the model is being trained.
- FIG. 3 is a flow diagram of an example process 300 for determining a privacy preserving noisy gradient for a given model parameter.
- the process 300 will be described as being performed by a system of one or more computers located in one or more locations.
- a model training system e.g., the model training system 100 of FIG. 1 , appropriately programmed, can perform the process 300 .
- the system can perform the process 300 for each model parameter of the machine learning model to generate a respective privacy preserving noisy gradient for each of the model parameters.
- the system computes an auxiliary transformation parameter for the model parameter from the standard deviation noisy gradient estimate for the given model parameter (step 302 ).
- auxiliary transformation parameter b i t for the model parameter i at iteration t of the process 200 is equal to or, more generally, directly proportional to:
- s i t is the standard deviation noisy gradient estimate for the model parameter i and d is the total number of model parameters.
- the system transforms the gradient for the given model parameter based on the mean noisy gradient estimate for the model parameter and the auxiliary transformation parameter for the model parameter to generate a transformed gradient (step 304 ).
- the system subtracts the mean noisy gradient estimate for the model parameter from the gradient for the model parameter to generate a difference and divides the difference by the auxiliary transformation parameter to generate the transformed gradient.
- the system clips the transformed gradient for the given model parameter such that a vector of the transformed gradients, i.e., a vector that includes the transformed gradients for all of the model parameters, has no greater than a fixed norm, e.g., 1 , to generate clipped transformed gradients (step 306 ).
- a vector of the transformed gradients i.e., a vector that includes the transformed gradients for all of the model parameters
- a fixed norm e.g. 1
- the system divides each transformed gradient by the norm to generate a corresponding clipped transformed gradient. This results in a vector of the clipped transformed gradients having norm 1 . If the norm of the gradient vector does not exceed 1, the system does not modify the transformed gradients and sets the clipped transformed gradients equal to the transformed gradients.
- the clipping scheme described above is both coordinate-wise, i.e., per-parameter, and adaptive. That is, because before clipping is applied the system generates transformed gradients using respective mean and standard deviation estimates that can differ between model parameters, the clipping is coordinate-wise. Because, as described above, the mean and standard deviation estimates are updated throughout training, the clipping is adaptive, i.e., the clipping scheme is not fixed prior to training.
- the system then adds noise to the transformed gradient to generate a noisy clipped transformed gradient for the given model parameter (step 308 ).
- the system can sample, for each model parameter, a respective noise value from a noise distribution that is defined by a noise scale value and then add the sampled noise value to the clipped transformed gradient for the model parameter to generate the noisy clipped transformed gradient.
- the noise distribution can be, e.g., a Normal distribution with zero mean and a standard deviation equal to the noise scale value.
- the system is able to add less noise to the gradient for each model parameter while still maintaining user privacy.
- the system can use a smaller noise scale value than other approaches while still maintaining user privacy. Adding less noise to the gradients results in more accurate model parameter updates and therefore reduces the training time required for training the model, results in a higher performing trained model, or both.
- the system then rescales the noisy clipped transformed gradient based on the mean noisy gradient estimate for the given model parameter and the auxiliary transformation parameter for the model parameter to generate the privacy preserving gradient (step 310 ).
- the system multiplies the noisy clipped transformed gradient by the auxiliary transformation parameter to generate a product and adds the mean noisy gradient estimate for the model parameter to the product. This rescaling results in the privacy preserving gradient having the same scale as the original gradient, i.e., the gradient of the objective function with respect to the model parameters.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations.
- the index database can include multiple collections of data, each of which may be organized and accessed differently.
- engine is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions.
- an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- the central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser.
- a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- a machine learning framework e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received at the server from the device.
Abstract
Methods, systems, and apparatus, including computer programs encoded on computer storage media, for privacy preserving training of a machine learning model.
Description
- This application claims priority to U.S. Provisional Application No. 62/886,889, filed on Aug. 14, 2020. The disclosure of the prior application is considered part of and is incorporated by reference in the disclosure of this application.
- This specification relates to training machine learning models, e.g., neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer. Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that trains a machine learning model, e.g., a neural network, to perform a particular task. In particular, the system trains the machine learning model on training data that includes user data from multiple users in a manner that preserves the privacy of the users.
- Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages.
- By training machine learning models on training data that is derived from user data as described in this specification, the system can train a machine learning model to perform well on any of a variety of machine learning tasks while maintaining the privacy of the users from whom the training data was derived.
- In particular, the system can train the machine learning model to have improved performance on any of a variety of machine learning tasks relative to other training mechanisms that preserve the privacy of user data while still maintaining privacy equally as well or better than the conventional techniques.
- By determining privacy preserving noisy gradients for each model parameter using coordinate-wise, i.e., per-parameter, adaptive clipping, the system is able to add less noise to the gradient for each model parameter while still maintaining user privacy. Adding less noise to gradients during training results in the model performing better after training. Additionally, adding less noise to gradients can result in faster convergence of the model parameters, decreasing the amount of time and computational resources required to train the model. More specifically, the system achieves this adaptive clipping by, for each parameter, modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter. These estimates are repeatedly updated during training, resulting in a scheme that is both adaptive and coordinate-wise.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1 shows an example model training system. -
FIG. 2 is a flow diagram of an example process for training the machine learning model. -
FIG. 3 is a flow diagram of an example process for determining a privacy preserving noisy gradient for a given model parameter. - Like reference numbers and designations in the various drawings indicate like elements.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that trains a machine learning model that is configured to perform a particular machine learning task.
- The machine learning model can be trained to perform any kind of machine learning task, i.e., can be configured to receive any kind of digital data input and to generate any kind of score, classification, or regression output based on the input.
- In some cases, the machine learning model is a neural network that is configured to perform an image processing task, i.e., receive an input image and to process the input image to generate a model output for the input image. For example, the task may be image classification and the output generated by the machine learning model for a given image may be scores for each of a set of object categories, with each score representing an estimated likelihood that the image contains an image of an object belonging to the category. As another example, the task can be image embedding generation and the output generated by the machine learning model can be a numeric embedding of the input image. As yet another example, the task can be object detection and the output generated by the machine learning model can identify locations in the input image at which particular types of objects are depicted. As yet another example, the task can be image segmentation and the output generated by the machine learning model can assign each pixel of the input image to a category from a set of categories.
- As another example, if the inputs to the machine learning model are Internet resources (e.g., web pages), documents, or portions of documents or features extracted from Internet resources, documents, or portions of documents, the task can be to classify the resource or document, i.e., the output generated by the machine learning model for a given Internet resource, document, or portion of a document may be a score for each of a set of topics, with each score representing an estimated likelihood that the Internet resource, document, or document portion is about the topic.
- As another example, if the inputs to the machine learning model are features of an impression context for a particular advertisement, the output generated by the machine learning model may be a score that represents an estimated likelihood that the particular advertisement will be clicked on.
- As another example, if the inputs to the machine learning model are features of a personalized recommendation for a user, e.g., features characterizing the context for the recommendation, e.g., features characterizing previous actions taken by the user, the output generated by the machine learning model may be a score for each of a set of content items, with each score representing an estimated likelihood that the user will respond favorably to being recommended the content item.
- As another example, if the input to the machine learning model is a sequence of text in one language, the output generated by the machine learning model may be a score for each of a set of pieces of text in another language, with each score representing an estimated likelihood that the piece of text in the other language is a proper translation of the input text into the other language.
- As another example, the task may be an audio processing task. For example, if the input to the machine learning model is a sequence representing a spoken utterance, the output generated by the machine learning model may be a score for each of a set of pieces of text, each score representing an estimated likelihood that the piece of text is the correct transcript for the utterance. As another example, the task may be a keyword spotting task where, if the input to the machine learning model is a sequence representing a spoken utterance, the output generated by the machine learning model can indicate whether a particular word or phrase (“hotword”) was spoken in the utterance. As another example, if the input to the machine learning model is a sequence representing a spoken utterance, the output generated by the machine learning model can identify the natural language in which the utterance was spoken.
- As another example, the task can be a natural language processing or understanding task, e.g., an entailment task, a paraphrase task, a textual similarity task, a sentiment task, a sentence completion task, a grammaticality task, and so on, that operates on a sequence of text in some natural language.
- As another example, the task can be a text to speech task, where the input is text in a natural language or features of text in a natural language and the model output is a spectrogram or other data defining audio of the text being spoken in the natural language.
- As another example, the task can be a health prediction task, where the input is electronic health record data for a patient and the output is a prediction that is relevant to the future health of the patient, e.g., a predicted treatment that should be prescribed to the patient, the likelihood that an adverse health event will occur to the patient, or a predicted diagnosis for the patient.
- As another example, the task can be an agent control task, where the input is an observation characterizing the state of an environment and the output defines an action to be performed by the agent in response to the observation. The agent can be, e.g., a real-world or simulated robot, a control system for an industrial facility, or a control system that controls a different kind of agent.
- Many of these tasks require the machine learning model to be trained on user data, i.e., data that is provided by a user, e.g., a label for an image that is provided by the user, data that is specific to a particular user, e.g., text written by a particular user or a medical record of a user, or that is generated as a result of user interaction with a system, e.g., search queries submitted by the user to a search engine or an interaction or selection history for a user that is based on interactions with content items by the user.
- This user data can include sensitive information that users may not wish to make public, e.g., data from typing histories, social networks, financial records, or medical records, and that users may not wish to be made transparent through predictions made by the model.
- Accordingly, the system trains the machine learning model in a manner that preserves the privacy of each of the users from which the training data for the model is derived. In particular, the system trains the machine learning model so that the outputs generated by the trained machine learning model are, to a specified degree, indistinguishable whether or not any given user's training data is included in the training data for the model. That is, the system performs the training such that, for any given user associated with any data in the training data set, outputs generated by the trained model are, to a specified degree, indistinguishable between the model being trained on (i) the entire training data set minus the data associated with the user or (ii) the entire training data set. The system accomplishes this by, for each batch of training examples that is used during training, determining respective privacy preserving gradients for each model parameter and using these privacy preserving gradients to update the model parameters.
- In some implementations, user data may be treated in one or more ways before it is stored or used for training the machine learning model, so that personally identifiable information is removed. For example, a user's identity may be treated so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.
-
FIG. 1 shows an examplemodel training system 100. Themodel training system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented. - The
model training system 100 is a system that obtainstraining data 102 for training amachine learning model 110 to perform a particular task. Thetraining data 102 includes user data, i.e., at least some of the training examples in thetraining data 102 are derived from users, i.e., are generated from data that is provided by a user, data that is specific to a particular user, or data that is generated as a result of user interaction with a system. - Generally, the
training data 102 includes a set of training examples, with each training example including a training input and, for each training input, a respective target output that should be generated by the machine learning model to perform the particular task. - The
system 100 can receive thetraining data 102 in any of a variety of ways. For example, thesystem 100 can receive training data as uploads from users of the system over a data communication network, e.g., using an application programming interface (API) made available by thesystem 100. As another example, thesystem 100 can receive an input from a user specifying which data that is already maintained by thesystem 100 should be used for training the machine learning model. - The
machine learning model 110 is a model, e.g., a neural network, having a set of parameters (“model parameters”) and that is configured to process model inputs in accordance with the model parameters to generate an output for the particular task. - The
machine learning model 110 can have any appropriate architecture that allows themodel 110 to receive model inputs of the type required by the particular task and to generate model outputs of the form required for the particular task. Examples ofmachine learning models 110 that can be trained by thesystem 100 include fully-connected neural networks, convolutional neural networks, recurrent neural networks, attention-based neural networks, e.g., Transformers, and so on. - Generally, a
training engine 120 within thesystem 100 trains themachine learning model 110 on thetraining data 102 over multiple training steps using an iterative gradient descent training technique. - In a typical gradient descent training technique, at each training step, the
training engine 120 would computerespective gradients 122 of an objective function with respect to each of the model parameters and on a batch of training examples selected from thetraining data 102, apply a learning rate to the computedgradients 122 to determine a parameter value update, and then apply the parameter value update to the current values of the model parameters of the machine learning model, i.e., by subtracting or adding the parameter value update with the current parameter values. - However, training the machine learning model in this manner may not result in the privacy of the users that provided the user data being preserved. For example, information related to a given user may be recoverable from the trained model because the model output for any given model input may be different depending on whether the given user's data was included in the training data or not.
- Therefore, instead of directly using the
gradients 122 to update the model parameters, at each iteration of the training process aprivacy preserving engine 130 within thesystem 100 first applies coordinate-wise, i.e., per-parameter, adaptive clipping to eachgradient 122 to determine respective privacy preservingnoisy gradients 132 for each of the model parameters. - The
training engine 120 then updates the model parameters, i.e., determines updatedmodel parameters 124, based on the privacy preservingnoisy gradients 132 instead of directly using thegradients 122. - Computing the privacy preserving
noisy gradients 132 and using the privacy preserving gradients to update the model parameters is described in more detail below with reference toFIGS. 2 and 3 . - By repeatedly performing training steps, the
training engine 120 repeatedly updates the values of the model parameters to improve the performance of themachine learning model 110 on the particular task. - The manner in which the
training engine 120 determines the parameter value update, i.e., how thetraining engine 120 applies the learning rate to any given privacy preservingnoisy gradient 132, is dependent on the optimizer that is being used in the training. - For example, in stochastic gradient descent, the update is a product of the learning rate and the gradient.
- As another example, in the Adam optimizer, the update is a product of the learning rate and an exponentially decayed average of past gradients.
- As another example, in the Adagrad optimizer, the system first adapts the learning rate per weight, i.e., per model parameter, based on the sums of the squares of the gradients and then computes, for each model parameter, a product of the gradient with respect to the parameter and the adapted learning rate.
- In some implementations, after the
machine learning model 110 has been trained, thesystem 100 deploys the trained machine learning model and then uses the trained model to process requests received from users, e.g., through the API provided by the system. In other words, after training, the system uses the trainedmachine learning model 110 to generate new model outputs for new model inputs. - Instead of or in addition to using the trained
machine learning model 110, thesystem 100 can provide data specifying the final model parameter values to a user who submitted a request to train the machine learning model or to the users from whom the training data was derived, e.g., through the API. -
FIG. 2 is a flow diagram of anexample process 200 for training the machine learning model. For convenience, theprocess 200 will be described as being performed by a system of one or more computers located in one or more locations. For example, a model training system, e.g., themodel training system 100 ofFIG. 1 , appropriately programmed, can perform theprocess 200. - The system can repeatedly perform the
process 200 on different batches of training data to determine trained values of the model parameters, i.e., by repeatedly updating the current values of the model parameters. For example, the system can continue performing theprocess 200 until a threshold number of iterations of the process have been performed, until a threshold amount of time has elapsed, or until the values of the model parameters have converged. - Prior to performing the
process 200, the system initializes the model parameters and, for each model parameter, a respective mean noisy gradient estimate and a respective standard deviation noisy gradient estimate. - The system can initialize the model parameters using a machine learning parameter initialization technique, e.g., random initialization.
- As will be described in more detail below, the system will use the mean noisy gradient estimates and the standard deviation noisy gradient estimates when adaptively clipping gradients to update the model parameters during training of the model.
- As a particular example, the system can initialize the mean noisy gradient estimate for each of the model parameters to a fixed value, e.g., zero.
- As another particular example, the system can initialize the standard deviation noisy gradient estimate for each of the model parameters based on predetermined maximum and minimum values for the standard deviation of the noisy gradients for the model parameters. For example, the system can initialize the standard deviation noisy gradient estimate for each of the model parameters to be equal to the square root of the product of the predetermined maximum value and the predetermined minimum value. These predetermined maximum and minimum values will then be used when updating the standard deviation noisy gradient estimate to ensure that the estimate stays remains within the range defined by these values.
- The system obtains a batch of training examples, i.e., a plurality of training examples that include user data from multiple different users (step 202). For example, each training example in the batch can be derived from a different user.
- The system computes, for each of the training examples and for each of the model parameters, a respective gradient of an objective function with respect to the model parameter (step 204). The objective function can be any appropriate objective function for the particular task that the model is being trained to perform and generally measures errors between an output generated by the machine learning model for a training input and a target output that should have been generated by the machine learning model for the training input. Examples of objective functions that may be appropriate for various tasks include cross-entropy losses, mean squared error losses, L2 distance losses, log likelihood objectives, and so on. Thus, for any given model parameter, the system computes a respective gradient for each training example in the batch.
- The system determines, for each of the training examples and for each of the model parameters, a respective privacy preserving noisy gradient (step 206).
- In particular, for a given training example and a given model parameter, the system modifies the respective gradient for the model parameter based on (i) the mean noisy gradient estimate for the model parameter and (ii) the standard deviation noisy gradient estimate for the model parameter to generate a privacy preserving noisy gradient.
- Generally, to perform the modification, the system adaptively clips the respective gradient for the model parameter based on (i) the mean noisy gradient estimate for the model parameter and (ii) the standard deviation noisy gradient estimate for the model parameter and then adds noise to the adaptively clipped gradient.
- Performing this modification is described in more detail below with reference to
FIG. 3 . - The system determines, for each of the model parameters, a respective privacy preserving update for the model parameter from the privacy preserving noisy gradients for the model parameter for the plurality of training examples (step 208).
- To determine the privacy preserving update for a given model parameter, the system computes an average noisy gradient for the given model parameter by averaging the privacy preserving noisy gradients for the model parameter for the plurality of training examples.
- For each model parameter, the system then applies a current learning rate to the average noisy gradient to generate the privacy preserving update for the model parameter. As described above, the manner in which the system generates the privacy preserving update will depend on the optimizer that is being used for the training.
- For example, in stochastic gradient descent the system can multiply the gradient by the current learning rate to generate the update.
- As another example, when using the Adam optimizer the system can determine a modified gradient based on the current gradient and one or more recently computed gradients and then multiply the modified gradient by the current learning rate to generate the update.
- The system updates the model parameters based on the privacy preserving updates (step 210). For example, the system can, for each model parameter, subtract the privacy preserving update for the model parameter from the model parameter to generate an updated model parameter.
- The system updates the mean noisy gradient estimate for each of the plurality of model parameters (step 212).
- In particular, the system updates the mean noisy gradient estimate for the model parameter based on the privacy preserving noisy gradients for the model parameter by interpolating between the mean noisy gradient estimate and the average privacy preserving noisy gradient for the model parameter for the plurality of training examples in accordance with a mean interpolation weight hyperparameter.
- That is, the system the updated mean noisy gradient estimate is an interpolation between (i) the currently maintained mean noisy gradient estimate and (ii) the average privacy preserving noisy gradient for the model parameter for the plurality of training examples, where the weight assigned to (i) and (ii) in the interpolation is governed by a mean interpolation weight hyperparameter that defines how quickly the estimate changes during the training.
- The system updates the standard deviation noisy gradient estimate for each of the plurality of model parameters (step 214). In particular, the system updates the standard deviation noisy gradient estimate for any given model parameter based on the mean estimate for the given model parameter and the privacy preserving noisy gradients for the given model parameter. More specifically, the system computes an exponential moving average using these quantities in order to determine the updated standard deviation noisy gradient estimate for the given model parameter.
- To update the standard deviation noisy gradient estimate, the system first computes an initial updated estimate for the given model parameter that is bounded below by the predetermined minimum value for the standard deviation and bounded above by the predetermined maximum value for the standard deviation.
- To compute the initial updated estimate, the system computes the difference between (i) a squared distance between the mean estimate for the given model parameter and the average privacy preserving noisy gradient for the given model parameter and (ii) the product of an auxiliary transformation parameter for the given model parameter (that is generated from the current noisy standard deviation estimate for the model parameter) used when clipping gradients and a noise scale used to apply noise to the clipped gradients.
- The auxiliary transformation parameter for the given model parameter and the noise scale will be described in more detail below with reference to
FIG. 3 . - If the difference is not less than the predetermined minimum value for the standard deviation and not greater than the predetermined maximum value for the standard deviation, the system sets the initial updated estimate equal to the difference.
- Alternatively, if the difference is less than the predetermined minimum value for the standard deviation, the system sets the initial updated estimate equal to the predetermined minimum value. Similarly, if the initial updated estimate is greater than the predetermined maximum value for the standard deviation, the system sets the initial updated estimate equal to the predetermined maximum value.
- The system then computes the exponential moving average by interpolating between the square of the current standard deviation noisy gradient estimate and the initial updated estimate for the given model parameter, with the interpolation weights being governed by a standard deviation interpolation weight hyperparameter that defines how quickly the estimate changes during the training.
- By repeatedly performing the
process 200, the system repeatedly updates the model parameters, resulting in trained machine learning model that both preserves user privacy and performs well on the particular task for which the model is being trained. -
FIG. 3 is a flow diagram of anexample process 300 for determining a privacy preserving noisy gradient for a given model parameter. For convenience, theprocess 300 will be described as being performed by a system of one or more computers located in one or more locations. For example, a model training system, e.g., themodel training system 100 ofFIG. 1 , appropriately programmed, can perform theprocess 300. - The system can perform the
process 300 for each model parameter of the machine learning model to generate a respective privacy preserving noisy gradient for each of the model parameters. - The system computes an auxiliary transformation parameter for the model parameter from the standard deviation noisy gradient estimate for the given model parameter (step 302).
- In particular, the auxiliary transformation parameter bi t for the model parameter i at iteration t of the
process 200, i.e., the t-th training step, is equal to or, more generally, directly proportional to: -
√{square root over (s i t)}×√{square root over (Σi=1 d s i t)}, - where si t is the standard deviation noisy gradient estimate for the model parameter i and d is the total number of model parameters.
- The system transforms the gradient for the given model parameter based on the mean noisy gradient estimate for the model parameter and the auxiliary transformation parameter for the model parameter to generate a transformed gradient (step 304). In particular, the system subtracts the mean noisy gradient estimate for the model parameter from the gradient for the model parameter to generate a difference and divides the difference by the auxiliary transformation parameter to generate the transformed gradient.
- The system clips the transformed gradient for the given model parameter such that a vector of the transformed gradients, i.e., a vector that includes the transformed gradients for all of the model parameters, has no greater than a fixed norm, e.g., 1, to generate clipped transformed gradients (step 306). In particular, if the norm of the gradient vector exceeds 1, the system divides each transformed gradient by the norm to generate a corresponding clipped transformed gradient. This results in a vector of the clipped transformed gradients having norm 1. If the norm of the gradient vector does not exceed 1, the system does not modify the transformed gradients and sets the clipped transformed gradients equal to the transformed gradients.
- The clipping scheme described above is both coordinate-wise, i.e., per-parameter, and adaptive. That is, because before clipping is applied the system generates transformed gradients using respective mean and standard deviation estimates that can differ between model parameters, the clipping is coordinate-wise. Because, as described above, the mean and standard deviation estimates are updated throughout training, the clipping is adaptive, i.e., the clipping scheme is not fixed prior to training.
- The system then adds noise to the transformed gradient to generate a noisy clipped transformed gradient for the given model parameter (step 308). In particular, the system can sample, for each model parameter, a respective noise value from a noise distribution that is defined by a noise scale value and then add the sampled noise value to the clipped transformed gradient for the model parameter to generate the noisy clipped transformed gradient. The noise distribution can be, e.g., a Normal distribution with zero mean and a standard deviation equal to the noise scale value.
- Because of the coordinate-wise, i.e., per-parameter, and adaptive clipping scheme, the system is able to add less noise to the gradient for each model parameter while still maintaining user privacy. In other words, the system can use a smaller noise scale value than other approaches while still maintaining user privacy. Adding less noise to the gradients results in more accurate model parameter updates and therefore reduces the training time required for training the model, results in a higher performing trained model, or both.
- The system then rescales the noisy clipped transformed gradient based on the mean noisy gradient estimate for the given model parameter and the auxiliary transformation parameter for the model parameter to generate the privacy preserving gradient (step 310). In particular, to rescale the noisy clipped transformed gradient, the system multiplies the noisy clipped transformed gradient by the auxiliary transformation parameter to generate a product and adds the mean noisy gradient estimate for the model parameter to the product. This rescaling results in the privacy preserving gradient having the same scale as the original gradient, i.e., the gradient of the objective function with respect to the model parameters.
- This specification uses the term “configured” in connection with systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program, which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- In this specification, the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations. Thus, for example, the index database can include multiple collections of data, each of which may be organized and accessed differently.
- Similarly, in this specification the term “engine” is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. Generally, an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser. Also, a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received at the server from the device.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially be claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings and recited in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (20)
1. A method of training of a machine learning model having a plurality of model parameters on user data from a plurality of users while preserving privacy of the user data, the method comprising repeatedly performing the following:
obtaining a plurality of training examples comprising data from a plurality of users;
for each of the plurality of training examples:
computing, for the training example and for each of the plurality of model parameters, a respective gradient of an objective function with respect to the model parameter; and
determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter;
determining, for each of the model parameters, a respective privacy preserving update for the model parameter from the privacy preserving noisy gradients for the model parameter for the plurality of training examples;
updating the model parameters based on the privacy preserving updates;
for each of the plurality of model parameters:
updating the standard deviation noisy gradient estimate for the model parameter based on the mean estimate for the model parameter and the privacy preserving noisy gradients for the model parameter; and
updating the mean noisy gradient estimate for the model parameter based on the privacy preserving noisy gradients for the model parameter.
2. The method of claim 1 , further comprising:
initializing the mean noisy gradient estimate for each of the model parameters to a fixed value.
3. The method of claim 1 , further comprising:
initializing the standard deviation noisy gradient estimate for each of the model parameters based on predetermined maximum and minimum values for a standard deviation of the noisy gradients for the model parameters.
4. The method of claim 1 , further comprising:
initializing the model parameters using a machine learning parameter initialization technique.
5. The method of claim 1 , wherein updating the mean noisy gradient estimate for the model parameter based on the privacy preserving noisy gradients for the model parameter comprises:
interpolating between the mean noisy gradient estimate and an average privacy preserving noisy gradient for the model parameter for the plurality of training examples in accordance with a mean interpolation weight hyperparameter.
6. The method of claim 1 , wherein determining, for each of the model parameters, a respective privacy preserving update for the model parameter from the privacy preserving noisy gradients for the model parameter for the plurality of training examples comprises:
computing an average noisy gradient for the model parameter from the privacy preserving noisy gradients for the model parameter for the plurality of training examples; and
applying a learning rate to the average noisy gradient for the model parameter to generate the privacy preserving update for the model parameter.
7. The method of claim 1 , wherein updating the model parameters based on the privacy preserving updates comprises:
for each model parameter, subtracting the privacy preserving update for the model parameter from the model parameter.
8. The method of claim 1 , wherein determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter comprises:
adaptively clipping the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter; and
adding noise to the adaptively clipped gradient.
9. The method of claim 1 , wherein determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter comprises
for each of the plurality of model parameters:
computing an auxiliary transformation parameter for the model parameter from the standard deviation noisy gradient estimate for the model parameter; and
transforming the gradient for the model parameter based on the mean noisy gradient estimate for the model parameter and the auxiliary transformation parameter for the model parameter to generate a transformed gradient.
10. The method of claim 9 , wherein determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter further comprises:
clipping the transformed gradients for the model parameters such that a vector of the transformed gradients has a fixed norm to generate clipped transformed gradients; and
adding noise to each transformed gradient to generate noisy clipped transformed gradients.
11. The method of claim 10 , wherein determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter further comprises:
for each of the plurality of model parameters:
rescaling the noisy clipped transformed gradient based on the mean noisy gradient estimate for the model parameter and the auxiliary transformation parameter for the model parameter to generate a transformed gradient.
12. The method of claim 9 , wherein transforming the gradient for the model parameter based on the mean noisy gradient estimate for the model parameter and the auxiliary transformation parameter for the model parameter to generate a transformed gradient comprises:
subtracting the mean noisy gradient estimate for the model parameter from the gradient for the model parameter to generate a difference; and
dividing the difference by the auxiliary transformation parameter to generate the transformed gradient.
13. The method of claim 11 , wherein rescaling the noisy clipped transformed gradient based on the mean noisy gradient estimate for the model parameter and the auxiliary transformation parameter for the model parameter comprises:
multiplying the noisy clipped transformed gradient by the auxiliary transformation parameter to generate a product; and
adding the mean noisy gradient estimate for the model parameter to the product.
14. The method of claim 9 , wherein the auxiliary transformation parameter for the model parameter i is directly proportional to:
√{square root over (s i t)}×√{square root over (Σi=1 d s i t)},
√{square root over (s i t)}×√{square root over (Σi=1 d s i t)},
where si t is the standard deviation noisy gradient estimate for the model parameter i and d is the total number of model parameters.
15. The method of claim 9 , wherein updating the standard deviation noisy gradient estimate for the model parameter based on the mean estimate for the model parameter and the privacy preserving noisy gradients for the model parameter comprises:
determining an initial updated estimate using the mean estimate for the model parameter and the privacy preserving noisy gradients for the model parameter; and
determining an interpolation between the standard deviation noisy gradient estimate and the initial updated estimate.
16. One or more non-transitory computer-readable storage media storing instructions that when executed by one or more computers cause the one or more computers to perform operations for training of a machine learning model having a plurality of model parameters on user data from a plurality of users while preserving privacy of the user data, the operations comprising repeatedly performing the following:
obtaining a plurality of training examples comprising data from a plurality of users;
for each of the plurality of training examples:
computing, for the training example and for each of the plurality of model parameters, a respective gradient of an objective function with respect to the model parameter; and
determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter;
determining, for each of the model parameters, a respective privacy preserving update for the model parameter from the privacy preserving noisy gradients for the model parameter for the plurality of training examples;
updating the model parameters based on the privacy preserving updates;
for each of the plurality of model parameters:
updating the standard deviation noisy gradient estimate for the model parameter based on the mean estimate for the model parameter and the privacy preserving noisy gradients for the model parameter; and
updating the mean noisy gradient estimate for the model parameter based on the privacy preserving noisy gradients for the model parameter.
17. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to perform operations for training of a machine learning model having a plurality of model parameters on user data from a plurality of users while preserving privacy of the user data, the operations comprising repeatedly performing the following:
obtaining a plurality of training examples comprising data from a plurality of users;
for each of the plurality of training examples:
computing, for the training example and for each of the plurality of model parameters, a respective gradient of an objective function with respect to the model parameter; and
determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter;
determining, for each of the model parameters, a respective privacy preserving update for the model parameter from the privacy preserving noisy gradients for the model parameter for the plurality of training examples;
updating the model parameters based on the privacy preserving updates;
for each of the plurality of model parameters:
updating the standard deviation noisy gradient estimate for the model parameter based on the mean estimate for the model parameter and the privacy preserving noisy gradients for the model parameter; and
updating the mean noisy gradient estimate for the model parameter based on the privacy preserving noisy gradients for the model parameter.
18. The system of claim 17 , wherein determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter comprises
for each of the plurality of model parameters:
computing an auxiliary transformation parameter for the model parameter from the standard deviation noisy gradient estimate for the model parameter; and
transforming the gradient for the model parameter based on the mean noisy gradient estimate for the model parameter and the auxiliary transformation parameter for the model parameter to generate a transformed gradient.
19. The system of claim 18 , wherein determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter further comprises:
clipping the transformed gradients for the model parameters such that a vector of the transformed gradients has a fixed norm to generate clipped transformed gradients; and
adding noise to each transformed gradient to generate noisy clipped transformed gradients.
20. The system of claim 19 , wherein determining, for each of the plurality of model parameters, a respective privacy preserving noisy gradient by modifying the respective gradient for the model parameter based on (i) a mean noisy gradient estimate for the model parameter and (ii) a standard deviation noisy gradient estimate for the model parameter further comprises:
for each of the plurality of model parameters:
rescaling the noisy clipped transformed gradient based on the mean noisy gradient estimate for the model parameter and the auxiliary transformation parameter for the model parameter to generate a transformed gradient.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/994,396 US20210049298A1 (en) | 2019-08-14 | 2020-08-14 | Privacy preserving machine learning model training |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962886889P | 2019-08-14 | 2019-08-14 | |
US16/994,396 US20210049298A1 (en) | 2019-08-14 | 2020-08-14 | Privacy preserving machine learning model training |
Publications (1)
Publication Number | Publication Date |
---|---|
US20210049298A1 true US20210049298A1 (en) | 2021-02-18 |
Family
ID=74566731
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/994,396 Pending US20210049298A1 (en) | 2019-08-14 | 2020-08-14 | Privacy preserving machine learning model training |
Country Status (1)
Country | Link |
---|---|
US (1) | US20210049298A1 (en) |
Cited By (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113032835A (en) * | 2021-04-21 | 2021-06-25 | 支付宝(杭州)信息技术有限公司 | Privacy protection model training method, system and device |
CN113435583A (en) * | 2021-07-05 | 2021-09-24 | 平安科技（深圳）有限公司 | Countermeasure generation network model training method based on federal learning and related equipment thereof |
CN113449318A (en) * | 2021-06-18 | 2021-09-28 | 北京明朝万达科技股份有限公司 | Data classification model training method and device, and data classification method and device |
US20210325837A1 (en) * | 2020-04-20 | 2021-10-21 | Kabushiki Kaisha Toshiba | Information processing apparatus, information processing method and computer program product |
CN113961967A (en) * | 2021-12-13 | 2022-01-21 | 支付宝(杭州)信息技术有限公司 | Method and device for jointly training natural language processing model based on privacy protection |
CN116257688A (en) * | 2023-03-14 | 2023-06-13 | 广东电力交易中心有限责任公司 | Information recommendation method based on differential privacy random gradient descent |
US11790227B1 (en) * | 2020-01-16 | 2023-10-17 | Educational Testing Service | Systems and methods for neural content scoring |
WO2024006007A1 (en) * | 2022-06-26 | 2024-01-04 | Google Llc | Privacy-sensitive neural network training |
CN117419828A (en) * | 2023-12-18 | 2024-01-19 | 南京品傲光电科技有限公司 | New energy battery temperature monitoring method based on optical fiber sensor |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190188402A1 (en) * | 2017-12-18 | 2019-06-20 | Mitsubishi Electric Research Laboratories, Inc. | Data-driven privacy-preserving communication |
US20200311520A1 (en) * | 2019-03-29 | 2020-10-01 | International Business Machines Corporation | Training machine learning model |
-
2020
- 2020-08-14 US US16/994,396 patent/US20210049298A1/en active Pending
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190188402A1 (en) * | 2017-12-18 | 2019-06-20 | Mitsubishi Electric Research Laboratories, Inc. | Data-driven privacy-preserving communication |
US20200311520A1 (en) * | 2019-03-29 | 2020-10-01 | International Business Machines Corporation | Training machine learning model |
Non-Patent Citations (6)
Title |
---|
Abadi et al., "Deep Learning with Differential Privacy," arXiv (2016) (Year: 2016) * |
Dwork et al., "The Algorithmic Foundations of Differential Privacy," Foundations and Trends® in Theoretical Computer Science, vol. 9, nos. 3–4, pp. 1–277 (2013) (Year: 2013) * |
Garcia et al., "Semantic Noise: Privacy-Protection of Nominal Microdata through Uncorrelated Noise Addition," IEEE (2015) (Year: 2015) * |
McMahan et al., "Learning Differentially Private Recurrent Language Models," arXiv (Feb 2018) (Year: 2018) * |
Xiping Liu, "Distributed Variational Inference and Privacy," University of Cambridge (12 Aug 2019) (Year: 2019) * |
Zhang et al., "Analysis of Gradient Clipping and Adaptive Scaling with a Relaxed Smoothness Condition," arXiv (May 2019) (Year: 2019) * |
Cited By (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11790227B1 (en) * | 2020-01-16 | 2023-10-17 | Educational Testing Service | Systems and methods for neural content scoring |
US20210325837A1 (en) * | 2020-04-20 | 2021-10-21 | Kabushiki Kaisha Toshiba | Information processing apparatus, information processing method and computer program product |
US11754985B2 (en) * | 2020-04-20 | 2023-09-12 | Kabushiki Kaisha Toshiba | Information processing apparatus, information processing method and computer program product |
CN113032835A (en) * | 2021-04-21 | 2021-06-25 | 支付宝(杭州)信息技术有限公司 | Privacy protection model training method, system and device |
CN113449318A (en) * | 2021-06-18 | 2021-09-28 | 北京明朝万达科技股份有限公司 | Data classification model training method and device, and data classification method and device |
CN113435583A (en) * | 2021-07-05 | 2021-09-24 | 平安科技（深圳）有限公司 | Countermeasure generation network model training method based on federal learning and related equipment thereof |
CN113961967A (en) * | 2021-12-13 | 2022-01-21 | 支付宝(杭州)信息技术有限公司 | Method and device for jointly training natural language processing model based on privacy protection |
WO2024006007A1 (en) * | 2022-06-26 | 2024-01-04 | Google Llc | Privacy-sensitive neural network training |
CN116257688A (en) * | 2023-03-14 | 2023-06-13 | 广东电力交易中心有限责任公司 | Information recommendation method based on differential privacy random gradient descent |
CN117419828A (en) * | 2023-12-18 | 2024-01-19 | 南京品傲光电科技有限公司 | New energy battery temperature monitoring method based on optical fiber sensor |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20210049298A1 (en) | Privacy preserving machine learning model training | |
AU2022201819B2 (en) | Batch normalization layers | |
US11681924B2 (en) | Training neural networks using a variational information bottleneck | |
US11651218B1 (en) | Adversartail training of neural networks | |
US20200104678A1 (en) | Training optimizer neural networks | |
US20200057936A1 (en) | Semi-supervised training of neural networks | |
US11922281B2 (en) | Training machine learning models using teacher annealing | |
US20220092416A1 (en) | Neural architecture search through a graph search space | |
US20240127058A1 (en) | Training neural networks using priority queues | |
US20210034973A1 (en) | Training neural networks using learned adaptive learning rates | |
EP3371747A1 (en) | Augmenting neural networks with external memory | |
US20220230065A1 (en) | Semi-supervised training of machine learning models using label guessing | |
US20220188636A1 (en) | Meta pseudo-labels | |
US20220108149A1 (en) | Neural networks with pre-normalized layers or regularization normalization layers | |
US20220253713A1 (en) | Training neural networks using layer-wise losses | |
US20220253694A1 (en) | Training neural networks with reinitialization | |
US20220019856A1 (en) | Predicting neural network performance using neural network gaussian process | |
US20190294967A1 (en) | Circulant neural networks | |
US20230206030A1 (en) | Hyperparameter neural network ensembles | |
US20220129760A1 (en) | Training neural networks with label differential privacy | |
US20230107247A1 (en) | Neural networks with transformed activation function layers | |
US20240119366A1 (en) | Online training of machine learning models using bayesian inference over noise | |
US20230359895A1 (en) | Training neural networks using sign and momentum based optimizers | |
US11409991B2 (en) | Regularizing the training of convolutional neural networks | |
WO2023154491A1 (en) | Training neural networks using layerwise fisher approximations |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SURESH, ANANDA THEERTHA;YU, XINNAN;KUMAR, SANJIV;AND OTHERS;SIGNING DATES FROM 20200914 TO 20200919;REEL/FRAME:053856/0331 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |