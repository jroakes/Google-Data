US20220300550A1 - Visual Search via Free-Form Visual Feature Selection - Google Patents
Visual Search via Free-Form Visual Feature Selection Download PDFInfo
- Publication number
- US20220300550A1 US20220300550A1 US17/698,795 US202217698795A US2022300550A1 US 20220300550 A1 US20220300550 A1 US 20220300550A1 US 202217698795 A US202217698795 A US 202217698795A US 2022300550 A1 US2022300550 A1 US 2022300550A1
- Authority
- US
- United States
- Prior art keywords
- computing system
- visual
- user
- image
- free
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000000007 visual effect Effects 0.000 title claims abstract description 263
- 238000000034 method Methods 0.000 claims abstract description 54
- 230000004044 response Effects 0.000 claims description 14
- 238000012545 processing Methods 0.000 abstract description 17
- 230000008569 process Effects 0.000 abstract description 11
- 238000001514 detection method Methods 0.000 abstract description 4
- 238000012015 optical character recognition Methods 0.000 abstract description 4
- 238000010586 diagram Methods 0.000 description 6
- 238000004891 communication Methods 0.000 description 3
- 230000004075 alteration Effects 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 238000003708 edge detection Methods 0.000 description 2
- 239000012530 fluid Substances 0.000 description 2
- 239000002243 precursor Substances 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 241001465754 Metazoa Species 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 235000013339 cereals Nutrition 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 238000010205 computational analysis Methods 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 239000004744 fabric Substances 0.000 description 1
- 235000013305 food Nutrition 0.000 description 1
- 238000009432 framing Methods 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000003909 pattern recognition Methods 0.000 description 1
- 230000026676 system process Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/06—Buying, selling or leasing transactions
- G06Q30/0601—Electronic shopping [e-shopping]
- G06Q30/0623—Item investigation
- G06Q30/0625—Directed, with specific intent or strategy
- G06Q30/0627—Directed, with specific intent or strategy using item specifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/532—Query formulation, e.g. graphical querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5846—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using extracted text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/06—Buying, selling or leasing transactions
- G06Q30/0601—Electronic shopping [e-shopping]
- G06Q30/0641—Shopping interfaces
- G06Q30/0643—Graphical representation of items or shoppers
Definitions
- the present disclosure relates generally to systems and methods for processing visual search queries. More particularly, the present disclosure relates to a computer visual search system that leverages user input free-form selection of visual features to detect and recognize objects (and/or specific visual features thereof) in images included in a visual query to provide more personalized and/or intelligent search results.
- Text-based or term-based searching is a process where a user inputs a word or phrase into a search engine and receives a variety of results.
- Term-based queries require a user to explicitly provide search terms in the form of words, phrases, and/or other terms. Therefore, term-based queries are inherently limited by the text-based input modality and do not enable a user to search based on visual characteristics of imagery.
- visual query search systems can provide a user with search results in response to a visual query that includes one or more images.
- Computer visual analysis techniques can be used to detect and recognize objects in images.
- OCR optical character recognition
- object detection techniques e.g., machine learning-based approaches
- Content related to the detected objects can be provided to the user (e.g., a user that captured the image in which the object is detected or that otherwise submitted or is associated with the visual query).
- certain existing visual query systems have a number of drawbacks.
- current visual search query systems and methods may provide a user with results that may only relate to the visual query with respect to visual characteristics of the query image as a whole, such as the same general color scheme or depicting the same items/objects as the image(s) of the visual query.
- certain existing visual query systems focus exclusively on identifying other images that contain holistically similar visual characteristics to the query image(s) as a whole, which may fail to reflect the user's true search intent.
- One example aspect of the present disclosure is directed to a computer-implemented method for free-form user selection of visual features for visual search.
- the method comprises providing for display within a user interface, by a computing system comprising one or more computing devices, an image that depicts one or more objects.
- the method comprises receiving, by the computing system, a free-form user input to the user interface that selects a particular sub-portion of the one or more objects depicted by the image, wherein the particular sub-portion comprises one or more visual features.
- the method comprises providing to a visual search system, by the computing system, a visual search query that comprises the particular sub-portion of the object selected by the free-form user input.
- the method comprises, in response to the visual search query, the method comprises the computing system receiving from the visual search system a set of visual search results responsive to visual features included in the particular sub-portion of the one or more objects.
- the method comprises providing one or more of the set of visual search results to a user.
- the computing system comprises one or more processors.
- the computing system comprises one or more non-transitory computer-readable media that store instructions that, when executed by the one or more processors, cause the computing system to perform operations.
- the operations comprise providing for display within a user interface, by the computing system, an image that depicts one or more objects.
- the operations comprise receiving, by the computing system, a free-form user input to the user interface that indicates a particular sub-portion of the one or more objects depicted by the image, wherein the particular sub-portion comprises one or more visual features.
- the operations comprise providing to a visual search system, by the computing system, a visual search query that comprises the particular sub-portion of the object indicated by the free-form user input.
- the operations comprise, in response to the visual search query, receiving from the visual search system, by the computing system, a set of visual search results responsive to visual features included in the particular sub-portion of the one or more objects.
- the operations comprise providing, one or more of the set of visual search results to a user.
- the computing system comprises one or more processors.
- the computing system comprises one or more non-transitory computer-readable media that store instructions that, when executed by the one or more processors, cause the computing system to perform operations.
- the operations comprise obtaining a visual search query, wherein the visual search query comprises an image that depicts a particular sub-portion of an object that has been selected by a user.
- the operations comprise accessing visual embeddings associated with candidate results to identify a first set of results associated with the object overall and a second set of results associated with the particular sub-portion of the object.
- the operations comprise selecting, based on the visual search query, a combined set of content that includes search results from both the first set of results and the second set of results.
- the operations comprise, in response to the visual search query, returning the combined set of content as search results.
- FIG. 1 depicts a block diagram of an example computing system according to example embodiments of the present disclosure.
- FIG. 2 illustrates an example user interface illustrating steps a user may take to perform a free-form selection of a visual feature to a visual query, in accordance with some embodiments.
- FIG. 3 illustrates an example user interface illustrating steps a user may take to perform a free-form selection of a visual feature to a visual query, in accordance with some embodiments.
- FIG. 4 depicts a flow chart diagram of an example method to perform a more personalized and/or intelligent visual search leveraging free-form user input according to example embodiments of the present disclosure.
- FIG. 5 depicts a flow chart diagram of an example method to perform a more personalized and/or intelligent visual search using a combined set of content including results associated with the object overall and results associated with a particular sub-portion of the object, according to example embodiments of the present disclosure.
- the present disclosure is directed to a computer visual search system that leverages user input free-form selection of visual features to detect and recognize objects (and/or specific visual features thereof) in images included in a visual query to provide more personalized and/or intelligent search results.
- aspects of the present disclosure enable the visual search system to more intelligently process a visual query to provide improved search results, including search results which are more personalized or user-driven or user-defined.
- a computer visual search system can leverage free-form user input that selects one or more visual features depicted in an image.
- the search system can use the selected visual features to perform or refine a visual search to return results that are more specifically relevant to the selected visual features (e.g., as opposed to the image as a whole or specific semantic objects depicted in the image).
- a search system can provide a user with improved visual search results which are more directly relevant to specific, user-selected visual features.
- a visual query can include one or more images.
- the images included in the visual query can be contemporaneously captured imagery or can be previously existing images.
- a visual query can include a single image.
- a visual query can include ten image frames from approximately three seconds of video capture.
- a visual query can include a corpus of images such as, for example, all images included in a user's photo library.
- a visual search system can leverage free-form user selection of visual features to provide more personalized search results.
- the visual search system can use the free-form user selection of visual features to return a combined set of content responding to multiple visual features responsive to visual search queries.
- visual search queries enable a more expressive and fluid modality of search input, both understanding the granularity and object(s) of user intent is a challenging task.
- a user submits as a visual query a dress.
- the user could be interested in dresses which have one or more characteristics in common with the dress in the query image, such as length, color, sleeve type, collar style, fabric pattern, or some combination thereof.
- determining by a computing system which specific visual aspects a user is interested in given an image is a challenging problem.
- understanding the intended granularity of a user's query is challenging.
- a visual query that includes a dress with a brand logo on it may be intended to search for dresses that look like the dress in the image or other articles of clothing which are entirely different but produced by the same brand.
- the computing system can receive a free-form user input to a user interface. More particularly, the free-form user input to the user interface can select a particular sub-portion of the image. The particular sub-portion can comprise one or more visual features.
- a visual search query can be constructed or refined based on the user input. For example, the visual search query can include the particular sub-portion of the object, for example, the particular sub-portion of the object selected by the free-form user input.
- the computing system can receive from the visual search system a set of visual search results.
- the visual search results can be responsive to visual features, such as visual features included in the particular sub-portion of the one or more objects in the image.
- the computing system can then provide one or more of the set of visual search results to a user.
- the proposed system may return content related to dresses that are different in color and shape but have the same style of sleeves.
- an initial query image provided or selected by the user can be displayed within a user interface.
- the image can be displayed on a touch sensitive display device.
- the free-form user input can be received by the touch sensitive display device.
- the free-form user input to the user interface can be illustrated using a swathe of translucent color overlayed on a particular sub-portion of the object.
- the particular sub-portion of the object can be selected by free-form user input. Selecting the particular sub-portion of the object by free-form user input can indicate that the particular sub-portion of the one or more objects depicted by the image has been selected by the user.
- a user can drag a tactile object (e.g., finger, stylus, etc.) over the image provided for display within the user interface and, in response, the user interface can overlay the swathe of translucent color wherever the tactile object touches (e.g., in a highlighting manner).
- a tactile object e.g., finger, stylus, etc.
- a user can use any method of interacting with a display within a user interface (e.g., a mouse) to overlay the swathe of translucent color using any method known in the art (e.g., click and drag).
- a user interface e.g., a mouse
- the user can drag a finger across an image of a dress's sleeve to overlay a swathe of translucent color over the sleeve of the dress.
- the visual query may provide visual search results of dresses with the same style of sleeve.
- the free-form user input to the user interface can be a user input that selects a subset of pixels.
- the subset of pixels can be selected by the user from a plurality of pixels.
- the pixels can be specific image pixels or groups of image pixels that are grouped together.
- the plurality of pixels can be derived from dividing the image depicting one or more objects into the plurality of pixels.
- the subset of pixels selected by the user from the plurality of pixels can comprise at least two groups of selected pixels which are separate from each other.
- the subset of pixels selected by the user from the plurality of pixels can comprise at least two groups of selected pixels which are non-adjacent to each other.
- the particular subset of pixels can be selected by free-form user input. Selecting the particular sub-portion of the object by free-form user input can indicate that the particular sub-portion of the one or more objects depicted by the selected pixels in the image has been selected by the user.
- a user can drag a tactile object (e.g., finger, stylus, etc.) over the image provided for display within the user interface to indicate which pixels are part of the sub-portion of the image containing the visual feature of interest (e.g., pixels may indicate being selected by changing colors).
- a user can use any method of interacting with a display within a user interface (e.g., a mouse) to indicate which pixels should be selected using any method known in the art (e.g., click and drag).
- the user can drag a finger across an image of both of a dress's sleeves to select the pixels over both sleeves of the dress and nothing in between.
- the visual query may provide visual search results of dresses with the same style of sleeve.
- the user can drag a finger across an image of both of a dress's sleeves and a bow to select the pixels over both sleeves and the bow of the dress where the sleeves and the bow are not connected by any pixels.
- the visual query may provide visual search results of dresses with the same style of sleeve and a bow.
- the free-form user input to the user interface can be a line drawn in a loop around a particular sub-portion of the object.
- the particular sub-portion of the object can be selected by free-form user input. Selecting the particular sub-portion of the object by free-form user input can indicate that the particular sub-portion of the one or more objects depicted by the image has been selected by the user.
- a user can drag a tactile object (e.g., finger, stylus, etc.) over the image provided for display within the user interface to draw a line wherever the tactile object touches (e.g., as if drawing with a pen or pencil, click and slide to increase size of circle, etc.).
- a tactile object e.g., finger, stylus, etc.
- a user can use any method of interacting with a display within a user interface (e.g., a mouse) to draw a loop using any method known in the art (e.g., click and drag, click and slide, etc.).
- a user can drag a finger around an image of a dress's sleeve to draw a loop over the sleeve of the dress.
- the visual query may provide visual search results of dresses with the same style of sleeve.
- one or more initial visual feature suggestions may be provided by the computing system.
- one or more initial visual features may be indicated as suggested visual features for the user to select.
- the one or more initial visual features may be indicated in any method suitable (e.g., marker icon overlay on visual feature, loop around visual feature, etc.)
- an input mode toggle may be available on the user interface, wherein the input mode toggle may allow a user to choose whether to remain in the initial visual feature suggestion mode or switch (e.g., by touching, sliding, or otherwise engaging the toggle) to a free-form user selection mode.
- the computing system can receive a user selection of an input mode toggle. Responsive to the user selection of the input mode toggle, the computing system can place the user interface in a free-form user selection mode.
- example techniques are provided which enable a visual search system to leverage user input such as free-form selection of visual features to more intelligently process a visual query and return content based on the free-form selection of visual features provided by the visual query that the user provides.
- the computer-implemented visual search system can return content for specific visual features while retaining features of the object in the image of a visual query as a whole responsive to a visual search query. It can be difficult to search, especially in visual queries, for objects with a general essence or semantic meaning of the object in the original query but particularly focusing on specific visual features.
- a user may desire to retain some aspects of an object as a whole while also focusing on particular visual features specifically when making a visual query. For example, a user may submit an image of a dress and indicate particular interest in the sleeves. However, rather than returning results of shirts, dresses, jumpsuits, and rompers with those particular sleeves, the user may desire to search for only dresses with the particular sleeves. It can be difficult for the fluid visual search to layer such subtleties of user desire and produce results.
- the computing system can obtain a visual search query.
- the search query can comprise an image that depicts a particular sub-portion of an object that has been selected by a user (e.g., by free-form, preselected suggestion, etc.).
- the computing system can access visual embeddings associated with candidate results to identify a first set of results associated with the object overall. More particularly, the first set of results can be associated with visual features of the object overall.
- the computing system can also access visual embeddings associated with candidate results to identify a second set of results associated with the particular sub-portion of the object. More particularly, the second set of results can be associated with visual features of the particular sub-portion.
- the computing device can select based on the visual search query a combined set of content that includes search results from both the first set of results and the second set of results.
- the computing device can return the combined set of content as search results in response to the visual search query.
- the combined set can include items at an intersection of the first and second sets of results.
- top ranked items from each set can be included in the combined set.
- respective scores from the first and second sets can be summed to generate a ranking for inclusion in the combined set.
- the visual search system can use the first set of results and the second set of results to return content containing only dresses with the particular style of sleeves rather than any arbitrary article of clothing with the particular style of sleeves.
- the combined set of content can be ranked by the object overall embedding first and the particular sub-portion embedding second.
- the combined set of content can be ranked by the particular sub-portion embedding first and the object overall embedding second.
- the combined set of content may prioritize results with the particular sleeves indicated by the user, or the combined set of content may prioritize results that are dresses.
- the results can be filtered such that only content with embeddings indicating likeness to both the overall and to the particular sleeves are available to return, however the content may be ranked based on the overall embedding likeness, particular sub-portion likeness, or some average of the two. When averaging the two likenesses together, the average can be more heavily weighted towards either the overall or particular sub-portion embedding similarity.
- example techniques are provided which enable a visual search system to leverage user input of visual features of visual interest while balancing the features of the object in the image as a whole to more intelligently process a visual query and return content based on the free-form selection of visual features provided by the visual query that the user provides.
- FIG. 1 depicts a block diagram of an example computing system 100 that performs personalized and/or intelligent searches in response to at least in part visual queries according to example embodiments of the present disclosure.
- the computing system 100 includes a user computing device 102 and a visual search system 104 that are communicatively coupled over a network 180 .
- the user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
- a personal computing device e.g., laptop or desktop
- a mobile computing device e.g., smartphone or tablet
- a gaming console or controller e.g., a gaming console or controller
- a wearable computing device e.g., an embedded computing device, or any other type of computing device.
- the user computing device 102 includes one or more processors 112 and a memory 114 .
- the one or more processors 112 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.). and can be one processor or a plurality of processors that are operatively connected.
- the memory 114 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 114 can store data 116 and instructions 118 which are executed by the processor 112 to cause the user computing device 102 to perform operations.
- the visual search application 126 of a user computing device 102 presents content related to objects recognized in a viewfinder of a camera 124 of the user computing device 102 .
- objects can be recognized which are currently displayed on a user interface 158 of the device 102 .
- the search application 126 can analyze images included in a webpage currently being shown in a browser application of the device 102 .
- the visual search application 126 can be a native application developed for a particular platform.
- the visual search application 126 can control the camera 124 of the user computing device 102 .
- the visual search application 126 may be a dedicated application for controlling the camera, a camera-first application that controls the camera 124 for use with other features of the application, or another type of application that can access and control the camera 124 .
- the visual search application 126 can present the viewfinder of the camera 124 in user interfaces 158 of the visual search application 126 .
- the visual search application 126 enables a user to view content (e.g., information or user experiences) related to objects depicted in the viewfinder of the camera 124 and/or view content related to objects depicted in images stored on the user computing device 102 or stored at another location accessible by the user computing device 102 .
- the viewfinder is a portion of the display of the user computing device 102 that presents a live image of what is in the field of the view of the camera's lens. As the user moves the camera 124 (e.g., by moving the user computing device 102 ), the viewfinder is updated to present the current field of view of the lens.
- the visual search application 126 can, in some implementations, an object detector 128 , a user interface generator 130 , and/or an on-device tracker 132 .
- the object detector 128 can detect objects in the viewfinder using edge detection and/or other object detection techniques.
- the object detector 128 includes a coarse classifier that determines whether an image includes an object in one or more particular classes (e.g., categories) of objects. For example, the coarse classifier may detect that an image includes an object of a particular class, with or without recognizing the actual object.
- the coarse classifier can detect the presence of a class of objects based on whether or not the image includes (e.g., depicts) one or more features that are indicative of the class of objects.
- the coarse classifier can include a light-weight model to perform a low computational analysis to detect the presence of objects within its class(es) of objects.
- the coarse classifier can detect, for each class of objects, a limited set of visual features depicted in the image to determine whether the image includes an object that falls within the class of objects.
- the coarse classifier can detect whether an image depicts an object that is classified in one or more of classes including but not limited to: text, barcode, landmark, people, food, media object, plant, etc.
- the coarse classifier can determine whether the image includes parallel lines with different widths. Similarly, for machine-readable codes (e.g., QR codes, etc.), the coarse classifier can determine whether the image includes a pattern indicative of the presence of a machine-readable code.
- machine-readable codes e.g., QR codes, etc.
- the coarse classifier can output data specifying whether a class of object has been detected in the image.
- the coarse classifier can also output a confidence value that indicates the confidence that the presence of a class of object has been detected in the image and/or a confidence value that indicates the confidence that an actual object, e.g., a cereal box, is depicted in the image.
- the object detector 128 can receive image data representing the field of view of the camera 124 (e.g., what is being presented in the viewfinder) and detect the presence of one or more objects in the image data. If at least one object is detected in the image data, the visual search application 126 can provide (e.g., transmit) the image data to a visual search system 104 over the network 180 . As described below, the visual search system 104 can recognize objects in the image data and provide content related to the objects to the user computing device 102 .
- the visual search application 126 is shown in FIG. 1 as being included in the device 102 , in other implementations some or all of the functionality of the visual search application 126 can be implemented at the visual search system 104 .
- the visual search system 104 includes one or more front-end servers 136 and one or more back-end servers 140 .
- the front-end servers 136 can receive image data from user computing devices, e.g., the user computing device 102 (e.g., from the visual search application 126 ).
- the front-end servers 136 can provide the image data to the back-end servers 140 .
- the back-end servers 140 can identify content related to objects recognized in the image data and provide the content to the front-end servers 136 .
- the front-end servers 136 can provide the content to the mobile device from which the image data was received.
- the back-end servers 140 includes one or more processor(s) 142 and a memory 146 .
- the one or more processor(s) 142 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.). and can be one processor or a plurality of processors that are operatively connected.
- the memory 146 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 146 can store data 148 and instructions 150 which are executed by the processor(s) 142 to cause the visual search system 104 to perform operations.
- the back-end servers 140 can also include object recognizer 152 , a query processing system 154 , and a ranking system 156 .
- the object recognizer 152 can process image data received from mobile devices (e.g., user computing device 102 , etc.) and recognize objects, if any, in the image data.
- the object recognizer 152 can use computer vision and/or other object recognition techniques (e.g., edge matching, pattern recognition, greyscale matching, gradient matching, etc.) to recognize objects in the image data.
- the visual search system 104 includes or is otherwise implemented by one or more server computing devices.
- server computing devices can operate according to sequential computing architectures, parallel computing architectures, or some combination thereof.
- the query processing system 154 includes multiple processing systems.
- One example system can allow the system to identify a plurality of candidate search results. For instance, the system can identify a plurality of candidate search results upon first receiving a visual query image.
- the system can identify a plurality of search results after further processing by the system has already been done. Specifically, the system can identify a plurality of search results based on a more targeted query that the system has generated. Even more particularly, a system can generate a plurality of candidate search results when the system first receives a visual query image and then regenerate a plurality of candidate search results after further processing, based on a more targeted query that the system has generated.
- the query processing system 154 can include a system related to a combined set of content. More particularly, the combined set of content can refer to multiple items that are responsive to a first set of content related to the object presented in the image as a whole and a second set of content related to the particular visual feature of interest selected by a user in the visual search query.
- the ranking system 156 can be used in multiple different points of the visual search system process to rank the candidate search results.
- One example application is to generate a ranking of the search results after the plurality of search results is first identified.
- the initial search results may be only preliminary, and the ranking system 156 can generate a ranking of the search results after the query processing system has created a more targeted query.
- the ranking system 156 can generate a ranking of the plurality of candidate search results when the system first identifies a set of candidate search results and then again after a more targeted query has been made (e.g., the preliminary ranking may be used to determine what combinations of whole object and specific visual feature are most likely).
- the ranking that is created by the ranking system 156 can be used to determine the final output of the candidate search results to the user by determining what order the search results will be output in, and/or whether the candidate search result will be output or not.
- the multiple processing systems contained in the query processing system 154 can be used in any combination with each other and in any order to process the visual queries submitted by users in the most intelligent way in order to provide the user with the most intelligent results.
- the ranking system 156 can be used in any combination with the query processing system 154 .
- the content can be provided to the user computing device 102 from which the image data was received, stored in a content cache 138 of the visual search system 104 , and/or stored at the top of a memory stack of the front-end servers 136 . In this way, the content can be quickly presented to the user in response to the user requesting the content.
- the visual search application 126 can store the content in a content cache 134 or other fast access memory. For example, the visual search application 126 can store the content for an object with a reference to the object so that the visual search application 126 can identify the appropriate content for the object in response to determining to present the content for the object.
- the visual search system 104 includes the object detector 128 , e.g., rather than the visual search application 126 .
- the visual search application 126 can transmit image data to the visual search system 104 continuously, e.g., in a stream of images, while the visual search application 126 is active or while the user has the visual search application 126 in a request content mode.
- the request content mode can allow the visual search application 126 to send image data to the visual search system 104 continuously in order to request content for objects recognized in the image data.
- the visual search system 104 can detect objects in the image, process the image (e.g., select visual indicators for the detected objects), and send the results (e.g., visual indicators) to the visual search application 126 for presentation in the user interface (e.g., viewfinder).
- the visual search system 104 can also continue processing the image data to recognize the objects, select content for each recognized object, and either cache the content or send the content to the visual search application 126 .
- the visual search application 126 includes an on-device object recognizer that recognizes objects in image data.
- the visual search application 126 can recognize the objects, and either request content for the recognized objects from the visual search system 104 or identify the content from an on-device content data store.
- the on-device object recognizer can be a lightweight object recognizer that recognizes a more limited set of objects or that uses less computationally expensive object recognition techniques than the object recognizer 152 of the visual search system 104 . This enables mobile devices with less processing power than typical servers to perform the object recognition process.
- the visual search application 126 can use the on-device recognizer to make an initial identification of an object and provide the image data to the visual search system 104 (or another object recognition system) for confirmation.
- the on-device content data store may also store a more limited set of content than the content data storage unit 160 or links to resources that include the content to preserve data storage resources of the user computing device 102 .
- the user computing device 102 can also include one or more user input components 122 that receive user input.
- the user input component 122 can be a touch-sensitive component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to the touch of a user input object (e.g., a finger or a stylus).
- the touch-sensitive component can serve to implement a virtual keyboard.
- Other example user input components include a microphone, a traditional keyboard, or other means by which a user can provide user input.
- the network 180 can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links.
- communication over the network 180 can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
- FIG. 1 illustrates one example computing system that can be used to implement the present disclosure. Other different distributions of components can be used as well. For example, some or all of the various aspects of the visual search system can instead be located and/or implemented at the user computing device 102 .
- FIG. 4 depicts a flow chart diagram of an example method 400 to provide more personalized search results according to example embodiments of the present disclosure.
- FIG. 4 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of the method 400 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.
- a computing system can provide an image that depicts one or more subjects for display.
- a computing system e.g., user computing device 102 and visual search system 104 in FIG. 1
- an image e.g., from the camera 124 of FIG. 1
- a user e.g., on a touch sensitive device such as a phone, tablet, computer, etc.
- the computing system can receive a free-form user input to the user interface that selects a particular sub-portion of the one or more objects depicted by the image (e.g., user input component 122 may include the free-form user input overlayed on the image as illustrated by FIG. 2 provided by the camera 124 of FIG. 1 ).
- the particular sub-portion can comprise one or more visual features (e.g., visual features can include decorative features of furniture, particular cuts of sleeves on clothing items, etc.)
- the computing system can provide a visual search query that comprises the particular sub-portion of the object selected by the free-form user input (e.g., the visual search query can be received by the visual search system 104 to be processed by the query processing system 154 of FIG. 1 ).
- the computing system can receive (e.g., from the visual search system 104 of FIG. 1 ), a set of visual search results (e.g., content 160 of FIG. 1 ) responsive to visual features included in the particular sub-portion of the one or more objects (e.g., content including a particular furniture decorative feature indicated by free-form user overlay of a swathe of translucent color on an image provided by the camera 124 ).
- a set of visual search results e.g., content 160 of FIG. 1
- visual features included in the particular sub-portion of the one or more objects e.g., content including a particular furniture decorative feature indicated by free-form user overlay of a swathe of translucent color on an image provided by the camera 124 .
- FIG. 2 illustrates an example implementation of the method described in FIG. 4 .
- 202 illustrates a first image a user can submit as precursor to a visual query. The user can then specify a second portion of the image that the user wishes to perform a visual query on 204 .
- the user interface may include a switch, toggle, or other user interface element such as an “add” button 206 to indicate that the user wishes to proceed with the visual query with the constraints placed 204 .
- the user interface may alter the image framing (e.g., to enlarge the portion of the image the user indicated as wanting to perform an image search on 208 ).
- the user interface can then provide one or more initial visual feature suggestions 210 . If the user does not wish to proceed with any of the initial visual feature suggestions 210 provided, the user can select an input mode toggle button 212 that can place the user interface in a free-form user selection mode instead. Once the user interface has been placed in the free-form user selection mode, the user can provide user input that selects a particular sub-portion of the image.
- the user interface can display a swathe of translucent color 214 to overlay on the particular sub-portion of the object to indicate interest in the visual feature underneath the swathe of translucent color to integrate into the visual search (e.g., the coffee table leg).
- the visual search e.g., the coffee table leg
- the particular sub-portion of the one or more objects can be indicated by a subset of pixels selected by the user from a plurality of pixels that make up an image (e.g., provided by the camera 124 of FIG. 1 ) such as illustrated by FIG. 3 .
- the user can first submit a first image as a precursor to the visual query constraining the raw image to a section of the image 302 before selecting a toggle or other user interface element such as an “add” button 304 to indicate that the user wishes to proceed with the visual query with the constraints placed 302 .
- FIG. 3 illustrates how the user interface can then be placed in a pixeled grid structure 306 .
- the user can then select which pixels contain visual features that the user wishes to integrate into the visual search in particular 308 (e.g., the puffed sleeves).
- 310 illustrates that more than one feature may be selected (e.g., puffed sleeves and bow) as well so that more than one visual feature is incorporated into the visual search. Furthermore, none of the selected pixels need to be adjacent to another.
- FIG. 5 depicts a flow chart diagram of an example method 500 to provide more personalized search results according to example embodiments of the present disclosure.
- FIG. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of the method 500 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.
- a computing system can obtain a visual search query (e.g., the visual search system 104 of FIG. 1 ).
- the visual search query can comprise an image that depicts a particular sub-portion of an object that has been selected by a user (e.g., the user input component 122 of FIG. 1 ).
- the selected sub portion can include free-form user overlay of a swathe of translucent color on an image as illustrated in FIG. 2 (e.g., provided by the camera 124 of FIG. 1 ).
- the selected sub portion can leverage a subset of pixels selected by the user from a plurality of pixels that make up an image (e.g., provided by the camera 124 of FIG. 1 ) such as illustrated by FIG. 3 .
- the computing system can access visual embeddings. More particularly, the computing system can access visual embeddings associated with candidate results (e.g., the object recognizer 152 of FIG. 1 ).
- the visual embedding associated with candidate results can be embedding associated with candidate results can be used to identify a first set of results associated with the object overall (e.g., a dress that is of midi length) and a second set of results associated with the particular sub-portion of the object (e.g., puffed sleeves).
- the computing system can select a combined set of content that includes search results from both the first set of results and the second set of results (e.g., the ranking system 156 of FIG. 1 ). More particularly, the combined set of content can be ranked, prioritizing results associated with the object overall, the first set, and the particular sub-portion, the second set to be ranked earlier. Even more particularly, content can be ranked to prioritize results that are from the first set of results over the second set of results or vice versa.
- the computing system can return the combined set of content as search results (e.g., as content 160 of FIG. 1 ). More particularly, the combined set of content can be returned as search results in response to the visual search query.
- the technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems.
- the inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components.
- processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination.
- Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
Abstract
A user can submit a visual query that includes one or more images with user free-form selected visual features of interest. Various processing techniques such as optical character recognition (OCR) techniques can be used to recognize text (e.g., in the image, surrounding image(s), etc.) and/or various object detection techniques (e.g., machine-learned object detection models, etc.) may be used to detect objects and particular visual features of objects (e.g., dress, sleeves, color, pattern, etc.) within or related to the visual query. Content related to the detected text or object(s) in combination with the user free-form selected visual feature of interest can be identified and potentially provided to a user as search results. As such, aspects of the present disclosure enable the visual search system to more intelligently process a visual query to provide improved search results and content feeds, including search results which are personalized to account for user search intent.
Description
- The present disclosure relates generally to systems and methods for processing visual search queries. More particularly, the present disclosure relates to a computer visual search system that leverages user input free-form selection of visual features to detect and recognize objects (and/or specific visual features thereof) in images included in a visual query to provide more personalized and/or intelligent search results.
- Text-based or term-based searching is a process where a user inputs a word or phrase into a search engine and receives a variety of results. Term-based queries require a user to explicitly provide search terms in the form of words, phrases, and/or other terms. Therefore, term-based queries are inherently limited by the text-based input modality and do not enable a user to search based on visual characteristics of imagery.
- Alternatively, visual query search systems can provide a user with search results in response to a visual query that includes one or more images. Computer visual analysis techniques can be used to detect and recognize objects in images. For example, optical character recognition (OCR) techniques can be used to recognize text in images and/or edge detection techniques or other object detection techniques (e.g., machine learning-based approaches) can be used to detect objects (e.g., products, landmarks, animals, etc.) in images. Content related to the detected objects can be provided to the user (e.g., a user that captured the image in which the object is detected or that otherwise submitted or is associated with the visual query).
- However, certain existing visual query systems have a number of drawbacks. As one example, current visual search query systems and methods may provide a user with results that may only relate to the visual query with respect to visual characteristics of the query image as a whole, such as the same general color scheme or depicting the same items/objects as the image(s) of the visual query. Stated differently, certain existing visual query systems focus exclusively on identifying other images that contain holistically similar visual characteristics to the query image(s) as a whole, which may fail to reflect the user's true search intent.
- Accordingly, a system that can more intelligently process a visual query to provide the user with improved search results would be desirable.
- Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments.
- One example aspect of the present disclosure is directed to a computer-implemented method for free-form user selection of visual features for visual search. The method comprises providing for display within a user interface, by a computing system comprising one or more computing devices, an image that depicts one or more objects. The method comprises receiving, by the computing system, a free-form user input to the user interface that selects a particular sub-portion of the one or more objects depicted by the image, wherein the particular sub-portion comprises one or more visual features. The method comprises providing to a visual search system, by the computing system, a visual search query that comprises the particular sub-portion of the object selected by the free-form user input. The method comprises, in response to the visual search query, the method comprises the computing system receiving from the visual search system a set of visual search results responsive to visual features included in the particular sub-portion of the one or more objects. The method comprises providing one or more of the set of visual search results to a user.
- Another example aspect of the present disclosure is directed to a computing system that returns content for specific visual features responsive to visual search queries. The computing system comprises one or more processors. The computing system comprises one or more non-transitory computer-readable media that store instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operations comprise providing for display within a user interface, by the computing system, an image that depicts one or more objects. The operations comprise receiving, by the computing system, a free-form user input to the user interface that indicates a particular sub-portion of the one or more objects depicted by the image, wherein the particular sub-portion comprises one or more visual features. The operations comprise providing to a visual search system, by the computing system, a visual search query that comprises the particular sub-portion of the object indicated by the free-form user input. The operations comprise, in response to the visual search query, receiving from the visual search system, by the computing system, a set of visual search results responsive to visual features included in the particular sub-portion of the one or more objects. The operations comprise providing, one or more of the set of visual search results to a user.
- Another example aspect of the present disclosure is directed to a computing system that returns content for specific visual features responsive to visual search queries, the computing system comprises one or more processors. The computing system comprises one or more non-transitory computer-readable media that store instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operations comprise obtaining a visual search query, wherein the visual search query comprises an image that depicts a particular sub-portion of an object that has been selected by a user. The operations comprise accessing visual embeddings associated with candidate results to identify a first set of results associated with the object overall and a second set of results associated with the particular sub-portion of the object. The operations comprise selecting, based on the visual search query, a combined set of content that includes search results from both the first set of results and the second set of results. The operations comprise, in response to the visual search query, returning the combined set of content as search results.
- Other aspects of the present disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
- These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the present disclosure and, together with the description, serve to explain the related principles.
- Detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, in which:
-
FIG. 1 depicts a block diagram of an example computing system according to example embodiments of the present disclosure. -
FIG. 2 illustrates an example user interface illustrating steps a user may take to perform a free-form selection of a visual feature to a visual query, in accordance with some embodiments. -
FIG. 3 illustrates an example user interface illustrating steps a user may take to perform a free-form selection of a visual feature to a visual query, in accordance with some embodiments. -
FIG. 4 depicts a flow chart diagram of an example method to perform a more personalized and/or intelligent visual search leveraging free-form user input according to example embodiments of the present disclosure. -
FIG. 5 depicts a flow chart diagram of an example method to perform a more personalized and/or intelligent visual search using a combined set of content including results associated with the object overall and results associated with a particular sub-portion of the object, according to example embodiments of the present disclosure. - Reference numerals that are repeated across plural figures are intended to identify the same features in various implementations.
- Generally, the present disclosure is directed to a computer visual search system that leverages user input free-form selection of visual features to detect and recognize objects (and/or specific visual features thereof) in images included in a visual query to provide more personalized and/or intelligent search results. Aspects of the present disclosure enable the visual search system to more intelligently process a visual query to provide improved search results, including search results which are more personalized or user-driven or user-defined. Specifically, a computer visual search system can leverage free-form user input that selects one or more visual features depicted in an image. The search system can use the selected visual features to perform or refine a visual search to return results that are more specifically relevant to the selected visual features (e.g., as opposed to the image as a whole or specific semantic objects depicted in the image). Thus, a search system can provide a user with improved visual search results which are more directly relevant to specific, user-selected visual features.
- A visual query can include one or more images. For example, the images included in the visual query can be contemporaneously captured imagery or can be previously existing images. In one example, a visual query can include a single image. In another example, a visual query can include ten image frames from approximately three seconds of video capture. In yet another example, a visual query can include a corpus of images such as, for example, all images included in a user's photo library.
- According to one example aspect, a visual search system can leverage free-form user selection of visual features to provide more personalized search results. In one example use, the visual search system can use the free-form user selection of visual features to return a combined set of content responding to multiple visual features responsive to visual search queries. In particular, because visual search queries enable a more expressive and fluid modality of search input, both understanding the granularity and object(s) of user intent is a challenging task.
- To provide an example, imagine that a user submits as a visual query a dress. There is a significant amount of variation in what the user intent could be in an image of a dress. The user could be interested in dresses which have one or more characteristics in common with the dress in the query image, such as length, color, sleeve type, collar style, fabric pattern, or some combination thereof. Thus, determining by a computing system which specific visual aspects a user is interested in given an image is a challenging problem. Conversely, understanding the intended granularity of a user's query is challenging. Continuing the dress example, a visual query that includes a dress with a brand logo on it may be intended to search for dresses that look like the dress in the image or other articles of clothing which are entirely different but produced by the same brand.
- The present disclosure resolves these challenges by enabling the return of content responsive to visual features indicated as points of interest by user free-form input. In particular, in the context of a visual search query that includes an image depicting one or more objects, the computing system can receive a free-form user input to a user interface. More particularly, the free-form user input to the user interface can select a particular sub-portion of the image. The particular sub-portion can comprise one or more visual features. A visual search query can be constructed or refined based on the user input. For example, the visual search query can include the particular sub-portion of the object, for example, the particular sub-portion of the object selected by the free-form user input.
- Furthermore, the computing system can receive from the visual search system a set of visual search results. The visual search results can be responsive to visual features, such as visual features included in the particular sub-portion of the one or more objects in the image. The computing system can then provide one or more of the set of visual search results to a user. To continue the example above, while certain existing systems may return content related only to dresses that look nearly identical to the dress in the image, if the user input has selected the sleeves of the dress in the query image then the proposed system may return content related to dresses that are different in color and shape but have the same style of sleeves.
- Various techniques can be used to enable the free-form user input of particular sub-portions of an image containing one or more visual features. In one example, an initial query image provided or selected by the user can be displayed within a user interface. Even more particularly, the image can be displayed on a touch sensitive display device. Thus, the free-form user input can be received by the touch sensitive display device.
- In one example, the free-form user input to the user interface can be illustrated using a swathe of translucent color overlayed on a particular sub-portion of the object. The particular sub-portion of the object can be selected by free-form user input. Selecting the particular sub-portion of the object by free-form user input can indicate that the particular sub-portion of the one or more objects depicted by the image has been selected by the user. Specifically, a user can drag a tactile object (e.g., finger, stylus, etc.) over the image provided for display within the user interface and, in response, the user interface can overlay the swathe of translucent color wherever the tactile object touches (e.g., in a highlighting manner). Alternatively, a user can use any method of interacting with a display within a user interface (e.g., a mouse) to overlay the swathe of translucent color using any method known in the art (e.g., click and drag). Continuing the example from above, the user can drag a finger across an image of a dress's sleeve to overlay a swathe of translucent color over the sleeve of the dress. Thus, the visual query may provide visual search results of dresses with the same style of sleeve.
- In another example, the free-form user input to the user interface can be a user input that selects a subset of pixels. In particular, the subset of pixels can be selected by the user from a plurality of pixels. The pixels can be specific image pixels or groups of image pixels that are grouped together. More particularly, the plurality of pixels can be derived from dividing the image depicting one or more objects into the plurality of pixels. Even more particularly, the subset of pixels selected by the user from the plurality of pixels can comprise at least two groups of selected pixels which are separate from each other. Stated differently, the subset of pixels selected by the user from the plurality of pixels can comprise at least two groups of selected pixels which are non-adjacent to each other. The particular subset of pixels can be selected by free-form user input. Selecting the particular sub-portion of the object by free-form user input can indicate that the particular sub-portion of the one or more objects depicted by the selected pixels in the image has been selected by the user.
- Specifically, in some implementations, a user can drag a tactile object (e.g., finger, stylus, etc.) over the image provided for display within the user interface to indicate which pixels are part of the sub-portion of the image containing the visual feature of interest (e.g., pixels may indicate being selected by changing colors). Alternatively, a user can use any method of interacting with a display within a user interface (e.g., a mouse) to indicate which pixels should be selected using any method known in the art (e.g., click and drag).
- Continuing the example from above, the user can drag a finger across an image of both of a dress's sleeves to select the pixels over both sleeves of the dress and nothing in between. Thus, the visual query may provide visual search results of dresses with the same style of sleeve. As another example, the user can drag a finger across an image of both of a dress's sleeves and a bow to select the pixels over both sleeves and the bow of the dress where the sleeves and the bow are not connected by any pixels. Thus, the visual query may provide visual search results of dresses with the same style of sleeve and a bow.
- In another example, the free-form user input to the user interface can be a line drawn in a loop around a particular sub-portion of the object. The particular sub-portion of the object can be selected by free-form user input. Selecting the particular sub-portion of the object by free-form user input can indicate that the particular sub-portion of the one or more objects depicted by the image has been selected by the user. Specifically, a user can drag a tactile object (e.g., finger, stylus, etc.) over the image provided for display within the user interface to draw a line wherever the tactile object touches (e.g., as if drawing with a pen or pencil, click and slide to increase size of circle, etc.). Alternatively, a user can use any method of interacting with a display within a user interface (e.g., a mouse) to draw a loop using any method known in the art (e.g., click and drag, click and slide, etc.). Continuing the example from above, the user can drag a finger around an image of a dress's sleeve to draw a loop over the sleeve of the dress. Thus, the visual query may provide visual search results of dresses with the same style of sleeve.
- In another example, one or more initial visual feature suggestions may be provided by the computing system. In particular, one or more initial visual features may be indicated as suggested visual features for the user to select. The one or more initial visual features may be indicated in any method suitable (e.g., marker icon overlay on visual feature, loop around visual feature, etc.)
- Furthermore, in some implementations, an input mode toggle may be available on the user interface, wherein the input mode toggle may allow a user to choose whether to remain in the initial visual feature suggestion mode or switch (e.g., by touching, sliding, or otherwise engaging the toggle) to a free-form user selection mode. The computing system can receive a user selection of an input mode toggle. Responsive to the user selection of the input mode toggle, the computing system can place the user interface in a free-form user selection mode.
- Thus, example techniques are provided which enable a visual search system to leverage user input such as free-form selection of visual features to more intelligently process a visual query and return content based on the free-form selection of visual features provided by the visual query that the user provides.
- According to another aspect, the computer-implemented visual search system can return content for specific visual features while retaining features of the object in the image of a visual query as a whole responsive to a visual search query. It can be difficult to search, especially in visual queries, for objects with a general essence or semantic meaning of the object in the original query but particularly focusing on specific visual features. In particular, a user may desire to retain some aspects of an object as a whole while also focusing on particular visual features specifically when making a visual query. For example, a user may submit an image of a dress and indicate particular interest in the sleeves. However, rather than returning results of shirts, dresses, jumpsuits, and rompers with those particular sleeves, the user may desire to search for only dresses with the particular sleeves. It can be difficult for the fluid visual search to layer such subtleties of user desire and produce results.
- Some example implementations of the present disclosure can resolve these challenges by generating and ranking search results by a first set of results and a second set of results and returning a combined set of content. Specifically, the computing system can obtain a visual search query. The search query can comprise an image that depicts a particular sub-portion of an object that has been selected by a user (e.g., by free-form, preselected suggestion, etc.). The computing system can access visual embeddings associated with candidate results to identify a first set of results associated with the object overall. More particularly, the first set of results can be associated with visual features of the object overall. The computing system can also access visual embeddings associated with candidate results to identify a second set of results associated with the particular sub-portion of the object. More particularly, the second set of results can be associated with visual features of the particular sub-portion.
- The computing device can select based on the visual search query a combined set of content that includes search results from both the first set of results and the second set of results. The computing device can return the combined set of content as search results in response to the visual search query. As one example, the combined set can include items at an intersection of the first and second sets of results. In another example, top ranked items from each set can be included in the combined set. In yet another example, respective scores from the first and second sets can be summed to generate a ranking for inclusion in the combined set. To continue the example given above, the visual search system can use the first set of results and the second set of results to return content containing only dresses with the particular style of sleeves rather than any arbitrary article of clothing with the particular style of sleeves.
- In one example, the combined set of content can be ranked by the object overall embedding first and the particular sub-portion embedding second. Alternatively, the combined set of content can be ranked by the particular sub-portion embedding first and the object overall embedding second. Continuing the example given above, the combined set of content may prioritize results with the particular sleeves indicated by the user, or the combined set of content may prioritize results that are dresses. Additionally, the results can be filtered such that only content with embeddings indicating likeness to both the overall and to the particular sleeves are available to return, however the content may be ranked based on the overall embedding likeness, particular sub-portion likeness, or some average of the two. When averaging the two likenesses together, the average can be more heavily weighted towards either the overall or particular sub-portion embedding similarity.
- Thus, example techniques are provided which enable a visual search system to leverage user input of visual features of visual interest while balancing the features of the object in the image as a whole to more intelligently process a visual query and return content based on the free-form selection of visual features provided by the visual query that the user provides.
- With reference now to the Figures, example embodiments of the present disclosure will be discussed in further detail.
-
FIG. 1 depicts a block diagram of anexample computing system 100 that performs personalized and/or intelligent searches in response to at least in part visual queries according to example embodiments of the present disclosure. Thecomputing system 100 includes auser computing device 102 and avisual search system 104 that are communicatively coupled over anetwork 180. - The
user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device. - The
user computing device 102 includes one ormore processors 112 and amemory 114. The one ormore processors 112 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.). and can be one processor or a plurality of processors that are operatively connected. Thememory 114 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 114 can storedata 116 andinstructions 118 which are executed by theprocessor 112 to cause theuser computing device 102 to perform operations. - In some implementations, the
visual search application 126 of auser computing device 102 presents content related to objects recognized in a viewfinder of acamera 124 of theuser computing device 102. Alternatively, objects can be recognized which are currently displayed on auser interface 158 of thedevice 102. For example, thesearch application 126 can analyze images included in a webpage currently being shown in a browser application of thedevice 102. - The
visual search application 126 can be a native application developed for a particular platform. Thevisual search application 126 can control thecamera 124 of theuser computing device 102. For example, thevisual search application 126 may be a dedicated application for controlling the camera, a camera-first application that controls thecamera 124 for use with other features of the application, or another type of application that can access and control thecamera 124. Thevisual search application 126 can present the viewfinder of thecamera 124 inuser interfaces 158 of thevisual search application 126. - In general, the
visual search application 126 enables a user to view content (e.g., information or user experiences) related to objects depicted in the viewfinder of thecamera 124 and/or view content related to objects depicted in images stored on theuser computing device 102 or stored at another location accessible by theuser computing device 102. The viewfinder is a portion of the display of theuser computing device 102 that presents a live image of what is in the field of the view of the camera's lens. As the user moves the camera 124 (e.g., by moving the user computing device 102), the viewfinder is updated to present the current field of view of the lens. - The
visual search application 126 can, in some implementations, anobject detector 128, auser interface generator 130, and/or an on-device tracker 132. Theobject detector 128 can detect objects in the viewfinder using edge detection and/or other object detection techniques. In some implementations, theobject detector 128 includes a coarse classifier that determines whether an image includes an object in one or more particular classes (e.g., categories) of objects. For example, the coarse classifier may detect that an image includes an object of a particular class, with or without recognizing the actual object. - The coarse classifier can detect the presence of a class of objects based on whether or not the image includes (e.g., depicts) one or more features that are indicative of the class of objects. The coarse classifier can include a light-weight model to perform a low computational analysis to detect the presence of objects within its class(es) of objects. For example, the coarse classifier can detect, for each class of objects, a limited set of visual features depicted in the image to determine whether the image includes an object that falls within the class of objects. In a particular example, the coarse classifier can detect whether an image depicts an object that is classified in one or more of classes including but not limited to: text, barcode, landmark, people, food, media object, plant, etc. For barcodes, the coarse classifier can determine whether the image includes parallel lines with different widths. Similarly, for machine-readable codes (e.g., QR codes, etc.), the coarse classifier can determine whether the image includes a pattern indicative of the presence of a machine-readable code.
- The coarse classifier can output data specifying whether a class of object has been detected in the image. The coarse classifier can also output a confidence value that indicates the confidence that the presence of a class of object has been detected in the image and/or a confidence value that indicates the confidence that an actual object, e.g., a cereal box, is depicted in the image.
- The
object detector 128 can receive image data representing the field of view of the camera 124 (e.g., what is being presented in the viewfinder) and detect the presence of one or more objects in the image data. If at least one object is detected in the image data, thevisual search application 126 can provide (e.g., transmit) the image data to avisual search system 104 over thenetwork 180. As described below, thevisual search system 104 can recognize objects in the image data and provide content related to the objects to theuser computing device 102. - Although the
visual search application 126 is shown inFIG. 1 as being included in thedevice 102, in other implementations some or all of the functionality of thevisual search application 126 can be implemented at thevisual search system 104. - The
visual search system 104 includes one or more front-end servers 136 and one or more back-end servers 140. The front-end servers 136 can receive image data from user computing devices, e.g., the user computing device 102 (e.g., from the visual search application 126). The front-end servers 136 can provide the image data to the back-end servers 140. The back-end servers 140 can identify content related to objects recognized in the image data and provide the content to the front-end servers 136. In turn, the front-end servers 136 can provide the content to the mobile device from which the image data was received. - The back-
end servers 140 includes one or more processor(s) 142 and amemory 146. The one or more processor(s) 142 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.). and can be one processor or a plurality of processors that are operatively connected. Thememory 146 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 146 can storedata 148 andinstructions 150 which are executed by the processor(s) 142 to cause thevisual search system 104 to perform operations. The back-end servers 140 can also includeobject recognizer 152, aquery processing system 154, and aranking system 156. Theobject recognizer 152 can process image data received from mobile devices (e.g.,user computing device 102, etc.) and recognize objects, if any, in the image data. As an example, theobject recognizer 152 can use computer vision and/or other object recognition techniques (e.g., edge matching, pattern recognition, greyscale matching, gradient matching, etc.) to recognize objects in the image data. - In some implementations, the
visual search system 104 includes or is otherwise implemented by one or more server computing devices. In instances in which thevisual search system 104 includes plural server computing devices, such server computing devices can operate according to sequential computing architectures, parallel computing architectures, or some combination thereof. - In some implementations, the
query processing system 154 includes multiple processing systems. One example system can allow the system to identify a plurality of candidate search results. For instance, the system can identify a plurality of candidate search results upon first receiving a visual query image. On the other hand, the system can identify a plurality of search results after further processing by the system has already been done. Specifically, the system can identify a plurality of search results based on a more targeted query that the system has generated. Even more particularly, a system can generate a plurality of candidate search results when the system first receives a visual query image and then regenerate a plurality of candidate search results after further processing, based on a more targeted query that the system has generated. - As another example, the
query processing system 154 can include a system related to a combined set of content. More particularly, the combined set of content can refer to multiple items that are responsive to a first set of content related to the object presented in the image as a whole and a second set of content related to the particular visual feature of interest selected by a user in the visual search query. - In some implementations, the
ranking system 156 can be used in multiple different points of the visual search system process to rank the candidate search results. One example application is to generate a ranking of the search results after the plurality of search results is first identified. On the other hand, the initial search results may be only preliminary, and theranking system 156 can generate a ranking of the search results after the query processing system has created a more targeted query. Even more particularly, theranking system 156 can generate a ranking of the plurality of candidate search results when the system first identifies a set of candidate search results and then again after a more targeted query has been made (e.g., the preliminary ranking may be used to determine what combinations of whole object and specific visual feature are most likely). The ranking that is created by theranking system 156 can be used to determine the final output of the candidate search results to the user by determining what order the search results will be output in, and/or whether the candidate search result will be output or not. - The multiple processing systems contained in the
query processing system 154 can be used in any combination with each other and in any order to process the visual queries submitted by users in the most intelligent way in order to provide the user with the most intelligent results. Furthermore, theranking system 156 can be used in any combination with thequery processing system 154. - After the content is selected, the content can be provided to the
user computing device 102 from which the image data was received, stored in acontent cache 138 of thevisual search system 104, and/or stored at the top of a memory stack of the front-end servers 136. In this way, the content can be quickly presented to the user in response to the user requesting the content. If the content is provided to theuser computing device 102, thevisual search application 126 can store the content in acontent cache 134 or other fast access memory. For example, thevisual search application 126 can store the content for an object with a reference to the object so that thevisual search application 126 can identify the appropriate content for the object in response to determining to present the content for the object. - In some implementations, the
visual search system 104 includes theobject detector 128, e.g., rather than thevisual search application 126. In such examples, thevisual search application 126 can transmit image data to thevisual search system 104 continuously, e.g., in a stream of images, while thevisual search application 126 is active or while the user has thevisual search application 126 in a request content mode. The request content mode can allow thevisual search application 126 to send image data to thevisual search system 104 continuously in order to request content for objects recognized in the image data. Thevisual search system 104 can detect objects in the image, process the image (e.g., select visual indicators for the detected objects), and send the results (e.g., visual indicators) to thevisual search application 126 for presentation in the user interface (e.g., viewfinder). Thevisual search system 104 can also continue processing the image data to recognize the objects, select content for each recognized object, and either cache the content or send the content to thevisual search application 126. - In some implementations, the
visual search application 126 includes an on-device object recognizer that recognizes objects in image data. In this example, thevisual search application 126 can recognize the objects, and either request content for the recognized objects from thevisual search system 104 or identify the content from an on-device content data store. The on-device object recognizer can be a lightweight object recognizer that recognizes a more limited set of objects or that uses less computationally expensive object recognition techniques than theobject recognizer 152 of thevisual search system 104. This enables mobile devices with less processing power than typical servers to perform the object recognition process. In some implementations, thevisual search application 126 can use the on-device recognizer to make an initial identification of an object and provide the image data to the visual search system 104 (or another object recognition system) for confirmation. The on-device content data store may also store a more limited set of content than the contentdata storage unit 160 or links to resources that include the content to preserve data storage resources of theuser computing device 102. - The
user computing device 102 can also include one or moreuser input components 122 that receive user input. For example, theuser input component 122 can be a touch-sensitive component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to the touch of a user input object (e.g., a finger or a stylus). The touch-sensitive component can serve to implement a virtual keyboard. Other example user input components include a microphone, a traditional keyboard, or other means by which a user can provide user input. - The
network 180 can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links. In general, communication over thenetwork 180 can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL). -
FIG. 1 illustrates one example computing system that can be used to implement the present disclosure. Other different distributions of components can be used as well. For example, some or all of the various aspects of the visual search system can instead be located and/or implemented at theuser computing device 102. -
FIG. 4 depicts a flow chart diagram of anexample method 400 to provide more personalized search results according to example embodiments of the present disclosure. AlthoughFIG. 4 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of themethod 400 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure. - At 402, a computing system can provide an image that depicts one or more subjects for display. For example, a computing system (e.g.,
user computing device 102 andvisual search system 104 inFIG. 1 ) can provide an image (e.g., from thecamera 124 ofFIG. 1 ) for display to a user (e.g., on a touch sensitive device such as a phone, tablet, computer, etc.). - At 404, the computing system can receive a free-form user input to the user interface that selects a particular sub-portion of the one or more objects depicted by the image (e.g.,
user input component 122 may include the free-form user input overlayed on the image as illustrated byFIG. 2 provided by thecamera 124 ofFIG. 1 ). The particular sub-portion can comprise one or more visual features (e.g., visual features can include decorative features of furniture, particular cuts of sleeves on clothing items, etc.) - At 406, the computing system can provide a visual search query that comprises the particular sub-portion of the object selected by the free-form user input (e.g., the visual search query can be received by the
visual search system 104 to be processed by thequery processing system 154 ofFIG. 1 ). - At 408, the computing system can receive (e.g., from the
visual search system 104 ofFIG. 1 ), a set of visual search results (e.g.,content 160 ofFIG. 1 ) responsive to visual features included in the particular sub-portion of the one or more objects (e.g., content including a particular furniture decorative feature indicated by free-form user overlay of a swathe of translucent color on an image provided by the camera 124). -
FIG. 2 illustrates an example implementation of the method described inFIG. 4 . 202 illustrates a first image a user can submit as precursor to a visual query. The user can then specify a second portion of the image that the user wishes to perform a visual query on 204. The user interface may include a switch, toggle, or other user interface element such as an “add”button 206 to indicate that the user wishes to proceed with the visual query with the constraints placed 204. - Once the visual query is submitted, the user interface may alter the image framing (e.g., to enlarge the portion of the image the user indicated as wanting to perform an image search on 208). The user interface can then provide one or more initial
visual feature suggestions 210. If the user does not wish to proceed with any of the initialvisual feature suggestions 210 provided, the user can select an inputmode toggle button 212 that can place the user interface in a free-form user selection mode instead. Once the user interface has been placed in the free-form user selection mode, the user can provide user input that selects a particular sub-portion of the image. In response, the user interface can display a swathe oftranslucent color 214 to overlay on the particular sub-portion of the object to indicate interest in the visual feature underneath the swathe of translucent color to integrate into the visual search (e.g., the coffee table leg). - In some implementations, the particular sub-portion of the one or more objects can be indicated by a subset of pixels selected by the user from a plurality of pixels that make up an image (e.g., provided by the
camera 124 ofFIG. 1 ) such as illustrated byFIG. 3 . Similarly to what was illustrated byFIG. 2 , it can be seen that the user can first submit a first image as a precursor to the visual query constraining the raw image to a section of theimage 302 before selecting a toggle or other user interface element such as an “add”button 304 to indicate that the user wishes to proceed with the visual query with the constraints placed 302.FIG. 3 illustrates how the user interface can then be placed in apixeled grid structure 306. The user can then select which pixels contain visual features that the user wishes to integrate into the visual search in particular 308 (e.g., the puffed sleeves). 310 illustrates that more than one feature may be selected (e.g., puffed sleeves and bow) as well so that more than one visual feature is incorporated into the visual search. Furthermore, none of the selected pixels need to be adjacent to another. -
FIG. 5 depicts a flow chart diagram of anexample method 500 to provide more personalized search results according to example embodiments of the present disclosure. AlthoughFIG. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of themethod 500 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure. - At 502, a computing system can obtain a visual search query (e.g., the
visual search system 104 ofFIG. 1 ). The visual search query can comprise an image that depicts a particular sub-portion of an object that has been selected by a user (e.g., theuser input component 122 ofFIG. 1 ). More particularly, the selected sub portion can include free-form user overlay of a swathe of translucent color on an image as illustrated inFIG. 2 (e.g., provided by thecamera 124 ofFIG. 1 ). As another example, the selected sub portion can leverage a subset of pixels selected by the user from a plurality of pixels that make up an image (e.g., provided by thecamera 124 ofFIG. 1 ) such as illustrated byFIG. 3 . - At 504, the computing system can access visual embeddings. More particularly, the computing system can access visual embeddings associated with candidate results (e.g., the
object recognizer 152 ofFIG. 1 ). The visual embedding associated with candidate results can be embedding associated with candidate results can be used to identify a first set of results associated with the object overall (e.g., a dress that is of midi length) and a second set of results associated with the particular sub-portion of the object (e.g., puffed sleeves). - At 506, the computing system can select a combined set of content that includes search results from both the first set of results and the second set of results (e.g., the
ranking system 156 ofFIG. 1 ). More particularly, the combined set of content can be ranked, prioritizing results associated with the object overall, the first set, and the particular sub-portion, the second set to be ranked earlier. Even more particularly, content can be ranked to prioritize results that are from the first set of results over the second set of results or vice versa. - At 508, the computing system can return the combined set of content as search results (e.g., as
content 160 ofFIG. 1 ). More particularly, the combined set of content can be returned as search results in response to the visual search query. - The technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. For instance, processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
- While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Those skilled in the art, upon attaining an understanding of the foregoing, can readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Thus, it is intended that the present disclosure cover such alterations, variations, and equivalents.
Claims (20)
1. A computer-implemented method for free-form user selection of visual features for visual search, the method comprising:
providing for display within a user interface, by a computing system comprising one or more computing devices, an image that depicts one or more objects;
receiving, by the computing system, a free-form user input to the user interface that selects a particular sub-portion of the one or more objects depicted by the image, wherein the particular sub-portion comprises one or more visual features;
providing to a visual search system, by the computing system, a visual search query that comprises the particular sub-portion of the object selected by the free-form user input;
in response to the visual search query, receiving from the visual search system, by the computing system, a set of visual search results responsive to visual features included in the particular sub-portion of the one or more objects; and
providing, one or more of the set of visual search results to a user.
2. The computer-implemented method of claim 1 , wherein receiving, by the computing system, the free-form user input to the user interface further comprises providing for display within the user interface, by the computing system, a swathe of translucent color overlayed on the particular sub-portion of the object selected by free-form user input to indicate that the particular sub-portion of the one or more objects depicted by the image has been selected by the user.
3. The computer-implemented method of claim 1 , wherein receiving, by the computing system, the free-form user input to the user interface further comprises receiving, by the computing system, a subset of pixels selected by the user from a plurality of pixels, wherein the image depicting one or more objects is divided into the plurality of pixels.
4. The computer-implemented method of claim 3 , wherein the subset of pixels selected by the user from the plurality of pixels comprises at least two groups of selected pixels which are separate from and non-adjacent to each other.
5. The computer-implemented method of claim 1 , wherein providing for display within the user interface, by the computing system, the image that depicts the one or more objects comprises providing, by the computing system, the image for display on a touch sensitive display device, wherein the free-form user input is received by the touch sensitive display device.
6. The computer-implemented method of claim 1 , further comprising, prior to receiving the free-form user input:
providing, by the computing system, to a user one or more initial visual feature suggestions;
receiving, by the computing system, a user selection of an input mode toggle; and
responsive to the user selection of the input mode toggle, placing, by the computing system, the user interface in a free-form user selection mode.
7. A computing system that returns content for specific visual features responsive to visual search queries, the computing system comprising:
one or more processors; and
one or more non-transitory computer-readable media that store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising:
providing for display within a user interface, by the computing system, an image that depicts one or more objects;
receiving, by the computing system, a free-form user input to the user interface that indicates a particular sub-portion of the one or more objects depicted by the image, wherein the particular sub-portion comprises one or more visual features;
providing to a visual search system, by the computing system, a visual search query that comprises the particular sub-portion of the object indicated by the free-form user input;
in response to the visual search query, receiving from the visual search system, by the computing system, a set of visual search results responsive to visual features included in the particular sub-portion of the one or more objects; and
providing, one or more of the set of visual search results to a user.
8. The computing system of claim 7 , wherein receiving, by the computing system, the free-form user input to the user interface further comprises providing for display within the user interface, by the computing system, a swathe of translucent color overlayed on the particular sub-portion of the object selected by free-form user input to indicate that the particular sub-portion of the one or more objects depicted by the image has been selected by the user.
9. The computing system of claim 7 , wherein receiving, by the computing system, the free-form user input to the user interface further comprises receiving, by the computing system, a subset of pixels selected by the user from a plurality of pixels, wherein the image depicting one or more objects is divided into the plurality of pixels.
10. A computing system that returns content for specific visual features responsive to visual search queries, the computing system comprising:
one or more processors; and
one or more non-transitory computer-readable media that store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising:
obtaining a visual search query, wherein the visual search query comprises an image that depicts a particular sub-portion of an object that has been selected by a user;
accessing visual embeddings associated with candidate results to identify a first set of results associated with the object overall and a second set of results associated with the particular sub-portion of the object;
selecting, based on the visual search query, a combined set of content that includes search results from both the first set of results and the second set of results; and
in response to the visual search query, returning the combined set of content as search results.
11. The computing system of claim 10 , wherein the first set of results associated with the object overall comprises visual features of the object overall; and
the second set of results associated with the particular sub-portion of the object comprises visual features of the particular sub-portion.
12. The computing system of claim 10 , wherein the combined set of content is ranked by the visual embeddings associated with the object overall first and the visual embeddings associated with the particular sub-portion second.
13. The computing system of claim 10 , wherein the combined set of content is ranked by the visual embeddings associated with the particular sub-portion first and the visual embeddings associated with the object overall second.
14. The computing system of claim 10 , wherein the image that depicts the particular sub-portion of the object comprises the image with free-form user input to a user interface wherein the free-form user input further comprises providing for display within the user interface, by the computing system, a swathe of translucent color overlayed on the particular sub-portion of the object selected by free-form user input to indicate that the particular sub-portion of the one or more objects depicted by the image has been selected by the user. free-form
15. The computing system of claim 10 , wherein the image that depicts a particular sub-portion of an object comprises the image with free-form user input to the user interface wherein the free-form user input further comprises providing for display within the user interface, by the computing system, a line drawn around the particular sub-portion of the object selected by free-form user input to indicate that the particular sub-portion of the one or more objects depicted by the image has been selected by the user.
16. The computing system of claim 10 , wherein the image that depicts a particular sub-portion of an object comprises the image with free-form user input to the user interface wherein free-form user input to the user interface further comprises receiving, by the computing system, a subset of pixels selected by the user from a plurality of pixels, wherein the image depicting one or more objects is divided into the plurality of pixels.
17. The computing system of claim 14 , wherein the subset of pixels selected by the user from the plurality of pixels comprises at least two groups of selected pixels which are separate from and non-adjacent to each other.
18. The computing system of claim 10 , wherein providing for display within the user interface, by the computing system, the image that depicts the one or more objects comprises providing, by the computing system, the image for display on a touch sensitive display device, wherein a free-form user input is received by the touch sensitive display device.
19. The computing system of claim 10 , wherein the image that depicts a particular sub-portion of an object comprises providing, by the computing system, to the user one or more initial visual feature suggestions.
20. The computing system of claim 10 , wherein the image that depicts a particular sub-portion of an object comprises the image with an altered frame to enlarge the particular sub-portion.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/698,795 US20220300550A1 (en) | 2021-03-19 | 2022-03-18 | Visual Search via Free-Form Visual Feature Selection |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163163177P | 2021-03-19 | 2021-03-19 | |
US17/698,795 US20220300550A1 (en) | 2021-03-19 | 2022-03-18 | Visual Search via Free-Form Visual Feature Selection |
Publications (1)
Publication Number | Publication Date |
---|---|
US20220300550A1 true US20220300550A1 (en) | 2022-09-22 |
Family
ID=83284915
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/698,795 Pending US20220300550A1 (en) | 2021-03-19 | 2022-03-18 | Visual Search via Free-Form Visual Feature Selection |
Country Status (1)
Country | Link |
---|---|
US (1) | US20220300550A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230034495A1 (en) * | 2021-08-02 | 2023-02-02 | Mulberry Technology Inc. | Systems and methods for automatic product category detection using multiple machine learning techniques |
Citations (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030050927A1 (en) * | 2001-09-07 | 2003-03-13 | Araha, Inc. | System and method for location, understanding and assimilation of digital documents through abstract indicia |
US20080263037A1 (en) * | 2006-10-18 | 2008-10-23 | International Business Machines Corporation | Method and apparatus for indicating content search results |
US20110125735A1 (en) * | 2009-08-07 | 2011-05-26 | David Petrou | Architecture for responding to a visual query |
US20140143250A1 (en) * | 2012-03-30 | 2014-05-22 | Xen, Inc. | Centralized Tracking of User Interest Information from Distributed Information Sources |
US8983193B1 (en) * | 2012-09-27 | 2015-03-17 | Google Inc. | Techniques for automatic photo album generation |
US20150170203A1 (en) * | 2011-05-24 | 2015-06-18 | David Kogan | Presenting search results |
US9690831B2 (en) * | 2013-04-19 | 2017-06-27 | Palo Alto Research Center Incorporated | Computer-implemented system and method for visual search construction, document triage, and coverage tracking |
US20180341808A1 (en) * | 2017-05-25 | 2018-11-29 | Fyusion, Inc. | Visual feature tagging in multi-view interactive digital media representations |
US20190272425A1 (en) * | 2018-03-05 | 2019-09-05 | A9.Com, Inc. | Visual feedback of process state |
US20190318405A1 (en) * | 2018-04-16 | 2019-10-17 | Microsoft Technology Licensing , LLC | Product identification in image with multiple products |
US20190354609A1 (en) * | 2018-05-21 | 2019-11-21 | Microsoft Technology Licensing, Llc | System and method for attribute-based visual search over a computer communication network |
US10528645B2 (en) * | 2015-09-16 | 2020-01-07 | Amazon Technologies, Inc. | Content search using visual styles |
US20200019628A1 (en) * | 2018-07-16 | 2020-01-16 | Microsoft Technology Licensing, Llc | Visual intent triggering for visual search |
US20200081912A1 (en) * | 2018-04-17 | 2020-03-12 | YesPlz, Inc. | Identifying physical objects using visual search query |
US10824942B1 (en) * | 2017-04-10 | 2020-11-03 | A9.Com, Inc. | Visual similarity and attribute manipulation using deep neural networks |
US20210034634A1 (en) * | 2019-07-30 | 2021-02-04 | Walmart Apollo, Llc | Methods and apparatus for automatically providing personalized search results |
US20210064652A1 (en) * | 2019-09-03 | 2021-03-04 | Google Llc | Camera input as an automated filter mechanism for video search |
US20210073295A1 (en) * | 2018-06-21 | 2021-03-11 | Google Llc | Digital supplement association and retrieval for visual search |
US11163779B1 (en) * | 2017-12-01 | 2021-11-02 | Pinterest, Inc. | Binary representations of objects based on object attributes |
US20220092105A1 (en) * | 2020-09-18 | 2022-03-24 | Google Llc | Intelligent Systems and Methods for Visual Search Queries |
US11354349B1 (en) * | 2018-02-09 | 2022-06-07 | Pinterest, Inc. | Identifying content related to a visual search query |
US20230315781A1 (en) * | 2022-03-31 | 2023-10-05 | Microsoft Technology Licensing, Llc | Web-scale personalized visual search recommendation service |
-
2022
- 2022-03-18 US US17/698,795 patent/US20220300550A1/en active Pending
Patent Citations (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030050927A1 (en) * | 2001-09-07 | 2003-03-13 | Araha, Inc. | System and method for location, understanding and assimilation of digital documents through abstract indicia |
US20080263037A1 (en) * | 2006-10-18 | 2008-10-23 | International Business Machines Corporation | Method and apparatus for indicating content search results |
US20110125735A1 (en) * | 2009-08-07 | 2011-05-26 | David Petrou | Architecture for responding to a visual query |
US20150170203A1 (en) * | 2011-05-24 | 2015-06-18 | David Kogan | Presenting search results |
US20140143250A1 (en) * | 2012-03-30 | 2014-05-22 | Xen, Inc. | Centralized Tracking of User Interest Information from Distributed Information Sources |
US8983193B1 (en) * | 2012-09-27 | 2015-03-17 | Google Inc. | Techniques for automatic photo album generation |
US9690831B2 (en) * | 2013-04-19 | 2017-06-27 | Palo Alto Research Center Incorporated | Computer-implemented system and method for visual search construction, document triage, and coverage tracking |
US10528645B2 (en) * | 2015-09-16 | 2020-01-07 | Amazon Technologies, Inc. | Content search using visual styles |
US10824942B1 (en) * | 2017-04-10 | 2020-11-03 | A9.Com, Inc. | Visual similarity and attribute manipulation using deep neural networks |
US20180341808A1 (en) * | 2017-05-25 | 2018-11-29 | Fyusion, Inc. | Visual feature tagging in multi-view interactive digital media representations |
US11163779B1 (en) * | 2017-12-01 | 2021-11-02 | Pinterest, Inc. | Binary representations of objects based on object attributes |
US11354349B1 (en) * | 2018-02-09 | 2022-06-07 | Pinterest, Inc. | Identifying content related to a visual search query |
US20190272425A1 (en) * | 2018-03-05 | 2019-09-05 | A9.Com, Inc. | Visual feedback of process state |
US20190318405A1 (en) * | 2018-04-16 | 2019-10-17 | Microsoft Technology Licensing , LLC | Product identification in image with multiple products |
US20200081912A1 (en) * | 2018-04-17 | 2020-03-12 | YesPlz, Inc. | Identifying physical objects using visual search query |
US20190354609A1 (en) * | 2018-05-21 | 2019-11-21 | Microsoft Technology Licensing, Llc | System and method for attribute-based visual search over a computer communication network |
US20210073295A1 (en) * | 2018-06-21 | 2021-03-11 | Google Llc | Digital supplement association and retrieval for visual search |
US20200019628A1 (en) * | 2018-07-16 | 2020-01-16 | Microsoft Technology Licensing, Llc | Visual intent triggering for visual search |
US20210034634A1 (en) * | 2019-07-30 | 2021-02-04 | Walmart Apollo, Llc | Methods and apparatus for automatically providing personalized search results |
US20210064652A1 (en) * | 2019-09-03 | 2021-03-04 | Google Llc | Camera input as an automated filter mechanism for video search |
US20220092105A1 (en) * | 2020-09-18 | 2022-03-24 | Google Llc | Intelligent Systems and Methods for Visual Search Queries |
US20230315781A1 (en) * | 2022-03-31 | 2023-10-05 | Microsoft Technology Licensing, Llc | Web-scale personalized visual search recommendation service |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230034495A1 (en) * | 2021-08-02 | 2023-02-02 | Mulberry Technology Inc. | Systems and methods for automatic product category detection using multiple machine learning techniques |
US11861644B2 (en) * | 2021-08-02 | 2024-01-02 | Mulberry Technology Inc. | Non-transitory processor-readable mediums for automatic product category detection using multiple machine learning techniques |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10043109B1 (en) | Attribute similarity-based search | |
US11423076B2 (en) | Image similarity-based group browsing | |
US10824942B1 (en) | Visual similarity and attribute manipulation using deep neural networks | |
US11620331B2 (en) | Textual and image based search | |
US10846327B2 (en) | Visual attribute determination for content selection | |
JP6328761B2 (en) | Image-based search | |
US10032072B1 (en) | Text recognition and localization with deep learning | |
CA3068761C (en) | Architecture for responding to a visual query | |
US20200342320A1 (en) | Non-binary gender filter | |
US10176198B1 (en) | Techniques for identifying visually similar content | |
CA2770186C (en) | User interface for presenting search results for multiple regions of a visual query | |
WO2018118803A1 (en) | Visual category representation with diverse ranking | |
US20110128288A1 (en) | Region of Interest Selector for Visual Queries | |
US10776417B1 (en) | Parts-based visual similarity search | |
CN104584033A (en) | Interactive clothes searching in online stores | |
US11841735B2 (en) | Object based image search | |
CN113330455A (en) | Finding complementary digital images using conditional generative countermeasure networks | |
JP2023162232A (en) | Intelligent systems and methods for visual search queries | |
US20220300550A1 (en) | Visual Search via Free-Form Visual Feature Selection | |
JP2021086438A (en) | Image searching apparatus, image searching method, and program | |
US11403697B1 (en) | Three-dimensional object identification using two-dimensional image data | |
KR20180068455A (en) | Method, apparatus, system and computer program for providing aimage retrieval model | |
Zhang et al. | Algorithm of 3D hand posture recognition with space coordinates based on optimal feature selection |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BJORNSSON, ERICA;WONG, HEIDI;SIGNING DATES FROM 20220208 TO 20220216;REEL/FRAME:059518/0499 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |