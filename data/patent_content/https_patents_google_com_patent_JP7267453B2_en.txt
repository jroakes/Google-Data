JP7267453B2 - image augmentation neural network - Google Patents
image augmentation neural network Download PDFInfo
- Publication number
- JP7267453B2 JP7267453B2 JP2021558609A JP2021558609A JP7267453B2 JP 7267453 B2 JP7267453 B2 JP 7267453B2 JP 2021558609 A JP2021558609 A JP 2021558609A JP 2021558609 A JP2021558609 A JP 2021558609A JP 7267453 B2 JP7267453 B2 JP 7267453B2
- Authority
- JP
- Japan
- Prior art keywords
- image
- neural network
- training
- augmented
- images
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000013528 artificial neural network Methods 0.000 title claims description 199
- 230000003416 augmentation Effects 0.000 title claims description 31
- 230000003190 augmentative effect Effects 0.000 claims description 146
- 238000012549 training Methods 0.000 claims description 132
- 238000000034 method Methods 0.000 claims description 70
- 238000012545 processing Methods 0.000 claims description 40
- 230000004044 response Effects 0.000 claims description 34
- 230000008569 process Effects 0.000 claims description 27
- 238000003860 storage Methods 0.000 claims description 21
- 238000010606 normalization Methods 0.000 claims description 6
- 238000013527 convolutional neural network Methods 0.000 claims description 5
- 238000011524 similarity measure Methods 0.000 claims description 4
- 238000009826 distribution Methods 0.000 description 50
- 230000006870 function Effects 0.000 description 37
- 230000005540 biological transmission Effects 0.000 description 19
- 238000004590 computer program Methods 0.000 description 13
- 238000010801 machine learning Methods 0.000 description 9
- 238000010586 diagram Methods 0.000 description 8
- 238000013515 script Methods 0.000 description 8
- 230000003750 conditioning effect Effects 0.000 description 7
- 230000009471 action Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 6
- 230000003993 interaction Effects 0.000 description 6
- 230000000007 visual effect Effects 0.000 description 5
- 230000001143 conditioned effect Effects 0.000 description 4
- 230000002452 interceptive effect Effects 0.000 description 4
- 230000008859 change Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 239000002131 composite material Substances 0.000 description 2
- 238000010295 mobile communication Methods 0.000 description 2
- 230000003252 repetitive effect Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000026676 system process Effects 0.000 description 2
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 1
- 241000009334 Singa Species 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 230000009193 crawling Effects 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 230000003111 delayed effect Effects 0.000 description 1
- 230000010339 dilation Effects 0.000 description 1
- 238000012854 evaluation process Methods 0.000 description 1
- 239000012634 fragment Substances 0.000 description 1
- 230000003902 lesion Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000000873 masking effect Effects 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000036961 partial effect Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000002829 reductive effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
Images
Classifications
-
- G06T5/77—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/10—Segmentation; Edge detection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20112—Image segmentation details
- G06T2207/20132—Image cropping
Description
本明細書は、機械学習モデルを使用して画像を処理することに関する。 This specification relates to processing images using machine learning models.
機械学習モデルは、入力を受信し、受信された入力に基づいて、出力、たとえば予測出力を生成する。いくつかの機械学習モデルは、パラメトリックモデルであり、受信した入力およびモデルのパラメータの値に基づいて出力を生成する。 A machine learning model receives input and generates an output, eg, a predicted output, based on the received input. Some machine learning models are parametric models and produce output based on the values of the input and parameters of the model that are received.
いくつかの機械学習モデルは、受信された入力のための出力を生成するためにモデルの複数の層を使用するディープモデルである。たとえば、ディープニューラルネットワークは、出力層と、出力を生成するために受信された入力に変換を各々適用する1つまたは複数の隠れ層とを含むディープ機械学習モデルである。 Some machine learning models are deep models that use multiple layers of models to generate outputs for received inputs. For example, a deep neural network is a deep machine learning model that includes an output layer and one or more hidden layers that each apply a transformation to received inputs to produce an output.
本明細書は、画像拡張を実行する1つまたは複数の位置にある1つまたは複数のコンピュータ上のコンピュータプログラムとして実装されるシステムについて説明する。 This specification describes a system implemented as a computer program on one or more computers at one or more locations that performs image augmentation.
第1の態様によれば、1つまたは複数のデータ処理装置によって実行される方法が提供され、この方法は、複数の生成ニューラルネットワークパラメータを有する生成ニューラルネットワーク(generative neural network)に、提供された画像を含む入力を提供するステップを含み、生成ニューラルネットワークは、拡張画像を生成するために、複数の生成ニューラルネットワークパラメータの訓練された値に従って入力を処理し、拡張画像は、(i)提供された画像よりも多くの行、多くの列、または両方を有し、(ii)提供された画像の現実的な拡張であると予測され、生成ニューラルネットワークは、敵対的損失目的関数を使用して訓練されている。 According to a first aspect, there is provided a method performed by one or more data processing devices, the method being provided to a generative neural network having a plurality of generative neural network parameters. providing an input comprising an image, the generative neural network processing the input according to trained values of a plurality of generative neural network parameters to generate the augmented image, the augmented image being (i) provided; has more rows, more columns, or both than the image provided, and (ii) is predicted to be a realistic extension of the provided image, the generative neural network using the adversarial loss objective function trained.
このようにして、追加の行および/または列は、提供された画像の元の境界のうちの1つまたは複数を越える拡張を提供し、たとえば、提供された画像の高レベルの意味的特性ならびに低レベルの構造およびテクスチャを維持するなど、提供された画像の予測された現実的な拡張を提供する。 In this way, the additional rows and/or columns provide extensions beyond one or more of the original boundaries of the provided image, e.g., high-level semantic properties of the provided image and Provide a predicted and realistic extension of the provided image, such as preserving low-level structure and texture.
この方法は、ブロック内に提示される画像の要求を受信するステップと、提供された画像が要求に応答していると決定するステップと、拡張画像を生成した後、要求に応答して拡張画像を提供するステップとを含み得る。 The method comprises the steps of: receiving a request for an image to be presented in a block; determining that the provided image is responsive to the request; generating an augmented image; and providing
この方法は、ブロックのサイズが、提供された画像のサイズとは異なることに基づいて、提供された画像が画像拡張に適格であると決定し、それに応答して、提供された画像を含む入力を生成ニューラルネットワークに提供するステップを含み得る。 The method determines that the provided image is eligible for image augmentation based on the size of the blocks being different from the size of the provided image, and in response, an input containing the provided image. to the generating neural network.
ブロックのサイズは、ブロックのアスペクト比を指定し得る。 The size of the block may specify the aspect ratio of the block.
要求は、ブロック内の画像とともに提示される追加要素を指定してもよく、要求に応答して拡張画像を提供するステップは、拡張画像の拡張部分上に追加要素をオーバーレイするステップを含み得る。 The request may specify additional elements to be presented with the image in the block, and providing the augmented image in response to the request may include overlaying the augmented element onto the augmented portion of the augmented image.
ブロックは、検索結果と一緒に、またはサードパーティウェブページ上に提示されてもよい。 Blocks may be presented alongside search results or on third party web pages.
生成ニューラルネットワークは、複数の畳み込みニューラルネットワーク層を含み得る。 A generative neural network may include multiple convolutional neural network layers.
生成ニューラルネットワークは、複数のスキップ接続を含み得る。 A generative neural network may include multiple skip connections.
生成ニューラルネットワークは、複数のインスタンス正規化層を含み得る。 A generative neural network may include multiple instance normalization layers.
生成ニューラルネットワークへの入力は、拡張画像と同じ数の行および列を有するベースライン画像であり、(i)提供された画像に対応する第1の部分、および(ii)デフォルトの画素値を有する第2の部分を含む、ベースライン画像と、拡張画像と同じ数の行および列を有するマスク画像であり、ベースライン画像の第1の部分および第2の部分を識別する、マスク画像とを含み得る。 The input to the generating neural network is a baseline image with the same number of rows and columns as the augmented image, with (i) a first portion corresponding to the provided image, and (ii) default pixel values. a baseline image including a second portion; and a mask image having the same number of rows and columns as the extended image and identifying the first portion and the second portion of the baseline image. obtain.
ベースライン画像の第1の部分に対応するマスク画像内の画素は各々、第1の画素値を有し、ベースライン画像の第2の部分に対応するマスク画像内の画素は各々、第1の画素値とは異なる第2の画素値を有し得る。 Pixels in the mask image corresponding to the first portion of the baseline image each have a first pixel value and pixels in the mask image corresponding to the second portion of the baseline image each have a first pixel value. It may have a second pixel value that is different than the pixel value.
生成ニューラルネットワークは、生成ニューラルネットワークを使用して所与の画像が生成された可能性を特徴付ける弁別出力を生成するために、所与の画像を処理するように構成された複数の弁別ニューラルネットワークパラメータを有する弁別ニューラルネットワーク(discriminative neural network)と共同で訓練され得る。 The generative neural network has a plurality of discrimination neural network parameters configured to process a given image to produce a discriminative output characterizing the likelihood that the given image was generated using the generative neural network. can be jointly trained with a discriminative neural network having
敵対的損失目的関数を使用して生成ニューラルネットワークを訓練するステップは、訓練画像を拡張する訓練拡張画像を生成するために、生成ニューラルネットワークを使用し、生成ニューラルネットワークパラメータの現在の値に従って、訓練画像を含む訓練入力を処理するステップと、訓練拡張画像に基づいて弁別ニューラルネットワーク入力を生成するステップと、生成ニューラルネットワークを使用して弁別ニューラルネットワーク入力が生成された可能性を特徴付ける弁別出力を生成するために、弁別ニューラルネットワークを使用し、弁別ニューラルネットワークパラメータの現在の値に従って、弁別ニューラルネットワーク入力を処理するステップと、敵対的損失目的関数に基づいて生成ニューラルネットワークパラメータの現在値を調整するステップであり、敵対的損失目的関数は、生成ニューラルネットワークを使用して弁別ニューラルネットワーク入力が生成された可能性を特徴付ける弁別出力に依存する、調整するステップと含み得る。 The step of training the generative neural network using the adversarial loss objective function includes using the generative neural network to generate training augmented images that dilate the training images, and performing training according to current values of the generative neural network parameters. processing a training input comprising an image; generating a discriminative neural network input based on the training augmented image; and generating a discriminative output characterizing the likelihood that the discriminative neural network input was generated using the generating neural network. using the discriminative neural network to process the discriminative neural network input according to the current values of the discriminative neural network parameters; and adjusting the current values of the generated neural network parameters based on the adversarial loss objective function. and the adversarial loss objective function may include a step of adjusting that depends on the discriminant output characterizing the likelihood that the discriminative neural network input was generated using the generating neural network.
訓練拡張画像に基づいて弁別ニューラルネットワーク入力を生成するステップは、訓練画像に対応する訓練拡張画像の一部を訓練画像で上書きするステップを含み得る。 Generating a discriminatory neural network input based on the training augmented images may include overwriting portions of the training augmented images that correspond to the training images with the training images.
この方法は、訓練拡張画像とターゲット画像との類似性を特徴付ける再構成損失目的関数に基づいて、生成ニューラルネットワークパラメータの現在値を調整するステップであり、訓練画像は、ターゲット画像のクロッピングされた表現である、調整するステップを含み得る。 The method consists in adjusting the current values of the generated neural network parameters based on a reconstruction loss objective function that characterizes the similarity between the training augmented image and the target image, the training image being a cropped representation of the target image. .
弁別ニューラルネットワークは、ターゲット画像の意味的特徴表現を条件とし、訓練画像は、ターゲット画像のクロッピングされた表現である。 A discriminative neural network is conditioned on the semantic feature representation of the target image, and the training images are cropped representations of the target image.
ターゲット画像の意味的特徴表現は、ターゲット画像を処理することによって、分類ニューラルネットワークの中間出力を使用して決定され得る。 A semantic feature representation of the target image can be determined using the intermediate output of the classification neural network by processing the target image.
弁別出力は、(i)弁別ニューラルネットワークの最終層の出力、および(ii)弁別ニューラルネットワークの中間出力とターゲット画像の意味的特徴表現との間の類似性尺度に基づき得る。 The discrimination output may be based on (i) the output of the final layer of the discrimination neural network and (ii) a similarity measure between the intermediate output of the discrimination neural network and the semantic feature representation of the target image.
別の態様によれば、1つまたは複数のコンピュータと、1つまたは複数のコンピュータによって実行されると、1つまたは複数のコンピュータに、第1の態様のそれぞれの方法の動作を実行させる命令を記憶する1つまたは複数の記憶デバイスとを備えるシステムが提供される。 According to another aspect, one or more computers and instructions that, when executed by the one or more computers, cause the one or more computers to perform the operations of the respective methods of the first aspect. A system is provided that includes one or more storage devices for storing.
別の態様によれば、1つまたは複数のコンピュータによって実行されると、1つまたは複数のコンピュータに、第1の態様のそれぞれの方法の動作を実行させる命令を記憶する1つまたは複数のコンピュータ記憶媒体が提供される。 According to another aspect, one or more computers storing instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the respective methods of the first aspect. A storage medium is provided.
一態様の随意の特徴を、必要に応じて別のものと組み合わせてもよい。本明細書に記載される主題の特定の実施形態は、以下の利点のうちの1つまたは複数を実現するように実施することができる。 Any optional feature of one aspect may be combined with another as appropriate. Particular embodiments of the subject matter described herein can be implemented to realize one or more of the following advantages.
本明細書に記載される画像拡張システムは、生成ニューラルネットワークを使用して、(たとえば、元の境界を越えて入力画像を拡張する)拡張画像を生成し得る。生成ニューラルネットワークは、入力画像が(i)「実」画像であるか、(ii)生成ニューラルネットワークによって生成された拡張画像であるかの予測を定義する出力を生成するために、入力画像を処理するように構成された「弁別」ニューラルネットワークと共同で訓練される。生成ニューラルネットワークと弁別ニューラルネットワークとの共同訓練を容易にするために、システムは、弁別ニューラルネットワークによって処理される入力画像に対応する意味的特徴について弁別ニューラルネットワークを条件付けることができる。このように弁別ニューラルネットワークを条件付けることにより、より少ない訓練反復にわたって許容可能なパフォーマンスレベルを達成するようにシステムを訓練することが可能になり、それによって、システムは、訓練中に、いくつかの従来のシステムよりも少ない計算リソース(たとえば、メモリおよび計算能力)を消費することが可能になる。 The image augmentation systems described herein may use generative neural networks to generate augmented images (eg, augmenting an input image beyond its original boundaries). A generator neural network processes an input image to produce an output that defines a prediction of whether the input image is (i) a "real" image or (ii) an augmented image generated by the generator neural network. It is jointly trained with a "discrimination" neural network configured to To facilitate joint training of the generative neural network and the discriminatory neural network, the system can condition the discriminative neural network on semantic features corresponding to input images processed by the discriminative neural network. Conditioning the discriminatory neural network in this way allows the system to be trained to achieve acceptable performance levels over fewer training iterations, whereby the system is trained to perform several It can consume less computational resources (eg, memory and computing power) than conventional systems.
いくつかの従来のインペインティングシステムは、元の画像データによってすべての方向において囲まれた画像の一部を「埋める」。対照的に、本明細書に記載される画像拡張システムは、その元の境界を越えて画像を拡張して、画像の高レベルの意味的特性ならびに低レベルの画像構造およびテクスチャを拡張する拡張画像を生成する。画像拡張タスクを実行するために従来のインペインティングシステムを直接適用することは、いくつかの場合には、本明細書に記載される画像拡張システムによって生成されるものよりも低品質の拡張画像をもたらす可能性がある。たとえば、従来のインペインティングシステムは、ぼやけたまたは反復的な画素および一貫性のないセマンティクスを有する拡張画像を生成することができるが、現在説明されている画像拡張システムによって生成される拡張画像は、同じぼやけた、反復的な画素、または一貫性のない意味的特性を有さない。 Some conventional inpainting systems "fill in" a portion of the image surrounded in all directions by the original image data. In contrast, the image augmentation system described herein expands an image beyond its original boundaries to extend the image's high-level semantic properties as well as its low-level image structure and texture. to generate Directly applying conventional inpainting systems to perform image augmentation tasks can in some cases produce augmented images of lower quality than those produced by the image augmentation systems described herein. can result in For example, conventional inpainting systems can produce augmented images with blurry or repetitive pixels and inconsistent semantics, whereas augmented images produced by the currently described image augmentation system are , do not have the same blurry, repetitive pixels, or inconsistent semantic properties.
本明細書に記載される画像拡張システムは、様々な用途、たとえば、仮想現実の用途、計算写真の用途、およびデジタルコンポーネント配信の用途のいずれかに使用することができる拡張画像を生成する。デジタルコンポーネント配信の用途では、デジタルコンポーネント配信システムは、たとえば、検索結果と一緒に、またはサードパーティウェブページ上に、ブロックで提示するためにデジタルコンポーネントを送信する。いくつかの場合には、デジタルコンポーネントに含まれる画像は、たとえば、画像とブロックのアスペクト比が異なるために、画像が定義されたブロックを満たすことを妨げるサイズを有することがある。この状況では、本明細書に記載される画像拡張システムは、画像がブロックを満たすことができるように、画像をサイズ変更するために使用することができ、それによって、リソースのより効率的な使用(特に、ブロック内で利用可能なスペースのより効率的な使用)を可能にし、ユーザインターフェース内のブランクスポットおよび/または歪んだ画像の提示を防止することができる。言い換えれば、本出願で説明されるシステムは、画像の「フィルイン」だけでなく、その境界を越えた画像の拡張も可能にし、これにより、単一の画像サイズのみが利用可能であるときでも、複数の異なるサイズのブロックに適合するように画像を修正することが可能になる。これは、元の画像の特徴を保持しながら、単一の画像を様々なサイズのブロックに適合するように修正することができるので、記憶する必要がある画像の数を低減する。画像拡張システムは、画像がブロックを満たすように画像のアスペクト比を変更するために、ある寸法に沿って画像を伸張または圧縮することによって、画像を歪める必要性をなくすことができる。さらに、デジタルコンポーネント配信の用途では、いくつかのデジタルコンポーネントプロバイダは、デジタルコンポーネントに含まれる画像が歪むことなく(たとえば、ある寸法に沿って伸張または圧縮されることによって)提示されることを必要とする場合がある。 The image augmentation system described herein produces augmented images that can be used in any of a variety of applications, such as virtual reality applications, computational photography applications, and digital component distribution applications. In a digital component delivery application, the digital component delivery system sends the digital component for presentation in blocks, for example, along with search results or on a third party web page. In some cases, the image contained in the digital component may have a size that prevents the image from filling the defined block, for example, due to different aspect ratios of the image and the block. In this situation, the image augmentation system described herein can be used to resize the image so that it can fill the block, thereby making more efficient use of resources. (especially more efficient use of the space available within the block) and prevent the presentation of blank spots and/or distorted images within the user interface. In other words, the system described in this application allows not only the "fill-in" of an image, but also the extension of an image beyond its boundaries, thereby allowing even when only a single image size is available. It allows the image to be modified to fit multiple different sized blocks. This reduces the number of images that need to be stored, as a single image can be modified to fit blocks of various sizes while preserving the features of the original image. Image enhancement systems can eliminate the need to distort an image by stretching or compressing the image along certain dimensions in order to change the aspect ratio of the image so that the image fills the block. Additionally, for digital component delivery applications, some digital component providers require that images contained in the digital component be presented without distortion (e.g., by being stretched or compressed along certain dimensions). sometimes.
本明細書の主題の1つまたは複数の実施形態の詳細は、添付の図面および以下の説明に記載される。主題の他の特徴、態様、および利点は、説明、図面、および特許請求の範囲から明らかになるであろう。 The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, drawings, and claims.
様々な図面における同様の参照番号および名称は、同様の要素を示す。 Like reference numbers and designations in the various drawings indicate like elements.
本明細書は、その元の境界を越えて画像を現実的に拡張する、たとえば、画像の高レベルの意味的特性ならびに低レベルの構造およびテクスチャを維持しながら、その元の境界を越えて画像を拡張する「拡張」画像を生成するために画像を処理する技術について説明する。そのような処理は、画像強調に使用することができる。たとえば、画像が配置されるブロックに一致しない寸法を画像が有する場合、画像寸法は、元の画像内のコンテンツの特性を維持しながら、ブロックに一致するように拡大されてもよい。別の例では、画像の一部が欠落しているなど、画像が損傷している場合、本開示の態様を使用して、欠落部分を自動的に生成し得る。たとえば、不適切にキャプチャされた、または不完全な損傷画像のキャプチャされたバージョンは、本明細書で説明する技法により再生成することができる。 This specification realistically extends an image beyond its original boundaries, e.g., an image beyond its original boundaries while preserving the image's high-level semantic properties and low-level structure and texture. Techniques for processing images to generate "dilated" images that dilate the are described. Such processing can be used for image enhancement. For example, if an image has dimensions that do not match the block in which it is placed, the image dimensions may be enlarged to match the block while maintaining the properties of the content within the original image. In another example, if an image is damaged, such as a portion of the image is missing, aspects of the present disclosure may be used to automatically generate the missing portion. For example, captured versions of improperly captured or incomplete lesion images can be regenerated by the techniques described herein.
画像を拡張するために、本明細書に記載される画像拡張システムは、対応する拡張画像を生成するために敵対的損失目的関数を使用して訓練される生成ニューラルネットワークを使用して画像を処理する。敵対的損失目的関数は、拡張画像を処理することによって、弁別ニューラルネットワークによって生成された出力に基づいて、生成ニューラルネットワークを使用して生成された拡張画像を評価する。弁別ニューラルネットワークは、入力画像が(i)「実」(すなわち、自然)画像であるか、(ii)生成ニューラルネットワークを使用して生成された拡張画像であるかを予測するように訓練される。生成ニューラルネットワークおよび弁別ニューラルネットワークを共同で訓練することにより、実画像と区別することが困難な拡張画像を生成することを、画像拡張システムに促す。 To augment an image, the image augmentation system described herein processes the image using a generative neural network trained using an adversarial loss objective function to generate a corresponding augmented image. do. The adversarial loss objective function evaluates the augmented image generated using the generative neural network based on the output generated by the discriminatory neural network by processing the augmented image. A discriminative neural network is trained to predict whether an input image is (i) a "real" (i.e., natural) image or (ii) an augmented image generated using a generative neural network. . By jointly training a generator neural network and a discriminant neural network, the image augmentation system is encouraged to produce augmented images that are difficult to distinguish from real images.
画像拡張システムは、たとえば、画像が歪むことなくブロックを満たすことを可能にするために、様々な寸法の定義されたブロックで提示されるデジタルコンポーネントに含まれる画像をサイズ変更するなど、様々な用途のいずれにも使用することができる。 Image augmentation systems have a variety of uses, such as resizing images contained in digital components that are presented in defined blocks of varying dimensions to allow the image to fill the block without distortion. can be used for either
本明細書全体にわたって使用されるように、デジタルコンポーネントという語句は、たとえば、画像、ビデオクリップ、オーディオクリップ、マルチメディアクリップ、テキストセグメント、またはユニフォームリソースロケータ(URL)のうちの1つまたは複数を含むことができるデジタルコンテンツまたはデジタル情報の個別ユニットを指す。デジタルコンポーネントは、単一のファイルとして、またはファイルの集合として物理メモリデバイスに電子的に記憶することができ、デジタルコンポーネントは、ビデオファイル、オーディオファイル、マルチメディアファイル、画像ファイル、またはテキストファイルの形態をとり、ストリーミングビデオ、ストリーミングオーディオ、ソーシャルネットワークポスト、ブログポスト、および/または広告情報を含むことができ、したがって、広告は、デジタルコンポーネントの一種である。一般に、デジタルコンポーネントは、単一のプロバイダまたはソース(たとえば、広告主、発行者、または他のコンテンツプロバイダ)によって定義される(または提供される)が、1つのソースから提供されるデジタルコンポーネントは、別のソースからのデータ(たとえば、気象情報、リアルタイムイベント情報、または別のソースから取得される他の情報)により強化され得る。 As used throughout this specification, the phrase digital component includes, for example, one or more of an image, video clip, audio clip, multimedia clip, text segment, or uniform resource locator (URL). Refers to a discrete unit of digital content or digital information that can be A digital component can be stored electronically in a physical memory device as a single file or as a collection of files; the digital component can be in the form of a video file, audio file, multimedia file, image file, or text file. , which can include streaming video, streaming audio, social network posts, blog posts, and/or advertising information, and thus advertising is a type of digital component. In general, digital components are defined (or provided) by a single provider or source (e.g., an advertiser, publisher, or other content provider), but digital components offered from one source are It may be enriched with data from another source (eg, weather information, real-time event information, or other information obtained from another source).
これらの特徴および他の特徴について、以下でより詳細に説明する。 These and other features are described in more detail below.
図1は、例示的な画像拡張システム100を示す。画像拡張システム100は、以下で説明するシステム、コンポーネント、および技法が実装される1つまたは複数の位置にある1つまたは複数のコンピュータ上のコンピュータプログラムとして実装されるシステムの一例である。
FIG. 1 shows an exemplary
画像拡張システム100は、その元の境界を越えて画像102を現実的に拡張する拡張画像104を生成するために、画像102を処理するように構成される。より具体的には、拡張画像104を表す画素の2次元(2D)アレイは、一般に、画像102よりも多くの行、多くの列、または両方を有する。図1に示す例では、拡張画像104は、元の画像102と比較してその右側に1つまたは複数の追加の列106を有する。特定の例では、画像102は、100×100次元であってもよく、拡張画像104は、100×120次元であってもよい。別の特定の例では、画像102は、80×120次元であってもよく、拡張画像は、90×125次元であってもよい。次元の単位は、画素の数など、任意の適切な単位であってもよい。システム100を使用して生成された拡張画像の一例を図3に示す。
いくつかの場合には、拡張画像104を生成する前に、画像拡張システム100は、元の画像102を均一に縮小または拡大し得る(すなわち、そのアスペクト比を維持し、それによって歪みを回避しながら)。特定の例では、次元100×200を有する入力画像から次元50×120を有する出力画像を生成するために、画像拡張システムは、まず、入力画像を次元50×100に一様に縮小し、次いで、次元50×100の縮小された画像から次元50×120を有する拡張画像を生成し得る。
In some cases, before generating the
画像102(および拡張画像104)は、任意の適切なフォーマットで、たとえば、各々が対応するグレースケール強度または適切な色空間内の画素の色を表す色ベクトルに関連付けられた画素の2Dアレイとして表すことができる。色空間は、たとえば、赤緑青(RGB)色空間またはCIELAB色空間であってもよい。 Image 102 (and augmented image 104) may be represented in any suitable format, for example, as a 2D array of pixels each associated with a corresponding grayscale intensity or color vector representing the color of the pixel in a suitable color space. be able to. The color space may be, for example, the red-green-blue (RGB) color space or the CIELAB color space.
拡張画像104を生成するために、システム100は、画像102から生成ネットワーク入力108を決定し、生成ニューラルネットワーク110を使用して生成ネットワーク入力108を処理する。生成ネットワーク入力のいくつかの例を以下に示す。
To generate
一例では、生成ネットワーク入力108は、両方とも生成ニューラルネットワーク110によって生成される拡張画像と同じ次元(すなわち、行および列の数)を有するベースライン画像112およびマスク画像114を含む。ベースライン画像112は、(i)画像102に対応する第1の部分112-A、および(ii)デフォルトの画素値(たとえば、黒または白の画素値)を有する第2の部分112-Bを含む。すなわち、ベースライン画像112は、画像102を含み、デフォルトの画素値の1つまたは複数の行または列だけ画像102を拡張すると理解することができる。マスク画像114は、ベースライン画像のどの部分が、(i)画像102、および(ii)デフォルトの画素値に対応するかを識別する。たとえば、画像102に一致するベースライン画像112の適切なサブセットに対応するマスク画像114内の画素は、第1の値(たとえば、値0)を有してもよく、マスク画像114内の残りの画素は、第2の値(たとえば、値1)を有してもよい。この例では、ベースライン画像112およびマスク画像114は、生成ネットワーク入力108を形成するために、チャネルごとに空間的に連結されてもよい。
In one example, generator network input 108 includes
別の例では、生成ネットワーク入力108は、すなわち、マスク画像114を含まず、ベースライン画像112のみを含み得る。
In another example, the generator network input 108 may, ie, not include the mask image 114 and only include the
別の例では、生成ネットワーク入力108は、すなわち、ベースライン画像112またはマスク画像114のいずれも含まず、画像102に直接対応し得る。
In another example, generator network input 108 may correspond directly to
生成ニューラルネットワーク110は、拡張画像104を生成するために、生成ニューラルネットワークパラメータのセットの訓練された値に従って生成ネットワーク入力108を処理するように構成される。生成ニューラルネットワーク110は、一般に、畳み込みニューラルネットワークアーキテクチャ、すなわち、1つまたは複数の畳み込みニューラルネットワーク層(および随意に、他の適切な種類のニューラルネットワーク層)を含むニューラルネットワークアーキテクチャを有する。
Generative
敵対的訓練システム116は、生成ニューラルネットワーク110が現実的な拡張画像104を生成することを可能にする生成ニューラルネットワークパラメータの訓練された値を決定するように構成される。敵対的訓練システム116は、図2を参照してより詳細に説明するように、敵対的損失目的関数を使用して、弁別ニューラルネットワークとともに生成的ニューラルネットワーク110を共同で訓練する。
Adversarial training system 116 is configured to determine trained values for generative neural network parameters that enable generative
いくつかの場合には、システム100は、元の画像を繰り返し拡張することによって「パノラマ」拡張画像を作成することができる。たとえば、システム100は、元の画像を拡張して第1の拡張画像を生成し、次いで、第1の拡張画像のクロッピングされた部分を拡張して第2の拡張画像を生成することなどが可能である。最後に、システムは、拡張画像を連結して、たとえば、2×、3×、5×、または任意の他の適切な係数だけ、元の画像を実質的に拡張するパノラマ画像を生成することができる。
In some cases, the
システム100によって生成された拡張画像104は、様々な用途のいずれかで使用することができる。いくつかの例を以下に示す。
一例では、バーチャルリアリティの用途では、元の画像をキャプチャするために実際に使用されたのとは異なるカメラの向きからキャプチャされた画像をシミュレートする必要がある場合がある。画像拡張システム100は、元の画像の境界の外側のコンテンツを埋めることによって、このように画像をシミュレートするために使用することができる。
In one example, virtual reality applications may need to simulate an image captured from a different camera orientation than was actually used to capture the original image. The
別の例では、パノラマステッチングの用途(たとえば、複数の画像が、単一の合成された画像を生成するために一緒に「スティッチ」される)では、いくつかの従来の技法は、長方形のパノラマを達成するために、スティッチされた投影のギザギザのエッジをクロッピングすることを必要とする。画像拡張システム100は、代わりに、ギザギザのエッジ間のギャップを埋める拡張画像を生成することによって、ギザギザのエッジをクロップする必要をなくすことができ、それによって、より効果的なパノラマステッチングを可能にする。
As another example, in panoramic stitching applications (e.g., multiple images are "stitched" together to produce a single composite image), some conventional techniques use rectangular To achieve a panorama, we need to crop the jagged edges of the stitched projections. The
別の例では、画像拡張システム100を使用して、あるアスペクト比でキャプチャされ撮られたビデオを、伸張またはクロッピングに頼ることなく、異なるアスペクト比で画面上に表示することができる。特に、画像拡張システム100を使用して、ビデオ内の各ビデオフレームを処理して、たとえば、ビデオが表示される画面と同じアスペクト比を有する、対応する拡張ビデオフレームを生成することができる。
In another example, the
別の例では、画像拡張システム100は、デジタルコンポーネント配信システムによって、検索結果と一緒に、またはサードパーティウェブサイト上に、定義されたブロックで表示される画像を拡張して、伸張またはクロッピングに頼ることなく、画像がブロックを満たすことを可能にするために使用することができる。デジタルコンポーネント配信システムの一部として画像拡張システム100を使用することについて、図4を参照してより詳細に説明する。
In another example, the
図2は、敵対的損失目的関数204を使用して、生成ニューラルネットワーク110を弁別ニューラルネットワーク202と共同で訓練するために敵対的訓練システム116によって実行され得る動作を示す例示的なデータフロー200である。一般に、生成ニューラルネットワーク110は、入力画像を現実的に拡張する拡張画像を生成するように訓練され、弁別ニューラルネットワーク202は、(すなわち、自然画像とは対照的に)生成ニューラルネットワーク110を使用して入力画像が生成された可能性を特徴付ける弁別出力を生成するように訓練される。次に、生成ニューラルネットワーク110の訓練について詳細に説明し、その後、弁別ニューラルネットワーク202の訓練について説明する。
FIG. 2 is an
生成ニューラルネットワーク110は、複数の訓練反復にわたって訓練され、各訓練反復において、生成ニューラルネットワークパラメータの現在のパラメータ値は、訓練例の現在のバッチ(すなわち、セット)に基づいて調整される。各訓練例は、(i)訓練生成ネットワーク入力、および(ii)対応する訓練生成ネットワーク入力を処理することによって生成ニューラルネットワーク110によって生成されるべきターゲット拡張画像を含む。
The generative
訓練例に含まれるターゲット拡張画像206は、一般に、実(すなわち、自然)画像である。ターゲット拡張画像206に対応する訓練生成ネットワーク入力は、図1を参照して説明したように、様々な方法のいずれかで表すことができる。一例では、ターゲット拡張画像206に対応する訓練生成ネットワーク入力は、ベースライン画像208およびマスク画像210を含む。ベースライン画像208は、ターゲット拡張画像206の一部を「マスキング」することによって、たとえば、ターゲット拡張画像206の一部の画素値をデフォルト値(たとえば、値0)に設定することによって生成される。マスク画像210は、ベースライン画像においてマスクされたターゲット拡張画像206の部分を識別する。訓練生成ネットワーク入力に含まれるターゲット拡張画像の部分は、本明細書では「訓練画像」と呼ばれることがある。
The target augmented
一般に、訓練例の現在のバッチは、複数の訓練例を含むことができる。しかしながら、便宜上、以下の説明は、現在のバッチにおける特定の訓練例を参照する。 In general, the current batch of training examples can contain multiple training examples. However, for convenience, the following description refers to specific training examples in the current batch.
生成ニューラルネットワーク110は、訓練例に含まれる訓練生成ネットワーク入力を処理して、対応する訓練拡張画像212を生成する。訓練拡張画像212を生成した後、敵対的訓練システム116は、生成ニューラルネットワーク110によって訓練拡張画像が生成された可能性を特徴付ける弁別出力を生成するために、弁別ニューラルネットワーク202を使用して訓練拡張画像212を処理する。したがって、弁別出力は、生成ニューラルネットワーク110によって生成された訓練拡張画像212が、訓練画像をもっともらしく拡張する現実的な画像であるかどうかを特徴付ける。以下でより詳細に説明するように、敵対的損失目的関数204は、弁別ニューラルネットワーク202によって生成される弁別出力に依存する。
Generative
いくつかの場合には、訓練拡張画像212を直接処理するのではなく、敵対的訓練システム116は、まず、訓練拡張画像212の対応する部分に訓練画像を上書きする。すなわち、敵対的訓練システム116は、訓練画像の境界を越えて延在する訓練拡張画像212の部分を修正することなく、訓練画像と一致すべき訓練拡張画像212の部分に訓練画像を上書きする。このように訓練拡張画像212を修正することは、訓練拡張画像が本質的に現実的な画像であるかどうかだけでなく、訓練拡張画像が元の訓練画像の現実的な拡張であるかどうかを特徴付けるために、弁別出力を促す。 In some cases, rather than processing training augmented images 212 directly, adversarial training system 116 first overwrites corresponding portions of training augmented images 212 with training images. That is, the adversarial training system 116 overwrites the training images on the portions of the training augmented images 212 that should match the training images without modifying the portions of the training augmented images 212 that extend beyond the boundaries of the training images. Modifying the training augmented images 212 in this manner determines not only whether the training augmented images are inherently realistic images, but also whether the training augmented images are realistic augmentations of the original training images. To characterize, prompt the discriminatory output.
訓練拡張画像212が弁別ニューラルネットワーク202に提供される前に、マスク画像210は、訓練拡張画像212にチャネルごとに空間的に連結されてもよい。
The
訓練拡張画像212(または訓練拡張画像212に基づく入力)を処理することに加えて、弁別ニューラルネットワーク202は、訓練例のターゲット拡張画像206の意味的特徴表現に「条件付け」され得る。すなわち、訓練拡張画像212のために弁別ニューラルネットワーク202によって生成される弁別出力は、ターゲット拡張画像206の意味的特徴表現に依存し得る。
In addition to processing the training augmented images 212 (or input based on the training augmented images 212), the discriminatory neural network 202 may be “conditioned” on the semantic feature representations of the target augmented
ターゲット拡張画像206の意味的特徴表現は、ターゲット拡張画像206の内容を暗黙的または明示的に特徴付ける、たとえば、数値のベクトルまたは行列など、数値の順序付けられた集合を指す。敵対的訓練システム116は、ターゲット拡張画像206を処理することによって、ターゲット拡張画像206の意味的特徴表現を、事前訓練された画像処理ニューラルネットワークによって生成された中間出力であると決定する。ニューラルネットワークの中間出力は、ニューラルネットワークの1つまたは複数の中間層、すなわち、入力層に続くが出力層に先行する層によって生成される出力を指す。
A semantic feature representation of the target augmented
画像処理ニューラルネットワークは、任意の適切なニューラルネットワークアーキテクチャ(たとえば、InceptionV3アーキテクチャ)を有し得、たとえば、分類タスクまたは回帰タスクなど、様々な画像処理タスクのいずれかを実行するように事前訓練され得る。特定の例では、画像処理ニューラルネットワークは、入力画像が所定数のオブジェクトクラス(たとえば、人、車両、自転車など)の各々からのオブジェクトを描写するそれぞれの可能性を特徴付ける出力を生成することによって、分類タスクを実行するように事前訓練されてもよい。別の特定の例では、画像処理ニューラルネットワークは、入力画像に描かれたオブジェクトを囲むと予測されるバウンディングボックスの位置を特徴付ける出力を生成することによって、回帰タスクを実行するように事前訓練されてもよい。「事前訓練された」画像処理ニューラルネットワークは、画像処理タスクを実行するように事前に訓練されたネットワークを指す。 The image processing neural network may have any suitable neural network architecture (e.g., the InceptionV3 architecture) and may be pretrained to perform any of a variety of image processing tasks, such as, for example, classification or regression tasks. . In a particular example, the image processing neural network produces an output characterizing each likelihood that the input image depicts objects from each of a predetermined number of object classes (e.g., people, vehicles, bicycles, etc.). It may be pre-trained to perform classification tasks. In another particular example, an image processing neural network is pre-trained to perform a regression task by producing an output characterizing the locations of bounding boxes that are expected to enclose objects depicted in the input image. good too. A "pre-trained" image processing neural network refers to a network that has been pre-trained to perform an image processing task.
いくつかの場合には、敵対的訓練システム116は、正規化エンジン214を使用して、弁別ニューラルネットワーク202を条件付けるためにそれを使用する前に、ターゲット拡張画像の意味表現を正規化する。たとえば、正規化エンジン214は、正規化された意味表現Cnormを以下のように決定し得る。 In some cases, adversarial training system 116 uses normalization engine 214 to normalize the semantic representation of the target augmented image before using it to condition discriminative neural network 202 . For example, normalization engine 214 may determine the normalized semantic representation C norm as follows.
式中、Cは正規化されていない意味表現であり、 where C is the unnormalized semantic representation,
は、(たとえば、訓練画像のセットからの)画像の意味表現の期待値であり、|・|2は、L2ノルムを指す。さらに、敵対的訓練システム116は、弁別ニューラルネットワークを条件付けるために意味表現を使用する前に、生成および弁別ニューラルネットワークと共同で訓練される1つまたは複数のニューラルネットワーク層216(たとえば、全結合層)を使用して、正規化された意味表現を処理し得る。 is the expected value of the semantic representation of an image (eg, from a set of training images) and |·| 2 refers to the L2 norm. Additionally, the adversarial training system 116 may include one or more neural network layers 216 (e.g., fully connected neural networks) that are jointly trained with the generative and discriminative neural networks prior to using the semantic representation to condition the discriminative neural network. layer) can be used to process the normalized semantic representation.
弁別ニューラルネットワーク202は、様々な方法のうちの任意の方法で、ターゲット拡張画像の意味表現を条件とすることができる。一例では、敵対的訓練システム116は、ターゲット拡張画像の意味表現を、たとえば、弁別ニューラルネットワークの入力層または中間層など、弁別ニューラルネットワークへの追加入力として提供し得る。別の例では、敵対的訓練システム116は、(i)ターゲット拡張画像の意味表現と、(ii)弁別ニューラルネットワークの中間出力との間の類似性尺度に少なくとも部分的に基づいて弁別出力を決定してもよく、たとえば、弁別出力Dは、以下によって与えられ得、
D=DO+<DN,S> (2)
式中、DOは、弁別ニューラルネットワークの最終ニューラルネットワーク層のスカラー出力であり、DNは、弁別ニューラルネットワークの中間出力(たとえば、中間層218によって生成される)であり、Sは、ターゲット拡張画像の意味表現であり、<・, ・>は、ドット積演算を指す。
Discrimination neural network 202 can be conditioned on the semantic representation of the target augmented image in any of a variety of ways. In one example, adversarial training system 116 may provide a semantic representation of the target augmented image as an additional input to the discriminatory neural network, eg, as an input or hidden layer of the discriminative neural network. In another example, the adversarial training system 116 determines the discriminatory output based at least in part on a similarity measure between (i) the semantic representation of the target augmented image and (ii) the intermediate output of the discriminatory neural network. For example, the discrimination output D may be given by
D=D O +<D N ,S> (2)
where D O is the scalar output of the final neural network layer of the discriminative neural network, D N is the intermediate output of the discriminative neural network (eg, produced by intermediate layer 218), and S is the target extension A semantic representation of an image, <·, ·> refers to the dot product operation.
弁別ニューラルネットワーク202を意味表現で条件付けることは、生成ニューラルネットワーク110および弁別ニューラルネットワーク202の共同訓練を安定化させ、訓練された生成ニューラルネットワーク110によって生成される拡張画像の品質も向上させ得る。特に、弁別ニューラルネットワーク202を意味表現で条件付けることは、弁別ニューラルネットワーク202が、ターゲット拡張画像のマスクされた部分からを含む、ターゲット拡張画像の意味的内容全体に依存する弁別出力を生成することを可能にし得る。意味表現は、ターゲット拡張画像の「グローバル」コンテキストを特徴付ける結果として、効果的な条件付け情報(conditioning information)(すなわち、弁別ニューラルネットワークがより正確な弁別出力を生成することを可能にする)を提供し得る。対照的に、たとえば、画素値は本質的に「局所的」であり、潜在的にノイズが多いので、弁別ネットワークをターゲット拡張画像の画素値で直接条件付けることは、あまり効果的ではない可能性がある。また、弁別ニューラルネットワークを意味表現で条件付けることは、その最終層の出力における意味内容を直接考慮するために、弁別ニューラルネットワーク上の負担を低減することによって、弁別ニューラルネットワークをより効果的に(たとえば、より少ない訓練反復で)訓練することを可能にし得る。たとえば、式(2)を参照すると、弁別出力は、弁別ニューラルネットワークの中間出力と意味表現との間の内積によって意味内容を考慮に入れることができる。
Conditioning the discriminative neural network 202 with a semantic representation stabilizes the joint training of the generative
弁別出力を生成した後、敵対的訓練システム116は、たとえば、以下のように、敵対的損失目的関数Ladv 204および再構成目的関数Lrec 220を含む目的関数LGの勾配を使用して、生成ニューラルネットワークパラメータの現在の値を調整することができ、
After generating the discriminatory output, the adversarial training system 116 uses the gradient of the objective function LG , which includes the adversarial loss objective function L adv 204 and the reconstruction
式中、λは、スカラーハイパーパラメータであり、xは、ターゲット拡張画像であり、 where λ is a scalar hyperparameter, x is the target augmented image,
は、生成ニューラルネットワークによって生成された訓練拡張画像であり、|・|1は、L1ノルムであり、 is the training augmented image generated by the generative neural network, |·| 1 is the L1 norm,
は、訓練拡張画像(または訓練拡張画像に基づく入力)を処理することによって弁別ニューラルネットワークによって生成される弁別出力である。 is the discriminatory output produced by the discriminatory neural network by processing the training augmented images (or input based on the training augmented images).
式(3)～(5)によって特徴付けられる目的関数は、目的関数の例示的な例として提供されるが、他の目的関数も可能である。一般に、再構成目的関数Lrecは、ターゲット拡張画像xと訓練拡張画像 The objective functions characterized by equations (3)-(5) are provided as illustrative examples of objective functions, although other objective functions are possible. In general, the reconstruction objective function L rec is the target augmented image x and the training augmented image
との間の類似性を様々な方法で特徴付けることができ、敵対的損失目的関数Ladvは、様々な方法で、弁別出力 can be characterized in various ways, and the adversarial loss objective function L adv can be characterized in various ways by the discriminatory output
に依存する。 depends on
敵対的訓練システム116は、たとえば、所定数の訓練反復の間、弁別ニューラルネットワーク202と生成ニューラルネットワーク110とを交互に訓練することによって、弁別ニューラルネットワーク202と生成ニューラルネットワーク110とを共同で訓練する。
Adversarial training system 116 jointly trains discriminative neural network 202 and generative
弁別ニューラルネットワークの訓練中の各訓練反復において、弁別ニューラルネットワークの現在のパラメータ値は、訓練例の現在のバッチに基づいて調整される。前述のように、訓練例の現在のバッチは、複数の訓練例を含むことができるが、便宜上、以下の説明は、特定の訓練例を参照する。 At each training iteration during training of the discrimination neural network, the current parameter values of the discrimination neural network are adjusted based on the current batch of training examples. As mentioned above, the current batch of training examples can include multiple training examples, but for convenience the following description will refer to a particular training example.
弁別ニューラルネットワーク202を訓練するために、敵対的訓練システム116は、対応する訓練拡張画像を生成するために、生成ニューラルネットワーク110を使用して、訓練例の訓練生成ネットワーク入力を処理する。敵対的訓練システム116は、対応する弁別出力を生成するために、弁別ニューラルネットワークを使用して、訓練拡張画像(または訓練拡張画像に基づく入力)と対応するターゲット拡張画像の両方を処理する。次いで、敵対的訓練システム116は、訓練拡張画像およびターゲット拡張画像に対して生成された弁別出力に依存する弁別目的関数に基づいて、弁別ニューラルネットワーク202の現在のパラメータ値を調整する。たとえば、弁別目的関数Ldiscは、以下によって与えられ得、
To train the discriminatory neural network 202, the adversarial training system 116 processes the training generative network input of training examples using the generative
式中、ReLU(・)は、整流された線形単位関数であり、D(x)は、ターゲット拡張画像のために弁別ニューラルネットワークによって生成された弁別出力であり、 where ReLU(·) is the rectified linear unit function, D(x) is the discrimination output produced by the discrimination neural network for the target augmented image,
は、生成ニューラルネットワークによって生成された訓練拡張画像のために弁別ニューラルネットワークによって生成された弁別出力である。式(6)によって特徴付けられる弁別目的関数は、例示の目的のためだけに提供され、他の弁別目的関数が可能である。 is the discriminant output generated by the discriminatory neural network for the training augmented images generated by the generative neural network. The discrimination objective function characterized by equation (6) is provided for illustrative purposes only, other discrimination objective functions are possible.
弁別ニューラルネットワーク202は、任意の適切なニューラルネットワークアーキテクチャを有することができる。一例では、弁別ニューラルネットワーク202は、全結合層が後に続くリーキーなReLU活性化機能を有する6つのストライド畳み込み層を有する。 Discrimination neural network 202 may have any suitable neural network architecture. In one example, the discriminatory neural network 202 has six strided convolutional layers with leaky ReLU activation functions followed by fully connected layers.
生成ニューラルネットワークおよび弁別ニューラルネットワークの訓練中に、敵対的訓練システム116は、たとえば、逆伝搬技法を使用して目的関数の勾配を決定することができる。敵対的訓練システム116は、目的関数の勾配を使用して、任意の適切な勾配降下最適化手順、たとえば、AdamまたはRMSpropを使用して、生成ニューラルネットワークおよび弁別ニューラルネットワークの現在のパラメータ値を調整することができる。 During training of the generative neural network and the discriminative neural network, the adversarial training system 116 may, for example, determine the gradient of the objective function using backpropagation techniques. The adversarial training system 116 uses the gradient of the objective function to adjust the current parameter values of the generative and discriminative neural networks using any suitable gradient descent optimization procedure, e.g., Adam or RMSprop. can do.
敵対的訓練システム116は、訓練終了基準が満たされるまで、たとえば、各ニューラルネットワークに対して所定数の訓練反復が実行されるまで、または性能基準が満たされるまで、生成ニューラルネットワーク110および弁別ニューラルネットワーク202を共同で訓練し続けることができる。
The adversarial training system 116 trains the generative
図3は、元の画像302を処理することによって、図1を参照して説明した画像拡張システム100によって生成された拡張画像300の一例の図である。拡張画像300は、元の画像302の高レベルの意味的特性ならびに低レベルの構造およびテクスチャを維持する元の画像302の現実的な拡張であることを諒解することができる。
FIG. 3 is an illustration of an example
図4は、デジタルコンポーネント配信システム410が画像拡張システム100を使用し、電子ドキュメントとともに提示するためにデジタルコンポーネントデータベース416からデジタルコンポーネントを送信する例示的な環境400のブロック図である。以下でより詳細に説明するように、デジタルコンポーネント配信システムは、デジタルコンポーネントが、ユーザデバイスにおいて、たとえば、検索結果と一緒に、またはサードパーティウェブサイト上に、ブロックで、電子ドキュメントとともに提示される旨の要求に応答して、デジタルコンポーネントを送信することができる。
FIG. 4 is a block diagram of an exemplary environment 400 in which digital
要求に応答して送信されるデジタルコンポーネントを識別した後、配信システムは、デジタルコンポーネントに含まれる画像が画像拡張に適格であることを識別し得る。一例では、配信システムは、画像のサイズ(たとえば、アスペクト比)が、画像が提示されるブロックのサイズと(たとえば、少なくともしきい値量だけ)異なることに基づいて、画像が画像拡張に適格であることを識別し得る。この例では、デジタルコンポーネント要求は、デジタルコンポーネントが提示されるブロックのサイズを特徴付けるデータを含むことができる。別の例では、配信システムは、デジタルコンポーネントの他の視覚要素(たとえば、テキスト、ロゴ、および対話型要素)をデジタルコンポーネントの画像上にオーバーレイすべきではないことを指定したデジタルコンポーネントのプロバイダに基づいて、画像が画像拡張に適格であることを識別し得る。 After identifying the digital component to be sent in response to the request, the distribution system may identify the image contained in the digital component as eligible for image enhancement. In one example, the distribution system determines that an image is eligible for image enhancement based on the size of the image (eg, aspect ratio) differing (eg, by at least a threshold amount) from the size of the block in which the image is presented. can identify something. In this example, the digital component request may include data characterizing the size of the block in which the digital component is presented. In another example, the distribution system may be based on the provider of the digital component specifying that other visual elements of the digital component (e.g., text, logos, and interactive elements) should not be overlaid on the image of the digital component. may identify the image as eligible for image augmentation.
デジタルコンポーネントに含まれる画像が拡張に適格であることを識別したことに応答して、配信システムは、拡張画像を生成するために画像拡張システム100を使用して画像を処理することができる。一例では、配信システムは、拡張画像が表示されるブロックのアスペクト比に一致するアスペクト比を有する拡張画像を生成し得る。別の例では、配信システムは、拡張画像を生成し、次いで、デジタルコンポーネントの追加の視覚要素(たとえば、テキスト、ロゴ、および対話型要素)を、画像の拡張部分上にのみ(すなわち、元の画像を修正することなく)オーバーレイしてもよい。
In response to identifying an image contained in a digital component as eligible for augmentation, the distribution system can process the image using
デジタルコンポーネントに含まれる画像を拡張した後、配信システムは、コンポーネント要求を生成したユーザデバイスにおけるブロックでの提示のために、拡張画像とともにデジタルコンポーネントを送信し得る。 After augmenting the image contained in the digital component, the distribution system may send the digital component along with the augmented image for presentation on the block at the user device that generated the component request.
次に、例示的な環境400および配信システムによって実行される動作について、より詳細に説明する。 The operations performed by the exemplary environment 400 and distribution system are now described in greater detail.
例示的な環境400は、ローカルエリアネットワーク(LAN)、ワイドエリアネットワーク(WAN)、インターネット、またはそれらの組合せなどのネットワーク402を含む。ネットワーク402は、電子ドキュメントサーバ404、クライアントデバイス406、デジタルコンポーネントサーバ408、およびデジタルコンポーネント配信システム410(「配信システム」410とも呼ばれる)を接続する。例示的な環境400は、多くの異なる電子ドキュメントサーバ404、クライアントデバイス406、およびデジタルコンポーネントサーバ408を含み得る。
Exemplary environment 400 includes a
クライアントデバイス406は、ネットワーク402を介してリソースを要求し、受信することが可能な電子デバイスである。例示的なクライアントデバイス406は、パーソナルコンピュータ、モバイル通信デバイス(たとえば、携帯電話)、およびネットワーク402を介してデータを送受信することができる他のデバイスを含む。クライアントデバイス406は、典型的には、ネットワーク402を介したデータの送受信を容易にするために、ウェブブラウザなどのユーザアプリケーションを含むが、クライアントデバイス406によって実行されるネイティブアプリケーションは、ネットワーク402を介したデータの送受信を容易にすることもできる。
電子ドキュメントは、クライアントデバイス406においてコンテンツのセットを提示するデータである。電子ドキュメントの例には、ウェブページ、ワードプロセシングドキュメント、ポータブルドキュメントフォーマット(PDF)ドキュメント、画像、ビデオ、検索結果ページ、およびフィードソースがある。モバイルコンピューティングデバイス、タブレットコンピューティングデバイス、またはデスクトップコンピューティングデバイスにインストールされたアプリケーションなどのネイティブアプリケーション(たとえば、「アプリ」)も、電子ドキュメントの例である。電子ドキュメントは、電子ドキュメントサーバ404(「電子ドキュメントサーバ」)によってクライアントデバイス406に提供することができる。たとえば、電子ドキュメントサーバ404は、発行者ウェブサイトをホストするサーバを含むことができる。この例では、クライアントデバイス406は、所与の発行者ウェブページに対する要求を開始することができ、所与の発行者ウェブページをホストする電子サーバ404は、クライアントデバイス406で所与のウェブページの提示を開始するマシン実行可能命令を送信することによって、要求に応答することができる。
An electronic document is data that presents a set of content on
別の例では、電子ドキュメントサーバ404は、クライアントデバイス406がアプリをダウンロードすることができるアプリサーバを含むことができる。この例では、クライアントデバイス406は、クライアントデバイス406においてアプリをインストールするために必要なファイルをダウンロードし、次いで、ダウンロードされたアプリをローカルに実行することができる。
In another example,
電子ドキュメントは、様々なコンテンツを含むことができる。たとえば、電子ドキュメントは、電子ドキュメント自体の中にあり、および/または時間とともに変化しない静的コンテンツ(たとえば、テキストまたは他の指定されたコンテンツ)を含むことができる。また、電子ドキュメントは、時間の経過とともに、または要求ごとに変化する可能性がある動的コンテンツを含むこともできる。たとえば、所与の電子ドキュメントの発行者は、電子ドキュメントの一部をポピュレートするために使用されるデータソースを維持することができる。この例では、所与の電子ドキュメントは、所与の電子ドキュメントがクライアントデバイス406によって処理(たとえば、レンダリングまたは実行)されると、クライアントデバイス406に、データソースからコンテンツを要求させる1つまたは複数のタグまたはスクリプトを含むことができる。クライアントデバイス406は、データソースから取得されたコンテンツを所与の電子ドキュメントに統合して、データソースから取得されたコンテンツを含む合成電子ドキュメントを作成する。
Electronic documents can contain a variety of content. For example, an electronic document can contain static content (eg, text or other specified content) that is within the electronic document itself and/or that does not change over time. Electronic documents can also contain dynamic content that can change over time or from request to request. For example, a publisher of a given electronic document may maintain data sources used to populate portions of the electronic document. In this example, a given electronic document includes one or more data sources that cause
いくつかの状況では、所与の電子ドキュメントは、デジタルコンポーネント配信システム410を参照する1つまたは複数のデジタルコンポーネントタグまたはデジタルコンポーネントスクリプトを含むことができる。これらの状況では、デジタルコンポーネントタグまたはデジタルコンポーネントスクリプトは、所与の電子ドキュメントがクライアントデバイス406によって処理されると、クライアントデバイス406によって実行される。デジタルコンポーネントタグまたはデジタルコンポーネントスクリプトの実行は、ネットワーク402を介してデジタルコンポーネント配信システム410に送信される1つまたは複数のデジタルコンポーネント412に対する要求(「コンポーネント要求」と呼ばれる)を生成するようにクライアントデバイス406を構成する。たとえば、デジタルコンポーネントタグまたはデジタルコンポーネントスクリプトは、クライアントデバイス406が、ヘッダおよびペイロードデータを含むパケット化されたデータ要求を生成することを可能にすることができる。コンポーネント要求412は、デジタルコンポーネントが要求されているサーバの名前(またはネットワークロケーション)、要求側デバイス(たとえば、クライアントデバイス406)の名前(またはネットワークロケーション)、および/またはデジタルコンポーネント配信システム410が、要求に応答して提供される1つまたは複数のデジタルコンポーネントを選択するために使用することができる情報などの特徴を指定するイベントデータを含むことができる。コンポーネント要求412は、クライアントデバイス406によって、ネットワーク402(たとえば、電気通信ネットワーク)を介して、デジタルコンポーネント配信システム410のサーバに送信される。
In some situations, a given electronic document may include one or more digital component tags or digital component scripts that reference digital
コンポーネント要求412は、要求されている電子ドキュメント、およびデジタルコンポーネントを提示することができる電子ドキュメントのロケーションの特性など、他のイベント特徴を指定するイベントデータを含むことができる。たとえば、デジタルコンポーネントが提示される電子ドキュメント(たとえば、ウェブページ)への参照(たとえば、URL)を指定するイベントデータ、デジタルコンポーネントを提示するために利用可能な電子ドキュメントの利用可能なロケーション、利用可能なロケーションのサイズ、および/またはロケーションでの提示に適格なメディアタイプを、デジタルコンポーネント配信システム410に提供することができる。同様に、電子ドキュメントによって参照される電子ドキュメント(「ドキュメントキーワード」)またはエンティティ(たとえば、人、場所、または物)に関連付けられたキーワードを指定するイベントデータも、コンポーネント要求412に(たとえば、ペイロードデータとして)含め、電子ドキュメントとの提示に適格なデジタルコンポーネントの識別を容易にするためにデジタルコンポーネント配信システム410に提供することができる。イベントデータはまた、検索結果ページを取得するためにクライアントデバイス406からサブミットされた検索クエリ、ならびに/あるいは検索結果および/または検索結果に含まれるテキスト、可聴、または他の視覚コンテンツを指定するデータを含むことができる。
コンポーネント要求412はまた、クライアントデバイスのユーザが提供した情報、コンポーネント要求がサブミットされた状態または領域を示す地理的情報、またはデジタルコンポーネントが表示される環境のコンテキストを提供する他の情報(たとえば、コンポーネント要求の時刻、コンポーネント要求の曜日、モバイルデバイスまたはタブレットデバイスなど、デジタルコンポーネントが表示されるデバイスのタイプ)など、他の情報に関連するイベントデータを含むこともできる。コンポーネント要求412は、たとえば、パケット化ネットワークを介して送信することができ、コンポーネント要求412自体は、ヘッダおよびペイロードデータを有するパケット化データとしてフォーマットすることができる。ヘッダは、パケットの宛先を指定することができ、ペイロードデータは、上述の情報のいずれかを含むことができる。
コンポーネント配信システム410は、コンポーネント要求412の受信に応答して、および/またはコンポーネント要求412に含まれる情報を使用して、所与の電子ドキュメントとともに提示されるデジタルコンポーネントを選択する。いくつかの実装形態では、デジタルコンポーネントの遅延選択によって引き起こされる可能性があるエラーを回避するために、(本明細書で説明される技法を使用して)1秒未満でデジタルコンポーネントが選択される。たとえば、コンポーネント要求412に応答してデジタルコンポーネントを提供する際の遅延は、クライアントデバイス406においてページロードエラーをもたらすか、または電子ドキュメントの他の部分がクライアントデバイス406において提示された後であっても、電子ドキュメントの一部がポピュレートされないままであることを引き起こす可能性がある。また、デジタルコンポーネントをクライアントデバイス406に提供する際の遅延が増大するにつれて、デジタルコンポーネントがクライアントデバイス406に配信されると、電子ドキュメントがもはやクライアントデバイス406に提示されなくなる可能性が高くなり、それによって、電子ドキュメントに関するユーザの経験に悪影響を及ぼす。さらに、デジタルコンポーネントを提供する際の遅延は、たとえば、デジタルコンポーネントが提供されるときに電子ドキュメントがもはやクライアントデバイス406に提示されない場合、デジタルコンポーネントの配信の失敗をもたらす可能性がある。
In response to receiving
いくつかの実装形態では、デジタルコンポーネント配信システム410は、たとえば、相互接続され、要求412に応答してデジタルコンポーネントを識別し、配信する、サーバおよび複数のコンピューティングデバイスのセット414を含む分散コンピューティングシステムに実装される。複数のコンピューティングデバイスのセット414は、一緒に動作して、何百万もの利用可能なデジタルコンポーネント(DC1～x)のコーパスから、電子ドキュメントに提示されるのに適格であるデジタルコンポーネントのセットを識別する。何百万もの利用可能なデジタルコンポーネントは、たとえば、デジタルコンポーネントデータベース416においてインデックス付けすることができる。各デジタルコンポーネントインデックスエントリは、対応するデジタルコンポーネントを参照することができ、および/または、対応するデジタルコンポーネントの配信/送信に寄与する(たとえば、条件付けまたは制限する)配信パラメータ(DP1～DPx)を含むことができる。たとえば、配信パラメータは、コンポーネント要求が、デジタルコンポーネントの配信パラメータのうちの1つに一致する(たとえば、正確に、または何らかの事前に指定された類似性レベルで)少なくとも1つの基準を含むことを要求することによって、デジタルコンポーネントの送信に寄与することができる。
In some implementations, the digital
いくつかの実装形態では、特定のデジタルコンポーネントの配信パラメータは、デジタルコンポーネントが提示に適格であるために、(たとえば、電子ドキュメント、ドキュメントキーワード、またはコンポーネント要求412で指定された用語によって)一致しなければならない配信キーワードを含むことができる。言い換えれば、配信パラメータは、ネットワーク402を介したデジタルコンポーネントの配信(たとえば、送信)をトリガするために使用される。配信パラメータはまた、コンポーネント要求412が、特定の地理的領域(たとえば、国または州)を指定する情報、および/または、デジタルコンポーネントが提示に適格であるために、コンポーネント要求412が特定のタイプのクライアントデバイス(たとえば、モバイルデバイスまたはタブレットデバイス)で発信されたことを指定する情報を含むことを要求することもできる。
In some implementations, delivery parameters for a particular digital component must be matched (eg, by electronic document, document keywords, or terms specified in component request 412) in order for the digital component to be eligible for presentation. It can contain delivery keywords that must be In other words, the distribution parameters are used to trigger distribution (eg, transmission) of digital components over
配信パラメータはまた、たとえば、コンポーネント評価プロセスによって、(たとえば、他の利用可能なデジタルコンポーネントの中で)配信/送信のためのデジタルコンポーネントの適格性を評価するために使用される適格性値(たとえば、ランキングスコア、入札、または何らかの他の指定された値)を指定することもできる。いくつかの状況では、適格性値は、デジタルコンポーネントのプロバイダが、(たとえば、デジタルコンポーネントとのユーザ対話など、デジタルコンポーネントの提示に起因する特定のイベントの各インスタンスについて)デジタルコンポーネントの送信に応答してサブミットする意思がある補償の最大量を指定することができる。 Distribution parameters also include eligibility values (e.g., , ranking score, bid, or some other specified value). In some situations, the eligibility value is a value that the provider of the digital component responded to the submission of the digital component (e.g., for each instance of a particular event resulting from the presentation of the digital component, such as user interaction with the digital component). You can specify the maximum amount of compensation you are willing to submit.
適格なデジタルコンポーネントの識別は、複数のタスク417a～417cにセグメント化することができ、これらのタスクは、次いで、複数のコンピューティングデバイスのセット414内のコンピューティングデバイスの間で割り当てられる。たとえば、セット414内の異なるコンピューティングデバイスは各々、デジタルコンポーネントデータベース416の異なる部分を分析して、コンポーネント要求412に含まれる情報に一致する配信パラメータを有する様々なデジタルコンポーネントを識別することができる。いくつかの実装形態では、セット414内の各所与のコンピューティングデバイスは、異なるデータ次元(または次元のセット)を分析し、分析の結果(結果1～結果3)418a～418cをデジタルコンポーネント配信システム410に渡す(たとえば、送信する)ことができる。たとえば、セット414内のコンピューティングデバイスの各々によって提供される結果418a～418cは、コンポーネント要求および/または特定の配信パラメータを有するデジタルコンポーネントのサブセットに応答して、配信に適格なデジタルコンポーネントのサブセットを識別し得る。デジタルコンポーネントのサブセットの識別は、たとえば、イベントデータを配信パラメータと比較することと、イベントデータの少なくともいくつかの特徴に一致する配信パラメータを有するデジタルコンポーネントのサブセットを識別することとを含むことができる。
The identification of eligible digital components can be segmented into
デジタルコンポーネント配信システム410は、複数のコンピューティングデバイスのセット414から受信された結果418a～418cを集約し、集約された結果に関連付けられた情報を使用して、(i)要求412に応答して提供される1つまたは複数のデジタルコンポーネントを選択し、(ii)1つまたは複数のデジタルコンポーネントの送信要件を決定する。たとえば、デジタルコンポーネント配信システム410は、1つまたは複数のコンポーネント評価プロセスの結果に基づいて、ウィニングデジタルコンポーネント(1つまたは複数のデジタルコンポーネント)のセットを選択することができる。次に、デジタルコンポーネント配信システム410は、ネットワーク402を介して、クライアントデバイス406がウィニングデジタルコンポーネントのセットを所与の電子ドキュメントに統合することを可能にする応答データ420(たとえば、応答を表すデジタルデータ)を生成し、送信することができ、その結果、ウィニングデジタルコンポーネントのセットおよび電子ドキュメントのコンテンツが、クライアントデバイス406のディスプレイに一緒に提示される。
Digital
いくつかの実装形態では、クライアントデバイス406は、応答データ420に含まれる命令を実行し、クライアントデバイス406が1つまたは複数のデジタルコンポーネントサーバからウィニングデジタルコンポーネントのセットを取得するように構成し、それを可能にする。たとえば、応答データ420内の命令は、ネットワークロケーション(たとえば、ユニフォームリソースロケータ(URL))、およびクライアントデバイス406に、サーバ要求(SR)421をデジタルコンポーネントサーバ408に送信させて、デジタルコンポーネントサーバ408から所与のウィニングデジタルコンポーネントを取得させるスクリプトを含むことができる。要求に応答して、デジタルコンポーネントサーバ408は、サーバ要求421(たとえば、複数のデジタルコンポーネントを記憶するデータベース内)で指定された所与のウィニングデジタルコンポーネントを識別し、クライアントデバイス406で電子ドキュメント内の所与のウィニングデジタルコンポーネントを提示するデジタルコンポーネントデータ(DCデータ)422をクライアントデバイス406に送信する。
In some implementations, the
電子ドキュメントの検索を容易にするために、環境400は、電子ドキュメントをクロールし、インデックス付けすることによって(たとえば、電子ドキュメントのクロールされたコンテンツに基づいてインデックス付けされた)、電子ドキュメントを識別する検索システム450を含むことができる。電子ドキュメントに関するデータは、データが関連付けられている電子ドキュメントに基づいてインデックス付けすることができる。インデックス付けされ、随意に、キャッシュされた電子ドキュメントのコピーは、検索インデックス452(たとえば、ハードウェアメモリデバイス)に記憶される。電子ドキュメントに関連付けられるデータは、電子ドキュメントに含まれるコンテンツおよび/または電子ドキュメントのメタデータを表すデータである。
To facilitate searching of electronic documents, environment 400 identifies electronic documents by crawling and indexing the electronic documents (eg, indexed based on crawled content of the electronic documents). A
クライアントデバイス406は、ネットワーク402を介して検索システム450に検索クエリをサブミットすることができる。これに応答して、検索システム450は、検索インデックス452にアクセスして、検索クエリに関連する電子ドキュメントを識別する。検索システム450は、検索結果の形式で電子ドキュメントを識別し、検索結果を検索結果ページでクライアントデバイス406に返す。検索結果は、特定の検索クエリに応答する(たとえば、関連する)電子ドキュメントを識別する検索システム450によって生成されたデータであり、検索結果とのユーザ対話に応答して、クライアントデバイスに、指定されたネットワークロケーション(たとえば、URL)からのデータを要求させるアクティブリンク(たとえば、ハイパーテキストリンク)を含む。例示的な検索結果は、ウェブページタイトル、ウェブページから抽出されたテキストの断片または画像の一部、およびウェブページのURLを含むことができる。別の例示的な検索結果は、ダウンロード可能なアプリケーションのタイトル、ダウンロード可能なアプリケーションを記述するテキストの断片、ダウンロード可能なアプリケーションのユーザインターフェースを描写する画像、および/またはアプリケーションをクライアントデバイス406にダウンロードすることができる位置へのURLを含むことができる。いくつかの状況では、検索システム450は、サブミットされた検索クエリに関連するダウンロード可能なアプリケーションに関する情報を提示するために、クライアントデバイス406にインストールするためにアプリケーションをダウンロードすることができるアプリケーションストア(またはオンラインポータル)の一部とすることができ、またはそれと対話することができる。他の電子ドキュメントと同様に、検索結果ページは、デジタルコンポーネント(たとえば、広告、ビデオクリップ、オーディオクリップ、画像、または他のデジタルコンポーネント)を提示することができる1つまたは複数のスロットを含むことができる。
コンポーネント要求に応答して送信されるデジタルコンポーネントを選択するために、配信システム410は、コンポーネント要求に応答して送信されるのに適格であるデジタルコンポーネントのセットを識別し得る。次いで、配信システム410は、たとえば、オークション手順を介して送信される適格なデジタルコンポーネントのうちの1つまたは複数を選択し得る。いくつかの実装形態では、配信システム410は、それぞれの適格性値に従って適格なデジタルコンポーネントをランク付けし、コンポーネント要求に応答して送信される1つまたは複数の最高ランクのデジタルコンポーネントを選択することによって、オークション手順を実行する。
To select digital components to be sent in response to a component request,
たとえば、配信システム410は、デジタルコンポーネントA、B、およびCを、コンポーネント要求に応答して送信されるのに適格であるものとして識別し得る。この例では、デジタルコンポーネントAは$5の適格性値を有し、デジタルコンポーネントBは$1の適格性値を有し、デジタルコンポーネントCは$5.5の適格性値を有し、デジタルコンポーネントの適格性値は、デジタルコンポーネントに関連付けられた入札を表す。配信システム410は、それぞれの適格性値に従って、C、A、Bのように、デジタルコンポーネントを(たとえば、降順に)ランク付けし得る。最後に、配信システム410は、コンポーネント要求に応答して、送信のために最高ランクのデジタルコンポーネントCを選択してもよい。
For example,
デジタルコンポーネント要求に応答して送信されるデジタルコンポーネントを選択した後、配信システム410は、選択されたデジタルコンポーネントの送信要件を決定する。送信要件は、デジタルコンポーネントの送信に応答してデジタルコンポーネントのプロバイダによって実行されるアクションを指定する。たとえば、送信要件は、デジタルコンポーネントのプロバイダが、デジタルコンポーネントの送信に応答して補償額をサブミットすることを指定し得る。いくつかの場合には、補償額は、デジタルコンポーネントの提示(たとえば、デジタルコンポーネントとのユーザ対話)に起因する特定のイベントの各インスタンスについてサブミットされる額を指定する。
After selecting the digital components to be transmitted in response to the digital component request,
配信システム410は、選択されたデジタルコンポーネントの適格性値、および/またはコンポーネント要求に応答して送信されるのに適格であると決定された他のデジタルコンポーネントの適格性値に基づいて、選択されたデジタルコンポーネントの送信要件を決定し得る。たとえば、配信システム410は、デジタルコンポーネント要求に応答して、デジタルコンポーネントA、B、およびCを送信に適格であると識別することができ、A、B、およびCは、$5、$1、および$5.5のそれぞれの適格値を有する。配信システム410は、(最高の適格性値を有するので)送信のためにデジタルコンポーネントCを選択してもよく、適格なデジタルコンポーネントの適格性値の中から次に高い適格性値であるデジタルコンポーネントCの送信要件を決定してもよい。この例では、次に高い適格性値は、$5(すなわち、デジタルコンポーネントAの適格性値)であり、したがって、配信システム410は、デジタルコンポーネントCの送信要件を$5と決定し得る。
The
図5は、たとえばスマートフォンなど、ユーザデバイスの画面上に表示されているデジタルコンポーネントを示す。デジタルコンポーネントは、デバイスの画面を満たす画像と、画像をオーバーレイするロゴ502、テキストのセグメント504、および対話型要素506を含む追加の視覚要素とを含む。この例では、画像は、デジタルコンポーネント配信システムに提供された第1の部分508と、画像拡張システム100を使用して生成された第2の部分510とを含む。デジタルコンポーネント配信システムは、ユーザデバイスに提供されるデジタルコンポーネントの要求を受信した後、デバイスの画面を満たす拡張画像を生成し、画像の拡張部分510上に追加要素502、504、および506をオーバーレイする。この例では、デジタルコンポーネントが表示される「ブロック」は、ユーザデバイスの画面全体に対応するが、ブロックは、画面のすべてを占めるわけではない。
FIG. 5 shows digital components being displayed on the screen of a user device, eg a smart phone. The digital components include images that fill the screen of the device and additional visual
図6は、たとえば、検索結果とともに、またはサードパーティウェブサイト上に、「バナー」ブロック内に表示されているデジタルコンポーネントを示す。図6と同様に、デジタルコンポーネントは、ブロックを満たす画像と、追加の要素602とを含む。この例では、画像は、デジタルコンポーネント配信システムに提供された第1の部分604と、画像拡張システム100を使用して生成された第2の部分606とを含む。デジタルコンポーネント配信システムは、ユーザデバイス上のバナーブロックに提示されるデジタルコンポーネントの要求を受信した後、ブロックを満たす拡張画像を生成し、画像の拡張部分606上に追加要素602をオーバーレイする。
Figure 6, for example, shows a digital component displayed within a "banner" block, either with search results or on a third-party website. Similar to FIG. 6, the digital component includes an image that fills the block and
図7は、(たとえば、図4を参照して説明したように)デジタルコンポーネント配信システムによる送信のためにデジタルコンポーネントを提供するエンティティ(たとえば、広告主、発行者、または他のコンテンツプロバイダ)に提示することができる例示的なユーザインターフェース700の図である。ユーザインターフェース700は、提供されたデジタルコンポーネントに含まれる画像702と、画像702を拡張する複数の拡張画像704-A～Cとを提示する。一般に、異なる画像拡張手順を使用して生成される結果、拡張画像704-A～Cの各々は異なる。たとえば、拡張画像のうちの1つは、図1を参照して説明した画像拡張システムを使用して生成されていてもよく、拡張画像のうちの別のものは、以下でより詳細に説明する偏微分方程式(PDE)に基づく画像拡張手順を使用して生成されていてもよい。
FIG. 7 is presented to entities (eg, advertisers, publishers, or other content providers) that provide digital components for transmission by digital component distribution systems (eg, as described with reference to FIG. 4). 7 is a diagram of an
ユーザインターフェース700は、デジタルコンポーネントのプロバイダに、(たとえば、デジタルコンポーネントプロバイダにとって最も視覚的に魅力的である)拡張画像のうちの好ましい1つを選択するように促す。デジタルコンポーネントプロバイダは、たとえば、マウス706を使用して(または様々な他の方法のいずれかで)特定の拡張画像をクリックすることによって、特定の拡張画像を選択することができる。配信システムが、提供されたデジタルコンポーネントに含まれる画像が拡張に適格であると後で決定した場合、配信は、ユーザによって選択された特定の拡張画像に対応する画像拡張手順を使用して、画像の拡張を生成する。
画像は、様々な方法で、たとえば、図1を参照して説明した画像拡張システムを使用して、またはPDEに基づく画像拡張手順を使用して、拡張することができる。PDE拡張方法では、画像の拡張部分の画素値は、元の画像の境界の一部の画素値によって指定されるPDE対象境界条件に対する(近似または正確な)解として識別される。PDEは、たとえば、拡散PDE、または任意の他の適切なPDEであってもよい。境界条件に従うPDEに対する解は、任意の適切な数値技法、たとえば、有限要素技法を使用して取得され得る。 Images can be augmented in various ways, for example using the image augmentation system described with reference to FIG. 1 or using a PDE-based image augmentation procedure. In the PDE extension method, the pixel values of the extended portion of the image are identified as the (approximate or exact) solution to the PDE target boundary condition specified by the pixel values of the portion of the boundary of the original image. The PDE may be, for example, a diffuse PDE, or any other suitable PDE. Solutions to PDEs subject to boundary conditions may be obtained using any suitable numerical technique, eg, finite element techniques.
図8は、要求に応答して拡張画像を提供するための例示的なプロセス800のフロー図である。便宜上、プロセス800は、1つまたは複数の位置に配置された1つまたは複数のコンピュータのシステムによって実行されるものとして説明する。たとえば、この仕様に従って適切にプログラムされた配信システム、たとえば、図1の配信システム100は、プロセス800を実行することができる。
FIG. 8 is a flow diagram of an exemplary process 800 for providing augmented images in response to requests. For convenience, process 800 is described as being performed by a system of one or more computers located at one or more locations. For example, a distribution system appropriately programmed according to this specification, such as
システムは、提供された画像が画像拡張に適格であると決定する(802)。一例では、システムは、たとえば、検索結果と一緒に、またはサードパーティウェブページ上に、ブロックに提示されるデジタルコンポーネントの要求を受信し、その後、提供された画像を含むデジタルコンポーネントが要求に応答していると決定し得る。この例では、システムは、たとえば、画像とブロックとが異なるアスペクト比を有する場合、画像のサイズがブロックのサイズとは異なることに基づいて、提供された画像が拡張に適格であると決定し得る。 The system determines (802) that the provided image is eligible for image augmentation. In one example, the system receives a request for a digital component to be presented in a block, e.g., alongside search results or on a third-party web page, and then responds to the request with the digital component containing the provided image. can be determined to be In this example, the system may determine that the provided image is eligible for expansion based on, for example, the size of the image being different than the size of the block if the image and block have different aspect ratios. .
システムは、拡張画像を生成するために生成ニューラルネットワークパラメータの訓練された値に従って入力を処理する生成ニューラルネットワークに画像を含む入力を提供する(804)。拡張画像は、(i)提供された画像よりも多くの行、多くの列、または両方を有し、(ii)提供された画像の現実的な拡張であると予測される。 The system provides 804 an input, including the image, to a generative neural network that processes the input according to trained values of generative neural network parameters to generate an augmented image. The augmented image is expected to (i) have more rows, more columns, or both than the provided image, and (ii) be a realistic extension of the provided image.
生成ニューラルネットワークによって処理される入力は、ベースライン画像およびマスク画像を含むことができ、両方とも拡張画像と同じ数の行および列を有する。ベースライン画像は、(i)提供された画像に対応する第1の部分、および(ii)デフォルトの画素値を有する第2の部分を含む。マスク画像は、ベースライン画像の第1の部分および第2の部分を識別する。一例では、ベースライン画像の第1の部分に対応するマスク画像内の画素は各々、第1の画素値を有し、ベースライン画像の第2の部分に対応するマスク画像内の画素は各々、異なる第2の画素値を有する。 The input processed by the generating neural network can include a baseline image and a mask image, both having the same number of rows and columns as the augmented image. The baseline image includes (i) a first portion corresponding to the provided image, and (ii) a second portion having default pixel values. The mask image identifies first and second portions of the baseline image. In one example, pixels in the mask image corresponding to the first portion of the baseline image each have a first pixel value, and pixels in the mask image corresponding to the second portion of the baseline image each: have different second pixel values.
生成ニューラルネットワークは、複数の畳み込みニューラルネットワーク層を含み、任意の適切なニューラルネットワークアーキテクチャを有することができる。たとえば、生成ニューラルネットワークは、1つもしくは複数のスキップ接続、1つもしくは複数のインスタンス正規化層、またはその両方を含み得る。 The generative neural network includes multiple convolutional neural network layers and can have any suitable neural network architecture. For example, a generative neural network may include one or more skip connections, one or more instance normalization layers, or both.
生成ニューラルネットワークは、敵対的損失目的関数を使用して、弁別ニューラルネットワークと共同で訓練されている。弁別ニューラルネットワークは、生成ニューラルネットワークを使用して所与の画像が生成された可能性を特徴付ける弁別出力を生成するために、所与の画像を処理するように構成される。 A generative neural network is jointly trained with a discriminative neural network using an adversarial loss objective function. A discrimination neural network is configured to process a given image to generate a discrimination output that characterizes the likelihood that the given image was generated using the generating neural network.
このシステムは、複数の訓練反復にわたって敵対的損失目的関数を使用して生成ニューラルネットワークを訓練する。各訓練反復において、システムは、生成ニューラルネットワークを使用して訓練画像を含む訓練入力を処理して、訓練画像を拡張する訓練拡張画像を生成し、次いで、訓練拡張画像に基づいて弁別ニューラルネットワーク入力を生成する。一例では、システムは、訓練画像を使用して、訓練画像に対応する訓練拡張画像の部分を上書きすることによって、弁別ニューラルネットワーク入力を生成する。システムは、生成ニューラルネットワークを使用して弁別ニューラルネットワーク入力が生成された可能性を特徴付ける弁別出力を生成するために、弁別ニューラルネットワークを使用して弁別ニューラルネットワーク入力を処理する。その後、システムは、敵対的損失目的関数に基づいて生成ニューラルネットワークパラメータの現在値を調整し、敵対的損失目的関数は、弁別ニューラルネットワークによって生成された弁別出力に依存する。 This system trains a generative neural network using an adversarial loss objective function over multiple training iterations. At each training iteration, the system processes training input including training images using a generative neural network to generate training augmented images that augment the training images, and then discriminates neural network inputs based on the training augmented images. to generate In one example, the system uses the training images to generate the discriminatory neural network input by overwriting the portions of the training augmented images that correspond to the training images. The system processes the discriminatory neural network input using a discriminative neural network to produce a discriminative output that characterizes the likelihood that the discriminative neural network input was generated using the generative neural network. The system then adjusts the current values of the generated neural network parameters based on the adversarial loss objective function, which depends on the discriminative output produced by the discriminatory neural network.
いくつかの場合には、訓練画像は、ターゲット画像のクロッピングされた表現であり、弁別ニューラルネットワークは、ターゲット画像の意味的特徴表現を条件とする。システムは、ターゲット画像を処理することによって、ターゲット画像の意味表現を、分類ニューラルネットワークによって生成された中間出力であると決定し得る。弁別ニューラルネットワークをターゲット画像の意味的特徴表現で条件付けることは、たとえば、弁別ニューラルネットワークの中間出力とターゲット画像の意味的特徴表現との間の類似性尺度に基づいて弁別出力を決定することを含み得る。弁別出力は、弁別ニューラルネットワークの最終層の出力に基づいてさらに決定されてもよい。 In some cases, the training images are cropped representations of the target images, and the discriminatory neural network is conditioned on semantic feature representations of the target images. By processing the target image, the system may determine the semantic representation of the target image to be the intermediate output produced by the classification neural network. Conditioning the discrimination neural network with the semantic feature representation of the target image means, for example, determining the discrimination output based on a similarity measure between the intermediate output of the discrimination neural network and the semantic feature representation of the target image. can contain. The discrimination output may be further determined based on the output of the final layer of the discrimination neural network.
敵対的損失目的関数を使用して生成ニューラルネットワークを訓練することに加えて、システムは、再構成損失目的関数を使用して生成ニューラルネットワークをさらに訓練してもよい。より具体的には、システムは、訓練拡張画像とターゲット画像との類似性を特徴付ける再構成損失目的関数に基づいて、生成ニューラルネットワークパラメータの現在値を調整し得る。 In addition to training the generative neural network using the adversarial loss objective function, the system may further train the generative neural network using the reconstruction loss objective function. More specifically, the system may adjust the current values of the generated neural network parameters based on a reconstruction loss objective function that characterizes the similarity between the training augmented image and the target image.
システムは、要求に応答して拡張画像を提供する(806)。いくつかの場合には、要求は、ブロック内の画像とともに提示される追加要素を指定し、システムは、拡張画像の拡張部分に追加要素をオーバーレイする。システムが、要求に応答して拡張画像を提供した後、拡張画像は、たとえば、検索結果と一緒に、またはサードパーティウェブページ上に提示されてもよい。 The system provides an augmented image in response to the request (806). In some cases, the request specifies additional elements to be presented with the image in the block, and the system overlays the additional elements on the extended portion of the augmented image. After the system provides the augmented image in response to the request, the augmented image may be presented, for example, alongside search results or on a third party web page.
図9は、上述の動作を実行するために使用することができる例示的なコンピュータシステム900のブロック図である。システム900は、プロセッサ910、メモリ920、記憶デバイス930、および入出力デバイス940を含む。コンポーネント910、920、930、および940の各々は、たとえば、システムバス950を使用して相互接続することができる。プロセッサ910は、システム900内で実行するための命令を処理することが可能である。一実装形態では、プロセッサ910は、シングルスレッドプロセッサである。別の実装形態では、プロセッサ910は、マルチスレッドプロセッサである。プロセッサ910は、メモリ920または記憶デバイス930に記憶された命令を処理することが可能である。
FIG. 9 is a block diagram of an
メモリ920は、システム900内に情報を記憶する。一実装形態では、メモリ920は、コンピュータ可読媒体である。一実装形態では、メモリ920は、揮発性メモリユニットである。別の実装形態では、メモリ920は、不揮発性メモリユニットである。
記憶デバイス930は、システム900のための大容量ストレージを提供することが可能である。一実装形態では、記憶デバイス930は、コンピュータ可読媒体である。様々な異なる実装形態では、記憶デバイス930は、たとえば、ハードディスクデバイス、光ディスクデバイス、複数のコンピューティングデバイス(たとえば、クラウド記憶デバイス)によってネットワーク上で共有される記憶デバイス、または他の何らかの大容量記憶デバイスを含むことができる。
入出力デバイス940は、システム900のための入出力動作を提供する。一実装形態では、入出力デバイス940は、1つまたは複数のネットワークインターフェースデバイス、たとえば、Ethernetカード、シリアル通信デバイス、たとえば、RS-232ポート、および/またはワイヤレスインターフェースデバイス、たとえば、802.11カードを含むことができる。別の実装形態では、入出力デバイスは、入力データを受信し、出力データを他の入出力デバイス、たとえば、キーボード、プリンタ、および表示デバイス960に送信するように構成されたドライバデバイスを含むことができる。しかしながら、モバイルコンピューティングデバイス、モバイル通信デバイス、セットトップボックステレビクライアントデバイスなど、他の実装形態も使用することができる。
Input/
図9には例示的な処理システムが記載されているが、本明細書に記載される主題および機能的動作の実装は、他のタイプのデジタル電子回路、または本明細書に開示された構造およびそれらの構造的均等物を含むコンピュータソフトウェア、ファームウェア、もしくはハードウェア、またはそれらの1つもしくは複数の組合せに実装することができる。 Although an exemplary processing system is described in FIG. 9, implementation of the subject matter and functional operations described herein may be implemented with other types of digital electronic circuits or with the structures and structures disclosed herein. It can be implemented in computer software, firmware, or hardware, including structural equivalents thereof, or any combination of one or more thereof.
本明細書では、システムおよびコンピュータプログラムコンポーネントに関連して「構成された」という用語を使用する。1つまたは複数のコンピュータのシステムが特定の動作またはアクションを実行するように構成されるとは、システムが、動作中、システムに動作またはアクションを実行させるソフトウェア、ファームウェア、ハードウェア、またはそれらの組合せをインストールしていることを意味する。1つまたは複数のコンピュータプログラムが特定の動作またはアクションを実行するように構成されるとは、1つまたは複数のプログラムが、データ処理装置によって実行されると、装置に動作またはアクションを実行させる命令を含むことを意味する。 The term "configured" is used herein in reference to system and computer program components. A system of one or more computers is configured to perform a particular operation or action if the system contains software, firmware, hardware, or a combination thereof that, during operation, causes the system to perform an operation or action. means that you are installing One or more computer programs are configured to perform a particular operation or action if the one or more programs are instructions that, when executed by a data processing device, cause the device to perform the operation or action. is meant to contain
本明細書に記載される主題および機能的動作の実施形態は、デジタル電子回路、有形に実施されたコンピュータソフトウェアまたはファームウェア、本明細書に開示される構造およびそれらの構造的均等物を含むコンピュータハードウェア、またはそれらの1つもしくは複数の組合せに実装することができる。本明細書に記載される主題の実施形態は、1つまたは複数のコンピュータプログラム、すなわち、データ処理装置によって実行される、またはデータ処理装置の動作を制御するための有形の非一時的記憶媒体上に符号化されたコンピュータプログラム命令の1つまたは複数のモジュールとして実装することができる。コンピュータ記憶媒体は、機械可読記憶デバイス、機械可読記憶基板、ランダムまたはシリアルアクセスメモリデバイス、またはそれらの1つもしくは複数の組合せとすることができる。代替的に、または追加として、プログラム命令は、データ処理装置によって実行するための適切な受信機装置への送信のために情報を符号化するために生成された、人工的に生成された伝搬信号、たとえば、機械生成電気、光学、または電磁信号上で符号化することができる。 Embodiments of the subject matter and functional operations described herein include computer hardware including digital electronic circuits, tangibly implemented computer software or firmware, structures disclosed herein and their structural equivalents. hardware, or a combination of one or more thereof. Embodiments of the subject matter described herein comprise one or more computer programs, i.e., stored on tangible, non-transitory storage media, for being executed by or controlling the operation of a data processing apparatus. can be implemented as one or more modules of computer program instructions encoded in A computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more thereof. Alternatively, or additionally, the program instructions may be implemented in an artificially generated propagated signal generated to encode information for transmission to appropriate receiver equipment for execution by data processing equipment. , for example, can be encoded on a machine-generated electrical, optical, or electromagnetic signal.
「データ処理装置」という用語は、データ処理ハードウェアを指し、例として、プログラマブルプロセッサ、コンピュータ、または複数のプロセッサもしくはコンピュータを含む、データを処理するためのあらゆる種類の装置、デバイスおよび機械を包含する。装置は、たとえば、FPGA(フィールドプログラマブルゲートアレイ)もしくはASIC(特定用途向け集積回路)などの専用論理回路でもよく、またはそれをさらに含むこともできる。装置は、随意に、ハードウェアに加えて、コンピュータプログラムの実行環境を作成するコード、たとえば、プロセッサファームウェア、プロトコルスタック、データベース管理システム、オペレーティングシステム、またはそれらの1つもしくは複数の組合せを構成するコードを含むことができる。 The term "data processing apparatus" refers to data processing hardware and encompasses, by way of example, all kinds of apparatus, devices and machines for processing data, including programmable processors, computers, or multiple processors or computers. . The device may, for example, be or may further include dedicated logic circuitry such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits). The apparatus optionally includes, in addition to hardware, code that creates an execution environment for computer programs, e.g. code that makes up processor firmware, protocol stacks, database management systems, operating systems, or combinations of one or more thereof. can include
プログラム、ソフトウェア、ソフトウェアアプリケーション、アプリ、モジュール、ソフトウェアモジュール、スクリプト、またはコードとも呼ばれるまたは記述されることもあるコンピュータプログラムは、コンパイルもしくはインタープリタ型言語、宣言型言語もしくは手続き型言語を含む、任意の形式のプログラミング言語で記述することができ、それは、スタンドアロンプログラムとして、またはモジュール、コンポーネント、サブルーチン、もしくはコンピューティング環境での使用に適した他のユニットとしてなど、あらゆる形式で展開することができる。プログラムは、必ずしも必要はないが、ファイルシステム内のファイルに対応し得る。プログラムは、当該のプログラム専用の単一のファイル、または、たとえば、1つもしくは複数のモジュール、サブプログラム、もしくはコードの一部を記憶するファイルなど、複数のコーディネートされたファイルに、たとえば、マークアップ言語文書に記憶された1つもしくは複数のスクリプトなど、他のプログラムもしくはデータを保持するファイルの一部に記憶することができる。コンピュータプログラムは、1つのコンピュータ上で、または1つのサイトに位置するか、もしくは複数のサイトに分散され、データ通信ネットワークによって相互接続された複数のコンピュータ上で実行されるように配備することができる。 A computer program, also called or written as a program, software, software application, app, module, software module, script, or code, may be written in any form, including compiled or interpreted language, declarative language, or procedural language. programming language, and it can be deployed in any form, such as as a stand-alone program or as modules, components, subroutines, or other units suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program may be stored in a single file dedicated to the program in question, or in multiple coordinated files, e.g. It can be stored in part of a file that holds other programs or data, such as one or more scripts stored in language documents. A computer program can be deployed to be executed on one computer or on multiple computers located at one site or distributed across multiple sites and interconnected by a data communication network. .
本明細書では、「エンジン」という用語は、1つもしくは複数の特定の機能を実行するようにプログラムされているソフトウェアベースのシステム、サブシステム、またはプロセスを指すために広く使用されている。一般に、エンジンは、1つまたは複数の位置にある1つまたは複数のコンピュータにインストールされた1つまたは複数のソフトウェアモジュールまたは構成要素として実装される。いくつかの場合には、1つまたは複数のコンピュータが特定のエンジンに専用であり、他の場合には、複数のエンジンを、同じ1つまたは複数のコンピュータにインストールし、そこにおいて実行することができる。 The term "engine" is used broadly herein to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. An engine is typically implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers are dedicated to a particular engine, and in other cases multiple engines may be installed and run on the same computer or computers. can.
本明細書に記載されるプロセスおよび論理フローは、入力データ上で動作し、出力を生成することによって機能を実行するために、1つまたは複数のコンピュータプログラムを実行する1つまたは複数のプログラム可能コンピュータによって実行することができる。プロセスおよび論理フローは、たとえばFPGAもしくはASICなどの専用論理回路によって、または専用論理回路と1つもしくは複数のプログラムされたコンピュータとの組合せによっても実行することができる。 The processes and logic flows described herein are one or more programmable programs that run one or more computer programs to perform functions by operating on input data and generating output. It can be run by a computer. The processes and logic flows can also be performed by dedicated logic circuits, such as FPGAs or ASICs, or by a combination of dedicated logic circuits and one or more programmed computers.
コンピュータプログラムの実行に適したコンピュータは、汎用マイクロプロセッサもしくは専用マイクロプロセッサ、もしくはその両方、または他の種類の中央処理ユニットに基づくことができる。一般に、中央処理ユニットは、読取り専用メモリまたはランダムアクセスメモリまたはその両方から命令およびデータを受信する。コンピュータの必須要素は、命令を実施または実行するための中央処理ユニット、ならびに命令およびデータを記憶するための1つまたは複数のメモリデバイスである。中央処理ユニットおよびメモリは、専用論理回路によって補うまたはそこに組み込むことができる。一般に、コンピュータは、たとえば磁気、光磁気ディスク、または光ディスクなど、データを記憶するための1つまたは複数の大容量記憶デバイスも含み、あるいは、1つまたは複数の大容量記憶デバイスからデータを受信する、それにデータを転送する、またはその両方のために動作可能に結合される。しかしながら、コンピュータはそのようなデバイスを有する必要はない。さらに、コンピュータは、別のデバイス、たとえば、ほんのいくつかの例を挙げれば、携帯電話、携帯情報端末(PDA)、モバイルオーディオもしくはビデオプレーヤ、ゲームコンソール、全地球測位システム(GPS)受信機、またはユニバーサルシリアルバス(USB)フラッシュドライブなどのポータブル記憶デバイス中に組み込むことができる。 Computers suitable for the execution of computer programs may be based on general or special purpose microprocessors, or both, or other types of central processing units. Generally, a central processing unit receives instructions and data from read-only memory and/or random-access memory. The essential elements of a computer are a central processing unit for implementing or executing instructions, and one or more memory devices for storing instructions and data. The central processing unit and memory may be supplemented by or incorporated in dedicated logic circuitry. Generally, a computer also includes one or more mass storage devices, such as magnetic, magneto-optical, or optical disks, for storing data on or receives data from one or more mass storage devices , to transfer data to it, or both. However, a computer need not have such devices. Additionally, the computer may be connected to another device such as a mobile phone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a global positioning system (GPS) receiver, or It can be incorporated into a portable storage device such as a Universal Serial Bus (USB) flash drive.
コンピュータプログラム命令およびデータを記憶するのに適したコンピュータ可読媒体は、例として、たとえば、EPROM、EEPROM、およびフラッシュメモリデバイスなどの半導体メモリデバイス、たとえば内部ハードディスクまたはリムーバブルディスクなどの磁気ディスク、光磁気ディスク、ならびにCD-ROMおよびDVD-ROMディスクを含むすべての形態の不揮発性メモリ、媒体、およびメモリデバイスを含む。 Computer readable media suitable for storing computer program instructions and data include, for example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; , and all forms of non-volatile memory, media and memory devices including CD-ROM and DVD-ROM discs.
ユーザとの対話を提供するために、本明細書に記載される主題の実施形態は、ユーザに情報を表示するための、CRT(陰極線管)またはLCD(液晶ディスプレイ)モニタなどのディスプレイデバイス、ならびにキーボードおよび、ユーザがコンピュータに入力を提供することができる、たとえば、マウスまたはトラックボールなどのポインティングデバイスを有するコンピュータ上に実装することができる。他の種類のデバイスを使用して、ユーザとの対話を提供することもでき、たとえば、ユーザに提供されるフィードバックは、たとえば、視覚フィードバック、聴覚フィードバック、または触覚フィードバックなど、任意の形態の感覚フィードバックとすることができ、ユーザからの入力は、音響、音声、または触覚入力を含む任意の形態で受信することができる。さらに、コンピュータは、たとえば、ウェブブラウザから受信された要求に応答して、ユーザのデバイス上のウェブブラウザにウェブページを送信することによってなど、ユーザによって使用されるデバイスとの間でドキュメントを送受信することによって、ユーザと対話することができる。また、コンピュータは、テキストメッセージまたは他の形態のメッセージをパーソナルデバイス、たとえば、メッセージングアプリケーションを実行しているスマートフォンに送信し、代わりに、ユーザから応答メッセージを受信することによって、ユーザと対話することができる。 To provide interaction with a user, embodiments of the subject matter described herein include a display device, such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, and a It can be implemented on a computer having a keyboard and a pointing device such as, for example, a mouse or trackball that allows a user to provide input to the computer. Other types of devices may also be used to provide user interaction, e.g., the feedback provided to the user may be any form of sensory feedback, e.g., visual, auditory, or tactile feedback. and input from the user can be received in any form, including acoustic, speech, or tactile input. Additionally, the computer sends and receives documents to and from the device used by the user, for example, by sending web pages to a web browser on the user's device in response to requests received from the web browser. By doing so, it is possible to interact with the user. Computers may also interact with users by sending text messages or other forms of messages to personal devices, e.g., smart phones running messaging applications, and in return receiving reply messages from the users. can.
機械学習モデルを実装するためのデータ処理装置はまた、たとえば、機械学習のトレーニングまたは製作、すなわち推論、作業負荷の共通部分および計算集約的部分を処理するための専用ハードウェアアクセラレータユニットを含むこともできる。 Data processors for implementing machine learning models may also include dedicated hardware accelerator units, for example, for processing machine learning training or production, i.e. inference, common and computationally intensive parts of the workload. can.
機械学習モデルは、機械学習フレームワーク、たとえば、TensorFlowフレームワーク、Microsoft Cognitive Toolkitフレームワーク、Apache Singaフレームワーク、またはApache MXNetフレームワークを使用して実装および展開することができる。 Machine learning models can be implemented and deployed using machine learning frameworks, such as the TensorFlow framework, the Microsoft Cognitive Toolkit framework, the Apache Singa framework, or the Apache MXNet framework.
本明細書に記載される主題の実施形態は、たとえばデータサーバとしてのバックエンド構成要素を含む、またはアプリケーションサーバなどのミドルウェア構成要素を含む、またはたとえば、ユーザが本明細書に記載される主題の実装と対話することができる、グラフィカルユーザインターフェース、ウェブブラウザ、またはアプリを有するクライアントコンピュータなどのフロントエンド構成要素を含む、または1つもしくは複数のそのようなバックエンド、ミドルウェア、またはフロントエンド構成要素の任意の組合せを含むコンピューティングシステムにおいて実装することができる。システムの構成要素は、たとえば、通信ネットワークなど、任意の形式または媒体のデジタルデータ通信によって相互接続することができる。通信ネットワークの例には、ローカルエリアネットワーク(LAN)およびワイドエリアネットワーク(WAN)、たとえばインターネットがある。 Embodiments of the subject matter described herein include back-end components, such as data servers, or middleware components, such as application servers, or, for example, when a user including a front-end component such as a client computer with a graphical user interface, web browser, or app capable of interacting with the implementation, or one or more of such back-end, middleware, or front-end components It can be implemented in any computing system including any combination. The components of the system can be interconnected by any form or medium of digital data communication, eg, a communication network. Examples of communication networks include local area networks (LAN) and wide area networks (WAN) such as the Internet.
コンピューティングシステムは、クライアントおよびサーバを含むことができる。クライアントとサーバとは、一般に、互いに遠隔であり、典型的には、通信ネットワークを介して対話する。クライアントとサーバとの関係は、それぞれのコンピュータ上で実行され、互いにクライアントサーバ関係を有するコンピュータプログラムによって生じる。いくつかの実施形態では、サーバは、たとえば、クライアントとして動作するデバイスと対話しているユーザにデータを表示し、ユーザからユーザ入力を受信するために、データ、たとえば、HTMLページをユーザデバイスに送信する。たとえば、ユーザ対話の結果など、ユーザデバイスにおいて生成されたデータは、デバイスからサーバにおいて受信することができる。 The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data, e.g., HTML pages, to the user device, e.g., to display data to a user interacting with the device acting as a client and to receive user input from the user. do. For example, data generated at the user device, such as the results of user interactions, can be received at the server from the device.
本明細書は、多くの具体的な実施の詳細を含むが、これらは、いかなる発明の範囲または特許請求される可能性のある範囲に対する限定ではなく、むしろ特定の発明の特定の実施形態に固有であり得る特徴の説明として解釈されるものとする。別個の実施形態の文脈において本明細書に記載されるいくつかの特徴は、単一の実施形態において組み合わせて実装することもできる。逆に、単一の実施形態の文脈で記載されている様々な特徴は、複数の実施形態で別々にまたは任意の適切な部分組合せで実装することもできる。さらに、特徴は、いくつかの組合せで作用するものとして上述されており、当初はそのように請求されているが、いくつかの場合、請求された組合せからの1つまたは複数の特徴を、組合せから削除することができ、請求された組合せは、部分組合せ、または部分組合せの変形を対象とし得る。 While this specification contains many specific implementation details, these are not limitations on the scope of any invention or that may be claimed, but rather are specific to particular embodiments of particular inventions. shall be construed as a description of the features that may be Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Further, although features have been described above and originally claimed as working in some combination, in some cases one or more features from the claimed combinations may be used in combination. and claimed combinations may cover subcombinations or variations of subcombinations.
同様に、動作が図面に示され、特許請求の範囲に特定の順序で記載されているが、これは、そのような動作が、示された特定の順序で、または逐次的な順序で実行されること、あるいは望ましい結果を達成するために、図示されたすべての動作が実行されることを必要とするものとして理解されないものとする。いくつかの状況では、マルチタスキングおよび並列処理が有利であり得る。さらに、上述した実施形態における様々なシステムモジュールおよび構成要素の分離は、すべての実施形態においてそのような分離を必要とするものと理解されないものとし、記述されたプログラム構成要素およびシステムを、一般に、単一のソフトウェア製品に一緒に組み入れることができ、または複数のソフトウェア製品にパッケージ化することができることを理解されたい。 Similarly, while acts are shown in the drawings and recited in the claims in a particular order, this does not mean that such acts are performed in the specific order shown or in sequential order. should not be construed as requiring that all illustrated acts be performed in order to achieve the desired result. Multitasking and parallel processing may be advantageous in some situations. Furthermore, the separation of various system modules and components in the above-described embodiments should not be understood to require such separation in all embodiments, and the program components and systems described generally It should be understood that they can be incorporated together in a single software product or packaged in multiple software products.
主題の特定の実施形態が記載されている。他の実施形態は、以下の特許請求の範囲内にある。たとえば、特許請求の範囲に列挙されたアクションは、異なる順序で実行され、依然として望ましい結果を達成することができる。一例として、添付の図面に示されるプロセスは、望ましい結果を達成するために、示された特定の順序または逐次的な順序を必ずしも必要としない。いくつかの場合には、マルチタスキングおよび並列処理が有利であり得る。 Specific embodiments of the subject matter have been described. Other embodiments are within the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As an example, the processes illustrated in the accompanying figures do not necessarily require the particular order shown or sequential order to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
100 画像拡張システム
102 画像
104 拡張画像
106 列
108 生成ネットワーク入力
110 生成ニューラルネットワーク
112 ベースライン画像
114 マスク画像
116 敵対的訓練システム
200 データフロー
202 弁別ニューラルネットワーク
204 敵対的損失目的関数
206 ターゲット拡張画像
208 ベースライン画像
210 マスク画像
212 訓練拡張画像
214 正規化エンジン
216 ニューラルネットワーク層
218 中間層
220 再構成目的関数
300 拡張画像
302 元の画像
400 環境
402 ネットワーク
404 電子ドキュメントサーバ
406 クライアントデバイス
408 デジタルコンポーネントサーバ
410 デジタルコンポーネント配信システム
412 コンポーネント要求
414 複数のコンピューティングデバイスのセット
416 デジタルコンポーネントデータベース
417a～417c タスク
418a～418c 結果
420 応答データ
421 サーバ要求(SR)
422 デジタルコンポーネントデータ(DCデータ)
450 検索システム
452 検索インデックス
502 ロゴ
504 テキストのセグメント
506 対話型要素
508 第1の部分
510 第2の部分
602 追加の要素
604 第1の部分
606 第2の部分
700 ユーザインターフェース
702 画像
704-A～C 拡張画像
706 マウス
900 コンピュータシステム
910 プロセッサ
920 メモリ
930 記憶デバイス
940 入出力デバイス
950 システムバス
960 表示デバイス
100 image enhancement system
102 images
104 Augmented Image
106 columns
108 generation network inputs
110 Generative Neural Networks
112 baseline images
114 mask image
116 Hostile Training System
200 data flow
202 Discrimination Neural Network
204 Adversarial Loss Objective Function
206 target augmented images
208 baseline images
210 mask images
212 training augmented images
214 normalization engine
216 neural network layers
218 middle layer
220 reconstruction objective function
300 extended images
302 original images
400 environment
402 network
404 Electronic Document Server
406 client device
408 Digital Component Server
410 Digital Component Distribution System
412 Component Request
414 set of multiple computing devices
416 Digital Component Database
417a-417c tasks
418a-418c Results
420 response data
421 Server Request (SR)
422 digital component data (DC data)
450 search system
452 search index
502 logo
504 segment of text
506 interactive elements
508 first part
510 second part
602 Additional Elements
604 first part
606 second part
700 User Interface
702 images
704-A-C Extended Images
706 mouse
900 computer system
910 processor
920 memory
930 storage device
940 input/output device
950 system bus
960 display device
Claims (20)
複数の生成ニューラルネットワークパラメータを有する生成ニューラルネットワークに、提供された画像を含む入力を提供するステップを含み、
前記生成ニューラルネットワークは、拡張画像を生成するために、前記複数の生成ニューラルネットワークパラメータの訓練された値に従って前記入力を処理し、
前記拡張画像は、(i)前記提供された画像よりも多くの行、多くの列、または両方を有し、(ii)前記提供された画像を現実的に拡張した画像であると予測され、
前記生成ニューラルネットワークは、敵対的損失目的関数を使用して訓練されている、
方法。 A method performed by one or more data processing apparatus, comprising:
providing input comprising the provided image to a generative neural network having a plurality of generative neural network parameters;
the generative neural network processes the input according to trained values of the plurality of generative neural network parameters to generate an augmented image;
the augmented image is expected to (i) have more rows, more columns, or both than the provided image, and (ii) be a realistic augmentation of the provided image;
wherein the generative neural network is trained using an adversarial loss objective function;
Method.
前記提供された画像が前記要求に応答していると決定するステップと、
前記拡張画像を生成した後、前記要求に応答して前記拡張画像を提供するステップと
をさらに含む請求項1に記載の方法。 receiving a request for an image to be presented in the block;
determining that the provided image is responsive to the request;
and providing the augmented image in response to the request after generating the augmented image.
をさらに含む請求項2に記載の方法。 determining that the provided image is eligible for image enhancement based on the size of the block being different than the size of the provided image, and responsively including the provided image; 3. The method of claim 2, further comprising: providing input to the generative neural network.
前記要求に応答して前記拡張画像を提供するステップは、前記拡張画像の拡張部分上に前記追加要素をオーバーレイするステップを含む、
請求項2～4のいずれか一項に記載の方法。 the request specifies additional elements to be presented with the image within the block;
providing the augmented image in response to the request includes overlaying the additional element on an augmented portion of the augmented image;
A method according to any one of claims 2-4.
前記拡張画像と同じ数の行および列を有するベースライン画像であり、(i)前記提供された画像に対応する第1の部分、および(ii)デフォルトの画素値を有する第2の部分を含む、ベースライン画像と、
前記拡張画像と同じ数の行および列を有するマスク画像であり、前記ベースライン画像の前記第1の部分および前記第2の部分を識別する、マスク画像と
を含む、請求項1～9のいずれか一項に記載の方法。 The input to the generative neural network comprises:
a baseline image having the same number of rows and columns as said augmented image, comprising (i) a first portion corresponding to said provided image, and (ii) a second portion having default pixel values; , the baseline image, and
a mask image having the same number of rows and columns as the augmented image, the mask image identifying the first portion and the second portion of the baseline image. or the method described in paragraph 1.
請求項10に記載の方法。 (i) pixels in the mask image that correspond to the first portion of the baseline image each have a first pixel value; and (ii) correspond to the second portion of the baseline image. each pixel in the mask image has a second pixel value that is different than the first pixel value;
11. The method of claim 10.
訓練画像を拡張する訓練拡張画像を生成するために、前記生成ニューラルネットワークを使用し、前記生成ニューラルネットワークパラメータの現在の値に従って、前記訓練画像を含む訓練入力を処理するステップと、
前記訓練拡張画像に基づいて弁別ニューラルネットワーク入力を生成するステップと、
前記生成ニューラルネットワークを使用して前記弁別ニューラルネットワーク入力が生成された可能性を特徴付ける弁別出力を生成するために、前記弁別ニューラルネットワークを使用し、前記弁別ニューラルネットワークパラメータの現在の値に従って、前記弁別ニューラルネットワーク入力を処理するステップと、
前記敵対的損失目的関数に基づいて前記生成ニューラルネットワークパラメータの現在値を調整するステップであり、前記敵対的損失目的関数は、前記生成ニューラルネットワークを使用して前記弁別ニューラルネットワーク入力が生成された前記可能性を特徴付ける前記弁別出力に依存する、調整するステップと
を含む、請求項12に記載の方法。 training the generative neural network using the adversarial loss objective function,
using the generating neural network to process a training input comprising the training images according to current values of the generating neural network parameters to generate training augmented images that augment the training images;
generating a discriminatory neural network input based on the training augmented images;
using the discrimination neural network to generate a discrimination output that characterizes the likelihood that the discrimination neural network input was generated using the generator neural network, the discrimination according to current values of the discrimination neural network parameters; processing a neural network input;
adjusting the current values of the generated neural network parameters based on the adversarial loss objective function, the adversarial loss objective function being the discriminatory neural network input generated using the generative neural network; 13. The method of claim 12, comprising: adjusting depending on the discrimination output characterizing probabilities.
前記訓練画像に対応する前記訓練拡張画像の一部を前記訓練画像で上書きするステップを含む、
請求項13に記載の方法。 generating a discriminative neural network input based on the training augmented images,
overwriting a portion of the training augmented image corresponding to the training image with the training image;
14. The method of claim 13.
をさらに含む請求項13または14に記載の方法。 adjusting the current values of the generated neural network parameters based on a reconstruction loss objective function characterizing the similarity between the training augmented image and a target image, the training image being a cropped image of the target image; 15. A method according to claim 13 or 14, further comprising the step of adjusting the expression.
に基づく、請求項16または17に記載の方法。 wherein said discrimination output is based on: (i) an output of a final layer of said discrimination neural network; and (ii) a similarity measure between an intermediate output of said discrimination neural network and said semantic feature representation of said target image. 18. The method of paragraph 16 or 17.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2023068765A JP7477682B2 (en) | 2019-05-24 | 2023-04-19 | Image Augmentation Neural Network |
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962852949P | 2019-05-24 | 2019-05-24 | |
US62/852,949 | 2019-05-24 | ||
US201962854833P | 2019-05-30 | 2019-05-30 | |
US62/854,833 | 2019-05-30 | ||
PCT/US2019/042509 WO2020242508A1 (en) | 2019-05-24 | 2019-07-19 | Image extension neural networks |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2023068765A Division JP7477682B2 (en) | 2019-05-24 | 2023-04-19 | Image Augmentation Neural Network |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2022533519A JP2022533519A (en) | 2022-07-25 |
JP7267453B2 true JP7267453B2 (en) | 2023-05-01 |
Family
ID=67515178
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021558609A Active JP7267453B2 (en) | 2019-05-24 | 2019-07-19 | image augmentation neural network |
JP2023068765A Active JP7477682B2 (en) | 2019-05-24 | 2023-04-19 | Image Augmentation Neural Network |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2023068765A Active JP7477682B2 (en) | 2019-05-24 | 2023-04-19 | Image Augmentation Neural Network |
Country Status (5)
Country | Link |
---|---|
US (1) | US20220148299A1 (en) |
EP (1) | EP3762873A1 (en) |
JP (2) | JP7267453B2 (en) |
CN (1) | CN113646773A (en) |
WO (1) | WO2020242508A1 (en) |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11455531B2 (en) * | 2019-10-15 | 2022-09-27 | Siemens Aktiengesellschaft | Trustworthy predictions using deep neural networks based on adversarial calibration |
US20210192684A1 (en) * | 2019-12-24 | 2021-06-24 | Nvidia Corporation | Panorama generation using one or more neural networks |
CN113034348A (en) * | 2021-03-24 | 2021-06-25 | 北京字节跳动网络技术有限公司 | Image processing method, image processing apparatus, storage medium, and device |
CN115100472B (en) * | 2022-06-20 | 2023-06-27 | 北京达佳互联信息技术有限公司 | Training method and device for display object recognition model and electronic equipment |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2015041138A (en) | 2013-08-20 | 2015-03-02 | 昭芳 村岡 | District correspondence type cooperative advertising method and site system using internet |
JP2018110011A (en) | 2012-08-16 | 2018-07-12 | ネイバー コーポレーションＮＡＶＥＲ Ｃｏｒｐｏｒａｔｉｏｎ | Image analysis based automatic image editing device, method and computer readable recording medium |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7154538B1 (en) * | 1999-11-15 | 2006-12-26 | Canon Kabushiki Kaisha | Image processing system, image processing method, image upload system, storage medium, and image upload server |
US11074495B2 (en) * | 2013-02-28 | 2021-07-27 | Z Advanced Computing, Inc. (Zac) | System and method for extremely efficient image and pattern recognition and artificial intelligence platform |
US9990712B2 (en) * | 2015-04-08 | 2018-06-05 | Algotec Systems Ltd. | Organ detection and segmentation |
US10319076B2 (en) * | 2016-06-16 | 2019-06-11 | Facebook, Inc. | Producing higher-quality samples of natural images |
US20180349526A1 (en) * | 2016-06-28 | 2018-12-06 | Cognata Ltd. | Method and system for creating and simulating a realistic 3d virtual world |
US11188783B2 (en) | 2017-10-19 | 2021-11-30 | Nokia Technologies Oy | Reverse neural network for object re-identification |
JP2019079114A (en) | 2017-10-20 | 2019-05-23 | キヤノン株式会社 | Image processing device, image processing method, and program |
US11741693B2 (en) | 2017-11-15 | 2023-08-29 | Palo Alto Research Center Incorporated | System and method for semi-supervised conditional generative modeling using adversarial networks |
CN108197525B (en) * | 2017-11-20 | 2020-08-11 | 中国科学院自动化研究所 | Face image generation method and device |
CN108875510B (en) * | 2017-11-28 | 2020-12-01 | 北京旷视科技有限公司 | Image processing method, device, system and computer storage medium |
CN109360232B (en) * | 2018-09-10 | 2021-04-06 | 南京邮电大学 | Indoor scene layout estimation method and device based on condition generation countermeasure network |
-
2019
- 2019-07-19 WO PCT/US2019/042509 patent/WO2020242508A1/en unknown
- 2019-07-19 JP JP2021558609A patent/JP7267453B2/en active Active
- 2019-07-19 EP EP19749105.3A patent/EP3762873A1/en active Pending
- 2019-07-19 US US17/438,687 patent/US20220148299A1/en active Pending
- 2019-07-19 CN CN201980095109.9A patent/CN113646773A/en active Pending
-
2023
- 2023-04-19 JP JP2023068765A patent/JP7477682B2/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2018110011A (en) | 2012-08-16 | 2018-07-12 | ネイバー コーポレーションＮＡＶＥＲ Ｃｏｒｐｏｒａｔｉｏｎ | Image analysis based automatic image editing device, method and computer readable recording medium |
JP2015041138A (en) | 2013-08-20 | 2015-03-02 | 昭芳 村岡 | District correspondence type cooperative advertising method and site system using internet |
Also Published As
Publication number | Publication date |
---|---|
CN113646773A (en) | 2021-11-12 |
EP3762873A1 (en) | 2021-01-13 |
US20220148299A1 (en) | 2022-05-12 |
JP7477682B2 (en) | 2024-05-01 |
WO2020242508A1 (en) | 2020-12-03 |
JP2022533519A (en) | 2022-07-25 |
JP2023110921A (en) | 2023-08-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7267453B2 (en) | image augmentation neural network | |
US20230177343A1 (en) | Scene understanding and generation using neural networks | |
US20210081796A1 (en) | Neural architecture search for dense image prediction tasks | |
US8374914B2 (en) | Advertising using image comparison | |
US20100312608A1 (en) | Content advertisements for video | |
US9678928B1 (en) | Webpage partial rendering engine | |
CN111800671B (en) | Method and apparatus for aligning paragraphs and video | |
CN110071938B (en) | Virtual image interaction method and device, electronic equipment and readable storage medium | |
JP7457800B2 (en) | Image replacement repair | |
CN111866610A (en) | Method and apparatus for generating information | |
US11257217B2 (en) | Image segmentation using neural networks | |
JP7299327B2 (en) | Generate video | |
US20180365536A1 (en) | Identification of fonts in an application | |
US10339469B2 (en) | Self-adaptive display layout system | |
WO2020154537A1 (en) | Convolutional neural networks with soft kernel selection | |
CN111897950A (en) | Method and apparatus for generating information | |
US20150046270A1 (en) | System and Method of Using Artificial Intelligence to Valuate Advertisements Embedded Within Images | |
US20120323702A1 (en) | System and method for client-server cooperation in selecting content for display | |
CN109472028B (en) | Method and device for generating information | |
US11887155B2 (en) | Method and a system for selecting a targeted message to be included within a web resource | |
US10740571B1 (en) | Generating neural network outputs using insertion operations | |
US10902479B2 (en) | Programmatic generation and optimization of images for a computerized graphical advertisement display | |
US20220253695A1 (en) | Parallel cascaded neural networks | |
US10891653B1 (en) | Approaches for retrieval of electronic advertisements | |
CN117743675A (en) | Resource recall method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20211019 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20221125 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20221219 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230307 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20230320 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20230419 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7267453Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |