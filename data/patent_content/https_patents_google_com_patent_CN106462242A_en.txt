CN106462242A - User interface control using gaze tracking - Google Patents
User interface control using gaze tracking Download PDFInfo
- Publication number
- CN106462242A CN106462242A CN201580026804.1A CN201580026804A CN106462242A CN 106462242 A CN106462242 A CN 106462242A CN 201580026804 A CN201580026804 A CN 201580026804A CN 106462242 A CN106462242 A CN 106462242A
- Authority
- CN
- China
- Prior art keywords
- image
- user
- face
- feature point
- face feature
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/013—Eye tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/012—Head tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/73—Determining position or orientation of objects or cameras using feature-based methods
- G06T7/74—Determining position or orientation of objects or cameras using feature-based methods involving reference images or patches
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
- G06V40/165—Detection; Localisation; Normalisation using facial parts and geometric relationships
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
- G06V40/167—Detection; Localisation; Normalisation using comparisons between temporally consecutive images
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/18—Eye characteristics, e.g. of the iris
- G06V40/19—Sensors therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/18—Eye characteristics, e.g. of the iris
- G06V40/193—Preprocessing; Feature extraction
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30196—Human being; Person
- G06T2207/30201—Face
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for identifying a direction in which a user is looking. In one aspect, a method includes receiving an image of a sequence of images. The image can depict a face of a user. A template image for each particular facial feature point can be compared to one or more image portions of the image. The template image for the particular facial feature point can include a portion of a previous image of the sequence of images that depicted the facial feature point. Based on the comparison, a matching image portion of the image that matches the template image for the particular facial feature point is identified. A location of the matching image portion is identified in the image. A direction in which the user is looking is determined based on the identified location for each template image.
Description
Cross-Reference to Related Applications
This application claims entitled " the USER INTERFACE CONTROL USING GAZE that on April 23rd, 2014 submits to
U.S. Provisional Patent Application No.61/983,232 of TRACKING (using the user interface control of eye tracking) " exists
Rights and interests under 35U.S.C. § 119 (e), entire contents are incorporated herein by reference.
Background technology
Input equipment for computer and other electronic equipments depends on touch interface, such as keyboard, mouse, touches
Template and touch screen.These touch interfaces typically require effective eye in one's hands coordinate combine handss arrive screen cursor coordination with by electricity
The object interaction that sub- equipment shows.Speech recognition technology can eliminate or reduce the co-ordination needed for touch interface.For example, use
Family can provide order using phonetic entry, and this order can be converted into for electronic equipment for speech recognition system
Machine readable instructions.
Content of the invention
Present specification describes the technology related to eye tracking.
In general, novel aspects of the theme described in this specification can be embodied in the side including following action
In method：Receive the image in image sequence, described image describes the face of user；For the described face described in described image
Face feature point in each specific face feature point, by template image and described figure for described specific face feature point
One or more image sections of picture are compared, and the described template image for described specific face feature point includes describing institute
State a part for the prior images in the described image sequence of face feature point；Based on described relatively come identify with for described spy
Determine the coupling image section of the described image of described template image coupling of face feature point；And identify institute in described image
State the position of coupling image section；And the side that user is seeing is determined based on the position being identified of each template image
To.The other embodiment of this aspect includes the correspondence being configured to execute the action of method on computer memory device for the coding
System, device and computer program.
These and other embodiments each can alternatively include one or more of following characteristics.Aspect can be wrapped
Include：Receive described prior images；Detect the face in described prior images；In response to the face in described prior images is detected
Portion, shape is fitted to described face, and the shape of described matching specifies the relative position of described face feature point；With
And generate the described template image for each face feature point from described prior images.
Identify that the described coupling image section of the described image of described specific face feature point can include：Determine described figure
The similarity score of each of one or more of image sections of picture image section, the specific image portion of described image
Point described similarity score specify between the template image of described specific face feature point and described specific image part
Similarity level；And select the image section with maximum similarity fraction as described coupling image section.
By for the described template image of described specific face feature point and one or more of images of described image
Part is compared and can include：In response to one or more of image sections be located in described image away from from described previously
In the image zooming-out threshold distance of the position of described template image, identify one or more of image sections for than
Relatively.
Determine that the direction that user is seeing can include based on the position being identified of each template image：Based on each
The described coupling image section of template image is with respect to the position of the described coupling image section of other template images one or more
The relative position put is determining the orientation of the face of user in described image.
Aspect can include：Based on determined by the direction seen of user determine that user is seeing and be in over the display
Existing special object；And seeing described special object so that executing to described special object in response to the described user of determination
Operation.Determine that described user is seeing that the special object presenting on the display can include：Based on described in the picture
The position of the pupil of user and determined by direction seeing described special object determining described user.
Aspect can include：Based on determined by determine that described user is seeing aobvious in the described user direction seen
Show a part for device；The position of the pupil of detection user described in described image；And the position based on described pupil determines
The subdivision of the described part of the described display that described user is seeing.Aspect can include：Receive in described image sequence
Successive image, described successive image receive described image after received；Described user is determined based on described successive image
Seeing the described part of described display；Determine and continue the position of pupil described in image and institute in described image in the rear
The position stating pupil is different；And continue the position of pupil described in image and institute in described image in the rear in response to determining
The position stating pupil different so as to be shown in described display described partly in user interface element execute operation.Institute
State operation can include：Position based on pupil described in described image and the position continuing pupil described in image in the rear
To move cursor on the display.
The specific embodiment of the theme described in this specification can be realized, to realize one of advantages below or many
Individual.Template image comparison techniques described herein can provide the direction determining that user is seeing using the image of user's face
Effective and relatively quick process.The individually combination of technology, for example, the mask being combined with template image comparison techniques is intended
Even if closing it is allowed to and use low-resolution image also to transmit the effective process of high spatial resolution in real time on low-performance equipment.
Therefore, the web camera from the mobile electronic device being arranged on such as smart mobile phone and tablet PC and camera capture
Image can be used for detecting the view direction of user, without expensive and heavy sensor.Mask matching and template
The combination of image comparison techniques is multiple by the calculating substantially reducing the image analysis technology for determining direction that user is seeing
Polygamy is reducing data processing needs.
Can view direction based on user and/or attitude come control user interface element, reduce to based on the control touching
The needs of system are without disperseing other people attention.For example, being interacted with user interface element using eye tracking can be than public
Voice command in place, office or around other people is more suitable, because eye tracking does not need to disperse other people to note
The sound input of power.
Elaborate the thin of one or more embodiments of theme described in this specification in the the accompanying drawings and the following description
Section.
According to specification, drawings and the claims, other features of theme, aspect and advantage will become clear from.
Brief description
Fig. 1 is the view direction based on user for user interface (UI) control module based on sight line of wherein example to display
Object execution action environment.
Fig. 2 is the object execution to display for the view direction based on user for the UI control module based on sight line of wherein example
Another kind of environment of action.
Fig. 3 depicts the figure of the example shapes model of the face feature point including face.
Fig. 4 depicts the shape of the face feature point of the image including the face being fitted to user.
Fig. 5 depicts the figure of the template image of the face for user.
Fig. 6 is to see for generating for the template image of face of user and determining user using this template image
The flow chart of the instantiation procedure in the direction seen.
Fig. 7 is the flow chart of the instantiation procedure executing U/I action for the view direction based on user.
Fig. 8 is for carrying out, using the position of template image and the pupil of user in the picture, the display that identifying user is being seen
The flow chart of the instantiation procedure of the position of device.
In various figures, the similar reference element similar with title instruction.
Specific embodiment
System can use one or more images of user's face for example, from video feed or other image sequences
The image obtaining determines the orientation of user's face.System can using determined by orientation determining the viewing side of user
To.For example, view direction can specify the direction that user is seeing with respect to display.Can view direction based on user
And/or the change of the view direction of user is controlling the object presenting over the display, such as icon, text or application.For example,
Based on the view direction of user, system can determine that user is watching the special icon being presented by display.Use in response to determining
Family has been viewed by icon and reaches at least threshold amount of time, such as two seconds or more second, or user having made particular pose, example
As nictation, system is so that with respect to icon execution action, the such as selection to icon.
In some embodiments, it is possible to use template image comparison techniques are determining the orientation of user's face.This system
The presence of face in detection image can be carried out using face detection technique.System can also be by the shape matching of common face
Face to the user detecting in the picture.For example, for the face of average-size and shape, shape can specify spy
Determine facial characteristics for example, the position of nose, eyes and face contour.Shape can be superimposed upon image by system
In user's face on, and the position of mobile facial characteristics is overlapping with feature corresponding with user's face, by shape mould
Type is fitted to the face of user.The shape of matching can specify the relative position of the facial characteristics of user's face in image.
System can also extract the template image for each facial characteristics.Template image for specific facial characteristics can
To include a part for the image at the position of specific facial characteristics in the picture.For example, for the nose of the face detecting
The template image of a part for son can be a part for the image of this part including nose.In order to determine in successive image
The face of user orientation, some of template image and successive image can be compared by system, with identify with every
A part for the successive image of individual template image coupling.System can be determined using the relative position of compatible portion to be schemed subsequent
The orientation of the face of user in picture.For example, if coupling image section the right side of the face of relative position instruction user and
The distance between nose of user is more than the distance between the left side of face of user and the nose of user, then system can determine
User eyes left.Difference between two distances is determined for user and how far is seeing to the left.
System can using determined by the orientation of user's face and display determine with respect to the position of user's face
The position on display that user is seeing.Can be with respect at display location determined by being located at or neighbouring object example
As icon, text, image or other UI objects execute operation.On the display that system can seen based on user
Position so that content item for example, advertisement is shown in this position.For example, system can be provided so that content item exists
The data highlighting in the visual field of user.
For systematic collection wherein discussed here with regard to the personal information of user or can using personal information in the case of,
Control program can be provided a user with or whether feature collects personal information (for example, with regard to the social networkies of user, social action
Or the information of activity, the current location of occupation, the preference of user or user) or control whether and/or how from content service
Device receives the chance of content that may be more relevant with user.Additionally, some data can be with one before it is stored or uses
Kind or various ways by anonymization so that personal recognizable information is removed.For example, the identity of user can be anonymous, makes
Personal discernible information must be determined for user, or the geographical position of user can be in the place obtaining positional information
It is generalized (for example to city, postcode or state rank) so that not can determine that the particular location of user.Therefore, Yong Huke
With the control having to how with regard to his or she gather information and how this information is used by content server.
Fig. 1 is the object to display for the view direction based on user for the wherein exemplary UI control module 130 based on sight line
The environment 100 of execution action.UI control module 130 based on sight line can be realized in software and/or hardware.Based on sight line
UI control module 130 may be mounted on electronic equipment 112 or is communicably for example wirelessly or non-wirelessly coupled to electricity
Sub- equipment 112.
Electronic equipment 112 can be smart phone, tablet PC, games system, navigation system or for example include or can
It is communicably coupled to the another type of electronic equipment of display 114.Exemplary electronic device 112 also includes camera 116, and it is permissible
Capturing still image and/or video.For example, can be equipped with can be with the phase of capturing still image and video for electronic equipment 112
The smart phone of machine.Camera 116 can also be separated with electronic equipment 112.For example, camera 116 can be communicably coupled to
The web camera of electronic equipment 112.Camera 116 can capture rest image and/or the video of user 105.
In some embodiments, electronic equipment 112 is for determining the view direction of user to allow users to
The equipment controlling other objects or using during with other object interactions.For example, electronic equipment 112 may be mounted at haulagman
Tool for example, in automobile, ship or aircraft, enables a user to the system interaction with means of transport.In particular example
In, electronic equipment 112 so that the attitude that can be made based on the direction that user is seeing and/or user of user come with automobile
Radio, navigation system or other system interaction.In order to realize this interaction, electronic equipment 112 can for example wired or nothing
Line ground enters row data communication with the system of means of transport.Electronic equipment 112 can be installed for example, is removedly attached
To the instrument board of automobile, with capture images to use when determining the view direction of user.
In another example, electronic equipment 112 so that user can with for example project to windshield or Radix Saposhnikoviae glass
Head up displays (HUD) interaction on glass.It is, for example possible to use HUD display information on the windshield.Electronic equipment 112 can
Enable a user to the direction seen based on user and/or attitude that user makes come with the information exchange being presented by HUD.
This allows user's information of being presented by HUD of change, and without not seeing to road.In some embodiments, user can press
The button being for example located on steering wheel, to be started and the interacting of HUD using eye tracking and/or attitude.By this way, by
The information that HUD shows will not change unintentionally, the attention of dispersion driver.
UI control module 130 based on sight line can determine user based on the image of the user 105 being captured by camera 116
105 view direction.Based on the UI control module 130 of sight line be also based on user view direction so that for be shown in aobvious
Show the object execution action on device 114.For example, based on the UI control module 130 of sight line can use determined by user sight
See direction, for example, together with the information of the position with respect to display 114 for the face of specified user, whether just to determine user 105
Seeing display special object on a display 114.If it is then UI control module 130 based on sight line is so that pin
To this object execution action, such as so that selecting in some way or manipulating this object.Such as can also be blinked using posture
Eye, head inclination or head crooked come starting operation, as described in greater detail.For example, if to as if by word
Manage the text that device shows, then the UI control module 130 based on sight line can be interacted with word processing device to make spy in response to user
Determine attitude and highlight the text watched by user 105.
Image extractor 132 is included based on the UI control module 130 of sight line.Image extractor 132 can be from video feed
Or other image sequences extract image.For example, the UI control module 130 based on sight line can receive video feed from camera 116.
Image extractor 132 can extract single image from video feed, for other groups of the UI control module 130 based on sight line
Part uses.
Eye tracker 134 is also included based on the UI control module 130 of sight line.Eye tracker 134 uses user's 105
Image is determining whether user 135 is seeing display, and if it is, determines that user 105 is watching display 114
What part.For example, eye tracker 134 can use the image being captured by camera 116 or be extracted by image extractor 132
Image carries out this determination.In other embodiments, eye tracker 134 can determine user using the image of user 105
105 what object (if any) watching outside display.For example, eye tracker 134 can use user 105
Image determining user 105 is watching what object of means of transport.
In order to determine the view direction of user 105, eye tracker 134 can be with the face of the user in detection image.?
In some embodiments, eye tracker 134 using detected based on the CF detecting in the picture face deposit
Face detection technique.For example, eye tracker 134 can identify the image with the color corresponding with skin color
Region, and determine the shape in the region of the image with corresponding color.Then, eye tracker 134 can determine for example
Whether this shape mates or similar to facial shape upon combination.In some embodiments, eye tracker 134 uses
Eigenfaces, Viola-Jones object detection framework or another suitable face/object detection technique are come in detection image
The face of user.
If eye tracker 134 detects the face in image, eye tracker 134 can be by shape matching
Face to the user detecting in the picture.Shape specifies the relative position of the face feature point of face, such as general
Or average user face.For example, shape can be specified the point of at least one of profile delineating face and/or be specified all
Relative position as the point of the internal facial feature of eyes, nose, mouth and/or eyebrow.Described in Fig. 3 and described below
Example shapes model.
Shape can be fitted to the face detecting in the picture by eye tracker 134, to generate user's face
Matching shape.In order to shape is fitted to the face of user, eye tracker 134 can be in user's face
It is superimposed shape on image, and move face feature point along image, by the facial alignment of mask and user.Example
As if the face of user is thinner than mask, eye tracker 134 can hook to the actual buccal of user and chin are mobile
Draw the buccal of general or average face and the face feature point of the profile of chin.Describe in the diagram and described below by
Shape is fitted to the example technique of the face of user.
Eye tracker 134 can generate the template image of each face feature point for user's face.For specific
The template image of face feature point is included in a part for the image of a part for the user's face at face feature point.In order to
Generate the template image for specific facial characteristics, eye tracker 134 can extract at the position of specific face feature point
A part for image.This part is less than whole image, the subdivision of such as image.For example, eye tracker 134 can be for use
The side of the right eye at family is extracted at this position and includes the part for the image of this side of right eye of user and directly encloses
Around a part for the image of this side of the right eye of user, for example, the picture of the number of thresholds around this side of the right eye of user
A part for image in element.Describe and described below one group of Prototype drawing that the face for user generates in Figure 5
The example of picture.
Eye tracker 134 can determine the orientation of the face of user in other images using template image, described
The image that other images receive such as after the image for generating template image.In some embodiments, eye tracking
Whether the part of template image and image is compared by device 134, mated with template image with the part determining image.For example,
Each template image can (for example, be existed by eye tracker 134 with close to the region of the prior images being mated with this template image
In threshold distance) one or more parts of image be compared.For example, it is contemplated that the image in video feed, its immediately
After generating the image of template image.The position of user's face and orientation may significantly not move between two images or become
Change.Therefore, eye tracker 134 can will correspond to the position generating template image first in template image and subsequent picture
Region is compared.For example, if template image coupling is located at the image section in the lower right corner of prior images, eye tracker
134 can by template image with the lower right corner of successive image or neighbouring image section is compared.Eye tracker
The part of the successive image in template image and threshold distance (for example, the pixel of number of thresholds) can be compared by 134, from
Find the distance of the image section mating with template image in prior images in this threshold distance.If do not had in this region
Find coupling, then comparison domain can be further expanded out or expand to whole successive image by eye tracker 134.By
Start at previous position and extend comparison domain in response to not finding coupling, at exhaustion or stochastic comparison
Reason, can save data processing and memory resource.
Whether the part in order to determine image is mated with template image, and eye tracker 134 can be by template image
Visual characteristic is compared with image section.For example, eye tracker 134 can by the color of template image, shape, edge and
Other characteristics and image section are compared, to determine the similarity degree of image section and template image.In some embodiments
In, eye tracker 134 can based on image section and template image relatively determining similarity score.Template image and figure
The vision similarity level between template image and image section can be specified as the similarity score of part.Eye tracker
Similarity score can be compared by 134 with similarity threshold, to determine whether image section is mated with template image.
If the template image that eye tracker 134 is each template image or at least number of thresholds or percentage ratio finds
Join image section, then eye tracker 134 can determine the face depicting user in the picture.Eye tracker 134 also may be used
To identify the relative position of the image section mating with template image.Using this relative position, eye tracker 134 can determine
The orientation of the face of user.For example, if the coupling left side of the face of relative position instruction user of image section and user
The distance between nose is less than the distance between the right side of face of user and the nose of user, then eye tracker 134 is permissible
Determine that user 105 eyes left.
Eye tracker 134 can determine the assorted of the display 114 that user is watching using the orientation of user's face
Part (if any).In some embodiments, eye tracker 134 uses the orientation of user's face and specified user
The information of the facial position with respect to display 114, to determine user is watching what part of display 114.For example, depending on
Line tracker 134 face based on user can generate the view direction of user with respect to the orientation of display 114 and position
Vector.Then, eye tracker 134 can will on this vector projection to display 114 with determine user 105 watch aobvious
Shown what part of device 114.
In some embodiments, eye tracker 134 execution learning process is to learn the face of user with respect to display
The position of device 114.For example, eye tracker 134 can point out user 105 to see the various training positions on display 114.For
Each trains position, and eye tracker 134 can for example pass through the template image for user 105 with user 105
The image capturing while seeing to prompting direction is compared to determine which direction user 114 is seeing.Eye tracker 134
Then the direction of observation of user 105 can be used for each training position, to determine user 105 with respect to display 114
Position.For example, it is contemplated that the first training position in the upper left corner of display and the bottom-right second training position in display
Put.If user 105 is eyeing left to two training positions, eye tracker 134 can determine that user 105 is located at display
The right side of device 114.The user's face of relative change in the orientation of to(for) this training position can be used for refining the face of user
Portion is with respect to the position of display 114.
In some embodiments, eye tracker 134 determines the position of the pupil of user in the picture.Eye tracker
The position of 134 pupils that can detect user using loop truss technology.For example, eye tracker 134 can be filtered using Sobel
Ripple device and/or Hough transformation carry out the pupil of the user in detection image.Eye tracker 134 can be refined using pupil position
The view direction of user 105.For example, eye tracker 134 can determine that using the orientation of user's face user 105 sees
Display 114 a part.Then, eye tracker 134 can determine this part of display 114 based on pupil position
Subdivision.If the pupil of user just can determine user 105 in the left side of eyes of user and bottom, eye tracker 125
Subdivision in the lower left of this part seen positioned at display 114.
UI controller 136 is also included based on the UI controller 130 of sight line.UI controller 136 can viewing based on user
Direction executes operation.For example, UI controller 136 can reach at least threshold amount of time or sound in response to user 105 viewing icon
Attitude should be done in user 105, for example, nictation or head move in particular directions, and icon is chosen.UI controller
136 can interact with the application with the object execution action with respect to display.For example, if user 105 is watching word processing and answering
With in text, then UI controller 136 can with word processing application interact with to text execution action, such as highlight or answer
Text processed.
UI controller 136 can assess the view direction of the user in multiple images, to determine whether execution action and to want
Executed any action.For example, in order to determine whether to select icon, UI controller 136 can be watched multiple suitable based on user 105
Icon in sequence image reaches threshold amount of time determining whether user 105 have viewed this icon.UI controller 136 is also based on
The eye state of the orientation of the face of the user in multiple images and/or user is determining whether to perform attitude.For example, UI
Whether in particular directions controller 136 can determine the head of user using the position of the user's face in sequential picture
Mobile.Whether the eyes that UI controller 136 is also based on user are opened in the first image and then are closed in the second image
And and then reopen to determine whether the eyes of user blink in the 3rd image, these three images not necessarily direct
Then one is received.Describe, below with reference to Fig. 7, the example action that executed by UI controller 136 and action can be initiated
Attitude.
Example based on the control module 130 of sight line can be also used for determine user watching multiple equipment or object
Which in (for example, the object in means of transport), and action is executed based on the equipment watched or object.Fig. 2 is
Another environment 200, the wherein view direction based on user 205 for the UI control module 130 based on sight line is held to shown object
Action is made.In this example, two computers 210 and 220 are communicably coupled to based on the control module 130 of sight line.Calculate
Machine 210 includes display 214, and computer 220 includes display 224.Also can be communicated based on the UI control module 130 of sight line
Be coupled to camera 215, for example can capture the rest image of user 205 and/or the web camera of video.
The eye tracker 134 of the control module 130 based on sight line can assess the image of user 205 to determine user just
Watched which display 214 or 224.For example, eye tracker 134 can use the image of user 205 to generate and be directed to user
205 template image, as mentioned above.Eye tracker 134 can also determine user in another image using template image
Face orientation.The letter of the orientation of the face based on user and identifying user 205 position with respect to display 214 and 224
Breath, eye tracker 134 can determine user 205 is watching which (if any) in display 214 and 224.
For example, eye tracker 134 can the orientation of face based on user and the face of user with respect to display 214 and 224
Position generates the vector of the view direction of user.Then, eye tracker 134 can be from the face of user to this vector of front projection
To determine whether user 205 is watching one of display 214 and 224.Eye tracker 134 can also be come using vector
Determine that user 205 is watching what part or point on display 214 or 224.
Eye tracker 134 can also learn the position that user's face is with respect to display 214 and 224 using learning process
Put.For example, eye tracker 134 can point out user 105 to see one or more training positions and display on display 214
One or more training positions on 224.Can user for example using camera 215 see each train position when capture images.
Eye tracker 134 can be by being compared in each image of determination the template image for user 205 with each image
User 205 view direction.Then eye tracker 134 can use view direction and each training position of user 105
The position with respect to display 214 and 224 for the face to determine user for the position.For example, if user 205 eyes left to watch
Display 214 and eye right to watch display 224, eye tracker 134 can determine that the face of user is located at two and shows
Show between device 214 and 224.If rotated further with the head of user compared with watching display 224 with the head inclination of user
To watch display 214, eye tracker 134 can determine user 205 than display 214 closer to display 224.
UI controller 136 can view direction based on user come activation equipment.For example, if eye tracker 134 is true
Determine user 205 and seeing display 214, then UI controller 136 can activate computer 210.UI controller 136 is also based on
The view direction of user 205 changes focus between devices.For example, if focus is changed to aobvious by user 205 from display 214
Show device 224, then focus can be transferred to computer 214 by UI controller 136.It is converted to computer 214 with focus, UI controls
Device 136 can view direction based on user with respect to the object execution action being presented on display 214.
Although describing Fig. 2 with regard to display 214 and 224, eye tracker 134 can determine user 205
Watch in multiple other kinds of objects which, and make execution action at this object.For example, eye tracker
134 can execute learning process to learn with respect to the multiple objects in means of transport or system (for example, radio, navigation system
System, atmosphere control system etc.) user's face position.The image that eye tracker 134 may then based on user 205 determines use
Which in object family 205 watching, and makes execution action at the object just watched.For example, if user
Watch radio and do attitude with his head in particular directions, then eye tracker 134 can to radio (for example,
Via personal area network or wired connection) send request radio being adjusted to volume.By this way, user 205 can
With adjust volume and without by his handss departure direction disk.
Fig. 3 depicts Figure 30 5 and 310 of the example shapes model of the face feature point 307 including face.As described above,
Shape is specified for face such as, the relative position of the face feature point 307 of common face or average face.
Shape can include the relative position of the face feature point and specified face feature point being represented by the point in mask
Data.The face feature point 307 of shape can be used for the specific facial characteristics of face.For example, face feature point 307 is permissible
Delineate the profile of the outside of general or average face, such as from the opposite side of side around chin for the head to head.Face
Portion's characteristic point 307 can also specify facial lip, nose, eyes, the shape of pupil, eyebrow or other facial characteristics.
Mask can also specify the position of such as basic point, and this basic point is, for example, top center or the nose of face.
For each other face feature point 307, mask can specify the position that this face feature point 307 is with respect to basic point.Example
As mask can be for the characteristic point at the center of facial right eye it is intended that right eye be centrally located at the specific range away from basic point
Place and with respect at the special angle of basic point or on specific direction.Every line in the mask described in Fig. 3 indicates by line
The relative position of two face feature points connecting.
Mask can be generated using the image of multiple faces.In some embodiments, eye tracker 134 or
The image of another assembly or system evaluation face with determine one group of for example predefined facial characteristics each facial characteristics flat
All or exemplary position.For example, eye tracker 134 can identify each face feature point with respect to basic point (such as, nose or
Other points a little) mean place.
This group face feature point 307 can be predefined by eye tracker 134 or learn.For example, predefined one group of face
Portion's characteristic point 307 can specify face feature point, the face feature point at the center in each nostril and the use for nose for nose
In the certain amount of face feature point delineating nose.Can use as shown in Figure 30 5 face point to camera image or
The image that wherein face orientation side as shown in Figure 31 0 is seen to generate mask.
Fig. 4 depicts the shape 415 of the face feature point 417 including the image being fitted to user's face 412.In figure
In 405, the shape 415 including face feature point 417 is superimposed upon on the image of user's face 412.When shape 415 not
During the image of face 412 of matching user, it is right with the face 412 of user that the face feature point 417 of shape 415 is moved to
Accurate.
In order to be directed at face feature point 415 with the face 412 of user, eye tracker 134 can be with identifying user face
The position of 412 corresponding facial characteristics, and face feature point 415 is moved to corresponding position.For example, eye tracker
134 can come the face of identifying user in the border between skin color based on user and another kind of color (such as, background color)
The profile in portion 412.In this example, the profile of the face 412 of user is thinner than shape 415.Therefore, eye tracker 134
Face feature point 417 can be moved towards the profile of user's face, until the profile of face feature point 417 and user's face 412
Alignment, as shown in Figure 41 0.Similarly, the face feature point 417 of the eyes of shape, nose and mouth can similarly move,
Until they are aligned with the character pair of the face of user.
The face feature point 417 of alignment forms the shape 420 of the matching for user's face 412.The shape of matching
The relative position of the facial characteristics of user specified by model 420.For example, the shape 420 of matching can specify each face special
Levy a little 417 with respect to one of basic point or face feature point 417 position.
Fig. 5 depicts Figure 50 0 of the template image 510 for user's face 505.In this example, have been directed towards user
Face 505, the mouth of user, the face feature point of the profile of the eyebrow of the nose of user, the eyes of user and user generate template
Image 510.Each template image includes the image of its corresponding face feature point in user's face 505.For example, template image
The image of 511 rightmost sides including user's nose.As set forth above, it is possible to each face feature point for shape generates mould
Plate image.For example, it is possible to the nose for user generates multiple template image to capture shape and the outward appearance of the nose of user.Often
Individual template image 510 can also be associated the position in user's face 505 with it.The position of each template image 510 is permissible
It is with respect to the relative position of basic point (for example, center of face or another appropriate location).The relative position of specific template image can
With the distance between assigned base point and specific template image and specific image with respect to the special angle residing for basic point.
Fig. 6 is template image for generating for user's face and is determined the side that user is seeing using template image
To instantiation procedure 600 flow chart.The operation of process 600 can for example by data processing equipment (such as Fig. 1 based on sight line
UI control module 130) realizing.Process 600 can also be realized by the instruction being stored on computer-readable storage medium, its
Described in instruct the operation making data processing equipment implementation procedure 600 by data processing equipment when being executed.
In frame 602, receive image.Image can be a part for image sequence.For example, it is possible to it is (all from being received from camera
As the camera 116 of Fig. 1) video feed in extract image.Image can include the face of user.
In frame 604, analysis of the image is with the face in detection image.For example, the eye tracker (eye tracking of such as Fig. 1
Device 134) one or more face detection technique can be applied to determine whether image includes face to image.
In frame 606, carry out the determination whether image includes the description of face.If being not detected by face in the picture,
Then receive another image and it is analyzed with the inclusion thing of face.This face detection routine can repeat, until in image
Till face is detected in image in sequence.
In frame 608, if face is detected in the picture, mask is fit to the user detecting in the picture
Face.As described above, mask specifies the relative position of the face feature point of face (such as common face or average face)
Put.For example, mask can specify one group of face feature point for specific facial characteristics, such as the profile of face, every
The profile of eyes, the profile of the profile of nose, the profile of mouth and/or each eyebrow.For each face feature point, mask
The position with respect to basic point or with respect to other face feature points for this face feature point can be specified.In this way it is possible to really
The distance between fixed two face feature points.For example, model of fit can indicate the eye separation specific range of face.
By, on the face of user in the picture by the figure superposition of mask, mask can be fit in figure
The face of the user in picture.Then the position of face feature point that mask can be adjusted is with corresponding with the face of user
Facial characteristics are aligned.For example, it is possible to adjust the face feature point of the profile of right eye delineating general or average face, until this point
Till being aligned with the profile of the right eye of the face of user in the picture.
The mask of matching can be used for quick again detect face and in successive image tracks facial.For example, if
Eyes (or other facial characteristics) is detected in successive image, then can position based on eyes in successive image and they
To estimate the relative position of this other facial characteristics with respect to the relative position of other facial characteristics in fitted shapes model.
In frame 610, the facial characteristics for user generate image generation template image from image.For example, it is possible to for plan
The face feature point of each matching of shape closed generates template image.Each template image can include its corresponding face
The image of portion's feature.Each template image can also be with the position data specifying the position of its corresponding facial characteristics in the picture
Associated.This position can be with respect to another point, such as, basic point.
In frame 612, receive another image from image sequence.The image being received can be in the image generating template image
The image receiving afterwards.For example, the image being received can be the subsequent frame of video feed.
In frame 614, the part of the template image of the face for user and the image being received is compared.Each mould
Plate image can be compared with one or more appropriate sections of image.Template image can with corresponding to and template image
The part of the image of a part for the prior images joined is compared.For example, template image may be compared with prior images
Orientation with the face of user in determination previously image.Template image may with the ad-hoc location in prior images
The part coupling of the prior images at place.Because the position of user's face and orientation may there is no change, therefore permissible
Template image is entered with the image section of the image being received corresponding to the position in the prior images being mated with template image
Row compares.If not finding coupling image section in this position, this position can will be located at nearby (for example, in number of threshold values
Amount image pixel in) image section be compared with template image.If still mismatched, can by image other
Region is compared with template image.
As set forth above, it is possible to compared to the specific template image corresponding to specific facial characteristics, true for each image section
Determine similarity score.The similarity score of image section can specify the similarity level between image section and template image.
In frame 616, at least a portion identification and matching image section of template image.In some embodiments, depending on
The image section of the image being received mating most with template image can be identified as the coupling of template image by line tracker 134
Image section.For example, if multiple images part is compared with specific template image, eye tracker 134 can be with base
Similarity score in the image section being compared to select the image section that mates with template image.In this example, sight line
Tracker 134 can select the image section with the highest similarity fraction meeting or exceeding minimum similarity degree threshold value.
Some template images may not have coupling image section in the image being received.For example, due to user's face
Orientation, the change of illumination or the face of user may partly remove the visual field of camera, the image being received may not have
There is the corresponding part with this template image coupling.Or, the face of user may move so that facial characteristics may not
It is fully visible, such as when user sees to side, the side of nose may not be fully visible.
In frame 618, identify the position in the image being received of each image section mating with template image.For
The position of the coupling image section of specific template part can be associated with specific template image.In this way it is possible to identification
Position with the face feature point comparing corresponding to template image.For example, eye tracker 134 may be determined for that user's nose
Template image mate with the image section at the preferred coordinates being located in received image.Eye tracker 134 can be by
Those coordinates are associated with template image, and are therefore associated with the facial characteristics corresponding to this template image.
In frame 620, can carry out whether the image being received fully is mated with the template image of the face for user
Determination.For example, eye tracker 134 can be only for the template image of little percentage ratio (for example, less than the mould of threshold percentage
Plate image) identification and matching image section.The coupling of this little percentage ratio may make the position of the template image using coupling
The orientation determining the face of user is infeasible.
In some embodiments, eye tracker 134 can be based on having coupling image portion in the image being received
Point multiple template image determining whether received image is mated with template image.Eye tracker 134 is also based on
It is confirmed as the summation score to determine received image for each similarity score of the image section of matching template image.As
Fruit summation score meets or exceedes threshold score, then eye tracker 134 can determine received image and for user's
The template image coupling of face.
If it is determined that the image being received is mismatched with the template image of the face for user, then eye tracker 134
The face of detection user in another image (image for example, subsequently receiving) can be attempted.Due to eye tracker 134
Face for user generates mask and the template image of matching, so eye tracker 134 can be by this template image
It is compared with the subsequent image receiving to attempt detecting the face of user.If eye tracker 134 is for example for number of threshold values
The image of amount is unsuccessful, then eye tracker 134 may return to frame 602 and re-executes face detection technique and to determine be
The no image capturing face.For example it is now possible to there be the face of different users user before camera and different
Portion may mismatch the face of the previous user generating template image.If face is detected, eye tracker 134 can be by
Shape is fitted to the face detecting and generates the template image for the face detecting.
It is fast for the value of summation score being caused to reduce and trigger the other conditions returning back to block 602 again to be detected
The mobile significant changes with lighting condition of the head of speed.For example, quick head movement may lead to fuzzy image or one
A little facial characteristics leave the visual field of camera.Similarly, illumination significant changes so that illumination change after capture figure
As different from the template image of capture during different lighting conditions.Condition can lead to summation score to drop to below threshold value,
And therefore trigger the detection again of face.
Return to frame 620 if it is determined that the image being received is mismatched with the template image of the face for user, then may be used
The direction that user is seeing is determined with the position being identified based on the template image in the image being received.Eye tracker
134 can assess the relative position of the template image orientation to determine the face of user in the image being received.For example, sight line
Tracker 134 can consider the distance between specific template image.Eye tracker 134 can determine and the nose for user
The image section of template matching and and the image section that mates of template image of every side of the face for user between
Distance.If being shorter than the distance from the nose of user to facial right side from the distance to facial left side for the nose of user,
Eye tracker 134 can determine that user eyes left.Additionally, eye tracker can based on the difference between two distances Lai
Determine that how far user is seeing to the left.Eye tracker 134 is also based on the viewing side to determine user for the difference between distance
To angle.For example, if apart from similar, user may see slightly to the left.If apart from significant changes, user may enter
Eye left to one step.
Eye tracker 134 can be based further on the orientation of the face of user and identify display with respect to user's
The information of the position of face is determining the position on the display that user is seeing.For example, eye tracker 134 can be based on use
The orientation of the face at family generates vector, and is arrived this vector projection with respect to the position of the face of user based on display and show
On device.As set forth above, it is possible to the view direction based on user is with respect to the object execution action presenting on display.
Fig. 7 is the flow chart of the instantiation procedure 700 executing U/I action for the view direction based on user.Process 700
Operation can for example be realized by data processing equipment (the UI control module 130 based on sight line of such as Fig. 1).Process 700 is also
Can be realized by the instruction being stored on computer-readable storage medium, wherein said instruction is when being executed by data processing equipment
Make the operation of data processing equipment implementation procedure 700.
In frame 702, receive the initial pictures in image sequence.For example, initial pictures can be from the video for user
The image that feeding is extracted.Initial pictures need not to be the first image in sequence, but can be the images being captured by camera
In any one.
In frame 704, analyze initial pictures to generate one group of template image for the face detecting in the first image.
For example, it is possible to generate template image as described above.
In frame 706, receive the successive image in image sequence.Can also extract follow-up from the video feed for user
Image.
In frame 708, determine the view direction of user for each successive image.For example, if user watches display,
Then the view direction of specific image can specify the position on the display that user is watching in this specific image.Can lead to
Cross the orientation that the part of image and template image are compared to determine the face of user in specific image, to determine specific
The view direction of image.Then can for example based on display with respect to the face of user position by the view direction of user
Vector projection on display, as mentioned above.
In frame 710, action is executed based on the view direction in successive image.Performed action can be based on user
Watch what part of display and/or based on the user's attitude detecting.For example, if user sees that special icon reaches at least threshold
Value time quantum, for example, reaches the successive image of number of thresholds, can select this icon.Or, if user's viewing special icon is simultaneously
Make attitude, such as nictation, eyes expands or nod, then can select this icon.
Eye tracker can be based on the orientation in the user's face in sequential picture or the certain surface in sequential picture
The state of portion's feature is detecting some attitudes.For example, it is possible to the orientation of the change of face based on user in sequential picture Lai
Detect nodding of user's head.Blinking of eyes can be detected based on the state that opens or closes of the eyes in sequential picture
Eye.Can the expansion to detect eyes of height based on the eyes in sequential picture.For example, if the height of eyes increases so
After reduce, then eye tracker 134 can determine user eyes expand.
Can face based on user or eyes the position to move UI object for the movement.For example, UI controller 136 is permissible
At least threshold amount of time is reached based on user's viewing object and determines this object of selection.Object can also be highlighted aobvious over the display
Show to indicate that this object has been chosen.Then, user by seeing different directions or can move mobile object.For example,
If the head movement of user makes user see to the right side of object, UI controller 136 can be in response to the sight of user
See that direction changes and object is moved to right side (position for example, seen to user).Or, UI controller 136 can respond
Move mobile object in pupil in particular directions.View direction based on user or eyes move, and can similarly exist
Mobile cursor of mouse on display.
Text input can also be received using on-screen keyboard.For example, user can see that character reaches at least threshold amount of time.Ring
Ying Yu detects user's viewing character, can select this character.Or, user can be when watching character by blinking or nodding
To select this character.
See that object reaches threshold amount of time in response to user, other actions can be executed, such as adjustment window size, selection literary composition
Originally and/or roll in text document or webpage.For example, if user sees that selectable control reaches threshold amount of time, such as window
Mouth zoom control, scroll bar or window minimize control, then can select this control.The attitude of such as nictation can be used for
User selects this control when watching control.
Head movement can be used for moving being hidden between the multiple windows after other windows.For example, in response to
Head user is detected turns forward, and UI controller 136 can cycle through window.
Fig. 8 is for carrying out, using the position of template image and the pupil of user in the picture, the display that identifying user is being seen
The flow chart of the instantiation procedure 800 of the position of device.The operation of process 800 can be for example by the data processing equipment (base of such as Fig. 1
UI control module 130 in sight line) realizing.Process 800 can also by the instruction that is stored on computer-readable storage medium Lai
Realize, wherein said instruction makes the operation of data processing equipment implementation procedure 800 by data processing equipment when being executed.
In frame 802, determine the orientation of the face of user in the picture using template image.For example, image can be
The image extracting from the video feed for user.As set forth above, it is possible to pass through the template image of the face of user and this figure
As being compared to the orientation of the face determining user.
In frame 804, the orientation of the face based on user is determining a part for display that user is seeing.For example, may be used
To determine display using the face of the orientation of the face of user and specified user with respect to the information of the position of display
Part.Then the vector of view direction of user can for example be generated with respect to the position of the face of user based on display simultaneously
It is projected on display.The display that vector is projected in partly can be identified as the display that user is watching
Part.This part of display is additionally may included in the region around the position of display that vector is projected in.For example, show
This showing device partly can include the aobvious of the scheduled volume on each direction with respect to vector projected position over the display
Show region.
In frame 806, the position of each of pupil of detection user in the picture.It is, for example possible to use loop truss skill
Art is detecting the position of the pupil of user.
In frame 808, the subdivision of the display seen come identifying user based on the pupil position detecting.Subdivision can
To be the subdivision of the part of identification in frame 804.For example, the orientation that pupil position can be used for refining based on user's face is true
Fixed viewing location.
The embodiment of the theme describing in this manual and operation can be implemented in Fundamental Digital Circuit or calculating
In machine software, firmware or hardware, including the structure disclosed in this specification and its equivalent structures or their one or many
Individual combination.The embodiment of the theme describing in this manual can be implemented as of coding on computer-readable storage medium
Or multiple computer program, i.e. one or more modules of computer program instructions, for being executed or controlled by data processing equipment
The operation of data processing equipment processed.Alternately or in addition, programmed instruction can be coded on manually generated transmitting signal,
For example, machine generates electricity, light or electromagnetic signal, its be generated with coding information for transmission to suitable receiver apparatus with by
Data processing equipment executes.Computer-readable storage medium can be computer readable storage devices, computer-readable memory substrate, with
Machine or serial access memory array or equipment or their one or more combination or be included therein.In addition although counting
Calculation machine storage medium is not transmitting signal, but computer-readable storage medium can be the meter of coding in manually generated transmitting signal
The source of calculation machine programmed instruction or destination.Computer-readable storage medium can also be or is included in one or more single
In physical assemblies or medium (for example, multiple CD, disk or other storage devices).
The operation describing in this manual may be implemented as by data processing equipment to being stored in one or more meters
The operation of the data execution receiving in calculation machine readable storage device or from other sources.
Term " data processing equipment " includes the device of all kinds for processing data, equipment and machine, for example, wrap
Include programmable processor, computer, SOC(system on a chip) or aforesaid multiple or combination.Device can include dedicated logic circuit, example
As FPGA (field programmable gate array) or ASIC (special IC).In addition to hardware, device can also include creating
The code of the performing environment of computer program for being discussed, for example, constitutes processor firmware, protocol stack, data base administration
System, operating system, the code of the combination of cross-platform runtime environment, virtual machine or one or more of which.Device and
Performing environment can realize various different computation model infrastructure, such as web services, Distributed Calculation and grid computing base
Infrastructure.
Computer program (also referred to as program, software, software application, script or code) can programming language in any form
Speech is write, and including compiling or interpretative code, declaratively or procedural language, and it can be disposed, in any form including conduct
Stand-alone program or as being suitable to the module, assembly, subroutine, object or other units that use in a computing environment.Computer journey
Sequence can but not necessarily correspond to the file in file system.Program can be stored in other programs of preservation or data (for example, is deposited
Storage in one or more of marking language document script) a part for file in, be exclusively used in the single literary composition of described program
(file of one or more modules, subprogram or code section for example, is stored) in part or in multiple coordinated files.Calculate
Machine program can be deployed as on a computer or positioned at one place or be distributed in multiple places and pass through communication network
Execute on multiple computers of network interconnection.
Process described in this specification and logic flow can be executed by one or more programmable processors, and this can be compiled
Thread processor executes one or more computer programs to execute action by being operated to input data and generated output.
Process and logic flow can also (for example, FPGA (field programmable gate array) or ASIC be (special integrated by dedicated logic circuit
Circuit)) executing, and device can also be embodied as this dedicated logic circuit.
The processor being adapted for carrying out computer program includes for example general and special microprocessor and any kind of
Any one or more processors of digital computer.Generally, processor will from read only memory or random access memory or
Both receive instruction and data.The primary element of computer is refer to for the processor according to instruction execution action and for storage
Make one or more memory devices of data.Generally, computer is also one or more big for data storage by including
Capacity storage device, for example, disk, magneto-optic disk or CD, or computer be operatively coupled with receive from it data or to
Its transmission data or both.However, computer does not need there is such equipment.Additionally, computer can be embedded in another setting
In standby, this another equipment is, for example, mobile phone, personal digital assistant (PDA), Mobile audio frequency or video player, game control
Platform processed, global positioning system (GPS) receptor or portable memory apparatus (for example, USB (universal serial bus) (USB) flash drive
Device), here only for several examples.The equipment being suitable for storing computer program instructions data includes the non-easy of form of ownership
The property lost memorizer, medium and storage device, for example, include：Semiconductor memory apparatus, such as EPROM, EEPROM and flash memory device；
Disk, for example, internal hard drive or removable disk；Magneto-optic disk；And, CD-ROM and DVD-ROM disk.Processor and memorizer are permissible
By supplemented or be incorporated in dedicated logic circuit.
In order to provide and the interacting of user, the embodiment of the theme described in this specification can be implemented on computer,
This computer has：Display device (for example, CRT (cathode ray tube) or LCD (liquid crystal display) monitor), for user
Display information；And, keyboard and instruction equipment, such as mouse or trace ball, user can by this keyboard and instruction equipment to
Computer provides input.Other kinds of equipment can be used for providing and the interacting of user；For example, it is supplied to the feedback of user
Can be any type of sensory feedback, such as visual feedback, auditory feedback or touch feedback；And can connect in any form
Receive the input from user, this any form includes sound, voice or sense of touch.In addition, computer can pass through following sides
Formula and user mutual：Send document to the equipment that user uses and the equipment from user's use receives document；For example, by response
The request receiving in the web browser on the client device of user, sends webpage to this web browser.
The embodiment of the theme describing in this manual can be implemented in computing system, and this computing system is included for example
As the aft-end assembly of data server, or include the middleware component of such as application server, or include for example having
The front end assemblies of the client computer of graphic user interface or Web browser, or the one or more such rear ends of inclusion,
Middleware or any combinations of front end assemblies, user can be by this graphic user interface or Web browser and this specification
The implementation interaction of the theme of description.The assembly of system can be by the digital data communications example of any form or medium
As interconnection of telecommunication network.The example of communication network includes LAN (" LAN ") and wide area network (" WAN "), Internet (example
As the Internet) and peer-to-peer network (for example, self-organizing peer-to-peer network).
Computing system can include client and server.Client and server is generally remote from each other and generally passes through
Communication network interaction.The relation of client and server by means of on corresponding computer run and have each other client-
The computer program of relationship server and generate.In certain embodiments, (for example, server transmits data to client device
Html page) (for example, in order to the user's video data interacting with client device and the mesh from this user's receiving user's input
).Data (for example, the knot of user mutual generating in client device can be received from client device at server
Really).
Although this specification comprises many specific embodiment details, these are not necessarily to be construed as to claimed
Scope restriction, but as the description to the specific feature of specific embodiment.In this manual in separate embodiments
Some features described in context can also combine realization in single embodiment.On the contrary, in the context of single embodiment
Described in various features can also individually or with any suitable sub-portfolio realize in various embodiments.Though additionally,
So feature can be described above as and work and even initially so claimed in some combinations, but is derived from and wanted
The one or more features asking the combination of protection can remove in some cases from combination, and combination required for protection
Can be for the version of sub-portfolio or sub-portfolio.
Similarly although describing operation with particular order in the accompanying drawings, but this is understood not to require these operations
With shown particular order or with sequential order execution, or all shown operations are performed, to realize desired result.?
In some cases, multitask and parallel processing are probably favourable.Additionally, the separation of the various system components in above-described embodiment
It is understood not to be required for such separation in all embodiments, and it is to be understood that described program assembly and be
System generally can be integrated in together in single software product or be encapsulated in multiple software product.
Therefore it has been described that the specific embodiment of theme.Other embodiment is within the scope of the appended claims.One
In the case of a little, the action described in claim can be executed in different order and still realize desired result.In addition,
The process described in accompanying drawing is not necessarily required to shown particular order or order sequentially to realize desired result.In some realities
Apply in mode, multitask and parallel processing are probably favourable.
Claims (20)
1. a kind of method being executed by data processing equipment, methods described includes：
Receive the image in image sequence, described image describes the face of user；
For each the specific face feature point in multiple face feature points of the described face described in described image：
To be compared with one or more image sections of described image for the template image of described specific face feature point,
Described template image for described specific face feature point is included in the described image sequence describe described face feature point
A part for prior images；
Identify the described image mated with the described template image for described specific face feature point based on described comparison
Coupling image section；And
The position of described coupling image section is identified in described image；And
The direction that described user is seeing is determined based on the position being identified of each template image.
2. method according to claim 1, also includes：
Receive described prior images；
Detect the face in described prior images；
In response to the described face in described prior images is detected, shape is fitted to described face, the shape of institute's matching
The relative position of the plurality of face feature point specified by shape model；And
Generate the described template image for each face feature point from described prior images.
3. method according to claim 1, wherein, identifies the described coupling of the described image of described specific face feature point
Image section includes：
Determine the similarity score of each of one or more of image sections of described image image section, described figure
The described similarity score of the specific image part of picture specifies described template image and institute for described specific face feature point
State the similarity level between specific image part；And
Select the image section with maximum similarity fraction as described coupling image section.
4. method according to claim 1, wherein, by for the described template image of described specific face feature point and institute
State image one or more of image sections be compared including：It is located at institute in response to one or more of image sections
State in image in the threshold distance of position being extracted described template image from described prior images, identification one or
Multiple images part is for comparing.
5. method according to claim 1, wherein, determines described use based on the position being identified of each template image
The direction that family is being seen includes：Described coupling image section based on each template image is with respect to other templates one or more
The orientation to determine the face of user described in described image for the relative position of the position of described coupling image section of image.
6. method according to claim 1, also includes：
Based on determined by the direction seen of described user determine that described user is seeing present over the display specific
Object；And
Seeing described special object in response to the described user of determination so that operation is executed to described special object.
7. method according to claim 6, wherein it is determined that described user is seeing present on the display specific
Object includes：The position of the pupil based on user described in described image and determined by direction determining described user
See described special object.
8. method according to claim 1, also includes：
Based on determined by the direction seen of described user determine the part of display that described user is seeing；
The position of the pupil of detection user described in described image；And
Determine the subdivision of the described part of the described display that described user is seeing based on the described position of described pupil.
9. method according to claim 8, also includes：
Receive the successive image in described image sequence, described successive image is received after receiving described image；
Determine that described user is seeing the described part of described display based on described successive image；
Determine that the position continuing pupil described in image in the rear is different from the position of pupil described in described image；And
It is different from the position of pupil described in described image in response to determining the position continuing pupil described in image in the rear,
So that to be shown in described display described partly in user interface element execute operation.
10. method according to claim 9, wherein, described operation includes：Position based on pupil described in described image
The position putting and continuing in the rear pupil described in image to move cursor on the display.
A kind of 11. systems, including：
Data processing equipment；And
Enter the memory storage apparatus of row data communication with described data processing equipment, described memory storage apparatus storage can
The instruction being executed by described data processing equipment, and described instruction makes described data processing equipment hold in such execution
Row operation, described operation includes：
Receive the image in image sequence, described image describes the face of user；
For each the specific face feature point in multiple face feature points of the described face described in described image：
To be compared with one or more image sections of described image for the template image of described specific face feature point,
Described template image for described specific face feature point is included in the described image sequence describe described face feature point
A part for prior images；
Identify the described image mated with the described template image for described specific face feature point based on described comparison
Coupling image section；And
The position of described coupling image section is identified in described image；And
The direction that described user is seeing is determined based on the position being identified of each template image.
12. systems according to claim 11, wherein, described instruction makes described data processing equipment execution upon execution
Further operation, described further operation includes：
Receive described prior images；
Detect the face in described prior images；
In response to the described face in described prior images is detected, shape is fitted to described face, the shape of institute's matching
The relative position of the plurality of face feature point specified by shape model；And
Generate the described template image for each face feature point from described prior images.
13. systems according to claim 11, wherein, described of the described image of the described specific face feature point of identification
Join image section to include：
Determine the similarity score of each of one or more of image sections of described image image section, described figure
The described similarity score of the specific image part of picture specifies described template image and institute for described specific face feature point
State the similarity level between specific image part；And
Select the image section with maximum similarity fraction as described coupling image section.
14. systems according to claim 11, wherein, by for described specific face feature point described template image with
One or more of image sections of described image be compared including：It is located in response to one or more of image sections
In described image in the threshold distance of position being extracted described template image from described prior images, identification one
Or multiple images part is for comparing.
15. systems according to claim 11, wherein, are determined described based on the position being identified of each template image
The direction that user is seeing includes：Described coupling image section based on each template image is with respect to other moulds one or more
The relative position of the position of described coupling image section of plate image is determining the determining of face of user described in described image
To.
16. systems according to claim 11, described instruction makes described data processing equipment execute into one upon execution
The operation of step, described further operation includes：
Based on determined by the direction seen of described user determine that described user is seeing present over the display specific
Object；And
Seeing described special object in response to the described user of determination so that operation is executed to described special object.
17. systems according to claim 11, wherein, described instruction makes described data processing equipment execution upon execution
Further operation, described further operation includes：
Based on determined by the direction seen of described user determine the part of display that described user is seeing；
The position of the pupil of detection user described in described image；And
Determine the subdivision of the described part of the described display that described user is seeing based on the described position of described pupil.
18. systems according to claim 17, described instruction makes described data processing equipment execute into one upon execution
The operation of step, described further operation includes：
Receive the successive image in described image sequence, described successive image is received after receiving described image；
Determine that described user is seeing the described part of described display based on described successive image；
Determine that the position continuing pupil described in image in the rear is different from the position of pupil described in described image；And
It is different from the position of pupil described in described image in response to determining the position continuing pupil described in image in the rear,
So that to be shown in described display described partly in user interface element execute operation.
19. systems according to claim 18, wherein, described operation includes：Based on pupil described in described image
Position and continue the position of pupil described in image in the rear and to move cursor on the display.
A kind of 20. computer-readable storage mediums of use computer program code, described program includes instructing, and described instruction is when by data
Described data processing equipment is made to execute operation during processing meanss execution, described operation includes：
Receive the image in image sequence, described image describes the face of user；
For each the specific face feature point in multiple face feature points of the described face described in described image：
To be compared with one or more image sections of described image for the template image of described specific face feature point,
Described template image for described specific face feature point is included in the described image sequence describe described face feature point
A part for prior images；
Identify the described image mated with the described template image for described specific face feature point based on described comparison
Coupling image section；And
The position of described coupling image section is identified in described image；And
The direction that described user is seeing is determined based on the position being identified of each template image.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201461983232P | 2014-04-23 | 2014-04-23 | |
US61/983,232 | 2014-04-23 | ||
PCT/US2015/027260 WO2015164584A1 (en) | 2014-04-23 | 2015-04-23 | User interface control using gaze tracking |
Publications (2)
Publication Number | Publication Date |
---|---|
CN106462242A true CN106462242A (en) | 2017-02-22 |
CN106462242B CN106462242B (en) | 2019-06-04 |
Family
ID=53059479
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201580026804.1A Active CN106462242B (en) | 2014-04-23 | 2015-04-23 | Use the user interface control of eye tracking |
Country Status (4)
Country | Link |
---|---|
US (1) | US9703373B2 (en) |
EP (1) | EP3134847A1 (en) |
CN (1) | CN106462242B (en) |
WO (1) | WO2015164584A1 (en) |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN107390874A (en) * | 2017-07-27 | 2017-11-24 | 深圳市泰衡诺科技有限公司 | A kind of intelligent terminal control method and control device based on human eye |
CN109461153A (en) * | 2018-11-15 | 2019-03-12 | 联想(北京)有限公司 | Data processing method and device |
CN109835260A (en) * | 2019-03-07 | 2019-06-04 | 百度在线网络技术（北京）有限公司 | A kind of information of vehicles display methods, device, terminal and storage medium |
CN110148224A (en) * | 2019-04-04 | 2019-08-20 | 精电(河源)显示技术有限公司 | HUD image display method, device and terminal device |
CN111103891A (en) * | 2019-12-30 | 2020-05-05 | 西安交通大学 | Unmanned aerial vehicle rapid posture control system and method based on skeleton point detection |
CN113906485A (en) * | 2020-04-30 | 2022-01-07 | 乐天集团股份有限公司 | Control device, system and method |
CN114356482A (en) * | 2021-12-30 | 2022-04-15 | 业成科技（成都）有限公司 | Method for interacting with human-computer interface by using sight line drop point |
Families Citing this family (42)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6163017B2 (en) * | 2013-05-29 | 2017-07-12 | 富士通テン株式会社 | Portable terminal and danger notification system |
KR102130797B1 (en) * | 2013-09-17 | 2020-07-03 | 엘지전자 주식회사 | Mobile terminal and control method for the mobile terminal |
US20160012426A1 (en) | 2014-07-11 | 2016-01-14 | Google Inc. | Hands-free transactions with a challenge and response |
US10185960B2 (en) | 2014-07-11 | 2019-01-22 | Google Llc | Hands-free transactions verified by location |
CN104574321B (en) * | 2015-01-29 | 2018-10-23 | 京东方科技集团股份有限公司 | Image correcting method, image correcting apparatus and video system |
GB201507210D0 (en) * | 2015-04-28 | 2015-06-10 | Microsoft Technology Licensing Llc | Eye gaze correction |
GB201507224D0 (en) | 2015-04-28 | 2015-06-10 | Microsoft Technology Licensing Llc | Eye gaze correction |
US10733587B2 (en) | 2015-04-30 | 2020-08-04 | Google Llc | Identifying consumers via facial recognition to provide services |
US10397220B2 (en) | 2015-04-30 | 2019-08-27 | Google Llc | Facial profile password to modify user account data for hands-free transactions |
US9619803B2 (en) | 2015-04-30 | 2017-04-11 | Google Inc. | Identifying consumers in a transaction via facial recognition |
US10409443B2 (en) * | 2015-06-24 | 2019-09-10 | Microsoft Technology Licensing, Llc | Contextual cursor display based on hand tracking |
US10178301B1 (en) * | 2015-06-25 | 2019-01-08 | Amazon Technologies, Inc. | User identification based on voice and face |
WO2017043132A1 (en) * | 2015-09-08 | 2017-03-16 | 日本電気株式会社 | Facial recognition system, facial recognition method, display control device, display control method, and display control program |
US9990044B2 (en) * | 2015-10-30 | 2018-06-05 | Intel Corporation | Gaze tracking system |
US10768772B2 (en) * | 2015-11-19 | 2020-09-08 | Microsoft Technology Licensing, Llc | Context-aware recommendations of relevant presentation content displayed in mixed environments |
JP2017107482A (en) * | 2015-12-11 | 2017-06-15 | ソニー株式会社 | Information processing device, information processing method and program |
KR102084174B1 (en) * | 2016-03-01 | 2020-04-23 | 구글 엘엘씨 | Modify face profile for hands-free trading |
CN109070748B (en) * | 2016-04-20 | 2021-11-30 | 日产自动车株式会社 | Information processing apparatus and display data determining method |
CN106067013B (en) * | 2016-06-30 | 2022-04-12 | 美的集团股份有限公司 | Face recognition method and device for embedded system |
US10474879B2 (en) | 2016-07-31 | 2019-11-12 | Google Llc | Automatic hands free service requests |
US10209772B2 (en) | 2016-09-19 | 2019-02-19 | International Business Machines Corporation | Hands-free time series or chart-based data investigation |
US10055818B2 (en) * | 2016-09-30 | 2018-08-21 | Intel Corporation | Methods, apparatus and articles of manufacture to use biometric sensors to control an orientation of a display |
US10353475B2 (en) * | 2016-10-03 | 2019-07-16 | Microsoft Technology Licensing, Llc | Automated E-tran application |
US11062304B2 (en) | 2016-10-20 | 2021-07-13 | Google Llc | Offline user identification |
CN106780662B (en) | 2016-11-16 | 2020-09-18 | 北京旷视科技有限公司 | Face image generation method, device and equipment |
CN106780658B (en) * | 2016-11-16 | 2021-03-09 | 北京旷视科技有限公司 | Face feature adding method, device and equipment |
US20180293754A1 (en) * | 2017-04-05 | 2018-10-11 | International Business Machines Corporation | Using dynamic facial landmarks for head gaze estimation |
US10481856B2 (en) | 2017-05-15 | 2019-11-19 | Microsoft Technology Licensing, Llc | Volume adjustment on hinged multi-screen device |
WO2018222232A1 (en) | 2017-05-31 | 2018-12-06 | Google Llc | Providing hands-free data for interactions |
CN107516105B (en) * | 2017-07-20 | 2020-06-16 | 阿里巴巴集团控股有限公司 | Image processing method and device |
US11250242B2 (en) * | 2017-09-13 | 2022-02-15 | Visualcamp Co., Ltd. | Eye tracking method and user terminal performing same |
WO2019075656A1 (en) * | 2017-10-18 | 2019-04-25 | 腾讯科技（深圳）有限公司 | Image processing method and device, terminal, and storage medium |
US10489487B2 (en) * | 2018-01-18 | 2019-11-26 | Microsoft Technology Licensing, Llc | Methods and devices to select presentation mode based on viewing angle |
JP7020215B2 (en) * | 2018-03-19 | 2022-02-16 | 日本電気株式会社 | Extra findings determination device, extra findings determination system, extra findings determination method, program |
JP6844568B2 (en) * | 2018-03-27 | 2021-03-17 | 日本電気株式会社 | Extra findings determination device, extra findings determination system, extra findings determination method, program |
KR102608471B1 (en) | 2018-11-06 | 2023-12-01 | 삼성전자주식회사 | Method and apparatus for eye tracking |
WO2020210298A1 (en) * | 2019-04-10 | 2020-10-15 | Ocelot Laboratories Llc | Techniques for participation in a shared setting |
WO2021134160A1 (en) * | 2019-12-30 | 2021-07-08 | Fresenius Medical Care Deutschland Gmbh | Method for driving a display, tracking monitor and storage medium |
US20230015224A1 (en) * | 2020-01-14 | 2023-01-19 | Hewlett-Packard Development Company, L.P. | Face orientation-based cursor positioning on display screens |
EP3859491A1 (en) * | 2020-01-29 | 2021-08-04 | Irisbond Crowdbonding, S.L. | Eye-tracker, system comprising eye-tracker and computer device and method for connection between eye-tracker and computer device |
US11532147B2 (en) | 2020-09-25 | 2022-12-20 | Microsoft Technology Licensing, Llc | Diagnostic tool for deep learning similarity models |
US20230266817A1 (en) * | 2022-02-23 | 2023-08-24 | International Business Machines Corporation | Gaze based text manipulation |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080187175A1 (en) * | 2007-02-07 | 2008-08-07 | Samsung Electronics Co., Ltd. | Method and apparatus for tracking object, and method and apparatus for calculating object pose information |
EP1968006A1 (en) * | 2005-12-27 | 2008-09-10 | Matsushita Electric Industrial Co., Ltd. | Image processing apparatus |
CN102239460A (en) * | 2008-11-20 | 2011-11-09 | 亚马逊技术股份有限公司 | Movement recognition as input mechanism |
CN103235644A (en) * | 2013-04-15 | 2013-08-07 | 北京百纳威尔科技有限公司 | Information displaying method and device |
CN103257707A (en) * | 2013-04-12 | 2013-08-21 | 中国科学院电子学研究所 | Three-dimensional roaming method utilizing eye gaze tracking and conventional mouse control device |
US20130325463A1 (en) * | 2012-05-31 | 2013-12-05 | Ca, Inc. | System, apparatus, and method for identifying related content based on eye movements |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
AU2003280516A1 (en) | 2002-07-01 | 2004-01-19 | The Regents Of The University Of California | Digital processing of video images |
US6637883B1 (en) | 2003-01-23 | 2003-10-28 | Vishwas V. Tengshe | Gaze tracking system and method |
US7599549B2 (en) * | 2004-12-22 | 2009-10-06 | Fujifilm Corporation | Image processing method, image processing apparatus, and computer readable medium, in which an image processing program is recorded |
US8793620B2 (en) * | 2011-04-21 | 2014-07-29 | Sony Computer Entertainment Inc. | Gaze-assisted computer interface |
JP4862447B2 (en) | 2006-03-23 | 2012-01-25 | 沖電気工業株式会社 | Face recognition system |
WO2009091029A1 (en) | 2008-01-16 | 2009-07-23 | Asahi Kasei Kabushiki Kaisha | Face posture estimating device, face posture estimating method, and face posture estimating program |
KR101537948B1 (en) | 2008-12-16 | 2015-07-20 | 삼성전자주식회사 | Photographing method and apparatus using pose estimation of face |
US8818034B2 (en) | 2009-11-30 | 2014-08-26 | Hewlett-Packard Development Company, L.P. | Face recognition apparatus and methods |
US9361510B2 (en) * | 2013-12-13 | 2016-06-07 | Intel Corporation | Efficient facial landmark tracking using online shape regression method |
-
2015
- 2015-04-23 WO PCT/US2015/027260 patent/WO2015164584A1/en active Application Filing
- 2015-04-23 EP EP15721455.2A patent/EP3134847A1/en not_active Ceased
- 2015-04-23 CN CN201580026804.1A patent/CN106462242B/en active Active
- 2015-04-23 US US14/694,131 patent/US9703373B2/en active Active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP1968006A1 (en) * | 2005-12-27 | 2008-09-10 | Matsushita Electric Industrial Co., Ltd. | Image processing apparatus |
US20080187175A1 (en) * | 2007-02-07 | 2008-08-07 | Samsung Electronics Co., Ltd. | Method and apparatus for tracking object, and method and apparatus for calculating object pose information |
CN102239460A (en) * | 2008-11-20 | 2011-11-09 | 亚马逊技术股份有限公司 | Movement recognition as input mechanism |
US20130325463A1 (en) * | 2012-05-31 | 2013-12-05 | Ca, Inc. | System, apparatus, and method for identifying related content based on eye movements |
CN103257707A (en) * | 2013-04-12 | 2013-08-21 | 中国科学院电子学研究所 | Three-dimensional roaming method utilizing eye gaze tracking and conventional mouse control device |
CN103235644A (en) * | 2013-04-15 | 2013-08-07 | 北京百纳威尔科技有限公司 | Information displaying method and device |
Non-Patent Citations (1)
Title |
---|
VOLKER BLANZ 等: "face recognition based on fitting 3D morphable model", 《IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 》 * |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN107390874A (en) * | 2017-07-27 | 2017-11-24 | 深圳市泰衡诺科技有限公司 | A kind of intelligent terminal control method and control device based on human eye |
CN109461153A (en) * | 2018-11-15 | 2019-03-12 | 联想(北京)有限公司 | Data processing method and device |
CN109835260A (en) * | 2019-03-07 | 2019-06-04 | 百度在线网络技术（北京）有限公司 | A kind of information of vehicles display methods, device, terminal and storage medium |
CN110148224A (en) * | 2019-04-04 | 2019-08-20 | 精电(河源)显示技术有限公司 | HUD image display method, device and terminal device |
CN111103891A (en) * | 2019-12-30 | 2020-05-05 | 西安交通大学 | Unmanned aerial vehicle rapid posture control system and method based on skeleton point detection |
CN113906485A (en) * | 2020-04-30 | 2022-01-07 | 乐天集团股份有限公司 | Control device, system and method |
CN114356482A (en) * | 2021-12-30 | 2022-04-15 | 业成科技（成都）有限公司 | Method for interacting with human-computer interface by using sight line drop point |
CN114356482B (en) * | 2021-12-30 | 2023-12-12 | 业成科技（成都）有限公司 | Method for interaction with human-computer interface by using line-of-sight drop point |
Also Published As
Publication number | Publication date |
---|---|
US9703373B2 (en) | 2017-07-11 |
US20150309569A1 (en) | 2015-10-29 |
EP3134847A1 (en) | 2017-03-01 |
CN106462242B (en) | 2019-06-04 |
WO2015164584A1 (en) | 2015-10-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN106462242B (en) | Use the user interface control of eye tracking | |
EP3467707B1 (en) | System and method for deep learning based hand gesture recognition in first person view | |
US11736756B2 (en) | Producing realistic body movement using body images | |
US9953214B2 (en) | Real time eye tracking for human computer interaction | |
US9563272B2 (en) | Gaze assisted object recognition | |
Mulfari et al. | Using Google Cloud Vision in assistive technology scenarios | |
US9671873B2 (en) | Device interaction with spatially aware gestures | |
US9269009B1 (en) | Using a front-facing camera to improve OCR with a rear-facing camera | |
Zhao et al. | An immersive system with multi-modal human-computer interaction | |
CN107004279A (en) | Natural user interface camera calibrated | |
CN108681399B (en) | Equipment control method, device, control equipment and storage medium | |
CN109375765B (en) | Eyeball tracking interaction method and device | |
CN107562186B (en) | 3D campus navigation method for emotion operation based on attention identification | |
WO2020197626A1 (en) | Methods for two-stage hand gesture input | |
US10937243B2 (en) | Real-world object interface for virtual, augmented, and mixed reality (xR) applications | |
US20190045270A1 (en) | Intelligent Chatting on Digital Communication Network | |
CN111527468A (en) | Air-to-air interaction method, device and equipment | |
CN110582781A (en) | Sight tracking system and method | |
KR20190015332A (en) | Devices affecting virtual objects in Augmented Reality | |
Schütt et al. | Semantic interaction in augmented reality environments for microsoft hololens | |
US10558951B2 (en) | Method and arrangement for generating event data | |
CN110018733A (en) | Determine that user triggers method, equipment and the memory devices being intended to | |
JP2018060375A (en) | Information processing system, information processing device and program | |
CN113093907A (en) | Man-machine interaction method, system, equipment and storage medium | |
Adiani et al. | Evaluation of webcam-based eye tracking for a job interview training platform: Preliminary results |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
GR01 | Patent grant | ||
GR01 | Patent grant |