CN116868187A - Computing platform for facilitating augmented reality experience with third party assets - Google Patents
Computing platform for facilitating augmented reality experience with third party assets Download PDFInfo
- Publication number
- CN116868187A CN116868187A CN202280016010.7A CN202280016010A CN116868187A CN 116868187 A CN116868187 A CN 116868187A CN 202280016010 A CN202280016010 A CN 202280016010A CN 116868187 A CN116868187 A CN 116868187A
- Authority
- CN
- China
- Prior art keywords
- data
- augmented reality
- computing device
- rendering
- data asset
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000003190 augmentative effect Effects 0.000 title claims abstract description 145
- 238000009877 rendering Methods 0.000 claims abstract description 176
- 238000000034 method Methods 0.000 claims abstract description 115
- 230000000694 effects Effects 0.000 claims description 70
- 238000012545 processing Methods 0.000 claims description 24
- 238000012549 training Methods 0.000 claims description 20
- 239000002537 cosmetic Substances 0.000 claims description 15
- 230000006870 function Effects 0.000 claims description 11
- 238000012360 testing method Methods 0.000 claims description 11
- 239000003086 colorant Substances 0.000 claims description 6
- 230000008447 perception Effects 0.000 claims description 5
- 230000003416 augmentation Effects 0.000 claims 2
- 230000018109 developmental process Effects 0.000 description 70
- 238000010801 machine learning Methods 0.000 description 50
- 230000008569 process Effects 0.000 description 48
- 238000010586 diagram Methods 0.000 description 16
- 238000013528 artificial neural network Methods 0.000 description 12
- 230000015654 memory Effects 0.000 description 11
- 230000011218 segmentation Effects 0.000 description 8
- 230000000007 visual effect Effects 0.000 description 8
- 239000008186 active pharmaceutical agent Substances 0.000 description 7
- 230000004048 modification Effects 0.000 description 6
- 238000012986 modification Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 5
- 230000003796 beauty Effects 0.000 description 4
- 238000004891 communication Methods 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 3
- 238000005259 measurement Methods 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 238000013515 script Methods 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 2
- 238000006243 chemical reaction Methods 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 238000003709 image segmentation Methods 0.000 description 2
- 230000002452 interceptive effect Effects 0.000 description 2
- 239000003973 paint Substances 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000009471 action Effects 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000002776 aggregation Effects 0.000 description 1
- 238000004220 aggregation Methods 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000000052 comparative effect Effects 0.000 description 1
- 238000013144 data compression Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 230000037406 food intake Effects 0.000 description 1
- 238000007373 indentation Methods 0.000 description 1
- 238000009434 installation Methods 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 230000004807 localization Effects 0.000 description 1
- 230000007787 long-term memory Effects 0.000 description 1
- 230000005012 migration Effects 0.000 description 1
- 238000013508 migration Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/70—Software maintenance or management
- G06F8/71—Version control; Configuration management
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/10—Protecting distributed programs or content, e.g. vending or licensing of copyrighted material ; Digital rights management [DRM]
- G06F21/12—Protecting executable software
- G06F21/14—Protecting executable software against software analysis or reverse engineering, e.g. by obfuscation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/50—Monitoring users, programs or devices to maintain the integrity of platforms, e.g. of processors, firmware or operating systems
- G06F21/52—Monitoring users, programs or devices to maintain the integrity of platforms, e.g. of processors, firmware or operating systems during program execution, e.g. stack integrity ; Preventing unwanted data erasure; Buffer overflow
- G06F21/54—Monitoring users, programs or devices to maintain the integrity of platforms, e.g. of processors, firmware or operating systems during program execution, e.g. stack integrity ; Preventing unwanted data erasure; Buffer overflow by adding security routines or objects to programs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/005—General purpose rendering architectures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/50—Lighting effects
- G06T15/80—Shading
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/30—Creation or generation of source code
- G06F8/36—Software reuse
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2219/00—Indexing scheme for manipulating 3D models or images for computer graphics
- G06T2219/024—Multi-user, collaborative environment
Abstract
Systems and methods for data asset acquisition and obfuscation may facilitate retrieval of augmented reality rendered data assets from a third party. Sending the software development kit and receiving back the data asset may ensure that the data asset is compatible with the augmented reality rendering experience in the user interface. Having a fuzzy data acquisition system may also ensure that code generated by third parties is semantically removed and readability reduced.
Description
RELATED APPLICATIONS
The present application claims priority and benefit from U.S. non-provisional patent application Ser. No. 17/144,002, filed on 1/7 at 2021. U.S. non-provisional patent application No. 17/144, 002 is incorporated herein by reference in its entirety.
Technical Field
The present disclosure relates generally to acquisition of data assets. More particularly, the present disclosure relates to acquiring data assets from a third party to provide an augmented reality experience to a user.
Background
Augmented Reality (AR) may refer to the creation and execution of an interactive experience of a real-world environment, where objects residing in the real world are augmented by computer-generated sensory information. As one example, the AR experience may include enhancing a scene captured by a user's camera by inserting virtual objects into the scene and/or modifying the appearance of real world objects included in the scene.
Some entities may provide users with the ability to participate in an AR experience that is related to objects that the entity manufactures or distributes (e.g., augmented reality "trial"). However, most entities only provide these AR experiences on their own websites. The limited accessibility of augmented reality products to "try out" may require that the consumer must go from one website to another to engage in different AR experiences for different objects (e.g., try out different products and compare products). This problem may lead to consumers spending more computing resources navigating between different websites to facilitate (facility) participation in different AR sessions.
Furthermore, network-based implementations of the augmented reality experience may result in lower frame rates and less realistic rendering than the augmented reality experience provided in the native application.
Another problem with participating in different augmented reality experiences is that different AR applications may be encoded in different encoding languages. Some encoding languages may cause security problems, which may be particularly alarming when the application processes images or video of the user's face.
Disclosure of Invention
Aspects and advantages of embodiments of the disclosure will be set forth in part in the description which follows, or may be learned from the description, or may be learned by practice of the embodiments.
One example aspect of the present disclosure is directed to a computer-implemented method for providing third party data assets to a client. The method may include transmitting, by the computing device, a software development kit. The software development kit may include templates for building one or more render effect shaders (shaders). The method may include receiving, by a computing device, a data asset. In some implementations, the data assets can include one or more render effect shaders built using a software development kit. The method may include storing, by a computing device, a data asset, and providing, by the computing device, an augmented reality rendering experience. In some implementations, the augmented reality rendering may be based at least in part on the data asset.
Another example aspect of the present disclosure is directed to a computing system. The computing system may include one or more processors and one or more non-transitory computer-readable media collectively storing instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operations may include receiving a software development kit including templates for generating an augmented reality experience. The operations may include receiving one or more inputs to modify the template. In some implementations, modifying the template may generate a product-specific augmented reality experience. In some implementations, the operations can include generating a data asset based at least in part on one or more inputs and transmitting the data asset to a second computing device.
Another example aspect of the disclosure is directed to one or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause a computing system to perform operations. The operations may include sending, by the computing device, a software development kit. In some implementations, the software development kit can include templates for building one or more render effect shaders. The operations may include receiving, by the computing device, a data asset. The data assets may include one or more render effect shaders built using a software development kit. The operations may include storing, by the computing device, the data asset, and providing, by the computing device, an augmented reality rendering experience. In some implementations, the augmented reality rendering may be based at least in part on the data asset.
Other aspects of the disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the principles of interest.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification in view of the accompanying drawings, wherein:
FIG. 1A depicts a block diagram of an example computing system performing data asset acquisition, according to an example embodiment of the disclosure.
FIG. 1B depicts a block diagram of an example computing system performing data asset acquisition and obfuscation, according to an example embodiment of the present disclosure.
FIG. 1C depicts a block diagram of an example computing device performing data asset acquisition and obfuscation, according to an example embodiment of the present disclosure.
FIG. 1D depicts a block diagram of an example computing device performing data asset acquisition and obfuscation, according to an example embodiment of the present disclosure.
FIG. 2 depicts a block diagram of an example data asset acquisition system, according to an example embodiment of the present disclosure.
Fig. 3 depicts an example blurred block diagram in accordance with an example embodiment of this disclosure.
FIG. 4 depicts a block diagram of an example data asset acquisition system, according to an example embodiment of the present disclosure.
FIG. 5 depicts a block diagram of an example data asset acquisition system, according to an example embodiment of the present disclosure.
FIG. 6 depicts a flowchart of an example method of performing data asset acquisition, according to an example embodiment of the disclosure.
FIG. 7 depicts a flowchart of an example method of performing data asset generation, according to an example embodiment of the disclosure.
FIG. 8 depicts a flowchart of an example method of performing data asset acquisition and obfuscation, according to an example embodiment of the present disclosure.
Repeated reference characters in the drawings are intended to represent like features in the various embodiments.
Detailed Description
SUMMARY
In general, the present disclosure is directed to a platform that facilitates collection of data assets from third parties to provide an augmented reality rendering experience to a user. For example, the platform may provide an interface for a third party to build and/or submit rendering effects to be provided to a user. The platform may provide an augmented reality rendering experience to a user, where the user may input user data and receive the augmented user data as output. Additionally, according to another example aspect, the platform may perform various obfuscation techniques on the received third-party data asset to protect the third-party code.
Accordingly, example aspects of the present disclosure may provide a system for acquiring third party data assets for augmented reality rendering and obscuring the acquired code to protect third party proprietary information. In some implementations, the systems and methods can include sending a software development kit to a third party. The software development kit may include templates for building rendering effect shaders. As one example, AR effects may be organized into a shared template and a plurality of presets according to the product. The template may contain shaders and other resources that are common to all products in a particular family (e.g., all lipsticks from a particular vendor). The presets contain parameters corresponding to the shaders, which may include uniform values and textures. The third party may provide the data asset back to the platform (e.g., packaged in an SDK). As an example, the data assets may include one or more render effect shaders built using a software development kit. The received data asset may then be stored. The data asset may then be used to facilitate an augmented reality rendering experience, where the data asset may help generate an augmented reality trial experience.
Systems and methods for data asset acquisition may involve one or more systems or devices. The first computing device may be a server, a facilitation computing device, or an intermediary computing device. The second computing device may be a third party computing device. The third party may be a video game company, a product manufacturer, or a product brand. The first computing device and the second computing device may exchange data to generate an augmented reality rendering experience for the user. The augmented reality rendering experience may include rendering an augmented reality view including one or more products or items. The product may be a cosmetic (e.g., lipstick, eye shadow, etc.), furniture or other household items (e.g., electronic equipment, cookware, glassware, ornaments, plants, etc.), apparel, paint colors, automobiles, various electronic products, or any other item.
The data asset acquisition may include the first computing device sending a software development kit to the second computing device. The software development kit may include templates for building rendering effect shaders. The software development kit may include example effects, tools to build rendering effects, and preview modes to help build augmented reality renderings. The second computing device may be used to build the rendering effect, and once the rendering effect is built, the second computing device may export the built rendering effect data into a renderable compressed file (e.g., ZIP file) that may include the data assets needed to recreate the rendering effect. The data asset may then be transmitted to the first computing device. The first computing device, upon receiving the data asset, may store the data asset for use in an augmented reality rendering experience provided to the user. The provided augmented reality rendering experience may be provided to a user, where the user may input his user data for processing, and the output may be augmented user data including rendering effects built on the second computing device. The user data may be image data or video data captured by the user device. In some implementations, the user data can be a real-time camera feed.
In some implementations, one or more rendering effect shaders may include data describing rendering characteristics. One or more rendering effect shaders may be used to render rendering effects of the augmented reality experience. For example, rendering lipstick on a face with enhanced reality may include utilizing lipstick shaders, textures, uniforms (uniforms), and filter shaders. In some implementations, textures and unification can be input into shaders to help generate rendering effects. In some implementations, the data asset can include a render effect shader. In some implementations, the render effect shader may be processed by a render calculator along with user data to generate a rendered image.
In some implementations, the received data assets can be used to facilitate generation of augmented reality renderings. The mesh model may be capable of processing the input data to generate a mesh and segmentation mask (mask). The enhancement model can then receive the mesh and segmentation mask, and can use the data assets to determine where and how to render a particular rendering. For example, the data asset may include anchor landmark data. Thus, the data assets can be used to determine where rendering needs to occur. In some implementations, the data assets can include shader data. The shader data may describe lipstick colors or eye shadow colors to help generate cosmetic augmented reality renderings for the user. Further, the shader data may describe the color, gloss, opacity, and/or various other characteristics of the product.
The augmented reality rendering experience may include a rendering calculator that generates a rendering based at least in part on the data asset. In some implementations, the augmented reality rendering experience can include a rendering engine, and the rendering engine can include a rendering calculator.
The augmented reality rendering experience may include a perceived sub-graph and a rendered sub-graph. The perceptual subgraphs may be uniform throughout the system. The perceptual subgraph may be used with a variety of different rendered subgraphs. The render subgraph may be built by a third party to generate rendering effects to provide to the user. The rendered subgraph may be constructed and then used by an augmented reality rendering experience platform that stores the perceived subgraph. Rendering subgraphs may vary depending on the rendering effect and third parties. In some implementations, a single perceptual subgraph may be used with multiple rendering subgraphs to render multiple renderings in an enhanced user image or video. For example, a picture or video of a user's face may be processed to generate an augmented reality rendering of lipstick, eye shadow, and mascara of the face. The process may include a single perceived subgraph, but includes rendered subgraphs for each respective product (i.e., lipstick, eye shadow, and mascara).
In some implementations, the data asset can include product data describing a product sold by a third party. In some implementations, the systems and methods disclosed herein may be used to augment a reality rendering experience for a retailer compiled (comp) product to allow consumers to virtually try out different products from a variety of different brands or providers. The retailer may be an online retailer where the consumer may comfortably virtually try the product in his home. In some implementations, the retailer may be a physical store retailer, where the augmented reality experience may be implemented by mobile applications or computing devices found within the store. In some implementations, the systems and methods disclosed herein may implement an augmented reality rendering experience within a search engine application, where a consumer may search for a brand of a product, a type of product, a color of the product, and so on; and the search engine may provide results based on the search query, where the results include an option to use augmented reality to try out the determined results.
In some implementations, the received data assets may allow for the generation of various augmented reality experiences. For example, the data assets may include data assets generated by a manufacturer, distributor, seller, etc. of furniture or other household items to facilitate rendering one or more of their products indoors. In this embodiment, a third party may modify and fine tune the software development kit to be able to generate a furniture or other household augmented reality experience. When the third parties are satisfied with the generated experience, they can export and send the data asset to the facilitation system/platform for storage. The facilitation system can then use the data assets to implement an augmented reality rendering experience that is accessible to the user.
Augmented reality rendering may be generated by: the method includes receiving user data, processing the user data with an encoder model to generate a user mesh, and processing the user mesh with an augmented model to generate an augmented reality rendering. In some implementations, the mesh may be a polygonal mesh. In some implementations, the enhancement model can include a shader that is based at least in part on the data asset. In some implementations, the augmented model may generate the augmented reality rendering by rendering a polygonal mesh using a custom shader, superimposed on the camera feed image.
In some implementations, the systems and methods can include a third party computing device obtaining (intake) and modifying a software development kit. The method may include receiving a software development kit configured to be modified to generate an augmented reality experience. Further, the method may include receiving one or more inputs to modify the software development kit, wherein modifying the software development kit generates a product-specific augmented reality experience. The method may include generating a data asset based at least in part on one or more inputs and transmitting the data asset to a second computing device.
In some implementations, the third party computing device may test the augmented reality experience for fine tuning. Testing the augmented reality experience may include comparing the rendered image to a pre-rendered reference ("golden") image. Alternatively, in some embodiments, testing may include: obtaining training data, processing the training data with an augmented reality experience to generate augmented reality media, and comparing the augmented reality media to reference data. One or more parameters of the data asset may be adjusted (e.g., automatically using a machine learning algorithm) based at least in part on the comparison (e.g., based on a loss function of the comparative augmented reality media and the reference data).
In some implementations, the systems and methods may process received data assets to obfuscate the code. Blurring may include removing spaces (whitespace) from the code, removing one or more annotations from the code, and renaming terms and symbols in the code. One or more annotations may be removed because the one or more annotations may include text describing code semantics. Renaming one or more terms in the data asset code may include uniform renaming across files. The unified renaming process may include indexing terms to provide a reference for future renaming uniformity. In some implementations, renaming may include using a hash function. The hash may be indexed in a global registry or index table. The new symbol may be actively added to the index upon receipt. Renamed terms and symbols may be deterministic terms. In some implementations, the system can rename symbols shared between multiple files, such as function names.
Renaming may include parsing the shader code and isolating specific declarations for replacement. Removing spaces may remove organization of code to remove some logic included in the formatted code in a particular manner. Blurring may include removing semantic content that may include shader information that a third party does not want to disclose (e.g., proprietary information found in OpenGL code, GLSL, or other sources of origin).
In some implementations, the platform can be used to generate a directory (directory) of product renderings. A catalog of augmented reality renderings may be implemented to provide renderings for advertisements, video network applications, or mobile applications. In some implementations, shared code may be integrated into the directory as library dependent items.
Receiving or ingesting a data asset may be performed in a variety of ways. In some implementations, ingestion may occur through built-in export functionality in the platform. In other embodiments, the data asset may be delivered by email sending a single effect template that is parameterized by a shader unification defined in a single top-level file. For example, for lipstick rendering, a third party may send a unified value for the shader for each lipstick product that they want to offer to the user. The systems and methods may combine the effect templates and the shader unification on a per-product basis to produce an AR effect per product (e.g., a beauty effect). In some implementations, the data assets can be ingested via an internet-based data feed rather than email. The data assets may be received individually or in whole.
In some implementations, the software development kits can be configured for different product types. For example, a software development kit that builds a data asset for rendering lipstick may include different templates and presets than a software development kit that builds a data asset for rendering sofa. In addition, organizing the cosmetic effects into shared "templates" and multiple presets according to the product can help third parties build data assets for rendering certain products. The template may contain shaders and other resources that are common to all products in a particular series (e.g., all lipsticks from vendor a). The presets may contain parameters corresponding to the shaders, which may include uniform values and textures.
In some implementations, the systems and methods disclosed herein may be implemented as a native application. The native application may provide the client with an augmented reality rendering experience that includes a third party product rendering for selection by the client.
The systems and methods disclosed herein may also be applicable to other technologies, including mixed reality. In some implementations, the third party can use a software development kit to build the interactive rendering. For example, a furniture brand may build various rendering effects for its various lounge chairs and various extensible tables. The render effect shader may be transferred to an augmented reality rendering experience platform to provide a mixed reality rendering experience in which users may render furniture in their homes. The user may then interact with the rendering map to view the rendering of the furniture in the default and alternative positions. Thus, the user can test how the lounge chair fits their home in both upright and reclined positions. The user may use the mixed reality experience to determine whether the extended table fits in a given room.
In addition, the data asset acquisition and obfuscation platform may be applied to a variety of other platforms for generating a supported user experience. The data acquisition platform may be used for supported application creation, embedded feature creation, and gadget creation.
Further, in some embodiments, the systems and methods may be used as a visual compatibility calculator. For example, the systems and methods may be used to ensure that a particular product or part will fit in a desired space or location. Systems and methods may be used to virtually test measurements/dimensions of a product using virtual reality. A third party may provide a data asset that includes data describing product measurements. The data asset may then be used to provide an augmented reality rendering experience to the user, where the product is rendered according to the measurements provided by the third party. This aspect may allow the consumer to "try out" the product to visualize the space that the product may occupy.
The systems and methods of the present disclosure provide a number of technical effects and benefits. As one example, the system and method may receive parameters for augmented reality from a third party to allow a client to virtually see a product in an application. The system and method may also be used to ensure the security of data provided from a third party. Further, the system and method may be capable of centrally collecting a virtual trial set of augmented reality data from various suppliers to allow users to trial products from multiple suppliers without having to navigate from one website to another or from one proprietary application to another.
Further, in some embodiments, the disclosed systems and methods may be implemented into native applications. Implementation of the system and method in a native application may provide higher frame rates and more realistic rendering than in a web application alternative.
Another technical benefit of the systems and methods of the present disclosure is the ability to obfuscate code (e.g., shader code). The system and method may turn the code into clusters without annotations or symbols revealing proprietary information found in the source code originally provided. Since the system and method may also obscure the received data assets, third parties may provide their data without revealing certain proprietary information.
Referring now to the drawings, example embodiments of the present disclosure will be discussed in more detail.
Example devices and systems
FIG. 1A depicts a block diagram of an example computing system 80 that performs data asset acquisition, according to an example embodiment of the disclosure. The system 80 includes a client computing system 20, an augmented reality platform 30, and a third party computing system.
As shown in fig. 1A, the augmented reality platform 30 may communicate with a third party computing system 60 to generate a third party Augmented Reality (AR) asset library 40. The third party AR asset library 40 may be utilized to provide an Augmented Reality (AR) experience to the client computing system 20 via the client interface 32.
For example, the augmented reality platform 30 may provide a Software Development Kit (SDK) with templates to the third party computing system 60. The third party computing system 60 may use the SDK with the template to construct an augmented reality rendering effect describing the product sold by the third party. The completed rendering effect may be provided back to the augmented reality platform 30 as a completed data asset. Each completed data asset may be stored in a third party AR asset library 40 of the augmented reality platform 30. The product assets stored in the third party AR asset library 40 may be from a single third party or from an excessive (plethora) of third parties. For example, product 1 asset 42, product 2 asset 44, up to product N asset 46 may come from a single third party computing system, and may include various products offered by third parties. Alternatively, product 1 asset 42 and product 2 asset 44 may be provided by different third parties, and may include data assets describing different products from different brands.
The augmented reality platform 30 may obtain client data via the client interface 32 for processing by the rendering engine 34 to provide an AR experience to the client computing system 20. Rendering engine 34 may employ a perception model and an enhancement model to process client data. The perceptual model may output one or more meshes and one or more segmentation masks that may be input into the enhancement model. The augmented model may process the client data, the one or more grids, and the one or more segmentation masks to output an augmented reality client image or video, which may be sent to the client computing system 20 via the client interface 32.
In some implementations, the AR experience may include the client computing system 20 sending a selection to the augmented reality platform 30 to indicate a product for which "trial" is desired. The augmented reality platform may use stored data assets from the third party AR asset library 40 to render products in the client-provided image or video by using an augmented model of the data asset parameterized rendering engine 34. For example, the client computing system 20 may use the client interface 32 to select the product 1 for virtual trial. Product 1 asset 42 may be provided to rendering engine 34 along with a set of client data. Rendering engine 34 may process the set of client data and product 1 assets 42 to generate an image or video of product 1 in the image or video provided by client computing system 20.
FIG. 1B depicts a block diagram of an example computing system 100 that performs data asset acquisition and obfuscation in accordance with an example embodiment of the present disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled by a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smart phone or tablet), a game console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and may be a processor or multiple processors operatively connected. Memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, as well as combinations thereof. Memory 114 may store data 116 and instructions 118 that are executed by processor 112 to cause user computing device 102 to perform operations.
In some implementations, the user computing device 102 may store or include one or more augmented reality rendering models 120. For example, the augmented reality rendering model 120 may be or may otherwise include various machine learning models, such as a neural network (e.g., deep neural network) or other types of machine learning models, including non-linear models and/or linear models. The neural network may include a feed forward neural network, a recurrent neural network (e.g., a long and short term memory recurrent neural network), a convolutional neural network, or other form of neural network. An example augmented reality rendering model 120 is discussed with reference to fig. 2 and 4.
In some implementations, the one or more augmented reality rendering models 120 can include data assets: which is received from the training computing system 150 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 can implement multiple parallel instances of a single augmented reality rendering model 120 (e.g., perform parallel rendering of effects).
More specifically, the server computing system 130 and the training computing system 150 may exchange data to generate data assets that may enable the augmented reality rendering model to process image or video data and output augmented image data or augmented video data.
Additionally or alternatively, one or more augmented reality rendering models 140 may be included in the server computing system 130 or otherwise stored and implemented by the server computing system 130, the server computing system 130 in communication with the user computing device 102 according to a client-server relationship. For example, the augmented reality rendering model 140 may be implemented by the server computing system 140 as part of a network service (e.g., a "real-time trial" service for cosmetics, clothing, electronics, automobiles, or furniture, or other household items). Accordingly, one or more models 120 may be stored and implemented at the user computing device 102 and/or one or more models 140 may be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other devices that a user may use to provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be a processor or multiple processors operatively connected. Memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. Memory 134 may store data 136 and instructions 138 that are executed by processor 132 to cause server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. Where the server computing system 130 includes multiple server computing devices, such server computing devices may operate in accordance with a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include one or more machine-learned augmented reality rendering models 140. For example, model 140 may be or may otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layer nonlinear models. Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. An example model 140 is discussed with reference to fig. 2 and 4.
The user computing device 102 and/or the server computing system 130 may train the models 120 and/or 140 via interactions with a training computing system 150 communicatively coupled via a network 180. The training computing system 150 may be separate from the server computing system 130 or may be part of the server computing system 130.
The training computing system 150 includes one or more processors 152 and memory 154. The one or more processors 152 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be a processor or multiple processors operatively connected. The memory 154 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, as well as combinations thereof. Memory 154 may store data 156 and instructions 158 that are executed by processor 152 to cause training computing system 150 to perform operations. In some implementations, the training computing system 150 includes or is otherwise implemented by one or more server computing devices.
Training computing system 150 may include a model trainer 160 that trains machine learning models 120 and/or 140 stored at user computing device 102 and/or server computing system 130 using various training or learning techniques (such as, for example, error back propagation). For example, the loss function may be counter-propagated through the model to update one or more parameters of the model (e.g., a gradient based on the loss function). Various loss functions may be used, such as mean square error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. Gradient descent techniques may be used to iteratively update parameters over multiple training iterations.
In some embodiments, performing back-propagation of the error may include performing truncated back-propagation over time. Model trainer 160 may perform a variety of generalization techniques (e.g., weight decay, discard (dropout), etc.) to enhance the generalization ability of the trained model.
In particular, model trainer 160 may train augmented reality rendering models 120 and/or 140 based on a set of training data 162. The training data 162 may include, for example, shaders built by third parties using a software development kit that the third parties receive from the facilitating computing device or the server computing system 130. By building and testing an augmented reality experience with a software development kit, a third party may have generated shaders and data assets.
In some implementations, the training examples can be provided by the user computing device 102 if the user has provided consent. Thus, in such embodiments, the model 120 provided to the user computing device 102 may be trained by the training computing system 150 on user-specific data received from the user computing device 102. In some cases, this process may be referred to as personalizing the model.
Model trainer 160 includes computer logic for providing the desired functionality. Model trainer 160 may be implemented as hardware, firmware, and/or software that controls a general purpose processor. For example, in some implementations, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other embodiments, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium, such as a RAM hard disk or an optical or magnetic medium.
The network 180 may be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof, and may include any number of wired or wireless links. In general, communications over network 180 may be carried via any type of wired and/or wireless connection using various communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), coding or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
The machine learning model described in this specification may be used in a variety of tasks, applications, and/or use cases.
In some implementations, the input to the machine learning model of the present disclosure can be image data. The machine learning model may process the image data to generate an output. As an example, the machine learning model may process the image data to generate an image recognition output (e.g., recognition of the image data, potential embedding of the image data, encoded representation of the image data, hash of the image data, etc.). As another example, the machine learning model may process the image data to generate an image segmentation output. As another example, the machine learning model may process image data to generate an image classification output. As another example, the machine learning model may process the image data to generate an image data modification output (e.g., a change in the image data, etc.). As another example, the machine learning model may process the image data to generate an encoded image data output (e.g., an encoded and/or compressed representation of the image data, etc.). As another example, the machine learning model may process the image data to generate an enhanced (upscaled) image data output. As another example, the machine learning model may process the image data to generate a prediction output.
In some implementations, the input of the machine learning model of the present disclosure can be text or natural language data. The machine learning model may process text or natural language data to generate an output. As an example, the machine learning model may process natural language data to generate a linguistic coded output. As another example, the machine learning model may process text or natural language data to generate a potential text-embedded output. As another example, the machine learning model may process text or natural language data to generate a translation output. As another example, the machine learning model may process text or natural language data to generate a classification output. As another example, the machine learning model may process text or natural language data to generate a text segmentation output. As another example, the machine learning model may process text or natural language data to generate semantic intent output. As another example, the machine learning model may process text or natural language data to generate an enhanced text or natural language output (e.g., text or natural language data of higher quality than the input text or natural language, etc.). As another example, the machine learning model may process text or natural language data to generate a predictive output.
In some implementations, the input of the machine learning model of the present disclosure can be potentially encoded data (e.g., a potential spatial representation of the input, etc.). The machine learning model may process the potentially encoded data to generate an output. As an example, the machine learning model may process the potentially encoded data to generate the recognition output. As another example, the machine learning model may process the potentially encoded data to generate a reconstructed output. As another example, the machine learning model may process the potentially encoded data to generate a search output. As another example, the machine learning model may process the potentially encoded data to generate a reclustering output. As another example, the machine learning model may process the potentially encoded data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure can be sensor data. The machine learning model may process the sensor data to generate an output. As an example, the machine learning model may process the sensor data to generate an identification output. As another example, the machine learning model may process the sensor data to generate a prediction output. As another example, the machine learning model may process the sensor data to generate a classification output. As another example, the machine learning model may process the sensor data to generate a classification output. As another example, the machine learning model may process the sensor data to generate a segmented output. As another example, the machine learning model may process the sensor data to generate a visual output. As another example, the machine learning model may process the sensor data to generate a diagnostic output. As another example, the machine learning model may process the sensor data to generate a detection output.
In some cases, the machine learning model may be configured to perform tasks that include encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding). In another example, the input includes visual data (e.g., one or more images or videos), the output includes compressed visual data, and the task is a visual data compression task. In another example, the task may include generating an embedding for input data (e.g., visual data).
In some cases, the input includes visual data and the task is a computer visual task. In some cases, pixel data including one or more images is input, and the task is an image processing task. For example, the image processing task may be an image classification, wherein the output is a set of scores, each score corresponding to a different object classification, and representing a likelihood that one or more images depict an object belonging to that object classification. The image processing task may be object detection, wherein the image processing output identifies one or more regions in the one or more images, and for each region, a likelihood that the region depicts the object of interest. As another example, the image processing task may be image segmentation, wherein the image processing output defines a respective likelihood for each category in a predetermined set of categories for each pixel in the one or more images. For example, the set of categories may be foreground and background. As another example, the set of categories may be object classifications. As another example, the image processing task may be depth estimation, wherein the image processing output defines a respective depth value for each pixel in the one or more images. As another example, the image processing task may be motion estimation, wherein the network input comprises a plurality of images, and the image processing output defines, for each pixel of one of the input images, a motion of a scene depicted at the pixel between the images in the network input.
FIG. 1B illustrates one example computing system that may be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such implementations, the model 120 may be trained and used locally at the user computing device 102. In some of such implementations, the user computing device 102 may implement a model trainer 160 to personalize the model 120 based on user-specific data.
Fig. 1C depicts a block diagram of an example computing device 10, performed in accordance with an example embodiment of the present disclosure. Computing device 10 may be a user computing device or a server computing device.
Computing device 10 includes a plurality of applications (e.g., applications 1 through N). Each application contains its own machine learning library and machine learning model. For example, each application may include a machine learning model. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like.
As shown in fig. 1C, each application may communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., public API). In some implementations, the API used by each application is specific to that application.
Fig. 1D depicts a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
Computing device 50 includes a plurality of applications (e.g., applications 1 through N). Each application communicates with a central intelligent layer. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like. In some implementations, each application can communicate with the central intelligence layer (and the models stored therein) using APIs (e.g., public APIs across all applications).
The central intelligence layer includes a plurality of machine learning models. For example, as shown in fig. 1D, a respective machine learning model (e.g., model) may be provided for each application and managed by a central intelligence layer. In other implementations, two or more applications may share a single machine learning model. For example, in some implementations, the central intelligence layer can provide a single model (e.g., a single model) for all applications. In some implementations, the central intelligence layer is included in the operating system of the computing device 50 or otherwise implemented by the operating system of the computing device 50.
The central intelligence layer may communicate with the central device data layer. The central device data layer may be a centralized data repository for computing devices 50. As shown in fig. 1D, the central device data layer may communicate with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or an add-on component. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Example model arrangement
In some implementations, the system and method may be used as a rendering pipeline (pipeline). The pipeline may include a Software Development Kit (SDK) that may include all of the tools required to build a renderable compressed file (e.g., ZIP file). In some implementations, a software development kit with compressed files (e.g., ZIP files) may be tested using viewers on various platforms. Source assets generated when built using a software development kit can be used to enhance still images or video.
The software development kit may be sent to a third party. A third party may construct and preview an internally facing rendering pipeline. The pipeline may be used for a variety of purposes including, but not limited to, beauty product trials using computers or mobile devices.
The software development kit may include scripts with associated binary files (binaries), preview tool sets, documents, and example effect sets. Scripts and associated binary files may be used to compile the product effect source into a renderable compressed file. The preview tool set may be used to visualize the rendering results. In addition, the preview tool may provide a joint interface for editing and previewing, or a quick interface switch between the two. The document may include original HTML or other forms of document for viewing to aid in the build process. The example effect set may help third parties understand the software development kit while providing a baseline.
The software development kit may be designed to be self-sufficient, easy to migrate, capable of running on inventory hardware, fast, and secure. The software development kit may be designed to run without dependencies to allow third parties or other creators to use the software development kit to build renderings without having to rely on other applications. The backend may reflect the backend used by the system to interact with the consumer. In addition, ease of migration may allow third parties to use their existing shaders with little or no modification. Furthermore, the software development kit may be designed to run on a variety of operating systems without requiring the installation of software outside of the software development kit. In some implementations, the software development kit can include features that allow the rendering graph and GPU shader to be opened for customization. The interface may eliminate the need to implement third party GPU code into a system that interfaces with the consumer, which may maintain user security. The software development kit may use a rendering calculator to convert incoming data into outgoing data.
The internally facing augmented reality effect generation may involve two components. The first component may relate to perception. The perception component may calculate and output pixel coordinates of landmarks in the image (e.g., lips on the face). The second component may relate to a rendering component. The rendering component can include rendering an augmented reality effect on the originally received frame, wherein the calculated landmarks facilitate localization. The result may then be output.
The graph for augmented reality rendering may be divided into a third party sub-graph and a facilitator sub-graph. The facilitator sub-graph may be a perceptual sub-graph and the third party sub-graph may be a rendered sub-graph editable by the third party and facilitator. The separation may allow the facilitator to modify the perceptual subgraph without affecting the rendered subgraph. Furthermore, the separation may allow a single perceived subgraph to be used even though multiple augmented reality effects are being rendered. Thus, multiple render subgraphs may be layered on a single perceptual subgraph to produce multiple renderings with computation of a single perceptual model process.
The aggregation of perceived and rendered subgraphs may produce complete graphics or augmented reality media.
The third party effect source may contain one or more directories of assets consumed by the rendering calculator in the sub-graph that render the sub-graph file.
In some implementations, the software development kit can include a perception model. The software development kit may call the bundled binary file to convert each type of source file into a processed format, which may preserve the input directory structure in the generated renderable compressed file while issuing any errors encountered in the process. Asset files such as primitive graphs (photo) shader and blueprints may reference other files that may be included in a software development kit or be part of an effect source. The texture may be converted to a webp format based on custom converted image_conversion.
In some implementations, the product effects built with the software development kit may include GLSL shaders, which may be provided to the user's device for runtime compilation. These shaders may represent valuable intellectual property rights from third party technology providers. The systems and methods disclosed herein may obscure shaders to help protect information. Blurring may include removing annotated and grammatically superfluous spaces and then systematically renaming most of the non-preserved symbols to obscure semantics. In some implementations, the ambiguous symbols may be uniformly ambiguous between files. After checking and testing the original shader obtained from the third party partner, blurring may occur.
The system and method may be applied to a variety of augmented reality renderings including, but not limited to, cosmetic renderings, furniture renderings, apparel renderings, video game renderings, and architectural structure renderings.
FIG. 2 depicts a block diagram of an example data asset acquisition system 200, according to an example embodiment of the disclosure. In some implementations, the data asset acquisition system 200 is trained to send a software development kit 212 for building an augmented reality rendering experience and receive templates and presets 228 from a third party as a result of sending the software development kit 212. Thus, in some implementations, the data acquisition system 200 may include a facilitator 210, a third party 220, and a user 250.
In particular, FIG. 2 depicts a system for data asset acquisition for generating an augmented reality rendering experience. The facilitator 210 can be a facilitation system that compiles data assets for augmented reality rendering by communicating with a third party 220. Facilitator 210 can be a server, a web application, or a facilitation computing system. When the facilitator 210 has received the data asset, the facilitation system can provide an augmented reality rendering experience to the user 250.
The data asset acquisition may include a facilitator-built Software Development Kit (SDK) 212 that is sent to a third party 220. A third party may use the software development kit 222 to build a rendering experience. The software development toolkit 222 may compile the data assets 224 and may allow for rendering previews 226 of the generated rendering experience.
The third party 220 may use the rendering previews to determine what source modifications 230 (if any) need to be made to the templates and presets 228 of its data assets. When the build, test, and trim are complete, the third party may send its data assets to the facilitation system, including templates and presets 228 and third party metadata 232. The facilitation system 210 can ingest the effects 214 and store them for later provision of an augmented reality rendering experience that can include rendering effects built by the third party 220. The facilitator may obtain user data 252 from user 250 and may output enhanced user data with rendering effects included in the enhanced user data.
Fig. 3 depicts a block diagram of an example ambiguity 300, according to an example embodiment of the present disclosure. Blur 300 may be included in data asset acquisition system 200 of FIG. 2 to blur a data asset.
More specifically, fig. 3 depicts the method under tri-pipe alignment for blurring. In this embodiment, the original code 302 may be obfuscated using an obfuscation system 304 to generate an obfuscated code 306. In some implementations, the original code 302 can be data asset code for augmented reality rendering.
Further, in this embodiment, the obfuscation system 304 may include renaming the symbol or term 308, removing the space 310, and removing the annotation 312 associated with the semantics.
Renaming 308 may include indexing symbols and terms from the original code and replacing the original symbols and terms with assigned symbols and terms. In some implementations, renaming 308 may include utilizing a hash function. Renaming between different files may be consistent.
Removing the space 310 may include removing indentations, space lines, and carriage returns. Removing spaces can obscure the logical format of the code, making the code more difficult to read.
Removing the annotation 312 may include removing one or more annotations related to code semantics. The obfuscation system may process the code and determine whether the comment is a semantic comment. If the annotation is used to interpret code semantics, the annotation may be removed.
These three components can reduce the readability of the code while also hiding third party symbols and terms.
Fig. 4 depicts a block diagram of an example data asset acquisition system 400, according to an example embodiment of the disclosure. The data asset acquisition system 400 is similar to the data asset acquisition system 200 of fig. 2, except that the data asset acquisition system 400 may be specifically configured for use in the beauty effect rendering.
In this embodiment, the cosmetic effect 402 may include cosmetic rendering. The cosmetic effect may include different sets of data. The data set may include a rendering entity 404 data set with blueprints, GLSL shaders 406, textures and geometry files. The cosmetic effects 402 may also include rendering subgraphs, shader unification, and image conversion. These data sets may be input into a software development kit 408 to build an augmented reality rendering experience. The software development kit 408 may allow the builder to preview and test newly built experiences. In particular, the GLSL shader 406 of cosmetic effects may be converted into a shader pipeline within a software development kit. The constructed augmented reality rendering experience may be concurrent with the pre-existing beauty effects 402 rendering experience outside of the software development kit 408. When the build is complete, the software development kit may generate a renderable zip file 410 for recreating the augmented reality rendering experience on another device.
Fig. 5 depicts a block diagram of an example augmented reality rendering experience model 500, according to an example embodiment of the disclosure. The augmented reality rendering experience model 500 is similar to the data asset acquisition system 200 of fig. 2, except that the augmented reality rendering experience model 500 also includes processing a camera feed with a mesh model and an augmented model with an augmented model using the generated data assets.
In particular, fig. 5 depicts an example perceptual subgraph 504 and an example rendering subgraph 508 for processing a camera feed 502 to generate a rendered image 510. Perceptual subgraph 504 may employ mesh model 506 to process camera feed 502 to generate a mesh and segmentation mask. The mesh and segmentation mask may be processed by a render calculator 512. Rendering calculator 512 may be included in rendering sub-graph 508 and may be affected by the shader. The render calculator 512 may process the mesh, the segmentation mask, and the camera feed 502 to generate a rendered image. Rendering the image may include augmented reality rendering. The rendering may be a rendering generated by a third party using a software development kit. Further, in some implementations, the rendering may be a cosmetic rendering, where the mesh model may be a face tracker, and the shader may include a lipstick shader, a texture shader, a unified shader, and/or a filter shader.
Example method
Fig. 6 depicts a flowchart of an example method performed in accordance with an example embodiment of the present disclosure. Although fig. 6 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 600 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 602, a computing system may send a software development kit. In some implementations, the software development kit can include templates for building one or more render effect shaders. The software development kit may include scripts and associated binary files for compiling the source effects into renderable files. The software development kit may also include a preview tool for configuring third parties to preview their rendered builds during the build for fine-tuning and testing. In some implementations, the software development kit can include documents and sample effects to help third parties how to employ the software development kit to build and fine-tune effects.
At 604, the computing system may receive a data asset. In some implementations, the data assets can include one or more render effect shaders built using a software development kit. The data asset may be received in the form of a renderable file (e.g., a ZIP file). The data asset may include shader data for rendering a product or item sold or provided by a third party. The product may be a cosmetic (e.g., lipstick, eye shadow, etc.), furniture or other household items (e.g., electronic equipment, cookware, glassware, ornaments, plants, etc.), clothing, paint colors, automobiles, or any other item.
At 606, the computing system may store the data asset. The data asset may be stored on a server, a user computing device, or a facilitator computing device.
At 608, the computing system may provide an augmented reality rendering experience. In some implementations, the augmented reality rendering may be based at least in part on the data asset. The augmented reality rendering may include a product rendering of a product sold by a third party.
Fig. 7 depicts a flowchart of an example method performed in accordance with an example embodiment of the present disclosure. Although fig. 7 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 700 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 702, a computing system may receive a software development kit. The software development kit may include templates for generating an augmented reality experience. The software development kits may be sent by the facilitating computing system or an intermediary system to compile a rendering experience for provision to a user.
At 704, the computing system may receive input to modify the software development kit. Modifying the software development kit may modify the templates and may generate a product-specific augmented reality experience. For example, a software development kit may be constructed and configured to generate cosmetic rendering effects. Cosmetic rendering effects may be implemented as an augmented reality rendering experience for users to "try out" using different lipstick colors or types provided by their personal computing devices.
At 706, the computing system may generate a data asset. In some implementations, the data asset can be generated based at least in part on the received input. The data asset may be converted into a renderable compressed file for transmission.
At 708, the computing system may send the data asset to the second computing device. The second computing device may be a sender of a software development kit. In some implementations, the second computing device may be a facilitating computing device or an intermediary computing device that interacts with the user to provide the user with the augmented reality rendering experience.
Fig. 8 depicts a flowchart of an example method performed in accordance with an example embodiment of the present disclosure. Although fig. 8 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 800 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 802, a computing system can send a software development kit. In some implementations, the software development kit can include templates for building one or more render effect shaders. The base application may be an augmented reality rendering application having example effects and guidelines for constructing a new rendering or translating previously constructed rendering effects.
At 804, the computing system may receive a data asset. In some implementations, the data assets can include one or more render effect shaders built using a software development kit. The data asset may be received as a renderable file.
At 806, the computing system may obfuscate the data asset. Obscuring data assets may include removing spaces from code, removing notes about code semantics, and renaming symbols and terms. Blurring can reduce code readability and remove proprietary information that may be included in the term or annotation. Blurring may help protect third party builders.
At 808, the computing system may store the data asset. The data assets may be stored locally or via a server. The data assets may be stored as easily accessible for use with a network application or mobile application.
At 810, the computing system can provide an augmented reality rendering experience. In some implementations, the augmented reality rendering may be based at least in part on the data asset. The augmented reality rendering experience may include an experience that provides enhanced user images and video to include rendering effects built by a third party. The rendering effect may be part of a video game or "real-time trial" experience. Further, the augmented reality rendering experience may be provided via a web application, a mobile application, or an in-store kiosk.
Additional disclosure
The technology discussed herein refers to servers, databases, software applications, and other computer-based systems, and actions taken and information sent to and received from such systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components working in combination. The database and application may be implemented on a single system or may be distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation and not limitation of the present disclosure. Modifications, variations and equivalents of such embodiments will be readily apparent to those skilled in the art in view of the foregoing teachings. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (20)
1. A computer-implemented method for providing third party data assets to a client, the method comprising:
transmitting, by the computing device, a software development kit, wherein the software development kit includes templates for building one or more render effect shaders;
receiving, by the computing device, a data asset, wherein the data asset includes one or more render effect shaders built using the software development kit;
storing, by the computing device, a data asset; and
an augmented reality rendering experience is provided by the computing device, wherein the augmented reality rendering is based at least in part on the data asset.
2. The method of any preceding claim, further comprising:
blurring, by the computing device, the data asset, wherein blurring comprises:
removing, by the computing device, a space from the code of the data asset;
removing, by the computing device, one or more annotations from code of the data asset; and
one or more terms in code of the data asset are renamed by the computing device.
3. A method according to any preceding claim, wherein the data asset comprises anchor landmark data.
4. A method according to any preceding claim, wherein the data assets comprise shader data.
5. The method of claim 4, wherein the shader data describes at least one of lipstick color, lipstick gloss, or lipstick opacity.
6. The method of claim 4, wherein the shader data describes eye shadow colors.
7. A method according to any preceding claim, wherein the data asset comprises data from a third party.
8. The method of any preceding claim, wherein blurring of the data asset by the computing device occurs before providing, by the computing device, an augmented reality rendering experience, wherein augmented reality rendering is based at least in part on the data asset.
9. The method of any preceding claim, wherein the augmented reality rendering is a cosmetic augmented reality rendering.
10. The method of any preceding claim, wherein the augmented reality rendering is a furniture augmented reality rendering.
11. The method of any preceding claim, wherein the augmented reality rendering is a video game augmented reality rendering.
12. A method according to any preceding claim, wherein the data assets comprise product data describing products sold by a third party.
13. The method of any preceding claim, wherein the augmented reality rendering is generated by:
receiving, by the computing device, user data;
processing, by the computing device, the user data using an encoder model to generate a user grid; and
processing, by the computing device, the user mesh with an augmentation model to generate an augmented reality rendering, wherein the augmentation model includes a shader based at least in part on the data asset.
14. The method of any preceding claim, wherein the one or more annotations comprise text describing code semantics.
15. The method of any preceding claim, wherein renaming, by the computing device, one or more terms in the code of the data asset comprises unified renaming across files.
16. A computing system, comprising:
one or more processors;
one or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause a computing system to perform operations comprising:
Receiving a software development kit including templates for generating an augmented reality experience;
receiving one or more inputs to modify the template, wherein modifying the software development kit generates a product-specific augmented reality experience;
generating a data asset based at least in part on the one or more inputs; and
the data asset is sent to a second computing device.
17. The computing system of claim 16, further comprising: the augmented reality experience is tested.
18. The computing system of claim 16 or 17, wherein testing the augmented reality experience comprises:
obtaining training data;
processing training data with an augmented reality experience to generate augmented reality media;
evaluating a loss function based at least in part on a comparison between the augmented reality media and the underlying real data; and
one or more parameters are adjusted based at least in part on the loss function.
19. One or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause a computing system to perform operations comprising:
transmitting, by the computing device, a software development kit, wherein the software development kit includes templates for building one or more render effect shaders;
Receiving, by the computing device, a data asset, wherein the data asset includes one or more render effect shaders built using the software development kit;
storing, by the computing device, the data asset; and
an augmented reality rendering experience is provided by the computing device, wherein the augmented reality rendering is based at least in part on the data asset.
20. The one or more non-transitory computer-readable media of claim 19, wherein the software development kit includes a perception model.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/144,002 US11521358B2 (en) | 2021-01-07 | 2021-01-07 | Computing platform for facilitating augmented reality experiences with third party assets |
US17/144,002 | 2021-01-07 | ||
PCT/US2022/011064 WO2022150273A1 (en) | 2021-01-07 | 2022-01-04 | Computing platform for facilitating augmented reality experiences with third party assets |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116868187A true CN116868187A (en) | 2023-10-10 |
Family
ID=80445765
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280016010.7A Pending CN116868187A (en) | 2021-01-07 | 2022-01-04 | Computing platform for facilitating augmented reality experience with third party assets |
Country Status (6)
Country | Link |
---|---|
US (2) | US11521358B2 (en) |
EP (1) | EP4256447A1 (en) |
JP (1) | JP7457211B2 (en) |
KR (1) | KR20230122172A (en) |
CN (1) | CN116868187A (en) |
WO (1) | WO2022150273A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230343037A1 (en) * | 2022-04-25 | 2023-10-26 | Snap Inc. | Persisting augmented reality experiences |
US20240019979A1 (en) * | 2022-07-15 | 2024-01-18 | Lenovo (Singapore) Pte. Ltd. | Conversion of 3d virtual actions into 2d actions |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160163111A1 (en) * | 2013-03-15 | 2016-06-09 | Daqri, Llc | Content creation tool |
CN106062705A (en) * | 2014-02-24 | 2016-10-26 | 微软技术许可有限责任公司 | Cross-platform rendering engine |
US20170132841A1 (en) * | 2015-09-22 | 2017-05-11 | 3D Product Imaging Inc. | Augmented reality e-commerce for home improvement |
CN107924584A (en) * | 2015-08-20 | 2018-04-17 | 微软技术许可有限责任公司 | Augmented reality |
US20190129715A1 (en) * | 2016-07-18 | 2019-05-02 | Tencent Technology (Shenzhen) Company Limited | Method, apparatus and system for generating augmented reality module and storage medium |
CN110140144A (en) * | 2017-10-31 | 2019-08-16 | 谷歌有限责任公司 | For verifying the image processing system of the data of rendering |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150040074A1 (en) * | 2011-08-18 | 2015-02-05 | Layar B.V. | Methods and systems for enabling creation of augmented reality content |
GB2518589B (en) * | 2013-07-30 | 2019-12-11 | Holition Ltd | Image processing |
US20150082298A1 (en) * | 2013-09-19 | 2015-03-19 | Qiu Shi WANG | Packaging and deploying hybrid applications |
US9373223B1 (en) | 2014-12-17 | 2016-06-21 | Jackpot Rising Inc. | Method and system for gaming revenue |
US20170206708A1 (en) * | 2016-01-19 | 2017-07-20 | Immersv, Inc. | Generating a virtual reality environment for displaying content |
WO2020097573A1 (en) * | 2018-11-08 | 2020-05-14 | Appdome Ltd. | Artificial intelligence mobile integration |
US11449713B2 (en) * | 2018-11-16 | 2022-09-20 | Uatc, Llc | Attention based feature compression and localization for autonomous devices |
US10665011B1 (en) * | 2019-05-31 | 2020-05-26 | Adobe Inc. | Dynamically estimating lighting parameters for positions within augmented-reality scenes based on global and local features |
-
2021
- 2021-01-07 US US17/144,002 patent/US11521358B2/en active Active
-
2022
- 2022-01-04 JP JP2023541344A patent/JP7457211B2/en active Active
- 2022-01-04 KR KR1020237026574A patent/KR20230122172A/en not_active Application Discontinuation
- 2022-01-04 WO PCT/US2022/011064 patent/WO2022150273A1/en active Application Filing
- 2022-01-04 EP EP22703466.7A patent/EP4256447A1/en active Pending
- 2022-01-04 CN CN202280016010.7A patent/CN116868187A/en active Pending
- 2022-11-23 US US17/993,334 patent/US20230092068A1/en active Pending
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160163111A1 (en) * | 2013-03-15 | 2016-06-09 | Daqri, Llc | Content creation tool |
CN106062705A (en) * | 2014-02-24 | 2016-10-26 | 微软技术许可有限责任公司 | Cross-platform rendering engine |
CN107924584A (en) * | 2015-08-20 | 2018-04-17 | 微软技术许可有限责任公司 | Augmented reality |
US20170132841A1 (en) * | 2015-09-22 | 2017-05-11 | 3D Product Imaging Inc. | Augmented reality e-commerce for home improvement |
US20190129715A1 (en) * | 2016-07-18 | 2019-05-02 | Tencent Technology (Shenzhen) Company Limited | Method, apparatus and system for generating augmented reality module and storage medium |
CN110140144A (en) * | 2017-10-31 | 2019-08-16 | 谷歌有限责任公司 | For verifying the image processing system of the data of rendering |
Also Published As
Publication number | Publication date |
---|---|
US11521358B2 (en) | 2022-12-06 |
JP2024504063A (en) | 2024-01-30 |
WO2022150273A1 (en) | 2022-07-14 |
JP7457211B2 (en) | 2024-03-27 |
EP4256447A1 (en) | 2023-10-11 |
US20230092068A1 (en) | 2023-03-23 |
US20220215633A1 (en) | 2022-07-07 |
KR20230122172A (en) | 2023-08-22 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Cordeil et al. | IATK: An immersive analytics toolkit | |
US10592238B2 (en) | Application system that enables a plurality of runtime versions of an application | |
US11670061B2 (en) | Generating augmented reality prerenderings using template images | |
Gecer et al. | Synthesizing coupled 3d face modalities by trunk-branch generative adversarial networks | |
Kim et al. | Near-exhaustive precomputation of secondary cloth effects | |
Flotyński et al. | Ontology‐Based Representation and Modelling of Synthetic 3D Content: A State‐of‐the‐Art Review | |
Qian et al. | Scalar: Authoring semantically adaptive augmented reality experiences in virtual reality | |
US20230092068A1 (en) | Computing Platform for Facilitating Augmented Reality Experiences with Third Party Assets | |
Ranjan et al. | Learning multi-human optical flow | |
Stemasov et al. | The road to ubiquitous personal fabrication: Modeling-free instead of increasingly simple | |
Zeng et al. | 3D human body reshaping with anthropometric modeling | |
Walczak et al. | Semantic query-based generation of customized 3D scenes | |
Jaspe-Villanueva et al. | Web-based exploration of annotated multi-layered relightable image models | |
Vilchis et al. | A survey on the pipeline evolution of facial capture and tracking for digital humans | |
Walczak et al. | Inference-based creation of synthetic 3D content with ontologies | |
US8379028B1 (en) | Rigweb | |
JP2024061750A (en) | Computing platform for facilitating augmented reality experiences with third party assets | |
Pucihar et al. | FUSE: Towards AI-Based Future Services for Generating Augmented Reality Experiences | |
US11869136B1 (en) | User-context aware rendering dataset selection | |
US20230281952A1 (en) | Generating object images with different lighting conditions | |
Robertson | Visualizing spatial data: The problem of paradigms | |
Choudhary et al. | 3D-Zeit: A Framework for Generating 3D Model as an AR Object Using Object Recognition | |
Clarke et al. | Generating Pseudo Random Volumes for Volumetric Research | |
CN117876579A (en) | Platform for enabling multiple users to generate and use neural radiation field models | |
Chauha | Study on Real-Time Streaming of 3D |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |