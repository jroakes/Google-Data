CN107608520B - Graphical interface with adjustable boundaries - Google Patents
Graphical interface with adjustable boundaries Download PDFInfo
- Publication number
- CN107608520B CN107608520B CN201710891777.5A CN201710891777A CN107608520B CN 107608520 B CN107608520 B CN 107608520B CN 201710891777 A CN201710891777 A CN 201710891777A CN 107608520 B CN107608520 B CN 107608520B
- Authority
- CN
- China
- Prior art keywords
- area
- graphical interface
- navigable
- navigable area
- boundary
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/012—Head tracking input arrangements
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/013—Eye tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
Abstract
Methods and systems relating to navigation of graphical interfaces are disclosed herein. The method comprises the following steps: causing a graphical display of a head-mountable display (HMD) to provide a graphical interface, the graphical interface comprising: (i) a graphical interface region comprising a portion of interest and another portion not of interest; (ii) a navigable area occupying a portion of the graphical interface area and corresponding to a portion of interest of the graphical interface area; and (iii) a view port occupying a portion of the navigable area; subsequently receiving head movement data indicative of movement of the HMD; causing the view port to move relative to the navigable area toward a boundary of the navigable area at a movement speed based on the head movement data; determining that the movement speed exceeds a predetermined movement speed threshold; and moving both the navigable area and the view port based at least on a determination that the speed of movement exceeds a predetermined speed of movement threshold such that the spacing of the view port from the boundaries of the navigable area remains at a predetermined difference.
Description
The application is a divisional application of an invention patent application with the international application date of 2012, 11/27, and the chinese application number of 201280068560.X, and the invention name of the invention is "graphical interface with adjustable boundary".
Cross Reference to Related Applications
This application claims priority to U.S. patent application No. 13/307,168, filed on 30/11/2011, the contents of which are hereby incorporated by reference in their entirety for all purposes.
Background
Unless otherwise indicated herein, the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.
Computing devices such as personal computers, laptop computers, tablet computers, cellular telephones, and countless other types of networking capable devices are becoming increasingly common in many aspects of modern life. Over time, the manner in which these devices provide information to users is becoming more intelligent, more efficient, more intuitive, and/or less obtrusive.
Furthermore, the trend toward miniaturization of computing hardware, peripherals, and sensors, detectors, and image and audio processors, as well as other technologies, has helped open a field sometimes referred to as "wearable computing. In particular, in the field of visual processing and production, it has become possible to implement wearable displays that place very small image display elements close enough to the wearer's (or user's) eyes (one or both) so that the displayed image fills or nearly fills the wearer's field of view and appears as a normal size image, such as may be displayed on conventional image display devices. This technique may be referred to as "near-eye display".
Near-eye displays are a basic component of wearable computers having a display, sometimes also referred to as "head-mounted displays" (HMDs). The emerging and intended use of the near-eye display may advantageously provide seamless use of the wearable computer. However, due to the limited size of near-eye displays, several challenges may arise, particularly when viewing, searching and/or browsing graphical interfaces that cannot be fully viewed within the display at one time. This limited size of the near-eye display can make some uses of wearable computers potentially cumbersome. Thus, improvements are desired.
Disclosure of Invention
The systems and methods described herein may help provide more convenient, efficient, and/or intuitive interaction with graphical interfaces that cannot be fully viewed within a display at one time. In one example embodiment, a system is provided. The system may include: (1) at least one processor; (2) a non-transitory computer readable medium; and (3) program instructions stored on a non-transitory computer readable medium and executable by the at least one processor to: (a) causing a Head Mounted Display (HMD) to provide a graphical interface including (i) a viewport having a viewport orientation (orientation) and (ii) at least one navigable area having at least one boundary with a first boundary orientation; (b) receiving input data indicating movement of the viewport towards at least one boundary; (c) determining that the view-port orientation is within a predetermined threshold distance from the first boundary orientation; and (d) adjusting the first boundary orientation from the first boundary orientation to the second boundary orientation based at least on the determination that the view-port orientation is within the predetermined threshold distance from the first boundary orientation.
An example embodiment may be directed to a wearable computing system, e.g., an HMD, that continuously collects information about a user's body movements (e.g., via sensors such as accelerometers, gyroscopes, and other input devices), and in response provides a viewing port that displays a portion of a navigable area of a graphical interface. Such embodiments may include an HMD with a rich set of sensors to closely track the user's body movements. For example, one embodiment may include the entire computing system within the HMD itself and/or may be networked with other computer systems for tracking such body movements. Other examples and variations are possible, some of which are discussed herein.
In another aspect, a non-transitory computer-readable medium is provided. The non-transitory computer-readable medium may include instructions comprising: (1) instructions for causing a Head Mounted Display (HMD) to provide a graphical interface including (i) a viewport having a viewport orientation and (ii) at least one navigable area having at least one boundary, the at least one boundary having a first boundary orientation; (2) instructions for receiving input data indicating movement of the view port toward the at least one boundary; (3) instructions for determining that the view-port orientation is within a predetermined threshold distance from the first boundary orientation; and (4) instructions for adjusting the first boundary orientation from the first boundary orientation to the second boundary orientation based at least on a determination that the view-port orientation is within a predetermined threshold distance from the first boundary orientation.
In yet another aspect, a computer-implemented method is provided. The method can comprise the following steps: (1) cause a Head Mounted Display (HMD) to provide a graphical interface including (i) a viewport having a viewport orientation and (ii) at least one navigable area having at least one boundary, the at least one boundary having a first boundary orientation; (2) receiving input data indicating movement of the viewport towards at least one boundary; (3) determining that the view-port orientation is within a predetermined threshold distance from the first boundary orientation; and (4) adjusting the first boundary orientation from the first boundary orientation to the second boundary orientation based at least on the determination that the view-port orientation is within the predetermined threshold distance from the first boundary orientation.
In yet another aspect, a system is provided, the system comprising: a non-transitory computer readable medium; and program instructions stored on a non-transitory computer readable medium and executable by the at least one processor to: causing a graphical display of a head-mountable display (HMD) to provide a graphical interface, the graphical interface comprising: (i) a graphical interface region comprising a portion of interest and another portion not of interest; (ii) a navigable area occupying a portion of the graphical interface area and corresponding to a portion of interest of the graphical interface area; and (iii) a view port occupying a portion of the navigable area, wherein initially the navigable area is fixed within the graphical interface area and the view port is movable within the navigable area, and wherein the view port corresponds to a viewing area provided by the graphical display such that a portion of the navigable area and a larger portion of the graphical interface area are outside of a viewable area provided in the graphical display; subsequently receiving head movement data indicative of movement of the HMD; causing the view port to move relative to the navigable area toward a boundary of the navigable area at a movement speed based on the head movement data; determining that the movement speed exceeds a predetermined movement speed threshold; and moving both the navigable area and the view port within the graphical interface area based at least on the determination that the speed of movement exceeds the predetermined speed of movement threshold such that a separation of the border of the navigable area and the view port is maintained at a predetermined difference.
In yet another aspect, a non-transitory computer-readable medium is provided having instructions stored therein, which when executed by a computing device, cause the computing device to perform functions comprising: causing a graphical display of a head-mountable display (HMD) to provide a graphical interface, the graphical interface comprising: (i) a graphical interface region comprising a portion of interest and another portion not of interest; (ii) a navigable area occupying a portion of the graphical interface area and corresponding to a portion of interest of the graphical interface area; and (iii) a view port occupying a portion of the navigable area, wherein initially the navigable area is fixed within the graphical interface area and the view port is movable within the navigable area, and wherein the view port corresponds to a field of view provided in the graphical display such that a portion of the navigable area and a larger portion of the graphical interface area are outside the field of view provided in the graphical display; subsequently receiving head movement data indicative of movement of the HMD; causing the view port to move relative to the navigable area toward a boundary of the navigable area at a movement speed based on the head movement data; determining that the movement speed exceeds a predetermined movement speed threshold; and moving both the navigable area and the view port within the graphical interface area based at least on the determination that the speed of movement exceeds the predetermined speed of movement threshold such that a separation of the border of the navigable area and the view port is maintained at a predetermined difference.
In yet another aspect, a method is provided, the method comprising: causing a graphical display of a head-mountable display (HMD) to provide a graphical interface, the graphical interface comprising: (i) a graphical interface region comprising a portion of interest and another portion not of interest; (ii) a navigable area occupying a portion of the graphical interface area and corresponding to a portion of interest of the graphical interface area; and (iii) a view port occupying a portion of the navigable area, wherein initially the navigable area is fixed within the graphical interface area and the view port is movable within the navigable area, and wherein the view port corresponds to a field of view provided in the graphical display such that a portion of the navigable area and a larger portion of the graphical interface area are outside the field of view provided in the graphical display; subsequently receiving head movement data indicative of movement of the HMD; causing the view port to move relative to the navigable area toward a boundary of the navigable area at a movement speed based on the head movement data; determining that the movement speed exceeds a predetermined movement speed threshold; and moving both the navigable area and the view port within the graphical interface area based at least on the determination that the speed of movement exceeds the predetermined speed of movement threshold such that a separation of the border of the navigable area and the view port is maintained at a predetermined difference.
These and other aspects, advantages, and alternatives will become apparent to one of ordinary skill in the art by reading the following detailed description, with appropriate reference to the accompanying drawings.
Drawings
FIG. 1A illustrates an example system for receiving, transmitting, and displaying data.
FIG. 1B shows an alternative view of the system illustrated in FIG. 1A.
FIG. 2A illustrates an example system for receiving, transmitting, and displaying data.
FIG. 2B illustrates an example system for receiving, transmitting, and displaying data.
FIG. 3 shows a flow diagram depicting an example method for adjusting boundary orientations.
Fig. 4A shows an example movable Head Mounted Display (HMD).
Fig. 4B illustrates the example HMD and example viewing port of fig. 4A.
Fig. 4C shows the example HMD and example navigable area of fig. 4A.
Fig. 5 shows the example HMD of fig. 4A-4C and a line representation of a view port within a navigable area.
FIG. 6 illustrates viewports within various example navigable areas of a graphical interface.
FIG. 7 illustrates a viewport having example navigable areas of various orientations within a graphical interface.
FIG. 8 illustrates another view port of an example navigable area within a graphical interface having multiple orientations.
FIG. 9 illustrates a view port within a variety of additional example navigable areas of a graphical interface.
FIG. 10 shows a simplified block diagram of an example computer network infrastructure.
FIG. 11 shows a simplified block diagram depicting example components of an example computing system.
FIG. 12A illustrates aspects of an example user interface.
FIG. 12B shows aspects of the example user interface after receiving movement data corresponding to an upward movement.
FIG. 12C shows aspects of the example user interface after selecting a selected content object.
FIG. 12D shows aspects of the example user interface after receiving input data corresponding to user input.
Detailed Description
In the following detailed description, reference is made to the accompanying drawings, which form a part hereof. In the drawings, like reference numerals generally identify like components unless context dictates otherwise. The illustrative embodiments described in the detailed description, drawings, and claims are not meant to be limiting. Other embodiments may be utilized, and other changes may be made, without departing from the spirit or scope of the subject matter presented here. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which have been contemplated herein.
1. Overview
Example embodiments relate to a wearable computer that may be configured to receive input data from a user's head and/or eye movements and in response control a position (orientation) of a view port (view port) within a navigable area (navigableearea) of a graphical interface. The viewing port, navigable area, and/or graphical interface may be displayed on a graphical display, such as a Head Mounted Display (HMD). The viewport may be configured to display only a portion of the navigable area and to hover (hover) or pan (pan) within the navigable area in response to certain gestures (e.g., movement of the head and/or eyes). In this case, the example viewing port may provide a wearer of the HMD with a feel of seeing through the window (i.e., viewing port) into a portion of the navigable area of the graphical interface.
The navigable area may have one or more boundaries that define or otherwise affect how the user interacts with the graphical interface via the viewport. For example, consider an HMD that is worn by a user and thus may rotate 360 degrees in real space (e.g., by the user turning their head and/or body). While the HMD may rotate a full 360 degrees in real space, there may be instances where the navigable area of the graphical interface implemented by the HMD is only 180 degrees in a given implementation. Such an implementation may be logical given the following: the user is stationary and able to move his head (relatively comfortably) 90 degrees to the left and 90 degrees to the right. In other words, the user can translate the viewport throughout the 180-degree navigable area simply by moving his head to the left and right.
However, consider the following case: the user (i) rotates his head more than 90 degrees to the left or right, or (ii) rotates his body while rotating his head at least 90 degrees to the left or right. In such a case, the viewing port may extend over a 180 degree navigable area of a 360 degree graphical interface. If the 180 degree navigable area is configured to contain content of interest (and another 180 degrees of the graphical interface, for example, does not contain any content of interest), then it may not be desirable for the view port to be moved outside of the 180 degree navigable area.
Such a situation may occur, for example, when the viewing port approaches the left boundary of the navigable area and then the user of the HMD turns left. In this example, the user may naturally turn his body and/or head further to the left, potentially inadvertently (and undesirably) moving the view port outside of the navigable area.
However, in accordance with the disclosure herein, the navigable area may be configured to have a boundary that, when approached or contacted by the viewport, causes the navigable area to be repositioned within the graphical interface based on the position and/or movement of the viewport. In this way, the graphical interface may be configured such that the viewport is not undesirably moved beyond the navigable area. In other words, in the context of an example of a user turning, the navigable area can be made to "turn" along with the user (and the viewport).
It should be appreciated that the above example is an example embodiment, is provided for purposes of illustration, and is but one of many possible applications of an example embodiment.
2. Example wearable computing device
FIG. 1A illustrates an example system 100 for receiving, transmitting, and displaying data. System 100 is shown in the form of a wearable computing device. Although fig. 1A illustrates the head-mounted device 102 as an example of a wearable computing device, other types of wearable computing devices may additionally or alternatively be used. As illustrated in fig. 1A, the head-mounted device 102 has frame elements including lens frames 104, 106 and a central frame support 108, lens elements 110, 112, and extending side arms 114, 116. The central frame support 108 and the extending side- arms 114, 116 are configured to secure the head-mounted device 102 to the face of the user via the nose and ears, respectively, of the user.
Each of the frame elements 104, 106, and 108 and the extending side arms 114, 116 may be formed of a solid structure of plastic and/or metal, or may be formed of a hollow structure of similar material to allow wiring and component interconnections to be routed internally through the head-mounted device 102. Other materials may also be possible.
One or more of each of the lens elements 110, 112 may be formed of any material capable of suitably displaying a projected image or graphic. Each of the lens elements 110, 112 may also be sufficiently transparent to allow a user to see through the lens elements. Combining these two features of the lens elements may facilitate an augmented reality or heads-up display in which the projected image or graphic is superimposed on the real-world view perceived by the user through the lens elements 110, 112.
The extending side arms 114, 116 may each be a protrusion extending away from the lens frames 104, 106, respectively, and may be positioned behind the user's ears to secure the head-mounted device 102 to the user. The extending side- arms 114, 116 may also secure the head-mounted device 102 to the user by extending around the back of the user's head. Additionally or alternatively, for example, the system 100 may be connected to or affixed within a head-worn helmet structure. Other possibilities also exist.
The system 100 may also include an on-board computing system 118, a video camera 120, a sensor 122, and a finger-operable touch pad 124. The on-board computing system 118 is shown positioned on the extended side arm 114 of the head-mounted device 102; however, on-board computing system 118 may be provided on other portions of head-mounted device 102 or may be located remotely from head-mounted device 102 (e.g., on-board computing system 118 may be wired or wirelessly connected to head-mounted device 102). The on-board computing system 118 may include, for example, a processor and memory. The on-board computing system 118 may be configured to receive and analyze data from the video camera 120, the sensor 122, and the finger-operable touch pad 124 (and possibly from other sensing devices, user interfaces, or both) and generate images for output by the lens elements 110 and 112. The on-board computing system 118 may additionally include a speaker or microphone for user input (not shown). An example computing system is further described below in connection with fig. 11.
The video camera 120 is shown positioned on the extended side arm 114 of the head mounted device 102; however, the video camera 120 may be provided on other portions of the head mounted device 102. The video camera 120 may be configured to capture images at various resolutions or at different frame rates. For example, video cameras with small form factors, such as those used in cell phones or webcams, may be incorporated into example embodiments of system 100.
Additionally, although fig. 1A illustrates one video camera 120, more video cameras may be used and each may be configured to capture the same view, or to capture different views. For example, the video camera 120 may be directed forward to capture at least a portion of the real-world view perceived by the user. This forward image captured by the video camera 120 can then be used to generate augmented reality, where the computer-generated image appears to interact with the real-world view perceived by the user.
The sensor 122 is shown on the extended side arm 116 of the head mounted device 102; however, the sensor 122 may be positioned on other portions of the head mounted device 102. The sensor 122 may include, for example, one or more of a gyroscope or an accelerometer. Other sensing devices may be included within sensor 122 or may be included in addition to sensor 122, or sensor 122 may perform other sensing functions.
A finger-operable touchpad 124 is shown on the extending side arm 114 of the head-mounted device 102. However, the finger-operable touch pad 124 may be positioned on other portions of the head-mounted device 102. Additionally, there may be more than one finger-operable touchpad on the head-mounted device 102. The finger-operable touch pad 124 may be used by a user to input commands. The finger-operable touch pad 124 may sense at least one of a position and a movement of the finger via capacitive sensing, resistive sensing, or a surface acoustic wave process, among others. The finger-operable touch pad 124 may be capable of sensing finger movement in a direction parallel to or in the same plane as the board surface, in a direction perpendicular to the board surface, or both, and may also be capable of sensing the level of pressure applied to the board surface. The finger-operable touch pad 124 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conductive layers. The edge of the finger-operable touch pad 124 may be formed with a raised, depressed, or roughened surface to provide tactile feedback to the user when the user's finger reaches the edge or other area of the finger-operable touch pad 124. If there is more than one finger operable touch pad, each finger operable touch pad may be independently operated and may provide different functions.
FIG. 1B illustrates an alternative view of the system 100 illustrated in FIG. 1A. As shown in fig. 1B, the lens elements 110, 112 may act as display elements. The head mounted device 102 may include a first projector 128, the first projector 128 coupled to an inner surface of the extending side arm 116 and configured to project a display 130 onto an inner surface of the lens element 112. Additionally or alternatively, a second projector 132 may be coupled to an inner surface of the extending side arm 114 and configured to project a display 134 onto the inner surface of the lens element 110.
The lens elements 110, 112 may act as combiners in the light projection system and may include a coating that reflects light projected thereon from the projectors 128, 132. In some embodiments, the reflective coating may be omitted (e.g., when the projectors 128, 132 are scanning laser devices).
In alternative embodiments, other types of display elements may be used. For example, the lens elements 110, 112 may themselves include: a transparent or translucent matrix display such as an electroluminescent display or a liquid crystal display, one or more waveguides for delivering images to the eye of a user, or other optical elements capable of delivering in-focus near-to-eye images to a user. Corresponding display drivers may be disposed within the frame elements 104, 106 for driving such a matrix display. Alternatively or additionally, a laser or Light Emitting Diode (LED) source and scanning system may be used to capture the raster display directly onto the retina of the user's eye or eyes. Other possibilities also exist.
FIG. 2A illustrates an example system 200 for receiving, transmitting, and displaying data. System 200 is shown in the form of a wearable computing device 202. Wearable computing device 202 may include a frame element and side arms, such as those described with respect to fig. 1A and 1B. Wearable computing device 202 may additionally include an on-board computing system 204 and a video camera 206, such as those described with respect to fig. 1A and 1B. Video camera 206 is shown mounted on the frame of wearable computing device 202; however, the video camera 206 may be mounted at other locations as well.
As shown in fig. 2A, wearable computing device 202 may include a single display 208, which display 208 may be coupled to the device. Display 208 may be formed on one of the lens elements of wearable computing device 202, such as the lens elements described with respect to fig. 1A and 1B, and display 208 may be configured to overlay computer-generated graphics in the user's view of the physical world. Display 208 is shown as being centered on a lens of wearable computing device 202, however, display 208 may be provided in other locations. The display 208 may be controlled via the computing system 204 coupled to the display 208 through the optical waveguide 210.
FIG. 2B illustrates an example system 220 for receiving, transmitting, and displaying data. System 220 is shown in the form of a wearable computing device 222. The wearable computing device 222 may include side arms 223, a central frame support 224, and a nose bridge portion 225 with a nose pad. In the example shown in fig. 2B, a central frame support 224 connects the side arms 223. The wearable computing device 222 does not include a lens frame containing lens elements. Wearable computing device 222 may additionally include an on-board computing system 226 and video camera 208, such as those described with respect to fig. 1A and 1B.
The wearable computing device 222 may include a single lens element 230, which lens element 230 may be coupled to one of the side arms 223 or the central frame support 224. The lens element 230 may include a display, such as the display described with reference to fig. 1A and 1B, and may be configured to overlay computer-generated graphics over the user's view of the physical world. In one example, a single lens element 230 may be coupled to one side of the extending side arm 223. When the wearable computing device 222 is worn by a user, the single lens element 230 may be positioned in front of or near the user's eyes. For example, a single lens element 230 may be positioned below the central frame support 224, as shown in fig. 2B.
2. Example method
FIG. 3 shows a flow diagram depicting an example method for adjusting boundary orientations. Method 300 is described as being performed by a wearable computer, and in particular by a wearable computer that includes an HMD, as an example. However, it should be understood that example methods such as method 300 may be performed by devices other than wearable computers and/or may be performed by subsystems in wearable computers or other devices. For example, the example method may alternatively be performed by a device, such as a mobile phone, configured to simultaneously display a viewport within a navigable area of a graphical interface. Other examples are also possible.
In addition, those skilled in the art will appreciate that the flow charts described herein illustrate the function and operation of certain implementations of example embodiments. Here, each block of the flow diagrams may represent a module, segment, or portion of program code, which comprises one or more instructions executable by a processor (e.g., processor 1102 described below with respect to system 1100) to perform the specified logical function or step in the process. The program code may be stored, for example, on any type of computer-readable medium (e.g., a computer-readable storage medium or a non-transitory medium such as memory 1104 or storage device 1106 described below with respect to system 1100), such as a storage device including a disk or hard drive. Further, each block may represent a circuit wired to perform a specific logical function in the process. Alternative implementations are included within the scope of the example embodiments of the present application in which functions may be executed out of order from that shown or discussed, including substantially concurrently or in reverse order, depending on the functionality involved, as would be understood by those reasonably skilled in the art.
Example methods may be used to allow a user to navigate through a navigable area via a view port via head movement. The navigable area may generally be "larger" than the view port. Further, the graphical interface may generally be "larger" than the navigable area.
The example method 300 includes, as shown in block 302, causing a Head Mounted Display (HMD) to provide a graphical interface including (i) a view port having a view port orientation and (ii) at least one navigable area having at least one border, the at least one border having a first border orientation. At block 304, the HMD receives input data indicating movement of the viewing port toward the at least one boundary. At block 306, the HMD determines that the view-port orientation is within a predetermined threshold distance from the first boundary orientation. And at block 308, the HMD adjusts the boundary orientation from the first boundary orientation to a second boundary orientation based at least on a determination that the view-port orientation is within a predetermined threshold distance from the boundary orientation. Each of these blocks is discussed further below.
a. Providing a graphical interface comprising
(i) Viewing port with viewing port orientation
(ii) Navigable area with border orientation
At block 302, method 300 includes causing the HMD to provide a graphical interface including (i) a view port having a view port orientation and (ii) at least one navigable area having at least one boundary with a first boundary orientation. The viewing port, navigable area, and graphical interface may be displayed via a graphical display of the HMD.
With respect to fig. 4, an example HMD400 is shown. HMD400 may take the form of any of those wearable computing systems discussed above with respect to fig. 1A-1B and fig. 2A-2B. HMD400 may also take other forms. It should be understood that the example HMD400 is shown for purposes of example and explanation only and should not be construed as limiting.
Fig. 4A illustrates that the HMD400 may rotate 360 degrees throughout at least one plane in real space. For example, when worn by a user, HMD400 may be rotated 404 to the left by the user turning their head and/or body to the left. As another example, HMD400 may be rotated 406 to the right by the user turning their head and/or body to the right when worn by the user. While HMD400 is shown as being rotatable in an entire plane, HMD400 may also be rotatable in any other plane, axis, and/or direction when worn by a user. The graphical interface 402 indicates that a 360 degree virtual interface around the user may be provided to the user or otherwise made available to the user.
Fig. 4B illustrates that the HMD400 may display a viewing port 408, that the viewing port 408 may be a virtual display overlaid on the user's real-world view, and that the viewing port 408 may provide a view of a portion of the 360-degree graphical interface 402. The viewing port 408 may move throughout the 360 degree graphical interface 402 depending on the rotation of the HMD 400.
In some embodiments, less than the entire 360 degrees of graphical interface 402 may contain content of interest to the user. FIG. 4C illustrates, for example, that graphical interface 402 may include navigable area 410 of only half or 180 degrees of the entire 360 degrees of graphical interface 402. The navigable area 410 may include content of interest to the wearer of the HMD400 while the remainder of the graphical interface 402 may not include content of interest to the wearer of the HMD 400. Thus, the user may pan the viewing port 408 throughout the navigable area 410 to view content within the navigable area 410 or otherwise interact with content within the navigable area 410.
For purposes of explanation, FIG. 5 shows a two-dimensional representation 500 of navigable area 410 and view port 408. The viewport 408 can be moved to the left and/or right throughout the navigable area 410 as indicated by the left arrow 502A and the right arrow 502B.
Additionally, the navigable area 410 is shown as having a left boundary 504A and a right boundary 504B. The left boundary 504A is shown with a boundary orientation of 0 degrees. On the other hand, the right boundary 504B is shown as having a boundary orientation of 180 degrees.
Similarly, the view port 408 may have a view port orientation. The viewport orientations may be based on the location of any single portion, point, or pixel of the viewport 408. For example, the viewport orientations may be based on the center 506B of the viewport 408. In the example shown, the central viewport location 506B may be 90 degrees (since the viewport 408 is currently located in the center of the navigable area 410). As another example, the view port orientation may be based on the left edge 506A of the view port 408. In the example shown, left viewing port orientation 506A may be some orientation between 0 degrees and 90 degrees. As yet another example, the viewport location may be based on the right edge 506C of the viewport 408. In the example shown, right viewing port orientation 506C may be some orientation between 90 degrees and 180 degrees.
b. Receiving input data indicating movement of a viewing port
At block 304, the method 300 includes receiving input data indicating movement of the view port toward the at least one boundary. In an embodiment, the input data may include head movement data and/or eye movement data indicative of movement of the HMD.
To explain the movement of the viewport towards the boundary of the navigable area, FIG. 6 shows the viewports within a variety of possible example navigable areas of a graphical interface. More specifically, FIG. 6 illustrates various graphical interfaces 600A-600D. Each of the graphical interfaces 600A-600D has a span of 360 degrees. In addition, each of graphical interfaces 600A-600D includes a respective navigable area. Note that in accordance with the disclosure herein, the graphical interface, navigable area, and viewing port can take any suitable form, including forms not explicitly described herein. That is, without limitation, the graphical interface, navigable area, and/or viewing port can take on other shapes and/or sizes than those shown herein.
As one example, graphical interface 600A includes a navigable area 602A. The navigable area 602A is shown as having a span of 90 degrees within the graphical interface 600A. The view port 604A may move to the left 606A or the right 606B within the navigable area 602A. The navigable area 602A has a first boundary 608A, the first boundary 608A having an orientation of 135 degrees. The navigable area 602A also has a second boundary 608B, the second boundary 608B having an orientation of 225 degrees. Thus, the view port 604A can move toward either of the first boundary 608A and the second boundary 608B.
As another example, graphical interface 600B includes a navigable area 602B. The navigable area 602B is shown as having a span of 180 degrees within the graphical interface 600B. The view port 604B may move to the left 606C or the right 606D within the navigable area 602B. The navigable area 602B has a first boundary 608C, the first boundary 608C having an orientation of 90 degrees. The navigable area 602B also has a second boundary 608D, the second boundary 608D having an orientation of 270 degrees. Thus, the view port 604B can move toward either of the first boundary 608C and the second boundary 608D.
As yet another example, graphical interface 600C includes a navigable area 602C. The navigable area 602C is shown as having a span of 270 degrees within the graphical interface 600C. The view port 604C may move to the left 606E or the right 606F within the navigable area 602C. The navigable area 602C has a first boundary 608E, the first boundary 608E having an orientation of 45 degrees. The navigable area 602C also has a second boundary 608F, the second boundary 608F having an orientation of 315 degrees. Thus, the view port 604C can move toward either of the first boundary 608E and the second boundary 608F.
As yet another example, graphical interface 600D includes a navigable area 602D. Navigable area 602D is shown as having a span of 360 degrees within graphical interface 600D (i.e., the entire graphical interface 600D). The view port 604D may move to the left 606G or the right 606H within the navigable area 602D. The navigable area 602D has a first boundary 608G with an orientation of 0 degrees 608G. The navigable area 602D also has a second boundary 608H, the second boundary 608H having an orientation of 360 degrees. Thus, the view port 604D can move toward any one of the first boundary 608G and the second boundary 608H.
c. Determining viewport orientations within threshold differences of boundary orientations
At block 306, method 300 includes determining that the view-port orientation is within a predetermined threshold distance from the first boundary orientation. FIG. 7 shows a viewport 704 in a navigable area 702 having multiple orientations within a graphical interface 700 at five different points in time, namely time A-time E.
More specifically, at time A, navigable area 702 has a left boundary position 702A of 90 degrees and a right boundary position 702B of 270 degrees, which corresponds to a total viewing span of 180 degrees within 360 degrees graphical interface 700. The viewport 704 is stationary within the navigable area 702 and centered within the navigable area 702, having a viewport orientation 706 of 180 degrees. The view-port position 706 is not within the threshold distance 708A of the right boundary position 702B. That is, the view-port position 706 falls outside of the threshold distance 708A of the right boundary position 702B.
At a later time B, navigable area 702 has the same left boundary position 702A of 90 degrees and right boundary position 702B of 270 degrees. However, the view port 704 is shown moving toward the right boundary, as indicated by arrow 706. Additionally, the view-port position 706 has been shifted toward the right boundary and the view-port position 706 is now within the threshold distance 708A of the right boundary position 702B. Accordingly, it may be determined that the view-port orientation is within a predetermined threshold distance from the first boundary orientation.
d. Adjusting boundary orientation
At block 308, the method 300 includes adjusting the boundary position from the first boundary position to the second boundary position based at least on determining that the view-port position is within a predetermined threshold distance from the boundary position.
Referring again to FIG. 7, at time C, navigable area 702 now has a left boundary position 702C, with the left boundary position 702C at a position to the right of the left boundary position 702A. Similarly, navigable area 702 has a right boundary position 702D, the right boundary position 702D being at a position to the right of the right boundary position 702B. However, navigable area 702 maintains a total viewing span of 180 degrees. Thus, based on a determination that view port position 706 is within threshold distance 708A of the right boundary of navigable area 702, navigable area 702 is effectively "shifted" to the right.
The boundary orientation may be adjusted to the second boundary orientation according to any suitable criteria, as per block 308. For example, adjusting the border orientation may include setting the border orientation equal to the view-port orientation. With respect to the example shown in FIG. 7 at time C, this approach may be particularly suitable when the viewport orientation is aligned with the right edge of viewport 704. In this way, the right edge of the viewport 704 may never move (to the right) past the right edge of the navigable area 702. In effect, when the viewport 704 is moved to the right, the navigable area 702 will also move to the right.
As another example, adjusting the boundary orientation may include setting the boundary orientation equal to a predetermined difference from the view-port orientation. With respect to the example shown in FIG. 7 at time C, it can be seen that the distance between view-port location 706 and right boundary location 702D is equal to the distance between view-port location 706 and right boundary location 702B at time B. In this way, the right edge of the viewport 704 may never move (to the right) past the right edge of the navigable area 702. In effect, when the viewport 704 is moved to the right, the navigable area 702 will also move to the right. At the same time, a spatial "buffer" may be maintained between the right edge of the viewport 704 and the right edge of the navigable area 702.
As shown with respect to time D and time E, block 308 may include not only adjusting the boundary orientation to the right-the boundary orientation may also be adjusted to the left. More specifically, at time instant D, navigable area 702 has a left boundary position 702C and a right boundary position 702D. However, the viewport 704 is shown moving toward the left boundary, as indicated by arrow 708. Additionally, the view-port position 706 has been shifted toward the left boundary such that the view-port position 706 is now within the threshold distance 708B of the left boundary position 702C.
At a later time E, navigable area 702 has a left boundary position 702A, with the left boundary position 702A at a position to the left of the left boundary position 702C. Similarly, navigable area 702 has a right boundary position 702B, the right boundary position 702B being at a position to the left of the right boundary position 702D. However, navigable area 702 maintains a total viewing span of 180 degrees. Thus, based on a determination that viewport location 706 is within threshold distance 708B, navigable area 702 is actually "shifted" to the left.
In one embodiment, adjusting the orientation of the boundary may include determining that a movement speed of the viewport exceeds a predetermined movement speed threshold before adjusting the orientation of the boundary. In such an embodiment, the speed parameter may map a given amount of input movement (e.g., head movement) to a certain amount of viewport movement. More specifically, the sensitivity of the viewport may be configured in Counts Per Inch (CPI), which includes the number of counts the viewport moves one inch (or one degree, or other suitable distance metric) across the graphical interface. The movement speed threshold may be set, for example, to a given CPI, such that the position of the navigable area may not be changed if the movement speed of the viewport does not exceed the threshold CPI. However, if the speed of movement of the viewport exceeds the threshold CPI, the position of the navigable area may be changed.
In another embodiment, adjusting the orientation of the boundary may include determining that the movement speed of the viewport does not exceed a predetermined movement speed threshold before adjusting the orientation of the boundary. The movement speed threshold may be set, for example, to a given CPI, so that the position of the navigable area may be changed if the movement speed of the viewport does not exceed the threshold CPI. However, if the speed of movement of the viewport exceeds the threshold CPI, the position of the navigable area may not be changed.
In yet another embodiment, adjusting the orientation of the boundary may include providing a continuous visual indication of the orientation of the boundary adjusted from the first boundary orientation to the second boundary orientation. FIG. 8 shows a viewport 804 in a navigable area 802 having multiple orientations within a graphical interface 800 at five different points in time, namely time F-time J. Generally, navigable area 802 moves to the left during time F through time J. However, between time F and time J, at times G, H and I, navigable area 802 is shown as moving continuously. That is, the left boundary of navigable area 802 is shown moving from the left boundary position 802A at time F, to the left boundary position 802C at time G, to the left boundary position 802E at time H, to the left boundary position 802G at time I, and to the left boundary position 802I at time J. The navigable area 802 between time F and time J can also be shown at other locations. In this way, the continuous visual indication of the navigable area 802 can provide a visual perception that the navigable area 802 "slides" into place.
In yet another embodiment, adjusting the orientation of the boundary may include determining a second boundary orientation based at least on the speed of movement of the viewport. That is, while it is described above that the second boundary orientation may be statically set based on, for example, setting the boundary orientation equal to the view-port orientation or setting the boundary orientation equal to a predetermined difference from the view-port orientation, this is not necessary. The second boundary orientation may also be set in a dynamic manner based at least on the speed of movement of the viewport.
For purposes of explanation, consider that at time F, viewing port 804 is moving to the left at first speed 806. The first speed 806 may be relatively slow and thus the left boundary position 802A may be adjusted in a relatively unaffected manner. For example, as shown in time G, the left boundary orientation may be adjusted by setting the boundary orientation equal to the view-port orientation or setting the boundary orientation equal to a predetermined difference from the view-port orientation. Other examples may also exist.
However, consider that at time F, the view port 804 is moving at a second speed 808 that is greater than the first speed 806. The second speed 808 may be relatively fast and thus the right boundary position 802B may be adjusted in a relatively dynamic manner. For example, as shown in time H, the left boundary position may be adjusted first to the left boundary position 802C at time G and then on to the left boundary position 802E at time H. As such, the navigable area 802 may appear to "pop" away from the view port 804 when the view port 804 is moved toward the left edge of the navigable area 802 at speed 808. The amount of "pop-up" may increase in proportion to an increase in the moving speed of the viewing port.
Alternatively, adjusting the orientation of the border may include setting the orientation of the border equal to an orientation such that the viewport is centered within the navigable area. For purposes of explanation, consider that at time F, view port 804 is moving to the left at velocity 806 or velocity 808. Upon determining that the boundary orientation needs to be adjusted, the boundary orientation may be adjusted to correspond to the left boundary orientation 802I, as shown at time J. As depicted, the left boundary position 802I corresponds to a position such that the view port 804 is centered within the navigable area 802.
Note that while the center of the navigable area 802 is being reset in this manner, the navigable area 802 can be shown as moving continuously from time F to time J. That is, the left boundary of navigable area 802 is shown moving from the left boundary position 802A at time F, to the left boundary position 802C at time G, to the left boundary position 802E at time H, to the left boundary position 802G at time I, and to the left boundary position 802I at time J. The navigable area 802 between time F and time J can also be shown at other locations. In this way, the continuous visual indication of the navigable area 802 can provide a visual perception that the navigable area 802 "slides" into place, centered around the view port 804.
To summarize some aspects of the example shown in fig. 8, the graphical interface may be a circular graphical interface (i.e., a 360 degree graphical interface). The graphical interface may include a navigable area. The navigable area may have a navigable span of 180 degrees and the view port may have a viewing span of less than 180 degrees.
In the above, embodiments have been discussed that include only a single navigable area within a graphical interface. However, it is possible that the graphical interface may include multiple navigable areas. In such embodiments, the navigable areas may be separated by what is referred to herein as a "soft boundary". The soft boundary may be configured such that the orientation of a given navigable area in a graphical interface having multiple navigable areas may be adjusted under some conditions. However, under other conditions, the view port may be allowed to cross the soft boundary from the first navigable area to the second navigable area.
FIG. 9 shows view ports within various additional navigable areas of the graphical interface. More specifically, FIG. 9 shows a graphical interface 900 that includes three navigable areas 904, 906, and 908. Graphical interface 900 also includes a viewing port 910.
At time K, viewport 910 is shown stationary within navigable area 906. The navigable area 906 is separated from the navigable area 904 at its left edge by a soft boundary 902A. The navigable area 906 is separated from the navigable area 908 at its right edge by a soft boundary 902B. Each of the soft boundaries 902A and 902B has a soft boundary orientation. More specifically, the soft boundary 902A has a soft boundary orientation 902C. The soft boundary 902B has a soft boundary position 902D.
In an embodiment, the soft boundary orientation may be adjusted. Adjusting the soft boundary position may include determining that (a) the movement speed of the view port does not exceed a predetermined movement speed threshold, and (b) the view port position is within a predetermined threshold distance from the soft boundary position; and adjusting the soft boundary position based at least on the determination that (a) the movement speed does not exceed the predetermined movement speed threshold and (b) the view port position is within a predetermined threshold distance from the soft boundary position. In other words, if the viewport approaches the soft boundary at a relatively slow movement speed, the soft boundary orientation may be adjusted to leave the viewport within the navigable area in which it is currently located.
For purposes of explanation, at time L, viewport 910 is shown moving to the left toward soft boundary 902A at a movement speed 912. Assume that the movement speed 912 does not exceed the predetermined movement speed threshold. The soft boundary position 902C may be adjusted when the view port 910 comes within a predetermined threshold distance from the soft boundary position 902C. For example, as shown at time M thereafter, the soft boundary position of soft boundary 902A has been adjusted to soft boundary position 902E, which is to the left of soft boundary position 902C.
Note that movement of navigable area 906 corresponds to similar movement of navigable areas 904 and 908. That is, when a given navigable area of a graphical interface is moved, other navigable areas of the graphical interface may also be moved.
Alternatively, the viewing port may be allowed to move from one navigable area to another. Moving the view port from one navigable area to another navigable area may include determining that (a) the movement speed exceeds a predetermined movement speed threshold and (b) the view port position is within a predetermined threshold distance from the soft boundary position; and causing the view port to enter the second navigable area based at least on a determination that (a) the movement speed exceeds a predetermined movement speed threshold and (b) the view port orientation is within a predetermined threshold distance from the soft boundary orientation. In other words, if the viewport approaches a soft boundary at a relatively fast moving speed, the viewport may cross the soft boundary and enter another navigable area.
For purposes of explanation, at time N, viewport 910 is shown moving rightward toward soft boundary 902B at a moving speed 914. Assume that the movement speed 914 exceeds a predetermined movement speed threshold. Upon the viewport 910 entering within a predetermined threshold distance from the soft boundary position 902F, the viewport 910 may be allowed to cross the soft boundary 902b, exit the navigable area 906, and enter the navigable area 908. For example, as shown at a later time O, viewport 910 is shown within navigable area 908.
To summarize some aspects of the example shown in fig. 9, the graphical interface may be a circular graphical interface (i.e., a 360 degree graphical interface). The graphical interface may include three navigable areas. Each navigable area may have a navigable span of 120 degrees and the view ports may have a viewing span of less than 120 degrees.
Note, however, that while the example shown in fig. 9 contains three navigable areas, this is not necessary. More generally, the graphical interface may include at least two navigable areas separated by a soft boundary having a soft boundary orientation. Thus, any number of navigable areas are possible, wherein the navigable areas are each separated by a soft boundary.
3. Example computing System and network architecture
FIG. 10 shows a simplified block diagram of an example computer network infrastructure. In system 1000, device 1010 communicates with remote device 1030 using a communication link 1020 (e.g., a wired or wireless connection). Device 1030 may be any type of device capable of receiving data and displaying information corresponding to or associated with the data. For example, the device 1030 may be a heads-up display system, such as the head mounted devices 102, 200, or 220 described with reference to fig. 1A-2B.
Thus, the device 1010 may include a display system 1012, the display system 1012 including a processor 1014 and a display 1016. Display 1016 may be, for example, an optical see-through display, an optical surround view display, or a video see-through display. Processor 1014 may receive data from remote device 1030 and configure the data for display on display 1016. The processor 1014 may be any type of processor such as, for example, a microprocessor or a digital signal processor.
The apparatus 1010 may also include on-board data storage, such as a memory 1018 coupled to the processor 1014. Memory 1018 may store, for example, software that is accessible and executable by processor 1014.
In fig. 10, communication link 1020 is illustrated as a wireless connection; however, a wired connection may also be used. For example, communication link 1020 may be a wired serial bus such as a universal serial bus, or a parallel bus, or other connection. Communication link 1020 may also be used, for exampleRemote device 1030 may be accessible via the internet and may include a computing cluster associated with a particular web service (e.g., social network, photo sharing, address book, etc.).
As described above in connection with fig. 1A-2B, an example wearable computing device may include or may otherwise be communicatively coupled to a computing system, such as computing system 118 or computing system 204. FIG. 11 shows a simplified block diagram depicting example components of an example computing system 1100. One or both of device 1010 and remote device 1030 may take the form of computing system 1100.
The computing system 1100 may include at least one processor 1102 and a system memory 1104. In an example embodiment, the computing system 1100 may include a system bus 1106 that communicatively connects the processor 1102 and the system memory 1104, as well as other components of the computing system 1100. Depending on the desired configuration, the processor 1102 may be any type of processor including, but not limited to, a microprocessor (μ P), a microcontroller (μ C), a Digital Signal Processor (DSP), or any combination thereof. Additionally, the system memory 1104 may be any type of memory now known or later developed including, but not limited to, volatile memory (e.g., RAM), non-volatile memory (e.g., ROM, flash memory, etc.), or any combination thereof.
The example computing system 1100 may also include various other components. For example, computing system 1100 includes an a/V processing unit 1108 for controlling a graphics display 1110 and speakers 1112 (via a/V port 1114), one or more communication interfaces 1116 for connecting to other computing devices 1118, and a power supply 1120. The graphical display 1110 may be arranged to provide a visual depiction of the various input regions provided by the user interface module 1122. For example, user interface module 1122 may be configured to provide a user interface, such as the example user interfaces described below in connection with fig. 12A-D, and graphical display 1110 may be configured to provide a visual depiction of the user interface. The user interface module 1122 may also be configured to receive data from and transmit data to (or otherwise be compatible with) one or more user interface devices 1128.
Further, the computing system 1100 may also include one or more data storage devices 1124, which may be removable storage devices, non-removable storage devices, or a combination thereof. Examples of removable and non-removable storage devices include magnetic disk devices such as floppy disk drives and Hard Disk Drives (HDDs), optical disk drives such as Compact Disk (CD) drives or Digital Versatile Disk (DVD) drives, Solid State Drives (SSDs), and/or any other storage device now known or later developed. Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules or other data. For example, computer storage media may take the form of: computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, Digital Versatile Disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing system 1100, now known or later developed.
According to an example embodiment, the computing system 1100 may include program instructions 1126, the program instructions 1126 being stored in the system memory 1104 (and/or possibly in additional data storage media) and executable by the processor 1102 to facilitate various functions described herein, including but not limited to those described with respect to fig. 3. While the various components of the computing system 1100 are illustrated as distributed components, it should be understood that any such components may be physically integrated and/or distributed in accordance with a desired configuration of the computing system.
4. Example user interface
12A-D illustrate aspects of an example user interface 1200. User interface 1200 may be displayed by a wearable computing device, such as described above with respect to fig. 1A-2B.
An example state of the user interface 1200 is shown in fig. 12A. The example state shown in fig. 12A may correspond to a first position of the wearable computing device. That is, when the wearable computing device is in the first position, the user interface 1200 may be displayed as shown in fig. 12A. In some embodiments, the first position of the wearable computing device may correspond to a position of the wearable computing device when a wearer of the wearable computing device is looking in a direction generally parallel to the ground (e.g., not corresponding to a position where the wearer is looking up or down). Other examples are also possible.
As shown, the user interface 1200 includes a viewing port 1202. Example boundaries of the view port 1202 are illustrated by dashed boxes. Although viewing port 1202 is shown as having a transverse shape (where viewing port 1202 has a width that is greater than a height), in other embodiments viewing port 1202 may have a longitudinal or square shape, or may have a non-rectangular shape, such as a circular or elliptical shape. The view port 1202 may also have other shapes.
The viewing port 1202 may be, for example, a viewable area between (or encompassing) the upper, lower, left, and right boundaries of a display on a wearable computing device. As shown, when the wearable computing device is in the first position, the view port 1202 is substantially free (e.g., completely free) of user interface elements, such that the user's view of their real-world environment is substantially unobscured and objects in the user's environment are not obscured.
In some embodiments, the viewing port 1202 may correspond to a field of view of a wearer of the wearable computing device, and the area outside of the viewing port 1202 may correspond to an area outside of the field of view of the wearer. In other embodiments, the viewing port 1202 may correspond to a non-peripheral portion of the wearer's field of view of the wearable computing device, and the region outside of the viewing port 1202 may correspond to a peripheral portion of the wearer's field of view. In still other embodiments, the user interface 1200 may be larger than or substantially the same as the field of view of the wearer of the wearable computing device, and the field of view of the wearer may be larger than the viewing port 1202 or substantially the same size as the viewing port 1202. The view port 1202 may take other forms as well.
Thus, the portion of the user interface 1200 outside of the viewing port 1202 may be outside of the field of view of the wearer of the wearable computing device or in a peripheral portion of the field of view of the wearer of the wearable computing device. For example, as shown, menu 1204 may be outside of the user's field of view in user interface 1200 or in a peripheral portion of the user's field of view. While menu 1204 is shown as not being visible in view port 1202, in some embodiments menu 1204 may be partially visible in view port 1202.
In some embodiments, the wearable computing device may be configured to receive movement data corresponding to, for example, the wearable computing device moving upward to a position above the first position. In these embodiments, the wearable computing device may cause one or both of the view port 1202 and the menu 1204 to move in response to receiving movement data corresponding to the upward movement, causing the menu 1204 to become more visible in the view port 1202. For example, the wearable computing device may cause the view port 1202 to move up and may cause the menu 1204 to move down. The view port 1202 and the menu 1204 may be moved the same amount or may be moved different amounts. In one embodiment, the menu 1204 may move farther than the view port 1202. As another example, wearable computing device may only cause menu 1204 to move. Other examples are also possible.
Although the word "upward" is used, it is understood that upward movement may include any movement with any combination of movement, tilting, rotating, shifting, sliding, or other movement that results in a generally upward movement. Additionally, in some embodiments, "upward" may refer to an upward movement in a frame of reference of a wearer of the wearable computing device. Other frames of reference are also possible. In embodiments where the wearable computing device is a head-mounted device, the upward movement of the wearable computing device may also be an upward movement of the wearer's head, e.g., the user looking up.
Movement data corresponding to upward movement may take several forms. For example, the movement data may be (or may be derived from) data received from one or more motion sensors, accelerometers, and/or gyroscopes configured to detect upward movement, such as sensor 122 described above in connection with fig. 1A. In some embodiments, the movement data may include a binary indication corresponding to an upward movement. In other embodiments, the movement data may include an indication corresponding to the upward movement and a magnitude of the upward movement. Movement data may also take other forms.
FIG. 12B shows aspects of the example user interface after receiving movement data corresponding to an upward movement. As shown, user interface 1200 includes a view port 1202 and a menu 1204.
As indicated above, in response to receiving movement data corresponding to an upward movement of the wearable computing device, the wearable computing device may move one or both of view port 1202 and menu 1204 to cause menu 1204 to become more visible in view port 1202.
As shown, menu 1204 is fully visible in view port 1202. However, in other embodiments, only a portion of the menu 1204 may be visible in the view port 1202. In some embodiments, the magnitude of the menu 1204 visible in the view port 1202 may be based at least in part on the magnitude of the upward movement.
Thus, the view port 1202 may be moved in response to receiving data corresponding to an upward movement. In some embodiments, the view port 1202 may be moved in an upward scrolling or panning motion. For example, the view port 1202 may appear to the wearer of the wearable computing device as being mapped to the interior of a static ball centered on the wearable computing device, and movement of the view port 1202 may be mapped to movement of the real-world environment relative to the wearable computing device. The speed, acceleration, and/or magnitude of the upward scrolling may be based at least in part on the speed, acceleration, and/or magnitude of the upward movement. In other embodiments, the view port 1202 may be moved by, for example, jumping between views. In still other embodiments, the view port 1202 may only be moved when the upward movement exceeds a threshold speed, acceleration, and/or magnitude. In response to receiving data corresponding to an upward movement that exceeds such one or more thresholds, the view port 1202 may pan, scroll, slide, or jump to a new field of view. The view port 1202 may also be moved in other ways.
While the above description focuses on upward movement, it is to be understood that the wearable computing device may also be configured to receive data corresponding to other directional movements (e.g., downward, leftward, rightward, etc.), and the view port 1202 may be moved in response to receiving such data in a manner similar to that described above in connection with upward movement.
As shown, menu 1204 includes several content objects 1206. In some embodiments, the content objects 1206 may be arranged as a ring (or partial ring) over and around the head of a wearer of the wearable computing device. In other embodiments, content objects 1206 may be arranged in a dome shape over the head of the wearer. The ring or dome may be centered over the wearable computing device and/or the wearer's head. In other embodiments, the content objects 1206 may be arranged in other ways as well.
The number of content objects 1206 in the menu 1204 may be fixed or may be variable. In a variable number embodiment, the size of content objects 1206 may vary depending on the number of content objects 1206 in menu 1204. In embodiments where the content objects 1206 extend in a circular shape around the wearer's head, just like a ring (or partial ring), only some of the content objects 1206 may be visible at a particular moment in time. To view other content objects 1204, a wearer of the wearable computing device may interact with the wearable computing device to rotate content object 1206, e.g., along a path around the wearer's head (e.g., clockwise or counterclockwise). To this end, the wearable computing device may be configured to receive data indicative of such interaction through, for example, a touchpad, such as the finger-operable touchpad 124. Alternatively or additionally, the wearable computing device may also be configured to receive such data through other input devices.
Depending on the application of the wearable computing device, the content object 1206 may take several forms. For example, the content object 1206 may include one or more of the following: a person, a contact, a group of people and/or contacts, a calendar entry, a list, a notification, an alert, a reminder, a status update, an incoming message, recorded media, an audio recording, a video recording, a photograph, a digital collage, a previously saved status, a web page, an application, and tools such as a still camera, a video camera, and an audio recorder. The content object 1206 may take other forms as well.
In embodiments where content object 1206 includes a tool, the tool may be located in a particular area of menu 1204, such as the center. In some embodiments, the tool may remain in the center of the menu 1204 even if the other content objects 1206 rotate, as described above. Tool content objects may also be located in other areas of menu 1204.
The particular content object 1206 included in the menu 1204 may be fixed or variable. For example, the content object 1206 may be pre-selected by a wearer of the wearable computing device. In another embodiment, the content objects 1206 for each content area may be automatically assembled by the wearable computing device from one or more physical or digital contexts, including, for example: people, places, and/or objects around the wearable computing device, address books, calendars, social networking web services or applications, photo sharing web services or applications, search histories, and/or other contexts. Additionally, some content objects 1206 may be fixed, while content objects 1206 may be variable. The content object 1206 may also be selected in other ways.
Similarly, the order or configuration in which the content objects 1206 are displayed may be fixed or variable. In one embodiment, the content objects 1206 may be pre-ordered by a wearer of the wearable computing device. In another embodiment, the content objects 1206 may be automatically ordered based on, for example, the following criteria: the frequency with which each content object 1206 is used (only on the wearable computing device or also in other contexts), when each content object 1206 was recently used (only on the wearable computing device or also in other contexts), an explicit or implicit importance or priority ranking of the content objects 1206, and/or other criteria.
In some embodiments, the wearable computing device may also be configured to receive a selection of the content object 1206 from the menu 1204 from the wearer. To this end, the user interface 1200 may include a cursor 1208, shown in FIG. 12B as a cross-hair, that may be used to navigate to and select a content object 1206 from the menu 1204. In some embodiments, the cursor 1208 may be controlled by the wearer of the wearable computing device through one or more predetermined movements. Thus, the wearable computing device may also be configured to receive selection data corresponding to the one or more predetermined movements.
The selection data may take several forms. For example, the selection data may be (or may be derived from) data received from one or more movement sensors, accelerometers, gyroscopes, and/or detectors configured to detect one or more predetermined movements. The one or more movement sensors may be included in a wearable computing device, like sensor 122, or may be included in a peripheral device communicatively coupled to the wearable computing device. As another example, the selection data may be (or may be derived from) data received from a touchpad (e.g., the finger-operable touchpad 124 described above in connection with fig. 1A), or from other input devices included in or coupled to the wearable computing device and configured to detect one or more predetermined movements. In some embodiments, the selection data may take the form of a binary indication corresponding to the predetermined movement. In other embodiments, the selection data may indicate a magnitude, direction, velocity, and/or acceleration associated with the predetermined movement. The selection data may take other forms as well.
The predetermined movement may take several forms. In some embodiments, the predetermined movement may be a particular movement or sequence of movements of the wearable computing device or peripheral device. In some embodiments, the predetermined movement may include one or more predetermined movements defined as no or substantially no movement, e.g., no or substantially no movement during a predetermined period of time. In embodiments where the wearable computing device is a head-mounted device, the one or more predetermined movements may involve predetermined movements of the wearer's head (assuming that they move the wearable computing device in a corresponding manner). Alternatively or additionally, the predetermined movement may involve a predetermined movement of a peripheral device communicatively coupled to the wearable computing device. The peripheral device may similarly be wearable by a wearer of the wearable computing device such that movement of the peripheral device may follow movement of the wearer, such as, for example, movement of the wearer's hand. Further, alternatively or additionally, the one or more predetermined movements may be movements on a finger-operable touch pad or other input device, for example. Other predetermined movements are also possible.
As shown, the wearer of the wearable computing device has navigated a cursor 1208 to the content object 1206 with one or more predetermined movements. To select the content object 1206, the wearer may perform additional predetermined movements, such as holding the cursor 1208 over the content object 1206 for a predetermined period of time. The wearer may select content object 1206 in other ways as well.
Once the content object 1206 is selected, the wearable computing device may cause the content object 1206 to be displayed in the viewing port 1202 as the selected content object. FIG. 12C illustrates aspects of an example user interface after selecting a selected content object, in accordance with an embodiment.
As indicated by the dashed arrow, content object 1206 is displayed in view port 1202 as selected content object 1210. As shown, the selected content object 1210 is displayed larger and more detailed in the viewport 1202 than in the menu 1204. However, in other embodiments, selected content object 1210 may be displayed in viewport 1202 smaller or the same size than menu 1204 and in less detail or as such. In some embodiments, additional content (e.g., actions to be applied to the selected content object 1210, actions to be applied with the selected content object 1210 or actions to be applied based on the selected content object 1210, information related to the selected content object 1210, and/or modifiable options, preferences, or parameters for the selected content object 1210, etc.) may be shown in the viewport 1202 adjacent to or near the selected content object 1210.
Once the selected content object 1210 is displayed in the viewing port 1202, the wearer of the wearable computing device may interact with the selected content object 1210. For example, because selected content object 1210 is shown as an email inbox, the wearer may wish to read one of the emails in the email inbox. Depending on the selected content object, the wearer may also interact with the selected content object in other ways (e.g., the wearer may locate additional information related to the selected content object 1210, modify, enhance, and/or delete the selected content object 1210, etc.). To this end, the wearable computing device may also be configured to receive input data corresponding to one or more predetermined movements indicative of interaction with the user interface 1200. The input data may take any of the forms described above in connection with the selection data.
FIG. 12D illustrates aspects of an example user interface after receiving input data corresponding to a user input, in accordance with an embodiment. As shown, the wearer of the wearable computing device has navigated the cursor 1208 to and selected a particular subject line in the email inbox. As a result, the email 1212 is displayed in the viewing port so that the wearer can read the email 1212. The wearer may also interact with user interface 1200 in other ways depending on, for example, the content object selected.
5. Conclusion
While various example aspects and example embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various exemplary aspects and exemplary embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope and spirit being indicated by the following claims.
Claims (11)
1. A system for navigation of a graphical interface, comprising:
a non-transitory computer readable medium; and
program instructions stored on a non-transitory computer readable medium and executable by at least one processor to:
causing a graphical display of the head-mountable display HMD to provide a graphical interface, the graphical interface comprising: (i) a graphical interface region comprising a portion of interest and another portion not of interest; (ii) a navigable area occupying a portion of the graphical interface area and corresponding to a portion of interest of the graphical interface area; and (iii) a view port occupying a portion of the navigable area, wherein initially the navigable area is fixed within the graphical interface area and the view port is movable within the navigable area, and wherein the view port corresponds to a viewing area provided by the graphical display such that a portion of the navigable area and a larger portion of the graphical interface area are outside of a viewable area provided in the graphical display;
subsequently receiving head movement data indicative of a movement of the head mountable display HMD;
causing the view port to move relative to the navigable area toward a boundary of the navigable area at a movement speed based on the head movement data;
determining that the movement speed exceeds a predetermined movement speed threshold; and
based at least on a determination that the movement speed exceeds a predetermined movement speed threshold, both the navigable area and the view port within the graphical interface area are moved such that a separation of a boundary of the navigable area and the view port is maintained at a predetermined difference.
2. The system of claim 1, wherein (i) the graphical interface region is a circular graphical interface region comprising a 360 degree span, (ii) the navigable region comprises a navigable span of 180 degrees, and (iii) the view port comprises a viewing span of less than 180 degrees.
3. The system of claim 1, further comprising program instructions stored on a non-transitory computer readable medium and executable by the at least one processor to:
a continuous visual indication of the movement of the navigable area is provided.
4. The system of claim 1, wherein the navigable area is a first navigable area, the graphical interface further comprising a second navigation area, the second navigation area occupying a different portion of the graphical interface area than the portion of the graphical interface area occupied by the first navigable area.
5. The system of claim 4, wherein the predetermined movement speed threshold is a first predetermined movement speed threshold, the system further comprising program instructions stored on a non-transitory computer readable medium and executable by the at least one processor to:
determining (i) that the movement speed exceeds a second predetermined movement speed threshold and (ii) that the view port is within a predetermined threshold distance from a boundary of the first navigable area; and
causing the viewport to enter the second navigable area based at least on a determination that (i) the speed of movement exceeds the second predetermined speed of movement threshold and (ii) the viewport is within a predetermined threshold distance from a boundary of the first navigable area.
6. The system of claim 4, wherein the graphical interface comprises a soft boundary separating the first navigable area and the second navigable area.
7. A non-transitory computer readable medium having stored therein instructions that, when executed by a computing device, cause the computing device to perform functions comprising:
causing a graphical display of the head-mountable display HMD to provide a graphical interface, the graphical interface comprising: (i) a graphical interface region comprising a portion of interest and another portion not of interest; (ii) a navigable area occupying a portion of the graphical interface area and corresponding to a portion of interest of the graphical interface area; and (iii) a view port occupying a portion of the navigable area, wherein initially the navigable area is fixed within the graphical interface area and the view port is movable within the navigable area, and wherein the view port corresponds to a field of view provided in the graphical display such that a portion of the navigable area and a larger portion of the graphical interface area are outside the field of view provided in the graphical display;
subsequently receiving head movement data indicative of a movement of the head mountable display HMD;
causing the view port to move relative to the navigable area toward a boundary of the navigable area at a movement speed based on the head movement data;
determining that the movement speed exceeds a predetermined movement speed threshold; and
based at least on a determination that the movement speed exceeds a predetermined movement speed threshold, both the navigable area and the view port within the graphical interface area are moved such that a separation of a boundary of the navigable area and the view port is maintained at a predetermined difference.
8. The non-transitory computer readable medium of claim 7, wherein (i) the graphical interface region is a circular graphical interface region comprising a 360 degree span, (ii) the navigable region comprises a navigable span of 180 degrees, and (iii) the view port comprises a viewing span of less than 180 degrees.
9. A method for navigation of a graphical interface comprising:
causing a graphical display of the head-mountable display HMD to provide a graphical interface, the graphical interface comprising: (i) a graphical interface region comprising a portion of interest and another portion not of interest; (ii) a navigable area occupying a portion of the graphical interface area and corresponding to a portion of interest of the graphical interface area; and (iii) a view port occupying a portion of the navigable area, wherein initially the navigable area is fixed within the graphical interface area and the view port is movable within the navigable area, and wherein the view port corresponds to a field of view provided in the graphical display such that a portion of the navigable area and a larger portion of the graphical interface area are outside the field of view provided in the graphical display;
subsequently receiving head movement data indicative of a movement of the head mountable display HMD;
causing the view port to move relative to the navigable area toward a boundary of the navigable area at a movement speed based on the head movement data;
determining that the movement speed exceeds a predetermined movement speed threshold; and
based at least on a determination that the movement speed exceeds a predetermined movement speed threshold, both the navigable area and the view port within the graphical interface area are moved such that a separation of a boundary of the navigable area and the view port is maintained at a predetermined difference.
10. The method of claim 9, wherein the navigable area is a first navigable area, the graphical interface further comprising a second navigation area, the second navigation area occupying a different portion of the graphical interface area than the portion of the graphical interface area occupied by the first navigable area.
11. The method of claim 10, wherein the predetermined movement speed threshold is a first predetermined movement speed threshold, the method further comprising:
determining (i) that the movement speed exceeds a second predetermined movement speed threshold and (ii) that the view port is within a predetermined threshold distance from a boundary of the first navigable area; and
causing the viewport to enter the second navigable area based at least on a determination that (i) the speed of movement exceeds the second predetermined speed of movement threshold and (ii) the viewport is within a predetermined threshold distance from a boundary of the first navigable area.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/307,168 US20130139082A1 (en) | 2011-11-30 | 2011-11-30 | Graphical Interface Having Adjustable Borders |
US13/307,168 | 2011-11-30 | ||
CN201280068560.XA CN104081256B (en) | 2011-11-30 | 2012-11-27 | Graphical interfaces with adjustable border |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201280068560.XA Division CN104081256B (en) | 2011-11-30 | 2012-11-27 | Graphical interfaces with adjustable border |
Publications (2)
Publication Number | Publication Date |
---|---|
CN107608520A CN107608520A (en) | 2018-01-19 |
CN107608520B true CN107608520B (en) | 2020-08-11 |
Family
ID=48467977
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201280068560.XA Active CN104081256B (en) | 2011-11-30 | 2012-11-27 | Graphical interfaces with adjustable border |
CN201710891777.5A Active CN107608520B (en) | 2011-11-30 | 2012-11-27 | Graphical interface with adjustable boundaries |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201280068560.XA Active CN104081256B (en) | 2011-11-30 | 2012-11-27 | Graphical interfaces with adjustable border |
Country Status (3)
Country | Link |
---|---|
US (3) | US20130139082A1 (en) |
CN (2) | CN104081256B (en) |
WO (1) | WO2013082034A1 (en) |
Families Citing this family (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9286711B2 (en) * | 2011-09-30 | 2016-03-15 | Microsoft Technology Licensing, Llc | Representing a location at a previous time period using an augmented reality display |
US9268406B2 (en) | 2011-09-30 | 2016-02-23 | Microsoft Technology Licensing, Llc | Virtual spectator experience with a personal audio/visual apparatus |
US9035878B1 (en) | 2012-02-29 | 2015-05-19 | Google Inc. | Input system |
US9691241B1 (en) | 2012-03-14 | 2017-06-27 | Google Inc. | Orientation of video based on the orientation of a display |
US8643951B1 (en) | 2012-03-15 | 2014-02-04 | Google Inc. | Graphical menu and interaction therewith through a viewing window |
US20140059472A1 (en) * | 2012-08-24 | 2014-02-27 | Recon Instruments Inc. | Methods and systems for controlling electronic devices |
US20140067869A1 (en) * | 2012-08-30 | 2014-03-06 | Atheer, Inc. | Method and apparatus for content association and history tracking in virtual and augmented reality |
US20140152558A1 (en) * | 2012-11-30 | 2014-06-05 | Tom Salter | Direct hologram manipulation using imu |
JP6123365B2 (en) * | 2013-03-11 | 2017-05-10 | セイコーエプソン株式会社 | Image display system and head-mounted display device |
US9041741B2 (en) * | 2013-03-14 | 2015-05-26 | Qualcomm Incorporated | User interface for a head mounted display |
JP6108926B2 (en) * | 2013-04-15 | 2017-04-05 | オリンパス株式会社 | Wearable device, program, and display control method for wearable device |
US9563331B2 (en) * | 2013-06-28 | 2017-02-07 | Microsoft Technology Licensing, Llc | Web-like hierarchical menu display configuration for a near-eye display |
US9465238B2 (en) | 2013-09-10 | 2016-10-11 | Jonathan Matthew Mazzola | Eyeglass system and method of use |
US9448687B1 (en) | 2014-02-05 | 2016-09-20 | Google Inc. | Zoomable/translatable browser interface for a head mounted device |
KR102227087B1 (en) * | 2014-07-08 | 2021-03-12 | 엘지전자 주식회사 | Wearable glass-type device and control method of the wearable glass-type device |
CN104306102B (en) * | 2014-10-10 | 2017-10-24 | 上海交通大学 | For the wear-type vision-aided system of dysopia patient |
CN104615241B (en) * | 2015-01-04 | 2017-08-25 | 谭希韬 | The Wearable glasses control method and system rotated based on head |
KR20160096422A (en) * | 2015-02-05 | 2016-08-16 | 삼성전자주식회사 | Method for displaying screen and electronic device |
US20180203437A1 (en) * | 2015-04-17 | 2018-07-19 | Tulip Interfaces, Inc. | Containerized communications gateway |
CN104834356B (en) | 2015-05-06 | 2018-04-13 | 百度在线网络技术（北京）有限公司 | The method and device of headset equipment and its visual feedback |
US10437323B2 (en) * | 2015-05-31 | 2019-10-08 | Fieldbit Ltd. | Controlling a head mounted device |
US20170092002A1 (en) * | 2015-09-30 | 2017-03-30 | Daqri, Llc | User interface for augmented reality system |
JP6518582B2 (en) * | 2015-12-21 | 2019-05-22 | 株式会社ソニー・インタラクティブエンタテインメント | Information processing apparatus and operation reception method |
US10976809B2 (en) * | 2016-03-14 | 2021-04-13 | Htc Corporation | Interaction method for virtual reality |
KR20170126295A (en) * | 2016-05-09 | 2017-11-17 | 엘지전자 주식회사 | Head mounted display device and method for controlling the same |
SG11201811520PA (en) * | 2016-07-04 | 2019-01-30 | Singapore Health Serv Pte Ltd | Apparatus and method for monitoring use of a device |
CN109478339A (en) * | 2016-07-29 | 2019-03-15 | 三菱电机株式会社 | Display device, display control unit and display control method |
US10339215B2 (en) * | 2016-12-14 | 2019-07-02 | International Business Machines Corporation | Determining a reading speed based on user behavior |
US11272160B2 (en) * | 2017-06-15 | 2022-03-08 | Lenovo (Singapore) Pte. Ltd. | Tracking a point of interest in a panoramic video |
US10489951B2 (en) | 2017-09-29 | 2019-11-26 | Qualcomm Incorporated | Display of a live scene and auxiliary object |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5812142A (en) * | 1994-09-30 | 1998-09-22 | Apple Computer, Inc. | Motion movement cueing through synchronized display port and image |
EP1116211A4 (en) * | 1998-09-22 | 2001-11-21 | Vega Vista Inc | Intuitive control of portable data displays |
US6445364B2 (en) * | 1995-11-28 | 2002-09-03 | Vega Vista, Inc. | Portable game display and method for controlling same |
CN1950863A (en) * | 2004-05-07 | 2007-04-18 | 株式会社日本耐美得 | Portable navigation terminal and program |
Family Cites Families (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6061064A (en) | 1993-08-31 | 2000-05-09 | Sun Microsystems, Inc. | System and method for providing and using a computer user interface with a view space having discrete portions |
JP3239641B2 (en) * | 1994-10-13 | 2001-12-17 | ミノルタ株式会社 | HMD |
US5742263A (en) | 1995-12-18 | 1998-04-21 | Telxon Corporation | Head tracking system for a head mounted display system |
AU2001233019A1 (en) * | 2000-01-28 | 2001-08-07 | Intersense, Inc. | Self-referenced tracking |
KR20030024021A (en) * | 2001-09-15 | 2003-03-26 | 김도균 | Head Mount Display |
US7056119B2 (en) * | 2001-11-29 | 2006-06-06 | Lsa, Inc. | Periscopic optical training system for operators of vehicles |
US6868360B1 (en) * | 2003-11-03 | 2005-03-15 | The United States Of America As Represented By The Secretary Of The Navy | Small head-mounted compass system with optical display |
KR100751290B1 (en) * | 2006-03-31 | 2007-08-23 | 한국과학기술연구원 | Image system for head mounted display |
EP2165234A1 (en) * | 2007-06-07 | 2010-03-24 | Panagiotis Pavlopoulos | An eyewear comprising at least one display device |
JP2009244869A (en) * | 2008-03-11 | 2009-10-22 | Panasonic Corp | Display apparatus, display method, goggle-type head-mounted display, and vehicle |
EP2211224A1 (en) | 2009-01-27 | 2010-07-28 | Thomson Licensing SA | Head-mounted display and operating method thereof |
US10440329B2 (en) * | 2009-05-22 | 2019-10-08 | Immersive Media Company | Hybrid media viewing application including a region of interest within a wide field of view |
-
2011
- 2011-11-30 US US13/307,168 patent/US20130139082A1/en not_active Abandoned
-
2012
- 2012-11-27 CN CN201280068560.XA patent/CN104081256B/en active Active
- 2012-11-27 WO PCT/US2012/066660 patent/WO2013082034A1/en active Application Filing
- 2012-11-27 CN CN201710891777.5A patent/CN107608520B/en active Active
-
2014
- 2014-05-27 US US14/288,033 patent/US10067559B2/en active Active
-
2018
- 2018-08-10 US US16/101,096 patent/US20190011982A1/en not_active Abandoned
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5812142A (en) * | 1994-09-30 | 1998-09-22 | Apple Computer, Inc. | Motion movement cueing through synchronized display port and image |
US6445364B2 (en) * | 1995-11-28 | 2002-09-03 | Vega Vista, Inc. | Portable game display and method for controlling same |
EP1116211A4 (en) * | 1998-09-22 | 2001-11-21 | Vega Vista Inc | Intuitive control of portable data displays |
CN1950863A (en) * | 2004-05-07 | 2007-04-18 | 株式会社日本耐美得 | Portable navigation terminal and program |
Also Published As
Publication number | Publication date |
---|---|
CN104081256B (en) | 2017-10-27 |
US20130139082A1 (en) | 2013-05-30 |
CN104081256A (en) | 2014-10-01 |
US20140258902A1 (en) | 2014-09-11 |
WO2013082034A1 (en) | 2013-06-06 |
CN107608520A (en) | 2018-01-19 |
US20190011982A1 (en) | 2019-01-10 |
US10067559B2 (en) | 2018-09-04 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN107608520B (en) | Graphical interface with adjustable boundaries | |
US8866852B2 (en) | Method and system for input detection | |
US9035878B1 (en) | Input system | |
US9552676B2 (en) | Wearable computer with nearby object response | |
US8643951B1 (en) | Graphical menu and interaction therewith through a viewing window | |
US20130246967A1 (en) | Head-Tracked User Interaction with Graphical Interface | |
US20150143297A1 (en) | Input detection for a head mounted device | |
US10330940B1 (en) | Content display methods | |
US9811154B2 (en) | Methods to pan, zoom, crop, and proportionally move on a head mountable display | |
US8745058B1 (en) | Dynamic data item searching | |
US20130117707A1 (en) | Velocity-Based Triggering | |
US9058054B2 (en) | Image capture apparatus | |
US9195306B2 (en) | Virtual window in head-mountable display | |
US9454288B2 (en) | One-dimensional to two-dimensional list navigation | |
US9448687B1 (en) | Zoomable/translatable browser interface for a head mounted device | |
US20160011724A1 (en) | Hands-Free Selection Using a Ring-Based User-Interface | |
US20150199081A1 (en) | Re-centering a user interface | |
US20130021269A1 (en) | Dynamic Control of an Active Input Region of a User Interface | |
US20130021374A1 (en) | Manipulating And Displaying An Image On A Wearable Computing System | |
US20150185971A1 (en) | Ring-Based User-Interface | |
US9153043B1 (en) | Systems and methods for providing a user interface in a field of view of a media item | |
US9547406B1 (en) | Velocity-based triggering |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |