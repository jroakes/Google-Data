US7676754B2 - Method and program product for resolving ambiguities through fading marks in a user interface - Google Patents
Method and program product for resolving ambiguities through fading marks in a user interface Download PDFInfo
- Publication number
- US7676754B2 US7676754B2 US10/838,444 US83844404A US7676754B2 US 7676754 B2 US7676754 B2 US 7676754B2 US 83844404 A US83844404 A US 83844404A US 7676754 B2 US7676754 B2 US 7676754B2
- Authority
- US
- United States
- Prior art keywords
- user
- input
- ambiguous
- ambiguity
- computer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04883—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures for inputting data by handwriting, e.g. gesture or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/012—Head tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/013—Eye tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/017—Gesture based interaction, e.g. based on a set of recognized hand gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
Definitions
- Our invention relates to a system, method, and program product where a user interaction is interpreted and used by a computer system to interactively access assistance and information, where the assistance and information is provided according to input ambiguity.
- the input ambiguity is determined by one or more of current conditions, requested operations, the location of a tactile impulse or cursor, or the contents of an audible input.
- the ambiguity may be resolved by initiating the resolved requested action, presenting help screens or menus to the user, warning the user, or reminding the user to take an action.
- Multimodal interfaces are becoming increasingly popular in many areas. Multimodal interfaces are finding utility in global positioning satellite navigation systems, diagnostic systems for automobiles, distance learning, web browsing, voting, and machine and process control. One characteristic of multimodal interfaces is the recognition and interpretation of multimodal user inputs, such as audible inputs and tactile inputs.
- a further problem with tactile multimodal interfaces is how to deal with ambiguous user actions and recognition of complex (and potentially ambiguous) user actions.
- One solution described, for example, in U.S. Pat. No. 6,587,818 to Kanevsky et al. for System And Method For Resolving Decoding Ambiguity Via Dialog, interrupts the process and asks the user questions.
- this method is not altogether acceptable in multimodal applications. This is because many features can be simultaneously confusing, and many questions may be necessary to resolve the ambiguity.
- GPSS Global Positioning Satellite System
- a user touches a touchscreen or clicks a mouse
- asks for directions from “Point A” to “Point B” ambiguity can arise because the system may not be able to determine exactly what the user pointed to.
- the same ambiguity can also arise in a voice recognition system, especially if the system is unsure what the user said.
- the problem can also become more acute if the user must identify a plurality of points, as in the query “What is the distance from Point A to Point B via Point C?”, or where there the user is potentially distracted, as in programming or interrogating a Global Positioning Satellite navigation system while driving.
- a clear need also exists to provide user-friendly feedback to an ambiguous entry by a distracted user.
- the existing graphical user interfaces are “tri modal” since they designed to be controlled by a keyboard and mouse, and the output is piped trough the monitor. From a user's perspective they are “bi modal.” This is because the user typically interacts with the computer by using hands and vision. Users with vision disabilities may use text to speech or BRAIL system as a way to receive the computer output.
- GUI controls static text, edit box, list box, check box, scrollbar, table, and views have evolved over years within the context of systems such as Motif, Mac OS, Windows, Linux and Web browsers such as NetScape, Mozilla and Internet Explorer. Most of these controls were adjusted to “tri modal” user interaction.
- the keyboard and mouse actions are transferred into GUI actions, when user can often can see how they being reinterpreted into the output stream, that is, highlights, selections, visual appearance of the windows, changed text and so on.
- the user input when an ambiguous user input is received, the user input is displayed, typically uniquely displayed, that is, the user input is displayed in a way that either the ambiguity is resolved or the ambiguity is indicated to the user by unique and distinctive graphical indications. Then, after waiting either for a time to receive a corrective input from the user or for a predetermined time, the display of the user input is faded.
- the ambiguity may be in the user intention, or in the computer's interpretation of user input, and the resolution of ambiguity may be as simple and straight forward as increasing user perceptual satisfaction.
- the system, method, and program product described herein receives and interprets a user input interaction.
- the ambiguity in the user input is identified by the system and thereafter interpreted and resolved and used by the computer system to the extent necessary to interactively access assistance and information.
- the system provides assistance or an intermediate output according to the nature and extent of input ambiguity.
- the input ambiguity is determined by one or more of current conditions, requested operations, the location of a tactile impulse or cursor, or the contents of an audible input.
- the system resolves the ambiguity by resolving and initiating the requested action, or by presenting help screens or menus to the user, or by warning the user, or by reminding the user to take an action.
- the user interface requirements lead to architecture requirements.
- the architecture requirements lead to a clean role division between the modules, straight forward development, and reusability of the code and scalability in relation to tasks and modalities. The last requirement is worth to elaborate on.
- the method, system, and program product described herein below provides an end user experience that is experienced when the end user uses a map on a touch screen device saying “Plot route from here (touch or point to or point in proximity to) to there (touch or point to or point in proximity to).” These actions may be asynchronous, where the “here” and later “there” touches are not necessarily located in the same point on the time scale as the surrounding words or other user inputs. According to our invention this input is responded to by reasonable output behavior.
- FIG. 1 illustrates the sensors that receive multimodal input, for example, a microphone and touch screen, although a camera or head mounted point or camera or the like could also provide multimodal input. Also shown is an associated multimodal ambiguity resolver, and output elements as a display and a speaker.
- multimodal input for example, a microphone and touch screen
- a camera or head mounted point or camera or the like could also provide multimodal input.
- an associated multimodal ambiguity resolver and output elements as a display and a speaker.
- FIG. 2 illustrates a flow chart of the method, system, and program product of the invention.
- FIG. 3 illustrates a detailed flow chart of element 203 of FIG. 2 illustrating one example of detection of input ambiguities.
- FIG. 4 illustrates a detailed flow chart of element 203 a of FIG. 2 illustrating the method of classification of detected ambiguities.
- FIG. 5 illustrates a detailed flow chart of module 203 b of FIG. 2 for transforming tactile and voice inputs into visual and voice marks.
- FIG. 6 illustrates a detailed flow chart of module 207 of FIG. 2 for modifying and displaying tactile and voice marks.
- FIG. 7 illustrates a detailed flow chart of one example of the methodology of the method, system, and program product of the invention.
- FIG. 8 illustrates one example of the invention where circles indicate where the user touched a screen in a sequence.
- the circles fade or disappear, and therefore their wideness decreases in sequence.
- the user input is displayed, typically uniquely displayed, that is, the user input is displayed in a way that either the ambiguity is resolved or the ambiguity is indicated to the user by unique and distinctive graphical indications. Then, after waiting either for a time to receive a corrective input from the user or for a predetermined time, the display of the user input is faded.
- the system, method, and program product described herein receives and interprets a user input interaction.
- the ambiguity in the user input is identified by the system and thereafter interpreted and resolved and used by the computer system to the extent necessary to interactively access assistance and information.
- the system provides assistance or an intermediate output according to the nature and extent of input ambiguity.
- the input ambiguity is determined by one or more of current conditions, requested operations, the location of a tactile impulse or cursor, or the contents of an audible input.
- the system resolves the ambiguity by resolving and initiating the requested action, or by presenting help screens or menus to the user, or by warning the user, or by reminding the user to take an action.
- the resolution of ambiguity may allow a user to interactively access assistance or information regarding an underlying application or operation.
- This assistance may be in the form of menus, opportunities for further user input, a metaphorical graphical image, or a help screen or page.
- the assistance or information provided may be adjusted and presented according to determined user proficiency or by current conditions, such as a warning or reminder to a user about an input event, or the position of a graphical location of a user controlled input tool, or a desired operation. By this expedient the user may be shown a pattern to follow in order to achieve a desired result.
- Touches, and pointing in proximity to (but spaced from the touch screen) also have a lifespan and behavior, which can be indicated visually by fading marks
- the user inputs may be tactile (as a touch screen), a mouse pointer, eye, head, or body movements, or audible, and the output may be audio data or video data or both audio and video data, with audio as well as visual interaction between a user and an operator interface. This is the situation where an audio input is used to interact with the interface, as in a voice controlled GUI.
- a trace that is, a glowing point or small circle
- the trace can glow, that is, remain illuminated, for a few seconds, and then disappear. This allows the user to check whether he or she indicated the right point on the screen.
- auditory information can be presented to the user for confirmation, with prosodic changes (that is, changes in, for example, intonation with a rising or falling pitch pattern, stress with a change in where the main accent occurs, and rhythm including the grouping of words).
- the trace and the prosodic change is especially useful where the user's ability to touch a specific precise point on a touch screen is compromised.
- a distracted user such as a driver entering or retrieving information from a Global Positioning Satellite Navigation Terminal or an Automotive Data Bus Terminal, can attempt an entry and continue driving.
- the entry remains on the screen and the screen does not immediately advance to the next screen. Instead, the entry remains bright, and then, after a time interval, fades before the screen advances to the next screen.
- the user can simultaneously point to two locations on a screen displayed map, wait for confirmation, such as a fading circle, and then enter a question or query, such as “What is the fastest route” or “what is the most direct route” or “how can I go from A to B via C?”
- the color or brightness of marker can indicate the confidence level in the user's entry, especially an audible entry.
- a marker as a circle, can be red for high confidence or pink for lower confidence.
- the visual confirmation of the tactile information provides memory enhancement and reinforcement. This is particularly important for users who are performing multiple tasks or who have cognitive or memory loss.
- the duration of the visual confirmation can be a function of prior knowledge about the user's attention span.
- FIG. 1 illustrates sensors that can receive multimodal input, for example, a microphone 100 , and a touch screen 101 . While a microphone and touch screen are illustrated, it is to be understood that the multimodal input may be gestures including pointing short of a touch, a headset mouse pointer that moves a screen cursor on the screen as a function of head movement, eye gestures or movements, and the input sensors may be cameras, image capture devices, motion detectors, proximity detectors, and the like. In the case of audible inputs, the inputs may be voice, speech reading, even something as simple as clicking of teeth, The multimodal input enters block 102 , the user multimodal ambiguity resolver.
- the inputs may be voice, speech reading, even something as simple as clicking of teeth.
- the ambiguity resolver 102 allows ambiguity in how a user enters input into the system and resolves the ambiguity.
- ambiguous input the intentions of the user may represent one such possibility. For example, if the user touched a spot on the screen near a display point designating a city (A), but nearby is another city (B), it may be confusing for the system to recognize the input (city A or B) desired by the user.
- Ambiguity may also exist when a system is trying to differentiate between multimodal inputs. For example, a user may touch a spot on a display screen and give the verbal command, “Show me this,” intending for the system to display that which he selected on the touch screen. If the user selected some attraction, like a theme park, then the system must decide what to display about the theme park, a long list of details versus a brief summary of information on the rides. This is an illustrative example of when a system must decide on how to perceive the commands of multimodal input.
- Ambiguity may also exist in how a computer interprets the intentions of a user. As a general rule computers must interpret user commands quite literally.
- Ambiguity in computer action is illustrated in the following example. If a user gives the computer a verbal command and the computer must take some time to decode the user's command (several seconds), but the user does not understand why there is a delay in response The delay may be due to the computer decoding the initial command or the system searching for the relevant information requested by the user. Nevertheless, the user may incorrectly assume that the computer does not want to reply or has already replied (when in reality the computer is still preparing the response), and begin another command or repeat the previous command again. These extraneous commands may seriously distract or interrupt the computer processing creating yet another ambiguity. In short, there are many times during a human-computer interaction when there is a possibility for one to misunderstand the other due to ambiguous events.
- a computer system should be capable of ensuring user satisfaction corresponding to the interaction occurring with the computer. For example, when a user touches a button, it is clear to the user that he/she has provided input and has executed a command because the button depresses. But, a user may not have the same feeling of satisfaction after having provided a tactile input on a touch screen. This is because the user did not perceive the command mechanism occurring as blatantly as it occurs with the depression of a button. This illustrates yet another example of potential ambiguity that may be result from the user interface.
- Block 102 the user multimodal ambiguity resolver, assesses ambiguities, and depending on how it assesses ambiguities, it performs specific interactions with the user via Block 103 through either a display 104 , or a speaker 105 .
- a detailed description of the user multimodal ambiguity resolver is found in FIG. 2 .
- modules 100 and 101 represent tactile and voice sensors, respectively.
- Module 200 represents the data input receiver that is responsible for receiving the multimodal data provided by the user.
- Module 203 detects if there is ambiguity of input, to be explained herein below. Once ambiguity has been detected, module 203 a is responsible for classifying the type of ambiguity. There are four types of ambiguity—user intentions, computer interpretation of user intentions, user perceptions of computer actions, user satisfaction relating to the interaction with the computer. Once the ambiguity has been classified it enters module 203 b.
- Module 203 b is a transformer of tactile/spoken input into visual/voice marks. The user receives feedback about his/her input so that he/she can better understand how his/her actions have affected the computer and how the computer is responding to the user's input.
- Module 204 is a displayer of marks.
- the computer gives the user feedback via some method. For example, when the user touches the screen, the computer can display a small circle around the area touched by the user, which fades after some time. Another option is that the computer makes a beeping sound when the user touches the screen. Alternatively, a user may request directions from one city to another on a displayed map and the computer can encircle each city selected by the user, allowing the circles to fade as the directions become available.
- Module 205 is the user corrective response detector. This module allows the user some period of time to recognize a potential error in their input and to change their request (for example, a user may have selected the wrong city and realizes this when the incorrect city is encircled, thereby allowing the user several seconds to select the correct city). This same module can function to alert the user to some other ambiguity in input allowing the user an opportunity to modify the ambiguity.
- the clocker 208 provides an allotted time frame for the user's corrective response. If the allotted time frame elapses without additional input from the user, the system understands that the user has accepted the displayed feedback to be a correct representation of initial input.
- Module 206 is the user corrective response interpreter that functions to assess any corrective input provided by the user.
- Module 207 is the modifier of tactile and voice displayed marks. This module functions to remove the displayed marks once the system understands that the user has no corrective response and is satisfied with the original input. On the other hand, this module may also enhance the markings to draw the user's attention to the selection.
- FIG. 3 illustrates module 203 , the detector of input ambiguity.
- Module 300 represents the user's intention understanding.
- Module 301 is the computer interpreter of user actions.
- the user intention understanding module 300 may employ many methods to understand a user's intentions. For example, it may use semantic interpretation of user actions as shown in module 401 of FIG. 4 .
- Semantic interpretation of a user's intentions is known. For example, if a user touches two consecutive points on a display screen (i.e. two cities), the system makes the assumption that there is some connection between the two points—the user may want direction from one point to another, or the user may want some other information about the two points. Depending on the phrase used by the user, the system can understand the commands it receives via multimodal input. There are existing references on this subject.
- the computer interpreter of user action 301 can utilize the semantic interpreter of user actions as shown in module 401 of FIG. 4 .
- Module 302 represents the confidence score identifier that is connected to the semantic interpreter of user actions 401 .
- Module 303 characterizes user perceptions of computer actions. Typically, 303 utilizes a variety of user biometrics—voice, facial expressions, etc. that facilitates the understanding of a user's satisfaction based on vocal cues and facial expressions. This information enters module 304 responsible for assessing user satisfaction level.
- module 305 All of this information from modules 300 , 301 , 302 and 304 , is forwarded to module 305 , as ambiguity data that is then sent to module 203 a ( FIG. 2 ) classifier of ambiguity.
- FIG. 4 illustrates a method for classification of detected ambiguity, as illustrated in block 203 a of FIG. 2 .
- Module 400 contains a satisfaction level/confidence score and threshold comparator for different types of ambiguity data. This module compares the various data and sends it to module 402 —ambiguity metrics.
- Module 402 utilizes ambiguity metrics (probability metrics) and some classification theory to classify the varying ambiguities according to—satisfaction, space (i.e. where the user pressed on the display screen), time (i.e. the user was unsure as to whether the system was still processing the initial request), direction (i.e. the direction of the user's gaze or user's position versus a command referring to another direction), semantic (from module 401 ).
- Module 401 provides a semantic interpretation of user actions and phrases to module 400 .
- FIG. 5 illustrates module 203 b , which is the transformer of tactile/spoken input into visual/voice marks.
- Module 500 is a detector of tactile input.
- Module 501 is a classifier of tactile input. Tactile input is being used just as an example in this scenario, it may be replaced with other forms of input—i.e. voice.
- the classifier of tactile input 501 is connected to block 502 .
- Block 502 contains a variety of characteristics for tactile input—size, pressure intensity, time sequence (i.e. several consequent tactile inputs may result if a user selects several cities in a row), location near meaningful objects (where the tactile input was located on a map, for example, if it was near an important location, the system would prevent a mark from obscuring important information like the name of a City, the system would allow a semicircle instead of a full circle to depict the user's selection but not obscure other information.), information about user attention and satisfaction received from other modules as well as via camera feed.
- the system may recognize if the user's attention is slightly misdirected from the display and have the marker encircling the user's selection be pulsating, larger, and brighter, versus if the user is clearly focused on the display screen, the marker encircling the user's selection may be small and discrete.
- the information from block 502 serves as an index for module 503 , the Table of possible marks and their behaviors. Depending on different information about the user, his behavior, semantic objects, and how the computer understands the user's posture and satisfaction, the system will decide how the marks should be displayed (i.e. circle, square, small, large, pulsating, static, bright, dull, etc.). This information enters the classifier of tactile input 501 , which then forwards its output to module 504 ( 204 ) the graphical adapter to device/screen. Depending on the modality of display that is being utilized by the system (i.e. PDA versus laptop versus desktop versus cellular phone), the graphical adapter presents the appropriate display with the correct graphic data to the user's satisfaction.
- the modality of display that is being utilized by the system (i.e. PDA versus laptop versus desktop versus cellular phone)
- the graphical adapter presents the appropriate display with the correct graphic data to the user's satisfaction.
- FIG. 6 illustrates module 207 , which is the modifier of tactile/voice displayed marks.
- Module 600 interprets user response. The system must understand the user's reaction to the displayed tactile marks—Has the user noticed the displayed marks? Has the user made any corrections once he saw that the original selection he made was incorrect? Has the user repeated tactile input? Has the user provided tactile input in another location? Depending on how module 600 interprets the user's response, the action manager 601 decides the next course of action for the system.
- the action manager may again connect to 602 ( 203 a ) where the ambiguities are classified, to 603 ( 203 b ) where new tactile or spoken input is transformed, or to 604 that is a table of transformation of behaviors that indicate how the system should react to various user responses (i.e. display a new mark, stop the program, continue the program, etc.).
- FIG. 7 illustrates the basic methodology of the invention.
- Module 701 receives tactile input.
- Module 701 determines if there is any ambiguity in the user's input. If no, 703 , then the system proceeds with interpretation and execution of the user's command(s). If yes, 702 , then the system must classify the ambiguity—Is this ambiguity in the user's intention 705 ? If yes, 704 , then bring the ambiguity to the user's attention. If no, 706 , is this ambiguity in the machine's performance? If yes, 707 , then represent ambiguity to the user as machine performance. If no, 709 , is the user unsatisfied? If yes, 710 , then represent different feedback.
- Modules 710 , 704 and 707 are all connected to module 708 that waits for the user's feedback.
- Module 709 is the user unsatisfied, is connected to module 711 that asks the user for more feedback. The system then repeats the cycle again by receiving tactile or voice input.
- FIG. 8 illustrates one example of the invention. As shown in the Figure the circles are illustrated with decreasing circumference line thickness. This may indicate where the user has touched a screen in a sequence. The circles fade or disappear, and therefore their wideness decreases in sequence.
- the tactile input means is typically chosen from the group consisting of a touch screen sensor or a mouse pointer, where the user's tactile input is touching a mouse, or display.
- the method, system, and program product displays the user interactions. That is, when the user first touches the mouse or the touch screen, the computer reveals the mouse cursor and performs a quick animation of a radius circle collapsing on the mouse cursor position.
- the quick animation of the radius circle helps to direct the user's attention to the focus of interaction, that is, when the user grabs or releases mouse or touches or pulls away from the touch screen.
- One illustration of this quick animation is stretching the map with a mouse or touch.
- a user first positions the cursor around an area of interest, where clicking and dragging the mouse and touchpad stretches the map. In this regard a dragging rectangle helps to show the area of the map that has been “grabbed.”
- the map updates, filling in additional detail.
- the map fades in some period of animation to reduce any visual discontinuities.
- deistic gestures When deistic gestures are produced on a touch screen, they can take forms that can lead to several sorts of ambiguities. Considering that the resolution of a multimodal reference requires the identification of the referents and the context (“reference domain”) from which these referents are extracted, that is, linguistics, gestures, and visual clues, the dialogue system may expand to comprehend the referring intention.
- the system may explore the links between words, gestures and perceptual groups, doing so in terms of the clues that delimits the reference domain.
- This resolution of ambiguity is a function of referential gestures, with integration of speech and gesture and referent and context identification.
- a method, system, and program product for implementing an on-demand user interface.
- the interface uses at least one input device capable of detecting touch.
- a sensed touch transition reflective of a user then making or breaking contact with the device such by touching the device with a finger of a non-preferred hand or lifting his(her) finger from the device, causes a sheet or screen to be displayed or dismissed.
- these detected transitions preferably initiate corresponding predefined animation sequences that occur over preset time intervals in which the display, for example, a Tool Glass sheet, either begins to fade into view as soon as user contact begins and then begins to fade out from view as soon as user contact ends.
- Such touch sensing can readily be used to provide “on-demand” display and dismissal of substantially any display widget, e.g., a toolbar, based on sensed contact between each hand of a user and a corresponding input device, such as between a preferred hand and a touch sensitive mouse.
- display clutter can be reduced and displayed application screen area increased at appropriate times during program execution consistent with user action and without imposing any significant cognitive burden on the user to do so; thereby, advantageously improving a “user experience”.
- the invention may be implemented, for example, by having the display application as a software application (as an operating system element), a dedicated processor, or a dedicated processor with dedicated code.
- the computer including its graphical user interface and user input/output applications, executes a sequence of machine-readable instructions, which can also be referred to as code. These instructions may reside in various types of signal-bearing media.
- one aspect of the present invention concerns a program product, comprising a signal-bearing media tangibly embodying a program of machine-readable instructions executable by a digital processing apparatus to perform the steps of receiving user inputs, either by touch screen, audible inputs, or pointers, identifying actual and potential ambiguities, indicating these actual and potential ambiguities to an end user, and after allowing the user time to resolve the ambiguities fading the indications associated with the ambiguities.
- This signal-bearing medium may comprise, for example, memory in server.
- the memory in the server may be non-volatile storage, a data disc, or even memory on a vendor server for downloading to a processor for installation.
- the instructions may be embodied in a signal-bearing medium such as the optical data storage disc.
- the instructions may be stored on any of a variety of machine-readable data storage mediums or media, which may include, for example, a “hard drive”, a RAID array, a RAMAC, a magnetic data storage diskette (such as a floppy disk), magnetic tape, digital optical tape, RAM, ROM, EPROM, EEPROM, flash memory, magneto-optical storage, paper punch cards, or any other suitable storage media.
- the machine-readable instructions may comprise software object code, compiled from a language such as “C++”.
- program code may, for example, be compressed, encrypted, or both, and may include executable files, script files and wizards for installation, as in Zip files and cab files.
Abstract
Description
-
- Speech recognition is inaccurate—and it is necessary to resolve recognition errors?
- Speech and keyboard/mouse input (speech and hands input from user's perspective) are asynchronous by nature; and the issue is how to provide reasonable behavior in situations when the combination speech and hand input:
- can be interpreted,
- can only be interpreted with ambiguity,
- can not be interpreted yet (the information is not complete, system is waiting for an additional input), or
- can not be interpreted at all (contradiction in the input data, the input data are incomplete, expiration of the original data or request).
-
- Objects: Stage, Requests, Touch;
- Stage—contains Requests and Touches; and
- Requests—where behavior differs by type.
Claims (33)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/838,444 US7676754B2 (en) | 2004-05-04 | 2004-05-04 | Method and program product for resolving ambiguities through fading marks in a user interface |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/838,444 US7676754B2 (en) | 2004-05-04 | 2004-05-04 | Method and program product for resolving ambiguities through fading marks in a user interface |
Publications (2)
Publication Number | Publication Date |
---|---|
US20050251746A1 US20050251746A1 (en) | 2005-11-10 |
US7676754B2 true US7676754B2 (en) | 2010-03-09 |
Family
ID=35240761
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/838,444 Active 2027-10-11 US7676754B2 (en) | 2004-05-04 | 2004-05-04 | Method and program product for resolving ambiguities through fading marks in a user interface |
Country Status (1)
Country | Link |
---|---|
US (1) | US7676754B2 (en) |
Cited By (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080088468A1 (en) * | 2006-10-16 | 2008-04-17 | Samsung Electronics Co., Ltd. | Universal input device |
US20100188423A1 (en) * | 2009-01-28 | 2010-07-29 | Tetsuo Ikeda | Information processing apparatus and display control method |
US20110106367A1 (en) * | 2009-10-30 | 2011-05-05 | Denso Corporation | In-vehicle apparatus and method for controlling same |
US20110313762A1 (en) * | 2010-06-20 | 2011-12-22 | International Business Machines Corporation | Speech output with confidence indication |
US20140267022A1 (en) * | 2013-03-14 | 2014-09-18 | Samsung Electronics Co ., Ltd. | Input control method and electronic device supporting the same |
US9265458B2 (en) | 2012-12-04 | 2016-02-23 | Sync-Think, Inc. | Application of smooth pursuit cognitive testing paradigms to clinical drug development |
US9342206B1 (en) * | 2008-01-11 | 2016-05-17 | Lockheed Martin Corporation | Fingerprint location indicator |
US9380976B2 (en) | 2013-03-11 | 2016-07-05 | Sync-Think, Inc. | Optical neuroinformatics |
US10783901B2 (en) * | 2018-12-10 | 2020-09-22 | Amazon Technologies, Inc. | Alternate response generation |
US10902220B2 (en) | 2019-04-12 | 2021-01-26 | The Toronto-Dominion Bank | Systems and methods of generating responses associated with natural language input |
Families Citing this family (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8213917B2 (en) | 2006-05-05 | 2012-07-03 | Waloomba Tech Ltd., L.L.C. | Reusable multimodal application |
JP2006331406A (en) * | 2005-04-25 | 2006-12-07 | Canon Inc | Information processing apparatus and method |
US20080028324A1 (en) * | 2006-03-07 | 2008-01-31 | Marengo Intellectual Property Ltd. | Multi-applicaton bulletin board |
US20080022224A1 (en) * | 2006-03-07 | 2008-01-24 | Marengo Intellectual Property Ltd. | Pushed and pulled information display on a computing device |
US20070214430A1 (en) * | 2006-03-07 | 2007-09-13 | Coutts Daryl D | Textpane for pushed and pulled information on a computing device |
KR20090003035A (en) * | 2006-12-04 | 2009-01-09 | 한국전자통신연구원 | Apparatus and method for encoding the five senses information, system and method for providing realistic service using five senses integration interface |
EP1953630A1 (en) * | 2007-02-05 | 2008-08-06 | Research In Motion Limited | Method and System for Cueing Panning |
US20080189650A1 (en) * | 2007-02-05 | 2008-08-07 | Sherryl Lee Lorraine Scott | Method and system for cueing panning |
WO2010006087A1 (en) * | 2008-07-08 | 2010-01-14 | David Seaberg | Process for providing and editing instructions, data, data structures, and algorithms in a computer system |
US20100083109A1 (en) * | 2008-09-29 | 2010-04-01 | Smart Technologies Ulc | Method for handling interactions with multiple users of an interactive input system, and interactive input system executing the method |
US20100281435A1 (en) * | 2009-04-30 | 2010-11-04 | At&T Intellectual Property I, L.P. | System and method for multimodal interaction using robust gesture processing |
JP2011253374A (en) * | 2010-06-02 | 2011-12-15 | Sony Corp | Information processing device, information processing method and program |
US20120245927A1 (en) * | 2011-03-21 | 2012-09-27 | On Semiconductor Trading Ltd. | System and method for monaural audio processing based preserving speech information |
US9263045B2 (en) * | 2011-05-17 | 2016-02-16 | Microsoft Technology Licensing, Llc | Multi-mode text input |
US8255218B1 (en) | 2011-09-26 | 2012-08-28 | Google Inc. | Directing dictation into input fields |
US9258653B2 (en) | 2012-03-21 | 2016-02-09 | Semiconductor Components Industries, Llc | Method and system for parameter based adaptation of clock speeds to listening devices and audio applications |
JP5945926B2 (en) * | 2012-03-26 | 2016-07-05 | コニカミノルタ株式会社 | Operation display device |
US8543397B1 (en) | 2012-10-11 | 2013-09-24 | Google Inc. | Mobile device voice activation |
KR102203810B1 (en) * | 2013-10-01 | 2021-01-15 | 삼성전자주식회사 | User interfacing apparatus and method using an event corresponding a user input |
US9792272B2 (en) * | 2013-12-31 | 2017-10-17 | Barnes & Noble College Booksellers, Llc | Deleting annotations of paginated digital content |
US10331777B2 (en) | 2013-12-31 | 2019-06-25 | Barnes & Noble College Booksellers, Llc | Merging annotations of paginated digital content |
DE102014202834A1 (en) * | 2014-02-17 | 2015-09-03 | Volkswagen Aktiengesellschaft | User interface and method for contactless operation of a hardware-designed control element in a 3D gesture mode |
US9377894B2 (en) | 2014-05-22 | 2016-06-28 | Sony Corporation | Selective turning off/dimming of touch screen display region |
DE102015215044A1 (en) * | 2015-08-06 | 2017-02-09 | Volkswagen Aktiengesellschaft | Method and system for processing multimodal input signals |
US10755027B2 (en) * | 2016-01-19 | 2020-08-25 | Lenovo (Singapore) Pte Ltd | Gesture ambiguity determination and resolution |
Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5265014A (en) * | 1990-04-10 | 1993-11-23 | Hewlett-Packard Company | Multi-modal user interface |
US5748974A (en) * | 1994-12-13 | 1998-05-05 | International Business Machines Corporation | Multimodal natural language interface for cross-application tasks |
US5825356A (en) | 1996-03-18 | 1998-10-20 | Wall Data Incorporated | Help system with semitransparent window for disabling controls |
US5877750A (en) | 1996-09-17 | 1999-03-02 | International Business Machines Corporation | Method and apparatus for in-place line width selection for graphics applications |
US5953541A (en) * | 1997-01-24 | 1999-09-14 | Tegic Communications, Inc. | Disambiguating system for disambiguating ambiguous input sequences by displaying objects associated with the generated input sequences in the order of decreasing frequency of use |
US6333753B1 (en) * | 1998-09-14 | 2001-12-25 | Microsoft Corporation | Technique for implementing an on-demand display widget through controlled fading initiated by user contact with a touch sensitive input device |
US6333752B1 (en) | 1998-03-13 | 2001-12-25 | Ricoh Company, Ltd. | Image processing apparatus, image processing method, and a computer-readable storage medium containing a computer program for image processing recorded thereon |
US6484094B1 (en) | 2002-02-19 | 2002-11-19 | Alpine Electronics, Inc. | Display method and apparatus for navigation system |
US6587818B2 (en) | 1999-10-28 | 2003-07-01 | International Business Machines Corporation | System and method for resolving decoding ambiguity via dialog |
US6779060B1 (en) * | 1998-08-05 | 2004-08-17 | British Telecommunications Public Limited Company | Multimodal user interface |
US20040193428A1 (en) * | 1999-05-12 | 2004-09-30 | Renate Fruchter | Concurrent voice to text and sketch processing with synchronized replay |
US20040263487A1 (en) * | 2003-06-30 | 2004-12-30 | Eddy Mayoraz | Application-independent text entry for touch-sensitive display |
US6964023B2 (en) * | 2001-02-05 | 2005-11-08 | International Business Machines Corporation | System and method for multi-modal focus detection, referential ambiguity resolution and mood classification using multi-modal input |
-
2004
- 2004-05-04 US US10/838,444 patent/US7676754B2/en active Active
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5265014A (en) * | 1990-04-10 | 1993-11-23 | Hewlett-Packard Company | Multi-modal user interface |
US5748974A (en) * | 1994-12-13 | 1998-05-05 | International Business Machines Corporation | Multimodal natural language interface for cross-application tasks |
US5825356A (en) | 1996-03-18 | 1998-10-20 | Wall Data Incorporated | Help system with semitransparent window for disabling controls |
US5877750A (en) | 1996-09-17 | 1999-03-02 | International Business Machines Corporation | Method and apparatus for in-place line width selection for graphics applications |
US5953541A (en) * | 1997-01-24 | 1999-09-14 | Tegic Communications, Inc. | Disambiguating system for disambiguating ambiguous input sequences by displaying objects associated with the generated input sequences in the order of decreasing frequency of use |
US6333752B1 (en) | 1998-03-13 | 2001-12-25 | Ricoh Company, Ltd. | Image processing apparatus, image processing method, and a computer-readable storage medium containing a computer program for image processing recorded thereon |
US6779060B1 (en) * | 1998-08-05 | 2004-08-17 | British Telecommunications Public Limited Company | Multimodal user interface |
US6333753B1 (en) * | 1998-09-14 | 2001-12-25 | Microsoft Corporation | Technique for implementing an on-demand display widget through controlled fading initiated by user contact with a touch sensitive input device |
US20040193428A1 (en) * | 1999-05-12 | 2004-09-30 | Renate Fruchter | Concurrent voice to text and sketch processing with synchronized replay |
US6587818B2 (en) | 1999-10-28 | 2003-07-01 | International Business Machines Corporation | System and method for resolving decoding ambiguity via dialog |
US6964023B2 (en) * | 2001-02-05 | 2005-11-08 | International Business Machines Corporation | System and method for multi-modal focus detection, referential ambiguity resolution and mood classification using multi-modal input |
US6484094B1 (en) | 2002-02-19 | 2002-11-19 | Alpine Electronics, Inc. | Display method and apparatus for navigation system |
US20040263487A1 (en) * | 2003-06-30 | 2004-12-30 | Eddy Mayoraz | Application-independent text entry for touch-sensitive display |
Non-Patent Citations (9)
Title |
---|
Bier, Stone, Fishkin, Buxton, Baudel, A Taxonomy of See-Through Tools; http://www2.parc.com/istl/projects/MagicLenses/Taxonomy. |
Bier. Stone, Pier, Buxton, and DeRose, Toolglass and Magic Lenses: The See Through Interface, http://www2.parc.com/istl/projects/MagicLenses/93Siggraph.htm. |
Buxton, Fitzmaurice, Balakrishnan, and Kurtenbach; Large Displays In Automotive Design; IEEE Graphics and Applications (Jul./Aug. 2000), on. 68-75. |
K. Hinckley, M. Czerwinski, M. Sinclair, Interaction and Modeling Techniques for Desktop Two-Handed Input; Proceedings of the ACM UIST'98, Symposium on User Interface Software and Technology, p. 49-58. |
Landragin, The Role of Gesture In Multimodal Referring Actions, Proceedings of the Fourth IEEE Conference On Multimodal Interfaces, (ICMI'02), 0-7695-1834-6/02. |
Mountfocus Keyboard Design, http://www.newfreeware.com/utils/663. |
On-Line Vending Machine and Catalog Product Icons, IBM Technical Disclosure Bulletin, vol. 38., No. 04, Aug. 1995, pp. 113-115. |
SM Button ActiveX, http://www.station-media.com/smbutton. |
SM Button, http://www.newfreeware.com/development/419. |
Cited By (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8502769B2 (en) * | 2006-10-16 | 2013-08-06 | Samsung Electronics Co., Ltd. | Universal input device |
US20080088468A1 (en) * | 2006-10-16 | 2008-04-17 | Samsung Electronics Co., Ltd. | Universal input device |
US9342206B1 (en) * | 2008-01-11 | 2016-05-17 | Lockheed Martin Corporation | Fingerprint location indicator |
US20100188423A1 (en) * | 2009-01-28 | 2010-07-29 | Tetsuo Ikeda | Information processing apparatus and display control method |
US8711182B2 (en) * | 2009-01-28 | 2014-04-29 | Sony Corporation | Information processing apparatus and display control method |
US20110106367A1 (en) * | 2009-10-30 | 2011-05-05 | Denso Corporation | In-vehicle apparatus and method for controlling same |
US8527144B2 (en) * | 2009-10-30 | 2013-09-03 | Denso Corporation | In-vehicle apparatus and method for controlling same |
US20110313762A1 (en) * | 2010-06-20 | 2011-12-22 | International Business Machines Corporation | Speech output with confidence indication |
US20130041669A1 (en) * | 2010-06-20 | 2013-02-14 | International Business Machines Corporation | Speech output with confidence indication |
US9265458B2 (en) | 2012-12-04 | 2016-02-23 | Sync-Think, Inc. | Application of smooth pursuit cognitive testing paradigms to clinical drug development |
US9380976B2 (en) | 2013-03-11 | 2016-07-05 | Sync-Think, Inc. | Optical neuroinformatics |
US20140267022A1 (en) * | 2013-03-14 | 2014-09-18 | Samsung Electronics Co ., Ltd. | Input control method and electronic device supporting the same |
US10783901B2 (en) * | 2018-12-10 | 2020-09-22 | Amazon Technologies, Inc. | Alternate response generation |
US11854573B2 (en) * | 2018-12-10 | 2023-12-26 | Amazon Technologies, Inc. | Alternate response generation |
US10902220B2 (en) | 2019-04-12 | 2021-01-26 | The Toronto-Dominion Bank | Systems and methods of generating responses associated with natural language input |
US11392776B2 (en) | 2019-04-12 | 2022-07-19 | The Toronto-Dominion Bank | Systems and methods of generating responses associated with natural language input |
Also Published As
Publication number | Publication date |
---|---|
US20050251746A1 (en) | 2005-11-10 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US7676754B2 (en) | Method and program product for resolving ambiguities through fading marks in a user interface | |
EP3304543B1 (en) | Device voice control | |
US20180349346A1 (en) | Lattice-based techniques for providing spelling corrections | |
US9176668B2 (en) | User interface for text input and virtual keyboard manipulation | |
US20180275851A1 (en) | Input Device Enhanced Interface | |
US10606474B2 (en) | Touch screen finger tracing device | |
US9229925B2 (en) | Apparatus, method and computer readable medium for a multifunctional interactive dictionary database for referencing polysemous symbol | |
US9691381B2 (en) | Voice command recognition method and related electronic device and computer-readable medium | |
JP2018515817A (en) | How to improve control by combining eye tracking and speech recognition | |
US20090100383A1 (en) | Predictive gesturing in graphical user interface | |
US20120188164A1 (en) | Gesture processing | |
DK201670560A9 (en) | Dictation that allows editing | |
KR101522375B1 (en) | Input method editor | |
RU2004105885A (en) | DYNAMIC FEEDBACK FOR GESTURES | |
CN109891374B (en) | Method and computing device for force-based interaction with digital agents | |
JP6991486B2 (en) | Methods and systems for inserting characters into strings | |
EP2849054A1 (en) | Apparatus and method for selecting a control object by voice recognition | |
EP2897055A1 (en) | Information processing device, information processing method, and program | |
CN110612567A (en) | Low latency intelligent automated assistant | |
TW201512968A (en) | Apparatus and method for generating an event by voice recognition | |
Paudyal et al. | Voiceye: A multimodal inclusive development environment | |
EP3938878A1 (en) | System and method for navigating interfaces using touch gesture inputs | |
CN111696546A (en) | Using a multimodal interface to facilitate discovery of spoken commands | |
Roider | Natural Multimodal Interaction in the Car-Generating Design Support for Speech, Gesture, and Gaze Interaction while Driving | |
Bouzit et al. | Polymodal menus: A model-based approach for designing multimodal adaptive menus for small screens |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: INTERNATIONAL BUSINESS MACHINES CORPORATION, NEW YFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BASSON, SARA;FAISMAN, ALEXANDER;KANEVSKY, DIMITRI;REEL/FRAME:014965/0769Effective date: 20040430Owner name: INTERNATIONAL BUSINESS MACHINES CORPORATION,NEW YOFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BASSON, SARA;FAISMAN, ALEXANDER;KANEVSKY, DIMITRI;REEL/FRAME:014965/0769Effective date: 20040430 |
|
FEPP | Fee payment procedure |
Free format text: PAYOR NUMBER ASSIGNED (ORIGINAL EVENT CODE: ASPN); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:INTERNATIONAL BUSINESS MACHINES CORPORATION;REEL/FRAME:027463/0594Effective date: 20111228 |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552)Year of fee payment: 8 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0610Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 12 |