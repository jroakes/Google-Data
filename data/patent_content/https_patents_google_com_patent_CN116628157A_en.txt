CN116628157A - Parameter collection and automatic dialog generation in dialog systems - Google Patents
Parameter collection and automatic dialog generation in dialog systems Download PDFInfo
- Publication number
- CN116628157A CN116628157A CN202310573301.2A CN202310573301A CN116628157A CN 116628157 A CN116628157 A CN 116628157A CN 202310573301 A CN202310573301 A CN 202310573301A CN 116628157 A CN116628157 A CN 116628157A
- Authority
- CN
- China
- Prior art keywords
- parameters
- dialog
- intent
- dialog system
- values
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000009471 action Effects 0.000 claims abstract description 127
- 238000000034 method Methods 0.000 claims abstract description 90
- 230000000977 initiatory effect Effects 0.000 claims abstract description 46
- 230000004044 response Effects 0.000 claims description 34
- 238000012545 processing Methods 0.000 claims description 27
- 238000004891 communication Methods 0.000 claims description 12
- 230000001960 triggered effect Effects 0.000 claims description 12
- 230000003213 activating effect Effects 0.000 claims 2
- 239000003795 chemical substances by application Substances 0.000 description 33
- 230000008569 process Effects 0.000 description 23
- 238000010801 machine learning Methods 0.000 description 15
- 235000013550 pizza Nutrition 0.000 description 14
- 230000000875 corresponding effect Effects 0.000 description 11
- 238000010586 diagram Methods 0.000 description 7
- 230000006870 function Effects 0.000 description 7
- 230000014509 gene expression Effects 0.000 description 7
- 230000003993 interaction Effects 0.000 description 7
- 238000003058 natural language processing Methods 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 5
- 238000004458 analytical method Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 239000004615 ingredient Substances 0.000 description 3
- 230000007774 longterm Effects 0.000 description 3
- 238000013507 mapping Methods 0.000 description 3
- 230000006855 networking Effects 0.000 description 3
- 238000007781 pre-processing Methods 0.000 description 3
- 230000009118 appropriate response Effects 0.000 description 2
- 238000013528 artificial neural network Methods 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 238000012790 confirmation Methods 0.000 description 2
- 238000012937 correction Methods 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000007474 system interaction Effects 0.000 description 2
- 239000013598 vector Substances 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 235000005956 Cosmos caudatus Nutrition 0.000 description 1
- 244000293323 Cosmos caudatus Species 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000013473 artificial intelligence Methods 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000002131 composite material Substances 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 235000013305 food Nutrition 0.000 description 1
- 238000002372 labelling Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000005065 mining Methods 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 230000000877 morphologic effect Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- ALZOLUNSQWINIR-UHFFFAOYSA-N quinmerac Chemical compound OC(=O)C1=C(Cl)C=CC2=CC(C)=CN=C21 ALZOLUNSQWINIR-UHFFFAOYSA-N 0.000 description 1
- 235000015067 sauces Nutrition 0.000 description 1
- 238000007619 statistical method Methods 0.000 description 1
- 230000001131 transforming effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3344—Query execution using natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M3/00—Automatic or semi-automatic exchanges
- H04M3/42—Systems providing special services or facilities to subscribers
- H04M3/487—Arrangements for providing information services, e.g. recorded voice services or time announcements
- H04M3/493—Interactive information services, e.g. directory enquiries ; Arrangements therefor, e.g. interactive voice response [IVR] systems or voice portals
- H04M3/4936—Speech interaction details
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3605—Destination input or retrieval
- G01C21/3608—Destination input or retrieval using speech input, e.g. using speech recognition
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2201/00—Electronic components, circuits, software, systems or apparatus used in telephone systems
- H04M2201/40—Electronic components, circuits, software, systems or apparatus used in telephone systems using speech recognition
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2203/00—Aspects of automatic or semi-automatic exchanges
- H04M2203/35—Aspects of automatic or semi-automatic exchanges related to information services provided via a voice call
- H04M2203/355—Interactive dialogue design tools, features or methods
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M7/00—Arrangements for interconnection between switching centres
- H04M7/0012—Details of application programming interfaces [API] for telephone networks; Arrangements which combine a telephonic communication equipment and a computer, i.e. computer telephony integration [CPI] arrangements
Abstract
The present application relates to parameter collection and automatic dialog generation in dialog systems. In one example, a method includes: identifying a dialog system intent associated with the voice input based on at least one predetermined intent keyword, the dialog system intent having a desired intent parameter; determining whether data for all desired intent parameters of the dialog system is available; selectively initiating a parameter collection dialog associated with the dialog system intent based on the determination, the parameter collection dialog operable to collect data for desired parameters not available for the dialog system intent; and generating action instructions based on the dialog system intent and one or more desired parameters.
Description
Description of the division
The application belongs to a divisional application of Chinese patent application No.201680061207.7 with the application date of 2016, 10 and 21.
Cross Reference to Related Applications
The present application claims the benefit of U.S. provisional patent application serial No. 62/244,560, entitled "Parameter Collection and Automatic Dialog Generation in Dialog Systems (parameter collection and automatic dialog generation in dialog system)" filed on month 10, 2015, which is incorporated herein by reference in its entirety for all purposes.
Background
Dialog systems are widely used in applications for portable devices. In general, dialog systems include computer-based agents having a human-machine interface for accessing, processing, managing, and communicating information. Dialog systems are also known as chat information systems, spoken dialog systems, conversation agents, chat bots (chat robotics), chat bots (chat bots), chat agents, digital personal assistants, automated online assistants, and the like.
Traditionally, dialog systems have interacted with humans using natural language to simulate intelligent conversations and to provide personalized assistance to users. For example, the user may query the dialog system "What is the weather like in Alexandria today? (what is the weather of today alexander)' and receives answers from the dialog system in the form of audio or text messages. The user may provide voice commands to the dialog system to cause certain operations to be performed, such as generating an email, making a call, searching for information, navigating, setting up notifications or reminders, and the like. These and other functions make dialog systems very popular among users, especially for users of portable electronic devices such as, for example, smart phones and tablet computers.
The dialog system may include a dialog system engine that is responsible for receiving user speech inputs, converting them into text inputs, interpreting the text inputs, generating appropriate responses to the text inputs, and communicating the responses to the user. Interpreting the input and finding the appropriate response may utilize artificial intelligence algorithms. Thus, although the demand for dialog systems is increasing, creating dialog systems is still a complex engineering task.
Disclosure of Invention
The natural language dialog system may maintain dialog with a user and provide intelligent responses or perform a wide range of actions in response to user requests. The user request may be interpreted by the natural language dialog system using a dialog system that "intent" facilitates a mapping between what the user speaks and the actions taken by the natural language dialog system. In some user machine dialog contexts (contexts), a natural language dialog system needs to obtain one or more intent parameters in order to achieve an action for a given intent. For example, when a user requests that the natural language conversation system order a pizza, the natural language conversation system needs to obtain parameters associated with the pizza, such as size, crust type, ingredients, vendor, time of delivery and address.
Examples in accordance with the present disclosure provide systems and methods for collecting intent parameters from a user with a machine-implemented parameter collection dialog mimicking a natural language dialog. It may be desirable to identify a user's intended intent in order to initiate collection of intent parameters. Once the intent parameters are collected, the natural language dialogue system implements a predetermined action associated with the intent (e.g., sending an electronic purchase order to a pizza restaurant) based on the collected parameters.
Examples in accordance with the present disclosure may further provide systems and methods for enabling a software developer to create a dialog agent that is configurable to run parameter collection dialogs and collect intent parameters. In one example disclosed herein, there is provided a method for intent parameter collection, comprising: receiving voice input of a user; identifying a dialog system intent associated with the voice input based on at least one predetermined intent keyword, the dialog system intent having a desired intent parameter; determining whether data for all desired intent parameters of the dialog system is available; selectively initiating a parameter collection dialog associated with the dialog system intent based on the determination, the parameter collection dialog operable to collect data for desired parameters not available for the dialog system intent; and generating an action instruction based on the dialog system intent and one or more desired parameters.
In one example, the method may further comprise: identifying at least one of the desired intent parameters in the voice input; and extracting the at least one of the desired intent parameters from the voice input. The method may further comprise: based on a determination that the voice input includes all missing intent parameters, desired intent parameters are extracted from the voice input without initiating the parameter collection dialog. The parameter collection session may include at least one predetermined prompt.
In one example, the method may further comprise: receiving at least one further voice input of the user in response to the at least one predetermined prompt; and extracting at least one of the desired intent parameters from the at least one additional voice input until all missing intent parameters are collected. The intent parameter may include at least one of: numerical values, words, phrases, sounds, and images. At least one of the intent parameters may be selected from a list of predetermined values.
In one example, the method may further include enabling, by a developer platform, a developer to create a conversation agent of the natural voice conversation system to automatically collect the absent intent parameters, the conversation agent being associated with a developer profile. The method may further comprise: providing, by the developer platform, a graphical interface to enable the developer to: creating the dialog agent; and providing one or more of the following: the dialog system intent, at least one intent parameter, and one or more prompts for the intent parameter. The method may further comprise: enabling, by the developer platform, the developer to specify a dialog system entity or data type for the intent parameter; and enabling, by the developer platform, the developer to specify a value type for each of the intent parameters. The action instructions may be configured to cause a server or user device to implement a predetermined action based on the action instructions and one or more desired intent parameters. The action instructions may include an Application Programming Interface (API) -specific response configured to cause an API service. The method further comprises the steps of: a confirmation message is provided that allows the user to confirm or clarify the action instruction, wherein the confirmation message recites one or more desired intent parameters.
In another example according to the present disclosure, a natural voice conversation system is provided that includes at least one processor and a memory storing processor executable code. The processor may be configured to, when executing the processor executable code, implement the following operations: identifying a dialog system intent associated with the voice input based on at least one predetermined intent keyword, the dialog system intent having all desired intent parameters; determining whether data for all desired intent parameters of the dialog system is available; selectively initiating a parameter collection dialog associated with the dialog system intent based on the determination, the parameter collection dialog operable to collect data for desired parameters not available for the dialog system intent; and generating an action instruction based on the dialog system intent and one or more desired parameters.
In one example, the at least one processor may be further configured to retrieve all missing intent parameters from the voice input without initiating the parameter collection dialog based on a determination that the voice input includes all missing intent parameters. The at least one processor may be further configured to generate at least one predetermined prompt for the parameter collection session. The at least one processor may be further configured to, when executing the processor executable code, perform the following: receiving at least one further voice input of the user in response to the at least one predetermined prompt; and retrieving at least one of the desired intent parameters from the at least one additional voice input until all missing intent parameters are collected.
In one example, at least one processor may be configured to, when executing the processor-executable code, implement the following: a dialog agent of the natural voice dialog system, which is associated with a developer profile, is enabled by a developer platform to be created by the developer to automatically collect desired intent parameters. The at least one processor may be configured to, when executing the processor executable code, perform the following: providing, by the developer platform, a graphical interface to: enabling the developer to create a dialog agent; and providing one or more of the following: the dialog system intent, at least one intent parameter, and one or more prompts for the parameter collection dialog for each of the intent parameters; the developer platform enables the developer to specify dialog system entities or data types for the intent parameters; and the developer platform enabling the developer to specify a value type for each of the intent parameters.
In yet another example according to the present disclosure, there is provided a non-transitory processor-readable medium having instructions stored thereon, which when executed by one or more processors, cause the one or more processors to implement a method for a natural voice conversation system, the method comprising: identifying a dialog system intent associated with the voice input based on at least one predetermined intent keyword, the dialog system intent having a desired intent parameter; determining whether data for all desired intent parameters of the dialog system is available; selectively initiating a parameter collection dialog associated with the dialog system intent based on the determination, the parameter collection dialog operable to collect data for desired parameters not available for the dialog system intent; and generating an action instruction based on the dialog system intent and one or more desired parameters.
This section is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter. The details of one or more examples of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
Embodiments are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings in which like reference numerals refer to similar elements.
FIG. 1 illustrates an environment in which systems and methods for creating a custom dialog system engine for a dialog system interface may be implemented in accordance with some embodiments of the present disclosure.
FIG. 2 is a process flow diagram illustrating a method for creating a custom dialog system engine using a platform and for operating the platform in accordance with an example embodiment.
Fig. 3 illustrates a high-level architecture of a dialog system engine, according to an example embodiment of the present disclosure.
Fig. 4 illustrates a Graphical User Interface (GUI) of a platform interface for creating a new dialog system entity in accordance with an example embodiment of the present disclosure.
Fig. 5 illustrates a GUI of a platform interface for creating new dialog system intents, according to an example embodiment of the present disclosure.
FIG. 6 illustrates a GUI of a platform interface for providing a log for processing requests of a dialog system, according to an example embodiment of the present disclosure;
fig. 7 is a process flow diagram illustrating a method for collecting intent parameters in accordance with an example embodiment of the present disclosure.
FIG. 8 illustrates an example GUI for creating a platform interface for a dialog agent for collecting intent parameters through a parameter collection dialog, according to an example embodiment of the disclosure.
FIG. 9 illustrates an example GUI of a platform interface for defining hints for a dialog agent in accordance with an example embodiment of the disclosure.
FIG. 10 is a high-level block diagram illustrating an example computing device suitable for implementing the methods described herein.
Detailed Description
Example aspects of the present disclosure generally relate to natural language dialog systems (also referred to as "dialog systems" for simplicity) configured to maintain intelligent human-machine interaction. The dialog system may receive voice input from a user, convert the voice input to text input, and process the text input using machine learning, statistics, heuristics, or other suitable algorithms. The results of the processing may include a response message to the user or an action performed by the client device or server. Example actions may include sending an email, making a reservation, setting a notification or reminder, booking a hotel, viewing weather forecast and navigation traffic, and so forth.
The human-machine interaction may be based on dialog system intent, which may include schemes, rules, or mappings between user input and actions to be taken by the dialog system and, in particular, dialog contexts. Dialog system intent may be automatically identified by the dialog system during human-machine interaction by detecting predetermined keywords or phrases in user input. For example, when a user asks the dialog system to subscribe to a hotel in a city, the intent may be identified. In another example, when a user requests that the conversation system send a text message or email to a particular recipient, the intent may be identified. In yet another example, when a user requests that the dialog system order pizza from a restaurant, intent may be identified.
Processing a predetermined dialog system intent can require collecting a wide range of intent parameters. For example, when an intent requires an action to send a text message to a recipient, the intent parameters required and sufficient to perform the intent may include the content of the text message and the name of the recipient. For an example of intent provided to electronically reserve a hotel, the desired parameters may include destination city, room type, arrival date, departure date, and optionally other parameters such as hotel rating, hotel name, and hotel services, among others. Rather than defining multiple intents and linking them via a dialog context, examples disclosed herein provide dialog agents for use in a dialog system such that each dialog agent may be associated with a single dialog system intent and one or more intent parameters of the dialog system intent.
When a particular dialog agent is activated, a parameter collection dialog may be initiated upon detection of dialog system intent in the user's voice input. The parameter collection dialog may detect intent parameters that have been provided by the user or that may be obtained from other sources. If it is determined that certain intent parameters are lost, the parameter collection dialog may provide a prompt message to the user to prompt the user to provide additional voice input. The hint message may be predefined or created as desired and selected to guide the user in providing the missing desired parameters and in some cases additional optional parameters. The dialog system may retrieve the intent parameters from one or more additional voice inputs of the user.
In some examples, some parameters may be obtained from pre-stored data. For example, if the user speaks "call a taxi service to give me a ride from where I am to home (call taxi service, send me home from the current location)", the user's home address may be obtained from memory. When at least all of the parameters required for a particular intent are collected or known, the parameter collection is completed and the dialog system may generate action instructions based on the intent. The action instructions may be based on some necessary and optional intent parameters collected from the user's voice input or obtained elsewhere. The dialog system, client device or server may then execute the action command to provide a dialog response to the user or perform a particular action.
For example, when a user speaks "Please book a hotel for me in PaloAlto (please book me hotel for me in palo alto)", the dialog system may recognize that the dialog system is intended to be "hotel reservation" and initiate a corresponding parameter collection dialog. First, the dialog system may determine that some desired parameters (i.e., that the city is Palo Alto) have been provided or may be obtained from other sources. The dialog system may further determine that other required parameters of the "hotel reservation" intent, such as date of arrival and stay, remain absent. A dialog system may use prompts such as "When would you like to arrive? * (when you want to arrive? (you will live for a few nights. When the user provides an answer, the dialog system may retrieve the intent parameters from the input and continue to provide prompts until all missing desired intent parameters are collected. Further, the dialog system may request that the user confirm that the desired parameters were collected by providing a summary output (such as, for example, "You want me to book a hotel in Palo Alto for you starting next Monday for two lights. Is present at correction.
Examples disclosed herein also relate to a developer platform that enables software developers to create custom dialog system engines and dialog agents, including dialog agents for collecting intent parameters as described above. Typically, the dialog system engine includes a backend service that interfaces with the custom dialog system interface. The dialog system interface may be implemented as at least a portion of a respective software application, mobile application, middleware application, firmware application, website, etc. In other words, the dialog system interface may provide a computer-human interface configured to at least obtain user input and communicate dialog system output to a user.
The dialog system engine may support the dialog system interface by processing user input and generating corresponding responses (or commands). Thus, the dialog system engine and dialog system interface may form a dialog system when interacting with each other. In some examples, a dialog system interface running on or accessed from a user device may be referred to as a "front-end" user interface, while a dialog system engine supporting the operation of the dialog system interface may be referred to as a "back-end" service. In some examples, the interface and engine may include a client-server model, where both communicate via a network connection. In other examples, the dialog system engine and dialog system interface may operate on a single device without requiring a networking connection to a server.
In accordance with examples of the present disclosure, a developer platform may allow a software developer to create a custom dialog system engine capable of supporting a front-end dialog system interface. For example, if a software developer wants to integrate dialog system functionality as an additional feature into a mobile application, the developer can use the platform to create and deploy a custom dialog system engine and link the custom dialog system engine with the mobile application. The mobile application may contain only dialog system interfaces. In this example, the user may activate the dialog system interface by interacting with the mobile application. The user may query the dialog system interface in the form of voice input or text input. Once the user query is received, the dialog system interface may transmit the query, with or without additional preprocessing, to a linked custom dialog system engine that was previously created using the developer platform. The custom dialog system engine may process and interpret the received user query and generate a response to the query based on predetermined rules and settings. The response may then be passed to a dialog system interface for further visual or audio presentation to the user. Alternatively, if the response includes an action instruction (command), the action instruction may be executed or sent to a server, web service, or client device for execution.
The dialog system interface may be run by and/or integrated into a wide range of software applications executable by a user device such as a personal computer or smart phone or remotely on a server or computing cloud resource such that the dialog system is part of a website or web service. As described above, the dialog system engine may be implemented on the same user device as the interface, on a companion device that communicates with the device on which the interface is implemented (such as a mobile phone and a smart watch that communicate via a Bluetooth connection), or on a server so that their functionality can be accessed by the dialog system interface through the internet, a mobile data network, or any other communication network.
FIG. 1 depicts a high-level block diagram of an example system environment 100 that may be suitable for use in practicing examples of the present disclosure. In particular, FIG. 1 illustrates a platform 110 for creating and maintaining a custom dialog system engine. As shown, the platform 110 includes a platform interface 112 for creating a custom dialog system engine and a backend service 114 for maintaining and running the custom dialog system engine 120. In some aspects of the present disclosure, platform 110 comprises a developer platform.
The platform interface 112 may include a Graphical User Interface (GUI) embedded in a web page and accessible by a developer via a network. In some other aspects of the disclosure, the platform interface 112 may be implemented as a software application, including a downloadable software application or any other software, such as middleware or firmware running on or accessible from an electronic device, such as a computer or smart phone. In the example shown in FIG. 1, platform interface 112 is executed as a web-accessible GUI (described in more detail below). For simplicity, the present disclosure provides only aspects in which the platform interface 112 is a server-based solution to be accessible via the internet. However, it should be understood that the platform interface 112 is not limited to such an embodiment and may allow one or more custom dialog system engines 120 or dialog agents to be created through various GUI tools.
Still referring to FIG. 1, the backend service 114 may be responsible for maintaining and running a custom dialog system engine 120 created, for example, through or by means of the platform interface 112. The backend service 114 may operate as a web service that provides functionality to the customized dialog system by: enabling its dialog system interface to interact with a custom dialog system engine 120 maintained by the backend services 114 of the platform 110.
As described above, the dialog system interface 130 may be provided on the client side 140. Dialog system interface 130 may be a GUI that enables a user to make a query (which is then passed to backend service 114 for processing by the corresponding dialog system processing engine 120) and receive a response to the query generated by dialog system engine 120. The dialog system interface 130 may be implemented as a stand-alone software application or it may be integrated into another software application, web service, website, etc. It should be understood that the client-server model is illustrated for purposes of explanation only. The system disclosed herein need not be a client-server system, and in some examples, dialog system interface 130 and dialog system engine 120 may be on the same (e.g., user) device.
Client side 140 may include user devices, client devices, terminals, portals, user devices, computing devices (e.g., laptops, tablets, desktops, workstations, personal computers, and smartphones), personal digital assistants, gaming consoles, remote controls, multimedia systems, smart television devices, set-top boxes, infotainment systems, in-vehicle computing devices, kiosks, and robots, among others. In these examples, one or more dialog system interfaces 130 may be implemented as software, middleware, or firmware.
In additional examples, client side 140 may refer to networking or online solutions such as servers, web hosting services, web services, websites, and cloud services. For example, dialog system interface 130 may include a widget (widget) or GUI provided on one or more web pages to allow an end user to make queries and receive responses thereto. This option may be applicable, for example, where a developer wants to integrate a dialog system into a website to provide enhanced customer service.
As can be seen in fig. 1, the interaction between the dialog system interface 130 and the corresponding custom dialog system engine 120 is ongoing via the communication network 150. The communication network 150 may include one or more of the following: the internet, an intranet, a mobile data network, a local area network, a wide area network, an IEEE 802.11 based network, a personal area network (e.g., bluetooth, zigbee), infrared, and so forth.
Fig. 1 also illustrates various third party web resources/services 160 that may provide information to the custom dialog system engine 120 or dialog system interface 130 as part of a dialog system response or perform some action or operation. For example, web resource/service 160 may provide email services, weather services, navigation services, hotel reservation services, taxi reservation services, online shopping services, e-commerce services, reservation services, and reminder services, among others. Accordingly, if the user says "What is the weather like today? (how does weather today. If the user speaks "Send an email to John to invite him to my party tonight (send email to john inviting him to a meeting in me evening)", the dialog system engine 120 can cause the third party web resource/service 160 to create an email and send the email to an email address associated with the recipient.
An example process of creating and operating the custom dialog system engine 120 will now be described with reference to fig. 1 and other figures. In particular, the platform interface 112 may provide one or more GUIs with a number of tools that enable a developer to create and customize one or more "dialog system elements" that serve as a basis for customizing the dialog system engine.
According to various examples, dialog system elements include at least "entities" and "intents. Each entity may include a plurality of objects having substantially the same or similar characteristics. In other words, an entity may contain a list of keywords defining the objects of the class. In one example, an entity may include a keyword and a set of synonyms corresponding to the keyword. In another example, an entity may include a keyword and a set of definitions corresponding to the keyword. In yet another example, the entity may include a list (e.g., a city list, a name list, a title list, a brand list, and a street name list). In some examples, an entity may be used in a particular dialog agent and depend on the parameter values expected to be returned as a result of the agent function.
In some examples of the present disclosure, a developer may not need to create entities for each concept mentioned in a conversation agent—only for those concepts that are needed for actionable data. For example, there may be three types of entities. The first type may include system entities such as universal date references, time references, number references, and city references. The second type may include developer entities, such as any unique synonym phrase mapped to reference values, such that a developer may create a food type entity by referencing values with "vegetarian" having synonyms of "veg" and "veggie". The third type may include user entities, such as entities defined for a particular end user, such as playlist entities that are specific to user preferences. Further, each of these entities may be a map (with reference values), an enumerated type (without reference values), or a complex (containing other entities with aliases and returned object type values).
In some examples, the list of objects associated with the entity may be automatically expanded. For example, a machine learning algorithm may be used to suggest one or more new objects to be associated with the entity. Machine learning algorithms may be trained using large text and/or vocabularies. By way of example and not limitation, a developer of custom dialog system engine 120 can define an entity @ city having values such as New York and Los Angeles. If the user of the custom dialog system engine speaks or enters the word "Washington, d.c.", the entity @ city "can automatically be extended to New York, los Angeles and Washington, d.c. because the machine learning algorithm can determine that" Washington, d.c "is related to the object listed in the entity @ city. In some examples, the user may be required to confirm that the suggested object relates to one or more entities.
In further examples, the entity may include a list of other entities. For example, a developer may define an entity @ car as a list of entities (@ make, @ model), where the values @ make and @ model are set to any object to which @ make and @ model may be associated. For example, the entity @ car may contain a composite object, such as { make: "Honda"; model: "accerd" } and { make: "Ford"; model: "Fiesta" }, and so forth.
In addition, each intent may include dialog system interaction schemes or rules describing a particular relationship between a user request and a dialog system response. In other words, intent may represent a mapping between what the user said and the action to be taken by the software application. In one example, an intent may be determined to explicitly include a pattern of one or more references to an entity. An example schema may include "@ city: how weather in the city" where "@ city: city" is a reference to an entity @ city and a parameter city within the entity @ city. In some other examples, to determine intent, a developer may merely provide example requests (phrases) to illustrate intent and entities, rather than providing patterns of explicit references containing "entities". In such examples, platform 110 automatically determines what "entities" and "intents" are implied in the example request using machine learning techniques.
Based on the example input, the platform 110 may create a corresponding action. Each action may include a name (entity) and one or more parameters. For example, a request may be entered as follows: "Weather forecast for Los Angeles (los Angeles weather forecast)". Machine learning techniques may use the names "weather" and parameter names: the city of the data type @ city determines the action.
Thus, a developer may use the platform interface 112 to generate multiple entities and multiple intents, both of which are specific to a particular application or industry. These multiple entities and intents may form the dialog system engine 120 and enable the dialog system engine 120 to perform certain actions or generate certain outputs in response to a wide range of end user inputs. In some examples, an intent may include a generic structure that includes: an intent name (for user reference only), a list of patterns and/or example requests, an action name, parameters associated with an action, and fulfillment associated with an action. Fulfillment may include text (text highlighted on screen) or a call to a web service and make a request to a database, and so on. In some examples, platform 112 may provide actions to a developer and allow the developer to integrate custom fulfillment associated with the actions directly into custom dialog system engine 120. In some examples, the developer may receive the actions (action names and parameters) and integrate custom fulfillment into the client side 140. For example, custom fulfillment may include a request for a website or database to retrieve information (e.g., predictions, traffic information, and navigation), and perform some operations of the device on which the dialog system interface is running, and so on.
In some examples, dialog system elements may include one or more contexts. The context may include one or more parameters including tags, keywords, or cues that are intended during the conversation of the particular end user. The context may include the pre-conditions and boolean expressions of the tags. The intent may be triggered based on the input context. For example, an intent may be triggered when a boolean expression that satisfies a certain precondition or precondition is true. When executing intent based on a user's request, the output context is set into the end user session. In various examples, a particular lifetime may be assigned to an output context within a user session that includes several user requests. In some examples, the lifetime of the output context may be measured as the number of requests made during the user session. Within a user session, there is: a current context state that exists before executing the intent in response to a next user request; and post-execution context state, which is set after execution intention. The post-execution context state may include one or more newly added contexts based on the user request and the results of the execution intent. Some old contexts may be deleted from the post-execution state based on the result of the execution intent or due to its expiration.
The context may be a string of characters representing the current context of the user request. This helps to distinguish phrases that may be ambiguous or have different meanings depending on the user's preference or geographic location or topic of conversation. For example, if a user is listening to a music player application and finds a band that may be of interest to him, he may say like "I want to hear more of this (i want to hear more of this)". The developer may include the name of the band in the context of the request so that the conversation agent can handle it more efficiently. In another example, the developer is a manufacturer of the smart home device and has a mobile application that remotely controls the home appliance. The user may say "Turn on the front door light (Turn on front door light)", then "Turn it off", and the mobile application will understand that the second phrase still refers to the light because it is in the same context. Now later, if the user says "Turn on the coffee machine (Turn on coffee machine)", followed by "Turn it off", it will result in a different operation than before due to the new context. The context may also be associated with the user session (e.g., with a session identifier passed in the API call). If the user expression matches an intent, the intent may set an output context that will be shared by the expression.
In a further example, one or more attributes or parameters may be assigned to each context. These attributes may be identified during execution of the intent and used in actions associated with the intent. The values retrieved from the context may form parameters of the action. These attributes may be further placed in an output context that is set after the intent is performed.
FIG. 2 is a process flow diagram illustrating a method 200 for creating a custom dialog system engine using the platform 110 and for operating the platform 110 in accordance with one aspect disclosed herein. The method 200 may be performed by processing logic that may comprise hardware (e.g., decision logic, dedicated logic, programmable logic, and microcode), software (such as that running on a general purpose computer system or a dedicated machine), or a combination of both. In one example, processing logic refers to one or more components of the platform 110 or computing device shown in fig. 10. The steps of method 200 may be implemented in a different order than that described and shown in fig. 2. Furthermore, the method 200 may have additional steps not shown herein, but which may be apparent to one of ordinary skill in the art in light of this disclosure. Method 200 may also have fewer steps than outlined below and shown in fig. 2.
The method 200 may begin at operation 202, where a developer is enabled to register with the platform 110 and create a developer profile through a first server comprising at least one processor and memory storing processor-executable code. For these purposes, the developer interacts with the platform interface 112. The developer profile may virtually link (associate) the developer's custom dialog system engine 120 and one or more dialog system interfaces 130 deployed on the client side 140. The links may include prescribed Application Programming Interface (API) code, interaction rules, and target addresses, among others. In some examples, the developer profile may be accessible to multiple developers. At operation 202, the method may allow a developer to create one or more dialog systems. Each dialog system may be associated with an access Identifier (ID). The access ID may be used to access the dialog system from the client side 140 via authentication. In various examples, the access ID may include a token, a digital key, and the like.
At operation 204, the platform 110 may receive one or more dialog system entities from a developer and store them in a memory or database. In some examples, the entity is not received, but is created by a developer using a web tool of the platform interface 112. The dialog system entity may include a keyword and at least one synonym of the keyword, at least one definition of the keyword and the keyword, a list of keywords defining objects of a class, and so on. The dialog system entity may also be associated with or include one or more parameters.
At operation 206, the platform 110 receives one or more dialog system intents from the developer and stores them in a memory or database. In some examples, the dialog system intent is not received, but is created by a developer using the tools of the platform interface 112. As described above, the intent forms dialog system elements (custom logic that enables the dialog system engine to generate responses tailored to specific needs). Dialog system intents may include dialog system interaction schemes, rules defining relationships between user requests and dialog system responses, rules defining relationships between particular actions and one or more dialog system entities, and so forth. In some examples, a developer may explicitly define one or more dialog system entities to be used in one or more dialog system intents. Additionally or alternatively, the developer may provide example requests (phrases). Based on the example request, the platform 110 may suggest one or more dialog system entities. To suggest an entity, the platform 110 may first search for an appropriate entity in the entity list provided by the developer at operation 204. In some examples, platform 110 may suggest new dialog system entities via machine learning techniques. The developer may be enabled to approve, modify or change parameters of the suggested new dialog system entity. The developer may also provide one or more intent parameters.
It should be noted that the definition of an entity is not static. During further operation, platform 110 may dynamically redefine the entities defined by the developer. The entity may be redefined (enhanced) based on user profile, preferences, user requests, and the like. The redefined entities are used by the platform 110 for further processing.
At operation 208, the platform 110 may associate one or more dialog system intents with one or more dialog system actions to create one or more custom dialog system engines 120 or dialog agents. The custom dialog system engine 120 is associated with one or more dialog system interfaces 130 of the developer. Each action is defined by a name and a set of group names associated with the dialog system entity.
Operations 202-208 illustrate the setup process of the custom dialog system engine 120 (dialog agent), while operations 210-218 illustrate the process of operating the custom dialog system engine 120. In particular, once all dialog system elements of the custom dialog system engine 120 are created, they are maintained as backend services and enable any associated dialog system interfaces 130 to provide the full functionality of the dialog system to the user according to predetermined settings.
Specifically, at operation 210, the platform 110 may receive a user request from the unidentified dialog system interface 130. The user request may be a voice (speech) input or a text input. In some examples, dialog system interface 130 may pre-process user input, for example, by recognizing spoken words and converting speech input to text input. In other examples, preprocessing may include audio enhancement, noise cancellation, encryption/decryption, and the like. However, in other examples, dialog system interface 130 does not perform preprocessing.
In operation 212, the platform 110 processes the user request and identifies the dialog system interface 130. The identification process may be based on retrieving an identifier of the dialog system interface 130 from the user request. For example, when a user request is sent from dialog system interface 130 to platform 110, the user request may be accompanied by an identifier.
At operation 214, based on the recognition results at operation 212, the platform 110 activates the custom dialog system engine 120 associated with the recognized dialog system interface 130. At the same operation 214, the platform 110 may also retrieve or identify one or more dialog system elements (i.e., one or more entities and one or more intents) based on the identification result at operation 212. At operation 214, the platform 110 may identify the context (one or more tags, keywords, threads, and logical expressions thereof) associated with the user request and the particular request session.
At operation 216, the customized dialog system engine 120 processes the user request using the dialog system elements (i.e., the one or more entities and the one or more intents) retrieved at operation 214. The intent may be triggered based on context. The context may be predefined, determined based on a user request, and further changed after triggering one or more intents. The context may be specific to a particular user and a particular session of the user. Some examples of dialog system processing are further described with reference to fig. 3.
At operation 218, the custom dialog system engine 120 may generate and send a response to the dialog system interface 130 associated with the custom dialog system engine 120. The response may include specific data (e.g., a text message) and/or one or more actions. Each action may include a name and a set of parameters of the action identified using one or more intents and one or more entities. The dialog system interface 130 may then display or playback the text message to the end user according to predetermined settings. The dialog system interface 130 may also perform one or more operations based on one or more actions and according to custom fulfillment associated with the actions using the action names and parameter sets. In some examples, custom dialog system engine 120 may process custom fulfillment and send the results to dialog system interface 130. The response of the custom dialog system engine 120 may also refer to action instructions that may be executed by a client device, web resource/service 160, platform 110, or a remote server.
Fig. 3 illustrates a high-level architecture 300 of dialog system engine 120 according to one example. It should be noted that each module of the dialog system engine 120 or associated architecture includes a hardware component, a software component, or a combination thereof. The dialog system engine 120 may be embedded or installed in a user device or server, or may be presented as a cloud computing module and/or a distributed computing module.
In the example illustrated in fig. 3, dialog system engine 120 includes an Automatic Speech Recognizer (ASR) 310 configured to receive speech-based user input and process it as a sequence of parameter vectors. The ASR 310 further converts the sequence of parameter vectors into an recognized input (i.e., a text input having one or more words, phrases, or sentences). The ASR 310 includes one or more speech recognizers, such as a pattern-based speech recognizer, a free-dictation recognizer, an address book-based recognizer, and a dynamically created recognizer, among others.
In addition, dialog system engine 120 includes a Natural Language Processing (NLP) module 320 for processing the interpreted language input. Specifically, NLP module 320 may decompose and parse the recognized input to produce utterances, which are then analyzed using, for example, morphological analysis, word class labeling, shallow parsing, neural networks, machine learning classifiers, pattern mining classifiers, and the like. The NLP module 320 may then map the recognized input or portion thereof to a meaning representation.
The dialog system engine 120 further includes a dialog manager 330 that coordinates the activities of all components, controls dialog flow, and communicates with external applications, devices, services, or resources. Dialog manager 330 may play a number of roles including a speech analysis, knowledge database queries, and speech context based system action prediction. In some examples, dialog manager 330 may contact one or more task managers (not shown) that may have knowledge about a particular task domain. In some examples, the conversation manager 330 may be in communication with various computing, logic, or storage resources 340, which computing, logic, or storage resources 340 may include, for example, content stores, rules databases, recommendation databases, push notification databases, electronic address books, email or text agents, conversation history databases, disparate knowledge databases, map databases, point of interest databases, geographic location determinants, clocks, wireless network detectors, search engines, social networking websites, blog websites, and news feed services, among others. In some examples, the computing or storage resources 340 include one or more web resources/services 160 discussed above.
During operation (e.g., within a dialog session), dialog manager 330 may control dialog flows according to input and output contexts. The input context represents some precondition that is intended to be performed. A particular intent is triggered only if a certain input context is present in the user request or as a result of executing a previous intent. If several intents can be triggered based on the same context, the decision as to which intent to perform can be based on the weight of the intent associated with the context, the age of the context, and other parameters as specified in the preferences. Newer contexts may have higher priorities. The output context may be set at the time of matching and executing the intent.
In various examples, dialog manager 330 may communicate the user request to the dialog system. The dialog system may comprise a custom dialog system designed by a developer, as described in fig. 1 and 2. Also, in some examples, user requests may be sent in parallel to the task domains. A task domain is a pre-built dialog system that can process requests and provide answers. In some examples, if the custom dialog system fails to provide an answer to the user request, dialog manager 330 continues to process the answer received from the task domain.
Dialog manager 330 may employ a number of different methods to generate an output in response to the recognized input. Some methods include the use of statistical analysis, machine learning algorithms (e.g., neural networks), heuristic analysis, and the like. The dialog manager 330 is one of the central components of the dialog system engine 120. The main role of dialog manager 330 is to select the correct system actions based on observed evidence and inferred dialog states (e.g., dialog actions, user goals, and speech history) from the NLP results. In addition, when the user input has ASR and NLP errors caused by noise or unexpected input, dialog manager 330 may handle the errors.
Dialog system engine 120 may also include an output renderer 350 for transforming the actions selected by dialog manager 330 into output in a form suitable for presentation to a user or in the form of computer-or processor-implementable instructions (e.g., API code). For example, the output renderer 350 may use a text-to-speech engine or may contact a pre-recorded audio database to generate an audio message corresponding to the output of the dialog manager 330. In some examples, the output renderer 350 can render or cause to be rendered the output of the dialog manager 330 as a text message, an image, or a video message for further display on a display screen of a user device.
In other examples, the output renderer 350 provides the selected actions (action name and parameter set) to the dialog system interface 130 on the client side 140. The developer may configure the dialog system interface 130 to process the selected action and perform one or more desired operations, such as sending a request to a web service, database operations, displaying a text message, playing audio or video on a user device, generating text, and processing it through a text-to-speech system, etc. In some examples, a developer may configure custom dialog system engine 120 to process an action according to the fulfillment associated with the action and provide the result to dialog system interface 130.
FIG. 4 illustrates an example GUI 400 for creating the platform interface 112 of a new dialog system entity as described above. When a developer desires to create a custom dialog system engine 120, he can define dialog system entities and intents using a web tool such as the platform interface 112 of the GUI 400. Using GUI 400, a developer can enter a reference value 402 for a keyword in a corresponding field provided by GUI 400 and enter a synonym 404 for the provided reference value. The dialog system entity may include keywords (or reference values) and synonyms for keywords, definitions of keywords and keywords, a list of keywords defining a class of objects, and so forth. Keywords or reference values and their synonyms and/or definitions constitute dialog system entities.
In some examples, each entity may have a title. For example, an entity may be referred to as a "city" and contains a list of cities such as Arlington, boston and Chicago. In other examples, an entity may be referred to as a keyword, and it may contain synonyms or definitions of the keyword. In one example, an entity called "music" may include terms of songs, singers, and songbists, among others. In another example, an entity called an "artist" may include a music band, or a list of music artists. In another example, an entity called "Beatles" may include a range of possible synonyms, such as "The Beatles", "Fab Four", "Liverpool Legends" and "John Lennon", among others. In yet another example, there may be an entity called "artist," which may include individual artist names, artist name synonyms, and music band names, among others.
FIG. 5 illustrates an example GUI 500 of the platform interface 112 for creating new dialog system intents as described above. Dialog system intent may define a relationship between a user request and a dialog system response and may be associated with a rule based on the relationship between a particular action and an entity. In general, each intent may be represented as the following computer readable program "[ Action ] @ [ Entity ]" or "[ Action ] @ [ enties ]". Using GUI 500, a developer can add user expressions 502 (e.g., "weather@city") to illustrate intent and entities. Based on the user expressions 502, the platform 110 uses machine learning techniques to automatically determine what entities and intents are implied in the example request and create corresponding rules. For example, the developer may simply provide an example request, such as "Weather forecast for Los Angeles (weather forecast for los Angeles)". The platform 110 may match "Los Angeles" to existing entities (system or user defined) and automatically generate corresponding "[ Action ] @ [ Entity ]" rules. Additionally or alternatively, a developer may provide example requests such as "What is the weather in@say.geo-city: geo-city-us (" day. Geo-city: weather of geo-city-us) ", with explicit presentation of one or more entities. In the example of FIG. 5, the "weather" and parameters associated with act 506 are "geo-city-us" and "geo-city". The developer may further modify act 506 and provide fulfillment 508 for act 506.
The created rule means that a particular action should be performed by the client side 140 (or server, web service, etc.) with respect to the entity or entities. For example, one intent may be expressed as "looking up forecast for $geo-city-us (find forecast for $geo-city-us)". In this example, the intent command dialog system engine 120 looks up a forecast of Los Angeles.
In some examples, GUI 500 provides control 510 for machine learning. Switching machine learning algorithms may allow for handling ambiguity of matching ranging from hard/rigid matching to extensive blurring or machine learning matching.
In some examples, the platform interface 112 may provide a GUI for providing a log of requests and process intents associated with a particular dialog system end user or group of end users. FIG. 6 illustrates an example GUI 600 for providing a log of requests of a particular user.
In various examples, the platform interface 112 may provide a tool for a developer to statistically analyze the effectiveness of the customized dialog system. The resulting statistics may include the number of sessions, the number of requests, the number of classified requests for which at least one intention is triggered, the number of unclassified requests for which the intention is not triggered, the precision of the requests, the recall, and the F-score, among others. In some examples, unclassified requests are divided into groups based on machine learning clustering.
In further examples, platform interface 112 may provide tools for tagging entities in unclassified requests by developers or machine learning techniques to modify or generate new entities, intents, actions, and fulfillment for the request. The platform interface 112 may include tools for reclassifying requests through one or more custom dialog systems.
FIG. 7 is a process flow diagram illustrating a method 700 for collecting intent parameters and operating a dialog system according to an example. Method 700 may be performed by processing logic that may comprise hardware (e.g., decision logic, dedicated logic, programmable logic Application Specific Integrated Circuits (ASICs), and microcode), software (such as that running on a general purpose computer system or a dedicated machine), or a combination of both. In one example, processing logic refers to platform 110, back-end service 114, custom dialog system engine 120, computing device 1000, or any combination thereof. The steps of method 700 described below may be performed in a different order than those described and illustrated. Furthermore, method 700 may have additional steps not shown herein, but which may be apparent to one of ordinary skill in the art in light of this disclosure. Method 700 may also have fewer steps than outlined below and shown in fig. 7.
The method 700 may begin at operation 702, where a dialog system (e.g., the custom dialog system engine 120) receives voice input of a user. Voice input may be provided to client 140 through dialog system interface 130. The voice input may be processed at the client 140 or through a dialog system. For example, voice input may be recognized and converted into computer-readable text input.
At operation 704, the dialog system (e.g., the customized dialog system engine 120) can identify or determine a dialog system intent associated with the voice input based on at least one predetermined intent keyword of the voice input. In other words, the dialog system can process the voice input to determine whether one or more intent keywords refer to one of the predetermined intents. For example, the voice input "Please Order a Pizza for me (please Order Pizza for me)" includes the intent keywords "Order" and "Pizza", which may cause the triggering or detection of a predetermined "Order Pizza" intent (which may be previously created by a developer via platform 110). Dialog system intent may indicate an entry point into a parameter collection dialog for collecting intent parameters. In a given example, the intent parameters may include pizza size, crust type, choice of ingredients, sauce, shipping address, shipping time, vendor, and the like. Thus, the intent parameter may be associated with a parameter name and its value. Parameter values may include values, words, phrases, sounds or images. In some implementations, some or all of the intent parameters may be selected from a list of predetermined values (e.g., one intent parameter may be selected from a list of predetermined city names).
At operation 706, the dialog system can determine whether the voice input already includes all of the missing desired intent parameters. If it is determined that there are all missing desired intent parameters in the voice input presented in operation 702, the method may proceed to operation 708. At operation 708, the dialog system can identify and collect the missing desired intent parameters from the voice input. The collected desired intent parameters may be temporarily stored in a cache or memory.
Once all of the desired missing intent parameters are collected, the dialog system generates action instructions associated with the dialog system based on the intent and intent parameters at operation 710. The action instructions may be configured to cause the dialog system, the server, the user device, or the dialog system interface to implement the predetermined action based on the action instructions and the one or more intent parameters. In the example given above, the action instructions may refer to an electronic purchase order that may be sent to the predetermined web service 160 for ordering pizza based on previously collected intent parameters. Thus, in some examples, the action instructions may include an API-specific response (or API code) configured to cause the API service. For example, the API code of the action instruction may be in JavaScript object notation (JSON) format.
In some examples, at operation 710, the dialog system may generate and present a response message to the user prior to generating the action instruction. The response message may repeat one or more of the collected intent parameters and ask the user to confirm whether the intent parameters are correct. For example, the dialog system may generate a text or audio message to be communicated via the dialog system interface 130, such as "You want to order a large margherita pizza to be delivered to your home in an hours. Is heat correction? (you want to send a large share of the Margarita pizza to you in one hour. Do it. Otherwise, the dialog system may provide one or more prompts to ask the user to elucidate his request.
If it is determined at operation 706 that the initial voice input (acquired at operation 702) does not include all of the missing desired intent parameters, the method may proceed to operation 712, as shown in FIG. 7. At operation 712, the dialog system may initiate a predetermined parameter collection dialog associated with the intent identified at operation 704. The predetermined parameter collection dialog may include a plurality of hint message parameters that the dialog system may query to obtain the desired intent that is absent and, in some examples, optional intent parameters. Thus, at operation 712, the dialog system provides one or more prompts to the user to prompt the user to enter one or more missing intent parameters. For example, at operation 712, the dialog system may ask the user "What pizza size would you like-large, medium or small? (what pizza size you want-big, medium or small? (tell me what ingredients you want?
At operation 714, the dialog system may receive one or more additional voice inputs from the user that include an answer to the prompt given in the previous operation. The dialog system may retrieve the desired intent parameters from additional voice input or from other sources. At operation 716, the dialog system may determine whether all desired intent parameters are available. If it is determined that all of the required intent parameters are available, the method may proceed to operation 710. Otherwise, if it is determined that not all of the required intent parameters are available, the method may return to operation 712, as shown in FIG. 7. Operations 712, 714, and 716 may be repeated until all missing desired intent parameters are collected.
As described above, the platform 110 may enable a developer to create or modify a conversation agent of a natural voice conversation system to automatically collect all desired intent parameters. Each developer may have a developer profile in which platform 110 stores all of the developers' custom dialog system engines and dialog agents. FIG. 8 illustrates an example GUI 800 of the platform interface 112 for creating a dialog agent for collecting intent parameters in a parameter collection dialog, according to an example.
As shown in fig. 8, GUI 800 includes actionable buttons 802 that may cause intent to create a dialog system. For example, a developer may create an intent to electronically reserve a hotel and create a parameter collection dialog for the intent to collect all of the intent parameters required to properly reserve the hotel reservation. First, the developer can name a newly created intent, such as a "check-in hotel" intent, as shown by gadget 804 in FIG. 8. Further, the developer may provide, via an interface of GUI 800, one or more example phrases or keywords that can trigger activation of the intent. To these ends, a developer may provide example phrases or keywords through gadget 806. Some example phrases or keywords for the "book a hol" intent include "book a hol", "book a hol in@sys.geo-city: city (booking @ sys. Geo-city: hotel in city)", or "book a hol in@sys.geo-city: city on@sys.date: date (booking @ sys. Date: hotel at the time @ sys. Geo-city: city). "here @ sys. Geo-city: city and @ sys. Date: date refer to entities" city "and" date ", respectively. In other words, in some examples, example phrases or keywords used to trigger intent may be associated with one or more entities. By clicking on button 808, the developer can add additional example phrases or keywords associated with the intent.
Further, when the dialog system performs an intent, the developer may select one of the predetermined actions or create a new action to be performed. Here, the gadget 810 shows that the developer has selected one of the predefined actions or created a new action "bookhol" that can be customized through another GUI of the platform interface 112. In addition, the developer may provide one or more intent parameters that will be collected in the parameter collection session when the parameter collection session is activated during the session with the user. The intent parameters may include "city" and "date" and so forth. As shown in FIG. 8, the intent parameters are provided, identified, or modified via the GUI gadget 812. By clicking on button 814, the developer may add the new intent parameter to the list. Each intent parameter may have a plurality of characteristics. For example, the developer may specify whether the intent parameters are necessary (mandatory) or optional. In addition, the developer may provide or modify the parameter name, parameter data type, and parameter value, and provide one or more hint messages. The dialog system may query the prompt message for each intent parameter during the parameter collection dialog to obtain all missing intent parameter values (e.g., as described above with reference to operations 712 and 714 of method 700).
FIG. 9 illustrates an example GUI of the platform interface 112 for defining prompts for a dialog agent, according to an example. As shown, each intent parameter may be associated with a parameter name (e.g., city), a parameter data type (e.g., @ sys. Geo-city), a parameter value (e.g., $city), and one or more hint messages 902. Here, the parameter data type and the parameter value may refer to dialog system entities. Further, the hint message 902 may refer to "Where are you going? (where do you go)? (what is the destination). The order in which prompts are selected and delivered to the user may be arbitrary or predetermined by the developer.
As described above, some parameters may be mandatory (necessary) in order to require and collect their respective values from the user, while some other parameters may be optional. For example, a "city" parameter may be mandatory, but the name of the chain hotel may be optional.
Fig. 10 is a high-level block diagram illustrating a computing device 1000 suitable for implementing the methods described herein. In particular, the computing device 1000 may be used to create and modify dialog systems by a developer and to execute dialog systems. Computing device 1000 may include, be, or be an integral part of, one or more of a variety of types of devices, such as general purpose computers, desktop computers, laptop computers, tablet computers, servers, netbooks, mobile phones, smart phones, infotainment systems, smart television devices, and so forth. In some examples, computing device 1000 may be considered an instance of a client device, a server, platform 110, or a dialog system.
As shown in fig. 10, computing device 1000 includes one or more processors 1010, memory 1020, one or more mass storage devices 1030, one or more input devices 1050, one or more output devices 1060, a network interface 1070, one or more optional peripheral devices 1080, and a communication bus 1090 for operatively interconnecting the elements listed above. The processor 1010 may be configured to implement functionality and/or process instructions for execution within the computing device 1000. For example, the processor 1010 may process instructions stored in the memory 1020 and/or instructions stored on the storage device 1030. Such instructions may include components of an operating system or software application.
According to one example, memory 1020 is configured to store information within computing device 1000 during operation. In some examples, memory 1020 may refer to a non-transitory computer-readable storage medium or a computer-readable storage device. In some examples, memory 1020 is temporary memory, meaning that the primary purpose of memory 1020 may not be long-term storage. Memory 1020 may also refer to volatile memory, meaning that memory 1020 does not maintain stored content when memory 1020 is not receiving power. Examples of volatile memory include Random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), and other forms of volatile memory known in the art. In some examples, memory 1020 is used to store program instructions that are executed by processor 1010. In one example, memory 1020 is used by a software application. In general, a software application refers to a software application suitable for implementing at least some operations of the methods for collecting intent parameters and operating a dialog system as described herein.
The mass storage device 1030 may also include one or more transitory or non-transitory computer-readable storage media and/or computer-readable storage devices. In some examples, mass storage device 1030 may be configured to store larger amounts of information than memory 1020. The mass storage device 1030 may also be configured for long term storage of information. In some examples, mass storage device 1030 includes non-volatile storage elements. Examples of such non-volatile storage elements include magnetic hard disks, optical disks, solid state disks, flash memory, forms of electrically programmable memory (EPROM) or electrically erasable and programmable memory, and other forms of non-volatile memory known in the art.
Still referring to FIG. 10, the computing device 1000 may also include one or more input devices 1050. Input device 1050 may be configured to receive input from a user through a tactile, audio, video, or biometric channel. Examples of input devices 1050 may include a keyboard, a keypad, a mouse, a trackball, a touch screen, a touchpad, a microphone, a video camera, an image sensor, a fingerprint sensor, or any other device capable of detecting input from a user or other source and relaying the input to computing device 1000 or components thereof. The output device 1060 may be configured to provide output to a user through a visual or auditory channel. The output devices 1060 may include: a video graphics adapter card; a display such as a Liquid Crystal Display (LCD) monitor, a Light Emitting Diode (LED) monitor, or an organic LED monitor; a sound card; a speaker; a lighting device; a projector; or any other device capable of generating an output that is understandable to the user. The output device 1060 may also include a touch screen, a presence-sensitive display, or other display with input/output capabilities known in the art.
Computing device 1000 may also include a network interface 1070. The network interface 1070 may be used to communicate with external devices via one or more networks such as one or more wired, wireless, or optical networks including, for example, the internet, intranets, local area networks, wide area networks, cellular telephone networks (e.g., global system for mobile communications networks, long term evolution communications networks, packet switched communications networks, circuit switched communications networks), bluetooth radios, and IEEE 802.11 based radio high frequency networks, etc. Network interface 1070 may be a network interface card such as an ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information.
An operating system of the computing device 1000 may control one or more functions of the computing device 1000 or components thereof. For example, an operating system can interact with software applications and can facilitate one or more interactions between the software applications and the processor 1010, memory 1020, storage 1030, input devices 1050, output devices 1060, and network interface 1070. The operating system may interact with or otherwise be coupled to the software application or components thereof. In some examples, the software application may be included in an operating system.
Thus, a method and system for collecting intent parameters in a dialog system has been described. Although certain aspects have been described with reference to specific examples, it will be evident that various modifications and changes may be made to these examples without departing from the broader spirit and scope of the application. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense.
The preceding detailed description includes references to the accompanying drawings that form a part hereof. The approaches described in this section are not prior art to the claims and are not admitted to be prior art by inclusion in this section. The drawings illustrate descriptions according to examples disclosed herein. These examples, which are also referred to herein as "examples," are described in sufficient detail to enable those skilled in the art to practice the present subject matter. Examples may be combined, other examples may be utilized, or structural, logical, and operational changes may be made without departing from the scope of the claims. The following detailed description is, therefore, not to be taken in a limiting sense, and the scope is defined by the appended claims and their equivalents.
The examples provided above are referenced in the figures by various blocks, components, circuits, steps, operations, procedures, algorithms, etc., which are collectively referred to as "elements". These elements may be implemented using electronic hardware, computer software, or any combination thereof. Whether such elements are implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system.
For example, an element, any portion of an element, or any combination of elements may be implemented with a "processing system" that includes one or more processors. Examples of processors include microprocessors, microcontrollers, central Processing Units (CPUs), digital Signal Processors (DSPs), field Programmable Gate Arrays (FPGAs), programmable Logic Devices (PLDs), state machines, gating logic, discrete hardware circuits, and other suitable hardware configured to perform the various functions described throughout this disclosure. One or more processors in a processing system may execute software, firmware, or middleware (collectively, "software"). The term "software" should be construed broadly to mean instructions, instruction sets, code segments, program code, programs, subroutines, software components, applications, software packages, routines, subroutines, objects, executables, threads of execution, programs, functions, etc., whether referred to as software, firmware, middleware, microcode, hardware description language, or otherwise.
Thus, in one or more examples, the described functions may be implemented in hardware, software, or any combination thereof. If implemented in software, the functions may be stored or encoded as one or more instructions or code on a non-transitory computer-readable medium. Computer readable media includes computer storage media. A storage media may be any available media that can be accessed by a computer. By way of example, and not limitation, such computer-readable media can comprise random access memory, read-only memory, electrically erasable programmable random access memory (EEPROM), optical disk random access memory or other optical disk storage, magnetic disk storage, solid state memory, or any other data storage device, combinations of the foregoing types of computer-readable media, or any other medium that can be used to store computer-executable code in the form of instructions or data structures that can be accessed by a computer.
For the purposes of this patent document, the terms "or" and "mean" and/or "unless otherwise specified or clearly indicated by the context of its use. The terms "a" and "an" mean "one or more" unless specified otherwise or where the use of "one or more" is clearly inappropriate. The terms "comprising" and "including" are interchangeable and are not intended to be limiting. For example, the term "comprising" should be interpreted to mean "including but not limited to".
The terms "natural language dialog system" and "dialog system" are used interchangeably and should be interpreted to mean a computer-implemented system for providing human-machine dialog interactions using text, voice, graphics, touch, gestures, computer-generated actions, and other modes of communication over input and output channels, wherein responses to user inputs are generated by one or more dialog system agents or dialog system engines, and wherein the system provides an interface for receiving, processing, managing, and communicating information. The terms "chat information system", "spoken dialog system", "talk agent", "chat bot", "chat agent", "digital personal assistant" and "automated online assistant" etc. are within the scope of the term "dialog system".
The terms "client device" and "user device" should be construed to mean any electronic or computing device on the client side 140 that has input and output capabilities, such as mobile devices, cellular telephones, mobile telephones, user devices, user terminals, smart phones, tablet computers, laptop computers, desktop computers, servers, personal digital assistants, music players, multimedia players, portable computing devices, navigation systems, vehicle computers, infotainment systems, gaming devices, game consoles, entertainment systems, television devices, network devices, modems, routers, and the like.
The term "user" means a user of "client device" and "user device". The term "developer" should be interpreted as a software developer, engineer or owner of a dialog system engine (agent) that may be created and maintained via platform 110.
The terms "dialog system agent" and "dialog system engine" may be used interchangeably and may be interpreted to represent a computer-implementable interface for processing user input based on one or more predetermined rules or criteria, such as dialog system elements, including dialog system intent and dialog system entities.
Technical effects disclosed herein may provide improvements to natural language dialog systems in processing user requests and collecting multiple parameters (or parameter values) associated with the user requests to generate computer-implemented actions based on the multiple parameters.
Further technical effects disclosed herein may provide improvements in hardware performance by reducing calls to memory to perform lookup attributes, thereby reducing latency and improving battery life, reducing steps, interfaces, and calls to memory required to establish voice actions, and the like.
Where the systems or methods discussed herein collect or may utilize personal information about a user, the user may be provided with an opportunity to control the collection and/or use of such personal information, thereby limiting the collection of some or all of such data. For example, when use of the context for identifying the parameter is available, the user may be provided with the ability to limit the collection of some or all of the context data. In addition, some data may be processed in one or more ways before it is stored or used, such that personally identifiable information is deleted or obscured. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized such that a particular location of the user cannot be determined. Thus, a user may be provided with control over how information is collected about the user and used by the systems and methods disclosed herein.
Although the present disclosure includes some details, these should not be construed as limitations on the scope of the disclosure or of what may be claimed, but rather as descriptions of features of example embodiments of the disclosure. Certain features that are described in this disclosure in the context of separate embodiments can also be provided in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be provided in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the present disclosure have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. For example, various forms of the flows shown above may be used, with steps reordered, added, or deleted. Accordingly, other embodiments are within the scope of the following claims.
Claims (24)
1. A method implemented by one or more processors, the method comprising:
receiving, via a platform interface, a plurality of instances of user interface input from a developer, the developer creating a dialog agent of a dialog system;
based on an instance of the user interface input:
determining at least one intent key, the at least one intent key being a trigger of dialog system intent of the dialog agent;
determining one or more system parameters for the dialog system intent, each of the one or more system parameters being defined based on a corresponding system entity indicated by a corresponding one of the instances entered by the user interface, wherein the corresponding system entity is predefined for the dialog system;
Determining one or more developer parameters for the dialog system intent, each of the one or more developer parameters being defined based on a corresponding developer entity, wherein each of the corresponding developer entities is created by the developer by providing one or more corresponding terms of the corresponding reference and one or more corresponding synonyms via one or more corresponding ones of the instances entered via the user interface;
determining, for each of the developer parameter and the system parameter, whether it is a mandatory parameter of the dialog system intent or an optional parameter of the dialog system intent;
determining a corresponding hint message for each of the mandatory parameters; and
determining an action to be performed when the dialog system is intended to be triggered and at least the value of the forcing parameter is decided, wherein the action is performed based on the decided value;
associating the at least one intent keyword with a trigger of the dialog system intent of the dialog agent, associating the dialog system intent with the action, and associating the developer parameter and the system parameter with the dialog system intent and with an indication of whether they are mandatory or optional parameters;
After association:
receiving voice input provided via a dialog system interface of a client device, the dialog system interface being associated with the dialog system;
processing the speech input using an automatic speech recognizer to generate a recognized input;
determining that the identified input includes the at least one intent key that is a trigger for the dialog system intent of the dialog agent;
responsive to determining that the identified input includes the at least one intent keyword that is a trigger for the dialog system intent of the dialog agent:
determining whether values of all of the mandatory parameters for the dialog system intent are determinable based on the identified inputs and without initiating a parameter collection dialog;
when it is determined that the values for all of the mandatory parameters are determinable without initiating the parameter collection session:
initiating performance of the action based on values of all of the mandatory parameters without initiating the parameter collection dialogue;
when it is determined that the values for all of the mandatory parameters are not determinable without initiating the parameter collection session:
Performing a parameter collection dialogue until all values of the mandatory parameters are decided, wherein the parameter collection dialogue is performed based on one or more of the corresponding hint messages for the mandatory parameters; and
after executing the parameter collection session, the execution of the action is initiated based on the values of all of the mandatory parameters.
2. The method of claim 1, further comprising:
determining an alternative value for one of the alternative parameters based on the identified input;
wherein upon determining that the values of all of the mandatory parameters are determinable, initiating performance of the action based on the values of all of the mandatory parameters without initiating the parameter collection session comprises:
the execution of the action is initiated based on the selectable value and the values of all of the forcing parameters.
3. The method of claim 2, further comprising:
determining that no alternative values for additional ones of the alternative parameters are determinable based on the identified inputs;
wherein when it is determined that the values of all of the mandatory parameters are determinable, initiating performance of the action based on the values of all of the mandatory parameters without initiating the parameter collection session comprises:
Execution of the action is initiated based on the selectable values, the values of all of the mandatory parameters, and without any values of additional ones of the selectable parameters.
4. The method of claim 1, further comprising:
determining that no alternative value for one of the alternative parameters is determinable based on the identified input;
wherein when it is determined that the values of all of the mandatory parameters are not determinable, performing the parameter collection session until the values of all of the mandatory parameters are determined comprises:
during the parameter collection session, determining values of all of the mandatory parameters without determining any value of one of the optional parameters; and
wherein after performing the parameter collection session, initiating performance of the action based on values of all of the mandatory parameters comprises:
execution of the action is initiated based on the values of all of the mandatory parameters, but without any value of one of the optional parameters.
5. The method of claim 1, wherein the corresponding system entity comprises one or more of a date reference, a time reference, a number reference, and a city reference.
6. The method of claim 1, wherein the action is performed by the client device or by a server in communication with the client device.
7. The method of claim 6, wherein the action comprises an Application Programming Interface (API) -specific response.
8. A system comprising at least one processor and a memory storing processor executable code, wherein the at least one processor is configured to, when executing the processor executable code, perform the following:
receiving, via a platform interface, a plurality of instances of user interface input from a developer, the developer creating a dialog agent of a dialog system;
based on an instance of the user interface input:
determining at least one intent key, the at least one intent key being a trigger of dialog system intent of the dialog agent;
determining one or more system parameters for the dialog system intent, each of the one or more system parameters being defined based on a corresponding system entity indicated by a corresponding one of the instances entered by the user interface, wherein the corresponding system entity is predefined for the dialog system;
Determining one or more developer parameters for the dialog system intent, each of the one or more developer parameters being defined based on a corresponding developer entity, wherein each of the corresponding developer entities is created by the developer by providing one or more corresponding terms of the corresponding reference and one or more corresponding synonyms via one or more corresponding ones of the instances entered via the user interface;
determining, for each of the developer parameter and the system parameter, whether it is a mandatory parameter of the dialog system intent or an optional parameter of the dialog system intent; and
determining an action to be performed when the dialog system is intended to be triggered and at least the value of the forcing parameter is decided, wherein the action is performed based on the decided value;
associating the at least one intent keyword with a trigger of the dialog system intent of the dialog agent, associating the dialog system intent with the action, and associating the developer parameter and the system parameter with the dialog system intent and with an indication of whether they are mandatory or optional parameters;
After association:
receiving voice input provided via a dialog system interface of a client device, the dialog system interface being associated with the dialog system;
processing the speech input using an automatic speech recognizer to generate a recognized input;
determining that the identified input includes the at least one intent key that is a trigger for the dialog system intent of the dialog agent;
responsive to determining that the identified input includes the at least one intent keyword that is a trigger for the dialog system intent of the dialog agent:
determining whether values of all of the mandatory parameters for the dialog system intent are determinable based on the identified inputs and without initiating a parameter collection dialog;
when it is determined that the values for all of the mandatory parameters are determinable without initiating the parameter collection session:
initiating performance of the action based on values of all of the mandatory parameters without initiating the parameter collection dialogue;
when it is determined that the values for all of the mandatory parameters are not determinable without initiating the parameter collection session:
Performing a parameter collection session until all values of the mandatory parameters are determined, wherein the parameter collection session requests input related to one or more of the non-determined mandatory parameters; and
after executing the parameter collection session, the execution of the action is initiated based on the values of all of the mandatory parameters.
9. The system of claim 8, wherein the at least one processor is further configured to:
determining an alternative value for one of the alternative parameters based on the identified input;
wherein upon determining that the values of all of the mandatory parameters are determinable, initiating performance of the action based on the values of all of the mandatory parameters without initiating the parameter collection session, the at least one processor is configured to:
the execution of the action is initiated based on the selectable value and the values of all of the forcing parameters.
10. The system of claim 9, wherein the at least one processor is further configured to:
determining that no alternative values for additional ones of the alternative parameters are determinable based on the identified inputs;
wherein, when it is determined that the values of all of the mandatory parameters are determinable, initiating performance of the action based on the values of all of the mandatory parameters without initiating the parameter collection dialogue, the at least one processor is configured to:
Execution of the action is initiated based on the selectable values, the values of all of the mandatory parameters, and without any values of additional ones of the selectable parameters.
11. The system of claim 8, wherein the at least one processor is further configured to:
determining that no alternative value for one of the alternative parameters is determinable based on the identified input;
wherein, when it is determined that the values of all of the mandatory parameters are not determinable, the parameter collection dialogue is performed until the values of all of the mandatory parameters are determined, the at least one processor is configured to:
during the parameter collection session, determining values of all of the mandatory parameters without determining any value of one of the optional parameters; and
wherein after performing the parameter collection session, initiating performance of the action based on values of all of the mandatory parameters, the at least one processor configured to:
execution of the action is initiated based on the values of all of the mandatory parameters, but without any value of one of the optional parameters.
12. The system of claim 8, wherein the corresponding system entity comprises one or more of a date reference, a time reference, a number reference, and a city reference.
13. The system of claim 8, wherein the actions are performed by the client device or by a server in communication with the client device.
14. The system of claim 13, wherein the action comprises an Application Programming Interface (API) -specific response.
15. A non-transitory processor-readable medium having instructions stored thereon that, when executed by one or more processors, cause the one or more processors to implement a method for a natural voice conversation system, the method comprising:
receiving, via a platform interface, a plurality of instances of user interface input from a developer, the developer creating a dialog agent of a dialog system;
based on an instance of the user interface input:
determining at least one intent key, the at least one intent key being a trigger of dialog system intent of the dialog agent;
determining one or more system parameters for the dialog system intent, each of the one or more system parameters being defined based on a corresponding system entity indicated by a corresponding one of the instances entered by the user interface, wherein the corresponding system entity is predefined for the dialog system;
Determining one or more developer parameters for the dialog system intent, each of the one or more developer parameters being defined based on a corresponding developer entity, wherein each of the corresponding developer entities is created by the developer by providing one or more corresponding terms of the corresponding reference and one or more corresponding synonyms via one or more corresponding ones of the instances entered via the user interface;
determining, for each of the developer parameter and the system parameter, whether it is a mandatory parameter of the dialog system intent or an optional parameter of the dialog system intent;
determining a corresponding hint message for each of the mandatory parameters; and
determining an action to be performed when the dialog system is intended to be triggered and at least the value of the forcing parameter is decided, wherein the action is performed based on the decided value;
associating the at least one intent keyword with a trigger of the dialog system intent of the dialog agent, associating the dialog system intent with the action, and associating the developer parameter and the system parameter with the dialog system intent and with an indication of whether they are mandatory or optional parameters;
After association:
receiving voice input provided via a dialog system interface of a client device, the dialog system interface being associated with the dialog system;
processing the speech input using an automatic speech recognizer to generate a recognized input;
determining that the identified input includes the at least one intent key that is a trigger for the dialog system intent of the dialog agent;
responsive to determining that the identified input includes the at least one intent keyword that is a trigger for the dialog system intent of the dialog agent:
determining whether values of all of the mandatory parameters for the dialog system intent are determinable based on the identified inputs and without initiating a parameter collection dialog;
when it is determined that the values for all of the mandatory parameters are determinable without initiating the parameter collection session:
initiating performance of the action based on values of all of the mandatory parameters without initiating the parameter collection dialogue;
when it is determined that the values for all of the mandatory parameters are not determinable without initiating the parameter collection session:
Performing a parameter collection dialogue until all values of the mandatory parameters are decided, wherein the parameter collection dialogue is performed based on one or more of the corresponding hint messages for the mandatory parameters; and
after executing the parameter collection session, the execution of the action is initiated based on the values of all of the mandatory parameters.
16. A method implemented by one or more processors, the method comprising:
based on user interface input from a developer of a dialog agent creating the dialog system:
determining dialog system intent of the dialog agent;
determining a plurality of parameters for the dialog system intent;
determining, for each of the parameters, whether it is a mandatory parameter of the dialog system intent or an optional parameter of the dialog system intent; and
determining an action to be performed when the dialog system is intended to be triggered and at least the value of the forcing parameter is decided, wherein the action is performed based on the decided value;
associating the dialog system intent with the action and associating the parameter with the dialog system intent and with an indication of whether they are mandatory or optional parameters;
After association:
receiving voice input provided via a dialog system interface of a client device, the dialog system interface being associated with the dialog system;
determining the dialog system intent to activate the dialog agent based on the processing of the voice input;
in response to the dialog system intent activating the dialog agent:
determining whether values of all of the mandatory parameters for the dialog system intent are determinable without initiating a parameter collection dialog;
when it is determined that the values for all of the mandatory parameters are determinable without initiating the parameter collection session:
initiating performance of the action based on values of all of the mandatory parameters without initiating the parameter collection dialogue;
when it is determined that the values for all of the mandatory parameters are not determinable without initiating the parameter collection session:
performing a parameter collection session until all values of said mandatory parameters are determined; and
after executing the parameter collection session, the execution of the action is initiated based on the values of all of the mandatory parameters.
17. The method of claim 16, wherein determining parameters of the dialog system intent comprises:
A developer parameter of the parameters is determined, the developer parameter being based on a developer entity created by the developer by providing at least one reference term and at least one synonym via the user interface input.
18. The method of claim 17, wherein determining parameters of the dialog system intent further comprises:
determining a system parameter of the parameters, the system parameter being defined based on a system entity indicated by the user interface input, wherein the system entity is predefined for the dialog system.
19. The method of claim 16, further comprising:
determining an alternative value for one of the alternative parameters based on the speech input;
wherein upon determining that the values of all of the mandatory parameters are determinable, initiating performance of the action based on the values of all of the mandatory parameters without initiating the parameter collection session comprises:
the execution of the action is initiated based on the selectable value and the values of all of the forcing parameters.
20. The method of claim 19, further comprising:
determining that no alternative values for additional ones of the alternative parameters are determinable based on the voice input;
Wherein when it is determined that the values of all of the mandatory parameters are determinable, initiating performance of the action based on the values of all of the mandatory parameters without initiating the parameter collection session comprises:
execution of the action is initiated based on the selectable values, the values of all of the mandatory parameters, and without any values of additional ones of the selectable parameters.
21. The method of claim 16, further comprising:
determining that no alternative value for one of the alternative parameters is determinable based on the voice input;
wherein when it is determined that the values of all of the mandatory parameters are not determinable, performing the parameter collection session until the values of all of the mandatory parameters are determined comprises:
during the parameter collection session, determining values of all of the mandatory parameters without determining any value of one of the optional parameters; and
wherein after performing the parameter collection session, initiating performance of the action based on values of all of the mandatory parameters comprises:
execution of the action is initiated based on the values of all of the mandatory parameters, but without any value of one of the optional parameters.
22. The method of claim 16, wherein the action is performed by the client device or by a server in communication with the client device.
23. The method of claim 22, wherein the action comprises an Application Programming Interface (API) -specific response.
24. A system comprising at least one processor and a memory storing processor executable code, wherein the at least one processor is configured to, when executing the processor executable code, perform the following:
based on user interface input from a developer of a dialog agent creating the dialog system:
determining a dialog system intent of the dialog agent,
determining a plurality of parameters for the dialog system intent,
determining for each of the parameters whether it is a mandatory parameter of the dialog system intent or an optional parameter of the dialog system intent, and
determining an action to be performed when the dialog system is intended to be triggered and at least the value of the forcing parameter is decided, wherein the action is performed based on the decided value;
associating the dialog system intent with the action and associating the parameter with the dialog system intent and with an indication of whether they are mandatory or optional parameters;
After association:
receiving voice input provided via a dialog system interface of a client device, the dialog system interface being associated with the dialog system;
determining the dialog system intent to activate the dialog agent based on the processing of the voice input;
in response to the dialog system intent activating the dialog agent:
determining whether values of all of the mandatory parameters for the dialog system intent are determinable without initiating a parameter collection dialog;
when it is determined that the values for all of the mandatory parameters are determinable without initiating the parameter collection session:
initiating performance of the action based on values of all of the mandatory parameters without initiating the parameter collection dialogue;
when it is determined that the values for all of the mandatory parameters are not determinable without initiating the parameter collection session:
performing a parameter collection session until all values of said mandatory parameters are determined; and
after executing the parameter collection session, the execution of the action is initiated based on the values of all of the mandatory parameters.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562244560P | 2015-10-21 | 2015-10-21 | |
US62/244,560 | 2015-10-21 | ||
PCT/US2016/058193 WO2017070522A1 (en) | 2015-10-21 | 2016-10-21 | Parameter collection and automatic dialog generation in dialog systems |
CN201680061207.7A CN108701454B (en) | 2015-10-21 | 2016-10-21 | Parameter collection and automatic dialog generation in dialog systems |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680061207.7A Division CN108701454B (en) | 2015-10-21 | 2016-10-21 | Parameter collection and automatic dialog generation in dialog systems |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116628157A true CN116628157A (en) | 2023-08-22 |
Family
ID=57209944
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680061207.7A Active CN108701454B (en) | 2015-10-21 | 2016-10-21 | Parameter collection and automatic dialog generation in dialog systems |
CN202310573301.2A Pending CN116628157A (en) | 2015-10-21 | 2016-10-21 | Parameter collection and automatic dialog generation in dialog systems |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680061207.7A Active CN108701454B (en) | 2015-10-21 | 2016-10-21 | Parameter collection and automatic dialog generation in dialog systems |
Country Status (9)
Country | Link |
---|---|
US (3) | US10170106B2 (en) |
EP (1) | EP3341933A1 (en) |
JP (2) | JP6960914B2 (en) |
KR (2) | KR102189855B1 (en) |
CN (2) | CN108701454B (en) |
DE (1) | DE112016004863T5 (en) |
GB (1) | GB2557532A (en) |
RU (1) | RU2018113724A (en) |
WO (1) | WO2017070522A1 (en) |
Families Citing this family (166)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US9634855B2 (en) | 2010-05-13 | 2017-04-25 | Alexander Poltorak | Electronic personal interactive device that determines topics of interest using a conversational agent |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
US10387888B2 (en) | 2016-07-08 | 2019-08-20 | Asapp, Inc. | Assisting entities in responding to a request of a user |
US10083451B2 (en) | 2016-07-08 | 2018-09-25 | Asapp, Inc. | Using semantic processing for customer support |
US10891152B2 (en) * | 2016-11-23 | 2021-01-12 | Amazon Technologies, Inc. | Back-end task fulfillment for dialog-driven applications |
US10565989B1 (en) * | 2016-12-16 | 2020-02-18 | Amazon Technogies Inc. | Ingesting device specific content |
US10109275B2 (en) | 2016-12-19 | 2018-10-23 | Asapp, Inc. | Word hash language model |
US10740373B2 (en) * | 2017-02-08 | 2020-08-11 | International Business Machines Corporation | Dialog mechanism responsive to query context |
US11171892B2 (en) * | 2017-02-27 | 2021-11-09 | Ncr Corporation | Service assistance and integration |
US10332505B2 (en) * | 2017-03-09 | 2019-06-25 | Capital One Services, Llc | Systems and methods for providing automated natural language dialogue with customers |
US10796088B2 (en) * | 2017-04-21 | 2020-10-06 | International Business Machines Corporation | Specifying a conversational computer agent and its outcome with a grammar |
KR102388539B1 (en) * | 2017-04-30 | 2022-04-20 | 삼성전자주식회사 | Electronic apparatus for processing user utterance |
US11960844B2 (en) * | 2017-05-10 | 2024-04-16 | Oracle International Corporation | Discourse parsing using semantic and syntactic relations |
US11373632B2 (en) * | 2017-05-10 | 2022-06-28 | Oracle International Corporation | Using communicative discourse trees to create a virtual persuasive dialogue |
US10839154B2 (en) | 2017-05-10 | 2020-11-17 | Oracle International Corporation | Enabling chatbots by detecting and supporting affective argumentation |
US10599885B2 (en) | 2017-05-10 | 2020-03-24 | Oracle International Corporation | Utilizing discourse structure of noisy user-generated content for chatbot learning |
US10679011B2 (en) | 2017-05-10 | 2020-06-09 | Oracle International Corporation | Enabling chatbots by detecting and supporting argumentation |
US11386274B2 (en) | 2017-05-10 | 2022-07-12 | Oracle International Corporation | Using communicative discourse trees to detect distributed incompetence |
CN110612525B (en) * | 2017-05-10 | 2024-03-19 | 甲骨文国际公司 | Enabling a tutorial analysis by using an alternating speech tree |
US11586827B2 (en) | 2017-05-10 | 2023-02-21 | Oracle International Corporation | Generating desired discourse structure from an arbitrary text |
US10817670B2 (en) * | 2017-05-10 | 2020-10-27 | Oracle International Corporation | Enabling chatbots by validating argumentation |
US11615145B2 (en) | 2017-05-10 | 2023-03-28 | Oracle International Corporation | Converting a document into a chatbot-accessible form via the use of communicative discourse trees |
DK180048B1 (en) | 2017-05-11 | 2020-02-04 | Apple Inc. | MAINTAINING THE DATA PROTECTION OF PERSONAL INFORMATION |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
US10275651B2 (en) * | 2017-05-16 | 2019-04-30 | Google Llc | Resolving automated assistant requests that are based on image(s) and/or other sensor data |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
US11043206B2 (en) | 2017-05-18 | 2021-06-22 | Aiqudo, Inc. | Systems and methods for crowdsourced actions and commands |
US11056105B2 (en) * | 2017-05-18 | 2021-07-06 | Aiqudo, Inc | Talk back from actions in applications |
US11340925B2 (en) | 2017-05-18 | 2022-05-24 | Peloton Interactive Inc. | Action recipes for a crowdsourced digital assistant system |
EP3635578A4 (en) | 2017-05-18 | 2021-08-25 | Aiqudo, Inc. | Systems and methods for crowdsourced actions and commands |
US11436265B2 (en) * | 2017-06-13 | 2022-09-06 | Microsoft Technology Licensing, Llc | System for presenting tailored content based on user sensibilities |
US10839161B2 (en) | 2017-06-15 | 2020-11-17 | Oracle International Corporation | Tree kernel learning for text classification into classes of intent |
US11100144B2 (en) | 2017-06-15 | 2021-08-24 | Oracle International Corporation | Data loss prevention system for cloud security based on document discourse analysis |
CN107342078B (en) * | 2017-06-23 | 2020-05-05 | 上海交通大学 | Conversation strategy optimized cold start system and method |
US10388285B2 (en) | 2017-08-31 | 2019-08-20 | International Business Machines Corporation | Generating chat bots from web API specifications |
US11182412B2 (en) | 2017-09-27 | 2021-11-23 | Oracle International Corporation | Search indexing using discourse trees |
US11809825B2 (en) | 2017-09-28 | 2023-11-07 | Oracle International Corporation | Management of a focused information sharing dialogue based on discourse trees |
JP7214719B2 (en) | 2017-09-28 | 2023-01-30 | オラクル・インターナショナル・コーポレイション | Allow autonomous agents to distinguish between questions and requests |
JP7187545B2 (en) | 2017-09-28 | 2022-12-12 | オラクル・インターナショナル・コーポレイション | Determining Cross-Document Rhetorical Connections Based on Parsing and Identifying Named Entities |
US10431219B2 (en) * | 2017-10-03 | 2019-10-01 | Google Llc | User-programmable automated assistant |
KR102348124B1 (en) * | 2017-11-07 | 2022-01-07 | 현대자동차주식회사 | Apparatus and method for recommending function of vehicle |
DE102017128651A1 (en) * | 2017-12-02 | 2019-06-06 | Tobias Rückert | Dialogue system and method for implementing a user's instructions |
US10497004B2 (en) | 2017-12-08 | 2019-12-03 | Asapp, Inc. | Automating communications using an intent classifier |
US10599640B2 (en) | 2017-12-19 | 2020-03-24 | At&T Intellectual Property I, L.P. | Predictive search with context filtering |
KR101944353B1 (en) * | 2017-12-20 | 2019-04-17 | 주식회사 카카오 | Method and apparatus for providing chatbot builder user interface |
CN109949800B (en) * | 2017-12-20 | 2021-08-10 | 北京京东尚科信息技术有限公司 | Voice taxi taking method and system |
US11024294B2 (en) | 2017-12-29 | 2021-06-01 | DMAI, Inc. | System and method for dialogue management |
US11003860B2 (en) | 2017-12-29 | 2021-05-11 | DMAI, Inc. | System and method for learning preferences in dialogue personalization |
WO2019133689A1 (en) | 2017-12-29 | 2019-07-04 | DMAI, Inc. | System and method for selective animatronic peripheral response for human machine dialogue |
EP3732676A4 (en) | 2017-12-29 | 2021-09-22 | DMAI, Inc. | System and method for intelligent initiation of a man-machine dialogue based on multi-modal sensory inputs |
US10489792B2 (en) | 2018-01-05 | 2019-11-26 | Asapp, Inc. | Maintaining quality of customer support messages |
CN111699483B (en) * | 2018-01-16 | 2024-04-09 | 谷歌有限责任公司 | Systems, methods, and apparatus providing assistant deep links to effect third party conversation session transfer |
EP3746916A1 (en) | 2018-01-30 | 2020-12-09 | Oracle International Corporation | Using communicative discourse trees to detect a request for an explanation |
US11537645B2 (en) | 2018-01-30 | 2022-12-27 | Oracle International Corporation | Building dialogue structure by using communicative discourse trees |
CN108376543B (en) * | 2018-02-11 | 2021-07-13 | 深圳创维－Rgb电子有限公司 | Control method, device, equipment and storage medium for electrical equipment |
US10210244B1 (en) * | 2018-02-12 | 2019-02-19 | Asapp, Inc. | Updating natural language interfaces by processing usage data |
WO2019160613A1 (en) | 2018-02-15 | 2019-08-22 | DMAI, Inc. | System and method for dynamic program configuration |
KR102445365B1 (en) * | 2018-03-19 | 2022-09-20 | 현대자동차주식회사 | Dialogue processing apparatus, vehicle having the same and dialogue processing method |
WO2019182586A1 (en) | 2018-03-21 | 2019-09-26 | Google Llc | Data transfer in secure processing environments |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
WO2019203859A1 (en) | 2018-04-19 | 2019-10-24 | Google Llc | Dependency graph conversation modeling for use in conducting human-to-computer dialog sessions with a computer-implemented automated assistant |
US10169315B1 (en) | 2018-04-27 | 2019-01-01 | Asapp, Inc. | Removing personal information from text using a neural network |
US10679622B2 (en) | 2018-05-01 | 2020-06-09 | Google Llc | Dependency graph generation in a networked system |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
EP3791292A1 (en) | 2018-05-09 | 2021-03-17 | Oracle International Corporation | Constructing imaginary discourse trees to improve answering convergent questions |
US10572232B2 (en) | 2018-05-17 | 2020-02-25 | International Business Machines Corporation | Automatically converting a textual data prompt embedded within a graphical user interface (GUI) to a widget |
US11520989B1 (en) * | 2018-05-17 | 2022-12-06 | Workday, Inc. | Natural language processing with keywords |
EP3576084B1 (en) * | 2018-05-29 | 2020-09-30 | Christoph Neumann | Efficient dialog design |
CN110556102B (en) * | 2018-05-30 | 2023-09-05 | 蔚来(安徽)控股有限公司 | Method, apparatus, in-vehicle voice dialogue system, and computer storage medium for intention recognition and execution |
US11455494B2 (en) | 2018-05-30 | 2022-09-27 | Oracle International Corporation | Automated building of expanded datasets for training of autonomous agents |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11157704B2 (en) * | 2018-06-18 | 2021-10-26 | DataChat.ai | Constrained natural language processing |
US11645459B2 (en) | 2018-07-02 | 2023-05-09 | Oracle International Corporation | Social autonomous agent implementation using lattice queries and relevancy detection |
US10824658B2 (en) | 2018-08-02 | 2020-11-03 | International Business Machines Corporation | Implicit dialog approach for creating conversational access to web content |
US10915588B2 (en) * | 2018-08-02 | 2021-02-09 | International Business Machines Corporation | Implicit dialog approach operating a conversational access interface to web content |
US11216510B2 (en) | 2018-08-03 | 2022-01-04 | Asapp, Inc. | Processing an incomplete message with a neural network to generate suggested messages |
US10839159B2 (en) * | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11100407B2 (en) * | 2018-10-10 | 2021-08-24 | International Business Machines Corporation | Building domain models from dialog interactions |
WO2020073248A1 (en) * | 2018-10-10 | 2020-04-16 | 华为技术有限公司 | Human-computer interaction method and electronic device |
US11562135B2 (en) * | 2018-10-16 | 2023-01-24 | Oracle International Corporation | Constructing conclusive answers for autonomous agents |
CN111104490B (en) * | 2018-10-25 | 2023-06-06 | 阿里巴巴集团控股有限公司 | Parameter deleting method and device |
US11501656B2 (en) * | 2018-10-29 | 2022-11-15 | Sestek Ses Ve Iletisim Bilgisayar Tek. San Ve Tic A.S. | Interactive and automated training system using real interactions |
US11551004B2 (en) | 2018-11-13 | 2023-01-10 | Asapp, Inc. | Intent discovery with a prototype classifier |
US10747957B2 (en) | 2018-11-13 | 2020-08-18 | Asapp, Inc. | Processing communications using a prototype classifier |
US11416785B2 (en) * | 2018-12-04 | 2022-08-16 | International Business Machines Corporation | Automated interactive support |
CN109739481A (en) * | 2018-12-11 | 2019-05-10 | 北京奇点机智科技有限公司 | Generate the method and system of interactive application |
CN109783608B (en) * | 2018-12-20 | 2021-01-05 | 出门问问信息科技有限公司 | Target hypothesis determination method and device, readable storage medium and electronic equipment |
US11164574B2 (en) | 2019-01-03 | 2021-11-02 | International Business Machines Corporation | Conversational agent generation |
JP2022520763A (en) * | 2019-02-08 | 2022-04-01 | アイ・ティー スピークス エル・エル・シー | Methods, systems, and computer program products for developing dialog templates for intelligent industry assistants |
US11321536B2 (en) | 2019-02-13 | 2022-05-03 | Oracle International Corporation | Chatbot conducting a virtual social dialogue |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
CA3133829A1 (en) * | 2019-03-19 | 2020-09-24 | Liveperson, Inc. | Dynamic communications routing to disparate endpoints |
CN111737408B (en) * | 2019-03-25 | 2024-05-03 | 阿里巴巴集团控股有限公司 | Dialogue method and equipment based on script, and electronic equipment |
US11715467B2 (en) | 2019-04-17 | 2023-08-01 | Tempus Labs, Inc. | Collaborative artificial intelligence method and system |
CN110046227B (en) * | 2019-04-17 | 2023-07-18 | 腾讯科技（深圳）有限公司 | Configuration method, interaction method, device, equipment and storage medium of dialogue system |
US11637792B2 (en) * | 2019-04-19 | 2023-04-25 | Oracle International Corporation | Systems and methods for a metadata driven integration of chatbot systems into back-end application services |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
CN110096583B (en) * | 2019-05-09 | 2021-05-14 | 思必驰科技股份有限公司 | Multi-field dialogue management system and construction method thereof |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US10671941B1 (en) * | 2019-05-23 | 2020-06-02 | Capital One Services, Llc | Managing multifaceted, implicit goals through dialogue |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
US11227599B2 (en) | 2019-06-01 | 2022-01-18 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
CA3045132C (en) * | 2019-06-03 | 2023-07-25 | Eidos Interactive Corp. | Communication with augmented reality virtual agents |
WO2021021012A1 (en) * | 2019-07-29 | 2021-02-04 | Ai Robotics Limited | Stickering method and system for linking contextual text elements to actions |
US11449682B2 (en) | 2019-08-29 | 2022-09-20 | Oracle International Corporation | Adjusting chatbot conversation to user personality and mood |
CN110798506B (en) * | 2019-09-27 | 2023-03-10 | 华为技术有限公司 | Method, device and equipment for executing command |
US11425064B2 (en) | 2019-10-25 | 2022-08-23 | Asapp, Inc. | Customized message suggestion with user embedding vectors |
WO2021096382A2 (en) * | 2019-11-15 | 2021-05-20 | Федеральное Государственное Автономное Образовательное Учреждение Высшего Образования "Московский Физико-Технический Институт" (Национальный Исследовательский Университет) (Мфти) | Method and system of controlling a conversation agent in a user interface channel |
US11775772B2 (en) | 2019-12-05 | 2023-10-03 | Oracle International Corporation | Chatbot providing a defeating reply |
KR20210113488A (en) | 2020-03-05 | 2021-09-16 | 삼성전자주식회사 | A method and an apparatus for automatic extraction of voice agent new functions by means of usage logs analytics |
US11501211B2 (en) | 2020-03-13 | 2022-11-15 | Hong Kong Applied Science And Technology Research Institute Co., Ltd | Chatbot system with model lifecycle management |
CN111312254A (en) * | 2020-03-26 | 2020-06-19 | 镁佳(北京)科技有限公司 | Voice conversation method and device |
US11580112B2 (en) | 2020-03-31 | 2023-02-14 | Pricewaterhousecoopers Llp | Systems and methods for automatically determining utterances, entities, and intents based on natural language inputs |
US11482223B2 (en) | 2020-03-31 | 2022-10-25 | Pricewaterhousecoopers Llp | Systems and methods for automatically determining utterances, entities, and intents based on natural language inputs |
US11095579B1 (en) | 2020-05-01 | 2021-08-17 | Yseop Sa | Chatbot with progressive summary generation |
US11061543B1 (en) | 2020-05-11 | 2021-07-13 | Apple Inc. | Providing relevant data items based on context |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
WO2021255484A1 (en) * | 2020-06-18 | 2021-12-23 | Citrix Systems, Inc. | Autonomous learning of entity values in artificial intelligence conversational systems |
US20220019195A1 (en) * | 2020-07-15 | 2022-01-20 | Automation Anywhere, Inc. | Robotic process automation with conversational user interface |
US11539650B2 (en) | 2020-07-16 | 2022-12-27 | International Business Machines Corporation | System and method for alerts for missing coverage of chatbot conversation messages |
US11490204B2 (en) | 2020-07-20 | 2022-11-01 | Apple Inc. | Multi-device audio adjustment coordination |
US11438683B2 (en) | 2020-07-21 | 2022-09-06 | Apple Inc. | User identification using headphones |
CN111914075A (en) * | 2020-08-06 | 2020-11-10 | 平安科技（深圳）有限公司 | Customer label determination method, device, equipment and medium based on deep learning |
US11816437B2 (en) * | 2020-12-15 | 2023-11-14 | International Business Machines Corporation | Automatical process application generation |
JP2022147362A (en) * | 2021-03-23 | 2022-10-06 | 株式会社リコー | Service providing system, information processing apparatus, information processing method, and program |
US11683283B2 (en) | 2021-03-30 | 2023-06-20 | International Business Machines Corporation | Method for electronic messaging |
US11533279B2 (en) | 2021-03-30 | 2022-12-20 | International Business Machines Corporation | Method for electronic messaging using image based noisy content |
US20220334709A1 (en) | 2021-04-14 | 2022-10-20 | DataChat.ai | User interface for data analytics systems |
JP6954549B1 (en) * | 2021-06-15 | 2021-10-27 | ソプラ株式会社 | Automatic generators and programs for entities, intents and corpora |
CN114265920B (en) * | 2021-12-27 | 2022-07-01 | 北京易聊科技有限公司 | Intelligent robot conversation method and system based on signals and scenes |
Family Cites Families (34)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6078886A (en) * | 1997-04-14 | 2000-06-20 | At&T Corporation | System and method for providing remote automatic speech recognition services via a packet network |
US7137126B1 (en) * | 1998-10-02 | 2006-11-14 | International Business Machines Corporation | Conversational computing via conversational virtual machine |
US6868385B1 (en) * | 1999-10-05 | 2005-03-15 | Yomobile, Inc. | Method and apparatus for the provision of information signals based upon speech recognition |
US9076448B2 (en) * | 1999-11-12 | 2015-07-07 | Nuance Communications, Inc. | Distributed real time speech recognition system |
WO2001084535A2 (en) * | 2000-05-02 | 2001-11-08 | Dragon Systems, Inc. | Error correction in speech recognition |
JP3949356B2 (en) * | 2000-07-12 | 2007-07-25 | 三菱電機株式会社 | Spoken dialogue system |
US7308404B2 (en) * | 2001-09-28 | 2007-12-11 | Sri International | Method and apparatus for speech recognition using a dynamic vocabulary |
US6999930B1 (en) * | 2002-03-27 | 2006-02-14 | Extended Systems, Inc. | Voice dialog server method and system |
US7302383B2 (en) * | 2002-09-12 | 2007-11-27 | Luis Calixto Valles | Apparatus and methods for developing conversational applications |
US7228275B1 (en) * | 2002-10-21 | 2007-06-05 | Toyota Infotechnology Center Co., Ltd. | Speech recognition system having multiple speech recognizers |
US7548858B2 (en) * | 2003-03-05 | 2009-06-16 | Microsoft Corporation | System and method for selective audible rendering of data to a user based on user input |
US8311835B2 (en) * | 2003-08-29 | 2012-11-13 | Microsoft Corporation | Assisted multi-modal dialogue |
US7266537B2 (en) * | 2004-01-14 | 2007-09-04 | Intelligent Results | Predictive selection of content transformation in predictive modeling systems |
US8024196B1 (en) * | 2005-09-19 | 2011-09-20 | Sap Ag | Techniques for creating and translating voice applications |
US9009046B1 (en) * | 2005-09-27 | 2015-04-14 | At&T Intellectual Property Ii, L.P. | System and method for disambiguating multiple intents in a natural language dialog system |
US8332218B2 (en) * | 2006-06-13 | 2012-12-11 | Nuance Communications, Inc. | Context-based grammars for automated speech recognition |
US9318108B2 (en) * | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US7752043B2 (en) * | 2006-09-29 | 2010-07-06 | Verint Americas Inc. | Multi-pass speech analytics |
JP4451435B2 (en) * | 2006-12-06 | 2010-04-14 | 本田技研工業株式会社 | Language understanding device, language understanding method, and computer program |
US20110060587A1 (en) * | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US8140335B2 (en) * | 2007-12-11 | 2012-03-20 | Voicebox Technologies, Inc. | System and method for providing a natural language voice user interface in an integrated voice navigation services environment |
US8874443B2 (en) * | 2008-08-27 | 2014-10-28 | Robert Bosch Gmbh | System and method for generating natural language phrases from user utterances in dialog systems |
US8700389B2 (en) * | 2010-12-23 | 2014-04-15 | Sap Ag | Systems and methods for model-based processing of linguistic user inputs using annotations |
US9082402B2 (en) * | 2011-12-08 | 2015-07-14 | Sri International | Generic virtual personal assistant platform |
US8892419B2 (en) * | 2012-04-10 | 2014-11-18 | Artificial Solutions Iberia SL | System and methods for semiautomatic generation and tuning of natural language interaction applications |
WO2013155619A1 (en) * | 2012-04-20 | 2013-10-24 | Sam Pasupalak | Conversational agent |
US10096316B2 (en) * | 2013-11-27 | 2018-10-09 | Sri International | Sharing intents to provide virtual assistance in a multi-person dialog |
WO2014209157A1 (en) * | 2013-06-27 | 2014-12-31 | Obschestvo S Ogranichennoy Otvetstvennostiyu "Speaktoit" | Generating dialog recommendations for chat information systems |
US10811004B2 (en) * | 2013-03-28 | 2020-10-20 | Nuance Communications, Inc. | Auto-generation of parsing grammars from a concept ontology |
US9110889B2 (en) * | 2013-04-23 | 2015-08-18 | Facebook, Inc. | Methods and systems for generation of flexible sentences in a social networking system |
US9633317B2 (en) * | 2013-06-20 | 2017-04-25 | Viv Labs, Inc. | Dynamically evolving cognitive architecture system based on a natural language intent interpreter |
US20150242395A1 (en) * | 2014-02-24 | 2015-08-27 | Transcriptic, Inc. | Systems and methods for equipment sharing |
CN107112013B (en) * | 2014-09-14 | 2020-10-23 | 谷歌有限责任公司 | Platform for creating customizable dialog system engines |
CN104360897B (en) * | 2014-10-29 | 2017-09-22 | 百度在线网络技术（北京）有限公司 | Dialog process method and dialog management system |
-
2016
- 2016-10-21 US US15/331,203 patent/US10170106B2/en active Active
- 2016-10-21 DE DE112016004863.7T patent/DE112016004863T5/en not_active Ceased
- 2016-10-21 EP EP16788388.3A patent/EP3341933A1/en not_active Withdrawn
- 2016-10-21 KR KR1020207013506A patent/KR102189855B1/en active IP Right Grant
- 2016-10-21 RU RU2018113724A patent/RU2018113724A/en not_active Application Discontinuation
- 2016-10-21 JP JP2018520601A patent/JP6960914B2/en active Active
- 2016-10-21 CN CN201680061207.7A patent/CN108701454B/en active Active
- 2016-10-21 WO PCT/US2016/058193 patent/WO2017070522A1/en active Application Filing
- 2016-10-21 CN CN202310573301.2A patent/CN116628157A/en active Pending
- 2016-10-21 GB GB1805212.6A patent/GB2557532A/en not_active Withdrawn
- 2016-10-21 KR KR1020187014399A patent/KR102112814B1/en active IP Right Grant
-
2018
- 2018-12-31 US US16/237,318 patent/US10490186B2/en active Active
-
2019
- 2019-10-14 US US16/601,055 patent/US10733983B2/en active Active
-
2020
- 2020-05-26 JP JP2020091306A patent/JP6942841B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
GB2557532A (en) | 2018-06-20 |
KR102112814B1 (en) | 2020-05-19 |
RU2018113724A (en) | 2019-11-21 |
US20200111486A1 (en) | 2020-04-09 |
US20190139538A1 (en) | 2019-05-09 |
US10733983B2 (en) | 2020-08-04 |
EP3341933A1 (en) | 2018-07-04 |
JP6942841B2 (en) | 2021-09-29 |
CN108701454B (en) | 2023-05-30 |
KR102189855B1 (en) | 2020-12-11 |
US10490186B2 (en) | 2019-11-26 |
KR20180070684A (en) | 2018-06-26 |
CN108701454A (en) | 2018-10-23 |
KR20200054338A (en) | 2020-05-19 |
JP6960914B2 (en) | 2021-11-05 |
US10170106B2 (en) | 2019-01-01 |
GB201805212D0 (en) | 2018-05-16 |
DE112016004863T5 (en) | 2018-07-19 |
US20170116982A1 (en) | 2017-04-27 |
JP2019503526A (en) | 2019-02-07 |
JP2020161153A (en) | 2020-10-01 |
WO2017070522A1 (en) | 2017-04-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108701454B (en) | Parameter collection and automatic dialog generation in dialog systems | |
US11232265B2 (en) | Context-based natural language processing | |
US11321116B2 (en) | Systems and methods for integrating third party services with a digital assistant | |
US10546067B2 (en) | Platform for creating customizable dialog system engines | |
JP2019503526A5 (en) | ||
CN106164869B (en) | Hybrid client/server architecture for parallel processing | |
US9875741B2 (en) | Selective speech recognition for chat and digital personal assistant systems | |
CN112270925B (en) | Platform for creating customizable dialog system engines |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |