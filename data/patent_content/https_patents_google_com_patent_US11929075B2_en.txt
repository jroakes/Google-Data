US11929075B2 - Voice action discoverability system - Google Patents
Voice action discoverability system Download PDFInfo
- Publication number
- US11929075B2 US11929075B2 US16/936,935 US202016936935A US11929075B2 US 11929075 B2 US11929075 B2 US 11929075B2 US 202016936935 A US202016936935 A US 202016936935A US 11929075 B2 US11929075 B2 US 11929075B2
- Authority
- US
- United States
- Prior art keywords
- voice action
- user device
- discoverability
- context
- application
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000009471 action Effects 0.000 title claims abstract description 717
- 230000004044 response Effects 0.000 claims abstract description 41
- 238000000034 method Methods 0.000 claims abstract description 28
- 230000001960 triggered effect Effects 0.000 claims abstract description 26
- 230000002123 temporal effect Effects 0.000 claims description 5
- 230000000694 effects Effects 0.000 description 43
- 238000013518 transcription Methods 0.000 description 42
- 230000035897 transcription Effects 0.000 description 42
- 230000006698 induction Effects 0.000 description 18
- 238000010200 validation analysis Methods 0.000 description 18
- 230000008569 process Effects 0.000 description 13
- 238000004891 communication Methods 0.000 description 11
- 238000004590 computer program Methods 0.000 description 10
- 230000037361 pathway Effects 0.000 description 7
- 238000012545 processing Methods 0.000 description 5
- 238000011161 development Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 230000001413 cellular effect Effects 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 238000012360 testing method Methods 0.000 description 3
- 238000010304 firing Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 230000000737 periodic effect Effects 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000004075 alteration Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000002996 emotional effect Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000001228 spectrum Methods 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/72—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for transmitting results of analysis
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/225—Feedback of the input speech
Definitions
- This specification relates to voice actions, and one particular implementation relates to configuring voice actions and educating users how to use voice actions.
- This application is a continuation of application Ser. No. 15/173,823, now U.S. Pat. No. 10,049,670 issued 14 Aug. 2018, and application Ser. No. 16/101,940, now U.S. Pat. No. 10,741,183 issued 11 Aug. 2020.
- a task in an application can include one or more activities defined in software that a user interacts with to perform a certain job.
- the activity is a class that controls the life cycle of the execution of the task, such the multiple activities exercised within the task can permit the user to perform the job.
- a command can be associated with an activity or an action related to an activity, such that a user submission of the command can trigger the activity or action.
- a specific intent can be fired to trigger the starting of the activity or performance of the action.
- a user must have knowledge of the command associated with the activity or action used to fire the intent corresponding to the activity or action.
- This specification describes a platform that allows an application developer to deploy new voice actions for previously installed software applications, and implement discoverability examples that inform users how to use the deployed voice actions.
- the application developer can submit information defining the new voice action, where the information specifies the application, an action that the new voice action is directed to triggering, a trigger term for triggering the action, and a context in which the trigger term should be effective to trigger the action in the application.
- the application developer can also submit information defining discoverability examples for the new voice action, such that a user of the application can be presented with an appropriate discoverability example indicating how the user can trigger the action.
- the context submitted by the application developer can specify a user device status when the voice action should be effective to trigger the action in the application.
- a device status can include, for example, which application(s) is operating in the foreground (i.e., currently active in a user interface of the device) or in the background (i.e., currently hidden from view in the user interface of the device), or can include specific information about applications, such as what activities they are currently running, the status of running activities, and so on.
- the platform When the application developer submits information defining the new voice action, the platform, designed as a service or tool, can validate the information defining the new voice action to determine whether the new voice action is compatible with the application, or to otherwise determine whether the new voice action can be implemented. If the new voice action is valid, a new instance of a passive data structure called an intent and having a specific format can be created for the voice action by inducting the information defining the new voice action.
- the intent can specify some or all of the information defining the new voice action, such as the application, the trigger term, the activity or action to be triggered in response to detecting the trigger term, and context that is necessary for the trigger term to trigger the activity or action.
- the voice action can then be deployed for the application, such that the voice action becomes an enabled voice action for the application without additional alterations being made to the application code to support the voice action.
- a user operating a user device can provide a speech input.
- the user device can submit context information for the user device, and the context information and a transcription of the speech input can be used to identify an intent and to trigger the identified intent.
- the intent is triggered, data for executing the activity or action is determined, and is transmitted to the user device to execute the activity or action in response to the voice input.
- an application developer of a media player application can define a new voice action for skipping ahead in a song by a predetermined amount of time that uses the trigger term “skip ahead.” For example, the application developer can define a new voice action for skipping ahead by 30 seconds, 60 seconds, or by some other predetermined amount of time in a song that the media player application is currently playing.
- the application developer can specify a context when the “skip ahead” voice action should be enabled. For example, the application developer may specify that the “skip ahead” voice action should only cause the media player application to skip ahead by the predetermined amount of time in the song when the media player application is running in the foreground and is in a mode that causes the media player application to operate as an audio player.
- a user having the media player application previously installed on their user device might submit the voice input “skip ahead.”
- the user device can submit context information indicating a status of the user device or of applications installed on the user device, such as context information indicating that the media player application is running in the foreground and is in audio player mode.
- context information indicating that the media player application is running in the foreground and is in audio player mode.
- data can be transmitted to the user device that causes the media player application to skip ahead by the predetermined amount of time in a song that the media player application is currently playing.
- the “skip ahead” voice input may not have any effect, or may cause a different operation to be performed at the user device.
- the platform allows application developers to define discoverability examples that can presented to users to inform the users of available voice actions for a specific context.
- an application developer may include information that defines one or more discoverability examples for the new voice action.
- Discoverability examples may include notifications, such as textual, image, video, or audio notifications, that can be presented to a user of the software application to inform a user that the voice action is available in the present context and how the user can trigger the new voice action.
- the information submitted by the application developer defining the discoverability example can be received by the platform and used to generate data that defines the discoverability example.
- the generated data can then be provided to user devices having the application installed, and stored at the respective user devices. Storing the generated data at a user device can enable the user device to provide discoverability examples off-line.
- the user device may be capable of determining the context and selecting discoverability examples stored at the device to provide based on the context, where the selected discoverability examples correspond to voice actions that are capable of being triggered in the current context.
- the application developer may also define a discoverability example for the “skip ahead” voice action that includes the textual notification, “Try Saying ‘Skip Ahead’.”
- the platform may generate data defining the discoverability example and may transmit the generated data to one or more user devices having the media player application installed.
- the generated data may be stored at the one or more user devices.
- a user of a user device having the media player installed may request discoverability examples for a current context, for example, by providing a voice input to the user device that includes “What voice commands can I say?”
- the user device may interpret this voice input as a request for discoverability examples, and may determine a context for the user device and the media player application.
- the user device may access information for the discoverability example, and provide a textual notification at the user device that includes the text of the discoverability example, “Try Saying ‘Skip Ahead.” The user may rely on this discoverability example in determining voice actions available to the user device given the current context of the user device or the media player application.
- these and other embodiments may each optionally include one or more of the following features.
- these and other embodiments may each optionally include features comprising: receiving, by the voice action system, data corresponding to content that is presentable at a user device as a notification of the one or more of the trigger terms, and providing, by the voice action system, the data corresponding to the content to the user device in response to the request, wherein the user device is configured, based at least on the data corresponding to the content to present the content as a notification of the one or more of the trigger terms when a status of the software application satisfies the specified context.
- these and other embodiments may each optionally include features comprising: generating, by the voice action system, data defining one or more unique candidate discoverability examples for the new voice action, wherein data defining each of the one or more unique candidate discoverability examples comprises one or more of the trigger terms, receiving, by the voice action system, data indicating a selection of a particular candidate discoverability example from among the one or more unique candidate discoverability examples, and providing, by the voice action system, data defining the particular candidate discoverability example to the user device in response to the request, wherein the user device is configured, based at least on the data defining the particular candidate discoverability example, to provide a notification of the one or more of the trigger terms that corresponds to the particular candidate discoverability example when a status of the software application satisfies the specified context.
- these and other embodiments may each optionally include features comprising: receiving, by the voice action system from the user device having the software application installed, data indicating a status of the software application, determining, by the voice action system and based on the data indicating the status of the software application, that the status of the software application satisfies the specified context; and providing, by the voice action system, the data defining the discoverability example to the user device in response to determining that the status of the software application satisfies the specified context, wherein the user device is configured, based at least on the data defining the discoverability example, to receive the discoverability example and to provide a notification of the one or more of the trigger terms in response to receiving the discoverability example.
- the specified context indicates that the software application is performing a specific activity; the specified context indicates that a specific activity that the software application is performing is in a particular activity state; the specified context indicates that the software application is operating in the foreground of a user device having the software application installed; the specified context indicates that the software application is operating in the background of a user device having the software application installed.
- these and other embodiments may each optionally include features comprising: receiving, by the voice action system, data specifying (i) trigger terms that trigger the software application to perform a second new voice action and (ii) the specified context, receiving, by the voice action system, data defining a second discoverability example for the second new voice action, wherein the data defining the second discoverability example comprises one or more of the trigger terms that trigger the software application to perform the second new voice action when a status of the software application satisfies the specified context, and providing, by the voice action system, the data defining the second discoverability example to the user device in response to the request, wherein the user device is configured, based at least on the data defining the second discoverability example, to provide a notification of the one or more of the trigger terms that trigger the software application to perform the new voice action and of the one or more of the trigger terms that trigger the software application to perform the second new voice action when a status of the software application satisfies the specified context.
- a status of the software application is determined in response to detecting a user input at the user device that requests a notification of trigger terms that trigger the software application to perform voice actions; a status of the software application is determined in response to determining that a status of the software application has changed; a status of the software application is determined in response to determining that the software application has been launched at the user device; the notification of the one or more of the trigger terms is one of a textual notification, an image notification, a video notification, or an audio notification.
- these and other embodiments may each optionally include features comprising: storing, at the voice action system, the data defining the discoverability example at a database that includes data defining one or more other discoverability examples, wherein at least one of the one or more other discoverability examples is a discoverability example for a different voice action.
- the user device is configured to: determine that a status of the software application satisfies a context specified for each of two or more different voice actions, identify discoverability examples that are defined for the two or more different voice actions, wherein each of the discoverability examples comprises one or more trigger terms that trigger the software application to perform one of the two or more different voice actions, select a subset of the discoverability examples, and provide a notification of the one or more trigger terms of each of the discoverability examples included in the subset.
- selecting the subset of the discoverability examples comprises: ranking the discoverability examples that are defined for the two or more different voice actions, and selecting the subset of the discoverability examples based at least on the ranking.
- selecting the subset of the discoverability examples comprises: determining a relevance of each of the two or more different voice actions to the context specified for each of the two or more different voice actions, and selecting the subset of the discoverability examples based at least on the determined relevance of each of the two or more different voice actions to the context specified for each of the two or more different voice actions.
- selecting the subset of the discoverability examples comprises: accessing information indicating past user activity at the user device when the status of the software application satisfies the context specified for each of the two or more different voice actions, and selecting the subset of the discoverability examples based at least on the accessed information indicating the past user activity at the user device when the status of the software application satisfies the context specified for each of the two or more different voice actions.
- FIGS. 1 A, 1 B, and 1 C depicts an example system for a voice action development system and service.
- FIG. 2 depicts an example system for generating discoverability examples for voice actions using a voice action development system and service.
- FIG. 3 depicts an example system for providing discoverability examples for voice actions.
- FIG. 4 is a flowchart of an example process associated with a voice action development system and service.
- FIG. 1 A illustrates an example of a voice action system 100 .
- the voice action system 100 provides a platform and service whereby an application developer can establish new voice actions for an application that has previously been installed on other users' devices. Thereafter, users of the application having the application installed on their devices can use voice commands in order to cause the application to perform a specific operation as specified by the new voice action.
- the voice action system 100 may also provide a platform and service, discussed in further detail with respect to FIGS. 1 B and 1 C , whereby the application developer can establish discoverability examples for voice actions. The discoverability examples can be provided to users to inform them of voice actions that are available and how those voice actions can be triggered.
- the voice action system 100 receives from terminals 102 a - 102 n data defining new voice actions submitted by application developers 101 a - 101 n , wherein the new voice actions are for one or more different applications.
- the voice action system 100 can determine whether each new voice action is a valid voice action.
- Each new voice action that is determined to be valid can be inducted to generate an intent corresponding to the new voice action, and the intent corresponding to the new voice action can be stored at an application voice action database 110 of the voice action system 100 .
- the voice action system 100 may have a validation engine that receives and validates the data defining the new voice action.
- Validating the data may include determining that the formatting of the data defining the voice action complies with requirements, determining that the new voice action is compatible with the specified application, or otherwise determining that a voice action can be established based on the data received from the application developer 101 a - 101 n .
- An indication engine of the voice action system 100 can receive the validated data defining the new voice action, and can induct the data to generate the intent corresponding to the new voice action. The intent can then be stored in the application voice action database 110 .
- the application voice action database 110 can store voice actions that pertain to activities or actions that can be performed within numerous different software applications.
- the voice actions included in the application voice action database 110 may include built-in voice actions that were submitted by application developers 101 a - 101 n when the application was built, as well as voice actions that were submitted by application developers 101 a - 101 n after the application was built for operations supported by the application.
- the voice actions can also include application-specific voice actions that are supported by default by the operating system. These pre-supported voice actions may be voice actions that are capable of being implemented in applications without having to be submitted to the voice action system 100 by an application developer 101 a - 101 n .
- the voice action “exit application” to exit a task or activity running in the foreground may be automatically available for any application, and may be included in the application voice action database 110 for each application, without an application developer 101 a - 101 n having to submit information to the voice action system 100 to define the “exit application” voice action.
- the voice action system 100 can include an operating system (OS) voice action database 120 that stores voice actions that are not associated with any particular application or context.
- OS operating system
- a “lock phone” voice action that causes a device to enter a locked state may be a voice action that is specified in the OS voice action database 120 as a voice action that is not associated with a particular application, or that is universally accessible, i.e., regardless of a device's status when the “lock device” voice action is detected.
- the voice actions stored in the OS voice action database 120 are voice actions that are not generated based on application developers 101 a - 101 n defining the voice actions, since the voice actions stored at the OS voice action database 120 are generic to a user device operating environment and not any particular application generated by third party application developers 101 a - 101 n.
- the voice action system 100 enables a user 105 having a user device 104 to provide spoken voice input to their user device 104 to cause actions to be performed by the user device 104 or applications operating on their user device 104 .
- the user 105 having the user device 104 has a media player application running on their user device 104 and provides the spoken input “skip ahead” while the media player application is running in the foreground of the user device 104 and is operating the media player application in an audio player mode.
- the audio data corresponding to the voice input “skip ahead” is received by a speech recognition engine 130 of the voice action system 100
- the context information indicating the status of the user device 104 is received by a context analysis engine 140 of the voice action system 100 .
- the context information may indicate that the media player application is running in the foreground of the user device 104 , that the media player application is currently in an audio player mode, and may indicate other information about the status of the user device 104 and applications installed on the user device 104 .
- the speech recognition engine 130 receives the audio data corresponding to the voice input, generates a transcription of the voice input, and provides the transcription of the voice input to the voice action selector 150 .
- the context analysis engine 140 receives the context information from the user device 104 and processes the context information to determine relevant context information. For example, the context analysis engine 140 may analyze the context information to identify applications that are installed on the user device 104 , and metadata associated with each of those applications may specify available voice actions for the application and context information that is relevant to determining which of the voice actions might be enabled for a given device status.
- the context analysis engine 140 may determine that additional context information is required to identify which voice actions should be enabled for a particular status of the user device 104 , and so the context analysis engine 140 may request additional context information from the user device 104 .
- the context analysis engine 140 forwards the processed context information to the voice action selector 150 .
- the speech recognition engine 130 may receive the audio data corresponding to the voice input “skip ahead” and may obtain a transcription of the voice input.
- the context analysis engine 140 receives context information from the user device 104 that specifies that the media player application is running in the foreground, that the media player application is operating in an audio player mode, and that specifies other information about the user device 104 .
- the context information may also indicate that a social network application is operating in the background of the user device 104 and that the battery life of the user device 104 is currently 50%.
- the context analysis engine 150 may receive information indicating that the user device 104 has both the media player and social network applications installed and may determine that neither the media player application nor the social network application enables voice actions based on context information that indicates a device battery life. Therefore, the context analysis engine 140 may filter the context information to indicate only that the media player application is operating in the foreground of the user device 104 , that the media player application is in an audio player mode, and that the social network application is operating in the background of the user device 104 . The filtered context information and the transcription of the voice input “skip ahead” may then be provided to the voice action selector 150 .
- the voice action selector 150 receives the transcription of the spoken input from the speech recognition engine 130 and a context from the context analysis engine 140 that includes processed context information.
- the voice action selector 150 uses the transcription and the context to identify a particular intent associated with a particular voice action to trigger at the user device 104 .
- the voice action selector 150 can access the application voice action database 110 and the OS voice action database 120 to identify a set of candidate voice actions that are enabled for the present context of the user device 104 . Identifying the set of candidate voice actions can include identifying a set of intents stored at the application voice action database 110 or the OS voice action database 120 that specify contexts matching the context received by the voice action selector 150 .
- the voice action selector 150 can compare the transcription of the voice input to one or more trigger phrases associated with each of the enabled voice actions.
- a trigger phrase can include one or more trigger terms, and operates as an identifier for a voice action, such that detection of the one or more terms of a particular trigger phrase results in identification and triggering of the voice action corresponding to the particular trigger phrase.
- the voice action selector 150 can compare the transcription to respective trigger phrases associated with the intents of the enabled voice actions.
- the voice action selector 150 identifies a particular intent for a particular voice action based on determining that the transcription matches a trigger term specified by the intent associated with the particular voice action.
- the voice action selector 150 can receive the transcription of the voice input “skip ahead” and the context for the user device 104 , and can use the received context to identify candidate voice actions to initiate at the user device 104 , i.e., voice actions that can be initiated at the user device 104 based on the current status of the user device 104 .
- the voice action selector 150 compares the transcription of the voice input “skip ahead” to trigger phrases specified by the intents of the candidate voice actions. The voice action selector 150 can determine that the transcription “skip ahead” matches the trigger phrase specified by the intent of the “skip ahead” voice action for the media player application.
- the “skip ahead” voice action may be a voice action for causing the media player application to skip ahead by a predetermined amount of time in a song that the media player application is currently playing.
- the “skip ahead” voice action may cause the media player application to fast forward by a predetermined amount of time, such as 30 seconds, 60 seconds, or other amount of time specified, for example, by an application developer 101 a - 101 n or user 105 .
- a predetermined amount of time such as 30 seconds, 60 seconds, or other amount of time specified, for example, by an application developer 101 a - 101 n or user 105 .
- the media player application may skip ahead in the song to a point 1 minute and 45 seconds into the song.
- the media player application may cause the media player application to skip ahead to a next song.
- a transcription is identified as matching a trigger phrase based on determining that at least a portion of the transcription matches one or more terms of the trigger phrase. For example, a match between a transcription and a trigger phrase may be identified based on each of the terms of the transcription matching each of the terms of the trigger phrase. In some implementations, a match may be identified based on determining that less than all of the terms of the transcription match terms of the trigger phrase. For example, a transcription may match a trigger phrase even if some of the terms of the trigger phrase are absent from the transcription, if the transcription includes terms in addition to those of the trigger phrase, or if one or more terms of the transcription are different from terms of the trigger phrase.
- each of the transcriptions “new email,” “send new email,” or “open up new email” may be identified as matching the trigger phrase “open new email,” despite each of the transcriptions failing to precisely match the trigger phrase.
- a transcription may be identified as matching a trigger phrase if the transcription includes some or all of the terms of the trigger phrase in a different order than the order specified by the trigger phrase.
- the transcription “lock phone” may be identified as matching a “phone lock” trigger phrase.
- a transcription may be identified as matching a trigger phrase based on determining that the transcription includes one or more synonyms or alternate terms for terms of the trigger phrase.
- the transcriptions “draft new email” or “open new email” may be identified as matching the trigger phrase “write new email” based on “draft” being a synonym of “write,” and “open” being an identified alternate term for “write.”
- the voice action selector 150 provides action trigger data to the user device 104 that causes the activity or action associated with the particular voice action to be initiated. To do so, the voice action selector 150 can obtain information for controlling the user device 104 to perform the action or activity associated with the selected voice action. In some instances, controlling the user device 104 to perform the action or activity associated with the selected voice action can include firing an intent of the selected voice action. Firing the intent of the selected voice action may cause information for initiating the activity or action associated with the selected voice action to be provided to the user device 104 .
- the intent for the selected voice action can include data that causes actions or activities associated with the voice action to be performed by the application associated with the voice action.
- the selected intent can be transmitted to the user device 104 , such that receipt of the intent by the user device 104 can act as a request or can trigger performance of the actions or activities associated with the voice action.
- the voice action selector 150 can determine other data to operate as action trigger data that causes the selected voice action to be performed at the user device 104 , and can transmit the information to the user device 104 .
- an intent may only identify the actions or activities to be performed by the user device 104 to perform the voice action, and the voice action selector 150 can determine action trigger data that can control the application on the user device 104 to perform the actions or activities associated with the voice action.
- the determined action trigger data can be transmitted to the user device 104 such that the action trigger data causes the actions or activities associated with the voice action to be performed.
- the voice action selector 150 transmits the intent for the “skip ahead” voice action to the user device 104 , or obtains other information for controlling the media player application running on the user device 104 to skip ahead by a predetermined amount of time in a song that the media player application is currently playing.
- the voice action selector 150 transmits the data for controlling the media player application to skip ahead by the predetermined amount of time in the song to the user device 104 , which in turn initiates a process for the media player application running on the user device 104 to skip ahead by the predetermined amount of time in the song, for example, to skip ahead by the predetermined amount of time in the song “Fame” by David Bowie.
- the system of FIG. 1 A includes one or more terminals 102 a - 102 n corresponding to one or more application developers 101 a - 101 n .
- the terminals 102 a - 102 n can each be in communication with the voice action system 100 , for example, over one or more wired or wireless networks.
- Each of the terminals 102 a - 102 n can be a mobile device, such as a cellular phone, smartphone, tablet computer, laptop computer, personal digital assistant (PDA), netbook computer, or other mobile computing device, or may be any stationary computing device, such as a desktop computer or other stationary computer.
- PDA personal digital assistant
- a user device 104 associated with a user 105 can also be in communication with the voice action system 100 over one or more wired or wireless networks, where the user device 104 may also be a mobile or stationary computing device, such as a cellular phone, smartphone, tablet computer, netbook, personal digital assistant, laptop computer, desktop computer, or other computing device.
- a mobile or stationary computing device such as a cellular phone, smartphone, tablet computer, netbook, personal digital assistant, laptop computer, desktop computer, or other computing device.
- the example voice action system 100 shown in FIG. 1 A includes an application voice action database 110 , operating system (OS) voice action database 120 , speech recognition engine 130 , context analysis engine 140 , and voice action selector 150 .
- Each of the components of the voice action system 100 including the application voice action database 110 , OS voice action database 120 , speech recognition engine 130 , context analysis engine 140 , and voice action selector 150 , may be in communication with one or more other components of the voice action system 100 over one or more wired or wireless data pathways that enable that exchange of electronic communications.
- one or more of the components of the voice action system 100 may be combined such that their functions are performed by a single component, or may be represented by two or more components such that their functions are dispersed across the two or more components.
- the components of the voice action system 100 may be implemented on a single computing device, such as a single server system, or may be implemented on multiple computing devices that are in communication over one or more wired or wireless data pathways that enable the exchange of electronic communications between the components.
- FIG. 1 B depicts a voice action system 100 that enables application developers to establish discoverability examples for voice actions, and to provide appropriate discoverability examples to users.
- Discoverability examples submitted by an application developer may pertain to a new voice action established by the application developer for a particular application, and may be presented at user devices having the application installed to inform users how to trigger the new voice action.
- the voice action system 100 receives data from terminals 102 a - 102 n that specifies discoverability examples submitted by application developers 101 a - 101 n .
- Each discoverability example submitted to the voice action system 100 by an application developer 101 a - 101 n may pertain to a new voice action submitted by the application developer 101 a - 101 n for a particular software application.
- Discoverability examples submitted by application developers can be stored at a voice action discoverability example database 170 of the voice action system 100 .
- an application developer 101 a - 101 n may submit information to the voice action system 100 to define a new voice action, as described with respect to FIG. 1 A .
- the application developer 101 a - 101 n may submit information specifying one or more discoverability examples that can inform a user of the software application how to trigger the new voice action.
- information defining a discoverability example for a new voice action can specify the new voice action to which the discoverability example relates, may specify a context when the new voice action is available to users, may specify one or more trigger terms used to trigger the new voice action, or can include other information.
- the information specifying the discoverability example submitted to the voice action system 100 by an application developer 101 a - 101 n may include the content of the discoverability example.
- the information transmitted to the voice action system 100 may include a textual notification or message, an image, a video, audio data, or other content that may be provided to users to inform a user how to trigger a voice action.
- the application developer 101 a - 101 n may provide information defining multiple discoverability examples for the same voice action, where different discoverability examples may be intended for presentation to different users, different types of user devices, or under different circumstances.
- an application developer 101 a - 101 n developing the “skip ahead” voice action for the media player application may submit information using a terminal 102 a - 102 n that defines a discoverability example for the “skip ahead” voice action.
- the application developer 101 a - 101 n may submit information that specifies one or more trigger terms used to trigger the “skip ahead” voice action, as well as information that specifies a context when the “skip ahead” voice action is capable of being triggered by a user.
- the submitted information defining the discoverability example may specify the trigger term “skip ahead” as well as a context specifying that the voice action can be performed when the media player application is operating in the foreground of a user device and is operating in an audio player mode.
- Additional or different information may also be submitted by the application developer 101 a - 101 n to establish the discoverability example.
- the application developer 101 a - 101 n may submit information specifying the content of the discoverability example, such as a message “Try Saying ‘Skip Ahead” that is to be presented to users.
- an application developer 101 a - 101 n may only need to provide information specifying the new voice action for a discoverability example for that voice action to be created by the voice action system 100 .
- the application developer 101 a - 101 n may submit information defining multiple discoverability examples for the “skip ahead” voice action. For example, the application developer 101 a - 101 n may submit information for a textual discoverability example for the “skip ahead” voice action, such as the message “Try Saying ‘Skip Ahead,” as well as information for an audio discoverability example for the “skip ahead” voice action, such as a recording of a voice saying “Try Saying ‘Skip Ahead.”
- the voice action system 100 may suggest discoverability examples to an application developer 101 a - 101 n , such that the application developer 101 a - 101 n need only select from among the suggested discoverability examples. For example, when an application developer 101 a - 101 n establishes a new voice action for an application, the voice action system 100 may be configured to generate one or more candidate discoverability examples for the new voice action. The voice action system 100 provides information to the terminal 102 a - 102 n of the application developer 101 a - 101 n that enables the application developer 101 a - 101 n to select one or more discoverability examples to establish for the new voice action. Data indicating the selections of the application developer 101 a - 101 n may then be provided to the voice action system 100 by the terminal 102 a - 102 n.
- the voice action system 100 may generate one or more candidate discoverability examples for the “skip ahead” voice action.
- the candidate discoverability examples may include, for example, the textual message, “Try Saying ‘Skip Ahead,” the textual message, “Say ‘Skip Ahead’ to Fast Forward 30 Seconds,” and a recording of a voice saying “Try Saying ‘Skip Ahead.”
- the voice action system 100 may provide the candidate discoverability examples to a terminal 102 a - 102 n of the application developer 101 a - 101 n , and the application developer 101 a - 101 n may provide input to the terminal 102 a - 102 n to select one or more of the candidate discoverability examples. Data indicating the selections may be provided to the voice action system 100 .
- the voice action system 100 can receive the information specifying or selecting a discoverability example from the terminal 102 a - 102 n of the application developer 101 a - 101 n , and can generate data for the discoverability example based on the received information. For example, the voice action system 100 may receive the information specifying the discoverability example, and may induct the received data to generate a discoverability example in a particular data type or format. In some implementations where the voice action system 100 suggests discoverability examples to application developers 101 a - 101 n for selection, the voice action system may generate the discoverability example, in the particular data type or format, before providing the suggestions to the application developers 101 a - 101 n .
- the voice action system 100 may be able to discard non-selected discoverability examples that the voice action system 100 generated, and may retain the selected discoverability example for storage.
- the voice action system 100 may provide discoverability examples to application developers 101 a - 101 n without generating the discoverability examples corresponding to the suggest discoverability examples.
- the suggestions provided to the application developers 101 a - 101 n may be representations of potential discoverability examples.
- the voice action system 100 may then generate the discoverability example in the particular data type or format.
- Inducting information specifying a discoverability example generates a discoverability example in a particular data type or format that may be necessary to presentation of the discoverability example at a user device, in a similar way that induction of the information defining a new voice action results in the generation of an intent for triggering the new voice action.
- the voice action system 100 may induct the information to generate an item of a particular data type or format that specifies information necessary for presentation of the discoverability example, such as the generation and formatting of a textual message to be presented to a user, information indicating devices that are compatible with the particular discoverability example, or other information required for presentation of the discoverability example at a user device.
- the inducted discoverability example may be stored at the voice action discoverability example database 170 .
- the voice action discoverability example database 170 may include discoverability examples pertaining to one or more different voice actions that can be performed with respect to one or more different applications, operating systems, or devices, and the inducted discoverability example may be stored at the voice action discoverability example database 170 .
- the voice action discoverability example database 170 may be included in the voice action system 100 , or may be accessible by the voice action system 100 over one or more wired or wireless data pathways.
- the voice action system 100 may also be capable of processing a request for discoverability examples received from a user device 104 , and of responding to the request by providing information to the user device 104 that causes one or more discoverability examples to be presented to the user 105 .
- the voice action system 100 may detect a request for discoverability examples received from the user device 104 , and in response may identify discoverability examples to provide to the user device 104 .
- the user 105 may request discoverability examples by providing the voice input “What voice commands can I say?” to the user device 104 .
- the user device 104 may have a connection to the voice action system 100 over one or more wired or wireless data pathways, and so may transmit audio data corresponding to the voice input to the voice action system 100 where it is received by the speech recognition engine 130 .
- the speech recognition engine 130 may generate a transcription of the voice input based on the received audio data. For example, upon receiving the audio data corresponding to the voice input of the user 105 , the speech recognition engine 130 may generate the transcription “What voice commands can I say?” and may provide the transcription to a discoverability example selector 160 of the voice action system 100 .
- Requests for discoverability examples can be provided to the voice action system 100 in other ways as well.
- a user device 104 may request discoverability examples from the voice action system 100 in response to a user providing an input requesting help or requesting discoverability examples, for example, by pressing a particular key or symbol at the user device 104 .
- the user device 104 may also request discoverability examples from the voice action system 100 without requiring an explicit user input.
- the user device 104 may determine that a new application has been launched or reopened at the user device 104 (e.g., brought to the foreground of the user device 104 ), or that a status of an application or of the user device 104 has changed, or that content displayed at the user device has changed, and in response to the determination may request discoverability examples from the voice action system 100 .
- the user device 104 may also determine a context of the user device 104 and applications operating on the user device 104 .
- the user device 104 may transmit contextual information to the voice action system 100 , where it may be received by the context analysis engine 140 .
- the context analysis engine 140 may process the received context information to determine a context that is relevant to identifying discoverability examples.
- the context analysis engine 140 may receive a variety of context information from the user device 104 , and may determine a subset of the context information that is pertinent to identifying discoverability examples that should be presented to the user 105 of the user device 104 . For instance, the context analysis engine 140 may receive contextual information from the user device 104 indicating that the media player application is operating in the foreground of the user device 104 , that the media player application is operating in an audio player mode, and that the battery life of the user device 104 is 50%.
- the context analysis engine 140 may determine that the battery life information is not relevant to determining discoverability examples to present to the user 105 , and therefore may provide only the context information indicating that the media player application is operating in the foreground of the user device 140 and is operating in an audio player mode to the discoverability example selector 160 .
- the discoverability example selector 160 may receive the transcription of the voice input of the user 104 from the speech recognition engine 130 and the relevant context from the context analysis engine 140 , and based on the received information may identify one or more discoverability examples to provide to the user device 104 for output to the user 105 . For example, the discoverability example selector 160 may determine, based on the transcription, that the voice input of the user 105 was a request for discoverability.
- the discoverability example selector 160 may access the voice action discoverability example database 170 to identify discoverability examples associated with contexts that satisfy the context received from the context analysis engine 140 . For example, the discoverability example selector 160 may receive the context information indicating that the media player application is operating in the foreground of the user device 104 and that the media player application is operating in an audio player mode, and so may identify discoverability examples stored at the voice action discoverability example database 170 that also specify a context that includes the media player application operating in the foreground of a device and being in an audio player mode.
- the discoverability example selector 160 may compare the context received from the context analysis engine 140 to each of the discoverability examples included in the voice action discoverability example database 170 , or may compare the received context to a subset of the discoverability examples stored at the voice action discoverability example database 170 . Based on the comparison, the discoverability example selector 160 may identify one or more discoverability examples to provide to the user device 104 for presentation to the user 105 .
- the discoverability example selector 160 may determine that the context specified by the discoverability example established by the application developer 101 a - 101 n for the “skip ahead” voice action matches the context received from the context analysis engine 140 . In response to this determination, the discoverability example selector 160 may transmit information to the user device 104 that causes the discoverability example for the “skip ahead” voice action to be output. For instance, the discoverability example selector 160 may transmit the inducted discoverability example for the “skip ahead” voice action, in the particular data type or format, to the user device 104 , and upon receiving the inducted information the user device 104 may provide the discoverability example for the “skip ahead” voice action for output.
- the user device 104 may receive the inducted information corresponding to the discoverability example for the “skip ahead” voice action, and the received information may control the user device 104 to display a textual notification stating “Try Saying ‘Skip Ahead.”
- the presentation of the “Try Saying ‘Skip Ahead” discoverability example to the user 105 can inform the user that the “skip ahead” voice action is an available voice action for the current status of the user device 104 and applications running thereon.
- the user 105 may subsequently provide the voice input “skip ahead,” and the voice input may be processed as discussed with respect to FIG. 1 A to skip ahead by a predetermined amount of time in a song that the media player application is currently playing.
- the discoverability example selector 160 may select multiple discoverability examples for presentation at the user device 104 , and may transmit information corresponding to the selected multiple discoverability examples to the user device 104 .
- the discoverability example selector 160 may determine that the context received from the context analysis engine 140 satisfies that specified by the discoverability example for the “skip ahead” voice action, as well as a discoverability for a “pause” voice action for controlling the media player application, and a “home” voice action associated with the operating of the user device 104 that causes the user device 104 to return to a home screen.
- the discoverability example selector 160 may provide data for each of the discoverability examples to the user device 104 such that all three of the discoverability examples is presented or available to be presented to the user 105 .
- FIG. 1 C illustrates a voice action system 100 that enables application developers to establish discoverability examples for voice actions, and for discoverability examples to be provided to users.
- the system of FIG. 1 C performs many of the same operations discussed with respect to FIG. 1 B . However, in FIG. 1 C many of these operations are performed at a user device 104 as opposed to at the voice action system 100 . By delegating some of the components and operations performed by the voice action system 100 in FIG. 1 B to the user device 104 , discoverability examples may be provided to the user 105 even if the user device 104 is offline.
- This implementation also reduces the needs of the user device 104 to contact the voice action system 100 each time discoverability examples are to be output at the user device 104 , thereby reducing the network bandwidth used by the user device 104 .
- the ability to provide discoverability examples without accessing a network may also reduce the power consumption incurred by providing discoverability examples, and may enable more rapid provision of discoverability examples to users.
- application developers 101 a - 101 n may each submit information specifying discoverability examples for new voice actions to the voice action system 100 using terminals 102 a - 102 n as in FIG. 1 B .
- the information specifying each submitted discoverability example may correspond to a voice action for a particular software application.
- the information submitted to the voice action system 100 for a new discoverability example corresponding to a voice action may include, for example, information specifying one or more trigger terms used to trigger the voice action, a context specified for the voice action, notification content to be presented to a user when the discoverability example is output, or other information.
- the voice action system 100 may receive information from the application developer 101 a - 101 n for establishing a new voice action, and the voice action system 100 may generate candidate discoverability examples that the application developer 101 a - 101 n can select from.
- the voice action system 100 may present the candidate discoverability examples to the application developer 101 a - 101 n at their corresponding terminal 102 a - 102 n , and may receive information indicating the selection of one or more of the candidate discoverability examples.
- the voice action system 100 Based on the received information, the voice action system 100 generates and stores discoverability examples at the voice action discoverability example database 170 .
- the voice action discoverability example database 170 can include discoverability examples that correspond to voice actions for controlling one or more different applications, operating systems, or devices.
- appropriate discoverability examples that are stored at the voice action discoverability example database 170 are provided to user devices, where they are stored at an application metadata backend that is unique to each device.
- the user device 104 may include an application metadata backend 180 that is configured to store discoverability examples that are pertinent to the user device 104 and applications or an operating system installed on the user device 104 .
- the user device 104 may regularly communicate with the voice action system 100 over one or more wired or wireless data pathways to identify discoverability examples that are pertinent to the user device 104 , applications installed on the user device 104 , and/or an operating system of the user device 104 .
- Discoverability examples identified as being relevant to the user device 104 , applications installed at the user device 104 , and/or the operating system of the user device 104 can be retrieved from the voice action discoverability example database 170 and stored at the application metadata backend 180 of the user device 104 .
- the user device 104 may be equipped with an application interface 190 that is configured to receive discoverability examples stored at the voice action discoverability example database 170 that are identified as being relevant to the user device 104 , applications installed at the user device 104 , and/or the operating system of the user device 104 .
- the application interface 190 may obtain the identified discoverability examples from the voice action discoverability example database 170 and may store the identified discoverability examples at the application metadata backend 180 .
- the voice action system 100 may be configured to automatically identify discoverability examples that are relevant to the user device 104 or software installed on the user device 104 , and to provide relevant discoverability examples to the user device 104 .
- the voice action system 100 may provide relevant discoverability examples to the user device 104 for storage at the application metadata backend 180 on a periodic basis, in response to user input that requests the discoverability examples be updated, in response to determining that a status of the user device 104 has changed, in response to determining that different applications or versions of applications are installed at the user device 104 , in response to determining that a different operating system or version of an operating system is operating on the user device 104 , or in response to other events.
- the system of FIG. 1 C also includes, at the user device 104 , components discussed with respect to FIG. 1 B for presenting discoverability examples to a user 105 .
- the user device 104 shown in FIG. 1 C includes a context analysis engine 140 similar to the context analysis engine 140 of FIGS. 1 A and 1 B , as well as a discoverability example selector 160 similar to the discoverability example selector 160 of FIG. 1 B .
- the user device 104 may be capable of generating transcriptions of voice inputs in a similar manner as the speech recognition engine 130 of FIG. 1 B .
- the application interface 190 or other component of the user device 104 may determine when discoverability examples should be provided for output at the user device 104 . For example, a user 105 of the user device 104 may provide a voice input stating, “What voice commands can I use?” and the user device 104 may generate a transcription of the voice input. The application interface 190 may receive the transcription of the voice input, and may determine that the voice input is a request for discoverability examples. Alternatively, the application developer 190 may otherwise determine to provide discoverability examples for output at the user device 104 .
- the application interface 190 may determine that a user 105 has provided other input to the user device 104 , such as a press and hold feature on a button of the user device 104 , that requests discoverability examples for a current status of the user device 104 or an application or operating system operating on the user device 104 .
- the application interface 190 may also determine to provide discoverability examples based on a determination that a status of the user device 104 , an application installed on the user device 104 , or an operating system of the user device 104 has changed.
- discoverability examples may be provided at the user device 104 in response to determining that a previous voice input of the user 105 does not match any known trigger term for a voice action, to inform the user 105 of trigger terms that may be used.
- another component of the user device 104 may determine that discoverability examples should be provided for output at the user device 104 .
- a transcription of the voice input “What voice commands can I say?” can be provided to the discoverability example selector 160 , and based on the transcription the discoverability example selector 160 may determine that discoverability examples should be provided for output at the user device 104 .
- the discoverability example selector 160 may determine to present discoverability examples at the user device 104 based on other information, such as information indicating other user input at the user device 104 or based on detecting that there has been a change in the status of the user device 104 , an application installed and/or running on the user device 104 , and/or an operating system of the user device 104 .
- the application interface 190 may obtain contextual information for the user device 104 that indicates a status of the user device 104 , one or more applications installed and/or running on the user device 104 , and/or an operating system of the user device 104 . Similar to the implementation discussed with respect to FIG. 1 B , upon obtaining the contextual information, the application interface 190 can provide the contextual information to the context analysis engine 140 where the contextual information is processed.
- the context analysis engine 140 may obtain the context information and process the information to identify a context that is relevant to identifying discoverability examples to output at the user device 104 . For example, if the context analysis engine 140 receives contextual information from the application interface 190 that indicates that the media player application is presently running in the foreground of the user device 104 , that the media player application is in an audio player mode, and that the battery life of the user device 104 is currently 50%, the context analysis engine 140 may determine that only the contextual information indicating that the media player application is running in the foreground and that the media player application is in an audio player mode is relevant to selecting discoverability examples to present at the user device 104 .
- the context analysis engine 140 may provide a context to the discoverability example selector 160 that indicates that the media player application installed on the user device 104 is operating in the foreground and is in an audio player mode, and does not indicate a battery life of the user device 104 .
- the discoverability example selector 160 can receive the context from the context analysis engine 140 , and can use the received context to identify discoverability examples stored at the application metadata backend 180 to provide for output. For example, as discussed with respect to FIG. 1 B , the discoverability example selector 160 can compare the received context with contexts specified by the discoverability examples stored at the application metadata backend 180 to identify one or more discoverability examples to provide for output. For example, the discoverability example selector 160 may identify the discoverability example for the “skip ahead” voice action based on determining that the context received from the context analysis engine 140 matches that of the discoverability example for the “skip ahead” voice action. In some implementations, the discoverability example selector 160 may identify multiple discoverability examples to provide for output at the user device 104 . For example, the discoverability example selector 160 may select discoverability examples for the “skip ahead” voice action, a “pause” voice action for controlling the media player application, and a “home” voice action for controlling the user device 104 to return to a home screen.
- the discoverability example selector 160 can provide the selected discoverability examples for output at the user device 104 .
- the discoverability example selector 160 may be able to provide the selected discoverability examples for output via the application interface 190 .
- the selected discoverability example for the “skip ahead” voice action may be output at the user device 104 , thereby causing the textual notification “Try Saying ‘Skip Ahead” to be output at a display of the user device 104 .
- FIG. 2 depicts an example voice action system 200 that is configured to enable the establishment of voice actions and discoverability examples for voice actions.
- the voice action system 200 includes a validation engine 210 that has access to validation criteria 215 , a discoverability example induction engine 220 , and a voice action discoverability example database 235 that is configured to store discoverability examples.
- the voice action system 200 may operate as a platform offering a service or tool that enables an application developer 201 to establish voice actions and discoverability examples for those voice actions via a terminal 202 .
- the application developer 201 may establish voice actions according to the methods disclosed in U.S. patent application Ser. No. 15/057,453, filed Mar. 1, 2016, which is hereby incorporated by reference in its entirety.
- the application developer 201 may submit information to the voice action system 200 to define a new voice action.
- the information defining the voice action may be submitted as a grammar in a format that can be processed by the voice action system 200 to generate an intent for the voice action.
- the application developer 201 may enter information at the terminal 202 that specifies that a new voice action is to be established for the media player application.
- the application developer 201 may specify that the new voice action uses the trigger phrase “skip ahead” to control the media player application to skip ahead by a predetermined amount of time in a song that the media player application is currently playing.
- the application developer 201 may further specify a context for the “skip ahead” voice action such that the “skip ahead” voice action is only capable of being triggered when the media player application is operating in the foreground of a user device, and only when operating in an audio player mode.
- the information submitted by the application developer 201 may be in the form of a grammar that is capable of being processed by the voice action system 200 , or the information submitted by the application developer 201 may be converted into a grammar format for submission to the voice action system 200 .
- the information defining the new voice action can be submitted to the voice action system 200 by the terminal 202 over one or more wired or wireless data pathways.
- the voice action system 200 can receive the information defining the new voice action at a validation engine 210 that is configured to validate new voice actions submitted by application developers.
- the validation engine 210 may receive a grammar defining a new voice action that has been submitted by the application developer 201 .
- the validation engine 210 may access one or more validation criteria 215 that can include one or more rules associated with defining a new voice action.
- the validation engine 210 may evaluate the information defining the new voice action in view of the accessed validation criteria 215 .
- the validation engine 210 may determine whether the media player application is an existing application, such that new voice actions can be created for the specified application.
- the validation engine 210 may access a rule that specifies that a trigger phrase must be more than one spoken syllable in length, and may validate the trigger phrase “skip ahead” based on determining that the trigger phrase “skip ahead” is longer than one spoken syllable.
- the validation rules may specify a list of possible actions that the media player application can perform, and may validate the specified action to skip ahead by a predetermined amount of time in a song that the media player application is currently playing based on determining that the media player application can skip ahead by a predetermined amount of time in a song currently playing in the application.
- the validation engine 210 may validate the context specified by the grammar to ensure that the context does not contain contradictions. For example, the validation engine may ensure that the specified context does not require the application to be operating in the foreground and also operating in the background for the new voice action to be enabled, since only one of those two conditions can be satisfied.
- Other validation criteria 215 may be applied to information defining a new voice action to determine whether the new voice action submitted by the application developer 201 is valid and can be inducted to generate an intent for the new voice action.
- the validation engine 210 may determine whether the application developer 201 is authorized to generate new voice actions for the specified application, may determine whether a fee required to generate the new voice action has been paid, or may otherwise determine whether a grammar submitted by an application developer 201 may result in the creation of an intent for the new voice action.
- the voice action system 200 can induct the information defining the new voice action to generate an intent for the new voice action as discussed with respect to FIG. 1 A .
- the generated intent can be stored at a voice action database similar to the application voice action database 110 , where the voice action may then be deployed by the application developer 201 to enable the application for users having the application installed on their user devices.
- the grammar or other information defining the “skip ahead” voice action may be inducted by the voice action system 200 to generate an intent for the “skip ahead” voice action, and the intent for the “skip ahead” voice action may be stored at an application voice action database.
- the application developer 201 may deploy the “skip ahead” voice action to enable the “skip ahead” voice action for users who have the media player application installed on their user devices.
- the voice action system 200 may also induct the information defining the new voice action to generate one or more candidate discoverability examples for the new voice action.
- the validated information defining the new voice action may be submitted to the discoverability example induction engine 220 , and the discoverability example induction engine 220 can induct the information defining the new voice action to generate one or more candidate discoverability examples for the new voice action.
- the discoverability example induction engine 220 may induct the information defining the new voice action to generate one or more candidate discoverability examples.
- the candidate discoverability examples may be in a format that, when provided to a user device, can control or enable the user device to provide a notification specified by the discoverability example to be output at the user device.
- inducting the data defining the voice action may result in the generation of a particular data item or data item in a particular format that, when received by a user device or triggered at a user device controls the user device to present a textual, image, video, audio, or other notification for output to inform a user of the device of trigger terms that can be used to trigger the associated voice action at the user device.
- inducting the data defining the new voice action to generate a candidate intent can include generating one or more textual phrases, spoken audio phrases, images, or videos that specify a trigger phrase for triggering the associated voice action.
- the discoverability example induction engine 220 may receive the data indicating the trigger phrase for the new voice action, and may generate one or more candidate phrases corresponding to one or more candidate discoverability examples.
- the discoverability example induction engine 220 may generate candidate phrase such as “Try Saying ‘Skip Ahead’,” “Say ‘Skip Ahead’ to Fast Forward 30 Seconds,” “Say, ‘Okay Computer, Skip Ahead,” or any number of other candidate phrases.
- the discoverability example induction engine 220 may also generate, as candidate notifications for candidate discoverability examples, spoken versions of the candidate phrases, candidate images that indicate the trigger phrase “skip ahead” associated with the new voice action, or candidate videos that indicate the trigger phrase “skip ahead.”
- a trigger phrase for a voice action may include one or more placeholders, such as one or more enumerated types, and the discoverability example induction engine 220 may generate candidate discoverability examples associated with notifications that include the placeholders.
- a voice command may be associated with the trigger phrase, “add % CurrentSong to favorites,” where “% CurrentSong” is a placeholder that refers to a song that is currently playing in the media player application.
- the discoverability example induction engine 220 may generate a candidate discoverability example that includes a phrase, “Try Saying, ‘Add % CurrentSong to Favorites’,” where the placeholder may be replaced by a name of a song currently playing in the media player application when the phrase is provided for output.
- the candidate discoverability examples generated by the discoverability example induction engine 220 also each specify the context when the new voice action is capable of being triggered, such that the discoverability examples for the new voice action are only capable of being output at a user device when a status of the a user device, application running on the user device, or operating system of the user device matches the context specified by the corresponding voice action. In this way, discoverability examples provided to users in any particular situation are limited to discoverability examples for voice actions that can be triggered at that time.
- a context specifies one or more conditions that must be satisfied for a voice action to be enabled, i.e., such that the voice action will not be triggered by a trigger phrase associated with the voice action unless the context is also satisfied.
- the application developer 201 can define a context as have one or more different requirements or conditions.
- a context may require a certain status of a user device, or may require a user device to have certain attributes. For instance, a context may specify whether the user device is powered on or locked, or whether the user device has a camera, gyroscope, barometer, or other component or feature. Thus, for example, a voice action to take a picture using an application installed on a user device may not be enabled if contextual information received from the user device indicates that the user device does not have a camera.
- a context may require that specific applications are operating in the foreground or background of a user device.
- the “skip ahead” voice action may only be enabled when the media player application is operating in the foreground of user device 204 , but not if the media player application is closed or is operating in the background of the user device 204 .
- a voice action may be defined with a context that enables the voice action when an application is operating in the background. For instance, a social network application may have an associated voice action to accept a request to connect with another user, and a user may be able to trigger the voice action to accept a request to connect with another user even if the social network application is only operating in the background of the user device.
- a context may additionally or alternatively require that a particular application is operating in a specific mode for a voice action to be enabled.
- a mode may be a particular activity or task that the application is executing.
- an email application may be determined to be in an email write mode
- a media player application may be in an audio player mode
- a camera application may be in a camera mode or a photo album viewer mode.
- the user device may be able to determine which of the modes a particular application is operating in, and may include that information in context information that is used to determine whether a particular voice action is enabled.
- a context may require that an application mode has a certain status.
- a context may indicate that a voice action to “pause” a movie may only be enabled when a media player application is in a movie player mode and when the status of the application in the movie player mode is that the application is currently playing the movie.
- a user device may not be able to determine the status of an application operating in a particular mode. In those instances, it may be necessary to configure the application to provide information indicating the status of the application in a particular mode. Thus, specifying such a condition in a context for a particular application may require modifications to the application itself, in order for context information provided by the user device to include the required status information.
- a context specified for a voice action may require that particular objects or types of objects are displayed on a screen of a user device to enable the voice action.
- a context may specify that a “save image” voice action is only enabled if an image is being output at the display of the user device, or may specify that a “highlight” voice action is only available if text is presented on the display.
- a voice action to select a particular item in a list such as the voice action to “pick the first one” may only be available if context information indicates that a number “1” or a bullet point is being presented on the display of the user device, or if there is a list of items being presented on the display.
- the user device may not be capable of determining what information is being presented at its display at a given moment, e.g., when an application is in control of the display.
- a context specifies that certain information or types of information must be output at the display for a voice action to be enabled, then it may be necessary to modify the application to provide that information.
- the user device can then include the information indicating what is being output at the display of the user device in contextual information that is used to determine whether a specific voice action should be enabled.
- a context may require that a notification or a particular type of notification has been output at a user device.
- an email application that provides pop-up notifications to users when a new email is received may have an associated voice action to “read email” that is enabled when a new email notification is being output to the display of a user device by the application.
- context information may indicate whether a notification is being output to the display of the user device, and that information may be used in determining whether the “read email” voice action should be enabled.
- a context may require a user device be in a particular geographic location for a voice action to be enabled.
- a user device may be capable of determining its geographic location using cellular tower triangulation, based on accessing an internet connection that is associated with a particular geographic location, using Global Positioning System (GPS) technology, or using other means.
- GPS Global Positioning System
- the user device may include information indicating its geographic location in context information, and a voice action may be enabled based on the geographic location satisfying a context's geolocation requirements.
- a context associated with a voice action for a retailer's application may specify that a certain voice action should only be processed if a user device having the retailer application is within a range of one of the retailer's stores.
- the voice action for the retailer application may be enabled based on context information from the user device indicating that the geographic location of the user device corresponds to a geographic location that is within the range of one of the retailer's stores.
- a context defined by an application developer can also indicate that a voice action associated with the context is only enabled during certain times, dates, months, seasons, or when other temporal conditions are met.
- a context may specify that a certain voice action is only enabled if context information received from a user device or from another information source satisfies a time condition.
- a voice action for a banking application may be associated with a context that includes a temporal condition, such that the voice action is only enabled if context information indicates that a time when the voice action is submitted is between 6:00 AM and 10:00 PM.
- a context may specify other requirements for enabling a voice action to be triggered.
- the voice action system 200 may have access to information from sources other than a user device, or may be able to receive information from a user device that is obtained from one or more information sources.
- the information received from the other sources can be required context information for enabling a voice action.
- Such information can include, for example, weather information, emotional information for a user, news information, stock market information, or other information.
- the voice action system 200 may be capable of accessing other information sources through one or more wired or wireless network connections, e.g., an Internet connection or other network connection to a server system.
- a context for a voice action may specify that the voice action is enabled only if weather information for a geographic area corresponding to the location of a relevant user device indicates that it is raining in that location.
- the voice action system 200 may be able to access weather information for a known location of a user device over the one or more network connections.
- the discoverability example induction engine 220 can generate multiple versions of a candidate discoverability example corresponding to different operating systems, different types of user devices, or different versions of an application. Because one particular data type or format may not be used to output a discoverability example at all types of devices, operating systems, or versions of an application, the discoverability example induction engine 220 may generate multiple versions of a discoverability example. Thus, correct versions of a discoverability example can be provided to different user devices to ensure proper output of the discoverability example at those user devices. Additionally or alternatively, in some implementations a discoverability example may be modified for provision at different user devices, operating systems, or versions of an application.
- a generated discoverability example may include an image notification in one format, e.g., using a particular size, resolution, color map, etc., and the image notification may be capable of being modified or formatted for presentation at different types of user devices, in different operating systems, or within different versions of an application.
- candidate discoverability examples can be presented to the application developer 201 for selection.
- the voice action system 200 may provide information to the terminal 202 of the application developer 201 that enables the application developer 201 to select specific discoverability examples that they would like to implement for the new voice action.
- the discoverability example induction engine 220 may present information at the terminal 202 indicating the candidate phrases “Try Saying ‘Skip Ahead’,” “Say ‘Skip Ahead’ to Fast Forward 30 Seconds,” and “Say, ‘Okay Computer, Skip Ahead,” and the application developer 201 may select one or more of the candidate phrases.
- the application developer may select the candidate phrase “Try Saying ‘Skip Ahead,” indicating that they would like a discoverability example for the “skip ahead” voice action to be a textual notification that is output to a user that says “Try Saying ‘Skip Ahead.”
- the application developer 201 may submit information defining discoverability examples for a voice action.
- the application developer 201 may submit information to the voice action system 200 that defines a discoverability example for a voice action.
- the submitted information defining the discoverability example may specify the voice action to which the discoverability example pertains, such as information specifying the “skip ahead” voice action, may specify a context when the discoverability example should be available for output at user devices, such as the context specified for the “skip ahead” voice action, and may submit content for a notification that includes the trigger term to be presented as the discoverability example for the voice action.
- the application developer 201 may submit information to the voice action system 200 that specifies the “skip ahead” voice action, a context that the discoverability example for the “skip ahead” voice action should only be available for output when the media player application is operating in the foreground and is in an audio player mode, and may submit an audio clip stating “Try Saying ‘Skip Ahead’” to the voice action system 200 .
- Other content may be submitted by the application developer 201 , such as a textual phrase, image, video, or other content that can be provided for output at a user device to inform a user of the device that the voice action to skip ahead by a predetermined amount of time in a song that the media player application is currently playing can be triggered by providing a voice input that states “skip ahead.”
- less, additional, or different information may be required from the application developer 201 to specify a discoverability example. For instance, the application developer 201 may only be required to submit information indicating the voice action and content to be output as a notification associated with the discoverability example.
- the voice action system 200 can receive the data indicating the candidate discoverability examples selected by the application developer 201 , and can store the selected candidate discoverability examples at a voice action discoverability example database 235 .
- the voice action system 200 can also receive data defining one or more discoverability examples defined by the application developer 201 , can induct the data defining the one or more discoverability examples to generate data items in the proper discoverability example format, and can store the discoverability examples generated by the induction process at the voice action discoverability example database 235 .
- the voice action system can store the discoverability example at the voice action discoverability example database 235 .
- the stored discoverability example can be a particular data item or data in a particular format named “Example Y” that specifies the “skip ahead” voice action, the trigger phrase “skip ahead” for the voice action, the notification “Try Saying ‘Skip Ahead’” that is provided for output when the discoverability example is triggered, and the context when the discoverability example is available for output requiring that the media player application is operating in the foreground and is in an audio player mode.
- a discoverability example defined by the application developer 201 can be inducted to generate an item of data in a particular format for the discoverability example.
- the discoverability example defined by the application developer 201 may be inducted to generated one or more versions of the discoverability example, such as versions of the discoverability example that correspond to different versions of the application to which the voice action relates, different devices, or different operating systems.
- the one or more discoverability examples generated during the induction process performed by the discoverability example induction engine 220 can be provided and stored at the voice action discoverability example database 235 .
- a discoverability example defined by the application developer 201 and inducted to generate a discoverability example can be stored at the voice action discoverability example database 235 as “Example X.”
- the voice action discoverability example database 235 can store multiple discoverability examples that correspond to one or more voice actions for one or more applications.
- the voice action discoverability example database 235 may include multiple discoverability examples for each of multiple different voice actions that have been developed for multiple different applications, devices, or operating systems.
- the voice action discoverability example database 235 may be a structured table, such as a look-up table, a linked list, a graph comprising nodes and edges between nodes, or may be any other data structure capable of storing discoverability examples such that discoverability examples can later be identified and provided for a proper status of a user device, operating system, or application.
- each discoverability example in the voice action discoverability example database 235 may be associated with information specifying the application to which the discoverability example pertains, a version of the application that the discoverability example is compatible with, a device or devices that the discoverability example is compatible with, an operating system or operating systems that the discoverability example is compatible with, or other information pertinent to selecting a particular discoverability example, such that appropriate discoverability examples can be identified in the voice action discoverability example database 235 without having to parse the data included in the discoverability example itself for such information.
- Such an arrangement may enable faster identification of appropriate discoverability examples to provide for output.
- the discoverability examples stored in the voice action discoverability example database 235 can be provided to user devices to control or enable the user devices to output discoverability examples. For instance, in the implementation of FIG. 1 B , one or more discoverability examples stored in the voice action discoverability example database 235 can be provided to a user device for output in response to receiving a request for discoverability examples from the user device. Or, as shown in the implementation of FIG. 1 C , one or more of the discoverability examples in the voice action discoverability example database 235 can be provided to a user device and stored at the user device, such that they may subsequently be triggered for output at the user device.
- the application developer 201 may be capable of testing a new discoverability example and after approving of the discoverability example may deploy the discoverability example to enable the discoverability example to be provided for output at a user device.
- the application developer 201 may be capable of testing or performing a test at the terminal 202 in which the application developer 201 may be shown an example of how the discoverability example will be provided for output at a user device, such as how a notification of the discoverability example will appear at a display of a user device or will sound when played by a user device.
- the application developer 201 may designate one or more other user devices separate from the terminal 202 where the discoverability example can be tested, to determine how the discoverability example appears or sounds when presented at different user devices. If the application developer 201 approves of the discoverability example, the application developer 201 may provide input at the terminal 202 to deploy the discoverability example, thereby enabling the discoverability example to be output at user devices. Additionally, in some implementations, the application developer 201 may be capable of rescinding deployment of the discoverability example, such that the discoverability example is no longer capable of being provided for output at user devices.
- FIG. 3 depicts a voice action system 300 that is configured to present discoverability examples at a user device 304 to inform a user 305 of the user device 304 of trigger phrases that can trigger voice actions.
- the voice action system 300 can be included in the user device 304 , such that the user device 304 can provide discoverability examples for output to the user 305 without communicating with another system.
- the voice action system 300 may be separate from the user device 304 , and the user device 304 may communicate with the voice action system 300 over one or more wired or wireless data pathways when providing discoverability examples for output.
- the voice action system 300 can include a speech recognition engine 330 similar to the speech recognition engine 130 of FIGS. 1 A and 1 B , a context analysis engine 340 similar to the context analysis engine 140 of FIGS. 1 A to 1 C , a matcher 350 , a disambiguation engine 360 , and a formatting engine 380 .
- the matcher 350 may have access to an application metadata backend 310 that includes one or more discoverability examples that are capable of being provided for output at the user device 304 .
- the disambiguation engine 360 may have access to a user activity history 370 that includes information indicating past operations performed at the user device 304 .
- the formatting engine 380 may have access to user device information 390 that indicates information relevant to the formatting of a discoverability example for output at the user device 304 .
- the voice action system 300 may include an application interface (not shown) similar to the application interface 190 of FIG. 1 C .
- Discoverability examples may be provided for output at the user device 304 in response to determining that the user 305 has requested discoverability examples or in response to determining that a condition associated with providing discoverability examples for output has been met.
- the user 305 may provide the voice input “What voice commands can I say?” to the user device 304 .
- Audio data corresponding to the voice input may be provided to the speech recognition engine 330 , and a transcription “What voice commands can I say?” may be generated based on the audio data.
- the transcription “What voice commands can I say?” may be interpreted, for example, by the matcher 350 of the voice action system 300 , as a request for discoverability examples.
- different user input such as different voice inputs, a selection of a “help” button, a press-and-hold input at a control of the user device 304 , or other user inputs may result in the voice action system determining to provide discoverability examples for output at the user device 304 .
- other information cause the voice action system 300 to determine to provide discoverability examples, such as information indicating that a status of the user device 304 , an application operating on the user device 304 , or an operating system of the user device 304 has changed.
- discoverability examples may be provided for output at the user device 304 based on determining that the user 305 has provided a voice input to the user device 304 that does not match any particular voice action trigger phrase.
- discoverability examples provided to the user 305 may be selected based on determining that the discoverability examples may correspond to an action that the user 305 intended to perform at the user device 304 .
- contextual information indicating a status of the user device 304 , one or more applications installed on user device 304 , or an operating system of the user device 304 can be provided to the voice action system 300 .
- the voice action system 300 may determine to provide discoverability examples for output at the user device 104 , and in response may obtain contextual information.
- the voice action system 300 may determine to provide discoverability examples at the user device 304 in response to receiving contextual information.
- the voice action system 300 may determine to provide discoverability examples for output at the user device 304 in response to one or more hardware components of the user device 304 , one or more applications installed on the user device 304 , or an operating system of the user device 304 providing information to the voice action system 300 indicating their status.
- the voice action system 300 may obtain or be provided with contextual information at regular intervals or continuously.
- the contextual information can be received by the context analysis engine 340 , where the context analysis engine 340 may process the contextual information to determine a relevant context.
- the context analysis engine 340 can receive contextual information that indicates a status of numerous components of the user device 304 , numerous applications installed on the user device 304 , or one or more operating systems operating on or associated with the user device 304 .
- the context analysis engine 340 can determine a relevant context of the user device 304 from the received information.
- the context analysis engine 340 may filter unnecessary contextual information to get the relevant context, can interpret the received contextual information to determine a relevant context, or can otherwise use the received contextual information to determine a relevant context.
- the context analysis engine 340 may receive contextual information that indicates that the media player application is operating in the foreground of the user device 304 , that the media player application is operating in an audio player mode, and that a battery life of the user device 304 is 50%.
- the context analysis engine 340 may filter out the battery life contextual information to generate a relevant context that indicates only that the media player application is operating in the foreground and is in an audio player mode.
- the context analysis engine 340 receives contextual information indicating that the media player application is operating in the foreground of the user device 304 , that a headphone jack of the user device 304 is providing output, and that a display of the user device 304 is currently dimmed.
- the context analysis engine 340 may interpret the received contextual information to determine a relevant context. For instance, the context analysis engine 340 may determine that the media player application operating in the foreground is likely in an audio player mode, since the headphone jack is providing sound output while the display is dimmed.
- the relevant context determined by the context analysis engine 340 is provided to the matcher 350 , where the matcher 350 identifies discoverability examples that may be provided for output at the user device 304 given the current status of the user device 304 , applications installed on the user device 304 , or an operating system of the user device 304 .
- the matcher 350 may select those discoverability examples that are associated with contexts that match the relevant context determined by the matcher 350 .
- the matcher 350 compares the relevant context to contexts specified by one or more discoverability examples stored in the application metadata backend 310 . For example, as discussed with respect to FIG.
- the application metadata backend 310 may store numerous discoverability examples that may be associated with numerous voice actions pertaining to numerous different applications, devices, or operating systems. Each discoverability example in the application metadata backend 310 may specify a context indicating when the corresponding voice action is available to be triggered, which also defines when the discoverability example is available to be provided for output at a user device. By comparing the received context with the contexts specified by the discoverability examples in the application metadata backend 310 , the matcher 350 identifies a set of candidate discoverability examples that are eligible to be provided for output at the user device 304 .
- the matcher 350 may compare the context indicating that the media player application is operating in the foreground and is in an audio player mode to multiple discoverability examples stored in the application metadata backend 310 . Based on the comparison, the matcher may identify a set of four discoverability examples that correspond to voice actions that can be triggered by a user given the present context.
- These discoverability examples may include, for example, a discoverability example for the “skip ahead” voice action, a discoverability example for a “pause” voice action for controlling the media player application to pause a song playing in the audio player application, a discoverability example for a “search for similar songs” voice action that controls the media player application to search a database or the Internet for songs that are determined to be similar to a song currently playing in the media player application, and a discoverability example for a “home” voice action that controls the user device 304 to return to a home screen of the user device 304 (e.g., a desktop or menu screen that the operating system presents to users at a display of the user device 304 ).
- a discoverability example for the “skip ahead” voice action e.g., a discoverability example for a “pause” voice action for controlling the media player application to pause a song playing in the audio player application
- a discoverability example for a “search for similar songs” voice action that controls the media player application to search a database
- the matcher 350 can identify the candidate discoverability examples and can provide the candidate discoverability examples to the disambiguation engine 360 .
- providing the candidate discoverability examples to the disambiguation engine 360 may require sending data items corresponding to the candidate discoverability examples to the matcher 350 .
- the disambiguation engine 360 may be capable of accessing the candidate discoverability examples at the application metadata backend 310 , such that the matcher 350 need only provide information to the disambiguation engine 360 that identifies the candidate discoverability examples, without needing to send the items of data corresponding to the candidate discoverability examples to the disambiguation engine 360 .
- identifying a candidate discoverability example includes identifying one or more versions of the candidate discoverability example as candidate discoverability examples.
- the “skip ahead” voice action may be associated with multiple discoverability examples that each specify a context corresponding to the context received from the context analysis engine 340 , such as a textual notification discoverability example and a spoken word audio notification discoverability example, and both the textual and spoken word audio notifications may be provided or identified to the disambiguation engine 360 .
- the disambiguation engine 360 can receive the candidate discoverability examples or information identifying the candidate discoverability examples from the matcher 350 , and can select one or more discoverability examples from among the candidate discoverability examples to provide for output to the user device 304 .
- the disambiguation engine 360 may select discoverability examples from among the candidate discoverability examples based on details of a user activity history 370 , based on the capabilities of the user device 304 to present discoverability examples, or based on the status of the user device 304 .
- the number of discoverability examples selected by the disambiguation engine 360 can depend on, for example, the type of discoverability examples that to be presented to a user, e.g., whether the discoverability examples output to the user 305 are textual, images, audio, video, etc.
- the number of discoverability examples selected may also depend on the capabilities of the user device 304 , for example, based on a display size of the user device 304 that could be a limiting factor in the quantity of textual or image notifications that can be presented at the user device 304 . Other factors may influence the selection of discoverability examples by the disambiguation engine 360 .
- the disambiguation engine may receive information indicating user preferences for the number of discoverability examples to be displayed, a size of a display of the user device 304 , or other information, and the received information may affect the specific discoverability examples selected or the number of discoverability examples selected by the disambiguation engine 360 .
- the disambiguation engine 360 may have access to a user history 370 that indicates operations previously performed at the user device 304 , and the disambiguation engine 360 may select discoverability examples to provide for output based on the user history 370 .
- the user history 370 may indicate operations previously performed at the user device 304 in the media player application, operations previously performed at the user device 304 when a status of the user device 304 matches the current context, when a status of the media player application matches the current context, when a user has requested discoverability examples indicating available voice actions, or may otherwise indicate a history of user operations performed at the user device 304 .
- the disambiguation engine 360 may rely upon the user history 370 to identify discoverability examples to provide for output at the user device 304 .
- the disambiguation engine 360 may determine that a user has previously skipped ahead (e.g., fast forwarded) in a song previously while operating the media player application in the foreground and in an audio player mode, and may therefore select the discoverability example for the “skip ahead” voice action as a discoverability example to be output at the user device 304 .
- the disambiguation engine 360 may otherwise utilize the user activity history 370 in selecting discoverability examples from among the candidate discoverability examples identified by the matcher 350 .
- the disambiguation engine 360 may access the user activity history 370 , and may determine to select a particular candidate discoverability example based on the user activity history 370 indicating that a user has previously performed a voice action corresponding to the particular candidate discoverability example, may determine not to select a particular candidate discoverability example based on the user activity history 370 indicating that a user has previously performed a voice action corresponding to the particular candidate discoverability example, may determine to select the particular candidate discoverability example based on determining that a user has previously or recently entered voice input that is similar to a trigger phrase associated with the particular candidate discoverability example, may determine to select a particular candidate discoverability example based on determining that a user has performed operations previously at the user device 304 that matches an action that the particular candidate discoverability example is associated with, or the disambiguation engine 360 may otherwise utilize the user activity history 370 in the selection of particular candidate discoverability examples
- the disambiguation engine 360 may utilize other available information to select discoverability examples.
- the disambiguation engine 360 may have access to information about the user device 304 , such as user device information 390 , or a status of the user device 304 , such as contextual information received from the user device 304 , and may select discoverability examples based on the available information.
- the disambiguation engine 360 may access information indicating that the user device 304 does not have a display, and therefore the disambiguation engine 360 may select only audio discoverability examples, since textual, image, or video discoverability examples cannot be effectively provided for output at the user device 304 .
- the disambiguation engine 360 may select candidate discoverability examples that are textual or image discoverability examples, and not audio or video discoverability examples since audio for those discoverability examples may not be output while the user device 304 is in “silent” mode.
- the disambiguation engine 360 may rely on other information, such as user preferences that indicate the preferences of the user 305 to receive discoverability examples of a certain type, in selecting from among the candidate discoverability examples.
- the disambiguation engine 360 may select from among the candidate discoverability examples based on the accessed information, such as user activity history 370 , based on assigning a ranking or relevance score to the candidate discoverability examples. For instance, the disambiguation engine 360 may use the accessed information to rank the discoverability examples according to their perceived relevance to the status of the user device 304 , an expected intent of the user 305 , or other criteria, and may select from among the candidate discoverability examples based on the ranking. Additionally, in some implementations, one or more discoverability examples may be discoverability examples that are always presented to users at the user device 304 . For example, a discoverability example for a “back” voice action that controls the user device 304 to return to previous screen may always be selected for output, or may always be selected for output when a particular context associated with the “back” voice action is satisfied.
- the disambiguation engine 360 may select different groups of candidate discoverability examples that may be provided for output at the user device 304 in different ways. For instance, the disambiguation engine 360 may identify some candidate discoverability examples to be output as audio notifications, and other candidate discoverability examples to be output as textual notifications. In some examples, the disambiguation engine 360 may determine that some discoverability examples should be output to a primary discoverability example window or pane of a user interface displayed at the user device 304 , and may select other candidate discoverability examples to include in a drop down menu, in a “view more” area of the user interface, or in other separate regions of a user interface.
- the selected candidate discoverability examples or information identifying the selected discoverability examples may be transmitted to the formatting engine 380 , where the formatting engine 380 can format the selected discoverability examples for output at the user device 304 .
- the formatting engine 380 may have access to user device information 390 that indicates characteristics of the user device 304 that are pertinent to presenting discoverability examples at the user device 304 , and may prepare the discoverability examples for output at the user device 304 based on the user device information 390 .
- User device information 390 can include, for example, a display size, resolution, color spectrum, refresh rate, or other parameters, an audio output frequency range or other speaker or headphone jack output parameters, application programming interface protocols or input/output formats, or other information relevant to provide discoverability examples for output at the user device 304 .
- the formatting engine 380 can receive or access the candidate discoverability examples selected by the disambiguation engine 370 and can format the selected candidate discoverability examples for output at the user device 304 .
- the formatting engine 380 may receive the selected candidate discoverability example data items, or can access the selected candidate discoverability examples at the application metadata backend 310 , and based on formatting the selected candidate discoverability examples, can provide the formatted discoverability examples for output at the user device 304 .
- the formatting engine 380 can format the selected candidate discoverability examples and can control the user device 304 to provide the formatted discoverability examples for output.
- the formatting engine 380 may format the textual notifications and can cause the formatted textual notifications to be provided for output at a display of the user device 304 .
- the formatted discoverability examples may be output at the user device 304 as shown in FIG. 3 , by presenting the phrase “Try Saying:” followed by the trigger phrases used to trigger each of the “skip ahead,” “pause,” and “home” voice actions.
- the user 305 may be presented with the textual notifications identifying the trigger phrases “skip ahead,” “pause,” and “home,” and based on the discoverability examples being provided at the user device 304 , the user 305 may provide a voice input to trigger one of the voice actions corresponding to the suggested trigger phrases. For example, the user 305 may provide the voice input “skip ahead” in response to being presented with the discoverability examples to control the media player application operating in the foreground of the user device 304 to skip ahead by a predetermined amount of time in a song that the media player application is currently playing.
- FIG. 4 is a flowchart of an example process 400 for establishing and providing discoverability examples for output at a user device.
- the process 400 may be performed by a voice action system, such as the voice action system 100 of FIGS. 1 A to 1 C .
- the voice action system receives data specifying trigger terms that trigger a software application to perform a new voice action, and a context that specifies a status of the software application when the new voice action can be triggered ( 402 ).
- an application developer 101 a - 101 n using a terminal 102 a - 102 n may submit information to the voice action system 100 for defining a new voice action for an application that is installed at one or more user devices 104 .
- the information submitted by the application developer 101 a - 101 n may specify a context that indicates a status of the software application when the new voice action can be triggered at a user device 104 .
- the information may further specify trigger terms that a user 105 can speak into a user device 104 to trigger the voice action.
- an application developer 101 a - 101 n may submit information to the voice action system 100 that defines a new “skip ahead” voice action for a media player application that controls the media player application to skip ahead by a predetermined amount of time in a song that the media player application is currently playing.
- the submitted information may include information indicating that the trigger phrase “skip ahead” is to be used to trigger the new voice action, and that the voice action can be triggered only when the media player application is operating in the foreground of the user device 104 and is operating in an audio player mode.
- the submitted information may include other information as well, such as information indicating the action to be performed in response to the voice action being triggered, or other information.
- the voice action system receives data defining a discoverability example for the new voice action, wherein the data defining the discoverability example comprises one or more of the trigger terms that trigger the software application to perform the new voice action when a status of the software application satisfies the specified context ( 404 ).
- the application developer 101 a - 101 n may submit information to the voice action system 100 that specifies a discoverability example for the new voice action.
- the information submitted by the application developer 101 a - 101 n and received by the voice action system 100 may specify one or more of the trigger terms that can be used by a user 105 to trigger the new voice action at the user device 104 .
- the application developer 101 a - 101 n can specify a discoverability example that can cause a notification to be provided for output at the user device 104 when a status of the software application installed on the user device 104 satisfies the specified context for the new voice action, where the notification presents the one or more of the trigger terms to the user 105 to inform the user 105 of the trigger terms that they can use to trigger the new voice action.
- the information defining the new voice action may be in a particular required format that enables user devices to use the discoverability example to provide a notification of the one or more trigger terms.
- the voice action system 100 may receive information from the application developer 101 a - 101 n that identifies the one or more trigger terms for the new voice action, or other information, but that is not in the particular required format. To generate a discoverability example in those instances, the voice action system 100 may induct the received data to generate data defining the discoverability example, where the data defining the discoverability examples generated through the induction process is in the particular required format.
- the voice action system 100 may generate the discoverability example based only on the received information specifying the new voice action, such as the information specifying trigger terms that trigger the software application to perform the new voice action. In other examples, the voice action system 100 may generate the discoverability example based on the received information specifying the new voice action and based on other information that specifies information about the discoverability example that is determined by the voice action system 100 or received at the voice action system 100 from the application developer 101 a - 101 n . Such information can include, for instance, the content of the notification to associate with the discoverability example. In other examples, the voice action system 100 may determine one or more candidate discoverability examples, and receiving the data defining the discoverability example may involve receiving a selection by the application developer 101 a - 101 n of a particular one of the candidate discoverability examples.
- the voice action system 100 may receiving information from the application developer 101 a - 101 n that defines a discoverability example for the “skip ahead” voice action, where the data defining the discoverability example specifies the trigger term “skip ahead” that triggers the media player application to skip ahead by a predetermined amount of time in a song that the media player application is currently playing.
- the information received from the application developer 101 a - 101 n may be in a particular required format, or the voice action system 100 may induct the information specifying the discoverability example for the “skip ahead” voice action to generate a discoverability example having the particular required format.
- the voice action system 100 may induct the information specifying the “skip ahead” voice action itself, i.e., without receiving other information separately defining a discoverability example for the “skip ahead” voice action, to generate the data defining the “skip ahead” voice action.
- the voice action system 100 may determine candidate notifications for the “skip ahead” voice action based on the received information, and the application developer 101 a - 101 n may select a particular candidate.
- the voice action system 100 may receive information defining the selected candidate discoverability example, or may receive information indicating the selection of the particular candidate and, in response to the selection, may generate a discoverability example for the “skip ahead” voice action according to the selection.
- the generated discoverability example can include information for a notification, such as a textual, image, audio, or video notification, that indicates the trigger term “skip ahead” used to trigger the voice action.
- a notification such as a textual, image, audio, or video notification
- the application developer 101 a - 101 n may submit additional information that includes notification content, and the voice action system 100 can generate a discoverability example for the “skip ahead” voice action that includes the notification content.
- the data defining the discoverability example is associated with the software application at the voice action system 100 , to enable a user device 104 having the software application installed to provide the notification for output when a context of the software application installed on the user device 104 satisfies the specified context.
- the voice action system 100 may store the discoverability example at a voice action discoverability database 170 in association with the software application. Once stored in association with the software application, the discoverability example may be provided for output to a user 105 of a user device 104 when a context of the software application satisfies the context specified for the new voice action.
- the voice action system receives, from a user device having the software application installed, a request for discoverability examples for the software application ( 406 ). For example, once the discoverability example is stored at the voice action discoverability example database 170 , as shown in FIG. 1 C , the voice action system 100 may receive a request from the user device 104 for discoverability examples for the software application. The user device 104 might request discoverability examples for the software application based on any number of reasons.
- the user device 104 may request discoverability examples for the software application installed at the user device 104 based on determining that a status of the software application has changed, may request discoverability examples for the application based on a user 105 of the user device 104 launching the software application or bringing the software application to the foreground of the user device 104 , may request discoverability examples for the application based on a periodic schedule or a schedule set by the user 105 , or may otherwise request discoverability examples for the software application from the voice action system 100 .
- requests for discoverability examples received from the user device 104 may include contextual information indicating a status of the software application installed at the user device 104 , such that the voice action system 100 may utilize the contextual information in selecting discoverability examples to provide to the user device 104 .
- the voice action system provides the data defining the discoverability example to the user device in response to the request, wherein the user device is configured, based at least on the data defining the discoverability example, to provide a notification of the one or more of the trigger terms when a status of the software application satisfies the specified context ( 408 ).
- the voice action system 100 may provide data defining the discoverability example for the new voice action to the user device 104 for storage at the user device 104 , such as at the application metadata backend 180 of the user device 104 .
- the voice action system 100 may be capable of determining that contextual information received from the user device 104 , e.g., as the request for discoverability examples or along with a request for discoverability examples, satisfies the context specified for the new voice action, and may therefore determine to provide the data defining the discoverability example for the new voice action to the user device 104 to control or trigger output of a notification associated with the discoverability example at the user device 104 .
- the voice action system 100 may provide the data defining the discoverability example to the user device 104 in response to the received request, and may store the data defining the discoverability example in association with the new voice action at the application metadata backend 180 .
- Storing the data defining the discoverability example at the application metadata backend 180 may configure the user device 104 to output a notification associated with the discoverability example. For example, when the user device 104 determines that a status of the software application installed on the user device 104 satisfies the context specified for the new voice action, the user device 104 may provide a notification of the one or more trigger terms used to trigger the new voice action for output at the user device 104 .
- the discoverability example for the “skip ahead” voice action may be stored at the voice action discoverability example database 170 in association with the media player application.
- the discoverability example for the “skip ahead” voice action is provided to the user device 104 .
- the discoverability example received by the user device 104 causes the textual notification “Try Saying ‘Skip Ahead’” to be provided for output at the user device 104 .
- the voice action system 100 can provide data defining a discoverability example for the “skip ahead” voice action to the user device 104 in response to a request for discoverability examples.
- the user device 104 can store the discoverability example for the “skip ahead” voice action, for example, at the application metadata backend 180 or by caching the discoverability example for the “skip ahead” voice action in a cache memory of the user device 104 .
- the user device 104 may then be capable of providing the textual notification “Try Saying ‘Skip Ahead’” for output when a determination is made that the status of the media player application satisfies the context specified for the “skip ahead” voice action.
- the textual notification may be presented in a region of a user interface displayed at the user device 104 that is configured to present discoverability examples for voice actions.
- the notification may be presented in a search results page displayed at the user device 104 , may be provided for output in a separate window in a user interface displayed at the user device 104 , may be provided as audio, an image, or video at the user device 104 , or may otherwise be provided for output at the user device 104 .
- the systems and/or methods discussed here may collect personal information about users, or may make use of personal information
- the users may be provided with an opportunity to control whether programs or features collect personal information, e.g., information about a user's social network, social actions or activities, profession, preferences, or current location, or to control whether and/or how the system and/or methods can perform operations more relevant to the user.
- certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be anonymized so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained, such as to a city, ZIP code, or state level, so that a particular location of a user cannot be determined.
- location information such as to a city, ZIP code, or state level, so that a particular location of a user cannot be determined.
- the user may have control over how information is collected about him or her and used.
- the voice action system 100 of FIGS. 1 A to 1 C may be utilized to develop and implement voice actions and discoverability examples for interacting with machinery, where the machinery has an associated computing system, may be used to develop and implement voice actions for interacting with a robot or system having robotic components, may be used to develop and implement voice actions for interacting with appliances, entertainment systems, or other devices, or may be used to develop and implement voice actions for interacting with a vehicle or other transportation system.
- Embodiments and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable medium may be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- FPGA field programmable gate array
- ASIC application specific integrated circuit
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- the processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation, or any combination of one or more such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- HTML file In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.
Abstract
Description
Claims (15)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/936,935 US11929075B2 (en) | 2016-06-06 | 2020-07-23 | Voice action discoverability system |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/173,823 US10049670B2 (en) | 2016-06-06 | 2016-06-06 | Providing voice action discoverability example for trigger term |
US16/101,940 US10741183B2 (en) | 2016-06-06 | 2018-08-13 | Trigger phrase for voice actions of software applications |
US16/936,935 US11929075B2 (en) | 2016-06-06 | 2020-07-23 | Voice action discoverability system |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/101,940 Continuation US10741183B2 (en) | 2016-06-06 | 2018-08-13 | Trigger phrase for voice actions of software applications |
Publications (2)
Publication Number | Publication Date |
---|---|
US20200357411A1 US20200357411A1 (en) | 2020-11-12 |
US11929075B2 true US11929075B2 (en) | 2024-03-12 |
Family
ID=57838540
Family Applications (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/173,823 Active US10049670B2 (en) | 2016-06-06 | 2016-06-06 | Providing voice action discoverability example for trigger term |
US16/101,940 Active US10741183B2 (en) | 2016-06-06 | 2018-08-13 | Trigger phrase for voice actions of software applications |
US16/936,935 Active US11929075B2 (en) | 2016-06-06 | 2020-07-23 | Voice action discoverability system |
Family Applications Before (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/173,823 Active US10049670B2 (en) | 2016-06-06 | 2016-06-06 | Providing voice action discoverability example for trigger term |
US16/101,940 Active US10741183B2 (en) | 2016-06-06 | 2018-08-13 | Trigger phrase for voice actions of software applications |
Country Status (8)
Country | Link |
---|---|
US (3) | US10049670B2 (en) |
EP (1) | EP3465413A1 (en) |
JP (1) | JP6799082B2 (en) |
KR (2) | KR102273095B1 (en) |
CN (2) | CN113571058B (en) |
DE (2) | DE102016125508B4 (en) |
GB (1) | GB2551232B (en) |
WO (1) | WO2017213707A1 (en) |
Families Citing this family (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10049670B2 (en) * | 2016-06-06 | 2018-08-14 | Google Llc | Providing voice action discoverability example for trigger term |
US20180188896A1 (en) * | 2016-12-31 | 2018-07-05 | Entefy Inc. | Real-time context generation and blended input framework for morphing user interface manipulation and navigation |
CN108231076A (en) * | 2018-01-04 | 2018-06-29 | 广州视源电子科技股份有限公司 | A kind of sound control method, device, equipment and storage medium |
US11145291B2 (en) * | 2018-01-31 | 2021-10-12 | Microsoft Technology Licensing, Llc | Training natural language system with generated dialogues |
US10861440B2 (en) * | 2018-02-05 | 2020-12-08 | Microsoft Technology Licensing, Llc | Utterance annotation user interface |
KR102508863B1 (en) * | 2018-03-19 | 2023-03-10 | 삼성전자 주식회사 | A electronic apparatus and a server for processing received data from the apparatus |
US11133001B2 (en) * | 2018-03-20 | 2021-09-28 | Microsoft Technology Licensing, Llc | Generating dialogue events for natural language system |
KR20190130376A (en) * | 2018-05-14 | 2019-11-22 | 삼성전자주식회사 | System for processing user utterance and controlling method thereof |
KR20200048701A (en) * | 2018-10-30 | 2020-05-08 | 삼성전자주식회사 | Electronic apparatus for sharing customized voice command and thereof control method |
WO2020106314A1 (en) * | 2018-11-21 | 2020-05-28 | Google Llc | Consolidation of responses from queries to disparate data sources |
US11972307B2 (en) * | 2019-05-06 | 2024-04-30 | Google Llc | Automated assistant for generating, in response to a request from a user, application input content using application data from other sources |
US20200388280A1 (en) | 2019-06-05 | 2020-12-10 | Google Llc | Action validation for digital assistant-based applications |
EP4270172A3 (en) * | 2019-06-05 | 2024-01-10 | Google LLC | Action validation for digital assistant-based applications |
CN110390935B (en) * | 2019-07-15 | 2021-12-31 | 百度在线网络技术（北京）有限公司 | Voice interaction method and device |
CN110674338B (en) * | 2019-09-27 | 2022-11-01 | 百度在线网络技术（北京）有限公司 | Voice skill recommendation method, device, equipment and storage medium |
CN112581946A (en) * | 2019-09-29 | 2021-03-30 | 百度在线网络技术（北京）有限公司 | Voice control method and device, electronic equipment and readable storage medium |
CN111124347B (en) * | 2019-12-03 | 2023-05-26 | 杭州蓦然认知科技有限公司 | Method and device for forming interaction engine cluster through aggregation |
JP2021093051A (en) * | 2019-12-12 | 2021-06-17 | レノボ・シンガポール・プライベート・リミテッド | Information processing apparatus, and control method |
US11462220B2 (en) * | 2020-03-04 | 2022-10-04 | Accenture Global Solutions Limited | Infrastructure automation platform to assist in performing actions in response to tasks |
WO2021251953A1 (en) * | 2020-06-09 | 2021-12-16 | Google Llc | Generation of interactive audio tracks from visual content |
CN111968631B (en) * | 2020-06-29 | 2023-10-10 | 百度在线网络技术（北京）有限公司 | Interaction method, device, equipment and storage medium of intelligent equipment |
US11558546B2 (en) * | 2020-11-24 | 2023-01-17 | Google Llc | Conditional camera control via automated assistant commands |
CN112579036A (en) * | 2020-12-17 | 2021-03-30 | 南方电网深圳数字电网研究院有限公司 | Voice input report designer realizing method, system, equipment and storage medium |
US11978449B2 (en) | 2021-03-02 | 2024-05-07 | Samsung Electronics Co., Ltd. | Electronic device for processing user utterance and operation method therefor |
KR20230023212A (en) * | 2021-08-10 | 2023-02-17 | 삼성전자주식회사 | Electronic device for outputting result for processing voice command according to the change of the state and operating method thereof |
Citations (79)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5388198A (en) | 1992-04-16 | 1995-02-07 | Symantec Corporation | Proactive presentation of automating features to a computer user |
US6012030A (en) * | 1998-04-21 | 2000-01-04 | Nortel Networks Corporation | Management of speech and audio prompts in multimodal interfaces |
US6085159A (en) * | 1998-03-26 | 2000-07-04 | International Business Machines Corporation | Displaying voice commands with multiple variables |
US6233559B1 (en) | 1998-04-01 | 2001-05-15 | Motorola, Inc. | Speech control of multiple applications using applets |
US6298324B1 (en) * | 1998-01-05 | 2001-10-02 | Microsoft Corporation | Speech recognition system with changing grammars and grammar help command |
US6308157B1 (en) * | 1999-06-08 | 2001-10-23 | International Business Machines Corp. | Method and apparatus for providing an event-based “What-Can-I-Say?” window |
US20020133354A1 (en) | 2001-01-12 | 2002-09-19 | International Business Machines Corporation | System and method for determining utterance context in a multi-context speech application |
US6505160B1 (en) | 1995-07-27 | 2003-01-07 | Digimarc Corporation | Connected audio and other media objects |
US20040260562A1 (en) | 2003-01-30 | 2004-12-23 | Toshihiro Kujirai | Speech interaction type arrangements |
US20050114140A1 (en) | 2003-11-26 | 2005-05-26 | Brackett Charles C. | Method and apparatus for contextual voice cues |
US20060116883A1 (en) | 2004-11-30 | 2006-06-01 | Fuji Xerox Co., Ltd. | Voice guidance system and voice guidance method therefor |
US20060212621A1 (en) * | 2005-02-25 | 2006-09-21 | Microsoft Corporation | Method and system for generating context-aware content from source content associated with a computing device |
CN1855101A (en) | 2005-04-20 | 2006-11-01 | 索尼计算机娱乐公司 | Conversation aid-device |
US20060293874A1 (en) * | 2005-06-27 | 2006-12-28 | Microsoft Corporation | Translation and capture architecture for output of conversational utterances |
US20070033054A1 (en) * | 2005-08-05 | 2007-02-08 | Microsoft Corporation | Selective confirmation for execution of a voice activated user interface |
US20070135096A1 (en) * | 2005-12-14 | 2007-06-14 | Symbol Technologies, Inc. | Interactive voice browsing server for mobile devices on wireless networks |
US20070213984A1 (en) | 2006-03-13 | 2007-09-13 | International Business Machines Corporation | Dynamic help including available speech commands from content contained within speech grammars |
US20080229198A1 (en) | 2004-09-30 | 2008-09-18 | Searete Llc, A Limited Liability Corporaiton Of The State Of Delaware | Electronically providing user assistance |
US20080248797A1 (en) * | 2007-04-03 | 2008-10-09 | Daniel Freeman | Method and System for Operating a Multi-Function Portable Electronic Device Using Voice-Activation |
US20090006100A1 (en) | 2007-06-29 | 2009-01-01 | Microsoft Corporation | Identification and selection of a software application via speech |
US20100312547A1 (en) | 2009-06-05 | 2010-12-09 | Apple Inc. | Contextual voice commands |
US20110144999A1 (en) * | 2009-12-11 | 2011-06-16 | Samsung Electronics Co., Ltd. | Dialogue system and dialogue method thereof |
US8151192B2 (en) | 2008-02-01 | 2012-04-03 | Microsoft Corporation | Context sensitive help |
EP2575128A2 (en) * | 2011-09-30 | 2013-04-03 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US20130085755A1 (en) | 2011-09-30 | 2013-04-04 | Google Inc. | Systems And Methods For Continual Speech Recognition And Detection In Mobile Computing Devices |
US20130198506A1 (en) * | 2012-01-26 | 2013-08-01 | International Business Machines Corporation | Intelligent application recommendation feature |
US20130238991A1 (en) | 2004-10-27 | 2013-09-12 | Searete Llc | Enhanced Contextual User Assistance |
US20130246050A1 (en) | 2012-03-16 | 2013-09-19 | France Telecom | Voice control of applications by associating user input with action-context identifier pairs |
US20130317823A1 (en) | 2012-05-23 | 2013-11-28 | Google Inc. | Customized voice action system |
US20130325484A1 (en) | 2012-05-29 | 2013-12-05 | Samsung Electronics Co., Ltd. | Method and apparatus for executing voice command in electronic device |
KR20140003610A (en) | 2011-03-25 | 2014-01-09 | 미쓰비시덴키 가부시키가이샤 | Elevator call registration device |
CN103517147A (en) | 2012-06-14 | 2014-01-15 | 三星电子株式会社 | Display apparatus, interactive server, and method for providing response information |
CN103591947A (en) | 2012-08-13 | 2014-02-19 | 百度在线网络技术（北京）有限公司 | Voice background navigation method of mobile terminal and mobile terminal |
US8688453B1 (en) | 2011-02-28 | 2014-04-01 | Nuance Communications, Inc. | Intent mining via analysis of utterances |
KR20140039961A (en) | 2012-09-20 | 2014-04-02 | 삼성전자주식회사 | Method and apparatus for providing context aware service in a user device |
US20140108927A1 (en) | 2012-10-16 | 2014-04-17 | Cellco Partnership D/B/A Verizon Wireless | Gesture based context-sensitive funtionality |
US20140136013A1 (en) * | 2012-11-15 | 2014-05-15 | Sri International | Vehicle personal assistant |
US20140136195A1 (en) | 2012-11-13 | 2014-05-15 | Unified Computer Intelligence Corporation | Voice-Operated Internet-Ready Ubiquitous Computing Device and Method Thereof |
US20140164312A1 (en) | 2012-12-11 | 2014-06-12 | Nuance Communications, Inc. | Systems and methods for informing virtual agent recommendation |
CN103902629A (en) | 2012-12-28 | 2014-07-02 | 联想(北京)有限公司 | Electronic device and method for offering operation help through speech |
CN103916708A (en) | 2013-01-07 | 2014-07-09 | 三星电子株式会社 | Display apparatus and method for controlling the display apparatus |
JP2014134675A (en) | 2013-01-10 | 2014-07-24 | Ntt Docomo Inc | Function execution system and speech example output method |
US20140229110A1 (en) | 2012-03-26 | 2014-08-14 | Navteq B.V. | Reverse Natural Guidance |
US20140244271A1 (en) | 2008-10-02 | 2014-08-28 | Apple Inc. | Electronic Devices with Voice Command and Contextual Data Processing Capabilities |
US20140278419A1 (en) | 2013-03-14 | 2014-09-18 | Microsoft Corporation | Voice command definitions used in launching application with a command |
US8849675B1 (en) | 2013-12-18 | 2014-09-30 | Google Inc. | Suggested query constructor for voice actions |
US8862467B1 (en) * | 2013-12-11 | 2014-10-14 | Google Inc. | Contextual speech recognition |
US20140323142A1 (en) | 2009-10-28 | 2014-10-30 | Digimarc Corporation | Intuitive computing methods and systems |
CN104217719A (en) | 2014-09-03 | 2014-12-17 | 深圳如果技术有限公司 | Triggering processing method |
US8938394B1 (en) * | 2014-01-09 | 2015-01-20 | Google Inc. | Audio triggers based on context |
US20150039319A1 (en) | 2012-08-09 | 2015-02-05 | Huawei Device Co., Ltd. | Command Handling Method, Apparatus, and System |
WO2015030796A1 (en) * | 2013-08-30 | 2015-03-05 | Intel Corporation | Extensible context-aware natural language interactions for virtual personal assistants |
US20150079943A1 (en) * | 2013-09-19 | 2015-03-19 | Aegis Mobility, Inc. | Restricting functionality of protected devices |
US20150106836A1 (en) | 2001-10-03 | 2015-04-16 | Promptu Systems Corporation | Global speech user interface |
US20150124944A1 (en) | 2013-11-01 | 2015-05-07 | Plantronics, Inc. | Interactive Device Registration, Setup and Use |
US20150179174A1 (en) | 2008-06-25 | 2015-06-25 | Verint Systems Ltd. | System and method for context sensitive inference in a speech processing system |
US20150185993A1 (en) * | 2013-12-27 | 2015-07-02 | United Video Properties, Inc. | Methods and systems for selecting modes based on the level of engagement of a user |
US9094453B2 (en) | 2013-11-06 | 2015-07-28 | Google Technology Holdings LLC | Method and apparatus for associating mobile devices using audio signature detection |
US9123330B1 (en) | 2013-05-01 | 2015-09-01 | Google Inc. | Large-scale speaker identification |
US20150254058A1 (en) | 2014-03-04 | 2015-09-10 | Microsoft Technology Licensing, Llc | Voice control shortcuts |
US20150309561A1 (en) | 2014-04-25 | 2015-10-29 | Lenovo (Singapore) Ptd. Ltd. | Strengthening prediction confidence and command priority using natural user interface (nui) inputs |
US20150379981A1 (en) | 2014-06-26 | 2015-12-31 | Nuance Communications, Inc. | Automatically presenting different user experiences, such as customized voices in automated communication systems |
US9257133B1 (en) | 2013-11-26 | 2016-02-09 | Amazon Technologies, Inc. | Secure input to a computing device |
KR20160022326A (en) | 2016-02-04 | 2016-02-29 | 삼성전자주식회사 | Display apparatus and method for controlling the display apparatus |
US20160098988A1 (en) | 2014-10-06 | 2016-04-07 | Nuance Communications, Inc. | Automatic data-driven dialog discovery system |
US20160133254A1 (en) * | 2014-11-06 | 2016-05-12 | Microsoft Technology Licensing, Llc | Context-based actions |
US20160132342A1 (en) * | 2014-11-06 | 2016-05-12 | Microsoft Technology Licensing, Llc | Context-based command surfacing |
US20160210451A1 (en) | 2015-01-15 | 2016-07-21 | Qualcomm Incorporated | Context-based access verification |
US20160219048A1 (en) * | 2015-01-27 | 2016-07-28 | Sri International | Natural language dialog-based security help agent for network administrator |
US20160293164A1 (en) | 2015-03-30 | 2016-10-06 | Alibaba Group Holding Limited | Method and apparatus for voice control |
US9466293B1 (en) | 2007-10-04 | 2016-10-11 | Samsung Electronics Co., Ltd. | Speech interface system and method for control and interaction with applications on a computing system |
US9472196B1 (en) | 2015-04-22 | 2016-10-18 | Google Inc. | Developer voice actions system |
US9691384B1 (en) | 2016-08-19 | 2017-06-27 | Google Inc. | Voice action biasing system |
US20170213559A1 (en) | 2016-01-27 | 2017-07-27 | Motorola Mobility Llc | Method and apparatus for managing multiple voice operation trigger phrases |
US9741343B1 (en) | 2013-12-19 | 2017-08-22 | Amazon Technologies, Inc. | Voice interaction application selection |
US20170256256A1 (en) | 2016-03-01 | 2017-09-07 | Google Inc. | Developer voice actions system |
US10049670B2 (en) | 2016-06-06 | 2018-08-14 | Google Llc | Providing voice action discoverability example for trigger term |
US10210003B2 (en) | 2014-09-30 | 2019-02-19 | Nuance Communications, Inc. | Methods and apparatus for module arbitration |
US10600418B2 (en) | 2016-12-07 | 2020-03-24 | Google Llc | Voice to text conversion based on third-party agent content |
-
2016
- 2016-06-06 US US15/173,823 patent/US10049670B2/en active Active
- 2016-12-19 GB GB1621600.4A patent/GB2551232B/en active Active
- 2016-12-22 DE DE102016125508.4A patent/DE102016125508B4/en active Active
- 2016-12-22 DE DE202016107300.6U patent/DE202016107300U1/en active Active
- 2016-12-29 KR KR1020217005450A patent/KR102273095B1/en active IP Right Grant
- 2016-12-29 KR KR1020187037683A patent/KR102223016B1/en active IP Right Grant
- 2016-12-29 WO PCT/US2016/069268 patent/WO2017213707A1/en unknown
- 2016-12-29 JP JP2018562608A patent/JP6799082B2/en active Active
- 2016-12-29 EP EP16828889.2A patent/EP3465413A1/en not_active Withdrawn
- 2016-12-30 CN CN202110677138.5A patent/CN113571058B/en active Active
- 2016-12-30 CN CN201611259927.2A patent/CN107464561B/en active Active
-
2018
- 2018-08-13 US US16/101,940 patent/US10741183B2/en active Active
-
2020
- 2020-07-23 US US16/936,935 patent/US11929075B2/en active Active
Patent Citations (92)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5388198A (en) | 1992-04-16 | 1995-02-07 | Symantec Corporation | Proactive presentation of automating features to a computer user |
US6505160B1 (en) | 1995-07-27 | 2003-01-07 | Digimarc Corporation | Connected audio and other media objects |
US6298324B1 (en) * | 1998-01-05 | 2001-10-02 | Microsoft Corporation | Speech recognition system with changing grammars and grammar help command |
US6085159A (en) * | 1998-03-26 | 2000-07-04 | International Business Machines Corporation | Displaying voice commands with multiple variables |
US6233559B1 (en) | 1998-04-01 | 2001-05-15 | Motorola, Inc. | Speech control of multiple applications using applets |
US6012030A (en) * | 1998-04-21 | 2000-01-04 | Nortel Networks Corporation | Management of speech and audio prompts in multimodal interfaces |
US6308157B1 (en) * | 1999-06-08 | 2001-10-23 | International Business Machines Corp. | Method and apparatus for providing an event-based “What-Can-I-Say?” window |
US20020133354A1 (en) | 2001-01-12 | 2002-09-19 | International Business Machines Corporation | System and method for determining utterance context in a multi-context speech application |
US20150106836A1 (en) | 2001-10-03 | 2015-04-16 | Promptu Systems Corporation | Global speech user interface |
US20040260562A1 (en) | 2003-01-30 | 2004-12-23 | Toshihiro Kujirai | Speech interaction type arrangements |
US20050114140A1 (en) | 2003-11-26 | 2005-05-26 | Brackett Charles C. | Method and apparatus for contextual voice cues |
US20080229198A1 (en) | 2004-09-30 | 2008-09-18 | Searete Llc, A Limited Liability Corporaiton Of The State Of Delaware | Electronically providing user assistance |
US20130238991A1 (en) | 2004-10-27 | 2013-09-12 | Searete Llc | Enhanced Contextual User Assistance |
US20060116883A1 (en) | 2004-11-30 | 2006-06-01 | Fuji Xerox Co., Ltd. | Voice guidance system and voice guidance method therefor |
US20060212621A1 (en) * | 2005-02-25 | 2006-09-21 | Microsoft Corporation | Method and system for generating context-aware content from source content associated with a computing device |
CN1855101A (en) | 2005-04-20 | 2006-11-01 | 索尼计算机娱乐公司 | Conversation aid-device |
US20060293874A1 (en) * | 2005-06-27 | 2006-12-28 | Microsoft Corporation | Translation and capture architecture for output of conversational utterances |
US20070033054A1 (en) * | 2005-08-05 | 2007-02-08 | Microsoft Corporation | Selective confirmation for execution of a voice activated user interface |
US20070135096A1 (en) * | 2005-12-14 | 2007-06-14 | Symbol Technologies, Inc. | Interactive voice browsing server for mobile devices on wireless networks |
JP2007249200A (en) | 2006-03-13 | 2007-09-27 | Internatl Business Mach Corp <Ibm> | Method and system for providing help to voice-enabled application, and speech grammar |
CN101038743A (en) | 2006-03-13 | 2007-09-19 | 国际商业机器公司 | Method and system for providing help to voice-enabled applications |
US20070213984A1 (en) | 2006-03-13 | 2007-09-13 | International Business Machines Corporation | Dynamic help including available speech commands from content contained within speech grammars |
US20080248797A1 (en) * | 2007-04-03 | 2008-10-09 | Daniel Freeman | Method and System for Operating a Multi-Function Portable Electronic Device Using Voice-Activation |
US20090006100A1 (en) | 2007-06-29 | 2009-01-01 | Microsoft Corporation | Identification and selection of a software application via speech |
US9466293B1 (en) | 2007-10-04 | 2016-10-11 | Samsung Electronics Co., Ltd. | Speech interface system and method for control and interaction with applications on a computing system |
US8151192B2 (en) | 2008-02-01 | 2012-04-03 | Microsoft Corporation | Context sensitive help |
US20150179174A1 (en) | 2008-06-25 | 2015-06-25 | Verint Systems Ltd. | System and method for context sensitive inference in a speech processing system |
US20140244271A1 (en) | 2008-10-02 | 2014-08-28 | Apple Inc. | Electronic Devices with Voice Command and Contextual Data Processing Capabilities |
US20190189125A1 (en) * | 2009-06-05 | 2019-06-20 | Apple Inc. | Contextual voice commands |
US20100312547A1 (en) | 2009-06-05 | 2010-12-09 | Apple Inc. | Contextual voice commands |
US20140323142A1 (en) | 2009-10-28 | 2014-10-30 | Digimarc Corporation | Intuitive computing methods and systems |
US20110144999A1 (en) * | 2009-12-11 | 2011-06-16 | Samsung Electronics Co., Ltd. | Dialogue system and dialogue method thereof |
US8688453B1 (en) | 2011-02-28 | 2014-04-01 | Nuance Communications, Inc. | Intent mining via analysis of utterances |
KR20140003610A (en) | 2011-03-25 | 2014-01-09 | 미쓰비시덴키 가부시키가이샤 | Elevator call registration device |
US20130085755A1 (en) | 2011-09-30 | 2013-04-04 | Google Inc. | Systems And Methods For Continual Speech Recognition And Detection In Mobile Computing Devices |
EP2575128A2 (en) * | 2011-09-30 | 2013-04-03 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US20130198506A1 (en) * | 2012-01-26 | 2013-08-01 | International Business Machines Corporation | Intelligent application recommendation feature |
US20130246050A1 (en) | 2012-03-16 | 2013-09-19 | France Telecom | Voice control of applications by associating user input with action-context identifier pairs |
US20140229110A1 (en) | 2012-03-26 | 2014-08-14 | Navteq B.V. | Reverse Natural Guidance |
US20160273933A1 (en) | 2012-03-26 | 2016-09-22 | Here Global B.V. | Reverse Natural Guidance |
US20130317823A1 (en) | 2012-05-23 | 2013-11-28 | Google Inc. | Customized voice action system |
US20130325484A1 (en) | 2012-05-29 | 2013-12-05 | Samsung Electronics Co., Ltd. | Method and apparatus for executing voice command in electronic device |
CN103517147A (en) | 2012-06-14 | 2014-01-15 | 三星电子株式会社 | Display apparatus, interactive server, and method for providing response information |
US20150039319A1 (en) | 2012-08-09 | 2015-02-05 | Huawei Device Co., Ltd. | Command Handling Method, Apparatus, and System |
CN103591947A (en) | 2012-08-13 | 2014-02-19 | 百度在线网络技术（北京）有限公司 | Voice background navigation method of mobile terminal and mobile terminal |
KR20140039961A (en) | 2012-09-20 | 2014-04-02 | 삼성전자주식회사 | Method and apparatus for providing context aware service in a user device |
US20140108927A1 (en) | 2012-10-16 | 2014-04-17 | Cellco Partnership D/B/A Verizon Wireless | Gesture based context-sensitive funtionality |
US20140136195A1 (en) | 2012-11-13 | 2014-05-15 | Unified Computer Intelligence Corporation | Voice-Operated Internet-Ready Ubiquitous Computing Device and Method Thereof |
US20140136013A1 (en) * | 2012-11-15 | 2014-05-15 | Sri International | Vehicle personal assistant |
US20140164312A1 (en) | 2012-12-11 | 2014-06-12 | Nuance Communications, Inc. | Systems and methods for informing virtual agent recommendation |
CN103902629A (en) | 2012-12-28 | 2014-07-02 | 联想(北京)有限公司 | Electronic device and method for offering operation help through speech |
US20140195243A1 (en) | 2013-01-07 | 2014-07-10 | Samsung Electronics Co., Ltd. | Display apparatus and method for controlling the display apparatus |
CN103916708A (en) | 2013-01-07 | 2014-07-09 | 三星电子株式会社 | Display apparatus and method for controlling the display apparatus |
KR20140089861A (en) | 2013-01-07 | 2014-07-16 | 삼성전자주식회사 | display apparatus and method for controlling the display apparatus |
JP2014134675A (en) | 2013-01-10 | 2014-07-24 | Ntt Docomo Inc | Function execution system and speech example output method |
US20140278419A1 (en) | 2013-03-14 | 2014-09-18 | Microsoft Corporation | Voice command definitions used in launching application with a command |
US20160275949A1 (en) * | 2013-03-14 | 2016-09-22 | Microsoft Technology Licensing, Llc | Voice command definitions used in launching application with a command |
US9123330B1 (en) | 2013-05-01 | 2015-09-01 | Google Inc. | Large-scale speaker identification |
WO2015030796A1 (en) * | 2013-08-30 | 2015-03-05 | Intel Corporation | Extensible context-aware natural language interactions for virtual personal assistants |
US20150079943A1 (en) * | 2013-09-19 | 2015-03-19 | Aegis Mobility, Inc. | Restricting functionality of protected devices |
US20150124944A1 (en) | 2013-11-01 | 2015-05-07 | Plantronics, Inc. | Interactive Device Registration, Setup and Use |
US9094453B2 (en) | 2013-11-06 | 2015-07-28 | Google Technology Holdings LLC | Method and apparatus for associating mobile devices using audio signature detection |
US9257133B1 (en) | 2013-11-26 | 2016-02-09 | Amazon Technologies, Inc. | Secure input to a computing device |
US8862467B1 (en) * | 2013-12-11 | 2014-10-14 | Google Inc. | Contextual speech recognition |
US8849675B1 (en) | 2013-12-18 | 2014-09-30 | Google Inc. | Suggested query constructor for voice actions |
US9741343B1 (en) | 2013-12-19 | 2017-08-22 | Amazon Technologies, Inc. | Voice interaction application selection |
US20150185993A1 (en) * | 2013-12-27 | 2015-07-02 | United Video Properties, Inc. | Methods and systems for selecting modes based on the level of engagement of a user |
US8938394B1 (en) * | 2014-01-09 | 2015-01-20 | Google Inc. | Audio triggers based on context |
US20150254058A1 (en) | 2014-03-04 | 2015-09-10 | Microsoft Technology Licensing, Llc | Voice control shortcuts |
US20150309561A1 (en) | 2014-04-25 | 2015-10-29 | Lenovo (Singapore) Ptd. Ltd. | Strengthening prediction confidence and command priority using natural user interface (nui) inputs |
US20150379981A1 (en) | 2014-06-26 | 2015-12-31 | Nuance Communications, Inc. | Automatically presenting different user experiences, such as customized voices in automated communication systems |
CN104217719A (en) | 2014-09-03 | 2014-12-17 | 深圳如果技术有限公司 | Triggering processing method |
US10210003B2 (en) | 2014-09-30 | 2019-02-19 | Nuance Communications, Inc. | Methods and apparatus for module arbitration |
US20160098988A1 (en) | 2014-10-06 | 2016-04-07 | Nuance Communications, Inc. | Automatic data-driven dialog discovery system |
US20160132342A1 (en) * | 2014-11-06 | 2016-05-12 | Microsoft Technology Licensing, Llc | Context-based command surfacing |
US20160133254A1 (en) * | 2014-11-06 | 2016-05-12 | Microsoft Technology Licensing, Llc | Context-based actions |
US20160210451A1 (en) | 2015-01-15 | 2016-07-21 | Qualcomm Incorporated | Context-based access verification |
US20160219048A1 (en) * | 2015-01-27 | 2016-07-28 | Sri International | Natural language dialog-based security help agent for network administrator |
US20160293164A1 (en) | 2015-03-30 | 2016-10-06 | Alibaba Group Holding Limited | Method and apparatus for voice control |
US20170186427A1 (en) | 2015-04-22 | 2017-06-29 | Google Inc. | Developer voice actions system |
US20160314791A1 (en) | 2015-04-22 | 2016-10-27 | Google Inc. | Developer voice actions system |
US9472196B1 (en) | 2015-04-22 | 2016-10-18 | Google Inc. | Developer voice actions system |
US20170213559A1 (en) | 2016-01-27 | 2017-07-27 | Motorola Mobility Llc | Method and apparatus for managing multiple voice operation trigger phrases |
KR20160022326A (en) | 2016-02-04 | 2016-02-29 | 삼성전자주식회사 | Display apparatus and method for controlling the display apparatus |
US20170256256A1 (en) | 2016-03-01 | 2017-09-07 | Google Inc. | Developer voice actions system |
US9922648B2 (en) | 2016-03-01 | 2018-03-20 | Google Llc | Developer voice actions system |
US10049670B2 (en) | 2016-06-06 | 2018-08-14 | Google Llc | Providing voice action discoverability example for trigger term |
US10741183B2 (en) * | 2016-06-06 | 2020-08-11 | Google Llc | Trigger phrase for voice actions of software applications |
US9691384B1 (en) | 2016-08-19 | 2017-06-27 | Google Inc. | Voice action biasing system |
US20180053507A1 (en) | 2016-08-19 | 2018-02-22 | Google Inc. | Voice action biasing system |
US10089982B2 (en) | 2016-08-19 | 2018-10-02 | Google Llc | Voice action biasing system |
US10600418B2 (en) | 2016-12-07 | 2020-03-24 | Google Llc | Voice to text conversion based on third-party agent content |
Non-Patent Citations (24)
Title |
---|
China National Intellectual Property Administration; Notice of Allowance issued in Application No. 201611259927.2; 4 pages; dated Apr. 7, 2021. |
China National Intellectual Property Administration; Notice of Grant issued in Application No. 2021106771385; 6 pages dated Aug. 10, 2022. |
China National Intellectual Property Administration; Office Action issue in Application No. 201611259927.2; dated Mar. 31, 2020. |
China National Intellectual Property Administration; Office Action issued for Application No. 202110677138.5, 7 pages, dated Jan. 28, 2022. |
China National Intellectual Property Administration; Second Office Action issue in Application No. 201611259927.2; 11 pages; dated Jan. 13, 2021. |
Deutsches Patent Office; Decision to Grant issue in Application No. 10202016125508.4; 13 pages; dated Nov. 29, 2021. |
Deutsches Patent Office; Examination Report issue in Application No. 10202016125508.4; 3 pages; dated Oct. 5, 2021. |
Deutsches Patent Office; Office Action issue in Application No. 10202016125508.4; 6 pages; dated Jan. 20, 2021. |
Deutsches Patent Office; Summons issue in Application No. 10202016125508.4; 19 pages; dated Jul. 30, 2021. |
European Patent Office; Communication Pursuant to Article 94(3) EPC issued in Application No. 16828889.2; 7 pages; dated Mar 23, 2021. |
European Patent Office; International Preliminary Report on Patentability of Ser. No. PCT/US2016/069268; 20 pages; dated Sep. 19, 2018. |
European Patent Office; International Search Report and the Written Opinion from PCT Application No. PCT/US2016/069268, dated Mar. 30, 2017 12 pages. |
European Patent Office; Summons to Attend Oral Proceedings issued in Application No. 16828889.2, 10 pages, dated Feb. 15, 2022. |
European Patent Office; Written Opinion of the International Preliminary Examining Authority of International Application No. PCT/US2016/069268; 7 pages; dated Apr. 19, 2018. |
Intellectual Property India; Office Action issued in Application No. 201927000502; 8 pages; dated Feb. 25, 2021. |
The Japanese Patent Office; Final Office Action issued in Application No. 2018-562608; 4 pages; dated Jun. 8, 2020. |
The Japanese Patent Office; Notice of Allowance issued in Application No. 2018-562608; 3 Pages; dated Oct. 26, 2020. |
The Japanese Patent Office; Office Action issued in Application No. 2018-562608 dated Feb. 10, 2020. (6 Pages). |
The Korean Intellectual Property Office; Notice of Allowance issue in Application No. 1020187037683; 4 pages; dated Dec. 1, 2020. |
The Korean Intellectual Property Office; Notice of Allowance issued in Application No. 20217005450; 6 pages; dated Apr. 9, 2021. |
The Korean Intellectual Property Office; Notice of Office Action issue in Application No. 1020187037683; 13 pages; dated May 20, 2020. |
United Kingdom Intellectual Property Office; Combined Search and Examination Report issued in Application No. GB1621600.4, dated Jun. 15, 2017, 8 pages. |
United Kingdom Intellectual Property Office; Intention to Grant issued for Application No. GB1621600.4 dated Dec. 21, 2018. |
United Kingdon Intellectual Property Office; Examination Report under Section 18(3) of Application No. GB1621600.4; 2 pages; dated Oct. 30, 2018. |
Also Published As
Publication number | Publication date |
---|---|
WO2017213707A1 (en) | 2017-12-14 |
DE102016125508A1 (en) | 2017-12-07 |
KR102223016B1 (en) | 2021-03-04 |
GB2551232B (en) | 2019-03-13 |
KR20190011286A (en) | 2019-02-01 |
US20190103114A1 (en) | 2019-04-04 |
DE202016107300U1 (en) | 2017-09-08 |
US20200357411A1 (en) | 2020-11-12 |
US10741183B2 (en) | 2020-08-11 |
JP2019523930A (en) | 2019-08-29 |
EP3465413A1 (en) | 2019-04-10 |
GB201621600D0 (en) | 2017-02-01 |
DE102016125508B4 (en) | 2022-03-10 |
JP6799082B2 (en) | 2020-12-09 |
CN107464561B (en) | 2021-06-29 |
KR102273095B1 (en) | 2021-07-05 |
CN113571058A (en) | 2021-10-29 |
CN113571058B (en) | 2022-11-11 |
US10049670B2 (en) | 2018-08-14 |
US20170352352A1 (en) | 2017-12-07 |
CN107464561A (en) | 2017-12-12 |
KR20210024224A (en) | 2021-03-04 |
GB2551232A (en) | 2017-12-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11929075B2 (en) | Voice action discoverability system | |
US9922648B2 (en) | Developer voice actions system | |
US10089982B2 (en) | Voice action biasing system | |
US11657816B2 (en) | Developer voice actions system | |
US20230335115A1 (en) | Systems and methods for crowdsourced actions and commands | |
US10698654B2 (en) | Ranking and boosting relevant distributable digital assistant operations | |
JP7439186B2 (en) | Coordinating overlapping audio queries | |
WO2020018826A1 (en) | Systems and methods for crowdsourced actions and commands |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:WANG, BO;VEMURI, SUNIL;JAMES, BARNABY JOHN;AND OTHERS;SIGNING DATES FROM 20160601 TO 20160603;REEL/FRAME:053385/0628Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:053387/0484Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |