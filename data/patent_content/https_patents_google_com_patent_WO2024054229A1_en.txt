WO2024054229A1 - Seamless spelling for automatic speech recognition systems - Google Patents
Seamless spelling for automatic speech recognition systems Download PDFInfo
- Publication number
- WO2024054229A1 WO2024054229A1 PCT/US2022/076074 US2022076074W WO2024054229A1 WO 2024054229 A1 WO2024054229 A1 WO 2024054229A1 US 2022076074 W US2022076074 W US 2022076074W WO 2024054229 A1 WO2024054229 A1 WO 2024054229A1
- Authority
- WO
- WIPO (PCT)
- Prior art keywords
- phrase
- individual characters
- spelling
- particular phrase
- sequence
- Prior art date
Links
- 238000013518 transcription Methods 0.000 claims abstract description 164
- 230000035897 transcription Effects 0.000 claims abstract description 164
- 238000000034 method Methods 0.000 claims abstract description 41
- 238000012545 processing Methods 0.000 claims abstract description 34
- 230000004044 response Effects 0.000 claims abstract description 10
- 230000015654 memory Effects 0.000 claims description 34
- 230000000977 initiatory effect Effects 0.000 claims description 29
- 238000004891 communication Methods 0.000 claims description 10
- 238000004364 calculation method Methods 0.000 claims description 5
- 238000001356 surgical procedure Methods 0.000 description 10
- 238000004590 computer program Methods 0.000 description 8
- 241000282994 Cervidae Species 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 6
- 230000008569 process Effects 0.000 description 5
- 239000000284 extract Substances 0.000 description 3
- 238000012805 post-processing Methods 0.000 description 3
- 208000027697 autoimmune lymphoproliferative syndrome due to CTLA4 haploinsuffiency Diseases 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000012937 correction Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 238000012549 training Methods 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 1
- 238000012217 deletion Methods 0.000 description 1
- 230000037430 deletion Effects 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000003780 insertion Methods 0.000 description 1
- 230000037431 insertion Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000004984 smart glass Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000006467 substitution reaction Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/193—Formal grammars, e.g. finite state automata, context free grammars or word networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/09—Supervised learning
Definitions
- This disclosure relates to providing seamless spelling for automatic speech recognition (ASR) systems.
- ASR automatic speech recognition
- ASR systems provide a technology that is typically used in mobile devices and/or other devices. In general, ASR systems attempt to provide accurate transcriptions of what a user speaks to a device. However, in some instances, ASR systems may generate transcriptions that do not match what the user intended or actually spoke.
- One aspect of the present disclosure provides a computer-implemented method that, when executed on data processing hardware, causes the data processing hardware to perform operations that include receiving audio data characterizing an utterance spoken by a user and processing, using an automatic speech recognition (ASR) model, the audio data to generate an initial transcription for the utterance.
- the utterance includes a particular phrase and a sequence of characters that provide a correct spelling of the particular phrase spoken after the particular phrase.
- the initial transcription includes a misrecognition of the particular phrase by the ASR model followed by the sequence of individual characters that provide the correct spelling of the particular phrase.
- the operations also include detecting a spelling structure in the initial transcription subsequent to the misrecognition of the particular phrase.
- the operations also include extracting, from the initial transcription, the sequence of individual characters that provide the correct spelling of the particular phrase, constructing a corrected phrase for the misrecognition of the particular phrase from the extracted sequence of individual characters, extracting, from the initial transcription, the misrecognition of the particular phrase, and normalizing the initial transcription to obtain a final transcription for the utterance by replacing the extracted misrecognition of the particular phrase with the corrected phrase constructed from the extracted sequence of individual characters.
- Implementations of the present disclosure include one or more of the following optional features.
- receiving the audio data includes receiving the audio data as the user speaks the utterance
- performing speech recognition on the audio data includes performing streaming speech recognition on the audio data as the audio data is received to generate, as output from the ASR model, streaming speech recognition result.
- the operations also include providing each streaming speech recognition result generated as output from the ASR model for display on a screen in communication with the data processing hardware.
- the operations also include displaying a partial speech recognition result including the misrecognition of the particular phrase on the screen before the user speaks the sequence of individual characters comprising the spelling of the particular phrase.
- detecting the spelling structure may include executing a weighted finite state transducer (wFST) having a spelling component configured to detect the spelling structure in the initial transcription by identifying one or more spell trigger words in the initial transcription subsequent to the misrecognition of the particular phrase and/or one or more initiating trigger words preceding the misrecognition of the particular phrase.
- the one or more initiating trigger words and the one or more spell trigger words may form a predefined spell command.
- the operations also include, after constructing the corrected phrase from the extracted sequence of individual characters, applying a capitalization normalizer to capitalize a first letter of at least one word in the corrected phrase.
- the misrecognition of the particular phrase may include one or more words.
- extracting the misrecognition of the particular phrase may be based on an edit distance between the sequence of individual characters in the initial transcription and the misrecognition of the particular phrase.
- the particular phrase spoken by the user in the utterance includes two or more particular words
- the sequence of individual characters that provide the correct spelling of the particular phrase includes two or more spans of consecutive individual characters that each provide a correct spelling for a corresponding one of the two or more particular words
- the utterance spoken by the user further includes the user speaking a space command between each adjacent pair of the two or more spans of consecutive individual characters.
- the initial transcription may further include a space token inserted between each adjacent pair of the two or more spans of consecutive individual characters of the sequence of individual characters, and constructing the corrected phrase from the extracted sequence of individual characters may include: for each span of consecutive individual characters, joining the individual characters to form the corresponding one of the two or more particular words; and replacing each space token in the initial transcription with a blank space.
- extracting the misrecognition of the particular phrase includes determining a corresponding edit distance between the sequence of individual characters and each word span of multiple word spans of different lengths in the initial transcription and identifying the word span of the multiple word spans of different lengths that has a shortest corresponding edit distance to the misrecognition of the particular phrase.
- the multiple word spans precede the sequence of individual characters.
- the corresponding edit distance may include a corresponding Levenshtein distance dynamically computed for each of the multiple word spans via a single calculation.
- detecting the spelling structure in the initial transcription and normalizing the initial transcription to obtain the final transcription may occur without receiving any user input after the user finishes speaking the utterance.
- Another aspect of the present disclosure provides a system that includes data processing hardware and memory hardware in communication with the data processing hardware and storing instructions that, when executed on the data processing hardware, cause the data processing hardware to perform operations that include receiving audio data characterizing an utterance spoken by a user and processing, using an automatic speech recognition (ASR) model, the audio data to generate an initial transcription for the utterance.
- the utterance includes a particular phrase and a sequence of characters that provide a correct spelling of the particular phrase spoken after the particular phrase.
- the initial transcription includes a misrecognition of the particular phrase by the ASR model followed by the sequence of individual characters that provide the correct spelling of the particular phrase.
- the operations also include detecting a spelling structure in the initial transcription subsequent to the misrecognition of the particular phrase.
- the operations also include extracting, from the initial transcription, the sequence of individual characters that provide the correct spelling of the particular phrase, constructing a corrected phrase for the misrecognition of the particular phrase from the extracted sequence of individual characters, extracting, from the initial transcription, the misrecognition of the particular phrase, and normalizing the initial transcription to obtain a final transcription for the utterance by replacing the extracted misrecognition of the particular phrase with the corrected phrase constructed from the extracted sequence of individual characters.
- Implementations of the present disclosure include one or more of the following optional features.
- receiving the audio data includes receiving the audio data as the user speaks the utterance
- performing speech recognition on the audio data includes performing streaming speech recognition on the audio data as the audio data is received to generate, as output from the ASR model, streaming speech recognition result.
- the operations also include providing each streaming speech recognition result generated as output from the ASR model for display on a screen in communication with the data processing hardware.
- the operations also include displaying a partial speech recognition result including the misrecognition of the particular phrase on the screen before the user speaks the sequence of individual characters comprising the spelling of the particular phrase.
- detecting the spelling structure may include executing a weighted finite state transducer (wFST) having a spelling component configured to detect the spelling structure in the initial transcription by identifying one or more spell trigger words in the initial transcription subsequent to the misrecognition of the particular phrase and/or one or more initiating trigger words preceding the misrecognition of the particular phrase.
- the one or more initiating trigger words and the one or more spell trigger words may form a predefined spell command.
- the operations also include, after constructing the corrected phrase from the extracted sequence of individual characters, applying a capitalization normalizer to capitalize a first letter of at least one word in the corrected phrase.
- the misrecognition of the particular phrase may include one or more words.
- extracting the misrecognition of the particular phrase may be based on an edit distance between the sequence of individual characters in the initial transcription and the misrecognition of the particular phrase.
- the particular phrase spoken by the user in the utterance includes two or more particular words
- the sequence of individual characters that provide the correct spelling of the particular phrase includes two or more spans of consecutive individual characters that each provide a correct spelling for a corresponding one of the two or more particular words
- the utterance spoken by the user further includes the user speaking a space command between each adjacent pair of the two or more spans of consecutive individual characters.
- the initial transcription may further include a space token inserted between each adjacent pair of the two or more spans of consecutive individual characters of the sequence of individual characters, and constructing the corrected phrase from the extracted sequence of individual characters may include: for each span of consecutive individual characters, joining the individual characters to form the corresponding one of the two or more particular words; and replacing each space token in the initial transcription with a blank space.
- extracting the misrecognition of the particular phrase includes determining a corresponding edit distance between the sequence of individual characters and each word span of multiple word spans of different lengths in the initial transcription and identifying the word span of the multiple word spans of different lengths that has a shortest corresponding edit distance to the misrecognition of the particular phrase.
- the multiple word spans precede the sequence of individual characters.
- the corresponding edit distance may include a corresponding Levenshtein distance dynamically computed for each of the multiple word spans via a single calculation.
- detecting the spelling structure in the initial transcription and normalizing the initial transcription to obtain the final transcription may occur without receiving any user input after the user finishes speaking the utterance
- FIG. l is a schematic view of an example system for providing seamless spelling for automatic speech recognition (ASR) systems.
- ASR automatic speech recognition
- FIGS. 2A and 2B are schematic views of diagrams that illustrate examples of word lattices.
- FIG. 3 is a schematic view of diagrams that illustrate an example spelling component of a finite state transducer for seamlessly detecting spelling structures.
- FIG. 4 is a table of example Levenshtein distances between sub-strings.
- FIG. 5 is a flowchart of an exemplary arrangement of operations for a method of providing seamless spelling for ASR systems.
- FIG. 6 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- ASR Automatic speech recognition
- a user may speak a proper name, such as “Khe Chai.”
- the ASR system may incorrectly transcribe what the user spoke as another word or phrase (e.g., “kitchen”) that is acoustically similar to “Khe Chai” and is already known to the ASR system.
- kitchen another word or phrase
- spelling is a simple way for users to voice input words that are not recognized by an ASR system (e.g., proper names)
- conventional ASR systems often do not handle spelling of particular words/phrases reliably and generally require some type of post-processing technique after an entire utterance is transcribed by joining a sequence of characters that provides a correct spelling of the particular word/phrase of interest.
- a series of characters intended to spell a word may instead be misrecognized as a sequence of words, preventing post-processing techniques from even recognizing that there is a sequence of characters to join into a word.
- a letter sequence “Y U N I” may be misrecognized as the sequence of words “why you and I,” instead of as spelling a single word “ Yuni” as the user intended
- a user may be aware that an ASR system may have difficulty recognizing a particular phrase in an utterance and opt to recite the particular phrase followed by a sequence of characters that spells the particular phrase, e.g., “my name is Khe Chai K H E space C H A I”.
- a conventional ASR model performing speech recognition on the example utterance would ultimately output a transcription of “my name is kitchen Khe Chai” where the particular phrase the user is providing a spelling for is misrecognized (e.g., kitchen) in the transcription followed by the spelling for the particular phrase as a result of joining the sequence of characters.
- Implementations herein are directed toward enabling a speech recognizer to recognize seamless spelling of a particular phrase in the midst of a spoken utterance, without the user having to explicitly enter a “spelling mode” or make corrections after the speech recognizer transcribes the utterance. This may be particularly handy for uncommon words (e.g., proper names) that the speech recognizer has not been trained to recognize, but are nonetheless conveyed in speech that a user expects the speech recognizer to learn to recognize.
- uncommon words e.g., proper names
- a user may speak an utterance that seamlessly spells-out a particular phrase by reciting, in the utterance, a spelling structure that includes the particular phrase (e.g., one or more spoken words such as “Khe Chai”) in combination with a sequence of individual characters (“K H E C H A I”) that spells the particular phrase.
- a spelling structure that includes the particular phrase (e.g., one or more spoken words such as “Khe Chai”) in combination with a sequence of individual characters (“K H E C H A I”) that spells the particular phrase.
- the user may recite two or more spans of consecutive individual characters that each provide a correct spelling for a corresponding one of the two or more particular words and further speak/recite a space command between each adjacent pair of the two or more spans of consecutive individual characters.
- the user may recite the space command with the sequence of individual characters as “K H E space C H A I” to inform the speech recognizer that the user is spelling two individual words/terms that should be separated by a space in the resulting transcription.
- the user may also speak other variants such as a double or triple command for repeated characters. For instance, the user may seamlessly spell-out the particular phrase “toggle” by reciting the particular phrase “toggle” followed by the consecutive individual characters “T O” then “double G” followed by “L E” that spells out the particular phrase.
- the speech recognizer is trained to detect the spelling structure in an initial transcription of the utterance so that any initial recognition of the particular phrase is omitted and replaced with the correct spelling of the particular phrase by joining together the sequence of characters. While in many instances the initial recognition of the particular phrase may be misrecognized in the initial transcription, in other instances the speech recognizer may still recognize the particular phrase correctly in the initial transcription yet still detect the spelling structure. In the latter case, the user providing the correct spelling for the particular phrase that was already recognized correctly in the initial transcription may serve to boost the confidence of the speech recognizer in recognizing the particular phrase in subsequent utterances spoken by the user.
- the spelling structure recited in the utterance to seamlessly spell-out the particular phrase further includes one or more spell trigger words (e.g., “spell,” “as,” “as in,” “spell . . . as,” etc.) that more explicitly indicate, or otherwise inform the speech recognizer while performing speech recognition on the utterance, that the sequence of individual characters provides a correct spelling of the particular phrase.
- spell trigger words e.g., “spell,” “as,” “as in,” “spell . . . as,” etc.
- a user may speak “Khe Chai spell K H E space C H A I,” “Khe Chai as K H E space C H A I,” “Khe Chai as in K H E space C H A I,” where spell, as, or as in all correspond to example spell trigger words that indicate that the sequence of characters “K H E space C H A I” spells the particular phrase “Khe Chai.”
- the user also recites the space command, space, to indicate the delineation between the adjacent spans of consecutive individual characters.
- the user when seamlessly spelling the particular phrase, the user speaks one or more initiating trigger words prior to reciting the particular phrase.
- the user may speak “ spell Khe Chai as in K H E space C H A I such that the user recites an initiating trigger word, spell, prior to reciting the particular phrase (Khe Chai) in combination with reciting spell trigger words, as in, after reciting the particular phrase but before reciting the sequence of individual characters providing the correct spelling of the particular phrase.
- the user may speak initiating trigger words as part of the spell structure prior to reciting the particular phrase with or without reciting spell trigger words prior to reciting the sequence of individual characters.
- the speech recognizer may transduce any initiating trigger words, spell trigger words, and space commands recognized in the initial transcription into special tokens that a spelling normalizer omits from a final transcription of the utterance.
- the spelling normalizer also extracts, from the initial transcription, the sequence of individual characters that provide the correct spelling of the particular phrase, constructs the particular phrase with the correct spelling from the extracted sequence of individual characters, and normalizes the initial transcription to obtain the final transcription for the utterance by replacing any initial recognition of the particular phrase with the corrected phrase constructed from the extracted sequence of individual characters.
- a user may seamlessly utter a spelling structure as part of a spoken utterance the user wants recognized.
- a seamless spelling structure or seamlessly speaking a spoken spelling structure may refer to the speaking of a spelling structure within, and as an integral part of, a longer spoken utterance, possibly including other portions for which spelling information is not provided. Therefore, seamless spelling occurs, during, and/or as an integral part of speaking utterances while a speech recognizer is performing speech recognition on the utterance. That is, spelling is not performed or handled post completion of a transcription after the user is finished speaking an utterance.
- the user may intuit or anticipate that an ASR model may not be able to recognize one or more words in a spoken utterance and, thus, proactively utter a spelling structure as an integral part of the spoken utterance to provide an accurate spelling for such words.
- the ASR model provides streaming speech recognition results on a display as the user speaks an utterance, and upon viewing that a particular phrase that the user last recited was misrecognized in the displayed speech recognition results, seamlessly recites the spelling structure that includes the sequence of characters and optionally a spell trigger that precedes the recitation of the sequence of characters before continuing to speak the remaining contents of the utterance.
- FIG. 1 illustrates an example system 100 for providing seamless spelling while performing speech recognition on audio data 102 captured by a user device 110 that characterizes an utterance (e.g., a query, command, message contents, dictated speech, etc.) 101 spoken by a user 10 of the user device 110.
- the user 10 seamlessly speaks a spelling structure 180 as part of the utterance 101 to provide a correct spelling for a particular phrase 182 (e.g., one or more particular words) recited in the utterance 101.
- a particular phrase 182 e.g., one or more particular words
- the spelling structure 180 recited by the user 10 in the utterance 101 includes the particular phrase “Khe Chai” 182; a spell trigger word “spell” 184; and a sequence of individual characters (e.g., “K H E space C H A I”) 186 that provides a correct spelling of the particular phrase 182.
- the system 100 includes a user device 110.
- the user device 110 includes data processing hardware 112 and memory hardware 113.
- the user device 110 may include one or more audio capture devices (e.g., microphone(s) 114) for capturing and converting utterances 101 spoken by the user 10 into the audio data 102 (e.g., digital data or electrical signals).
- the microphone 114 is separate from the user device 110 and in communication with the user device 110 to provide the utterance 101 to the user device 110.
- the user device 110 can be any computing device capable of communicating with the computing system 130 through the network 120.
- the user device 110 includes, but is not limited to, desktop computing devices and mobile computing devices, such as laptops, tablets, smart phones, smart keyboards, digital assistants, smart speakers/displays, vehicle infotainment systems, smart appliances, Intemet-of-things (loT) devices, and wearable computing devices (e.g., smart glasses/headsets and/or watches and/or smart headphones).
- desktop computing devices and mobile computing devices such as laptops, tablets, smart phones, smart keyboards, digital assistants, smart speakers/displays, vehicle infotainment systems, smart appliances, Intemet-of-things (loT) devices, and wearable computing devices (e.g., smart glasses/headsets and/or watches and/or smart headphones).
- the system 100 may also include a computing system 130 in communication with the user device 110 via a network 120.
- the computing system 130 may be a distributed system (e.g., a cloud computing environment) having scalable elastic resources.
- the resources include computing resources 132 (e.g., data processing hardware) and/or storage resources 134 (e.g., memory hardware).
- the network 120 can be wired, wireless, or a combination thereof, and may include private networks and/or public networks, such as the Internet.
- the user device 110 executes (i.e., on the data processing hardware 112) a speech recognizer 140, for processing the audio data 102 to generate an initial transcription 107 and a spelling normalizer 150 to normalize the initial transcription 107 into a final transcription 170 entirely on-device without requiring any connection to the computing system 170.
- a speech recognizer 140 for processing the audio data 102 to generate an initial transcription 107
- a spelling normalizer 150 to normalize the initial transcription 107 into a final transcription 170 entirely on-device without requiring any connection to the computing system 170.
- the speech recognizer 140 and the spelling normalizer 150 may execute on the computing system 130 (i.e., on the data processing hardware 132) in addition to, or in lieu of, the user device 110 without departing from the scope of the present disclosure.
- the computing system 130 may receive the audio data 102 from the user device 110 via the network 120, and execute the speech recognizer 140 to process the audio data 102 and generate the initial transcription 107.
- the spelling normalizer 150 executing on the computing system 130 (or the user device 110) may normalize the initial transcription 107 into the final transcription 170 and provide the final transcription 170 to an output module 160.
- the user 10 speaks a sequence of individual characters 186 that includes two or more spans of consecutive individual characters 186a-n each providing a correct spelling for a corresponding word or portion of the particular phrase 182.
- a first span of consecutive individual characters 186a “K H E” provides the correct spelling for a first word “Khe” in the particular phrase 182 while a second span of consecutive individual characters 186b “C H A I” provides the correct spelling for a second word “Chai” in the particular phrase 182.
- the user 10 may also speak a space command 185 “space” between each adjacent pair of the two or more spans of consecutive individual characters 186a, 186b to separate the two corresponding words spelled therefrom by a space in the final transcription 170.
- the user 10 pausing between speaking two adjacent characters in a sequence of characters (e.g., between “E” and “C” in a sequence “K H E C H A I”) divides the sequence of characters into the two or more spans of spoken individual characters (e.g., “K H E” and “C H A I”) without requiring the user 10 to explicitly recite a space command 185.
- the user 10 neither speaks the space command, nor pauses between two adjacent characters. In such instances, the speech recognizer 140 and spelling normalizer 150 may nevertheless recognize that a sequence of characters spells a particular phrase containing more than one word that should be delineated by a space.
- the user 10 also recites one or more spell trigger words 184 after reciting the particular phrase 184 but before reciting the sequence of individual characters 186.
- a spell trigger word 184 e.g., “spell”
- the recitation of a spell trigger word 184 serves as a hint to inform the speech recognizer 140 that the following sequence of individual characters 186 the user 10 is to recite corresponds to a correct spelling of the particular phrase 182 recited by the user 10 just before reciting the spell trigger word 184.
- the user may recite an initiating trigger word (not shown) in the utterance 101 prior to reciting the particular phrase 182.
- the initiating trigger word may serve as a hint to inform the speech recognizer 140 that the user 10 is providing a spelling structure 180 for spellingout the particular phrase 182 that immediately follows the initiating trigger word in the utterance 101 spoken by the user 10.
- the speech recognizer 140 processes the audio data 102 characterizing the utterance 101 captured by the user device 110.
- the speech recognizer 140 includes an ASR model 142 that performs speech recognition on the audio data 102 to generate a word lattice 200 indicating multiple candidate hypotheses 105, 105a-n for the utterance 101.
- the word lattice 200 may also be referred to as a decoding graph. Accordingly, the ASR model 142 may evaluate potential paths through the word lattice 200 to determine the multiple candidate hypotheses 105.
- the ASR model 142 may include an end-to-end neural network model that converts audio data directly into corresponding text, without requiring the use of separately trained acoustic, pronunciation, and language models.
- the ASR model 142 includes a streaming speech recognition model capable of emitting streaming transcription results as the user speaks.
- the ASR model 142 may continuously emit initial transcriptions 107 as the audio data 102 is processed in a streaming fashion and display the most current initial transcription 107 for display on a screen (e.g., graphical user interface) 115 in communication with the user device 110.
- An example streaming speech recognition model 142 may include a recurrent neural network-transducer (RNN-T) architecture.
- FIG. 2A illustrates an example of a word lattice 200, 200a that may be output from the ASR model 142 of FIG. 1.
- the word lattice 200a represents multiple possible combinations of words that may form different candidate hypotheses 105 (FIG. 1) for a spoken utterance 101.
- the word lattice 200a includes one or more nodes 202a-g that correspond to the possible boundaries between words.
- the word lattice 200a includes multiple edges 204a-l for the possible words in the candidate hypotheses that result from the word lattice 200a.
- each of the edges 204a-l can have one or more weights or probabilities of that edge being the correct edge from the corresponding node.
- the weights are determined by the ASR model 142 and can be based on, for example, a confidence in the match between the speech data and the word for that edge and how well the word fits grammatically and/or lexically with other words in the word lattice 200a.
- a lattice may include a graph of sub-word units where nodes of the lattice represent sub-word units output by the ASR model 142 that are used to form words based on the weights or probabilities of the edges connecting the nodes.
- a lattice may include a graph of words and sub-word units, or any other parts of speech.
- Subwords in the word lattice 200 may include any combination of words, wordpieces, and individual characters/graphemes.
- the most probable path (e.g., most probable candidate hypothesis 200) through the word lattice 200a may include the edges 204c, 204e, 204i, 204k, which have the text “we’re coming about 11 :30.”
- a second best path (e.g., second best candidate hypothesis 200) through the word lattice 200a may include the edges 204d, 204h, 204j, 2041, which have the text “deer hunting scouts 7:30.”
- Each pair of nodes may have one or more paths corresponding to the alternate words in the various candidate hypotheses 200.
- the initial most probable path between the node pair beginning at node 202a and ending at the node 202c is the edge 204c “we’re.”
- This path has alternate paths that include the edges 204a, 204b “we are” and the edge 204d “deer.”
- FIG. 2B is an example of a hierarchical word lattice 200, 200b that may be provided by the ASR model 142 of FIG. 1.
- the word lattice 200b includes nodes 252a-l that represent words that make up the various candidate hypotheses 200 for a spoken utterance 101.
- the edges between the nodes 252a-l show that the possible candidate hypotheses 200 include: (1) the nodes 252c, 252e, 252i, 252k “we’re coming about 11 :30”; (2) the nodes 252a, 252b, 252e, 252i, 252k “we are coming about 11 :30”; (3) the nodes 252a, 252b, 252f, 252g, 252i, 252k “we are come at about 11 :30”; (4) the nodes 252d, 252f, 252g, 252i, 252k “deer come at about 11 :30”; (5) the nodes 252d, 252h, 252j, 252k “deer hunting scouts 11 :30”; and (6) the nodes 252d, 252h, 252j, 2521 “deer hunting scouts 7:30.”
- edges between the nodes 252a-l may have associated weights or probabilities based on the confidence in the speech recognition (e.g., candidate hypothesis) and the grammatical/lexical analysis of the resulting text.
- “we’re coming about 11 :30” may currently be the best hypothesis and “deer hunting scouts 7:30” may be the next best hypothesis.
- One or more divisions, 354a-d can be made in the word lattice 200b that group a word and its alternates together.
- the division 254a includes the word “we’re” and the alternates “we are” and “deer.”
- the division 252b includes the word “coming” and the alternates “come at” and “hunting.”
- the division 254c includes the word “about” and the alternate “scouts” and the division 254d includes the word “11 :30” and the alternate “7:30.”
- the speech recognizer 140 determines an initial transcription 107 as a highest-ranking candidate hypothesis 105 among the multiple candidate hypotheses in the word lattice 200.
- the initial transcription 107 includes a misrecognition of the particular phrase 182 by the ASR model 142 where transcription 107 includes the misrecognized phrase 182m “kitchen” instead of the correct particular phrase 182 “Khe Chai”.
- the initial transcription 107 also includes the sequence of individual characters 186 that provide the correct spelling of the particular phrase 182.
- Implementations herein are directed toward the speech recognizer 140 detecting/recognizing, from the lattice 200, the spelling structure 180 spoken by the user 10 to indicate that the user 10 is speaking the sequence of individual characters 186 to represent a correct spelling for the preceding particular phrase 182.
- the speech recognizer 140 includes a finite state transducer (FST) 300 (e.g., a weighted FST) downstream of the ASR model 142 for evaluating the potential paths through the word lattice 200 that form the different candidate hypotheses 105 for the spoken utterance 101.
- FST finite state transducer
- the FST 300 may determine confidence scores for each of the candidate hypotheses and identify the initial transcription 107 as a highest ranking one of the candidate hypotheses 105 output by the ASR model 142 at a corresponding output step.
- the FST 300 may, for example, execute a beam search on the word lattice 200 to identify and score candidate hypotheses 105 for the audio data 102, and select or determine an initial transcription 107 of the spoken utterance 101 to be the candidate hypotheses 105 having the highest likelihood score 106.
- the FST 300 includes a spelling component 310 configured to detect the presence of the spelling structure 180 in the initial transcription 107 by identifying the one or more spell trigger words 184 in the initial transcription 107 subsequent to the misrecognition of the particular phrase 182m. Specifically, the weighted FST 300 may transduce recognition of a trigger word 184 in the initial transcription 107 into a corresponding trigger word token 184t that the spelling normalizer 150 will remove from inclusion in the resulting final transcription 170.
- detecting the spelling structure 180 in the initial transcription 108 by the spelling component 310 may cause the weighted FST 300 to suppress selection of any spell trigger words 184 as well as any initiating triggers words detected by the spelling component 310 in the lattice 200 by setting their outputs to epsilon and including any trigger word tokens 184t (e.g., “ ⁇ spell>”) in the initial transcription 107.
- Other spell tokens may be used.
- the trigger word token 184t may be used regardless of what type of trigger words (e.g., spell trigger word or initiating trigger word) are detected.
- the trigger word token 184t may be included when a detected spelling structure contains one or more spell trigger words (e.g., “spell,” “as in,” “spell . .
- the initiating trigger word and the spell trigger word form a predefined spell command operative to inform the spelling component 310 the user 10 is reciting a sequence of individual characters to provide a correct spelling for a particular phrase that precedes the sequence of individual characters.
- the spelling component 310 of the weighted FST 300 detects a spelling structure in the initial transcription 107 that does not include trigger word tokens 184t indicative of the user 10 never reciting the initiating trigger word or the spell trigger word in the utterance 101.
- the spelling component 310 may detect/recognize the sequence of individual characters spoken by the user in the utterance, and then use acoustic modeling and/or language modeling techniques to determine that the sequence of individual characters provides a correct spelling for a particular phrase (whether misrecognized or recognized correctly) that precedes the sequence of individual characters in the transcription 107.
- the speech recognizer 140 may also identify pauses in a spoken sequence of individual characters and/or utterances of a space command 185, such as “space,” within the sequence of individual characters. Detecting the space command 185 in the initial transcription 107 causes the weighted F ST 300 to output a corresponding space command token 185t in the initial transcription 107 in order to suppress selection of the space command 185 detected by the spelling component 310 in the lattice 200 by setting its output to epsilon.
- the weighted FST 300 may include, in the initial transcription 107, the space token 185t, such as “ ⁇ space>.”
- FIG. 3 is a schematic view of an example spelling component 310 of the weighted FST 300 configured to detect a presence of a spelling structure 180 spoken as an integral part of an utterance 101 spoken by the user 10.
- the spelling component 310 represents the detection of various possible spelling structures containing a particular phrase, a sequence of individual characters, optional spell trigger words 184, optional initiating trigger words 181, and/or optional space commands 185.
- the weighted FST 300 is modified to include the spelling component 310 between a pair of nodes 302A and 302B of the word lattice 200 output by the ASR model 142.
- FIG. 3 shows operations (A) to (D) that illustrate a flow of data.
- the spelling component 310 executes an initiating trigger word decoding graph 320 for determining whether a corresponding subword 340 in the lattice 200 includes an initiating trigger word 181 (e.g., “spell”) that a user 10 may speak to indicate a start of the spelling structure 180 within the utterance before speaking the particular phrase 182 the user wants to seamless spell. That is, the initiating trigger word 181 is spoken immediately before speaking the particular phrase 182 followed by the sequence of individual characters 184 that provide the correct spelling for the particular phrase 182.
- an initiating trigger word 181 e.g., “spell”
- the initiating trigger word decoding graph 320 may detect the initiating trigger word 181 “spell” in an utterance QV spell Khe Chai as K H E C H A I.”
- the F ST 300 sets likelihood scores of triggers detected by the decoding graph 320 to epsilon in other portions of the FST 300 such that they do not appear in the initial transcription 107, but are replaced with a trigger word token 184t in the initial transcription 107, as described above.
- the spelling component 310 next executes a spoken word decoding graph 330 for determining whether a next corresponding sub word 340 in the lattice as the particular phrase 182.
- the decoding 330 detects words by detecting a prefix token (e.g., “_A,” “_B,” “_a,” “_b,” etc., where signifies a space before a character) followed by one or more non-prefix tokens (e.g., “A,” “B,” “a”, “b,” etc.).
- the FST 300 applies a small penalty weight to scoring of words 340 detected by the decoding graph 330 in other portions of the FST 300 to slightly increase a likelihood score for the spelling component 310 over other possible paths in the lattice 200.
- FIG. 3 depicts the spoken word decoding graph 330 of operation (B) occurring after performing the initiating trigger word decoding graph 320 of operation (A)
- the spoken word decoding graph 330 may detect the word 340 corresponding to the particular phrase 182 as the start of a spelling structure regardless of whether the initiating trigger word decoding graph 320 performed by operation (F) detected a preceding initiating trigger word.
- the spoken word decoding graph 330 detects the misrecognized phrase 182m “kitchen” representing a misrecognition of the particular phrase 182 “Khe Chai” by the ASR model 182.
- the spelling component 310 executes a spell trigger decoding graph 350 for determining whether a next corresponding subword 340 in the lattice 200 includes a spell trigger word 184 that the user may speak after speaking the particular phrase 182 but prior to speaking the sequence of individual characters 186 that provides the correct spelling for the particular phrase 182.
- the spell trigger decoding graph 350 may determine that the corresponding word 340 “spell” in an utterance of “Khe Chai spell K H E C H A I,” or the word “as” in an utterance of “ spell Khe Chai as K H E C H Al” as spell trigger words 184.
- spell trigger words detected by the decoding graph 350 may be a continuance of initiating spell trigger words detected by the decoding graph 320.
- the spelling structure 310 executes an individual character decoding graph 360 for determining whether a next corresponding sub word 340 in the lattice 200 is part of the sequence of individual characters 186 (e.g., “ A,” “_B,” “_a,” “_b,” etc., where signifies a space before a character) in the spelling structure 180 that provide the correct spelling for the particular phrase 182.
- the individual character decoding graph 360 may continuously loop upon determining each individual character in the sequence of individual characters 186. Accordingly, the decoding graphs 320, 330, 350, 360 performed by operations (A) to (D) may detect the presence of the spelling structure 180 in the initial transcription 107.
- the spelling normalizer 150 in response to detecting the spelling structure 180 in the initial transcription 107, is configured to normalize the initial transcription 107 to obtain the final transcription 170 for the utterance 101 by replacing the misrecognition of the particular phrase 182m with a corrected phrase constructed by joining the sequence of individual characters 186 spoken by the user 10 seamlessly in the utterance 101.
- the spelling normalizer 150 replaces the misrecognized phrase 182m in the initial transcription 107 (e.g., “kitchen”) with a spelled or corrected phrase (e.g., “Khe Chai) constructed from the sequence of individual characters 186.
- the spelling normalizer 150 may execute a spell structure detector 152 to detect the spelling structure 180 in the initial transcription 107.
- the spell structure detector 152 may detect any of the trigger word tokens 184t (e.g., “ ⁇ spell>”) in addition to the individual sequence of individual characters 186.
- the spelling normalizer 150 proceeds to execute a spelling extractor 153 to construct, or otherwise form, a corrected phrase (e.g., “Khe Chai”) from the sequence of individual characters 186.
- the spelling extractor 153 first extracts a longest sequence of individual characters 186, blank spaces, and space token 185 subsequent to the trigger word token 184t in the initial transcription 107 (e.g., “K H E ⁇ space> C H A I”).
- the spelling extractor 153 constructs the corrected phrase from the extracted sequence of individual characters 186 by joining the individual characters for each span 186a, 186b to form a corresponding one of two or more particular words (e.g., Khe and Chai) and replacing each space token (e.g., ⁇ space>) 185 in the initial transcription with a blank space.
- the spelling extractor 153 may also remove spaces (white/blank spaces) between the characters when joining the individual characters together to form the corrected phrase.
- the spelling extractor 153 may additionally normalize the spelled words by executing a capitalization module 154.
- the capitalization module 154 may capitalize each spelled word, as may be suitable for proper names (e.g., forming a spelled or corrected phrase of “Khe Chai”).
- the spelling normalizer 150 also executes a phrase extractor 155 to identify and extract the misrecognized particular phrase 182m from the initial transcription 107.
- the particular phrase 182 may be recognized correctly, however the spelling normalizer needs to replace the particular phrase with the correct phrase constructed from the sequence of individual characters to avoid repeating the particular phrase in the final transcription 170.
- the phrase extractor 155 determines the misrecognized phrase to be a span of words preceding the spell token that best matches the corrected particular phrase (e.g., “Khe Chai”).
- the decoding graph 330 of the spelling component 310 may identify or detect the span of words.
- the number of words identified by the spelling component 310 may be different from the number of words in the corrected particular phrase. For example, “Khe Chai” may be misrecognized as “kitchen.” Continuing with the example, the phrase extractor 155 identifies “kitchen” as the misrecognized particular phrase 182m in the initial transcription 107. [0056] In some examples, the phrase extractor 155 uses Levenshtein distance to measure the distance (e.g., minimum edits) between two strings.
- the phrase extractor 156 may use Levenshtein distances to compare the spelled words of the corrected phrase with multiple spans of words of different lengths that precede the spell token in the initial transcription 107, and choose the span of words with the smallest Levenshtein distance as the misrecognized particular phrase.
- the phrase extractor 155 computes Levenshtein distances between the corrected phrase “Khe Chai” and the word spans “kitchen,” “is kitchen,” “name is kitchen,” and “My name is kitchen.” In some examples, the phrase extractor 155 computes Levenshtein distances using dynamic programming by progressively computing the Levenshtein distances of sub-strings (e.g., “kitchen” is a sub-string of “name is kitchen”). That is, the phrase extractor 155 may compute Levenshtein distances of multiple word spans by performing only one set of Levenshtein distance calculations based on the longest span (e.g., “My name is kitchen”), as shown in FIG. 4.
- FIG. 4 is a table 400 representing example Levenshtein distances between sub-strings.
- each cell of the table 400 represents a Levenshtein distance between two particular sub-strings.
- the example of FIG. 4 represents Levenshtein distances between sub-strings of a spelled corrected phrase “Tsim Sha Tsui,” and substrings of candidate misrecognized phrases: “surgery,” “Tim surgery,” “is Tim surgery,” and “Where is Tim surgery.”
- An example entry in cell 405 indicates that the Levenshtein distance between “im Surgery” and “Tsui” is 8.
- Cells 410, 415, 420, and 425 contain example Levenshtein distances between the spelled corrected phrase “Tsim Sha Tsui” and respective ones of the four candidate sub-strings: “surgery,” “Tim surgery,” “is Tim surgery,” and “Where is Tim surgery.”
- the sub-string “Tim Surgery” corresponding to cell 415 is identified as misrecognized phrase because it has the smallest Levenshtein distance from the spelled phrase “Tsim Sha Tsui.”
- the table 400 has (R + 1) x (C + 1) entries, where R is the number of characters (including spaces) in the spelled phrase, and C is the number of characters (including spaces) in the longest candidate phrase.
- Row indices shown in the right-most column, and column indices shown in the bottom-most row are specified with respect to the origin at the bottom right.
- the (r, c) th entry at row r and column c can be computed recursively using the following mathematical expression: where Cdei, Cms, and Csub are the costs for deletion, insertion, and substitution, respectively.
- the spelling normalizer executes a normalizer 156 for replacing the misrecognized particular phrase (e.g., “kitchen”) identified by the phrase extractor 155 with the spelled corrected phrase (e.g., “Khe Chai”) to provide the final transcription 104 “My name is Khe Chai”.
- misrecognized particular phrase e.g., “kitchen”
- spelled corrected phrase e.g., “Khe Chai”
- An output module 160 may output the final transcription 104 for display on the screen 115 of the user device 110.
- the output module 160 may also receive initial transcriptions 107 output by the speech recognizer and display the initial transcriptions as partial speech recognition results on the screen 115 in a streaming fashion.
- the output module 160 may also include a downstream application (e.g., digital assistant) that performs semantic interpretation (e.g., natural language understanding (NLU)) on the transcription 104 to identify a command/query for the application to perform.
- the output module 160 may include a search engine.
- the output module 160 may include an input method editor (IME).
- the output module 160 may include a messaging application for communicating the transcription 104 as contents of a message to another user.
- the output module 160 may further include a model updater for personalizing the ASR model 142 to improve recognition of the particular phrase in a subsequent utterance.
- the output module 160 may include a natural language processing (NLP) module for interpreting the transcription 170.
- NLP natural language processing
- Other types of output modules 160 may also be implemented.
- the audio data 102 is received as streaming data as the user 10 speaks an utterance (e.g., “My name is Khe Chai, and I am glad to meet you”), and the speech recognizer 140 performs streaming speech recognition on the streaming audio data 102 as the audio data 102 is received to provide streaming speech recognition results. For example, as the user 10 speaks the utterance, the speech recognizer 140 may determine partial initial transcriptions 108 that reflect what the user 10 has already spoken.
- the output module 160 may output a partial initial transcription 108 of “My name is kitchen” on the screen 115.
- the user 10 may, in response to seeing or recognizing the misrecognition of the particular phrase “Khe Chai” as “kitchen” in the displayed partial initial transcription 108, seamlessly speak a spoken spelling structure as a seamless or integral part of the rest of their utterance.
- the user 10 may continue by speaking a spelling structure “spell K H E space C H A I,” thus, providing a correct spelling for the misrecognized particular phrase “Khe Chai,” and then continue with speaking the rest of their utterance “and I am glad to meet you.” In this way, the user 10 can temporally and seamlessly change to spelling in the midst of ongoing ASR, and then seamlessly return to speaking the rest of their utterance.
- FIG. 5 is a flowchart of an exemplary arrangement of operations for a method 500 that may be performed by data processing hardware for providing seamless spelling for ASR systems.
- Data processing hardware e.g., the data processing hardware 112 of the user device 110 and/or the data processing hardware 132 of the computing system 130 of FIG. 1
- the method 500 includes receiving audio data 102 corresponding to an utterance 101 spoken by a user 10.
- the utterance 101 including a particular phrase (e.g., the phrase 103a of FIG. 1) and a sequence of characters that provide a correct spelling of the particular phrase (e.g., the characters 103c) spoken after the particular phrase.
- the method 500 includes processing the audio data 102, using an ASR model 142, to generate an initial transcription (e.g., the initial transcription 107).
- the initial transcription containing a misrecognition of the particular phrase by the ASR model (e.g., “kitchen”) followed by the sequence of individual characters that provide a correct spelling of the particular phrase.
- the method 500 includes detecting a spelling structure in the initial transcription (e.g., the spelling structure 103) subsequent to the misrecognition of the particular phrase.
- the method 500 includes extracting, from the initial transcription, the sequence of individual characters that provide the correct spelling of the particular phrase (e.g., the sequence of individual characters 107g).
- the method 500 includes constructing a corrected phrase for the misrecognized particular phrase from the extracted sequence of individual characters (e.g., “Khe Chai”). The method 500 continues at operation 512 to replace the misrecognized phrase with the corrected phrase in the initial transcription, and at operation 514 to normalize the initial transcription to obtain a final transcription (e.g., the final transcription 104).
- a corrected phrase for the misrecognized particular phrase from the extracted sequence of individual characters (e.g., “Khe Chai”).
- the method 500 continues at operation 512 to replace the misrecognized phrase with the corrected phrase in the initial transcription, and at operation 514 to normalize the initial transcription to obtain a final transcription (e.g., the final transcription 104).
- FIG. 6 is a schematic view of an example computing device 600 that may be used to implement the systems and methods described in this document.
- the computing device 600 may be used to implement the user device 110 and/or the computing system 130.
- the computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 600 includes a processor 610 that may be used to implement the data processing hardware 112 and/or 132, memory 620 that may be used to implement the memory hardware 113 and/or 134, a storage device 630 that may be used to implement the memory hardware 113 and/or 134, a high-speed interface/controller 640 connecting to the memory 620 and high-speed expansion ports 650, and a low speed interface/controller 660 connecting to a low speed bus 670 and a storage device 630.
- Each of the components 610, 620, 630, 640, 650, and 660 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 610 can process instructions for execution within the computing device 600, including instructions stored in the memory 620 or on the storage device 630 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 680 coupled to high speed interface 640.
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 600 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multiprocessor system).
- the memory 620 stores information non-transitorily within the computing device 600.
- the memory 620 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 620 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 600.
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable readonly memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 630 is capable of providing mass storage for the computing device 600.
- the storage device 630 is a computer- readable medium.
- the storage device 630 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 620, the storage device 630, or memory on processor 610.
- the high speed controller 640 manages bandwidth-intensive operations for the computing device 600, while the low speed controller 660 manages lower bandwidthintensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 640 is coupled to the memory 620, the display 680 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 650, which may accept various expansion cards (not shown).
- the low-speed controller 660 is coupled to the storage device 630 and a low-speed expansion port 690.
- the low-speed expansion port 690 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 600 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 600a or multiple times in a group of such servers 600a, as a laptop computer 600b, or as part of a rack server system 600c.
- Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- processors and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a keyboard and a pointing device e.g., a mouse or a trackball
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
Abstract
A method (500) includes receiving audio data 102 characterizing an utterance (101) including a particular phrase (182) and a subsequent sequence of individual characters (184) that spell the particular phrase. The method also includes processing, using a speech recognition model (142), the audio data to generate an initial transcription (107) including a misrecognition of the particular phrase. The method also includes, in response to detecting a spelling structure (180) in the initial transcription subsequent to the misrecognition of the particular phrase: extracting, from the initial transcription, the sequence of characters; constructing a corrected phrase for the misrecognition of the particular phrase from the extracted sequence of characters; and normalizing the transcription to obtain a final transcription (170) for the utterance by replacing the extracted misrecognition of the particular phrase with the corrected phrase constructed from the extracted sequence of individual characters.
Description
Seamless Spelling for Automatic Speech Recognition Systems
TECHNICAL FIELD
[0001] This disclosure relates to providing seamless spelling for automatic speech recognition (ASR) systems.
BACKGROUND
[0002] ASR systems provide a technology that is typically used in mobile devices and/or other devices. In general, ASR systems attempt to provide accurate transcriptions of what a user speaks to a device. However, in some instances, ASR systems may generate transcriptions that do not match what the user intended or actually spoke.
SUMMARY
[0003] One aspect of the present disclosure provides a computer-implemented method that, when executed on data processing hardware, causes the data processing hardware to perform operations that include receiving audio data characterizing an utterance spoken by a user and processing, using an automatic speech recognition (ASR) model, the audio data to generate an initial transcription for the utterance. The utterance includes a particular phrase and a sequence of characters that provide a correct spelling of the particular phrase spoken after the particular phrase. The initial transcription includes a misrecognition of the particular phrase by the ASR model followed by the sequence of individual characters that provide the correct spelling of the particular phrase. The operations also include detecting a spelling structure in the initial transcription subsequent to the misrecognition of the particular phrase. In response to detecting the spelling structure in the initial transcription, the operations also include extracting, from the initial transcription, the sequence of individual characters that provide the correct spelling of the particular phrase, constructing a corrected phrase for the misrecognition of the particular phrase from the extracted sequence of individual characters, extracting, from the initial transcription, the misrecognition of the particular phrase, and normalizing the initial transcription to obtain a final transcription for the utterance by replacing the
extracted misrecognition of the particular phrase with the corrected phrase constructed from the extracted sequence of individual characters.
[0004] Implementations of the present disclosure include one or more of the following optional features. In some implementations, receiving the audio data includes receiving the audio data as the user speaks the utterance, performing speech recognition on the audio data includes performing streaming speech recognition on the audio data as the audio data is received to generate, as output from the ASR model, streaming speech recognition result. In these implementations, the operations also include providing each streaming speech recognition result generated as output from the ASR model for display on a screen in communication with the data processing hardware.
[0005] In some examples, the operations also include displaying a partial speech recognition result including the misrecognition of the particular phrase on the screen before the user speaks the sequence of individual characters comprising the spelling of the particular phrase. Moreover, detecting the spelling structure may include executing a weighted finite state transducer (wFST) having a spelling component configured to detect the spelling structure in the initial transcription by identifying one or more spell trigger words in the initial transcription subsequent to the misrecognition of the particular phrase and/or one or more initiating trigger words preceding the misrecognition of the particular phrase. The one or more initiating trigger words and the one or more spell trigger words may form a predefined spell command.
[0006] In some examples, the operations also include, after constructing the corrected phrase from the extracted sequence of individual characters, applying a capitalization normalizer to capitalize a first letter of at least one word in the corrected phrase. The misrecognition of the particular phrase may include one or more words. Optionally, extracting the misrecognition of the particular phrase may be based on an edit distance between the sequence of individual characters in the initial transcription and the misrecognition of the particular phrase.
[0007] In some implementations, the particular phrase spoken by the user in the utterance includes two or more particular words, the sequence of individual characters that provide the correct spelling of the particular phrase includes two or more spans of
consecutive individual characters that each provide a correct spelling for a corresponding one of the two or more particular words, and the utterance spoken by the user further includes the user speaking a space command between each adjacent pair of the two or more spans of consecutive individual characters. In these implementations, the initial transcription may further include a space token inserted between each adjacent pair of the two or more spans of consecutive individual characters of the sequence of individual characters, and constructing the corrected phrase from the extracted sequence of individual characters may include: for each span of consecutive individual characters, joining the individual characters to form the corresponding one of the two or more particular words; and replacing each space token in the initial transcription with a blank space.
[0008] In some examples, extracting the misrecognition of the particular phrase includes determining a corresponding edit distance between the sequence of individual characters and each word span of multiple word spans of different lengths in the initial transcription and identifying the word span of the multiple word spans of different lengths that has a shortest corresponding edit distance to the misrecognition of the particular phrase. The multiple word spans precede the sequence of individual characters. In these examples, the corresponding edit distance may include a corresponding Levenshtein distance dynamically computed for each of the multiple word spans via a single calculation. Additionally or alternatively, detecting the spelling structure in the initial transcription and normalizing the initial transcription to obtain the final transcription may occur without receiving any user input after the user finishes speaking the utterance.
[0009] Another aspect of the present disclosure provides a system that includes data processing hardware and memory hardware in communication with the data processing hardware and storing instructions that, when executed on the data processing hardware, cause the data processing hardware to perform operations that include receiving audio data characterizing an utterance spoken by a user and processing, using an automatic speech recognition (ASR) model, the audio data to generate an initial transcription for the utterance. The utterance includes a particular phrase and a sequence of characters that
provide a correct spelling of the particular phrase spoken after the particular phrase. The initial transcription includes a misrecognition of the particular phrase by the ASR model followed by the sequence of individual characters that provide the correct spelling of the particular phrase. The operations also include detecting a spelling structure in the initial transcription subsequent to the misrecognition of the particular phrase. In response to detecting the spelling structure in the initial transcription, the operations also include extracting, from the initial transcription, the sequence of individual characters that provide the correct spelling of the particular phrase, constructing a corrected phrase for the misrecognition of the particular phrase from the extracted sequence of individual characters, extracting, from the initial transcription, the misrecognition of the particular phrase, and normalizing the initial transcription to obtain a final transcription for the utterance by replacing the extracted misrecognition of the particular phrase with the corrected phrase constructed from the extracted sequence of individual characters.
[0010] Implementations of the present disclosure include one or more of the following optional features. In some implementations, receiving the audio data includes receiving the audio data as the user speaks the utterance, performing speech recognition on the audio data includes performing streaming speech recognition on the audio data as the audio data is received to generate, as output from the ASR model, streaming speech recognition result. In these implementations, the operations also include providing each streaming speech recognition result generated as output from the ASR model for display on a screen in communication with the data processing hardware.
[0011] In some examples, the operations also include displaying a partial speech recognition result including the misrecognition of the particular phrase on the screen before the user speaks the sequence of individual characters comprising the spelling of the particular phrase. Moreover, detecting the spelling structure may include executing a weighted finite state transducer (wFST) having a spelling component configured to detect the spelling structure in the initial transcription by identifying one or more spell trigger words in the initial transcription subsequent to the misrecognition of the particular phrase and/or one or more initiating trigger words preceding the misrecognition of the particular
phrase. The one or more initiating trigger words and the one or more spell trigger words may form a predefined spell command.
[0012] In some examples, the operations also include, after constructing the corrected phrase from the extracted sequence of individual characters, applying a capitalization normalizer to capitalize a first letter of at least one word in the corrected phrase. The misrecognition of the particular phrase may include one or more words. Optionally, extracting the misrecognition of the particular phrase may be based on an edit distance between the sequence of individual characters in the initial transcription and the misrecognition of the particular phrase.
[0013] In some implementations, the particular phrase spoken by the user in the utterance includes two or more particular words, the sequence of individual characters that provide the correct spelling of the particular phrase includes two or more spans of consecutive individual characters that each provide a correct spelling for a corresponding one of the two or more particular words, and the utterance spoken by the user further includes the user speaking a space command between each adjacent pair of the two or more spans of consecutive individual characters. In these implementations, the initial transcription may further include a space token inserted between each adjacent pair of the two or more spans of consecutive individual characters of the sequence of individual characters, and constructing the corrected phrase from the extracted sequence of individual characters may include: for each span of consecutive individual characters, joining the individual characters to form the corresponding one of the two or more particular words; and replacing each space token in the initial transcription with a blank space.
[0014] In some examples, extracting the misrecognition of the particular phrase includes determining a corresponding edit distance between the sequence of individual characters and each word span of multiple word spans of different lengths in the initial transcription and identifying the word span of the multiple word spans of different lengths that has a shortest corresponding edit distance to the misrecognition of the particular phrase. The multiple word spans precede the sequence of individual characters. In these examples, the corresponding edit distance may include a
corresponding Levenshtein distance dynamically computed for each of the multiple word spans via a single calculation. Additionally or alternatively, detecting the spelling structure in the initial transcription and normalizing the initial transcription to obtain the final transcription may occur without receiving any user input after the user finishes speaking the utterance
[0015] The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
DESCRIPTION OF DRAWINGS
[0016] FIG. l is a schematic view of an example system for providing seamless spelling for automatic speech recognition (ASR) systems.
[0017] FIGS. 2A and 2B are schematic views of diagrams that illustrate examples of word lattices.
[0018] FIG. 3 is a schematic view of diagrams that illustrate an example spelling component of a finite state transducer for seamlessly detecting spelling structures.
[0019] FIG. 4 is a table of example Levenshtein distances between sub-strings.
[0020] FIG. 5 is a flowchart of an exemplary arrangement of operations for a method of providing seamless spelling for ASR systems.
[0021] FIG. 6 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
[0022] Like reference symbols in the various drawings indicate like elements.
DETAILED DESCRIPTION
[0023] Automatic speech recognition (ASR) systems are becoming increasingly popular in client devices as the ASR systems continue to provide more accurate transcriptions of what users speak. Still, in some instances, ASR systems may generate inaccurate transcriptions when they are unable to recognize words/terms that were not, or rarely, seen during training. This may be the case when the user speaks an utterance containing a unique, uncommon, or rare word that was not represented well by the
training data used to train the ASR system. For example, a user may speak a proper name, such as “Khe Chai.” When the speech recognition model of an ASR system has not been trained to recognize such words, the ASR system may incorrectly transcribe what the user spoke as another word or phrase (e.g., “kitchen”) that is acoustically similar to “Khe Chai” and is already known to the ASR system. While spelling is a simple way for users to voice input words that are not recognized by an ASR system (e.g., proper names), conventional ASR systems often do not handle spelling of particular words/phrases reliably and generally require some type of post-processing technique after an entire utterance is transcribed by joining a sequence of characters that provides a correct spelling of the particular word/phrase of interest. However, in some instances, a series of characters intended to spell a word may instead be misrecognized as a sequence of words, preventing post-processing techniques from even recognizing that there is a sequence of characters to join into a word. For example, a letter sequence “Y U N I” may be misrecognized as the sequence of words “why you and I,” instead of as spelling a single word “ Yuni” as the user intended Furthermore, there are no current solutions to handle the speaking of individual characters to provide the correct spelling of a particular phrase in a seamless manner while performing speech recognition on the user’s speech where the user first recites the particular phrase in spoken form and subsequently recites the individual characters that spell the particular phrase. That is, a user may be aware that an ASR system may have difficulty recognizing a particular phrase in an utterance and opt to recite the particular phrase followed by a sequence of characters that spells the particular phrase, e.g., “my name is Khe Chai K H E space C H A I”. A conventional ASR model performing speech recognition on the example utterance would ultimately output a transcription of “my name is kitchen Khe Chai” where the particular phrase the user is providing a spelling for is misrecognized (e.g., kitchen) in the transcription followed by the spelling for the particular phrase as a result of joining the sequence of characters. In this scenario, post processing is required where the user may provide a user correction removing the misrecognized phrase (e.g., kitchen) from the transcription or a downstream semantic interpretation is performed to determine that “kitchen” should be removed from the transcription output from the ASR system.
[0024] Implementations herein are directed toward enabling a speech recognizer to recognize seamless spelling of a particular phrase in the midst of a spoken utterance, without the user having to explicitly enter a “spelling mode” or make corrections after the speech recognizer transcribes the utterance. This may be particularly handy for uncommon words (e.g., proper names) that the speech recognizer has not been trained to recognize, but are nonetheless conveyed in speech that a user expects the speech recognizer to learn to recognize. For example, a user may speak an utterance that seamlessly spells-out a particular phrase by reciting, in the utterance, a spelling structure that includes the particular phrase (e.g., one or more spoken words such as “Khe Chai”) in combination with a sequence of individual characters (“K H E C H A I”) that spells the particular phrase. For particular phrases having two or more words, the user may recite two or more spans of consecutive individual characters that each provide a correct spelling for a corresponding one of the two or more particular words and further speak/recite a space command between each adjacent pair of the two or more spans of consecutive individual characters. In the example, the user may recite the space command with the sequence of individual characters as “K H E space C H A I” to inform the speech recognizer that the user is spelling two individual words/terms that should be separated by a space in the resulting transcription. In addition to reciting the space command and individual characters, the user may also speak other variants such as a double or triple command for repeated characters. For instance, the user may seamlessly spell-out the particular phrase “toggle” by reciting the particular phrase “toggle” followed by the consecutive individual characters “T O” then “double G” followed by “L E” that spells out the particular phrase. As will become apparent, the speech recognizer is trained to detect the spelling structure in an initial transcription of the utterance so that any initial recognition of the particular phrase is omitted and replaced with the correct spelling of the particular phrase by joining together the sequence of characters. While in many instances the initial recognition of the particular phrase may be misrecognized in the initial transcription, in other instances the speech recognizer may still recognize the particular phrase correctly in the initial transcription yet still detect the spelling structure. In the latter case, the user providing the correct spelling for the particular phrase that was
already recognized correctly in the initial transcription may serve to boost the confidence of the speech recognizer in recognizing the particular phrase in subsequent utterances spoken by the user.
[0025] In some implementations, the spelling structure recited in the utterance to seamlessly spell-out the particular phrase further includes one or more spell trigger words (e.g., “spell,” “as,” “as in,” “spell . . . as,” etc.) that more explicitly indicate, or otherwise inform the speech recognizer while performing speech recognition on the utterance, that the sequence of individual characters provides a correct spelling of the particular phrase. For example, a user may speak “Khe Chai spell K H E space C H A I,” “Khe Chai as K H E space C H A I,” “Khe Chai as in K H E space C H A I,” where spell, as, or as in all correspond to example spell trigger words that indicate that the sequence of characters “K H E space C H A I” spells the particular phrase “Khe Chai.” Notably, the user also recites the space command, space, to indicate the delineation between the adjacent spans of consecutive individual characters. In some examples, when seamlessly spelling the particular phrase, the user speaks one or more initiating trigger words prior to reciting the particular phrase. In the example, the user may speak “ spell Khe Chai as in K H E space C H A I such that the user recites an initiating trigger word, spell, prior to reciting the particular phrase (Khe Chai) in combination with reciting spell trigger words, as in, after reciting the particular phrase but before reciting the sequence of individual characters providing the correct spelling of the particular phrase. The user may speak initiating trigger words as part of the spell structure prior to reciting the particular phrase with or without reciting spell trigger words prior to reciting the sequence of individual characters. [0026] As will become apparent, upon the speech recognizer detecting the spelling structure in an initial transcription of an utterance while performing speech recognition on audio data characterizing the utterance, the speech recognizer may transduce any initiating trigger words, spell trigger words, and space commands recognized in the initial transcription into special tokens that a spelling normalizer omits from a final transcription of the utterance. By the same notion, the spelling normalizer also extracts, from the initial transcription, the sequence of individual characters that provide the correct spelling of the particular phrase, constructs the particular phrase with the correct spelling from the
extracted sequence of individual characters, and normalizes the initial transcription to obtain the final transcription for the utterance by replacing any initial recognition of the particular phrase with the corrected phrase constructed from the extracted sequence of individual characters.
[0027] A user may seamlessly utter a spelling structure as part of a spoken utterance the user wants recognized. As used herein, a seamless spelling structure or seamlessly speaking a spoken spelling structure may refer to the speaking of a spelling structure within, and as an integral part of, a longer spoken utterance, possibly including other portions for which spelling information is not provided. Therefore, seamless spelling occurs, during, and/or as an integral part of speaking utterances while a speech recognizer is performing speech recognition on the utterance. That is, spelling is not performed or handled post completion of a transcription after the user is finished speaking an utterance. [0028] For example, the user may intuit or anticipate that an ASR model may not be able to recognize one or more words in a spoken utterance and, thus, proactively utter a spelling structure as an integral part of the spoken utterance to provide an accurate spelling for such words. In some examples, the ASR model provides streaming speech recognition results on a display as the user speaks an utterance, and upon viewing that a particular phrase that the user last recited was misrecognized in the displayed speech recognition results, seamlessly recites the spelling structure that includes the sequence of characters and optionally a spell trigger that precedes the recitation of the sequence of characters before continuing to speak the remaining contents of the utterance.
[0029] FIG. 1 illustrates an example system 100 for providing seamless spelling while performing speech recognition on audio data 102 captured by a user device 110 that characterizes an utterance (e.g., a query, command, message contents, dictated speech, etc.) 101 spoken by a user 10 of the user device 110. In the example shown, the user 10 seamlessly speaks a spelling structure 180 as part of the utterance 101 to provide a correct spelling for a particular phrase 182 (e.g., one or more particular words) recited in the utterance 101. Here, the spelling structure 180 recited by the user 10 in the utterance 101 includes the particular phrase “Khe Chai” 182; a spell trigger word “spell”
184; and a sequence of individual characters (e.g., “K H E space C H A I”) 186 that provides a correct spelling of the particular phrase 182.
[0030] The system 100 includes a user device 110. The user device 110 includes data processing hardware 112 and memory hardware 113. The user device 110 may include one or more audio capture devices (e.g., microphone(s) 114) for capturing and converting utterances 101 spoken by the user 10 into the audio data 102 (e.g., digital data or electrical signals). In some examples, the microphone 114 is separate from the user device 110 and in communication with the user device 110 to provide the utterance 101 to the user device 110. The user device 110 can be any computing device capable of communicating with the computing system 130 through the network 120. The user device 110 includes, but is not limited to, desktop computing devices and mobile computing devices, such as laptops, tablets, smart phones, smart keyboards, digital assistants, smart speakers/displays, vehicle infotainment systems, smart appliances, Intemet-of-things (loT) devices, and wearable computing devices (e.g., smart glasses/headsets and/or watches and/or smart headphones).
[0031] The system 100 may also include a computing system 130 in communication with the user device 110 via a network 120. The computing system 130 may be a distributed system (e.g., a cloud computing environment) having scalable elastic resources. The resources include computing resources 132 (e.g., data processing hardware) and/or storage resources 134 (e.g., memory hardware). The network 120 can be wired, wireless, or a combination thereof, and may include private networks and/or public networks, such as the Internet.
[0032] In some examples, the user device 110 executes (i.e., on the data processing hardware 112) a speech recognizer 140, for processing the audio data 102 to generate an initial transcription 107 and a spelling normalizer 150 to normalize the initial transcription 107 into a final transcription 170 entirely on-device without requiring any connection to the computing system 170. While examples herein depict the speech recognizer 140 and the spelling normalizer 150 executing on the user device 110 (i.e., on the data processing hardware 112), some or all of the functionality of the speech recognizer 140 and/or the spelling normalizer 150 may execute on the computing system
130 (i.e., on the data processing hardware 132) in addition to, or in lieu of, the user device 110 without departing from the scope of the present disclosure. For instance, the computing system 130 may receive the audio data 102 from the user device 110 via the network 120, and execute the speech recognizer 140 to process the audio data 102 and generate the initial transcription 107. The spelling normalizer 150 executing on the computing system 130 (or the user device 110) may normalize the initial transcription 107 into the final transcription 170 and provide the final transcription 170 to an output module 160.
[0033] In the example shown, the user 10 speaks a sequence of individual characters 186 that includes two or more spans of consecutive individual characters 186a-n each providing a correct spelling for a corresponding word or portion of the particular phrase 182. For instance, a first span of consecutive individual characters 186a “K H E” provides the correct spelling for a first word “Khe” in the particular phrase 182 while a second span of consecutive individual characters 186b “C H A I” provides the correct spelling for a second word “Chai” in the particular phrase 182. Moreover, the user 10 may also speak a space command 185 “space” between each adjacent pair of the two or more spans of consecutive individual characters 186a, 186b to separate the two corresponding words spelled therefrom by a space in the final transcription 170. In other examples, the user 10 pausing between speaking two adjacent characters in a sequence of characters (e.g., between “E” and “C” in a sequence “K H E C H A I”) divides the sequence of characters into the two or more spans of spoken individual characters (e.g., “K H E” and “C H A I”) without requiring the user 10 to explicitly recite a space command 185. In some instances, the user 10 neither speaks the space command, nor pauses between two adjacent characters. In such instances, the speech recognizer 140 and spelling normalizer 150 may nevertheless recognize that a sequence of characters spells a particular phrase containing more than one word that should be delineated by a space.
[0034] In some implementations, as part of the spelling structure 180, the user 10 also recites one or more spell trigger words 184 after reciting the particular phrase 184 but before reciting the sequence of individual characters 186. As will become apparent, the
recitation of a spell trigger word 184 (e.g., “spell”) by the user 10 while speaking the utterance 101 serves as a hint to inform the speech recognizer 140 that the following sequence of individual characters 186 the user 10 is to recite corresponds to a correct spelling of the particular phrase 182 recited by the user 10 just before reciting the spell trigger word 184. Additionally or alternatively, the user may recite an initiating trigger word (not shown) in the utterance 101 prior to reciting the particular phrase 182. As with the spell trigger word 184, the initiating trigger word may serve as a hint to inform the speech recognizer 140 that the user 10 is providing a spelling structure 180 for spellingout the particular phrase 182 that immediately follows the initiating trigger word in the utterance 101 spoken by the user 10.
[0035] The speech recognizer 140 processes the audio data 102 characterizing the utterance 101 captured by the user device 110. The speech recognizer 140 includes an ASR model 142 that performs speech recognition on the audio data 102 to generate a word lattice 200 indicating multiple candidate hypotheses 105, 105a-n for the utterance 101. The word lattice 200 may also be referred to as a decoding graph. Accordingly, the ASR model 142 may evaluate potential paths through the word lattice 200 to determine the multiple candidate hypotheses 105.
[0036] The ASR model 142 may include an end-to-end neural network model that converts audio data directly into corresponding text, without requiring the use of separately trained acoustic, pronunciation, and language models. In some examples, the ASR model 142 includes a streaming speech recognition model capable of emitting streaming transcription results as the user speaks. In these examples, the ASR model 142 may continuously emit initial transcriptions 107 as the audio data 102 is processed in a streaming fashion and display the most current initial transcription 107 for display on a screen (e.g., graphical user interface) 115 in communication with the user device 110. An example streaming speech recognition model 142 may include a recurrent neural network-transducer (RNN-T) architecture.
[0037] FIG. 2A illustrates an example of a word lattice 200, 200a that may be output from the ASR model 142 of FIG. 1. The word lattice 200a represents multiple possible combinations of words that may form different candidate hypotheses 105 (FIG. 1) for a
spoken utterance 101. The word lattice 200a includes one or more nodes 202a-g that correspond to the possible boundaries between words. The word lattice 200a includes multiple edges 204a-l for the possible words in the candidate hypotheses that result from the word lattice 200a. In addition, each of the edges 204a-l can have one or more weights or probabilities of that edge being the correct edge from the corresponding node. The weights are determined by the ASR model 142 and can be based on, for example, a confidence in the match between the speech data and the word for that edge and how well the word fits grammatically and/or lexically with other words in the word lattice 200a. [0038] While the word lattice 200a and a word lattice 200b shown in FIG. 2B are described with reference to words as nodes, this is for illustration purposes only. Alternatively, a lattice may include a graph of sub-word units where nodes of the lattice represent sub-word units output by the ASR model 142 that are used to form words based on the weights or probabilities of the edges connecting the nodes. Moreover, a lattice may include a graph of words and sub-word units, or any other parts of speech. Subwords in the word lattice 200 may include any combination of words, wordpieces, and individual characters/graphemes.
[0039] For example, initially, the most probable path (e.g., most probable candidate hypothesis 200) through the word lattice 200a may include the edges 204c, 204e, 204i, 204k, which have the text “we’re coming about 11 :30.” A second best path (e.g., second best candidate hypothesis 200) through the word lattice 200a may include the edges 204d, 204h, 204j, 2041, which have the text “deer hunting scouts 7:30.”
[0040] Each pair of nodes may have one or more paths corresponding to the alternate words in the various candidate hypotheses 200. For example, the initial most probable path between the node pair beginning at node 202a and ending at the node 202c is the edge 204c “we’re.” This path has alternate paths that include the edges 204a, 204b “we are” and the edge 204d “deer.”
[0041] FIG. 2B is an example of a hierarchical word lattice 200, 200b that may be provided by the ASR model 142 of FIG. 1. The word lattice 200b includes nodes 252a-l that represent words that make up the various candidate hypotheses 200 for a spoken utterance 101. The edges between the nodes 252a-l show that the possible candidate
hypotheses 200 include: (1) the nodes 252c, 252e, 252i, 252k “we’re coming about 11 :30”; (2) the nodes 252a, 252b, 252e, 252i, 252k “we are coming about 11 :30”; (3) the nodes 252a, 252b, 252f, 252g, 252i, 252k “we are come at about 11 :30”; (4) the nodes 252d, 252f, 252g, 252i, 252k “deer come at about 11 :30”; (5) the nodes 252d, 252h, 252j, 252k “deer hunting scouts 11 :30”; and (6) the nodes 252d, 252h, 252j, 2521 “deer hunting scouts 7:30.”
[0042] Again, the edges between the nodes 252a-l may have associated weights or probabilities based on the confidence in the speech recognition (e.g., candidate hypothesis) and the grammatical/lexical analysis of the resulting text. In this example, “we’re coming about 11 :30” may currently be the best hypothesis and “deer hunting scouts 7:30” may be the next best hypothesis. One or more divisions, 354a-d, can be made in the word lattice 200b that group a word and its alternates together. For example, the division 254a includes the word “we’re” and the alternates “we are” and “deer.” The division 252b includes the word “coming” and the alternates “come at” and “hunting.” The division 254c includes the word “about” and the alternate “scouts” and the division 254d includes the word “11 :30” and the alternate “7:30.”
[0043] Referring back to FIG. 1, the speech recognizer 140 determines an initial transcription 107 as a highest-ranking candidate hypothesis 105 among the multiple candidate hypotheses in the word lattice 200. Notably, the initial transcription 107 includes a misrecognition of the particular phrase 182 by the ASR model 142 where transcription 107 includes the misrecognized phrase 182m “kitchen” instead of the correct particular phrase 182 “Khe Chai”. Following the misrecognized phrase 182m, the initial transcription 107 also includes the sequence of individual characters 186 that provide the correct spelling of the particular phrase 182. Implementations herein are directed toward the speech recognizer 140 detecting/recognizing, from the lattice 200, the spelling structure 180 spoken by the user 10 to indicate that the user 10 is speaking the sequence of individual characters 186 to represent a correct spelling for the preceding particular phrase 182. In some implementations, the speech recognizer 140 includes a finite state transducer (FST) 300 (e.g., a weighted FST) downstream of the ASR model 142 for evaluating the potential paths through the word lattice 200 that form the different
candidate hypotheses 105 for the spoken utterance 101. The FST 300 may determine confidence scores for each of the candidate hypotheses and identify the initial transcription 107 as a highest ranking one of the candidate hypotheses 105 output by the ASR model 142 at a corresponding output step. The FST 300 may, for example, execute a beam search on the word lattice 200 to identify and score candidate hypotheses 105 for the audio data 102, and select or determine an initial transcription 107 of the spoken utterance 101 to be the candidate hypotheses 105 having the highest likelihood score 106. [0044] In some examples, the FST 300 includes a spelling component 310 configured to detect the presence of the spelling structure 180 in the initial transcription 107 by identifying the one or more spell trigger words 184 in the initial transcription 107 subsequent to the misrecognition of the particular phrase 182m. Specifically, the weighted FST 300 may transduce recognition of a trigger word 184 in the initial transcription 107 into a corresponding trigger word token 184t that the spelling normalizer 150 will remove from inclusion in the resulting final transcription 170. That is, detecting the spelling structure 180 in the initial transcription 108 by the spelling component 310 may cause the weighted FST 300 to suppress selection of any spell trigger words 184 as well as any initiating triggers words detected by the spelling component 310 in the lattice 200 by setting their outputs to epsilon and including any trigger word tokens 184t (e.g., “<spell>”) in the initial transcription 107. Other spell tokens may be used. The trigger word token 184t may be used regardless of what type of trigger words (e.g., spell trigger word or initiating trigger word) are detected. The trigger word token 184t may be included when a detected spelling structure contains one or more spell trigger words (e.g., “spell,” “as in,” “spell . . . as,” “spell ... as in,” etc.). When recited by the user 10, the initiating trigger word and the spell trigger word form a predefined spell command operative to inform the spelling component 310 the user 10 is reciting a sequence of individual characters to provide a correct spelling for a particular phrase that precedes the sequence of individual characters.
[0045] In some examples, the spelling component 310 of the weighted FST 300 detects a spelling structure in the initial transcription 107 that does not include trigger word tokens 184t indicative of the user 10 never reciting the initiating trigger word or the
spell trigger word in the utterance 101. For example, the spelling component 310 may detect/recognize the sequence of individual characters spoken by the user in the utterance, and then use acoustic modeling and/or language modeling techniques to determine that the sequence of individual characters provides a correct spelling for a particular phrase (whether misrecognized or recognized correctly) that precedes the sequence of individual characters in the transcription 107.
[0046] The speech recognizer 140 may also identify pauses in a spoken sequence of individual characters and/or utterances of a space command 185, such as “space,” within the sequence of individual characters. Detecting the space command 185 in the initial transcription 107 causes the weighted F ST 300 to output a corresponding space command token 185t in the initial transcription 107 in order to suppress selection of the space command 185 detected by the spelling component 310 in the lattice 200 by setting its output to epsilon. For example, the weighted FST 300 may include, in the initial transcription 107, the space token 185t, such as “<space>.”
[0047] FIG. 3 is a schematic view of an example spelling component 310 of the weighted FST 300 configured to detect a presence of a spelling structure 180 spoken as an integral part of an utterance 101 spoken by the user 10. The spelling component 310 represents the detection of various possible spelling structures containing a particular phrase, a sequence of individual characters, optional spell trigger words 184, optional initiating trigger words 181, and/or optional space commands 185. In the example shown, the weighted FST 300 is modified to include the spelling component 310 between a pair of nodes 302A and 302B of the word lattice 200 output by the ASR model 142.
For example, included between a pair of possible boundaries between possible transcribed words or subwords for the spoken utterance (e.g., between nodes 202 A and 202B of FIG. 2A or nodes 252a and 252b of FIG. 2B). The spelling structure 310 is configured to detect the presence of a spelling structure 180 in the lattice 200 providing multiple candidate hypotheses 105 (FIG. 1) for the utterance 101 spoken by the user 10. [0048] FIG. 3 shows operations (A) to (D) that illustrate a flow of data. During operation (A), the spelling component 310 executes an initiating trigger word decoding graph 320 for determining whether a corresponding subword 340 in the lattice 200
includes an initiating trigger word 181 (e.g., “spell”) that a user 10 may speak to indicate a start of the spelling structure 180 within the utterance before speaking the particular phrase 182 the user wants to seamless spell. That is, the initiating trigger word 181 is spoken immediately before speaking the particular phrase 182 followed by the sequence of individual characters 184 that provide the correct spelling for the particular phrase 182. Continuing with the example, the initiating trigger word decoding graph 320 may detect the initiating trigger word 181 “spell” in an utterance QV spell Khe Chai as K H E C H A I.” In some examples, the F ST 300 sets likelihood scores of triggers detected by the decoding graph 320 to epsilon in other portions of the FST 300 such that they do not appear in the initial transcription 107, but are replaced with a trigger word token 184t in the initial transcription 107, as described above.
[0049] During operation (B), the spelling component 310 next executes a spoken word decoding graph 330 for determining whether a next corresponding sub word 340 in the lattice as the particular phrase 182. In some examples, the decoding 330 detects words by detecting a prefix token (e.g., “_A,” “_B,” “_a,” “_b,” etc., where signifies a space before a character) followed by one or more non-prefix tokens (e.g., “A,” “B,” “a”, “b,” etc.). In some examples, the FST 300 applies a small penalty weight to scoring of words 340 detected by the decoding graph 330 in other portions of the FST 300 to slightly increase a likelihood score for the spelling component 310 over other possible paths in the lattice 200. While FIG. 3 depicts the spoken word decoding graph 330 of operation (B) occurring after performing the initiating trigger word decoding graph 320 of operation (A), the spoken word decoding graph 330 may detect the word 340 corresponding to the particular phrase 182 as the start of a spelling structure regardless of whether the initiating trigger word decoding graph 320 performed by operation (F) detected a preceding initiating trigger word. Continuing with the example, the spoken word decoding graph 330 detects the misrecognized phrase 182m “kitchen” representing a misrecognition of the particular phrase 182 “Khe Chai” by the ASR model 182.
[0050] During operation (C), the spelling component 310 executes a spell trigger decoding graph 350 for determining whether a next corresponding subword 340 in the lattice 200 includes a spell trigger word 184 that the user may speak after speaking the
particular phrase 182 but prior to speaking the sequence of individual characters 186 that provides the correct spelling for the particular phrase 182. For example, the spell trigger decoding graph 350 may determine that the corresponding word 340 “spell” in an utterance of “Khe Chai spell K H E C H A I,” or the word “as” in an utterance of “ spell Khe Chai as K H E C H Al” as spell trigger words 184. As shown, spell trigger words detected by the decoding graph 350 may be a continuance of initiating spell trigger words detected by the decoding graph 320.
[0051] During operation (D), the spelling structure 310 executes an individual character decoding graph 360 for determining whether a next corresponding sub word 340 in the lattice 200 is part of the sequence of individual characters 186 (e.g., “ A,” “_B,” “_a,” “_b,” etc., where signifies a space before a character) in the spelling structure 180 that provide the correct spelling for the particular phrase 182. The individual character decoding graph 360 may continuously loop upon determining each individual character in the sequence of individual characters 186. Accordingly, the decoding graphs 320, 330, 350, 360 performed by operations (A) to (D) may detect the presence of the spelling structure 180 in the initial transcription 107.
[0052] Returning to FIG. 1, in response to detecting the spelling structure 180 in the initial transcription 107, the spelling normalizer 150 is configured to normalize the initial transcription 107 to obtain the final transcription 170 for the utterance 101 by replacing the misrecognition of the particular phrase 182m with a corrected phrase constructed by joining the sequence of individual characters 186 spoken by the user 10 seamlessly in the utterance 101. Continuing with the example, the spelling normalizer 150 replaces the misrecognized phrase 182m in the initial transcription 107 (e.g., “kitchen”) with a spelled or corrected phrase (e.g., “Khe Chai) constructed from the sequence of individual characters 186.
[0053] The spelling normalizer 150 may execute a spell structure detector 152 to detect the spelling structure 180 in the initial transcription 107. For instance, the spell structure detector 152 may detect any of the trigger word tokens 184t (e.g., “<spell>”) in addition to the individual sequence of individual characters 186. In response to detecting the spelling structure 180 in the initial transcription 107, the spelling normalizer 150
proceeds to execute a spelling extractor 153 to construct, or otherwise form, a corrected phrase (e.g., “Khe Chai”) from the sequence of individual characters 186. In particular, the spelling extractor 153 first extracts a longest sequence of individual characters 186, blank spaces, and space token 185 subsequent to the trigger word token 184t in the initial transcription 107 (e.g., “K H E <space> C H A I”).
[0054] The spelling extractor 153 then constructs the corrected phrase from the extracted sequence of individual characters 186 by joining the individual characters for each span 186a, 186b to form a corresponding one of two or more particular words (e.g., Khe and Chai) and replacing each space token (e.g., <space>) 185 in the initial transcription with a blank space. The spelling extractor 153 may also remove spaces (white/blank spaces) between the characters when joining the individual characters together to form the corrected phrase. The spelling extractor 153 may additionally normalize the spelled words by executing a capitalization module 154. The capitalization module 154 may capitalize each spelled word, as may be suitable for proper names (e.g., forming a spelled or corrected phrase of “Khe Chai”).
[0055] The spelling normalizer 150 also executes a phrase extractor 155 to identify and extract the misrecognized particular phrase 182m from the initial transcription 107. Notably, the particular phrase 182 may be recognized correctly, however the spelling normalizer needs to replace the particular phrase with the correct phrase constructed from the sequence of individual characters to avoid repeating the particular phrase in the final transcription 170. The phrase extractor 155 determines the misrecognized phrase to be a span of words preceding the spell token that best matches the corrected particular phrase (e.g., “Khe Chai”). As described above, the decoding graph 330 of the spelling component 310 (see FIG. 3) may identify or detect the span of words. In some example, the number of words identified by the spelling component 310 may be different from the number of words in the corrected particular phrase. For example, “Khe Chai” may be misrecognized as “kitchen.” Continuing with the example, the phrase extractor 155 identifies “kitchen” as the misrecognized particular phrase 182m in the initial transcription 107.
[0056] In some examples, the phrase extractor 155 uses Levenshtein distance to measure the distance (e.g., minimum edits) between two strings. In particular, the phrase extractor 156 may use Levenshtein distances to compare the spelled words of the corrected phrase with multiple spans of words of different lengths that precede the spell token in the initial transcription 107, and choose the span of words with the smallest Levenshtein distance as the misrecognized particular phrase. In the example shown, the phrase extractor 155 computes Levenshtein distances between the corrected phrase “Khe Chai” and the word spans “kitchen,” “is kitchen,” “name is kitchen,” and “My name is kitchen.” In some examples, the phrase extractor 155 computes Levenshtein distances using dynamic programming by progressively computing the Levenshtein distances of sub-strings (e.g., “kitchen” is a sub-string of “name is kitchen”). That is, the phrase extractor 155 may compute Levenshtein distances of multiple word spans by performing only one set of Levenshtein distance calculations based on the longest span (e.g., “My name is kitchen”), as shown in FIG. 4.
[0057] FIG. 4 is a table 400 representing example Levenshtein distances between sub-strings. In particular, each cell of the table 400 represents a Levenshtein distance between two particular sub-strings. The example of FIG. 4 represents Levenshtein distances between sub-strings of a spelled corrected phrase “Tsim Sha Tsui,” and substrings of candidate misrecognized phrases: “surgery,” “Tim surgery,” “is Tim surgery,” and “Where is Tim surgery.” An example entry in cell 405 indicates that the Levenshtein distance between “im Surgery” and “Tsui” is 8. Cells 410, 415, 420, and 425 contain example Levenshtein distances between the spelled corrected phrase “Tsim Sha Tsui” and respective ones of the four candidate sub-strings: “surgery,” “Tim surgery,” “is Tim surgery,” and “Where is Tim surgery.” In this example, the sub-string “Tim Surgery” corresponding to cell 415 is identified as misrecognized phrase because it has the smallest Levenshtein distance from the spelled phrase “Tsim Sha Tsui.” [0058] The table 400 has (R + 1) x (C + 1) entries, where R is the number of characters (including spaces) in the spelled phrase, and C is the number of characters (including spaces) in the longest candidate phrase. Row indices shown in the right-most
column, and column indices shown in the bottom-most row are specified with respect to the origin at the bottom right. The (r, c)th entry at row r and column c can be computed recursively using the following mathematical expression:
[0059] Referring back to FIG. 1, the spelling normalizer executes a normalizer 156 for replacing the misrecognized particular phrase (e.g., “kitchen”) identified by the phrase extractor 155 with the spelled corrected phrase (e.g., “Khe Chai”) to provide the final transcription 104 “My name is Khe Chai”.
[0060] An output module 160 may output the final transcription 104 for display on the screen 115 of the user device 110. The output module 160 may also receive initial transcriptions 107 output by the speech recognizer and display the initial transcriptions as partial speech recognition results on the screen 115 in a streaming fashion. The output module 160 may also include a downstream application (e.g., digital assistant) that performs semantic interpretation (e.g., natural language understanding (NLU)) on the transcription 104 to identify a command/query for the application to perform. The output module 160 may include a search engine. The output module 160 may include an input method editor (IME). The output module 160 may include a messaging application for communicating the transcription 104 as contents of a message to another user. The output module 160 ma further include a model updater for personalizing the ASR model 142 to improve recognition of the particular phrase in a subsequent utterance. The output module 160 may include a natural language processing (NLP) module for interpreting the transcription 170. Other types of output modules 160 may also be implemented.
[0061] In some examples, the audio data 102 is received as streaming data as the user 10 speaks an utterance (e.g., “My name is Khe Chai, and I am glad to meet you”), and the speech recognizer 140 performs streaming speech recognition on the streaming audio data 102 as the audio data 102 is received to provide streaming speech recognition
results. For example, as the user 10 speaks the utterance, the speech recognizer 140 may determine partial initial transcriptions 108 that reflect what the user 10 has already spoken. For instance, as and after the user 10 speaks “My name is Khe Chai,” but before they speak the rest of the utterance 101 (e.g., “and I am glad to meet you”), the output module 160 may output a partial initial transcription 108 of “My name is kitchen” on the screen 115. The user 10 may, in response to seeing or recognizing the misrecognition of the particular phrase “Khe Chai” as “kitchen” in the displayed partial initial transcription 108, seamlessly speak a spoken spelling structure as a seamless or integral part of the rest of their utterance. For example, the user 10 may continue by speaking a spelling structure “spell K H E space C H A I,” thus, providing a correct spelling for the misrecognized particular phrase “Khe Chai,” and then continue with speaking the rest of their utterance “and I am glad to meet you.” In this way, the user 10 can temporally and seamlessly change to spelling in the midst of ongoing ASR, and then seamlessly return to speaking the rest of their utterance.
[0062] FIG. 5 is a flowchart of an exemplary arrangement of operations for a method 500 that may be performed by data processing hardware for providing seamless spelling for ASR systems. Data processing hardware (e.g., the data processing hardware 112 of the user device 110 and/or the data processing hardware 132 of the computing system 130 of FIG. 1) may execute the operations for the method 500 by executing instructions stored on memory hardware (e.g., the memory hardware 113, 134).
[0063] At operation 502, the method 500 includes receiving audio data 102 corresponding to an utterance 101 spoken by a user 10. The utterance 101 including a particular phrase (e.g., the phrase 103a of FIG. 1) and a sequence of characters that provide a correct spelling of the particular phrase (e.g., the characters 103c) spoken after the particular phrase.
[0064] At operation 504, the method 500 includes processing the audio data 102, using an ASR model 142, to generate an initial transcription (e.g., the initial transcription 107). The initial transcription containing a misrecognition of the particular phrase by the ASR model (e.g., “kitchen”) followed by the sequence of individual characters that provide a correct spelling of the particular phrase.
[0065] At operation 506, the method 500 includes detecting a spelling structure in the initial transcription (e.g., the spelling structure 103) subsequent to the misrecognition of the particular phrase. At operation 508, and in response to detecting the spelling structure in the initial transcription, the method 500 includes extracting, from the initial transcription, the sequence of individual characters that provide the correct spelling of the particular phrase (e.g., the sequence of individual characters 107g).
[0066] At operation 510, the method 500 includes constructing a corrected phrase for the misrecognized particular phrase from the extracted sequence of individual characters (e.g., “Khe Chai”). The method 500 continues at operation 512 to replace the misrecognized phrase with the corrected phrase in the initial transcription, and at operation 514 to normalize the initial transcription to obtain a final transcription (e.g., the final transcription 104).
[0067] FIG. 6 is a schematic view of an example computing device 600 that may be used to implement the systems and methods described in this document. For example, the computing device 600 may be used to implement the user device 110 and/or the computing system 130. The computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
[0068] The computing device 600 includes a processor 610 that may be used to implement the data processing hardware 112 and/or 132, memory 620 that may be used to implement the memory hardware 113 and/or 134, a storage device 630 that may be used to implement the memory hardware 113 and/or 134, a high-speed interface/controller 640 connecting to the memory 620 and high-speed expansion ports 650, and a low speed interface/controller 660 connecting to a low speed bus 670 and a storage device 630. Each of the components 610, 620, 630, 640, 650, and 660, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 610 can process instructions for
execution within the computing device 600, including instructions stored in the memory 620 or on the storage device 630 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 680 coupled to high speed interface 640. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices 600 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multiprocessor system).
[0069] The memory 620 stores information non-transitorily within the computing device 600. The memory 620 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). The non-transitory memory 620 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 600. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable readonly memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
[0070] The storage device 630 is capable of providing mass storage for the computing device 600. In some implementations, the storage device 630 is a computer- readable medium. In various different implementations, the storage device 630 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or
machine-readable medium, such as the memory 620, the storage device 630, or memory on processor 610.
[0071] The high speed controller 640 manages bandwidth-intensive operations for the computing device 600, while the low speed controller 660 manages lower bandwidthintensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 640 is coupled to the memory 620, the display 680 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 650, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 660 is coupled to the storage device 630 and a low-speed expansion port 690. The low-speed expansion port 690, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
[0072] The computing device 600 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 600a or multiple times in a group of such servers 600a, as a laptop computer 600b, or as part of a rack server system 600c.
[0073] Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
[0074] These computer programs (also known as programs, software, software applications, or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming
language, and/or in assembly/machine language. As used herein, the terms “machine- readable medium” and “computer-readable medium” refer to any computer program product, non-transitory computer readable medium, apparatus, and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
[0075] The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
[0076] To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
[0077] A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims
1. A computer-implemented method (500) that, when executed on data processing hardware (610), causes the data processing hardware (610) to perform operations comprising: receiving audio data (102) characterizing an utterance (101) spoken by a user, the utterance (101) comprising a particular phrase (182) and a sequence of individual characters (186) that provide a correct spelling of the particular phrase (182) spoken after the particular phrase (182); processing, using an automatic speech recognition (ASR) model, the audio data (102) to generate an initial transcription (107) for the utterance (101), the initial transcription (107) comprising a misrecognition of the particular phrase (182) by the ASR model (142) followed by the sequence of individual characters (186) that provide the correct spelling of the particular phrase (182); detecting a spelling structure (103) in the initial transcription (107) subsequent to the misrecognition of the particular phrase (182); and in response to detecting the spelling structure (103) in the initial transcription (107): extracting, from the initial transcription (107), the sequence of individual characters (186) that provide the correct spelling of the particular phrase (182); constructing a corrected phrase for the misrecognition of the particular phrase (182) from the extracted sequence of individual characters (186); extracting, from the initial transcription (107), the misrecognition of the particular phrase (182); and normalizing the initial transcription (107) to obtain a final transcription (104) for the utterance (101) by replacing the extracted misrecognition of the particular phrase (182) with the corrected phrase constructed from the extracted sequence of individual characters (186).
2. The method (500) of claim 1, wherein:
receiving the audio data (102) comprises receiving the audio data (102) as the user speaks the utterance (101); performing speech recognition on the audio data (102) comprises performing streaming speech recognition on the audio data (102) as the audio data (102) is received to generate, as output from the ASR model (142), streaming speech recognition results; and the operations further comprise providing each streaming speech recognition result generated as output from the ASR model (142) for display on a screen (115) in communication with the data processing hardware (610).
3. The method (500) of claims 1 or 2, wherein the operations further comprise displaying a partial speech recognition result including the misrecognition of the particular phrase (182) on the screen (115) before the user speaks the sequence of individual characters (186) comprising the spelling of the particular phrase (182).
4. The method (500) of any of claims 1-3, wherein detecting the spelling structure (103) comprises executing a weighted finite state transducer (wFST) having a spelling component (310) configured to detect the spelling structure (103) in the initial transcription (107) by identifying one or more spell trigger words (184) in the initial transcription (107) subsequent to the misrecognition of the particular phrase (182).
5. The method (500) of claim 4, wherein detecting the spelling structure (103) comprises executing a weighted finite state transducer (wFST) having a spelling component (310) configured to detect the spelling structure (103) in the initial transcription (107) by identifying one or more initiating trigger words in the initial transcription (107) preceding the misrecognition of the particular phrase (182).
6. The method (500) of claim 5, wherein the one or more initiating trigger words and the one or more spell trigger words (184) form a predefined spell command.
7. The method (500) of claim 5 or 6, wherein detecting the spelling structure (103) in the initial transcription (107) is further based on identifying the sequence of individual characters (186) in the initial transcription (107) subsequent to identifying the one or more spell trigger words (184).
8. The method (500) of any of claims 1-7, wherein the operations further comprise, after constructing the corrected phrase from the extracted sequence of individual characters (186), applying a capitalization normalizer (156) to capitalize a first letter of at least one word in the corrected phrase.
9. The method (500) of any of claims 1-8, wherein: the particular phrase (182) spoken by the user in the utterance (101) comprises two or more particular words; the sequence of individual characters (186) that provide the correct spelling of the particular phrase (182) comprise two or more spans of consecutive individual characters (186) that each provide a correct spelling for a corresponding one of the two or more particular words; and the utterance (101) spoken by the user further comprises the user speaking a space command (185) between each adjacent pair of the two or more spans of consecutive individual characters (186).
10. The method (500) of claim 9, wherein: the initial transcription (107) further comprises a space token (185t) inserted between each adjacent pair of the two or more spans of consecutive individual characters (186) of the sequence of individual characters (186); and constructing the corrected phrase from the extracted sequence of individual characters (186) comprises: for each span of consecutive individual characters (186), joining the individual characters (186) to form the corresponding one of the two or more particular words; and
replacing each space token (185t) in the initial transcription (107) with a blank space.
11. The method (500) of any of claims 1-10, wherein the misrecognition of the particular phrase (182) comprises one or more words.
12. The method (500) of any of claims 1-11, wherein extracting the misrecognition of the particular phrase (182) is based on an edit distance between the sequence of individual characters (186) in the initial transcription (107) and the misrecognition of the particular phrase (182).
13. The method (500) of any of claims 1-12, wherein extracting the misrecognition of the particular phrase (182) comprises: determining a corresponding edit distance between the sequence of individual characters (186) and each word span of multiple word spans of different lengths in the initial transcription (107), the multiple word spans preceding the sequence of individual characters (186); and identifying the word span of the multiple word spans of different lengths that has a shortest corresponding edit distance to the misrecognition of the particular phrase (182).
14. The method (500) of claim 13, wherein the corresponding edit distance comprises a corresponding Levenshtein distance dynamically computed for each of the multiple word spans via a single calculation.
15. The method (500) of any of claims 1-14, wherein detecting the spelling structure (103) in the initial transcription (107) and normalizing the initial transcription (107) to obtain the final transcription (104) occurs without receiving any user input after the user finishes speaking the utterance (101).
16. A system (100) comprising:
data processing hardware (610); and memory hardware (620) in communication with the data processing hardware (610) and storing instructions that, when executed on the data processing hardware (610), cause the data processing hardware (610) to perform operations comprising: receiving audio data (102) characterizing an utterance (101) spoken by a user, the utterance (101) comprising a particular phrase (182), and a sequence of individual characters (186) that provide a correct spelling of the particular phrase (182) spoken after the particular phrase (182); processing, using an automatic speech recognition (ASR) model, the audio data (102) to generate an initial transcription (107) for the utterance (101), the initial transcription (107) comprising a misrecognition of the particular phrase (182) by the ASR model (142) followed by the sequence of individual characters (186) that provide the correct spelling of the particular phrase (182); detecting a spelling structure (103) in the initial transcription (107) subsequent to the misrecognition of the particular phrase (182); and in response to detecting the spelling structure (103) in the initial transcription (107): extracting, from the initial transcription (107), the sequence of individual characters (186) that provide the correct spelling of the particular phrase (182); constructing a corrected phrase for the misrecognition of the particular phrase (182) from the extracted sequence of individual characters (186); extracting, from the initial transcription (107), the misrecognition of the particular phrase (182); and normalizing the initial transcription (107) to obtain a final transcription (104) for the utterance (101) by replacing the extracted misrecognition of the particular phrase (182) with the corrected phrase constructed from the extracted sequence of individual characters (186).
17. The system (100) of claim 16, wherein:
receiving the audio data (102) comprises receiving the audio data (102) as the user speaks the utterance (101); performing speech recognition on the audio data (102) comprises performing streaming speech recognition on the audio data (102) as the audio data (102) is received to generate, as output from the ASR model (142), streaming speech recognition results; and the operations further comprise providing each streaming speech recognition result generated as output from the ASR model (142) for display on a screen (115) in communication with the data processing hardware (610).
18. The system (100) of claims 16 or 17, wherein the operations further comprise displaying a partial speech recognition result including the misrecognition of the particular phrase (182) on the screen (115) before the user speaks the sequence of individual characters (186) comprising the spelling of the particular phrase (182).
19. The system (100) of any of claims 16-18, wherein detecting the spelling structure (103) comprises executing a weighted finite state transducer (wFST) having a spelling component (310) configured to detect the spelling structure (103) in the initial transcription (107) by identifying one or more spell trigger words (184) in the initial transcription (107) subsequent to the misrecognition of the particular phrase (182).
20. The system (100) of claim 19, wherein detecting the spelling structure (103) comprises executing a weighted finite state transducer (wFST) having a spelling component (310) configured to detect the spelling structure (103) in the initial transcription (107) by identifying one or more initiating trigger words in the initial transcription (107) preceding the misrecognition of the particular phrase (182).
21. The system (100) of claim 20, wherein the one or more initiating trigger words and the one or more spell trigger words (184) form a predefined spell command.
22. The system (100) of claim 20 or 21, wherein detecting the spelling structure (103) in the initial transcription (107) is further based on identifying the sequence of individual characters (186) in the initial transcription (107) subsequent to identifying the one or more spell trigger words (184).
23. The system (100) of any of claims 16-22, wherein the operations further comprise, after constructing the corrected phrase from the extracted sequence of individual characters (186), applying a capitalization normalizer (156) to capitalize a first letter of at least one word in the corrected phrase.
24. The system (100) of any of claims 16-23, wherein: the particular phrase (182) spoken by the user in the utterance (101) comprises two or more particular words; the sequence of individual characters (186) that provide the correct spelling of the particular phrase (182) comprise two or more spans of consecutive individual characters (186) that each provide a correct spelling for a corresponding one of the two or more particular words; and the utterance (101) spoken by the user further comprises the user speaking a space command (185) between each adjacent pair of the two or more spans of consecutive individual characters (186).
25. The system (100) of claim 24, wherein: the initial transcription (107) further comprises a space token (185t) inserted between each adjacent pair of the two or more spans of consecutive individual characters (186) of the sequence of individual characters (186); and constructing the corrected phrase from the extracted sequence of individual characters (186) comprises: for each span of consecutive individual characters (186), joining the individual characters (186) to form the corresponding one of the two or more particular words; and
replacing each space token (185t) in the initial transcription (107) with a blank space.
26. The system (100) of any of claims 16-25, wherein the misrecognition of the particular phrase (182) comprises one or more words.
27. The system (100) of any of claims 16-26, wherein extracting the misrecognition of the particular phrase (182) is based on an edit distance between the sequence of individual characters (186) in the initial transcription (107) and the misrecognition of the particular phrase (182).
28. The system (100) of any of claims 16-27, wherein extracting the misrecognition of the particular phrase (182) comprises: determining a corresponding edit distance between the sequence of individual characters (186) and each word span of multiple word spans of different lengths in the initial transcription (107), the multiple word spans preceding the sequence of individual characters (186); and identifying the word span of the multiple word spans of different lengths that has a shortest corresponding edit distance to the misrecognition of the particular phrase (182).
29. The system (100) of claim 28, wherein the corresponding edit distance comprises a corresponding Levenshtein distance dynamically computed for each of the multiple word spans via a single calculation.
30. The system (100) of any of claims 16-29, wherein detecting the spelling structure (103) in the initial transcription (107) and normalizing the initial transcription (107) to obtain the final transcription (104) occurs without receiving any user input after the user finishes speaking the utterance (101).
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2022/076074 WO2024054229A1 (en) | 2022-09-07 | 2022-09-07 | Seamless spelling for automatic speech recognition systems |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2022/076074 WO2024054229A1 (en) | 2022-09-07 | 2022-09-07 | Seamless spelling for automatic speech recognition systems |
Publications (1)
Publication Number | Publication Date |
---|---|
WO2024054229A1 true WO2024054229A1 (en) | 2024-03-14 |
Family
ID=83594232
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/US2022/076074 WO2024054229A1 (en) | 2022-09-07 | 2022-09-07 | Seamless spelling for automatic speech recognition systems |
Country Status (1)
Country | Link |
---|---|
WO (1) | WO2024054229A1 (en) |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170263248A1 (en) * | 2016-03-14 | 2017-09-14 | Apple Inc. | Dictation that allows editing |
-
2022
- 2022-09-07 WO PCT/US2022/076074 patent/WO2024054229A1/en unknown
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170263248A1 (en) * | 2016-03-14 | 2017-09-14 | Apple Inc. | Dictation that allows editing |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9292487B1 (en) | Discriminative language model pruning | |
JP4791984B2 (en) | Apparatus, method and program for processing input voice | |
WO2020001458A1 (en) | Speech recognition method, device, and system | |
US20140244258A1 (en) | Speech recognition method of sentence having multiple instructions | |
US20080215328A1 (en) | Method and system for automatically detecting morphemes in a task classification system using lattices | |
CN109637537B (en) | Method for automatically acquiring annotated data to optimize user-defined awakening model | |
US20210064822A1 (en) | Word lattice augmentation for automatic speech recognition | |
US11093110B1 (en) | Messaging feedback mechanism | |
US8255220B2 (en) | Device, method, and medium for establishing language model for expanding finite state grammar using a general grammar database | |
Pennell et al. | Toward text message normalization: Modeling abbreviation generation | |
JP2008216756A (en) | Technique for acquiring character string or the like to be newly recognized as phrase | |
JP2007041319A (en) | Speech recognition device and speech recognition method | |
US9135912B1 (en) | Updating phonetic dictionaries | |
EP3948849A1 (en) | Phoneme-based contextualization for cross-lingual speech recognition in end-to-end models | |
US11978434B2 (en) | Developing an automatic speech recognition system using normalization | |
JP7400112B2 (en) | Biasing alphanumeric strings for automatic speech recognition | |
US9583095B2 (en) | Speech processing device, method, and storage medium | |
US11972758B2 (en) | Enhancing ASR system performance for agglutinative languages | |
US20220310067A1 (en) | Lookup-Table Recurrent Language Model | |
WO2024054229A1 (en) | Seamless spelling for automatic speech recognition systems | |
JP2003162524A (en) | Language processor | |
US20230186898A1 (en) | Lattice Speech Corrections | |
JP2938865B1 (en) | Voice recognition device | |
US11632345B1 (en) | Message management for communal account | |
KR20220090586A (en) | Automatic Speech Recognition Hypothesis Rescoring Using Audio-Visual Matching |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
121 | Ep: the epo has been informed by wipo that ep was designated in this application |
Ref document number: 22785884Country of ref document: EPKind code of ref document: A1 |