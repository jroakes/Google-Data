DE202017106616U1 - Language model influencing system - Google Patents
Language model influencing system Download PDFInfo
- Publication number
- DE202017106616U1 DE202017106616U1 DE202017106616.9U DE202017106616U DE202017106616U1 DE 202017106616 U1 DE202017106616 U1 DE 202017106616U1 DE 202017106616 U DE202017106616 U DE 202017106616U DE 202017106616 U1 DE202017106616 U1 DE 202017106616U1
- Authority
- DE
- Germany
- Prior art keywords
- grams
- language model
- speech recognition
- extended set
- gram
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000013518 transcription Methods 0.000 claims abstract description 85
- 230000035897 transcription Effects 0.000 claims abstract description 85
- 238000000034 method Methods 0.000 claims abstract description 27
- 230000007704 transition Effects 0.000 claims description 25
- 230000004044 response Effects 0.000 claims description 16
- 238000012545 processing Methods 0.000 claims description 14
- 230000008569 process Effects 0.000 description 16
- 238000004590 computer program Methods 0.000 description 11
- 238000004891 communication Methods 0.000 description 10
- 230000009471 action Effects 0.000 description 7
- 230000003993 interaction Effects 0.000 description 4
- 238000010586 diagram Methods 0.000 description 3
- 230000008859 change Effects 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 235000013305 food Nutrition 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 230000005236 sound signal Effects 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- BYHQTRFJOGIQAO-GOSISDBHSA-N 3-(4-bromophenyl)-8-[(2R)-2-hydroxypropyl]-1-[(3-methoxyphenyl)methyl]-1,3,8-triazaspiro[4.5]decan-2-one Chemical compound C[C@H](CN1CCC2(CC1)CN(C(=O)N2CC3=CC(=CC=C3)OC)C4=CC=C(C=C4)Br)O BYHQTRFJOGIQAO-GOSISDBHSA-N 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 150000001875 compounds Chemical class 0.000 description 1
- 239000000470 constituent Substances 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000010006 flight Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000033001 locomotion Effects 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 235000013550 pizza Nutrition 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/065—Adaptation
- G10L15/07—Adaptation to the speaker
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/187—Phonemic context, e.g. pronunciation rules, phonotactical constraints or phoneme n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/01—Assessment or evaluation of speech recognition systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
Abstract
Nicht transitorisches computerlesbares Speichergerät, das Software speichert, die Befehle umfasst, die durch einen oder mehrere Computer ausführbar sind, die bei einer solchen Ausführung den einen oder die mehreren Computer zum Ausführen eines computerimplementierten Verfahrens veranlassen, umfassend: Empfangen von Audiodaten, die einer Benutzeräußerung entsprechen, und von Kontextdaten für die Benutzeräußerung; Identifizieren eines anfänglichen Satzes von einem oder mehreren N-Grammen für die Kontextdaten; Erzeugen eines erweiterten Satzes von einem oder mehreren N-Grammen basierend mindestens auf dem anfänglichen Satz von N-Grammen, wobei der erweiterte Satz von N-Grammen eines oder mehrere N-Gramme umfasst, die sich von den N-Grammen im anfänglichen Satz von N-Grammen unterscheiden; Anpassen eines Sprachmodells basierend mindestens auf dem erweiterten Satz von N-Grammen; Ermitteln eines oder mehrerer Spracherkennungskandidaten für mindestens einen Teil der Benutzeräußerung mithilfe des angepassten Sprachmodells, wobei jeder Spracherkennungskandidat eines oder mehrere Wörter umfasst; nach dem Ermitteln des einen oder der mehreren Spracherkennungskandidaten, Anpassen einer Bewertung für einen bestimmten Spracherkennungskandidaten, von dem ermittelt wurde, dass er in den erweiterten Satz der N-Gramme einbezogen werden soll; nach Anpassen der Bewertung für den bestimmten Spracherkennungskandidaten, Ermitteln einer Transkription für die Benutzeräußerung, die mindestens einen des einen oder der mehreren Spracherkennungskandidaten beinhaltet; und Bereitstellen der Transkription der Benutzeräußerung für die Ausgabe.A non-transitory computer-readable storage device that stores software that includes instructions that are executable by one or more computers that, in such an embodiment, cause the one or more computers to perform a computer-implemented method, comprising: receiving audio data that corresponds to a user utterance , and contextual data for the user utterance; Identifying an initial set of one or more N-grams for the context data; Generating an extended set of one or more N-grams based at least on the initial set of N-grams, wherein the extended set of N-grams comprises one or more N-grams other than the N-grams in the initial set of N Differentiate between rams; Adapting a language model based at least on the extended set of N-grams; Determining, using the customized language model, one or more speech recognition candidates for at least a portion of the user utterance, each speech recognition candidate comprising one or more words; after determining the one or more speech recognition candidates, adjusting a score for a particular speech recognition candidate that has been determined to be included in the extended set of N-grams; after adjusting the score for the particular speech recognition candidate, determining a transcription for the user utterance that includes at least one of the one or more speech recognition candidates; and providing the transcription of the user utterance for the output.
Description
TECHNISCHES GEBIET TECHNICAL AREA
In Übereinstimmung mit den Bestimmungen des Gebrauchsmustergesetzes sind nur Vorrichtungen, wie sie in den anliegenden Ansprüchen definiert sind, geschützt und vom Gebrauchsmuster abgedeckt, nicht jedoch Verfahren. Soweit in der nachstehenden Beschreibung gegebenenfalls auf Verfahren Bezug genommen wird, dienen diese Hinweise nur zur exemplarischen Erläuterung der mit den anliegenden Schutzansprüchen geschützten Vorrichtung(en). In accordance with the provisions of the Utility Model Law, only devices as defined in the appended claims are protected and covered by utility model, but not methods. Insofar as reference is made in the description below to methods, these instructions serve only to exemplify the device (s) protected by the attached claims.
Diese Spezifikation bezieht sich auf Spracherkennung und eine bestimmte Implementierung bezieht sich auf die Beeinflussung von Sprachmodellen, die bei der Durchführung von Spracherkennung verwendet werden. This specification relates to speech recognition and a particular implementation relates to influencing speech models used in performing speech recognition.
HINTERGRUND BACKGROUND
Spracheingaben werden häufig verwendet, um verschiedene Operationen auf einem elektronischen Gerät durchzuführen. Einige Mobiltelefone oder andere Computergeräte ermöglichen es zum Beispiel Benutzern, Sprachbefehle bereitzustellen oder andere Spracheingaben bereitzustellen, um Operationen durchzuführen, Benutzerauswahlen anzugeben, Anwendungen zu starten oder die Kommunikation mithilfe des Mobiltelefons zu initiieren. Eine Spracheingabe eines Benutzers kann durch ein Mikrofon eines Computergeräts erkannt und durch eine automatische Spracherkennungsvorrichtung verarbeitet werden. Speech inputs are often used to perform various operations on an electronic device. For example, some mobile phones or other computing devices allow users to provide voice commands or provide other voice inputs to perform operations, indicate user selections, launch applications, or initiate communication using the mobile phone. A user's voice input may be detected by a microphone of a computing device and processed by an automatic speech recognition device.
Eine automatische Spracherkennungsvorrichtung kann ein akustisches Modell und ein Sprachmodell verwenden, um Spracherkennung durchzuführen. Die automatische Spracherkennungsvorrichtung kann das akustische Modell verwenden, um Beziehungen zwischen einem Audiosignal und phonetischen Einheiten in der Sprache des Benutzers zu ermitteln. Das Sprachmodell gibt wahrscheinliche phonetische oder Wortsequenzen für die Sprache des Benutzers an. Mithilfe dieser zwei Modelle kann die automatische Spracherkennungsvorrichtung Kandidatentranskriptionen der Spracheingabe erzeugen. An automatic speech recognition device may use an acoustic model and a speech model to perform speech recognition. The automatic speech recognition device may use the acoustic model to determine relationships between an audio signal and phonetic units in the language of the user. The language model indicates probable phonetic or word sequences for the user's language. Using these two models, the automatic speech recognition device can generate candidate transcriptions of the speech input.
ZUSAMMENFASSUNG SUMMARY
Diese Spezifikation beschreibt ein System, das die Genauigkeit der Spracherkennung durch Beeinflussung eines Sprachmodells, das durch eine automatische Spracherkennungsvorrichtung verwendet wird, verbessern kann. In einigen Implementierungen beeinflusst das System ein allgemeines Sprachmodell auf Basis von Kontextinformationen. Die Kontextinformationen können zum Beispiel eines oder mehrere N-Gramme (z. B. N-Gramme von Wörtern, wie z. B. einzelnen Wörtern oder Ausdrücke, die mehrere Wörter beinhalten) beinhalten, die vom System empfangen werden. Das System kann diese N-Gramme an einen oder mehrere Spracherweiterungsdienste senden und in Reaktion zusätzliche N-Gramme empfangen, die durch den Spracherweiterungsdienst identifiziert werden. Das allgemeine Sprachmodell kann auf Basis der N-Gramme beeinflusst werden, die in den Kontextinformationen enthalten sind, und der N-Gramme, die vom Spracherweiterungsdienst empfangen werden. Eine automatische Spracherkennungsvorrichtung kann das beeinflusste allgemeine Sprachmodell verwenden, um Spracherkennung durchzuführen. Die Beeinflussung des Sprachmodells auf diese Weise kann das Modell dynamisch so abstimmen, dass es Wörter bevorzugt, die für die aktuelle Aufgabe oder Situation des Benutzers relevant sind, was es dem System ermöglicht, Sprache genauer zu transkribieren, insbesondere bei Wörtern, die in der normalen Sprache ungewöhnlich sind. This specification describes a system that can improve the accuracy of speech recognition by influencing a speech model used by an automatic speech recognition device. In some implementations, the system influences a general language model based on context information. The context information may include, for example, one or more N-grams (eg, N-grams of words, such as individual words or phrases that include multiple words) received by the system. The system may send these N-grams to one or more voice extension services and in response receive additional N-grams identified by the voice extension service. The general language model may be influenced based on the N-grams contained in the context information and the N-grams received from the voice extension service. An automatic speech recognition device may use the affected general speech model to perform speech recognition. Influencing the language model in this manner can dynamically tune the model to favor words relevant to the current task or situation of the user, allowing the system to more accurately transcribe speech, especially words that are normal Language are unusual.
Ein Entwickler einer Anwendung, die so konfiguriert ist, dass sie auf Spracheingaben reagiert, kann dem System zum Beispiel eine Liste von Wörtern und/oder Ausdrücken bereitstellen, die der Anwendung zugeordnet sind. Die Liste kann Wörter beinhalten, die ein Benutzer wahrscheinlich ausspricht, wenn die Anwendung in einem bestimmten Zustand ist, zum Beispiel auf einem bestimmten Menübildschirm. Das System kann die Liste der Wörter als einen anfänglichen Satz von N-Grammen empfangen und es kann den anfänglichen Satz von N-Grammen an verschiedene Spracherweiterungsdienste senden. Die Spracherweiterungsdienste können andere N-Gramme basierend auf dem anfänglichen Satz von N-Grammen erzeugen, die das System als erweiterten Satz von N-Grammen sammeln kann. Das System kann ein allgemeines Sprachmodell basierend auf dem anfänglichen Satz von N-Grammen und/oder dem erweiterten Satz von N-Grammen beeinflussen. Das beeinflusste allgemeine Sprachmodell kann durch eine automatische Spracherkennungsvorrichtung verwendet werden, um eine Transkription einer Spracheingabe zu erzeugen, die von einem Benutzer bereitgestellt wird. For example, a developer of an application configured to respond to voice inputs may provide the system with a list of words and / or phrases associated with the application. The list may include words that a user is likely to pronounce when the application is in a particular state, for example, on a particular menu screen. The system may receive the list of words as an initial set of N-grams and may send the initial set of N-grams to various language extension services. The language extension services may generate other N-grams based on the initial set of N-grams that the system can collect as an extended set of N-grams. The system may affect a generic language model based on the initial set of N-grams and / or the extended set of N-grams. The affected general language model may be used by an automatic speech recognition device to generate a transcription of a speech input provided by a user.
Nachdem die Spracherkennungsvorrichtung das beeinflusste allgemeine Sprachmodell verwendet hat, um Spracherkennung bei einer Spracheingabe durchzuführen, können Präferenzen für Spracherkennungskandidaten, die durch die Spracherkennungsvorrichtung erzeugt wurden, auf Basis der identifizierten N-Gramme angepasst werden. Wahrscheinlichkeiten, Erkennungsbewertungen oder Wahrscheinlichkeitsmassen, die Spracherkennungskandidaten zugewiesen sind, die identifizierte N-Gramme beinhalten, können zum Beispiel angepasst werden, um eine Präferenz für diese Spracherkennungskandidaten zu erhöhen. Nach Durchführung dieser Anpassungen kann die Spracherkennungsvorrichtung eine Transkription für die Spracheingabe ermitteln, die eine oder mehrere Spracherkennungskandidaten beinhaltet. After the speech recognition device has used the affected general speech model to perform speech recognition in speech input, preferences for speech recognition candidates generated by the speech recognition device may be adjusted based on the identified N-grams. For example, probabilities, recognition scores, or probability weights assigned to speech recognition candidates that include identified N-grams may be adjusted to be a preference for them Increase speech recognition candidates. After making these adjustments, the speech recognition device may determine a transcription for the speech input that includes one or more speech recognition candidates.
In einigen Implementierungen kann das allgemeine Sprachmodell für den aktuellen Kontext des Benutzers beeinflusst werden, bevor eine Spracheingabe vom Benutzer empfangen wird. In anderen Implementierungen kann das System die Beeinflussung des allgemeinen Sprachmodells aktualisieren, wenn der Benutzer Spracheingabe bereitstellt, z. B. regelmäßig wenn die automatische Spracherkennungsvorrichtung Audiodaten verarbeitet, die der Spracheingabe entsprechen. Wenn zum Beispiel Wörter in der Spracheingabe erkannt werden, kann das System diese Wörter an die Spracherweiterungsdienste senden, um erweiterte Sätze von N-Grammen zu erhalten, die verwendet werden, um das allgemeine Sprachmodell zu beeinflussen. In some implementations, the general language model may be influenced for the current context of the user before a voice input is received from the user. In other implementations, the system may update the influence of the general language model when the user provides voice input, e.g. B. regularly when the automatic speech recognition device processes audio data corresponding to the speech input. For example, if words are recognized in the voice input, the system may send these words to the voice extension services to obtain extended sets of N-grams that are used to influence the general language model.
Die Beeinflussung eines allgemeinen Sprachmodells ermöglicht eine genauere und effizientere Spracherkennung, als über andere Verfahren der Sprachmodellbeeinflussung erreicht werden kann. Die Beeinflussung eines einzelnen, allgemeinen Sprachmodells kann zum Beispiel in Bezug auf Verarbeitungsanforderungen und Speicherplatz effizienter sein als das Interpolieren eines allgemeinen Sprachmodells mit einem oder mehreren kontextspezifischen Sprachmodellen. Die Beeinflussung eines allgemeinen Sprachmodells im Gegensatz zum Auswählen kontextspezifischer Sprachmodelle oder Interpolieren allgemeiner und kontextspezifischer Sprachmodelle ermöglicht es außerdem einer automatischen Spracherkennungsvorrichtung, Sprachmodelle zu verwenden, die mithilfe eines spezifischeren Kontexts beeinflusst sind und die häufiger beeinflusst werden, wenn sich ein Kontext entwickelt, wodurch die Genauigkeit der Spracherkennung verbessert wird. Außerdem können dieselben Informationen, die bei der Beeinflussung des allgemeinen Sprachmodells verwendet werden, verwendet werden, um bevorzugte Spracherkennungskandidaten zu identifizieren, die mithilfe des beeinflussten allgemeinen Sprachmodells erzeugt werden. Diese zweite Form der Beeinflussung der Spracherkennungskandidaten kann die Genauigkeit der Spracherkennung weiter verbessern. Influencing a common language model allows for more accurate and efficient speech recognition than can be achieved through other methods of language model manipulation. For example, influencing a single, general language model may be more efficient in terms of processing requirements and memory space than interpolating a common language model with one or more context-specific language models. Influencing a general language model as opposed to choosing context-specific language models or interpolating general and context-specific language models also allows an automatic speech recognition device to use language models that are influenced by a more specific context and are more likely to be affected as a context evolves, thereby increasing accuracy the speech recognition is improved. In addition, the same information used in influencing the generic language model can be used to identify preferred speech recognition candidates that are generated using the affected generic speech model. This second form of influencing the speech recognition candidates can further improve the accuracy of the speech recognition.
Innovative Aspekte der in dieser Spezifikation beschriebenen Gegenstands können in Verfahren verkörpert sein, die die Aktionen des Empfangens von Audiodaten, die einer Benutzeräußerung entsprechen, und Kontextdaten für die Benutzeräußerung, Identifizierens eines anfänglichen Satzes von einem oder mehreren N-Grammen aus den Kontextdaten, Erzeugens eines erweiterten Satzes von einem oder mehreren N-Grammen basierend mindestens auf dem anfänglichen Satz von N-Grammen, wobei der erweiterte Satz von N-Grammen eines oder mehrere N-Gramme umfasst, die sich von den N-Grammen im anfänglichen Satz von N-Grammen unterscheiden, Anpassens eines Sprachmodells basierend mindestens auf dem erweiterten Satz von N-Grammen, Ermittelns eines oder mehrerer Spracherkennungskandidaten für mindestens einen Teil der Benutzeräußerung mithilfe des angepassten Sprachmodells, wobei jeder Spracherkennungskandidat eines oder mehrere Wörter umfasst, nach dem Ermitteln des einen oder der mehreren Spracherkennungskandidaten, Anpassens eine Bewertung für einen bestimmten Spracherkennungskandidaten, von dem ermittelt wurde, dass er in den erweiterten Satz von N-Grammen einbezogen werden soll, nach Anpassen der Bewertung für den bestimmten Spracherkennungskandidaten, Ermittelns einer Transkription für die Benutzeräußerung, die mindestens einen der ein oder mehreren Spracherkennungskandidaten beinhaltet, und Bereitstellens der Transkription der Benutzeräußerung für die Ausgabe, beinhaltet. Innovative aspects of the subject matter described in this specification may be embodied in methods including the actions of receiving audio data corresponding to a user utterance and contextual data for user utterance, identifying an initial set of one or more N-grams from the context data, generating a extended set of one or more N-grams based at least on the initial set of N-grams, wherein the extended set of N-grams comprises one or more N-grams different from the N-grams in the initial set of N-grams discriminating, adapting a language model based at least on the extended set of N-grams, determining one or more speech recognition candidates for at least a portion of the user utterance using the customized language model, wherein each speech recognition candidate comprises one or more words after determining the one or more languages candidate, adjusting a score for a particular speech recognition candidate that is determined to be included in the extended set of N-grams, after adjusting the score for the particular speech recognition candidate, determining a transcription for the user utterance that includes at least one of the or multiple speech recognition candidates, and providing the transcription of the user utterance for the output.
Diese und andere Ausführungsformen können jeweils optional eines oder mehrere der folgenden Merkmale umfassen. In verschiedenen Beispielen umfasst das Anpassen des Sprachmodells basierend mindestens auf dem erweiterten Satz von N-Grammen das Anpassen von Erkennungsbewertungen für eines oder mehrere der N-Gramme des erweiterten Satzes von N-Grammen im Sprachmodell; das Anpassen des Sprachmodells basierend mindestens auf dem erweiterten Satz von N-Grammen umfasst das Anpassen von Wahrscheinlichkeitsmassen, die einem oder mehreren N-Grammen des erweiterten Satzes der N-Gramme im Sprachmodell zugewiesen sind. These and other embodiments may each optionally include one or more of the following features. In various examples, adapting the language model based on at least the extended set of N-grams includes adjusting recognition scores for one or more of the N-grams of the extended set of N-grams in the language model; adapting the language model based at least on the extended set of N-grams comprises adjusting probability weights assigned to one or more N-grams of the extended set of N-grams in the language model.
Ausführungsformen können außerdem jeweils eines oder mehrere der folgenden optionalen Merkmale beinhalten. In verschiedenen Beispielen beinhaltet das Anpassen des Sprachmodells basierend mindestens auf dem erweiterten Satz von N-Grammen das Hinzufügen von einem oder mehreren N-Grammen des erweiterten Satzes von N-Grammen zum Sprachmodell; das Sprachmodell beinhaltet einen oder mehrere Platzhalterübergänge, und das Hinzufügen des einen oder der mehreren N-Gramme des erweiterten Satzes von N-Grammen zum Sprachmodell umfasst das Zuweisen eines bestimmten N-Gramms des erweiterten Satzes von N-Grammen zu einem bestimmten Platzhalterübergang; das Zuweisen des bestimmten N-Gramms des erweiterten Satzes von N-Grammen zum bestimmten Platzhalterübergang umfasst das Anpassen einer Erkennungsbewertung für den bestimmten Platzhalterübergang, der dem bestimmten N-Gramm des erweiterten Satzes von N-Grammen zugewiesen ist. Embodiments may also each include one or more of the following optional features. In various examples, adapting the language model based on at least the extended set of N-grams includes adding one or more N-grams of the extended set of N-grams to the language model; the language model includes one or more wildcard transitions, and adding the one or more N-grams of the extended set of N-grams to the language model comprises assigning a particular N-gram of the extended set of N-grams to a particular wildcard transition; assigning the determined N-gram of the extended set of N-grams to the particular wildcard transition comprises adjusting a recognition score for the particular wildcard transition assigned to the particular N-gram of the extended set of N-grams.
Ausführungsformen können außerdem jeweils eines oder mehrere der folgenden optionalen Merkmale beinhalten. In einigen Beispielen umfasst das Erzeugen des erweiterten Satzes von N-Grammen basierend mindestens auf dem anfänglichen Satz von N-Grammen das Senden eines oder mehrerer des anfänglichen Satzes von N-Grammen an jeden des einen oder der mehreren Spracherweiterungsdienste, und das Empfangen, von dem einen oder den mehreren Spracherweiterungsdiensten und in Reaktion auf das Senden des einen oder der mehreren des anfänglichen Satzes von N-Grammen an jeden des einen oder der mehreren Spracherweiterungsdienste, des erweiterten Satzes von einem oder mehreren N-Grammem; eines oder mehrere des erweiterten Satzes von N-Grammen sind in einer Hash-Map enthalten, und das Anpassen der Bewertung für den bestimmten Spracherkennungskandidaten, von dem ermittelt wurde, dass er in den erweiterten Satz von N-Grammen einbezogen werden soll, umfasst das Ermitteln, dass der bestimmte Spracherkennungskandidat in der Hash-Map enthalten ist; die Audiodaten, die der Benutzeräußerung entsprechen, entsprechen einem Frame einer gesprochenen Benutzereingabe, die mehrere Frames umfasst; das Sprachmodell ist ein allgemeines Sprachmodell oder ein allgemeines Sprachmodell, das während der Verarbeitung eines vorhergehenden Frames der gesprochenen Benutzereingabe beeinflusst wurde. Embodiments may also each include one or more of the following optional features. In some examples, generating the extended set of N-grams based on at least the initial set of N-grams includes sending one or more of the initial set of N-grams to each of the one or more language extension services, and receiving, from one or more voice extension services and in response to sending the one or more of the initial set of N-grams to each of the one or more voice extension services, the extended set of one or more N-grams; one or more of the extended set of N-grams are included in a hash map, and adjusting the score for the particular speech recognition candidate that has been determined to be included in the extended set of N-grams comprises determining in that the particular speech recognition candidate is included in the hash map; the audio data corresponding to the user utterance corresponds to a frame of a spoken user input comprising a plurality of frames; the language model is a generic language model or general language model that has been affected during the processing of a previous frame of the spoken user input.
Ausführungsformen können außerdem jeweils eines oder mehrere der folgenden optionalen Merkmale beinhalten. In manchen Fällen umfassen die Kontextdaten eines oder mehrere Kontextwörter; das eine oder die mehreren Kontextwörter werden jeweils von einem Anwendungsentwickler einer Anwendung gesendet, die so konfiguriert ist, dass sie Operationen in Reaktion auf gesprochene Benutzereingaben durchführt; die Kontextdaten beinhalten eines oder mehrere Wörter, die in einer früheren Transkription einer Benutzeräußerung enthalten sind, nicht; das Sprachmodell wird basierend mindestens auf dem erweiterten Satz von N-Grammen angepasst, bevor die Audiodaten empfangen werden, die der Benutzeräußerung entsprechen; die Merkmale können das Anpassen des Sprachmodells basierend mindestens auf dem anfänglichen Satz von N-Grammen umfassen. Embodiments may also each include one or more of the following optional features. In some cases, the context data includes one or more contextual words; the one or more context words are each sent by an application developer of an application configured to perform operations in response to spoken user input; the context data does not include one or more words contained in an earlier transcription of a user utterance; the language model is adjusted based at least on the extended set of N-grams before receiving the audio data corresponding to the user's utterance; the features may include adapting the language model based at least on the initial set of N-grams.
Die Details von einer oder mehreren Ausführungsformen des in dieser Spezifikation beschriebenen Gegenstandes sind in den begleitenden Zeichnungen und der nachfolgenden Beschreibung dargelegt. Andere potenzielle Merkmale, Aspekte und Vorteile des Gegenstands werden aus der Beschreibung, den Zeichnungen und den Ansprüchen deutlich. The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will be apparent from the description, drawings, and claims.
KURZE BESCHREIBUNG DER ZEICHNUNGEN BRIEF DESCRIPTION OF THE DRAWINGS
Gleiche Bezugszeichen in den verschiedenen Zeichnungen verweisen auf gleiche Elemente. Like reference numerals in the various drawings refer to like elements.
AUSFÜHRLICHE BESCHREIBUNG DETAILED DESCRIPTION
In einigen Implementierungen kann das Transkriptionssystem
Das Benutzergerät
Das Transkriptionssystem
Die Spracherkennungsvorrichtung
Der Benutzer
Die Spracherkennungsvorrichtung
Die Spracherkennungsvorrichtung
Das Sprachmodell, auf das von der Sprachmodellbeeinflussung-Engine
Das allgemeine Sprachmodell
Der N-Gramm-Cache
In einigen Implementierungen können N-Grammen im N-Gramm-Cache
In einigen Implementierungen beinhaltet der Kontext Wörter, die auf einem Bildschirm des Benutzergeräts vorhanden sind. Der anfängliche Satz von N-Grammen kann aus Wörtern und Ausdrücken bestehen, die aus Text extrahiert wurden, der auf dem Bildschirm des Benutzers sichtbar ist. Als anderes Beispiel kann der anfängliche Satz von N-Grammen aus N-Grammen bestehen, die durch eine Anwendung bereitgestellt werden, die auf dem Benutzergerät ausgeführt wird. Die Anwendung kann zum Beispiel verschiedene Dialogzustände haben, z. B. verschiedene Ansicht, die verschiedenen Aufgaben oder verschiedenen Schritten in einer Aufgabe entspricht. Jeder Dialogzustand, der verschiedenen Schnittstellen oder Ansichten der Anwendung entsprechen kann, kann einen vorher bestimmten Satz von N-Grammen haben, die nicht auf dem Bildschirm angezeigt werden, aber durch den Anwendungsentwickler bestimmt wurden. Ein Satz von N-Grammen kann zum Beispiel als relevant für eine Ansicht für das Eintippen des Texts einer E-Mail-Nachricht bestimmt werden, ein zweiter Satz von N-Grammen kann für eine Ansicht bestimmt werden, die für das Auswählen eines Kontakts in derselben Anwendung verwendet wird, und so weiter. Ähnlich können Informationen, die den Kontext angeben, als eine Kennung bereitgestellt werden, z. B. eine Anwendungskennung und/oder eine Dialogzustandskennung, die das Spracherkennungssystem verwenden kann, um entsprechende N-Gramme aus einer Datenbank oder von einem anderen Serversystem nachzuschlagen. In some implementations, the context includes words that are present on a screen of the user device. The initial set of N-grams may consist of words and phrases extracted from text that is visible on the user's screen. As another example, the initial set of N-grams may consist of N-grams provided by an application running on the user device. The application may, for example, have various dialog states, e.g. For example, different views that correspond to different tasks or different steps in a task. Each dialog state that may correspond to different interfaces or views of the application may have a predetermined set of N-grams that are not displayed on the screen but determined by the application developer. For example, a set of N-grams may be determined to be relevant to a view for typing in the text of an e-mail message, a second set of N-grams may be designated for a view useful for selecting a contact therein Application is used, and so on. Similarly, information indicating the context may be provided as an identifier, e.g. For example, an application identifier and / or a dialog state identifier that the speech recognition system may use to look up corresponding N-grams from a database or from another server system.
In einigen Implementierungen kann die Sprachmodellbeeinflussung-Engine
Die Sprachmodellbeeinflussung-Engine
In anderen Implementierungen kann die Sprachmodellbeeinflussung-Engine
In anderen Implementierungen kann die Sprachmodellbeeinflussung-Engine
In einigen Implementierungen empfängt das Sprachmodellbeeinflussungssystem
Eine solche Beeinflussung kann das Anpassen von Wahrscheinlichkeiten, Erkennungsbewertungen oder Wahrscheinlichkeitsmassen, die N-Grammen der anfänglichen oder erweiterten Sätze von N-Grammen im allgemeinen Sprachmodell
In einigen Implementierungen beeinflusst das Transkriptionssystem
Die Spracherkennungsvorrichtung
Nach Erzeugen der Spracherkennungskandidaten kann die Spracherkennungsvorrichtung
Die Spracherkennungsvorrichtung
Zusätzlich oder alternativ kann die Sprachmodellbeeinflussung-Engine
Nach weiterer Beeinflussung des allgemeinen Sprachmodells
Die Spracherkennungsvorrichtung
Beim Auswählen eines bestimmten Spracherkennungskandidaten kann die Sprachmodellbeeinflussung-Engine
Die Spracherkennungsvorrichtung
In einigen Implementierungen führt, jedes Mal, wenn die Sprachmodellbeeinflussung-Engine
In anderen Implementierungen beeinflusst die Sprachmodellbeeinflussung-Engine
In einigen Implementierungen beinhaltet der N-Gramm-Cache
Kurz, das in
Das Transkriptionssystem
Der Benutzer
Das Transkriptionssystem
In einigen Implementierungen kann die N-Gramm-Erweiterungsengine
Die N-Gramm-Erweiterungsengine
In einigen Implementierungen können die Spracherweiterungsdienste [1]–[N] einen Synonym-Spracherweiterungsdienst beinhalten. Der Synonym-Spracherweiterungsdienst kann N-Gramme identifizieren, die Synonyme der N-Gramme sind, die von der N-Gramm-Erweiterungsengine
In einigen Implementierungen können die Spracherweiterungsdienste [1]–[N] einen Spracherweiterungsdienst für ähnliche Suchergebnisse beinhalten. Der Spracherweiterungsdienst für ähnliche Suchergebnisse kann N-Gramme identifizieren, die, wenn sie als Suchabfrage, z. B. eine Internetsuche, bereitgestellt werden, ähnliche Suchergebnisse wie andere N-Gramme ergeben. Die N-Gramm-Erweiterungsengine
In einigen Implementierungen beinhalten die Spracherweiterungsdienste [1]–[N] einen Ontologie-Spracherweiterungsdienst, der N-Gramme identifiziert, die in derselben Kategorie oder Ontologie sind oder die sonst als mit anderen N-Grammen verbunden oder für sie relevant identifiziert werden. In Reaktion auf das Empfangen des N-Gramms „Italian“ kann der Ontologie-Spracherweiterungsdienst zum Beispiel auf eine oder mehrere Ontologien zugreifen, die das N-Gramm „Italian“ beinhalten, um zusätzliche N-Gramme zu identifizieren. Der Ontologie-Spracherweiterungsdienst kann zum Beispiel die N-Gramme „Tuscan“, „Roman“ und „Venetian“ aus einer ersten Ontologie, z. B. in Verbindung mit italienischen Stilen oder Beschreibungen, identifizieren und er kann die N-Gramme „French“, „Castilian“ und „Romanian“ in einer zweiten Ontologie, z. B. in Verbindung mit romanischen Sprachen oder Sprachen allgemein identifizieren. In some implementations, the voice extension services [1] - [N] include an ontology voice extension service that identifies N-grams that are in the same category or ontology or that are otherwise identified as being associated with or relevant to other N-grams. For example, in response to receiving the N-gram "Italian", the ontology language extension service may access one or more ontologies that include the N-gram "Italian" to identify additional N-grams. The ontology language extension service may, for example, use the N-grams "Tuscan", "Roman" and "Venetian" from a first ontology, e.g. In connection with Italian styles or descriptions, and he may use the N-grams "French", "Castilian" and "Romanian" in a second ontology, e.g. In general, for example, in connection with Romance languages or languages.
Ähnlich können die Spracherweiterungsdienste [1]–[N] einen Spracherweiterungsdienst für ähnliche Themen beinhalten. Der Spracherweiterungsdienst für ähnliche Themen kann N-Gramme identifizieren, die als mit demselben Thema wie ein bestimmtes N-Gramm verbunden identifiziert werden. Die N-Gramm-Erweiterungsengine
In einigen Implementierungen können die Spracherweiterungsdienste [1]–[N] einen Wortgruppierung-Spracherweiterungsdienst beinhalten, der N-Gramme identifiziert, die N-Grammen, die von der N-Gramm-Erweiterungsengine
Die Spracherweiterungsdienste [1]–[N] können ferner einen Spracherweiterungsdienst für gebeugte Formen beinhalten, der gebeugte Formen von N-Grammen erstellt, die durch die N-Gramm-Erweiterungsengine
Die N-Gramm-Erweiterungsengine
Die N-Gramm-Erweiterungsengine
Kurz, das allgemeine Sprachmodell
Vor Empfangen einer Spracheingabe oder nach Empfangen der Spracheingabe und bei jeder Wortentscheidung (z. B. Wortgrenze) können N-Gramme oder Wörter einem oder mehreren Platzhalterübergängen zugewiesen werden. Im Allgemeinen werden Wörter aus dem Cache oder erweiterten N-Gramm-Satz den Platzhalterrändern nur zugewiesen, wenn das Wort nicht bereits als potenzielle Wahl für das nächste Wort im Modell vorhanden ist. Wahrscheinlichkeiten, Erkennungsbewertungen oder Wahrscheinlichkeitsmassen für diese Platzhalterränder oder andere Ränder im allgemeinen Sprachmodell
In einigen Implementierungen kann jedes anfängliche Wort aus dem Cache in einen getrennten Platzhalterrand für jede Wortentscheidung ausgefüllt werden. Wenn der Cache zum Beispiel 10 N-Gramme beinhaltet, kann das erste Wort jedes der 10 N-Gramme als ein potenzielles nächstes Wort ausgefüllt werden. Nach Durchführen dieser Wortentscheidung können dieselben 10 Wörter als potenzielle Wörter für die nächste Entscheidung ausgefüllt werden, und so weiter. Selbstverständlich kann, wenn sich der Cache ändert, der Satz von Wörtern, die in die Platzhalter ausgefüllt werden, sich entsprechend ändern. In some implementations, each initial word from the cache may be populated into a separate wildcard for each word decision. For example, if the cache includes 10 N-grams, the first word of each of the 10 N-grams may be populated as a potential next word. After making this word decision, the same 10 words can be filled in as potential words for the next decision, and so on. Of course, as the cache changes, the set of words that are filled in the placeholders may change accordingly.
In einigen Implementierungen kann jedwedes Wort im Cache in einen Platzhalter als potenzielles neues Wort ausgefüllt werden. Es können zum Beispiel 10 N-Gramme im Cache sein, aber da einige der N-Gramme 2, 3 oder mehr Wörter beinhalten können, kann es insgesamt 75 verschiedene Wörter im Cache geben. Als Folge kann das System jedes der 75 Wörter in Platzhalter als potenzielle nächste Wörter ausfüllen, unabhängig von der Position der Wörter innerhalb der N-Gramme. Diese 75 Wörter, oder welche Wörter auch immer sich im Cache befinden, können für mehrere Wortentscheidungen verwendet werden, wobei sie in Platzhalterränder für jede Entscheidung ausgefüllt werden. In some implementations, any word in the cache may be populated into a wildcard as a potential new word. For example, there may be 10 N-grams in the cache, but since some of the N-grams may contain 2, 3 or more words, there may be a total of 75 different words in the cache. As a result, the system can fill any of the 75 words in wildcards as potential next words, regardless of the position of the words within the N-grams. These 75 words, or whatever words are in the cache, can be used for multiple word decisions, filling in wildcard margins for each decision.
In einigen Implementierungen werden Wörter im N-Gramm-Cache in Platzhalter selektiv basierend auf den identifizierten Ähnlichkeiten mit einem oder mehreren Kandidaten oder erkannten Wörtern der Äußerung des Benutzers ausgefüllt. Statt zum Beispiel jedes Wort oder jedes einzelne anfängliche Wort aus dem Cache als potenzielles neue Wort hinzuzufügen, kann das System eine Teilmenge identifizieren, die zu einem Sprachmuster passt, das in der Äußerung identifiziert wurde. Einer der Spracherkennungskandidaten für einen Teil der Äußerung kann zum Beispiel das Wort „Italian“ sein. Das System kann ermitteln, ob der Cache dieses Wort beinhaltet, und, falls ja, was die folgenden Wörter in den N-Grammen im Cache sind. Es kann zum Beispiel ermittelt werden, dass der N-Gramm-Cache „Italian music“, „eat Italian food“ und „Italian style“ beinhaltet, während andere N-Gramme im Cache das Wort „Italian“ nicht beinhalten. Von der Analyse kann das System „music“, „food“ und „style“ als potenzielle nächste Wörter zum Ausfüllen in die Platzhalterränder identifizieren, da diese Wörter darstellen, die „Italian“ in den N-Grammen im Cache unmittelbar folgen. Somit kann ein spezifischer Satz von Wörtern, die als relevant ermittelt werden, in die Platzhalter ausgefüllt werden, ohne andere Wörter aus dem Cache einzubeziehen. In some implementations, words in the N-gram cache are wildcarded selectively based on the identified similarities with one or more candidates or recognized words of the user's utterance. For example, rather than adding each word or each individual initial word from the cache as a potential new word, the system may identify a subset that matches a speech pattern identified in the utterance. One of the speech recognition candidates for a part of the utterance may be, for example, the word "Italian". The system can determine if the cache contains this word and, if so, what the following words in the N-grams are in the cache. For example, it can be determined that the N-gram cache includes Italian music, eat Italian food, and Italian style, while other N-grams in the cache do not include the word Italian. From the analysis, the system can identify "music", "food" and "style" as potential next words to fill in the wildcard margins, as these represent words that immediately follow "Italian" in the N-grams in the cache. Thus, a specific set of words that are determined to be relevant can be filled in the placeholders without including other words from the cache.
In einigen Implementierungen kann ein komplettes N-Gramm mehrerer Wörter oder ein Ausdruck mehrerer Wörter in einem N-Gramm in einen Platzhalterrand ausgefüllt werden. Wenn zum Beispiel ein N-Gramm „Italian music“ im Cache vorkommt, kann dieses komplette N-Gramm in einen Platzhalter als potenzieller nächster Ausdruck ausgefüllt werden. In some implementations, a complete N-gram of multiple words or an expression of multiple words in an N-gram may be filled in a wildcard. For example, if an N-gram "Italian music" is cached, this complete N-gram may be filled in a wildcard as a potential next expression.
Wie oben angemerkt, kann das Ausfüllen von Wörtern oder N-Grammen in Platzhalterübergangsränder vom Zuweisen einer Nicht-Null-Wahrscheinlichkeitsbewertung für jeden ausgefüllten Rand verbunden sein. Das System verwendet diese Sprachmodellbewertung sowie eine akustische Modellbewertung, um anzugeben, wie gut das Wort mit dem Klang des Teils der Äußerung übereinstimmt, die beurteilt wird, um einen Satz von Kandidatenwörtern für die Wortentscheidung auszuwählen. Außerdem können Präferenzen für bestimmte Spracherkennungskandidaten basierend darauf, ob sie bestimmte N-Gramme oder Wörter beinhalten, z. B. N-Gramme, die in einem N-Gramm-Cache enthalte sind, angepasst werden. Wenn zum Beispiel einer der Kandidaten mit einem Wort im N-Gramm-Cache übereinstimmt, ganz gleich, ob der Kandidat von einem Platzhalterrand ermittelt wurde oder ein Standardmerkmal des Sprachmodells war, kann die Wahrscheinlichkeitsbewertung für den Kandidaten in Reaktion auf das Identifizieren der Übereinstimmung mit einem Wort oder N-Gramm im N-Gramm-Cache gesteigert werden. As noted above, filling in words or N-grams into wildcard transition edges may be associated with assigning a non-zero probability score for each filled edge. The system uses this language model rating, as well as an acoustic model score, to indicate how well the word matches the sound of the portion of the utterance that is judged to select a set of candidate words for the word decision. Additionally, preferences for particular speech recognition candidates may be based on whether they include certain N-grams or words, e.g. B. N-grams, which are contained in an N-gram cache, adapted. For example, if one of the candidates matches a word in the N-gram cache, whether the candidate was from a wildcard edge or was a standard feature of the language model, the candidate's probabilistic score may be evaluated in response to identifying the match Word or N-grams in the N-gram cache.
Diese Zuweisung von Wörtern oder N-Grammen zu Platzhalterrändern kann vorübergehend, zum Beispiel nur die Entscheidung über ein bestimmtes Wort einer Äußerung, sein. Nachdem Kandidaten für das Wort ermittelt wurden oder eine Entscheidung für das Wort getroffen wurde, werden die Platzhalterzuweisungen entfernt und die Bewertungen für die Platzhalter werden auf null gesetzt. Auf diese Weise kann jede Wortentscheidung mit ihrem eigenen Satz von Platzhalterzuweisungen getroffen werden und diese Zuweisungen können andere Wortentscheidungen in der Äußerung, andere Äußerungen oder die Erkennung von Äußerungen anderer Benutzer nicht beeinflussen. This assignment of words or N-grams to placeholder margins may be temporary, for example, only the decision about a particular word of utterance. After candidates for the word have been identified or a decision has been made for the word, the wildcard assignments are removed and the wildcard ratings are set to zero. In this way, each word decision may be made with its own set of wildcard assignments, and these assignments may not affect other word decisions in the utterance, other utterances, or the recognition of utterances of other users.
Wie in
Im allgemeinen Sprachmodell entspricht der Rand
Der Rand
Die Beeinflussung des allgemeinen Sprachmodells
In der beeinflussten Version
Für die Ränder
Außerdem kann, auf Basis dieser Ermittlung, die Sprachmodellbeeinflussung-Engine
Das System empfängt Audiodaten, die einer Benutzeräußerung entsprechen, und Kontextdaten für die Benutzeräußerung (
Das System identifiziert einen anfänglichen Satz von einem oder mehreren N-Grammen für die Kontextdaten (
Das System erzeugt einen erweiterten Satz von einem oder mehreren N-Grammen basierend mindestens auf dem anfänglichen Satz von N-Grammen, wobei der erweiterte Satz von N-Grammen eines oder mehrere N-Gramme umfasst, die sich von den N-Grammen im anfänglichen Satz von N-Grammen unterscheiden (
Das System passt ein Sprachmodell basierend mindestens auf dem erweiterten Satz von N-Grammen an (
Das System ermittelt einen oder mehrere Spracherkennungskandidaten für mindestens einen Teil der Benutzeräußerung mithilfe des angepassten Sprachmodells, wobei jeder Spracherkennungskandidat eines oder mehrere Wörter umfasst (
Nach dem Ermitteln des einen oder der mehreren Spracherkennungskandidaten passt das System eine Bewertung für einen bestimmten Spracherkennungskandidaten an, von dem ermittelt wurde, dass er in den erweiterten Satz der N-Gramme einbezogen werden soll (
Nach Anpassen der Bewertungen für den bestimmten Spracherkennungskandidaten ermittelt das System eine Transkription für die Benutzeräußerung, die mindestens einen der ein oder mehreren Spracherkennungskandidaten beinhaltet (
Das System stellt die Transkription der Benutzeräußerung für die Ausgabe bereit (
Es wurde eine Reihe von Implementierungen beschrieben. Trotzdem versteht es sich, dass verschiedene Modifikationen durchgeführt werden können, ohne vom Sinn und Umfang der Offenbarung abzuweichen. Zum Beispiel können verschiedene Formen der vorstehend dargestellten Abläufe verwendet und Schritte neu geordnet, hinzugefügt oder entfernt werden. Dementsprechend liegen andere Implementierungen im Geltungsbereich der folgenden Ansprüche. A number of implementations have been described. Nevertheless, it will be understood that various modifications can be made without departing from the spirit and scope of the disclosure. For example, various forms of the processes outlined above may be used and steps rearranged, added or removed. Accordingly, other implementations are within the scope of the following claims.
In Fällen, in denen die Systeme und/oder Verfahren, die hier erörtert werden, persönliche Informationen über Benutzer sammeln oder persönliche Informationen nutzen können, kann für die Benutzer eine Möglichkeit der Kontrolle bereitgestellt werden, ob Programme oder Funktionen persönliche Informationen sammeln, z. B. Informationen über das soziale Netzwerk eines Benutzers, soziale Handlungen oder Aktivitäten, Beruf, Präferenzen oder den derzeitigen Standort, oder um zu kontrollieren, ob und/oder wie das System und/oder die Verfahren für den Benutzer relevantere Operationen ausführen können. Zusätzlich können gewisse Daten auf einem oder mehreren Wegen anonymisiert werden, bevor sie gespeichert oder verwendet werden, so dass personenbezogene Informationen entfernt werden. Eine Benutzeridentität kann beispielsweise so anonymisiert werden, dass keine persönlichen identifizierbaren Informationen für den Benutzer bestimmt werden können, oder ein Standort des Benutzers kann verallgemeinert werden, wobei Standortinformationen entnommen werden, wie beispielsweise eine Stadt, Postleitzahl oder ein Bundesland, sodass ein bestimmter Standort eines Benutzers nicht festgestellt werden kann. So hat der Benutzer Kontrolle darüber, wie Informationen über ihn oder eingeholt und verwendet werden. In cases where the systems and / or methods discussed herein may collect personal information about users or use personal information, users may be provided with a means of controlling whether programs or features collect personal information, e.g. Information about a user's social network, social actions or activities, occupation, preferences or current location, or to control whether and / or how the system and / or methods can perform more relevant operations for the user. In addition, certain data may be anonymized in one or more ways before being stored or used so that personal information is removed. For example, a user identity may be anonymized such that no personally identifiable information can be determined for the user, or a location of the user may be generalized, taking location information such as a city, zip code, or state, such as a particular user's location can not be determined. So the user has control over how information about him or her is obtained and used.
Während die vorgenannten Ausführungsformen hauptsächlich im Hinblick auf die Entwicklung von Spracheingaben zur Nutzung mit auf Benutzergeräten installierten Anwendungen beschrieben wurden, können die beschriebenen Funktionen auch im Zusammenhang mit Maschinen, anderen Geräten, Robotern oder anderen Systemen genutzt werden. Die beschriebenen Systeme und Verfahren können zum Beispiel verwendet werden, um Interaktionen mit Maschinen zu verbessern, wobei die Maschinen ein zugeordnete Computersystem besitzen, sie können genutzt werden, um Sprachaktionen zur Interaktion mit Robotern oder Systemen mit Roboter-Komponenten zu entwickeln und zu implementieren, sie können genutzt werden, um Sprachaktionen zur Interaktion mit Haushaltsgeräten, Entertainment-Systemen oder anderen Geräten zu entwickeln und zu implementieren, oder sie können genutzt werden, um Sprachaktionen zur Interaktion mit einem Fahrzeug oder einem anderen Transportsystem zu entwickeln und zu implementieren. While the foregoing embodiments have been described primarily with regard to the development of voice inputs for use with applications installed on user devices, the described functions may also be used in connection with machines, other devices, robots, or other systems. For example, the systems and methods described may be used to enhance interactions with machines, the machines having an associated computer system, they may be used to develop and implement language actions for interacting with robots or systems having robotic components can be used to develop and implement voice actions to interact with home appliances, entertainment systems, or other devices, or they can be used to develop and implement voice actions for interacting with a vehicle or other transport system.
Ausführungsformen und alle funktionsfähigen in dieser Beschreibung beschriebenen Operationen können in einer digitalen elektronischen Schaltung, in physisch greifbarer Computer-Software oder Firmware, in Computer-Hardware, darunter auch in aus dieser Beschreibung hervorgehenden Strukturen und deren strukturellen Äquivalenten oder in Kombinationen einer oder mehrerer derselben implementiert werden. Ausführungsformen können als eines oder mehrere Computerprogrammprodukte, d. h. als eines oder mehrere Module von Computerprogrammanweisungen, die auf einem computerlesbaren Medium für die Ausführung durch oder für die Kontrolle der Operation der datenverarbeitenden Einrichtung verschlüsselt sind, implementiert werden. Das computerlesbare Medium kann ein maschinenlesbares Speichergerät, ein maschinenlesbares Speichersubstrat, ein Speichergerät, eine Materialzusammensetzung, die ein maschinenlesbares propagiertes Signal bewirkt, oder eine Kombination aus einem oder mehreren derselben sein. Der Begriff „Datenverarbeitungsvorrichtung“ umfasst alle Vorrichtungen, Geräte und Maschinen zum Verarbeiten von Daten, einschließlich beispielsweise eines programmierbaren Prozessors, eines Computers oder mehrerer Prozessoren oder Computer. Die Vorrichtung kann zusätzlich zur Hardware Code umfassen, der eine Ausführungsumgebung für das betreffende Computerprogramm, wie z. B. Code, erzeugt, der Prozessorfirmware, einen Protokollstapel, ein Datenbankmanagementsystem, ein Betriebssystem oder eine Kombination aus einem oder mehreren derselben bildet. Ein propagiertes Signal ist ein künstlich erzeugtes Signal, wie z. B. ein maschinengeneriertes elektrisches, optisches oder elektromagnetisches Signal, das erzeugt wird, um Informationen für die Übermittlung an eine geeignete Empfangsvorrichtung zu codieren. Embodiments and all operational operations described in this specification may be implemented in a digital electronic circuit, in tangible computer software or firmware, in computer hardware, including structures resulting from this description and their structural equivalents, or in combinations of any one or more thereof become. Embodiments may be embodied as one or more computer program products, i. H. as one or more modules of computer program instructions encrypted on a computer readable medium for execution by or for the control of the operation of the data processing device. The computer-readable medium may be a machine-readable storage device, a machine-readable storage substrate, a storage device, a material composition that effects a machine-readable propagated signal, or a combination of one or more of them. The term "data processing device" includes all devices, devices and machines for processing data including, for example, a programmable processor, a computer or multiple processors or computers. The device may include, in addition to the hardware code, an execution environment for the particular computer program, such as a computer program. Code that generates processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, such. As a machine-generated electrical, optical or electromagnetic signal which is generated to encode information for transmission to a suitable receiving device.
Ein Computerprogramm (auch bekannt als ein Programm, Software, Softwareanwendung, Skript oder Code) kann in irgendeiner Form von Programmiersprache, einschließlich kompilierter Sprachen oder Interpretersprachen geschrieben sein, und es kann in irgendeiner Form eingesetzt werden, einschließlich als ein Einzelprogramm oder als ein Modul, eine Komponente, ein Unterprogramm oder eine andere Einheit, die zur Verwendung in einer Computerumgebung geeignet ist. Ein Computerprogramm entspricht nicht unbedingt einer Datei in einem Dateisystem. Ein Programm kann in einem Teil einer Datei, die andere Programme oder Daten (z. B. ein oder mehrere Scripts, die in einem Dokument in Markup-Sprache gespeichert sind) enthält, in einer einzelnen Datei speziell für das betreffende Programm oder in mehreren koordinierten Dateien (z. B. Dateien, die ein oder mehrere Module, Unterprogramme oder Teile von Code speichern) gespeichert sein. Ein Computerprogramm kann derart eingesetzt werden, dass es auf einem Computer oder auf mehreren Computern ausgeführt wird, die sich an einem Standort oder verteilt über mehrere Standorte befinden und miteinander durch ein Kommunikationsnetzwerk verbunden sind. A computer program (also known as a program, software, software application, script or code) may be written in any form of programming language, including compiled languages or interpreter languages, and may be used in any form, including as a single program or as a module. a component, subroutine, or other device suitable for use in a computer environment. A computer program does not necessarily correspond to a file in a file system. A program may be in a portion of a file containing other programs or data (eg, one or more scripts stored in a markup language document) in a single file specific to that program or in multiple coordinated ones Files (such as files that store one or more modules, subprograms, or pieces of code). A computer program may be deployed to run on a computer or on multiple computers located at one site or distributed over multiple sites and interconnected by a communications network.
Die in dieser Beschreibung beschriebenen Prozesse und Logikabläufe können durch einen oder mehrere programmierbare Prozessoren ausgeführt werden, die ein oder mehrere Computerprogramme ausführen, um Funktionen durch Verarbeiten von Eingabedaten und Erzeugen von Ausgaben auszuführen. Die Prozesse und Logikabläufe können auch als, Spezial-Logikschaltungen, wie z. B. ein FPGA (feldprogrammierbarer Universalschaltkreis) oder eine ASIC (anwendungsspezifische integrierte Schaltung), implementiert werden, zudem können auch Vorrichtungen als diese implementiert werden. The processes and logic operations described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by processing input data and generating outputs. The processes and logic operations may also be referred to as special logic circuits, such as logic circuits. For example, an FPGA (Field Programmable Universal Circuit) or an ASIC (Application Specific Integrated Circuit) may be implemented, and devices may also be implemented as such.
Prozessoren, die für die Ausführung eines Computerprogramms geeignet sind, beinhalten beispielsweise sowohl Universal- als auch Spezialmikroprozessoren, sowie einen oder mehrere Prozessoren einer beliebigen Art von digitalem Computer. Im Allgemeinen empfängt ein Prozessor Befehle und Daten von einem Festwertspeicher oder einem wahlfreien Zugriffsspeicher oder von beiden. Processors suitable for executing a computer program include, for example, both general purpose and specialty microprocessors, as well as one or more processors of any type of digital computer. In general, a processor receives instructions and data from a read-only memory or random access memory, or both.
Die wesentlichen Elemente eines Computers sind ein Prozessor für das Ausführen von Anweisungen und ein oder mehrere Speichergeräte für das Speichern von Anweisungen und Daten. Im Allgemeinen gehören zu einem Computer außerdem ein oder mehrere Massenspeichergeräte zum Speichern von Daten, wie z. B. magnetische, magneto-optische oder optische Datenträger, um Daten zu empfangen und/oder zu senden, und/oder ein Computer ist operativ an dasselbe/dieselben Speichergerät(e) gekoppelt. Ein Computer muss jedoch nicht über diese Geräte verfügen. Des Weiteren kann ein Computer in einem anderen Gerät, wie z. B. einem Tablet-Computer, einem Mobiltelefon, einem persönlichen digitalen Assistenten (PDA), einem mobilen Audioplayer, einem globalen Positionsbestimmungssystem-(GPS)-Empfänger integriert sein, um nur einige Beispiele zu nennen. Computerlesbare Medien, die zum Speichern von Computerprogrammanweisungen und Daten geeignet sind, beinhalten alle Formen von nicht flüchtigem Speicher, Medien und Speichergeräten, einschließlich beispielsweise Halbleiterspeichergeräte, wie z. B. EPROM, EEPROM und Flash-Speichergeräte, magnetische Datenträger, wie z. B. eingebaute Festplattenlaufwerken oder Wechselplatten, magneto-optische Platten sowie CD-ROMs und DVD-ROMs. Der Prozessor und der Speicher können durch Spezial-Logikschaltungen ergänzt werden oder darin integriert sein. The essential elements of a computer are a processor for executing instructions and one or more storage devices for storing instructions and data. In general, a computer also includes one or more mass storage devices for storing data, such as data storage. Magnetic, magneto-optical or optical media to receive and / or transmit data, and / or a computer is operatively coupled to the same storage device (s). However, a computer does not need to have these devices. Furthermore, a computer in another device, such. A tablet computer, a mobile phone, a personal digital assistant (PDA), a mobile audio player, a global positioning system (GPS) receiver, to name just a few examples. Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and storage devices, including, for example, semiconductor memory devices such as memory cards. As EPROM, EEPROM and flash memory devices, magnetic media such. Built-in hard disk drives or removable disks, magneto-optical disks and CD-ROMs and DVD-ROMs. The processor and memory may be supplemented or integrated with special purpose logic circuits.
Um eine Interaktion mit einem Benutzer bereitzustellen, können Ausführungsformen auf einem Computer mit einer Anzeigevorrichtung, wie z. B. ein CTR-(Kathodenstrahlröhren-) oder LCD-(Flüssigkristallanzeige-)Monitor zum Anzeigen von Informationen für den Benutzer und einer Tastatur und einem Zeigegerät, z. B. einer Maus oder einem Trackball, über die der Nutzer Eingaben in den Computer bereitstellen kann, erfolgen. Es können auch andere Arten von Geräten verwendet werden, um eine Interaktion mit einem Benutzer bereitzustellen; beispielsweise kann eine an den Benutzer bereitgestellte Rückmeldung in einer beliebigen Form von sensorischer Rückmeldung, wie z. B. einer visuellen Rückmeldung, akustischen Rückmeldung oder taktilen Rückmeldung, vorliegen; zudem kann eine Eingabe vom Benutzer in beliebiger Form, darunter auch als akustische, taktile oder Spracheingabe empfangen werden. In order to provide interaction with a user, embodiments may be performed on a computer having a display device, such as a display device. A CTR (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and pointing device, e.g. As a mouse or a trackball, via which the user can provide input to the computer, take place. Other types of devices may also be used to provide interaction with a user; For example, a feedback provided to the user may be in any form of sensory feedback, such as, e.g. A visual feedback, audible feedback, or tactile feedback; In addition, an input can be received by the user in any form, including as acoustic, tactile or voice input.
Ausführungsformen können in einem Rechensystem implementiert werden, das Folgendes beinhaltet: eine Backendkomponente, z. B. als Datenserver; eine Middlewarekomponente, z. B. einen Applikationsserver; eine Frontendkomponente, z. B. einen Clientcomputer mit grafischer Benutzeroberfläche oder einen Webbrowser, mit dem ein Benutzer mit einer Implementierung oder einer beliebigen Kombination einer oder mehrerer dieser Backend-, Middleware- oder Frontendkomponenten interagieren kann. Die Komponenten des Systems können durch eine beliebige Form oder ein beliebiges Medium digitaler Datenkommunikation, wie z. B. ein Kommunikationsnetzwerk, miteinander verbunden sein. So beinhalten beispielsweise Kommunikationsnetzwerke ein lokales Netzwerk („LAN“) und ein Großraumnetzwerk („WAN“), wie z. B. das Internet. Embodiments may be implemented in a computing system including: a backend component, e.g. As a data server; a middleware component, e.g. B. an application server; a front end component, e.g. A client computer with a graphical user interface or a web browser that allows a user to interact with an implementation or any combination of one or more of these backend, middleware, or frontend components. The components of the system may be replaced by any form or medium of digital data communication, such as digital data communication. As a communication network, be connected to each other. For example, communication networks include a local area network ("LAN") and a wide area network ("WAN"), such as the Internet. For example, the Internet.
Das Computersystem kann Clients und Server beinhalten. Ein Client und ein Server befinden sich im Allgemeinen entfernt voneinander und interagieren typischerweise über ein Kommunikationsnetzwerk. Die Beziehung zwischen Client und Server entsteht aufgrund von Computerprogrammen, die auf den jeweiligen Computern ausgeführt werden und die eine Client-Server-Beziehung zueinander aufweisen. The computer system may include clients and servers. A client and a server are generally remote and typically interact over a communications network. The relationship between client and server arises because of computer programs running on the respective computers and having a client-server relationship with each other.
Während diese Beschreibung viele Details enthält, sollten diese nicht als Begrenzungen bezüglich des Umfangs der Offenbarung oder dessen ausgelegt werden, was ggf. beansprucht ist, sondern vielmehr als Beschreibungen von Merkmalen, die für bestimmte Ausführungsformen spezifisch sind. Bestimmte Merkmale, die in dieser Beschreibung im Kontext von separaten Ausführungsformen beschrieben sind, können auch in Kombination in einer einzelnen Ausführungsform implementiert werden. Im umgekehrten Fall können verschiedene Merkmale, die im Kontext einer einzelnen Ausführungsform beschrieben sind, auch in mehreren Ausführungsformen separat oder in jeder geeigneten Unterkombination implementiert werden. Obwohl Merkmale vorstehend als in bestimmten Kombinationen agierend und sogar anfänglich als solche beansprucht sein können, können des Weiteren ein oder mehrere Merkmale einer beanspruchten Kombination in einigen Fällen aus der Kombination herausgelöst und die beanspruchte Kombination auf eine Teilkombination oder Variation einer Teilkombination gerichtet sein. While this description contains many details, these should not be construed as limitations on the scope of the disclosure or of what may be claimed, but rather as descriptions of features specific to particular embodiments. Certain features described in this specification in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features described in the context of a single embodiment may also be implemented separately or in any suitable subcombination in several embodiments. Although features may be listed above as acting in certain combinations and even initially as such, in some instances one or more features of a claimed combination may be released from the combination and the claimed combination directed to a partial combination or variation of a partial combination.
Gleichermaßen soll, obwohl die Operationen in den Zeichnungen in einer bestimmten Reihenfolge dargestellt sind, dies nicht so verstanden werden, dass die besagten Operationen in der dargestellten Reihenfolge oder in fortlaufender Reihenfolge durchgeführt werden müssen bzw. alle veranschaulichten Operationen durchgeführt werden müssen, um die erwünschten Ergebnisse zu erzielen. Unter bestimmten Umständen können Multitasking und Parallelverarbeitung von Vorteil sein. Darüber hinaus sollte die Trennung verschiedener Systemkomponenten in den vorstehend beschriebenen Ausführungsformen als nicht in allen Ausführungsformen erforderlich aufgefasst werden, zudem versteht sich, dass die beschriebenen Programmkomponenten und Systeme im Allgemeinen zusammen in ein einziges Softwareprodukt integriert oder in mehrere Softwareprodukte aufgeteilt sein können. Likewise, although the operations in the drawings are illustrated in a particular order, they are not to be understood as having to perform said operations in the illustrated order or in sequential order, or all illustrated operations must be performed to achieve the desired results to achieve. Under certain circumstances, multitasking and parallel processing can be beneficial. Moreover, the separation of various system components in the embodiments described above should not be construed as required in all embodiments, and it should be understood that the described program components and systems generally can be integrated together into a single software product or split into multiple software products.
In allen Fällen, in denen eine HTML-Datei erwähnt ist, können stattdessen andere Dateitypen oder Formate verwendet werden. Zum Beispiel kann eine HTML-Datei durch XML, JSON, Klartext oder andere Arten von Dateien ersetzt werden. Des Weiteren können an Stellen, an denen eine Tabelle oder Hashtabelle angegeben wird, andere Datenstrukturen (wie z. B. Tabellenkalkulationen, relationale Datenbanken oder strukturierte Dateien) verwendet werden. In all cases where an HTML file is mentioned, other file types or formats can be used instead. For example, an HTML file can be replaced by XML, JSON, plain text or other types of files. In addition, other data structures (such as spreadsheets, relational databases, or structured files) can be used at locations where a table or hash table is specified.
Somit wurden bestimmte Ausführungsformen beschrieben. Weitere Ausführungsformen liegen innerhalb des Schutzumfangs der folgenden Ansprüche. Die in den Ansprüchen angeführten Aktionen können beispielsweise in einer anderen Reihenfolge ausgeführt werden und dennoch gewünschte Ergebnisse erzielen. Thus, certain embodiments have been described. Other embodiments are within the scope of the following claims. For example, the actions listed in the claims may be executed in a different order and still achieve desired results.
Claims (26)
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/432,620 | 2017-02-14 | ||
US15/432,620 US10311860B2 (en) | 2017-02-14 | 2017-02-14 | Language model biasing system |
Publications (1)
Publication Number | Publication Date |
---|---|
DE202017106616U1 true DE202017106616U1 (en) | 2018-01-18 |
Family
ID=60263034
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE202017106616.9U Active DE202017106616U1 (en) | 2017-02-14 | 2017-10-31 | Language model influencing system |
Country Status (5)
Country | Link |
---|---|
US (4) | US10311860B2 (en) |
EP (3) | EP3559943B1 (en) |
CN (2) | CN110291582B (en) |
DE (1) | DE202017106616U1 (en) |
WO (1) | WO2018151768A1 (en) |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10311860B2 (en) | 2017-02-14 | 2019-06-04 | Google Llc | Language model biasing system |
US10623246B1 (en) * | 2018-03-27 | 2020-04-14 | Amazon Technologies, Inc. | Device configuration by natural language processing system |
US10679610B2 (en) * | 2018-07-16 | 2020-06-09 | Microsoft Technology Licensing, Llc | Eyes-off training for automatic speech recognition |
US11527234B2 (en) | 2019-10-01 | 2022-12-13 | Rovi Guides, Inc. | Method and apparatus for generating hint words for automated speech recognition |
US11610588B1 (en) * | 2019-10-28 | 2023-03-21 | Meta Platforms, Inc. | Generating contextually relevant text transcripts of voice recordings within a message thread |
US20220382973A1 (en) * | 2021-05-28 | 2022-12-01 | Microsoft Technology Licensing, Llc | Word Prediction Using Alternative N-gram Contexts |
US20230087611A1 (en) * | 2021-09-22 | 2023-03-23 | Apple Inc. | Highlighting reading based on adaptive prediction |
US11790678B1 (en) * | 2022-03-30 | 2023-10-17 | Cometgaze Limited | Method for identifying entity data in a data set |
US20240021195A1 (en) * | 2022-07-14 | 2024-01-18 | Snap Inc. | Boosting words in automated speech recognition |
CN115132209B (en) * | 2022-09-01 | 2022-11-08 | 北京百度网讯科技有限公司 | Speech recognition method, apparatus, device and medium |
Family Cites Families (205)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4820059A (en) | 1985-10-30 | 1989-04-11 | Central Institute For The Deaf | Speech processing apparatus and methods |
US5477451A (en) | 1991-07-25 | 1995-12-19 | International Business Machines Corp. | Method and system for natural language translation |
US5267345A (en) | 1992-02-10 | 1993-11-30 | International Business Machines Corporation | Speech recognition apparatus which predicts word classes from context and words from word classes |
DE69326431T2 (en) | 1992-12-28 | 2000-02-03 | Toshiba Kawasaki Kk | Voice recognition interface system that can be used as a window system and voice mail system |
TW323364B (en) | 1993-11-24 | 1997-12-21 | At & T Corp | |
US5638487A (en) | 1994-12-30 | 1997-06-10 | Purespeech, Inc. | Automatic speech recognition |
US5715367A (en) | 1995-01-23 | 1998-02-03 | Dragon Systems, Inc. | Apparatuses and methods for developing and using models for speech recognition |
US5710866A (en) * | 1995-05-26 | 1998-01-20 | Microsoft Corporation | System and method for speech recognition using dynamically adjusted confidence measure |
DE19533541C1 (en) | 1995-09-11 | 1997-03-27 | Daimler Benz Aerospace Ag | Method for the automatic control of one or more devices by voice commands or by voice dialog in real time and device for executing the method |
US6173261B1 (en) | 1998-09-30 | 2001-01-09 | At&T Corp | Grammar fragment acquisition using syntactic and semantic clustering |
EP0954797A1 (en) | 1995-12-08 | 1999-11-10 | Bell Communications Research, Inc. | Method and system for placing advertisements in a computer network |
US6397180B1 (en) | 1996-05-22 | 2002-05-28 | Qwest Communications International Inc. | Method and system for performing speech recognition based on best-word scoring of repeated speech attempts |
US6021403A (en) | 1996-07-19 | 2000-02-01 | Microsoft Corporation | Intelligent user assistance facility |
US5822730A (en) | 1996-08-22 | 1998-10-13 | Dragon Systems, Inc. | Lexical tree pre-filtering in speech recognition |
US5819220A (en) * | 1996-09-30 | 1998-10-06 | Hewlett-Packard Company | Web triggered word set boosting for speech interfaces to the world wide web |
US6167377A (en) | 1997-03-28 | 2000-12-26 | Dragon Systems, Inc. | Speech recognition language models |
US6119186A (en) | 1997-05-30 | 2000-09-12 | Texas Instruments Incorporated | Computer system with environmental manager for detecting and responding to changing environmental conditions |
US6182038B1 (en) | 1997-12-01 | 2001-01-30 | Motorola, Inc. | Context dependent phoneme networks for encoding speech information |
US6317712B1 (en) | 1998-02-03 | 2001-11-13 | Texas Instruments Incorporated | Method of phonetic modeling using acoustic decision tree |
US6418431B1 (en) | 1998-03-30 | 2002-07-09 | Microsoft Corporation | Information retrieval and speech recognition based on language models |
US6574597B1 (en) | 1998-05-08 | 2003-06-03 | At&T Corp. | Fully expanded context-dependent networks for speech recognition |
US7881936B2 (en) | 1998-12-04 | 2011-02-01 | Tegic Communications, Inc. | Multimodal disambiguation of speech recognition |
US8938688B2 (en) | 1998-12-04 | 2015-01-20 | Nuance Communications, Inc. | Contextual prediction of user words and user actions |
US6922669B2 (en) | 1998-12-29 | 2005-07-26 | Koninklijke Philips Electronics N.V. | Knowledge-based strategies applied to N-best lists in automatic speech recognition systems |
US7058573B1 (en) | 1999-04-20 | 2006-06-06 | Nuance Communications Inc. | Speech recognition system to selectively utilize different speech recognition techniques over multiple speech recognition passes |
US6928615B1 (en) | 1999-07-07 | 2005-08-09 | Netzero, Inc. | Independent internet client object with ad display capabilities |
US6912499B1 (en) | 1999-08-31 | 2005-06-28 | Nortel Networks Limited | Method and apparatus for training a multilingual speech model set |
JP4292646B2 (en) | 1999-09-16 | 2009-07-08 | 株式会社デンソー | User interface device, navigation system, information processing device, and recording medium |
US6789231B1 (en) | 1999-10-05 | 2004-09-07 | Microsoft Corporation | Method and system for providing alternatives for text derived from stochastic input sources |
US6581033B1 (en) | 1999-10-19 | 2003-06-17 | Microsoft Corporation | System and method for correction of speech recognition mode errors |
US6778959B1 (en) | 1999-10-21 | 2004-08-17 | Sony Corporation | System and method for speech verification using out-of-vocabulary models |
US6446041B1 (en) | 1999-10-27 | 2002-09-03 | Microsoft Corporation | Method and system for providing audio playback of a multi-source document |
US20020111990A1 (en) | 1999-11-01 | 2002-08-15 | Wood Christopher Noah | Internet based message management system |
EP1226697B1 (en) | 1999-11-03 | 2010-09-22 | Wayport, Inc. | Distributed network communication system which enables multiple network providers to use a common distributed network infrastructure |
US7403888B1 (en) | 1999-11-05 | 2008-07-22 | Microsoft Corporation | Language input user interface |
ATE405918T1 (en) | 1999-12-20 | 2008-09-15 | British Telecomm | LEARNING DIALOGUE STATES AND LANGUAGE MODELS OF THE SPOKEN INFORMATION SYSTEM |
US20020111907A1 (en) | 2000-01-26 | 2002-08-15 | Ling Marvin T. | Systems and methods for conducting electronic commerce transactions requiring micropayment |
AU4869601A (en) | 2000-03-20 | 2001-10-03 | Robert J. Freeman | Natural-language processing system using a large corpus |
US6769010B1 (en) | 2000-05-11 | 2004-07-27 | Howzone.Com Inc. | Apparatus for distributing information over a network-based environment, method of distributing information to users, and method for associating content objects with a database wherein the content objects are accessible over a network communication medium by a user |
US6678415B1 (en) | 2000-05-12 | 2004-01-13 | Xerox Corporation | Document image decoding using an integrated stochastic language model |
US6539358B1 (en) | 2000-05-24 | 2003-03-25 | Delphi Technologies, Inc. | Voice-interactive docking station for a portable computing device |
US7149970B1 (en) | 2000-06-23 | 2006-12-12 | Microsoft Corporation | Method and system for filtering and selecting from a candidate list generated by a stochastic input method |
US7623648B1 (en) | 2004-12-01 | 2009-11-24 | Tellme Networks, Inc. | Method and system of generating reference variations for directory assistance data |
GB2366478B (en) | 2000-08-16 | 2005-02-09 | Roke Manor Research | Lan services delivery system |
US7599851B2 (en) | 2000-09-05 | 2009-10-06 | Renee Frengut | Method for providing customized user interface and targeted marketing forum |
US7457750B2 (en) | 2000-10-13 | 2008-11-25 | At&T Corp. | Systems and methods for dynamic re-configurable speech recognition |
US7043422B2 (en) | 2000-10-13 | 2006-05-09 | Microsoft Corporation | Method and apparatus for distribution-based language model adaptation |
US7219058B1 (en) | 2000-10-13 | 2007-05-15 | At&T Corp. | System and method for processing speech recognition results |
US6876966B1 (en) | 2000-10-16 | 2005-04-05 | Microsoft Corporation | Pattern recognition training method and apparatus using inserted noise followed by noise reduction |
ATE297588T1 (en) | 2000-11-14 | 2005-06-15 | Ibm | ADJUSTING PHONETIC CONTEXT TO IMPROVE SPEECH RECOGNITION |
ATE391986T1 (en) | 2000-11-23 | 2008-04-15 | Ibm | VOICE NAVIGATION IN WEB APPLICATIONS |
US6915262B2 (en) | 2000-11-30 | 2005-07-05 | Telesector Resources Group, Inc. | Methods and apparatus for performing speech recognition and using speech recognition results |
US20020087309A1 (en) | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Computer-implemented speech expectation-based probability method and system |
DE10100725C1 (en) | 2001-01-10 | 2002-01-24 | Philips Corp Intellectual Pty | Automatic dialogue system for speech interrogation of databank entries uses speech recognition system assisted by speech model obtained before beginning of dialogue |
US7027987B1 (en) | 2001-02-07 | 2006-04-11 | Google Inc. | Voice interface for a search engine |
US6754626B2 (en) | 2001-03-01 | 2004-06-22 | International Business Machines Corporation | Creating a hierarchical tree of language models for a dialog system based on prompt and dialog context |
US7072838B1 (en) | 2001-03-20 | 2006-07-04 | Nuance Communications, Inc. | Method and apparatus for improving human-machine dialogs using language models learned automatically from personalized data |
US7778816B2 (en) | 2001-04-24 | 2010-08-17 | Microsoft Corporation | Method and system for applying input mode bias |
US7587321B2 (en) * | 2001-05-08 | 2009-09-08 | Intel Corporation | Method, apparatus, and system for building context dependent models for a large vocabulary continuous speech recognition (LVCSR) system |
US6714778B2 (en) | 2001-05-15 | 2004-03-30 | Nokia Corporation | Context sensitive web services |
US20030008680A1 (en) | 2001-05-24 | 2003-01-09 | Huh Stephen S. | Using identification information obtained from a portable phone |
JP2003067630A (en) | 2001-08-28 | 2003-03-07 | Malibu Entertainment Co | Internet connection service with automatic advertisement function using radio lan |
US7526431B2 (en) | 2001-09-05 | 2009-04-28 | Voice Signal Technologies, Inc. | Speech recognition using ambiguous or phone key spelling and/or filtering |
US7225130B2 (en) | 2001-09-05 | 2007-05-29 | Voice Signal Technologies, Inc. | Methods, systems, and programming for performing speech recognition |
US6901364B2 (en) | 2001-09-13 | 2005-05-31 | Matsushita Electric Industrial Co., Ltd. | Focused language models for improved speech input of structured documents |
US6959276B2 (en) | 2001-09-27 | 2005-10-25 | Microsoft Corporation | Including the category of environmental noise when processing speech signals |
US7533020B2 (en) | 2001-09-28 | 2009-05-12 | Nuance Communications, Inc. | Method and apparatus for performing relational speech recognition |
JP2003150841A (en) | 2001-11-01 | 2003-05-23 | Internatl Business Mach Corp <Ibm> | Advertising method on public network, host server and computer program product |
US6950796B2 (en) | 2001-11-05 | 2005-09-27 | Motorola, Inc. | Speech recognition by dynamical noise model adaptation |
US6999931B2 (en) | 2002-02-01 | 2006-02-14 | Intel Corporation | Spoken dialog system using a best-fit language model and best-fit grammar |
US8010174B2 (en) | 2003-08-22 | 2011-08-30 | Dexcom, Inc. | Systems and methods for replacing signal artifacts in a glucose sensor data stream |
US6914975B2 (en) | 2002-02-21 | 2005-07-05 | Sbc Properties, L.P. | Interactive dialog-based training method |
US7143035B2 (en) | 2002-03-27 | 2006-11-28 | International Business Machines Corporation | Methods and apparatus for generating dialog state conditioned language models |
US7716161B2 (en) | 2002-09-24 | 2010-05-11 | Google, Inc, | Methods and apparatus for serving relevant advertisements |
US7720044B1 (en) | 2002-04-19 | 2010-05-18 | Nokia Corporation | System and method for terminal configuration |
US7174288B2 (en) | 2002-05-08 | 2007-02-06 | Microsoft Corporation | Multi-modal entry of ideogrammatic languages |
US7403890B2 (en) | 2002-05-13 | 2008-07-22 | Roushar Joseph C | Multi-dimensional method and apparatus for automated language interpretation |
US7224981B2 (en) | 2002-06-20 | 2007-05-29 | Intel Corporation | Speech recognition of mobile devices |
JP2004070884A (en) | 2002-08-09 | 2004-03-04 | Ntt Comware Corp | Advertisement distributing system |
US7570943B2 (en) | 2002-08-29 | 2009-08-04 | Nokia Corporation | System and method for providing context sensitive recommendations to digital services |
JP2004102470A (en) | 2002-09-06 | 2004-04-02 | Global Vision:Kk | Method for displaying bulletin board information, and system for displaying bulletin board information |
JP4109063B2 (en) | 2002-09-18 | 2008-06-25 | パイオニア株式会社 | Speech recognition apparatus and speech recognition method |
US7184957B2 (en) | 2002-09-25 | 2007-02-27 | Toyota Infotechnology Center Co., Ltd. | Multiple pass speech recognition method and system |
JP4352790B2 (en) | 2002-10-31 | 2009-10-28 | セイコーエプソン株式会社 | Acoustic model creation method, speech recognition device, and vehicle having speech recognition device |
US7149688B2 (en) | 2002-11-04 | 2006-12-12 | Speechworks International, Inc. | Multi-lingual speech recognition with cross-language context modeling |
US6993615B2 (en) | 2002-11-15 | 2006-01-31 | Microsoft Corporation | Portable computing device-integrated appliance |
US7457745B2 (en) | 2002-12-03 | 2008-11-25 | Hrl Laboratories, Llc | Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments |
JP2004185389A (en) | 2002-12-04 | 2004-07-02 | Ok Web Inc | Terminal, program and q and a system |
US7043239B2 (en) | 2002-12-09 | 2006-05-09 | Qualcomm Incorporated | Download and display of system tags in wireless communication systems |
WO2004053836A1 (en) | 2002-12-10 | 2004-06-24 | Kirusa, Inc. | Techniques for disambiguating speech input using multimodal interfaces |
US7373300B1 (en) | 2002-12-18 | 2008-05-13 | At&T Corp. | System and method of providing a spoken dialog interface to a website |
US7698136B1 (en) | 2003-01-28 | 2010-04-13 | Voxify, Inc. | Methods and apparatus for flexible speech recognition |
US7805299B2 (en) | 2004-03-01 | 2010-09-28 | Coifman Robert E | Method and apparatus for improving the transcription accuracy of speech recognition software |
CA2428821C (en) | 2003-05-15 | 2009-03-17 | Ibm Canada Limited - Ibm Canada Limitee | Accessing a platform independent input method editor from an underlying operating system |
US7200559B2 (en) * | 2003-05-29 | 2007-04-03 | Microsoft Corporation | Semantic object synchronous understanding implemented with speech application language tags |
US20040243415A1 (en) | 2003-06-02 | 2004-12-02 | International Business Machines Corporation | Architecture for a speech input method editor for handheld portable devices |
US7392188B2 (en) | 2003-07-31 | 2008-06-24 | Telefonaktiebolaget Lm Ericsson (Publ) | System and method enabling acoustic barge-in |
JP4548646B2 (en) | 2003-09-12 | 2010-09-22 | 株式会社エヌ・ティ・ティ・ドコモ | Noise model noise adaptation system, noise adaptation method, and speech recognition noise adaptation program |
US7693827B2 (en) | 2003-09-30 | 2010-04-06 | Google Inc. | Personalization of placed content ordering in search results |
US7634720B2 (en) | 2003-10-24 | 2009-12-15 | Microsoft Corporation | System and method for providing context to an input method |
FI20031566A (en) | 2003-10-27 | 2005-04-28 | Nokia Corp | Select a language for word recognition |
CA2486128C (en) | 2003-10-30 | 2011-08-23 | At&T Corp. | System and method for using meta-data dependent language modeling for automatic speech recognition |
CA2486125C (en) | 2003-10-30 | 2011-02-08 | At&T Corp. | A system and method of using meta-data in speech-processing |
US20050114474A1 (en) | 2003-11-20 | 2005-05-26 | International Business Machines Corporation | Automatic configuration of the network devices via connection to specific switch ports |
WO2005050621A2 (en) | 2003-11-21 | 2005-06-02 | Philips Intellectual Property & Standards Gmbh | Topic specific models for text formatting and speech recognition |
US7542907B2 (en) * | 2003-12-19 | 2009-06-02 | International Business Machines Corporation | Biasing a speech recognizer based on prompt context |
US7634095B2 (en) | 2004-02-23 | 2009-12-15 | General Motors Company | Dynamic tuning of hands-free algorithm for noise and driving conditions |
US7400878B2 (en) | 2004-02-26 | 2008-07-15 | Research In Motion Limited | Computing device with environment aware features |
US20050246325A1 (en) | 2004-04-30 | 2005-11-03 | Microsoft Corporation | Method and system for recording and accessing usage of an item in a computer system |
JP3923513B2 (en) | 2004-06-08 | 2007-06-06 | 松下電器産業株式会社 | Speech recognition apparatus and speech recognition method |
US7299181B2 (en) | 2004-06-30 | 2007-11-20 | Microsoft Corporation | Homonym processing in the context of voice-activated command systems |
US7562069B1 (en) | 2004-07-01 | 2009-07-14 | Aol Llc | Query disambiguation |
US20060009974A1 (en) | 2004-07-09 | 2006-01-12 | Matsushita Electric Industrial Co., Ltd. | Hands-free voice dialing for portable and remote devices |
US7580363B2 (en) | 2004-08-16 | 2009-08-25 | Nokia Corporation | Apparatus and method for facilitating contact selection in communication devices |
GB0420097D0 (en) | 2004-09-10 | 2004-10-13 | Cotares Ltd | Apparatus for and method of providing data to an external application |
US7698124B2 (en) | 2004-11-04 | 2010-04-13 | Microsoft Corporaiton | Machine translation system incorporating syntactic dependency treelets into a statistical framework |
US8498865B1 (en) | 2004-11-30 | 2013-07-30 | Vocera Communications, Inc. | Speech recognition system and method using group call statistics |
JP3955880B2 (en) | 2004-11-30 | 2007-08-08 | 松下電器産業株式会社 | Voice recognition device |
US7747437B2 (en) * | 2004-12-16 | 2010-06-29 | Nuance Communications, Inc. | N-best list rescoring in speech recognition |
US8009678B2 (en) | 2005-03-17 | 2011-08-30 | Microsoft Corporation | System and method for generating a dynamic prioritized contact list |
US7739286B2 (en) | 2005-03-17 | 2010-06-15 | University Of Southern California | Topic specific language models built from large numbers of documents |
US20070060114A1 (en) | 2005-09-14 | 2007-03-15 | Jorey Ramer | Predictive text completion for a mobile communication facility |
US7672833B2 (en) | 2005-09-22 | 2010-03-02 | Fair Isaac Corporation | Method and apparatus for automatic entity disambiguation |
EP1791114B1 (en) | 2005-11-25 | 2009-08-12 | Swisscom AG | A method for personalization of a service |
JP4961755B2 (en) | 2006-01-23 | 2012-06-27 | 富士ゼロックス株式会社 | Word alignment device, word alignment method, word alignment program |
JP5218052B2 (en) | 2006-06-26 | 2013-06-26 | 日本電気株式会社 | Language model generation system, language model generation method, and language model generation program |
US8001130B2 (en) | 2006-07-25 | 2011-08-16 | Microsoft Corporation | Web object retrieval based on a language model |
US8564544B2 (en) | 2006-09-06 | 2013-10-22 | Apple Inc. | Touch screen device, method, and graphical user interface for customizing display of content category icons |
US7907705B1 (en) | 2006-10-10 | 2011-03-15 | Intuit Inc. | Speech to text for assisted form completion |
US7890326B2 (en) | 2006-10-13 | 2011-02-15 | Google Inc. | Business listing search |
US8041568B2 (en) | 2006-10-13 | 2011-10-18 | Google Inc. | Business listing search |
US8073681B2 (en) | 2006-10-16 | 2011-12-06 | Voicebox Technologies, Inc. | System and method for a cooperative conversational voice user interface |
US9128926B2 (en) | 2006-10-26 | 2015-09-08 | Facebook, Inc. | Simultaneous translation of open domain lectures and speeches |
WO2008067562A2 (en) | 2006-11-30 | 2008-06-05 | Rao Ashwin P | Multimodal speech recognition system |
US20080131851A1 (en) | 2006-12-04 | 2008-06-05 | Dimitri Kanevsky | Context-sensitive language learning |
US7941189B2 (en) | 2007-02-07 | 2011-05-10 | Denso Corporation | Communicating road noise control system, in-vehicle road noise controller, and server |
US20100325109A1 (en) | 2007-02-09 | 2010-12-23 | Agency For Science, Technology And Rearch | Keyword classification and determination in language modelling |
US20110060587A1 (en) | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US20080221901A1 (en) | 2007-03-07 | 2008-09-11 | Joseph Cerra | Mobile general search environment speech processing facility |
US20090030687A1 (en) | 2007-03-07 | 2009-01-29 | Cerra Joseph P | Adapting an unstructured language model speech recognition system based on usage |
US8838457B2 (en) | 2007-03-07 | 2014-09-16 | Vlingo Corporation | Using results of unstructured language model based speech recognition to control a system-level function of a mobile communications facility |
US8060373B2 (en) | 2007-03-21 | 2011-11-15 | At&T Intellectual Property I, L.P. | System and method of identifying contact information |
US7877258B1 (en) * | 2007-03-29 | 2011-01-25 | Google Inc. | Representing n-gram language models for compact storage and fast retrieval |
US8396713B2 (en) | 2007-04-30 | 2013-03-12 | Nuance Communications, Inc. | Method and system for using a statistical language model and an action classifier in parallel with grammar for better handling of out-of-grammar utterances |
US8762143B2 (en) | 2007-05-29 | 2014-06-24 | At&T Intellectual Property Ii, L.P. | Method and apparatus for identifying acoustic background environments based on time and speed to enhance automatic speech recognition |
US7831427B2 (en) | 2007-06-20 | 2010-11-09 | Microsoft Corporation | Concept monitoring in spoken-word audio |
US7983902B2 (en) | 2007-08-23 | 2011-07-19 | Google Inc. | Domain dictionary creation by detection of new topic words using divergence value comparison |
US8321219B2 (en) | 2007-10-05 | 2012-11-27 | Sensory, Inc. | Systems and methods of performing speech recognition using gestures |
US8478787B2 (en) * | 2007-12-06 | 2013-07-02 | Google Inc. | Name detection |
US7953692B2 (en) | 2007-12-07 | 2011-05-31 | Microsoft Corporation | Predicting candidates using information sources |
US8423362B2 (en) | 2007-12-21 | 2013-04-16 | General Motors Llc | In-vehicle circumstantial speech recognition |
US8473276B2 (en) | 2008-02-19 | 2013-06-25 | Google Inc. | Universal language input |
US8121837B2 (en) | 2008-04-24 | 2012-02-21 | Nuance Communications, Inc. | Adjusting a speech engine for a mobile computing device based on background noise |
US8090738B2 (en) | 2008-05-14 | 2012-01-03 | Microsoft Corporation | Multi-modal search wildcards |
US8364481B2 (en) | 2008-07-02 | 2013-01-29 | Google Inc. | Speech recognition with parallel recognition tasks |
US8027973B2 (en) | 2008-08-04 | 2011-09-27 | Microsoft Corporation | Searching questions based on topic and focus |
CA2680304C (en) | 2008-09-25 | 2017-08-22 | Multimodal Technologies, Inc. | Decoding-time prediction of non-verbalized tokens |
US8407236B2 (en) | 2008-10-03 | 2013-03-26 | Microsoft Corp. | Mining new words from a query log for input method editors |
GB2477653B (en) | 2008-10-10 | 2012-11-14 | Nuance Communications Inc | Generating and processing forms for receiving speech data |
US9798720B2 (en) | 2008-10-24 | 2017-10-24 | Ebay Inc. | Hybrid machine translation |
US9043209B2 (en) | 2008-11-28 | 2015-05-26 | Nec Corporation | Language model creation device |
US8352321B2 (en) | 2008-12-12 | 2013-01-08 | Microsoft Corporation | In-text embedded advertising |
US8509398B2 (en) | 2009-04-02 | 2013-08-13 | Microsoft Corporation | Voice scratchpad |
US20100318531A1 (en) | 2009-06-10 | 2010-12-16 | Microsoft Corporation | Smoothing clickthrough data for web search ranking |
US9892730B2 (en) | 2009-07-01 | 2018-02-13 | Comcast Interactive Media, Llc | Generating topic-specific language models |
CN101604520A (en) * | 2009-07-16 | 2009-12-16 | 北京森博克智能科技有限公司 | Spoken language voice recognition method based on statistical model and syntax rule |
US8364612B2 (en) | 2009-09-15 | 2013-01-29 | Microsoft Corporation | Machine learning using relational databases |
US8255217B2 (en) | 2009-10-16 | 2012-08-28 | At&T Intellectual Property I, Lp | Systems and methods for creating and using geo-centric language models |
US8589163B2 (en) | 2009-12-04 | 2013-11-19 | At&T Intellectual Property I, L.P. | Adapting language models with a bit mask for a subset of related words |
EP2339576B1 (en) | 2009-12-23 | 2019-08-07 | Google LLC | Multi-modal input on an electronic device |
US20110162035A1 (en) | 2009-12-31 | 2011-06-30 | Apple Inc. | Location-based dock for a computing device |
US8996368B2 (en) | 2010-02-22 | 2015-03-31 | Nuance Communications, Inc. | Online maximum-likelihood mean and variance normalization for speech recognition |
US8265928B2 (en) | 2010-04-14 | 2012-09-11 | Google Inc. | Geotagged environmental audio for enhanced speech recognition accuracy |
US8694313B2 (en) | 2010-05-19 | 2014-04-08 | Google Inc. | Disambiguation of contact information using historical data |
US8468012B2 (en) | 2010-05-26 | 2013-06-18 | Google Inc. | Acoustic model adaptation using geographic information |
US20120016671A1 (en) * | 2010-07-15 | 2012-01-19 | Pawan Jaggi | Tool and method for enhanced human machine collaboration for rapid and accurate transcriptions |
US8700392B1 (en) | 2010-09-10 | 2014-04-15 | Amazon Technologies, Inc. | Speech-inclusive device interfaces |
US8606581B1 (en) | 2010-12-14 | 2013-12-10 | Nuance Communications, Inc. | Multi-pass speech recognition |
US8352245B1 (en) | 2010-12-30 | 2013-01-08 | Google Inc. | Adjusting language models |
US8296142B2 (en) * | 2011-01-21 | 2012-10-23 | Google Inc. | Speech recognition using dock context |
WO2013101051A1 (en) * | 2011-12-29 | 2013-07-04 | Intel Corporation | Speech recognition utilizing a dynamic set of grammar elements |
US8775177B1 (en) | 2012-03-08 | 2014-07-08 | Google Inc. | Speech recognition process |
US9043205B2 (en) | 2012-06-21 | 2015-05-26 | Google Inc. | Dynamic language model |
US10354650B2 (en) | 2012-06-26 | 2019-07-16 | Google Llc | Recognizing speech with mixed speech recognition models to generate transcriptions |
US9047868B1 (en) | 2012-07-31 | 2015-06-02 | Amazon Technologies, Inc. | Language model data collection |
US9117450B2 (en) | 2012-12-12 | 2015-08-25 | Nuance Communications, Inc. | Combining re-speaking, partial agent transcription and ASR for improved accuracy / human guided ASR |
US20140278349A1 (en) * | 2013-03-14 | 2014-09-18 | Microsoft Corporation | Language Model Dictionaries for Text Predictions |
US9153231B1 (en) * | 2013-03-15 | 2015-10-06 | Amazon Technologies, Inc. | Adaptive neural network speech recognition models |
WO2014197336A1 (en) * | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US9842592B2 (en) * | 2014-02-12 | 2017-12-12 | Google Inc. | Language models using non-linguistic context |
US9338493B2 (en) * | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9502032B2 (en) * | 2014-10-08 | 2016-11-22 | Google Inc. | Dynamically biasing language models |
US10134394B2 (en) * | 2015-03-20 | 2018-11-20 | Google Llc | Speech recognition using log-linear model |
US9460713B1 (en) * | 2015-03-30 | 2016-10-04 | Google Inc. | Language model biasing modulation |
US9691380B2 (en) * | 2015-06-15 | 2017-06-27 | Google Inc. | Negative n-gram biasing |
US9704483B2 (en) * | 2015-07-28 | 2017-07-11 | Google Inc. | Collaborative language model biasing |
US9576578B1 (en) * | 2015-08-12 | 2017-02-21 | Google Inc. | Contextual improvement of voice query recognition |
US10896681B2 (en) * | 2015-12-29 | 2021-01-19 | Google Llc | Speech recognition with selective use of dynamic language models |
US10055403B2 (en) * | 2016-02-05 | 2018-08-21 | Adobe Systems Incorporated | Rule-based dialog state tracking |
US9978367B2 (en) * | 2016-03-16 | 2018-05-22 | Google Llc | Determining dialog states for language models |
US20170345426A1 (en) * | 2016-05-31 | 2017-11-30 | Julia Komissarchik | System and methods for robust voice-based human-iot communication |
US10067938B2 (en) * | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US9691384B1 (en) * | 2016-08-19 | 2017-06-27 | Google Inc. | Voice action biasing system |
US10311860B2 (en) | 2017-02-14 | 2019-06-04 | Google Llc | Language model biasing system |
KR102281515B1 (en) * | 2019-07-23 | 2021-07-26 | 엘지전자 주식회사 | Artificial intelligence apparatus for recognizing speech of user using personalized language model and method for the same |
-
2017
- 2017-02-14 US US15/432,620 patent/US10311860B2/en active Active
- 2017-10-19 EP EP17794504.5A patent/EP3559943B1/en active Active
- 2017-10-19 CN CN201780086257.5A patent/CN110291582B/en active Active
- 2017-10-19 CN CN202310134122.9A patent/CN116229956A/en active Pending
- 2017-10-19 WO PCT/US2017/057369 patent/WO2018151768A1/en unknown
- 2017-10-19 EP EP23179079.1A patent/EP4235648A3/en active Pending
- 2017-10-19 EP EP23179646.7A patent/EP4235649A3/en active Pending
- 2017-10-31 DE DE202017106616.9U patent/DE202017106616U1/en active Active
-
2019
- 2019-05-21 US US16/417,714 patent/US11037551B2/en active Active
-
2021
- 2021-06-02 US US17/337,400 patent/US11682383B2/en active Active
-
2023
- 2023-05-16 US US18/318,495 patent/US20230290339A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20190341024A1 (en) | 2019-11-07 |
CN110291582B (en) | 2023-02-28 |
US10311860B2 (en) | 2019-06-04 |
EP3559943B1 (en) | 2023-07-19 |
WO2018151768A1 (en) | 2018-08-23 |
EP4235648A3 (en) | 2023-10-25 |
US11682383B2 (en) | 2023-06-20 |
US20210358479A1 (en) | 2021-11-18 |
EP4235649A2 (en) | 2023-08-30 |
US11037551B2 (en) | 2021-06-15 |
EP4235648A2 (en) | 2023-08-30 |
US20230290339A1 (en) | 2023-09-14 |
EP4235649A3 (en) | 2023-10-25 |
CN110291582A (en) | 2019-09-27 |
CN116229956A (en) | 2023-06-06 |
EP3559943A1 (en) | 2019-10-30 |
US20180233131A1 (en) | 2018-08-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
DE202017106616U1 (en) | Language model influencing system | |
US11874877B2 (en) | Using natural language processing for visual analysis of a data set | |
US20190171969A1 (en) | Method and system for generating natural language training data | |
US10847139B1 (en) | Crowd sourced based training for natural language interface systems | |
US9582608B2 (en) | Unified ranking with entropy-weighted information for phrase-based semantic auto-completion | |
DE102016226302B4 (en) | VOICE ACTION SYSTEM FOR DEVELOPERS | |
US20190272269A1 (en) | Method and system of classification in a natural language user interface | |
DE102016125508A1 (en) | Discovery system for voice actions | |
DE112016001852T5 (en) | Developers Language Action System | |
DE112019001533T5 (en) | EXTENSION OF TRAINING DATA FOR THE CLASSIFICATION OF NATURAL LANGUAGE | |
DE102017125001A1 (en) | Real-time streaming dialog management | |
DE112016005286T5 (en) | Simulated hyperlinks on a mobile device | |
DE112014002819T5 (en) | System and method for recognizing speech | |
US11790010B2 (en) | Inferring intent and utilizing context for natural language expressions in a data visualization user interface | |
CN112035506A (en) | Semantic recognition method and equipment | |
US20230094730A1 (en) | Model training method and method for human-machine interaction | |
DE112021003943T5 (en) | Voice response systems based on personalized vocabulary and user profiling - Personalized AI linguistic engines | |
EP3079083A1 (en) | Providing app store search results | |
US20170177561A1 (en) | Natural language interface for software customization | |
DE102017104094A1 (en) | LANGUAGE PROCESSING SYSTEM AND LANGUAGE PROCESSING METHOD | |
US20230032208A1 (en) | Augmenting data sets for machine learning models | |
DE112018002133T5 (en) | ACTIVITY CLASSIFICATION BASED ON THE SOUND EMISSION OF A USER INTERFACE | |
US11694033B2 (en) | Transparent iterative multi-concept semantic search | |
CN115023695A (en) | Updating training examples for artificial intelligence | |
US11868316B2 (en) | Event management device and method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
R082 | Change of representative |
Representative=s name: VENNER SHIPLEY LLP, DERepresentative=s name: VENNER SHIPLEY LLP, GB |
|
R207 | Utility model specification | ||
R082 | Change of representative |
Representative=s name: VENNER SHIPLEY GERMANY LLP, DERepresentative=s name: VENNER SHIPLEY LLP, DE |
|
R150 | Utility model maintained after payment of first maintenance fee after three years | ||
R151 | Utility model maintained after payment of second maintenance fee after six years |