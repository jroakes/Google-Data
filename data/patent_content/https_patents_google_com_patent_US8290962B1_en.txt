BACKGROUND
A. Field of the Invention
Implementations consistent with the principles of the invention relate generally to similarity detection and, more particularly, to comparing sets of documents to find similarities or differences.
B. Description of Related Art
There are a number of situations in which it may be desirable to be able to determine whether documents, or sets of documents, are similar to one another. One particular instance of this situation occurs in software engineering. Consider the situation in which there are two bases of source program code (“codebases”) that each define different versions of the same program. Each codebase may include, for example, a number of source code files. In some situations, the number of files may number into the hundreds or thousands. It may be desirable to determine what the differences are between the two codebases.
One existing technique for monitoring differences between two sets of documents, such as two sets of files, is to keep an explicit history of changes in the files. That is, each time a file is modified, the changes are logged and stored. Keeping an explicit history of file differences, however, is not always possible or may simply have not been done when the files were being modified.
Another existing technique for monitoring differences between two sets of files is to compare the files to obtain a list of differences between the files. Software for comparing files is well known. Comparing two files to obtain a list of their differences, however, generally only works well if the filename and directory structure of the file sets are similar or if the number of files is small enough to manually examine.
When relatively large codebases are being examined without an explicit history of changes, however, existing techniques can be lacking in the ability to effectively determine the differences in the codebases.
SUMMARY
One aspect is directed to a computer-implemented method for comparing a first set of documents to a second set of documents. The method includes constructing a matrix relating pairs of documents from the first and second sets of documents to a number of lines that are present in documents in each of the pairs of documents. The method further includes calculating similarity scores for the pairs of documents based on the matrix and outputting the similarity scores.
Yet another aspect is directed to a method that includes receiving an identification of a first set of documents and receiving an identification of a second set of documents. The method further includes processing document segments that represent portions of the documents in the first and second sets to construct an index that relates each of the segments to the documents in which the segment occurs. The method still further includes calculating similarity scores for pairs of documents in the first and second sets and outputting the similarity scores.
BRIEF DESCRIPTION OF THE DRAWINGS
The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate an embodiment of the invention and, together with the description, explain the invention. In the drawings,
FIG. 1 is a diagram illustrating an example of the comparison of two sets of documents;
FIG. 2 is a diagram of an exemplary system in which systems and methods consistent with the principles of the invention may be implemented;
FIG. 3 is an exemplary diagram of the computing device shown in FIG. 2;
FIG. 4 is a flow chart illustrating exemplary operations performed by the comparison engine shown in FIG. 2 when comparing two sets of documents;
FIG. 5 is a flow chart illustrating one technique for removing identical files as performed in the flowchart of FIG. 4;
FIG. 6 is a diagram illustrating two exemplary file sets;
FIG. 7 is a diagram illustrating portions of indexes relating lines in files to the files in which the lines occur;
FIG. 8 is a diagram illustrating a portion of an exemplary composite index;
FIG. 9 is a diagram illustrating an exemplary portion of a term count matrix;
FIG. 10 is a diagram illustrating exemplary similarity scores calculated for pairs of documents; and
FIG. 11 is a diagram illustrating an exemplary sorted list of file pairs and their corresponding similarity scores.
DETAILED DESCRIPTION
The following detailed description of the invention refers to the accompanying drawings. The detailed description does not limit the invention.
Overview
Techniques consistent with aspects of the invention can compare two sets of documents and automatically determine documents within each set that are similar to one another. FIG. 1 is a diagram illustrating an example of the comparison of two sets of documents. A first document set and a second document set are input to a software tool that will be called a comparison engine herein.
In this example, each document set may include a number of files, possibly hundreds or thousands of files. The file names across the two sets may or may not be related to the contents of the files. That is, some of the files may contain similar data but have different names while some of the files may have the same names but include different data.
The comparison engine may receive the two sets of files and generate output data that assists users in analyzing the similarities between the two sets of files. The output data may describe, or be manipulated to describe, which files are similar, which files were deleted from the second set to the first set, and which files have been newly added to the second set.
In the exemplary implementations described herein, the document sets will be described as files in different versions of software codebases. However, one of ordinary skill in the art will recognize that techniques similar to those described herein could be applied to other applications in which it is desirable to compare different sets or versions of documents.
Exemplary System
FIG. 2 is an exemplary diagram of a system 200 in which systems and methods consistent with the principles of the invention may be implemented. System 200 may include computing devices 210 connected via a network 240. Network 240 may include a local area network (LAN), a wide area network (WAN), a telephone network, such as the Public Switched Telephone Network (PSTN), an intranet, the Internet, or a combination of networks. Computing devices 210 may include client computing devices, server computing devices, or other types of computing devices. Three computing devices 210 are illustrated as connected to network 240 for simplicity. In practice, there may be more or fewer computing devices connected to a network.
A computing device 210 may include a device, such as a personal computer, wireless telephone, a personal digital assistant (PDA), a lap top, or another type of computation or communication device, a thread or process running on one of these devices, and/or an object executable by one of these devices. Computing devices 210 may connect to network 240 via wired, wireless, or optical connections.
One of computing devices 210 is illustrated in FIG. 2 as including comparison engine 220. As mentioned, comparison engine 220 may be a software tool that assists users in comparing two sets of documents. The two sets of documents may be stored in a database 225 or in other storage devices or structures.
Exemplary Computing
Device Architecture
FIG. 3 is an exemplary diagram of computing device 210. Computing device 210 may include a bus 310, a processor 320, a main memory 330, a read only memory (ROM) 340, a storage device 350, an input device 360, an output device 370, and a communication interface 380. Bus 310 may include a path that permits communication among the components of computing device 210.
Processor 320 may include any type of conventional processor, microprocessor, or processing logic that may interpret and execute instructions. Main memory 330 may include a random access memory (RAM) or another type of dynamic storage device that may store information and instructions for execution by processor 320. ROM 340 may include a conventional ROM device or another type of static storage device that may store static information and instructions for use by processor 320. Storage device 350 may include a magnetic and/or optical recording medium and its corresponding drive.
Input device 360 may include a conventional mechanism that permits a user to input information to computing device 210, such as a keyboard, a mouse, a pen, voice recognition and/or biometric mechanisms, etc. Output device 370 may include a conventional mechanism that outputs information to the user, including a display, a printer, a speaker, etc. Communication interface 380 may include any transceiver-like mechanism that enables computing device 210 to communicate with other devices and/or systems. For example, communication interface 380 may include mechanisms for communicating with another device or system via a network, such as network 240.
Comparison engine 220 may be implemented in software and stored in a computer-readable medium, such as memory 330. A computer-readable medium may be defined as one or more physical or logical memory devices and/or carrier waves.
The software instructions defining comparison engine 220 may be read into memory 330 from another computer-readable medium, such as data storage device 350, or from another device via communication interface 380. The software instructions contained in memory 330 cause processor 320 to perform processes that will be described later. Alternatively, hardwired circuitry may be used in place of or in combination with software instructions to implement processes consistent with the present invention. Thus, implementations consistent with the principles of the invention are not limited to any specific combination of hardware circuitry and software.
Comparison Engine 220
FIG. 4 is a flow chart illustrating exemplary operations performed by comparison engine 220 in comparing two sets of documents. In general, comparison engine 220 may operate to solve the problem: given m documents (i.e., the number of documents in the first set) and n documents (i.e., the number of documents in the second set), generate m·n similarity scores that can be used to learn how the second set was changed relative to the first set. This change information can take the form of, for example, which documents are similar and what changes were made between similar documents, which documents were deleted from the second set to the first set, and which documents have been newly added to the second set.
In the exemplary implementations described herein, the two sets of documents will be described as two sets of files, such as two sets of files that define different versions of a program codebase. In other implementations, the two sets of documents may represent text documents other than software codebases. Also, it will be assumed that each file is treated as a series of discrete segments, such as lines, without ordering information between the lines. In the context of codebases, each line may correspond to a line of programming code.
Comparison engine 220 may begin a comparison operation by receiving the location of the sets of files that are to be compared (act 401). Each file set may, for example, be stored as a number of different files and subdirectories within a main directory. A user may input the two different main directories that contain the two file sets. Alternatively, the user may input a simple list of files for each of the two sets of files. In some implementations, the user may specify that the files in each set are to be separated into (exclusive) sets based on file types. For examples, all files with a “.cpp” extension could be in one set, all files with a “.h” extension could be in another set, and all files with a “.java” extension could be in yet another set. That is, a user can specificy a priori that a “.cpp” file will not be similar to a “.java” file. Comparison engine 220 may then be applied to each set independently.
As an optional preprocessing step, identical files within the two file sets may next be removed from the two file sets (act 402). FIG. 5 is a flow chart illustrating one technique for removing identical files between the two sets. A checksum may be calculated for each file to create a mapping between calculated checksum values and a list of files that correspond to a checksum (act 501). A checksum, as is well known in the art, can represent a file or other data structures as a fixed length value based on the sum of the contents of the file (such as the sum of the bytes in the file). In one implementation, the checksum may be calculated based on the complete contents of each file. In other implementations, the checksum may be calculated based on a modified or normalized version of each line of each file, such as, for computer source code, on a version of the file in which comments or other non-essential information is removed. Within each line, it is often desirable to simply remove all whitespace.
Identical files across the sets may then be noted by iterating over the mapping for multiple files corresponding to a checksum (act 502). Multiple files having the same checksum can be inferred as identical files.
Referring back to FIG. 4, filename information can optionally be used to infer similarity between files (act 403). For instance, filenames between the first file set and the second file set can be analyzed for similar files by, for example, using a filename-based text comparison or by hashing the filenames to create a mapping between hash values and files.
Files that were found to be identical in either of or both of acts 402 and 403 can be marked as identical and do not need to be considered as part of the file sets for the operations performed in acts 404 through 406. At this point, assume that there are m files remaining in the first set and n files remaining in the second set.
As a simplified example of two file sets, consider the two example sets of files illustrated in FIG. 6, labeled as SET1 (Document set A) and SET2 (Document set B). SET1 includes four files (files A0 through A3), each of which corresponds to a row 610. More generally, SET1 may include m rows 610 (where m is 4 in this example). Each row may include a number of lines. In this example, for simplicity and brevity, each line is represented by a letter. In particular, each letter represents a line in the file or a checksum value generated from the line. For example, file A0 in SET1 includes the lines: ‘a’, ‘b’, ‘c’, ‘d’, ‘e’, ‘f’, ‘g’, ‘h’, ‘i’, and ‘j’. SET2 similarly includes a number of rows 620 that represent a number of files (B0 through B4), such as n files, where n equals five in this example. Each file includes a number of exemplary lines represented as letters. File B1 in SET2 includes, for instance, the lines: ‘d’, ‘f’, ‘h’, ‘j’, and ‘l’. File B1 in SET2 includes a number of lines in common with file A0 in SET1 (i.e., lines ‘d’, ‘f’, ‘h’, and ‘j’).
Comparison engine 220 may construct an index for each file set (act 404). The index may be an inverted index that maps from lines to lists of documents in which the lines are present. FIG. 7 is a diagram illustrating portions of such indexes for the first set and the second set of files. More particularly, index 725 represents an inverted index in which each line in the documents of the first set is mapped to the files in the first set in which the line occurs. For example, line ‘a’ in the first set occurs in file A0 and line ‘b’ in the first set occurs in files A0 and A3. Similarly, index 730 represents an inverted index in which each line in the documents of the second set is mapped to the files in the second set in which the line occurs. In other words, indexes 725 and 730 may each include a number of rows that each relate a particular line or a succinct representation of the line, such as a checksum value, with the files in which the line occurs. In some implementations, two lines that are nearly but not exactly identical may be considered to be identical.
While creating indexes 725 and 730, comparison engine 220 may additionally record the number of lines in each file. These numbers may later be used when calculating similarity values (described in more detail below).
Indexes 725 and 730 may be intersected (act 405). The intersected index, which will be called a composite index herein, relates each line that occurs in both the first and second file sets to the files in which the lines occur. FIG. 8 is a diagram illustrating a portion of such a composite index 810. As can be seen from composite index 810, not all of the lines present in indexes 725 and 730 are present in composite index 810. Line ‘o’, for instance, which is not present in the first set of files, and line ‘c’, which is not present in the second set of files, are not present in composite index 810.
Composite index 810 may optionally be pruned (act 406). Pruning may be used to intelligently remove entries in composite index 810, which can simplify or help to speed up further processing performed on composite index 810. One of ordinary skill in the art will recognize that a number of possible pruning strategies could be used. Two of these possible pruning strategies will next be described in more detail.
A first possible pruning strategy may generally delete all entries out of the composite index except those where the line appears in exactly one document in each set. In other words, if a line appears in two or more documents in the first file set or the second file set, the line is deleted from composite index 810. If this pruning strategy was applied to composite index 810, for instance, the following lines would be deleted: ‘b’, ‘d’, ‘e’, ‘f’, ‘g’, ‘h’, ‘i’, ‘j’, ‘k’, ‘l’, ‘m’, ‘n’, and ‘q’. In other words, only lines ‘a’ and ‘t’ would remain.
This first pruning strategy, although potentially aggressive, can be relatively accurate in many cases. In the case of source code, this considers only lines of code that appear exactly once in each document set, which given the nature of source code, corresponds to the significant lines. That is, it could be said that these tend to be the lines that distinguish one file from another. From a computational complexity standpoint, this pruning technique can reduce computational complexity so that it varies linearly with the number of lines rather than quadratically.
A second, more conservative pruning technique, is based on the pruning of common or “junk” terms. Some lines may appear in a large number of files. In the C programming language, for example, lines such as “} else {” or “∥” may tend to appear in a large number of the files. Such lines can be deleted from composite index 810. In one implementation, for example, lines that occur in a certain threshold number or portion of the files may be deleted. Line ‘h’, for example, occurs in all four of the files from the first file set. Accordingly, this line may be a candidate for pruning from composite index 810. Because iterating over all pairs of files is quadratic in computational difficulty, pruning lines that appear in many documents can lead to a large overall speed increase in calculating file similarity with little or no negative effect on the quality of the results.
When pruning with either the first or second pruning techniques described above, the number of lines recorded for each file may be correspondingly adjusted. Thus, if a particular line is removed that is present in one set of files, the number of lines recorded for each file in the set may be decremented by one.
In generating indexes 725 and 730 and composite index 810, each file may be sequentially read-in from disk only once. This can be particularly advantageous with large file sets, as, when a checksum is used as a proxy for a line, only the currently read-in line needs to be stored in memory at any given time.
Based on composite index 810, comparison engine 220 constructs a mapping that relates pairs of files in the two file sets to the number of lines that the pairs of files have in common (act 407). The mapping may be represented by a matrix referred to as a term count matrix herein.
FIG. 9 is a diagram illustrating an exemplary portion of a term count matrix 910 that stores the mapping constructed in act 407. Each row in term count matrix 910 relates a file pair to the number of lines present in both files of the pair. The first entry in term count matrix 910, for instance, indicates that file A0 of SET1 and file B0 of SET2 include five lines in common (the lines ‘g’, ‘f’, ‘h’, ‘i’, and ‘j’). Term count matrix 910 may be constructed based on composite index 810 by going through each row (line) in composite index 810 and incrementing by one each appropriate file pair in term count matrix 910. For example, when line ‘b’ is processed, the number stored for file pair entries (A0,B2) and (A3,B2) will be incremented by one. If line ‘b’ is the first line encountered for file pair entries (A0, B2) or (A3, B2), this file pair entry may be initially placed in term count matrix 910 and set to a value of one. To save memory, file pairs with no lines in common will not be included in term count matrix 910. In other words, term count matrix may be a partial matrix of less than all the possible m·n file pairings.
In one possible implementation, when creating term count matrix 910, pairs of files in which the sizes of the files differ by a certain amount, such as by a certain number of lines or a predetermined percent, may be excluded from term count matrix 910. Depending on the situation, this option may be desirable, as a user may know a priori that files of significantly different sizes are not similar. One technique for excluding files based on size is to iterate through p·q file pairings and exclude those file pairings in term count matrix 910 that are determined to differ in size by the predetermined amount or more. Another possible technique, that can be more efficient in some situations, is based on generating a histogram of file sizes from each of SET1 and SET2 (or from subsets of SET1 and SET2). A file in a particular range in one of the histograms only needs to be compared to the files corresponding to the range in the second histogram. Accordingly, less than p·q file comparisons need to be performed.
Term count matrix 910 may next be used to generate a similarity score for each pair of files (i.e., each entry) in term count matrix 910 (act 408). In one implementation, the similarity score for each pair of files may be calculated as the maximum of: (1) the percent of lines of the first file of the pair that are present in the second file of the pair and (2) the percent of lines of the second file of the pair that are present in the first file of the pair. As an example of this, the similarity score for file pair (A0, B0) may be calculated as 0.5. This similarity is arrived at by dividing 5 (the number of lines in file A0 in SET1 that are in file B0 of SET2) into 10 (the number of lines in file A0 of SET1) to obtain 0.5 and by dividing 5 (the number of lines in file B0 of SET2 that are in file A0 of SET1) into 20 (the number of lines in file B0 of SET2) to obtain 0.25; and taking the larger of these two values (0.5). Other techniques for calculating a similarity score based on common lines could be used.
FIG. 10 is a diagram illustrating the term count matrix of FIG. 9, but additionally including a column 1011 containing a calculated similarity score for each entry in term count matrix 910. Each similarity score in column 1011 may be calculated using techniques such as those performed in act 408.
The file pairs may next be sorted by similarity score (act 408). File pairs that were previously determined to be identical in acts 402 and 403 may be added back into the file pairs that are being sorted and may be given a similarity score that indicates maximum similarity (e.g., a value of one in FIG. 10). To increasing sorting speed, file pairs below a predetermined similarity threshold may first be removed. Sorting by similarity score can be used to easily determine a relative relatedness of a pair of files. This can be done by manual or automatic examination of the similarity scores and files. FIG. 11 is a diagram illustrating an exemplary list of file pairs and their corresponding similarity scores. In FIG. 11, column 1115 represents the files in the first set, column 1120 represents the files in the second set, and column 1130 lists the sorted similarity score for each respective pair of files.
The sorted similarity scores can allow a user to quickly determine files that are particularly similar. From this information, a user can infer when one file was copied in its entirety to another, which may be important to the user. In the example of FIG. 11, for instance, visual inspection shows that there is a fairly hard similarity score cutoff around 0.8 as there are not any file pairs between 0.6 and 0.8. It is reasonable to infer that file pairs having a similarity score of 0.8 or greater are probably derived from one another. The user may then perform further file analysis, such as by manually “diffing” the file pairs determined to be similar to obtain more detailed information regarding how the file pairs are similar or by examining the common lines in each file pair. In other implementations, determining which file pairs are similar to one another based on the similarity scores can be done using more automated techniques, such as by pre-setting a similarity score threshold that classifies pairs of files as being similar.
Additional similar processing may be implemented in the determined pairs of potentially similar files. For example, the user may wish to calculate, for all file pairs that are above a similarity threshold, another similarity score based on a different similarity metric, such as an ‘edit distance.”
One of ordinary skill in the art will recognize that many of the specific techniques discussed above for determining similar pairs of documents are exemplary and that different techniques or modifications to these techniques are possible. Additionally, although the above discussion primarily relates to similarity of codebases, concepts consistent with those described may be applied to other applications, such as plagiarism detection or detection of duplicate code within the same codebase Instead of locating similar documents within two sets of files, similar files may be located within one set of documents. An example of this may be the situation in which similar web pages are to be located within a large set of web pages. Additionally, concepts similar to those discussed above may be applied to one set of documents or three or more sets of documents.
CONCLUSION
Comparison engine 220 may be used to compare sets of text documents in which the documents may generally represent any type of information. In the exemplary implementation discussed above, the sets of documents are files from two different codebases, such as two different versions of a program. The similarity information generated by comparison engine 220 may be used as a final comparison result or as a basis for additional comparisons. For example, a user may determine, based on the similarity scores generated by comparison engine 220, that two files are highly similar. The user may then compare the two files using well known file comparison software to display a marked-up version of the first file relative to the second file.
The foregoing description of exemplary embodiments of the invention provides illustration and description, but is not intended to be exhaustive or to limit the invention to the precise form disclosed. Modifications and variations are possible in light of the above teachings or may be acquired from practice of the invention.
Moreover, while a series of acts have been described with regard to FIGS. 4 and 5, the order of the acts may be varied in other implementations consistent with the invention. Moreover, non-dependent acts may be implemented in parallel.
It will also be apparent to one of ordinary skill in the art that aspects of the invention, as described above, may be implemented in many different forms of software, firmware, and hardware in the implementations illustrated in the figures. The actual software code or specialized control hardware used to implement aspects consistent with the principles of the invention is not limiting of the invention. Thus, the operation and behavior of the aspects of the invention were described without reference to the specific software code—it being understood that one of ordinary skill in the art would be able to design software and control hardware to implement the aspects based on the description herein.
Further, certain portions of the invention may be implemented as “logic” that performs one or more functions. This logic may include hardware, such as an application specific integrated circuit or a field programmable gate array, software, or a combination of hardware and software.
No element, act, or instruction used in the description of the invention should be construed as critical or essential to the invention unless explicitly described as such. Also, as used herein, the article “a” is intended to include one or more items. Where only one item is intended, the term “one” or similar language is used. Further, the phrase “based on” is intended to mean “based, at least in part, on” unless explicitly stated otherwise.