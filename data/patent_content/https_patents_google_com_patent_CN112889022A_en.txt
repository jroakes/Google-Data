CN112889022A - Dynamic adjustment of story time special effects based on contextual data - Google Patents
Dynamic adjustment of story time special effects based on contextual data Download PDFInfo
- Publication number
- CN112889022A CN112889022A CN201880098327.3A CN201880098327A CN112889022A CN 112889022 A CN112889022 A CN 112889022A CN 201880098327 A CN201880098327 A CN 201880098327A CN 112889022 A CN112889022 A CN 112889022A
- Authority
- CN
- China
- Prior art keywords
- data
- text source
- user
- effect
- physical effect
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/016—Input arrangements with force or tactile feedback as computer generated output to the user
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/165—Management of the audio stream, e.g. setting of volume, audio stream path
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/10—Speech classification or search using distance or distortion measures between unknown speech and reference templates
-
- H—ELECTRICITY
- H05—ELECTRIC TECHNIQUES NOT OTHERWISE PROVIDED FOR
- H05B—ELECTRIC HEATING; ELECTRIC LIGHT SOURCES NOT OTHERWISE PROVIDED FOR; CIRCUIT ARRANGEMENTS FOR ELECTRIC LIGHT SOURCES, IN GENERAL
- H05B47/00—Circuit arrangements for operating light sources in general, i.e. where the type of light source is not relevant
- H05B47/10—Controlling the light source
- H05B47/105—Controlling the light source in response to determined parameters
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
- G10L2015/025—Phonemes, fenemes or fenones being the recognition units
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/227—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of the speaker; Human-factor methodology
Abstract
The present disclosure provides techniques for enabling a computing device to provide context-sensitive special effects that complement text sources when they are read vocally. One example method includes receiving, by a processing device, audio data comprising spoken words of a user; analyzing context data associated with a user; determining a match between the audio data and data of the text source; and initiating a physical effect in response to determining the match, wherein the physical effect corresponds to the text source and is based on the contextual data.
Description
Technical Field
The present disclosure relates to the field of virtual assistants, and more particularly, to enhancing the ability of a virtual assistant to provide special effects while phonetically reading text sources.
Background
The capabilities and use of virtual assistants are expanding rapidly. Conventional virtual assistants include some form of computer human-machine interface that enables a human to interact with the virtual assistant and cause the virtual assistant to perform tasks or services. The virtual assistant will typically record and understand human speech and may respond by synthesizing a reply. The virtual assistant may be initiated in response to a touch or gesture based command, or may continuously analyze its environment to detect spoken commands. When a command is detected, the virtual assistant can respond to or perform one or more actions.
Disclosure of Invention
The following is a simplified summary of the disclosure in order to provide a basic understanding of some aspects of the disclosure. This summary is not an extensive overview of the disclosure. It is intended to neither identify key or critical elements of the disclosure nor delineate any scope of particular embodiments of the disclosure or any scope of the claims. Its sole purpose is to present some concepts of the disclosure in a simplified form as a prelude to the more detailed description that is presented later.
According to a first aspect of the disclosure, a method comprises: receiving, by a processing device, audio data comprising spoken words of a user; analyzing context data associated with a user; determining a match between the audio data and the text source data; and initiating a physical effect in response to determining the match, wherein the physical effect corresponds to the text source and is based on the contextual data.
Optionally, the physical effect corresponding to the text source may modify the user's environment. The physical effect may include at least one of an acoustic effect, an optical effect, and a haptic effect. The context data may comprise at least one of sound data, light data, time data, weather data, calendar data and user profile data. The context data may comprise sound data of the user environment. The physical effect may include an acoustic effect based on a volume of the sound data. The context data may comprise light data of the user's environment. The physical effect may comprise an optical effect that modifies the brightness of the light source based on the light data. The text source may include words. Initiating the physical effect may be in response to detecting that the audio data includes the word. The method may include selecting a physical effect based on words of the text source. The method may include updating the attribute of the physical effect based on the context data. Determining a match between the audio data and the data of the text source may include: the method includes detecting that the audio data includes words of the text source using the phonological data of the text source. The data of the text source may include phoneme data. The determination of a match may include calculating a phonological edit distance between the phonological data of the text source and the phonological data of the audio data. The context data may include user profile data indicating the age of the child. The physical effect may include an acoustic effect selected based on the age of the child.
According to a second aspect of the disclosure, a system comprises: a processing device configured to receive audio data comprising spoken words of a user; analyzing context data associated with a user; determining a match between the audio data and the text source data; and initiate a physical effect in response to determining the match, wherein the physical effect corresponds to the text source and is based on the context data.
The system may include a data store. The system may include a communication system for communicating over a network, such as a local area network and/or a wide area network. The system may be, may be included in, or may be configured to implement a virtual assistant. The system may be configured to implement the method of the first aspect.
According to a third aspect of the present disclosure, a computer program product is configured such that, when processed by a processing device, the computer program product causes the processing device to perform the method of the first aspect.
Individual features and/or combinations of features defined above or below with respect to any particular embodiment according to any aspect of the present disclosure may be used alone, individually, alone or in combination with any other defined feature, in any other aspect or embodiment. Furthermore, the present disclosure is intended to cover an apparatus configured to perform any of the features described herein with respect to a method and/or a method of using or producing, using or manufacturing any of the apparatus features described herein.
Drawings
The present disclosure is illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings.
FIG. 1 illustrates an example environment with one or more computing devices according to embodiments of the present disclosure.
FIG. 2 is a block diagram illustrating an example computing device having components and modules for comparing phonological data derived from user input and phonological data derived from a text source in accordance with embodiments of the present disclosure.
FIG. 3 is a block diagram illustrating an example computing device having components and modules for identifying locations in text sources based on a user's audio input in accordance with embodiments of the present invention.
FIG. 4 is a block diagram illustrating an example computing device having components and modules for providing physical effects to enhance one or more users' experiences, according to embodiments of the invention.
Fig. 5 is a flow chart illustrating an example of a method according to an embodiment of the present disclosure.
Fig. 6 is a flow chart illustrating an example of a method according to an embodiment of the present disclosure.
Fig. 7 is a flow chart illustrating an example of a method according to an embodiment of the present disclosure.
Fig. 8 is a flow chart illustrating an example of a method according to an embodiment of the present disclosure.
Fig. 9 is a block diagram illustrating another example of a computing device in accordance with an embodiment of the present disclosure.
The drawings may be better understood when viewed in conjunction with the following detailed description.
Detailed Description
Modern computing devices often provide features to detect and understand human speech. These features may be associated with a virtual assistant that may be accessed via a resource-constrained computing device, such as a smart speaker, mobile phone, smart watch, or other user device. The computing device may be associated with a microphone that can record human speech and may analyze the human speech using a combination of local and remote computing resources. Analyzing voice is typically a resource intensive operation, and a computing device may be configured to perform some processing local to the computing device, and with some processing performed remotely at a server or via a cloud service. Many virtual assistants use some form of remote speech recognition service that treats audio data as input and converts the audio data to text, which is returned to the computing device.
Modern computing devices have begun to employ traditional virtual assistant features to provide sound effects that complement the environment as the user speaks to read a book. For example, the computing device may provide a bark effect when the user vocally reads the word "bark". The sound effects are typically provided by the same entity that provides the text source, and may correspond directly to a portion of the text source. As a result, special effects may be the same independent of the user or environment and may not be optimized for the user's particular reading environment.
Aspects and embodiments of the present technology address the above and other deficiencies by enabling computing devices to provide a variety of special effects based on user environment. In one example, the techniques may enable a computing device to analyze context data of a user environment and select or customize special effects. A special effect may be a physical effect that alters the user's environment to include an acoustic effect (e.g., music, sound effect music), an optical effect (e.g., flashing lights, ambient light), a haptic effect (e.g., vibration, wind, temperature change), other effects, or a combination thereof. The techniques may involve receiving and analyzing context data associated with a user. The contextual data may relate to weather, lighting, time of day, user feedback, user profiles, other information, or combinations thereof. The techniques may select or modify a physical effect corresponding to the text source based on the context data. This may result in, for example, selecting or modifying volume, brightness, speed, pitch, or other attributes of the physical effect.
The systems and methods described herein include techniques to enhance the technical fields of virtual assistants and home automation. In particular, the techniques may enable a computing device to optimize an environment by using contextual data about a user and the environment to add, remove, or modify physical effects to enhance the listening experience of the user.
The techniques discussed below include a number of enhancements to computing devices with or without virtual assistant features. The enhancements may be used alone or together to optimize the computing device's ability to follow when reading text sources audibly and to provide special effects to supplement the listening user's environment. In one example, the environment may include a parent speaking a book to one or more children. In another example, the environment may include one or more users providing lectures, voice, or other performances to an audience. In either example, the technique can be used to enhance the environment with special effects based on an analysis of data associated with the text source. Special effects may be synchronized with specific portions of the text source, such as specific spoken words or flipping pages.
Fig. 1 illustrates an example environment 100 that includes a text source being read vocally and one or more devices that supplement the environment to enhance a user's listening experience, in accordance with one or more aspects of the present disclosure. The environment 100 may be a physical environment such as an indoor setting (e.g., bedroom, meeting room), an outdoor setting (park, field), or other location. The environment 100 may be referred to as a pervasive computing environment or pervasive computing environment, and may include embedded computing functionality. Embedded computing functionality may provide environmental intelligence that is sensitive to and responsive to the presence of humans. In one example, environment 100 may include one or more users 110A and 110B, text source 120, one or more computing devices 130A and 130B, and one or more physical effects devices 140A-C.
The users 110A and 110B may include human users that are able to perceive the content of the text source. The user 110A may be an individual user who is reading the content of the text sources, or may be multiple users who are each reading a portion of one or more text sources. User 110A may be referred to as a reader, a speaker, an announcer, an actor, other terminology, or a combination thereof. User 110B may listen to the content of the text source of the spoken reading. User 110B may or may not read with user 110A. In one example, the user 110A may be a parent who is reading for the child user 110B. In another example, user 110A may include one or more speakers speaking with one or more users 110B who are members of the audience. In either example, the content of the text source 120 can be announced for listening by one or more other users.
The text source 120 may be any source of content that may be interpreted and read audibly. The text source 120 can include content that includes numbers, characters, words, symbols, images, or combinations thereof. The content may be arranged in a sequence that can be spoken by the user when reading or after remembering. The text source 120 may be a physical or electronic book, magazine, presentation, voice, script, memo, announcement, article, blog, post, message, other text arrangement, or a combination thereof. In the example of fig. 1, the text source 120 may be a children's book that includes a sequence of words and images that can be spoken to a child.
The sensors 131A-C may be coupled to the computing device 130A and may enable the computing device to sense aspects of the environment 100. The sensors 131A-C may include one or more audio sensors (e.g., microphones), optical sensors (e.g., ambient light sensors, cameras), atmospheric sensors (e.g., thermometers, barometers, gravitometers), motion sensors (e.g., accelerometers, gyroscopes, etc.), location sensors (e.g., global positioning system sensors (GPS)), proximity sensors, other sensing devices, or combinations thereof. In the example shown in fig. 1, sensor 131A may be an audio sensor, sensor 131B may be an optical sensor, and sensor 131C may be a temperature sensor. One or more of the sensors 131A-C can be internal to the computing device 130A, external to the computing device 130A, or a combination thereof, and can be via a wired or wireless connection (e.g., Bluetooth)
Computing device 130A may be any computing device capable of receiving and processing data derived from sensors 131A-C. Computing device 130A may function as a voice command device and provide access to the integrated virtual assistant. In one example, the computing device 130A may include smart speakers, mobile devices (e.g., phones, tablets), wearable devices (e.g., smartwatches), digital media players (e.g., smart televisions, mini-consoles, set-top boxes), personal computers (e.g., laptops, desktops, workstations), home automation devices, other computing devices, or a combination thereof. In some implementations, the computing device 130A may also be referred to as a "user device," consumption device, "or" client device. The data generated by the sensors 131A-C may be received by the computing device 130A and may be processed locally by the computing device 130A, or may be remotely transmitted from the computing device 130A to another computing device (e.g., 130B).
Computing device 130A may include one or more components for processing sensor data. In the example shown in FIG. 1, the computing device 130A may include an audio analysis component 132, a text source analysis component 133, a comparison component 134, a non-linear reading recognition component 135, a physical effect determination component 136, a predictive loading component 137, and an effect provision component 138. In other examples, one or more of these components or one or more features of the components may be performed by another computing device (e.g., computing device 130B). These components are discussed in more detail with respect to fig. 2-4, and may be used to detect a current reading position and indicate one or more physical effects devices 140A-C to enhance the listening experience.
The physical effects devices 140A-C may be any computing device capable of causing or providing a physical effect. The physical effect may be perceived via the senses (e.g., hearing, line of sight, touch, smell, and taste) of the users 110A and 110B. Each of the physical effects devices 140A-C may produce one or more physical effects, and the computing device 130A may function as one or more of the physical effects devices 130A-C. The physical effects devices 140A-C may provide the physical effect 145 or may instruct another device to provide the physical effect 145. In one example, one or more physical effects devices 140A-C may be part of or integrated with a home automation system, or may be separate from the home automation system. As shown in fig. 1, the physical effects device 130A may comprise a speaker or other device capable of causing or emitting an acoustic effect. The physical effects device 130B may include one or more light sources (e.g., light bulbs, pixels) or other devices capable of altering the amount of light present in the environment 100 (e.g., motorized window treatments or blinds). Physical effect device 130C may include one or more devices that can cause a haptic effect, and may include a vibration source (e.g., a massage chair), a fan that generates wind (e.g., a ceiling fan or air conditioner), a heating or cooling source (e.g., a thermostat), other devices, or combinations thereof.
The physical effect 145 may be any modification to the environment 100 that a user or computing device may perceive and may include an acoustic effect, a haptic effect, an optical effect, other effects, or a combination thereof. The acoustic effect may be a physical effect related to sound and may be propagated via sound waves. The acoustic effects may include human or animal sounds (e.g., speech or noise), atmospheric sounds (e.g., thunder, rain, wind, or other weather sounds), musical sounds (e.g., musical instruments, background music, theme music), object sounds (e.g., knock doors, open doors, close windows, break glass, object impact, automobile driving), other sound effects, or a combination thereof. The haptic effect may be a physical effect related to the user's touch sensation. The haptic effect may include a breeze, vibration, temperature change, other tactile sensation, or a combination thereof. The optical effect may be a physical effect related to light and may propagate via visible electromagnetic radiation. The optical effect may include an increase or decrease in ambient lighting, a light flash, animation, other changes in the amount of light, or a combination thereof. The optical effect may come from a lamp (e.g., ceiling lamp, desk lamp), flash lamp (e.g., telephone lamp), window covering (e.g., blinds or window treatments), projector, electronic display, holographic display, laser, other light source, or a combination thereof. Other effects may include effects related to smell or taste (e.g., olfactory effects).
The media items may correspond to physical effects, text sources, profile information, voice models, instructions, other data, or combinations thereof. Examples of media items include, but are not limited to, digital sound effects, digital music, digital animation, social media information, electronic books (e-books), electronic magazines, digital newspapers, digital audio books, digital videos, digital photographs, web content, electronic periodicals, web blogs, Really Simple Syndication (RSS) feeds, electronic comics, software applications, and so forth. In some implementations, the media items can be referred to as content items and can be provided over the internet and/or via the computing device 130A (e.g., smart speakers). As used herein, "media," "media item," "digital media item," "content," and "content item" may include electronic files or records that may be loaded or executed using software, firmware, or hardware configured to present content to one or more users in the environment 100. In one implementation, computing device 130B may store the media items using one or more data stores and provide the media items to computing device 130A over network 150.
Network 150 may include one or more of a private network (e.g., a Local Area Network (LAN), a public network (e.g., the internet), a Wide Area Network (WAN)), a wired network (e.g., ethernet), a wireless network (e.g., a Wi-Fi or bluetooth connection), a cellular network (e.g., a Long Term Evolution (LTE) network), a router, a hub, a switch, a server computer, and/or combinations thereof.
In general, functions described as being performed by computing device 130A, computing device 130B, or physical effects devices 140A-C in one embodiment may be performed by one or more other devices in other embodiments. In addition, the functionality attributed to a particular component can be performed by different or multiple components operating together. Computing devices 130A and 130B may also be accessed as a service provided to other systems or devices through an appropriate application programming interface. Although embodiments of the present disclosure are discussed in terms of smart speakers, these embodiments may also incorporate one or more features of a cloud service or content sharing platform.
Where the systems discussed herein collect or may utilize personal information about a client device or user, the user may be provided with an opportunity to control whether the computing device is able to collect user information (e.g., information about the user's audio input, the user's preferences, the user's current location, social networks, social behaviors, activities, or professions) or to control whether and/or how more relevant content to the user is received from the computing device. In addition, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized where location information is obtained (such as to a city, zip code, or state level) such that a particular location of the user cannot be determined. Thus, the user may control how information about the user is collected and used by the computing device.
Fig. 2-4 illustrate block diagrams of an example computing device 130 that can detect reading locations within a text source and supplement the environment with physical effects to enhance the listening experience, according to one or more aspects of the present disclosure. Computing device 130 may be the same as or similar to computing device 130A, computing device 130B, or a combination thereof. FIG. 2 discusses features that enable the computing device 130 to receive and compare the user's audio data with data of a text source. Fig. 3 discusses features that enable the computing device 130 to analyze data based on audio data and text source data to detect a reading position. FIG. 4 discusses features that enable the computing device 130 to provide physical effects to modify the environment of one or more listeners. The components and modules provided in fig. 2-4 are exemplary, and more or fewer components or modules may be included without loss of generality. For example, two or more components may be combined into a single component, or a feature of one component may be divided into two or more components. In one implementation, one or more components may reside on different computing devices (e.g., a client device and a server device).
Referring to fig. 2, the computing device 130 may include an audio analysis component 132, a text source analysis component 133, a comparison component 134, and a data store 240. The audio analysis component 132 can receive and access audio data extracted from the environment while the user vocally reads the text source. In one example, the audio analysis component 132 may include an audio data receiving module 212 and an acoustic modeling module 214.
The audio data reception module 212 may receive audio data 241 including one or more audible actions of the user. The audio data may include spoken words, pages, or other audible actions captured from the user environment. The audio data 241 may be received directly from one or more sensors in the form of audio signals, or may be received indirectly from the data store 240 or other computing device after the sensors store the audio data 241. The audio data 241 may be in any digital or analog format and may be accessed or received from one or more storage objects (e.g., files, database records), data streams (e.g., audio streams, video streams), data signals, other data transmission or storage protocols, or combinations thereof. The audio data 241 may be an audio recording and may be segmented into one or more durations (e.g., portions, blocks, or other units) before, during, or after analysis by the acoustic modeling module 214.
The acoustic modeling module 214 may analyze the audio data 241 using acoustic models to identify phoneme data 243A. The acoustic model may represent a known relationship between audible action and phonemes. A phoneme may be a unit of sound and may correspond to a sound pattern of audible action (e.g., a spoken word). The phonemes may be linguistic units, non-linguistic units, other units, or combinations thereof. The acoustic modeling module 214 may convert the audio data into phonemes, which are stored as phoneme data 243A in the data store 240.
The phoneme data 243A may include values representing one or more phonemes extracted from the audio data 241. The phoneme data 243A may represent the phoneme sequence using a standard or proprietary notation (notation). The token may include a particular arrangement of one or more bits, bytes, symbols, or characters representing a phoneme. In one example, the particular arrangement may include symbols placed next to or between one or more delimiters. Delimiters may include slashes, brackets, vertical lines, brackets, commas, tabs, spaces, linens, other delimiters, or combinations thereof. The phonemes may be arranged into a sequence of phonemes representing a portion of one or more audible actions.
The text source analysis component 133 can receive and analyze data related to the text source 120. The text source 120 may be determined in view of user input that is text-based, voice-based, touch-based, gesture-based, or otherwise. For example, the user may identify the text source 120 by speaking the name of the text source 120 (e.g., the title or author of the book), by typing and searching the text source, by selecting a displayed text source, other selection mechanisms, or a combination thereof. In the example shown in FIG. 2, the text source analysis component 133 can include a data access module 222 and a phoneme determination module 224.
Data access module 222 may access data associated with text source 120 and may store the accessed data as text source data 242. The data access module 222 may access data from one or more sources, which may include local sources, remote sources, or a combination thereof. The local source may be a storage of computing device 130, while the remote source may be a storage of computing device accessible through a network connection. In one example, the remote source may be the same as or similar to computing device 130B (e.g., a server or cloud service). The local or remote source may store data for one or more of the media items discussed above, and the computing source may access the data. The data may then be analyzed, filtered, combined, or modified and subsequently stored as text source data 242.
Text source data 242 can be any data associated with text source 120 and can be provided by or accessed from an author, an issuer, a distributor, a partner, a remote server, a third party service, other sources, or a combination thereof. The text source data 242 may include descriptive data, textual data, phonological data, other data, or a combination thereof. The descriptive data may indicate a title, abstract, source (e.g., author, publisher, distributor), catalog (e.g., section, page), index (e.g., phrase, page indicator), other data, or a combination thereof.
The text data may include one or more words of the text source 120. In one example, the words may be organized into a sequence of words 122 with or without one or more images 124. The text data may be a data structure in which words (e.g., a series of consecutive words) are arranged in the same or similar manner as a user reads them. The word sequence may be limited to only words that appear in the text source 120 or may be supplemented with words or data indicating the presence or content of non-textual information (e.g., illustrations, images, tables, formatting, paragraphs, pages). In another example, words may also or alternatively be arranged in an index data structure that indicates unique words that are present in the text source 120 but are not arranged consecutively in the manner spoken by the user. Any of the data structures may be supplemented with additional information that may include word locations within the text source (e.g., pages, lines, slides), times of occurrence, variations of words (e.g., tenses, multiples), other data, or combinations thereof. In one example, the text source 120 can be a physical book, and the text source data 242 can include words from a corresponding electronic book (e.g., an e-book), a third party service, other sources, or a combination thereof.
The phonological data of the text source 120 may be the same as or similar to the phonological data 243B and may be a phonological encoding of the text source 120 in a format that is the same as or similar to phonological data derived from audio (e.g., the phonological data 243A). In the example discussed above, the phoneme data 243B of the text source 120 may be included as part of the text source data 242 and accessed by the phoneme determination module 224. In another example, the text source data 242 may not have phoneme data 243B and the phoneme data 243B may be generated by the phoneme determination module 224.
The phoneme determination module 224 may determine the phoneme data for a particular text source 120. This may involve the phoneme determination module 224 accessing existing phoneme data 243B from a remote source, generating phoneme data 243B based on the text data, or a combination thereof. When generating the phoneme data 243B, the phoneme determination module 224 may access and analyze the text data of the text source data 242 and convert (e.g., derive, convert, transform, encode) the text data into the phoneme data 243B. The generated phonological data may then be associated with the text source 120 for future use by the computing device 130 or one or more other computing devices. In one example, the text data may include a sequence of words, and the generated phonological data may include a pronunciation code (phonetic encoding) that includes a sequence of pronunciation values representing the sequence of words. The same sequence of pronunciation values may correspond to two words that sound the same but differ in spelling (e.g., homophones). Likewise, different pronunciation value sequences may correspond to words that sound different even though they are spelled the same (e.g., homomorphic).
As described above, the phoneme data 243A and 243B may each include a phoneme sequence represented using a standard or proprietary notation. The token may be referred to as an pronunciation transcription or a phonological transcription and may include a particular arrangement of phonological values representing the language segment. A language fragment may be any discrete unit that can be physically or audibly identified in a voice stream. The tone value may comprise one or more symbols, characters, bytes, bits, other values, or combinations thereof. In one example, the phoneme values may be represented by one or more Unicode characters, American Standard Code for Information Interchange (ASCII) characters, other characters, or a combination thereof. A sequence of phoneme values may represent a single word and each individual phoneme value may represent a portion of the word. For example, the first sequence of phonemes may be/θ Λ m/, and represent the spoken word "thumb", while the second sequence of phonemes may be/d Λ m/, and represent the spoken word "dumb". In the examples discussed below, the phoneme data 243A and 243B may include a sequence of values, and each value may represent a phoneme of the phoneme vocabulary.
The phonemic vocabulary may include a collection of possible phonemic values for one or more languages. The phonological vocabulary may be a letter system of phonetic symbols and may represent the qualitative nature (qualities) of speech as part of spoken language: phonemes, intonations, and word and syllable separations. The phonological vocabulary may or may not represent the added quality of the voice and variants of the voice prompt (e.g., tongue-biting, mispronunciations, accents, dialects). The phonemic vocabulary may be the same or similar to phonemic letters, character sets, vocabularies, dictionaries, other variants, or combinations thereof. In one example, the phonemic vocabulary may be based on international phonetic symbols (IPA). The IPA symbol may be composed of one or more elements related to letters and diacritics. For example, English letters<t>May use a single letter t in IPA]Or by letters plus diacritical marks
The comparison component 134 can compare the audio of the user 110A with the content of the text source 120. The examples discussed below use corresponding phonological data of audio and text sources and compare them without converting the audio to text using speech recognition. Other examples may also or alternatively use textual data, descriptive data, audio data, other data, or a combination thereof. The comparison may be performed by the computing device 130, by a remote computing device (e.g., a cloud service), or a combination thereof. In one example, the comparison component 134 may select a phoneme sequence derived from audio and compare it to a plurality of phoneme sequences derived from a text source. In another example, the comparison component 134 can compare the sequence of phonemes of the text source to a plurality of sequences of phonemes derived from audio. In either example, the calculation of the similarity measure data may be based on the phoneme edit distance.
Phoneme edit distance module 232 may quantify the similarity of two phoneme sequences to each other by determining the minimum number of operations required to convert one phoneme sequence into an exact match for the other phoneme sequence. The operation may include any modification of a phoneme value (e.g., symbol) within one of the phoneme sequences. Example operations may include original operations, such as phoneme removal, insertion, replacement, transposition, other operations, or a combination thereof. In the example discussed above, the first sequence of phonemes may be/θ Λ m/and represent "thumb", while the second sequence of phonemes may be/d Λ m/and represent "dumb". Although two words differ by two letters, their phonological edit distance is a numerical value of 1, since converting a sequence to an exact match would involve the replacement of a single phonological (e.g., replacing θ with d). In one example, the phonological edit distance may be a linear edit distance that is the same as or similar to the Levenshtein distance. The Levenshtein distance may be based on a minimum number of removal, insertion, or replacement operations required to make two phoneme sequences equal. In other examples, the phoneme edit distance may also or alternatively include a transposition or other operation. In either example, the phoneme edit distance may be a numerical value used to determine the similarity measure data 244.
The similarity measurement module 234 may access the data of the phonological edit distance module to determine a degree of similarity or dissimilarity between the audio and text sources. The similarity measurement module 234 may analyze the phoneme edit distance module data to calculate similarity measurement data 244. The similarity measurement data 244 may represent the similarity between two or more phoneme sequences (e.g., pronunciation representations of words or sets of words) and may include numerical data, non-numerical data, other data, or a combination thereof. The similarity measurement data 244 may be based on edit distances of one or more phoneme sequences. In one example, the similarity measurement data 244 may include a numerical value of the phonological edit distance. In another example, the similarity measure data 244 may include probability values derived from numerical values of phoneme edit distances. For example, the similarity measure data may be a percentage, ratio, or other value based on one or more phoneme edit distances and one or more other values. Other values may be one or more of the sequence of phonemes or the number of phonemes in the portion of the text source.
The data store 240 can be a memory (e.g., random access memory), a cache, a drive (e.g., a solid state drive, a hard drive, a flash drive), a database system, or another type of component or device capable of storing data. The data store 240 can also include multiple storage components (e.g., multiple drives or multiple databases) that can span one or more computing devices (e.g., multiple server computers).
Fig. 3 depicts a block diagram illustrating example components that enable the computing device 130 to analyze the data discussed above to determine a reading location or the absence of a reading location within a text source. As discussed above, portions of audio may not exactly match the text source, as the user may add, skip, repeat, or reorder the content of the text source while reading aloud. As a result, the phoneme data derived from the audio and the phoneme data derived from the text source may be challenging to compare and align. In the example shown in fig. 1, the computing device 130 may include a non-linear reading recognition component 135 that enables the computing device to determine a location within the text source that best aligns with the audio data. In one example, the non-linear reading recognition component 135 can include a fuzzy matching module 352, a location identification module 354, a reading speed module 356, and a reading interruption module 358.
The fuzzy matching module 352 may enable the computing device 130 to determine whether a match between audio and text sources exists. The match may be the same as or similar to a probabilistic match, a best match, a closest match, or any match that may not be an exact match but satisfies a predetermined threshold. In one example, determining a match between audio and a text source may involve detecting that an audio snippet includes one or more words of the text source. A match may be detected even if the audio or text source contains other words, missing words, or variants of the contained words (e.g., mispronunciations, missing plural forms). This match may be referred to as a fuzzy match or an approximate match and may be detected using fuzzy match logic. Fuzzy matching logic may be used to compare sequences of phoneme values and may operate under syllable level segments, word level segments, phrase level segments, sentence level segments, other segments, or combinations thereof. In one example, fuzzy matching may be performed using an audio segment having a predetermined length. The predetermined length may be customizable and may be any duration (e.g., 3+ seconds) or any number of word tokens (tokens) (e.g., 3-4 words). Having a predetermined length that is much smaller than the length of the text source may enhance accuracy and performance when considering non-linear reading.
The fuzzy matching module 352 may impose one or more constraints to determine a match. In one example, detecting a match may involve using one or more global unweighted costs. The global unweighted cost may be related to the total number of raw operations required to convert a candidate sequence of phonemes (e.g., candidate patterns from a text source) to a selected sequence of phonemes (e.g., patterns from audio). In another example, detecting a match may involve specifying the number of operations of each type separately, while others set the total cost, but allow different weights to be assigned to different original operations. The fuzzy matching module 352 may also apply separate assignments of constraints and weights to individual phoneme values in the sequence.
The location identification module 354 may access data of the fuzzy matching module 352 to identify locations within the text source that correspond to audible actions of audio (e.g., spoken words). In one example, the text source may be a children's book, and the location may be a reading location within a word sequence of the book. In other examples, the location may be in a voice, speech, script, other text source, or a combination thereof. In either example, the location may be a past, current, or future reading location within the text source, and may be stored as location data 345. The location data may be digital or non-digital data identifying one or more particular phonemes, words, paragraphs, pages, segments, chapters, tables, images, slides, other locations, or combinations thereof.
The location identification module 354 may determine that the audible action matches a plurality of different portions of the text source. This may occur when the same word or phrase (e.g., a sequence of phonemes) is repeated multiple times in the text source. The location recognition module 354 may detect the spoken word by analyzing the phonological data and detect that the spoken word matches a plurality of candidate locations within the text source. The location identification module 354 may select one or more of the plurality of candidate locations based on the data of the fuzzy matching module 352. The location recognition module 354 may further narrow down the candidate locations (e.g., expand a predetermined segment length or use adjacent segments) by selecting particular locations based on phonological data of the audio that occurred before, during, or after the spoken word.
The reading speed module 356 may access and analyze the location data 345 to determine the reading speed of the user. Reading speed data may be determined in view of location data, text source data, audio data, other data, or a combination thereof, and may be stored as reading speed data 346. The reading speed may be based on a portion of the location data 345 that identifies at least two locations in the text source. The location may correspond to a particular time, and determining the reading speed may be based on the amount of words and the amount of time between two or more locations. In one example, the amount of words may be based on the content of the text source and may not take into account content that the user added, skipped, or repeated. In another example, the amount of words may be based on the content of the text source and also based on the content of the audio. This may be advantageous because the content of the audio may indicate that words are added, skipped, repeated, other actions, or a combination thereof. In either example, the reading speed module 356 may update the reading speed data to represent the reading speed of the user for one or more durations.
The reading interruption module 358 may access and analyze any of the data discussed above to detect whether the user has interrupted reading the text source or is still reading the text source. This can be challenging because the user may have stopped reading the text source, but is discussing concepts related to the text source. As a result, there may be overlap in the content of the spoken words and the text source. Detecting interruptions in reading may be important because it may allow the computing device to avoid recording private discussions. The reading interruption module 358 may determine whether the user has interrupted reading the text source by calculating one or more corresponding metrics.
The correspondence metric may indicate a degree of similarity or dissimilarity between the audio segment and a corresponding portion of the text source. The correspondence metric may be a probability value indicative of a probability that the audio segment corresponds to the location of the text source. The probability values may be numeric or non-numeric values, and may be the same or similar to percentages, ratios, decimal or other values, or combinations thereof. In one example, the value may be between 0 and 1 (e.g., 0.97), between 0 and 100 (e.g., 98), or other range of values. One end of the range may indicate that the segment of audio absolutely corresponds to the location of the text source (e.g., 1.0 or 100), while another range may indicate that the segment of audio absolutely does not correspond to the location of the text source (e.g., a value of 0).
The correspondence metric may be based on or related to a plurality of similarity measures. For example, both measurements may be used to compare or contrast data derived from audio (e.g., the phonological data 243A) with data derived from a text source (e.g., the phonological data 243B). A similarity measure (e.g., phonological edit distance) may be used to compare or contrast written words of the text source with spoken words, while a correspondence metric may be used to compare or contrast a set of written words with a set of words spoken over a duration of time. The duration of the audio (e.g., segment) may be any length of time, and may include a collection of words and one or more other audible actions (e.g., page turn, book close). In one example, the user's audio may include a first duration and a second duration, and the reading interruption module 358 may calculate one or more corresponding measures of the first duration and one or more corresponding measures of the second duration. The correspondence metric may be stored as correspondence metric data 347. In other examples, the correspondence metric may also or alternatively take into account one or more signals, such as the absence of voice input over a duration of time, the absence of recognition of story text, or recognition of a particular word or phrase that may indicate a stop. These words or phrases may include "let's stop reading", "let's finish tomorrow", "OK, I'm done", "let's pause", other phrases or combinations thereof.
The reading interruption module 358 may compare the corresponding measurement data 347 for each duration to one or more predetermined thresholds. In response to the corresponding measurement data 347 for the first duration not satisfying the threshold (e.g., above or below the threshold), the reading interruption module 358 may determine that the duration of the audio corresponds to the text source and the user audio data corresponds to the user reading the text source. In response to the corresponding measurement data 347 at the second duration satisfying a threshold (e.g., below or above a threshold), the reading interruption may determine that the duration of the audio does not correspond to the text source and that the user has stopped reading the text source. In one example, determining that the correspondence metric satisfies the threshold may indicate that the audio data does not have a match with the data of the text source or that the audio data is different from the content of the text source.
The reading interruption module 358 may perform one or more actions in response to determining that the user has interrupted reading the text source. In one example, the reading interruption module 358 may transmit a signal to deactivate one or more microphones associated with the computing device to avoid capturing or recording additional audio data. In another example, the read interrupt module 358 may transmit a signal to terminate analyzing the audio data (e.g., comparing the audio data to data of a text source). The latter example may record audio but not access or analyze audio data. In yet another example, the reading interruption module may cause the computing device 130 to interact with the user before, during, or after transmitting the signal. For example, the computing device may interact with the user by providing prompts (e.g., audio, visual, or a combination thereof). The prompt may ask the user whether to exit the story time mode or may inform the user that the story time mode has exited and may or may not enable the user to re-enable the story time mode.
FIG. 4 depicts a block diagram illustrating exemplary components that enable the computing device 130 to provide physical effects to enhance the user's experience. As discussed above, the physical effect may modify the environment and may include an acoustic effect, a haptic effect, an optical effect, other effects, or a combination thereof. In the illustrated example, the computing device 130 may include a physical effects determination component 136, a predictive loading component 137, and an effects provision component 138.
The physical effect determination component 136 enables the computing device 130 to identify and provide a physical effect corresponding to a particular portion of the text source. In one example, physical effectiveness determination component 136 may include an audible action correlation module 462, a contextual data module 464, and an effectiveness selection module 466.
The audible action correlation module 462 may enable the computing device to correlate a particular physical effect with a particular audible action associated with the text source. The audible action correlation module 462 can determine a correlation based on the performance data 448 of the text source. The effects data 448 can indicate which physical effects correspond to which portions of the text source. The effects data 448 can relate a particular physical effect to a particular location in a text source, a particular audible action of a user, a particular trigger condition (discussed below), or a combination thereof. The location in the text source may be related to an audible action (e.g., a spoken word or flipping a page) or unrelated to an audible action (e.g., a user viewing a graphical image). In one example, the effects data 448 can identify an audible action of a particular spoken word (e.g., dog) that includes the text source, and the physical effect can involve initiating an acoustic effect (e.g., a bark) that corresponds to the spoken word. In another example, the effects data 448 may identify an audible action (e.g., turning a page) and the physical effect may involve modifying an existing physical effect (e.g., readjusting ambient sound, light, or temperature).
The effects data 448 may be accessed by the computing device 130 or may be created by the computing device 130. In one example, the computing device 130 may access or receive the effects data 448 directly or indirectly from an author, a publisher, a distributor, a partner, a third party service, other source, or a combination thereof. Effects data 448 may be included within text source data 242 or may be separate from text source data 242. In another example, computing device 130 may create effect data based on text source data 242. For example, the audible action correlation module 462 may analyze the text data or phonological data and identify a physical effect corresponding to a particular portion of the text source. In either example, the effects data 448 may be stored in the data store 240 for enhanced access by the computing device 130.
Contextual data module 464 may enable computing device 130 to gather contextual data 449 associated with a user. Context data 449 may be based on the user's environment and may be obtained using one or more sensors (e.g., sensors 131A-C). Context data 449 may also or alternatively be based on profile data about the user, which may be accessible to computing device 130 via direct user input or via a remote source (e.g., a network connection to a content platform or social network). In one example, contextual data 449 may include sound data (e.g., ambient sound measurements), light data (e.g., ambient light measurements), time data (e.g., morning or evening), calendar data (appointment tomorrow in advance), geo-location data (e.g., zip code, address, latitude/longitude), weather data (e.g., rain, lighting, thunderstorm, strong wind, cloudy), user profile (e.g., child's name, age, or gender), user audio feedback (e.g., child crying or clapping), other data, or a combination thereof.
The effect selection module 466 may enable the computing device 130 to select and modify physical effects based on the effect data 448, contextual data 449, text source data 242, other data, or a combination thereof. The effect selection module 466 may be used to select a particular physical effect (e.g., an acoustic effect) or modify a property of a physical effect. The attribute may relate to the intensity, timing, tone, transition (e.g., fade in/out), other characteristics of the physical effect, or a combination thereof. The intensity may be related to the magnitude of the modification to the environment, and may be related to the volume (e.g., loudness) or brightness (e.g., brightness) of the physical effect. The timing may be related to the speed or duration of the physical effect. Computing device 130 may select a physical effect based on words of the text source and may update properties of the physical effect based on the context data. In one example, the contextual data may include sound data of the user's environment, and the physical effect may be an acoustic effect in terms of volume based on the sound data. In another example, the contextual data may include light data of the user's environment, and the physical effect may be an optical effect that modifies the brightness of the light source (e.g., dims or brightens the light) based on the light data. In yet another example, the contextual data may include user profile data for a parent or child and indicate the age of the audience, and wherein the physical effects include acoustic effects selected based on the age of the user (e.g., more amusing barking for young children and more severe barking for older children).
The effect selection module 466 can use the context data to identify timing aspects related to reading of the text source. For example, time data or calendar data may be used to distinguish between text sources reading in the evening or reading in the morning. At night, the effect selection module 466 may select a more calming (e.g., less stimulating) physical effect to encourage the listener to prepare to go to bed. This may involve reducing brightness and volume settings for acoustic and optical effects and/or selecting effects with lower pitch (e.g., softer impact effects or whisper as opposed to yelling). In the morning, the effect selection module 466 may select a more stimulating physical effect to encourage the user to prepare for the day. This may involve increasing brightness and volume settings for acoustic and optical effects. Calendar data may also indicate whether the reading time is associated with a weekend or a weekday, or whether an appointment is coming soon (e.g., later in the day or early in the next morning). Any of these may affect how fast the user reads the text source and how long or how often the physical effect should be provided.
The predictive loading component 137 may enable the computing device 130 to predictively load content for a physical effect in advance of the content being desired. Predictive loading may accelerate the ability of the computing device 130 to provide a physical effect by loading the content of the physical effect before the physical effect is initiated. The speculative load may be the same or similar to prefetching, pre-caching, cache prefetching, other concepts, or a combination thereof. In one example, the predictive loading component 137 may include a prediction module 472, a trigger determination module 474, and a content loading module 476.
The prediction module 472 can enable the computing device 130 to predict a time at which a user will arrive at a particular portion of the text source. For example, the prediction module 472 can determine a time at which a word of the text source will be spoken before the user speaks the word. The predicted time may be a future time and may be determined based on a reading speed of the user, a reading location of the text source, other data, or a combination thereof. In one example, the time may be calculated based on the user's reading speed (e.g., words per minute, pages per minute) and the difference between the current reading position and the target position in the text source (e.g., number of words, paragraphs, or pages). In other examples, the prediction module 472 may use predictive models, machine learning, neural networks, or other techniques to enhance the prediction based on current data, historical data, or a combination thereof.
The determination of the trigger condition may be based on one or more factors related to the content, the computing device, the user, other aspects of the environment, or a combination thereof. Factors related to the content may include the amount of content (e.g., 1MB file size), the location of the content (e.g., remote storage), the format of the content (e.g., downloadable files, streaming blocks, or formats that require transcoding), the duration of the content (e.g., 2 second sound), other aspects of the content, or combinations thereof. The factors related to the computing device may correspond to the amount and/or availability of computing resources of the computing device 130 or other computing devices. The computing resources may include connection speed (e.g., networking bandwidth), storage space (e.g., available solid state storage), processing power (e.g., CPU speed or load), other computing resources, or a combination thereof. The factors related to the user may include the user's reading speed, current reading position, speech intelligibility, other aspects, or a combination thereof.
The trigger determination module 474 may use one or more factors to calculate the duration of time to load or provide the content of the physical effect. The duration associated with loading the content may be referred to as a predicted load time and may or may not include the duration of providing (e.g., playing) the content. In one example, the trigger determination module 474 may determine the duration of time to load the content of the physical effect based on the size of the content and the network bandwidth of the computing device 130. The trigger determination module 474 may use the predicted load time to identify a particular time or location of the trigger condition. In one example, the trigger condition may be set to a time greater than or equal to the predicted time of the audible action (e.g., spoken word) minus the predicted load time (e.g., 5 seconds). In another example, the trigger condition may be set to a location within the text source that is equal to or prior to the location at which the physical effect is expected to align. This may involve selecting a location in the text source based on the predicted load time and reading speed. For example, if the user reads at a rate of 120 words per minute (i.e., 2 words per second) and the predicted load time is 5 seconds, the trigger position may be 10 or more words before the word with which the physical effect should align.
The effect providing component 138 may enable the computing device 130 to provide physical effects to modify the user's environment. The effects providing component 138 may be initiated after loading content for the physical effect and may be timed such that the physical effect is provided at a time aligned with the audible action that is intended to be aligned therewith. In one example, effects providing component 138 may include an instruction access module 482 and a physical effects initiation module 484.
Physical effect initiation module 484 can access instruction data and execute the instruction data to initiate a physical effect. Physical effect initiation module 484 can initiate instructions before, during, or after detecting an initiation trigger condition (e.g., an audible action) corresponding to a physical effect. In one example, the text source may include particular words, and initiating the physical effect may be in response to detecting that the audio data includes words (e.g., matching phonemes). In another example, physical effect initiation module 484 may determine an initiation trigger condition to initiate a physical effect. The process of determining the trigger condition for initiating the physical effect may be the same as or similar to the trigger condition for initiating the loading of the content of the physical effect. The instructions may cause the computing device 130 to provide a physical effect or may cause the computing device 130 to communicate with one or more physical effect devices to provide a physical effect. In either example, the computing device 130 may cause the physical effect to modify the user's environment, either directly or indirectly, to enhance the listening user's experience.
In one example, the physical effect initiation module 484 or the effect selection module 466 may select and/or initiate a physical effect using one or more confidence thresholds. The one or more confidence thresholds may be grouped into one or more confidence intervals that categorize a probability that the audio matches a particular location of the text source (e.g., that a spoken word matches a word of the text source). There may be any number of confidence intervals, and the first confidence interval may indicate that there is a low probability (e.g., > 50%) that audio matches the text source location, and each successive confidence interval may be higher (e.g., > 75%, > 95%, etc.). The effect data relating the physical effect to the location may also include a particular confidence threshold (e.g., a minimum confidence interval). For example, providing a sound effect may be associated with a higher confidence interval and then transitioning a background effect. The computing device 130 may determine whether a confidence threshold is met before selecting or initiating the physical effect. This may involve comparing corresponding metric data, similarity metric data, other data associated with fuzzy matches, or a combination thereof. In one example, a particular location in the text source may be associated with a plurality of different physical effects, and each physical effect may correspond to a different confidence interval associated with the current reading location. When the confidence interval is high (e.g., trusting that the current reading position is accurate), a particular sound effect may be initiated (e.g., the sound effect of a single dog barking at a higher volume), while when the confidence interval is low (e.g., without determining whether the current reading position is accurate), a different sound effect may be initiated (e.g., background noise of multiple dogs barking at a lower volume).
Fig. 5-8 depict flow diagrams of respective methods 500, 600, 700, and 800 for enhancing the ability of a computing device to follow and provide special effects in real-time as a text source is audibly read, in accordance with one or more aspects of the present disclosure. The method 500 may involve estimating reading progress using phoneme data and fuzzy matching. The method 600 may optimize the ability of a computing device to detect when a user stops reading from a text source and is conducting private discussions. The method 700 may enable a computing device to provide a physical effect that takes into account a user's context and the user's environment. The method 800 may enable a computing device to pre-cache content of a physical effect to reduce latency and better synchronize the physical effect with audible actions associated with a text source.
Each of the methods of fig. 5-8 and their individual functions, routines, subroutines, or operations may be performed by one or more processors of a computer device executing the method. In some implementations, one or more of the methods may be performed by a single computing device. Alternatively, one or more of the methods may be performed by two or more computing devices, each computing device performing one or more separate functions, routines, subroutines, or operations of the methods. For simplicity of explanation, the methodologies of the present disclosure are depicted and described as a series of acts. However, acts in accordance with the present disclosure may occur in various orders and/or concurrently, and with other acts not presented and described herein. Moreover, not all illustrated acts may be required to implement a methodology in accordance with the disclosed subject matter. In addition, those skilled in the art will understand and appreciate that a methodology could alternatively be represented as a series of interrelated states via a state diagram or events. Additionally, it should be appreciated that the methodologies disclosed in this specification are capable of being stored on an article of manufacture to facilitate transporting and transferring such methodologies to computing devices. The term "article of manufacture" as used herein is intended to encompass a computer program accessible from any computer-readable device or storage media. In one embodiment, the method may be performed by one or more of the components in fig. 1-4.
Referring to fig. 5, the method 500 may be performed by a processing device of a client device (e.g., smart speakers), a server device (e.g., cloud services), other device, or a combination thereof, and may begin at block 502. At block 502, the processing device may determine phoneme data for a text source. The text source may include a sequence of words and the phonological data may be a pronunciation code for the sequence of words, the pronunciation code including one or more sequences of pronunciation values. Each pronunciation value may correspond to a phoneme, and the sequence of phonemes may correspond to a spoken word. The same sequence of pronunciation values may correspond to words that sound the same but that are spelled differently (e.g., homophones), while different sequences of pronunciation values may correspond to words that are spelled the same but that sound different (e.g., homomorphic heterowords).
The processing device may access the phonological data from a source of the text source or may generate the phonological data for the text source. The processing device may generate the phoneme data by vocoding a sequence of words. This may involve accessing text data of a text source and generating (e.g., converting, transforming, deriving) phoneme data based on the text data. The phoneme data may then be associated with the phoneme data for future use.
At block 504, the processing device may receive audio data including spoken words associated with a text source. The audio data may include one or more audible actions of the user, and may include spoken words, flipping pages, or other audible actions captured from the user's environment. In one example, the processing device may receive audio data in the form of audio signals directly from one or more sensors. In another example, the processing device may receive audio data from a data store or another computing device. The audio data may be in any digital or analog format and may be accessed or received via one or more storage objects (e.g., files, database records), data streams (e.g., audio streams, video streams), data signals, other data transmission or storage protocols, or a combination thereof.
At block 506, the processing device may compare the phoneme data of the text source with the phoneme data of the audio data. The comparison of the audio data and the text source may occur without converting the audio data to text (e.g., recognized words) using speech recognition, and may involve comparing the phoneme data corresponding to the audio data and the phoneme data corresponding to the text source. The comparison may include calculating a numerical value representing a degree of similarity between the two or more sequences of pronunciation values. The numerical value may be a phonological edit distance between the phonological data of the audio data and the phonological data of the text source. The comparison may also involve performing a fuzzy match between the phonological data corresponding to the audio data and the phonological data of the text source.
At block 508, the processing device may identify a location in the word sequence based on a comparison of the phoneme data of the text source and the phoneme data of the audio. The identification of the location may involve determining that the spoken word matches a word in a sequence of words of the text source. In one example, the text source may be a book and the location may be a current reading location in the book. In response to completing the operations described above with reference to block 508, the method may terminate.
Referring to fig. 6, method 600 may be performed by the same processing device as discussed above or a different processing device, and may begin at block 602. At block 602, the processing device may receive audio data including spoken words associated with a text source. The audio data may be segmented (e.g., tokenize, sliced, split, divided) into a first duration and a second duration. In one example, the text source may be a book, and the first portion of the audio data may correspond to content of the book (e.g., including spoken words of the book), while the second portion of the audio data may not correspond to content of the book (e.g., there may be no spoken words from the book).
At block 604, the processing device may compare the audio data to data of the text source. The data of the text source may include phoneme data, and comparing the audio data and the data of the text source may involve phoneme comparison. In one example, comparing the phoneme data may involve calculating a phoneme edit distance between the phoneme data of the text source and the phoneme data of the audio data.
At block 606, the processing device may calculate a correspondence metric between the second duration of the audio data and the data of the text source. Calculating the correspondence metric may involve calculating the correspondence metric based on a plurality of phonological edit distances. In one example, the processing device may select a set of spoken words (e.g., 3, 4, 5+ words) and compare the set of spoken words to the content of the text source. The phonological edit distance may be determined for each word or combination of one or more words in the collection. The resulting values may then be weighted, aggregated, or modified to determine the corresponding metric.
At block 608, in response to determining that the corresponding metric satisfies the threshold, the processing device may transmit a signal that terminates comparing the audio data to the data of the text source. Determining that the correspondence metric satisfies the threshold may involve determining that the correspondence metric is below or above the threshold. The determination may also be based on a duration of time for which the corresponding metric meets or does not meet the threshold. Determining that the correspondence metric satisfies the threshold may indicate that the second duration of the audio data includes content that is different from the content of the text source, and may or may not indicate that the audio data does not have a match with the data of the text source. Transmitting the signal may involve transmitting a signal that deactivates one or more microphones that capture audio data. In one example, in response to determining that the second duration of audio data does not exist for the content of the text source, the processing device may cause the computing device to prompt the user to exit the story time mode. The prompt may be an audio prompt, a visual prompt, other prompt, or a combination thereof. In response to completing the operations described above with reference to block 608, the method may terminate.
Referring to fig. 7, method 700 may be performed by the same processing device as discussed above or a different processing device, and may begin at block 702. At block 702, the processing device may receive audio data including spoken words of a user. The spoken word may be associated with a text source that the user is reading vocally, and may include one or more other audible actions, such as turning pages, spoken words not within the text source, and other audible actions captured from the user's environment. In one example, the processing device may receive audio data directly from one or more sensors in the form of audio signals (e.g., for use in real-time or real-time perception). In another example, the processing device may receive audio data from a data store or another computing device. The audio data may be in any digital or analog format and may be accessed or received from one or more storage objects (e.g., files, database records), data streams (e.g., audio streams, video streams), data signals, other data transmission or storage protocols, or a combination thereof.
At block 704, the processing device may analyze context data associated with the user. The contextual data may include sound data, light data, time data, weather data, calendar data, user profile data, other data, or a combination thereof. In some examples, the context data may be associated with a physical effect such that the processing device may provide the physical effect taking into account the context of the user and the environment of the user. In one example, the contextual data may include sound data of the user environment, and the physical effect may include an acoustic effect at a volume based on the sound data. In another example, the contextual data may include light data of the user environment, and the physical effect may include an optical effect that modifies a brightness of the light source based on the light data. In yet another example, the contextual data may include user profile data indicating an age of the child, and the physical effect may include an acoustic effect selected based on the age of the child.
At block 706, the processing device may determine a match between the audio data and the data of the text source. The processing device may identify the text source based on user input (e.g., audio data or touch data) and retrieve data of the text source. The data of the text source may include phoneme data, and determining a match may involve calculating a phoneme edit distance between the phoneme data of the text source and the phoneme data of the audio data. In one example, determining a match between the audio data and the data of the text source may involve using the phonological data of the text source to detect the audio data that includes words of the text source.
At block 708, the processing device may initiate a physical effect in response to determining a match. The physical effect may correspond to a text source and be based on the context data. The physical effect may modify the user's environment and may include at least one of an acoustic effect, an optical effect, and a haptic effect. The text source may include words, and initiating the physical effect may be in response to detecting that the audio data includes words. In one example, the processing device may select a physical effect based on words of the text source, and may update a property (e.g., volume or brightness) of the physical effect based on the context data. In response to completing the operations described above with reference to block 708, the method may terminate.
Referring to fig. 8, method 800 may be performed by a processing device of a server device or a client device and may begin at block 802. At block 802, the processing device may identify effect data for a text source, where the effect data relates a physical effect to an audible action of a user. The performance data may indicate a physical performance and indicate a location in the text source related to the audible action. The location may correspond to a word, paragraph, page, or other location of the text source. In one example, the audible action may be a spoken word of the text source, and the physical effect may be an acoustic effect corresponding to the spoken word. In another example, the audible action may include turning a page, and the physical effect may be a modification to an existing acoustic effect, optical effect, or haptic effect.
At block 804, the processing device may receive audio data comprising a plurality of audible actions. The plurality of audible actions may include one or more spoken words of the text source and one or more other audible actions, such as turning pages, spoken words not within the text source, and other audible actions captured from the user's environment. In one example, the processing device may receive audio data directly from one or more sensors in the form of audio signals (e.g., for real-time or near/perceptual real-time use). In another example, the processing device may receive audio data from a data store or another computing device. The audio data may be in any digital or analog format and may be accessed or received from one or more storage objects (e.g., files, database records), data streams (e.g., audio streams, video streams), data signals, other data transmission or storage protocols, or a combination thereof.
At block 806, the processing device may determine a trigger condition based on the effect data and the text source. In one example, determining the trigger condition may involve determining a physical effect associated with a first location in the text source and selecting a second location in the text source that precedes the first location. The selection may be based on a reading speed and a loading time associated with the physical effect, and the second location may be associated with at least one of a particular instance of a word, paragraph, page, or chapter of the text source. The processing device may then set the trigger condition to correspond to the second location in the text source. In another example, determining the trigger condition may involve calculating a duration of loading the content based on an amount of content and an amount of available computing resources of the physical effect. The computing resources may be related to one or more of networking bandwidth, storage space, or processing power, and the duration may be longer when the available computing resources are lower. In one example, determining a future time at which the audible action will occur may involve identifying a time at which to initiate loading based on the calculated duration and the determined time of the audible action, and initiating loading of the content at or before the identified time. In another example, determining the time includes calculating a future time based on the reading speed and a current reading position in the text source. In yet another example, determining the time includes predicting a time at which words of the text source will be spoken before the words are spoken.
At block 808, the processing device may load content for the physical effect in response to the trigger condition being satisfied. The trigger condition may be satisfied before the audible action occurs.
At block 810, the processing device may provide a physical effect to modify the user's environment. In response to completing the operations described above with reference to block 810, the method may terminate.
The techniques discussed herein include a variety of enhanced functionality for computing devices with or without virtual assistant features. The following discussion includes a number of different enhanced functionality that may be used separately or together to optimize the ability of a computing device to follow when a text source is read vocally and to provide special effects to supplement the user's environment. In one example, the environment may include a parent speaking a book to one or more children. In another example, the environment may include one or more users providing a lecture, voice, or other performance to an audience. In either example, the technique can be used to enhance the environment with special effects based on an analysis of data associated with the text source. Special effects may be synchronized with specific portions of the text source, such as specific spoken words or flipping pages.
In a first example, the enhancement may relate to reading progress estimation based on pronunciation fuzzy matching and confidence intervals, and may relate to the field of computer-based human voice recognition, and in particular to enhancing the ability of a computer device to identify reading locations in text sources when a user vocally reads the text sources. When a user vocally reads a text source, a number of technical problems arise when a computing device attempts to follow using traditional virtual assistant features. Some problems arise because conventional virtual assistant features perform voice recognition to convert audio into text/recognized words. Speech recognition typically involves an acoustic step to convert the audio into phonemes and a linguistic step to convert the phonemes into text/recognized words. The language step typically waits for subsequent spoken words to establish context before converting the spoken words to text. The language step introduces unnecessary time delays and consumes additional computing resources. Additionally, performing traditional text-based comparisons with text sources using recognized text may be more error prone than performing pronunciation comparisons (e.g., phoneme comparisons). This is often the case because many words that sound the same or similar may be spelled very differently and yield false negatives when the text is compared. In addition, conventional text comparisons may not properly address situations where a user may jump around while reading a text source. For example, portions of the text source may be skipped, repeated, or new content added. This can make it challenging to identify the current reading location within the text source and to correctly detect the reading speed. Aspects and embodiments of the present technology address the above and other deficiencies by providing enhancements to enable a computing device to detect a current reading location in a text source while the text source is being read vocally. In one example, the techniques may avoid the linguistic step of traditional speech recognition by comparing the phonological data derived from audio with the phonological data derived from a text source. The text source may be a book, magazine, lecture, voice, script, or other source containing sequences of words. The techniques may receive audio data including words spoken by a user, and may convert the audio data to phonemic data locally or through an assistant or remote server (e.g., a cloud service). The phoneme data of the audio may then be compared to the text source via pronunciation comparisons rather than more traditional text comparisons. Pronunciation comparisons may be accompanied by fuzzy matching to identify a location (e.g., a current reading location) within a word sequence. The systems and methods described herein include techniques to enhance the technical field of computer-based human voice recognition. In particular, the disclosed techniques enhance the latency, accuracy, and computational resources required to identify the current reading position. This may be the result of modifying the voice analysis process (e.g., voice recognition) to avoid converting the audio data into text/words. The technique may use a voice analysis process that uses an acoustic model to convert audio into phonological data, but may avoid the language step of using a language model to convert phonological data into text/words. Avoiding the language step reduces the time delay and consumption of computing resources. Performing a phoneme comparison and using fuzzy matching may enhance the accuracy of identifying the current reading position because it may better compensate for non-linear reading of the text source (e.g., skipping, repeating, or adding content).
In a second example, the enhancement may relate to an algorithmic determination that a story reader discontinues reading, and may relate to the field of computer-based human voice recognition, and in particular to enhancing the ability of a computer device to determine that a user is no longer speaking reading the content of a text source. A number of technical problems arise when a computing device attempts to follow up using conventional virtual assistant features when a user vocally reads a text source. Some problems may arise because conventional virtual assistants may not be able to detect when the user has completed audio input if the user continues to talk about other things. This may cause the computing device to continue recording the user's audio, which may be problematic if the user transitions to discussing private things. When a user does not follow text and skips, repeats, or adds new content while reading the text source audibly, detecting when the user stops reading from the text source can be more challenging. Aspects and embodiments of the present technology address the above and other deficiencies by enhancing the ability of a computing device to detect when a user discontinues reading a text source. In one example, the techniques may enable the virtual assistant to more accurately detect that the user has left the source of reading text to take a break, and may deactivate the microphone to avoid capturing private audio content. This may involve receiving audio data including spoken words associated with the text source and comparing the audio data to the data of the text source. The techniques may calculate a correspondence metric between content of the audio data and content of the text source. The correspondence metric may be a probability value based on a comparison of the phonological data, textual data, or other data, and may involve the use of fuzzy matching logic. When the correspondence metric satisfies a threshold (e.g., is below a minimum correspondence threshold), the technique may cause a signal to be transmitted that will terminate analysis of subsequent audio data. The systems and methods described herein include techniques to enhance the technical field of computer-based human voice recognition. In particular, the techniques may solve technical problems by avoiding inadvertent recordings of user private conversations through the use of comparisons that may better compensate for non-linear reading of text sources (e.g., skipping, repeating, adding content). For example, the above techniques may facilitate more accurate and/or more rapid automatic control of a virtual assistant to record and/or process only relevant audio. The techniques may also enable the computing device to reduce power consumption by disabling an audio sensor (e.g., a microphone) and associated data processing when the computing device detects that a user has stopped reading text. Furthermore, the above techniques may enable a computing device to reduce utilization of computing resources, such as processing capacity, network bandwidth, data storage, and so forth, that may otherwise be used to record and/or process audio data once a user has stopped reading text.
In a third example, the enhancement may relate to dynamic adjustment of a story time special effect based on the context data, and may relate to the field of virtual assistants, particularly enhancing the ability of the virtual assistant to provide the special effect while phonetically reading the text source. Modern computing devices may be configured to employ traditional virtual assistant features to provide sound effects that supplement the environment as the user speaks to read the book. For example, the computing device may provide a bark effect when the user vocally reads the word "bark". The sound effects are typically provided by the same entity that provides the text source, and may correspond directly to a portion of the text source. As a result, special effects may be the same independent of the user or environment and may not be optimized for the user's particular reading environment. Aspects and embodiments of the present technology address the above and other deficiencies by enabling computing devices to provide a wide variety of special effects based on user environment. In one example, the techniques may enable a computing device to analyze context data of a user environment and select or customize special effects. A special effect may be a physical effect that alters the user's environment, including an acoustic effect (e.g., music, sound effect music), an optical effect (e.g., flashing lights, ambient light), a haptic effect (e.g., vibration, wind, temperature change), other effects, or a combination thereof. The techniques may involve receiving and analyzing context data associated with a user. The contextual data may relate to weather, lighting, time of day, user feedback, user profiles, other information, or a combination thereof. The techniques may select or modify a physical effect corresponding to the text source based on the context data. This may result in, for example, selecting or modifying volume, brightness, speed, pitch, or other attributes of the physical effect. The systems and methods described herein include techniques to enhance the technical fields of virtual assistants and home automation. In particular, the techniques may enable a computing device to optimize an environment by using contextual data about a user and the environment to add, remove, or modify physical effects to enhance the user's listening experience.
In a fourth example, the augmentation may relate to the detection of story reader progress to pre-cache special effects, and may relate to the realm of virtual assistants, particularly to augmenting the ability of a virtual assistant to pre-cache special effects for text sources being vocally read. In attempting to use conventional virtual assistant functionality to provide sound effects that are synchronized with the voice content of a text source, a number of technical problems arise. Some problems arise because conventional virtual assistants perform voice recognition to convert audio to text and then compare based on the text. Speech recognition typically involves an acoustic step to convert audio into phonemes and a linguistic step to convert phonemes into text. The language step typically waits for subsequent spoken words to establish context before converting the spoken words to text. The language step introduces time delays and consumes additional computing resources on the computing device that may be resource constrained. The delay may be further exacerbated because the sound effect may be a large audio file downloaded from a remote data source. Traditional approaches may involve downloading sound effects in response to detecting a spoken word, but delays may result in special effects being provided long after the word is spoken. Another approach may involve downloading all sound effects when the text source is initially identified, but may be problematic when the computing device is a resource-constrained device (e.g., a smart speaker). Aspects and embodiments of the present technology address the above and other deficiencies by providing enhancements to a computing device that enhance its ability to conserve computing resources and yet provide special effects that are synchronized with a text source being read vocally. This may be accomplished by using data from a text source (e.g., a book) to predict future audible actions and pre-fetching the associated physical effects before the corresponding audible actions occur. In one example, the techniques may enable a computing device to predict when a user reaches a word in a text source before the word is spoken. This may involve identifying effect data for a text source that correlates a physical effect with one or more audible actions of the user. The audible action may include words spoken by the user, or may be a page turn, a book close, or other action that produces an audible response. The technique may determine the trigger condition based on the current reading position, reading speed, other data, or a combination thereof. In response to detecting that the trigger condition is satisfied, the technique can cause the computing device to load content for the physical effect and then provide the physical effect to modify the user's environment. The systems and methods described herein include techniques in the art to enhance pre-caching based on recognition of human speech. In particular, the disclosed technology can solve technical problems related to resource consumption when analyzing voice and downloading special effects. The techniques may also reduce the delay in providing special effects, thereby better synchronizing the special effects with human voice.
Fig. 9 is a block diagram illustrating a computer system operating in accordance with one or more aspects of the present disclosure. In various illustrative examples, computing system 900 may correspond to computing device 130 of fig. 2-4. The computing system may be included within a data center that supports virtualization. In some embodiments, computer system 900 may be connected to other computer systems (e.g., via a network such as a Local Area Network (LAN), intranet, extranet, or the internet). Computer system 900 may operate in the capacity of a server or a client computer in a client-server environment, or as a peer computer in a peer-to-peer or distributed network environment. Computer system 900 may be provided by a Personal Computer (PC), a tablet PC, a set-top box (STB), a Personal Digital Assistant (PDA), a cellular telephone, a web appliance, a server, a network router, switch or bridge, or any device capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that device. Additionally, the term "computer" shall include any collection of computers that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies described herein.
In another aspect, computer system 900 may include a processing device 902, a volatile memory 904 (e.g., Random Access Memory (RAM)), a non-volatile memory 906 (e.g., Read Only Memory (ROM)) or electrically erasable programmable ROM (eeprom)) and a data storage device 916, which may communicate with each other via a bus 908.
The processing device 902 may be provided by one or more processors, such as a general purpose processor (such as, for example, a Complex Instruction Set Computing (CISC) microprocessor, a Reduced Instruction Set Computing (RISC) microprocessor, a Very Long Instruction Word (VLIW) microprocessor, a microprocessor implementing other types of instruction sets, or a microprocessor implementing a combination of types of instruction sets) or a special purpose processor (such as, for example, an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Digital Signal Processor (DSP), or a network processor).
The computer system 900 may further include a network interface device 922. Computer system 900 may also include a video display unit 910 (e.g., an LCD), an alphanumeric input device 912 (e.g., a keyboard), a cursor control device 914 (e.g., a mouse), and a signal generation device 920.
The data storage 916 may include a non-transitory computer-readable storage medium 924 on which may be stored instructions 926 encoding any one or more of the methods or functions described herein, including instructions for implementing the methods 500, 600, 700, or 800 and any of the components or modules in fig. 1-4.
The instructions 926 may also reside, completely or partially, within the volatile memory 904 and/or within the processing device 902 during execution thereof by the computer system 900, the volatile memory 904 and the processing device 902 thus also constituting machine-readable storage media.
While the computer-readable storage medium 924 is shown in an illustrative example to be a single medium, the term "computer-readable storage medium" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of executable instructions. The term "computer-readable storage medium" shall also be taken to include a tangible medium that is capable of storing or encoding a set of instructions for execution by the computer and that cause the computer to perform any one or more of the methodologies described herein. The term "computer readable storage medium" shall include, but not be limited to, solid-state memories, optical media, and magnetic media.
The methods, components and features described herein may be implemented by discrete hardware components or may be integrated in the functionality of other hardware components such as ASICs, FPGAs, DSPs or similar devices. Furthermore, the methods, components and features may be implemented by firmware modules or functional circuits within hardware resources. Additionally, methods, components, and features may be implemented with any combination of hardware resources and computer program components or computer programs.
Unless specifically stated otherwise, terms such as "initiating," "transmitting," "receiving," "analyzing," or the like, refer to actions and processes performed or effected by a computer system that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices. In addition, the terms "first," "second," "third," "fourth," and the like, as used herein, refer to labels used to distinguish between different elements and may not have an ordinal meaning according to their numerical name.
Examples described herein also relate to an apparatus for performing the methods described herein. This apparatus may be specially constructed for performing the methods described herein, or it may comprise a general-purpose computer system selectively programmed by a computer program stored in the computer system. Such a computer program may be stored in a computer readable tangible storage medium.
The methods and illustrative examples described herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with the teachings described herein, or it may prove convenient to construct a more specialized apparatus to perform each of the methods 500, 600, 700, 800, and/or individual functions, routines, subroutines, or operations thereof. Examples of the structure of various of these systems are set forth in the foregoing description.
The above description is intended to be illustrative, and not restrictive. While the present disclosure has been described with reference to specific illustrative examples and embodiments, it should be recognized that the present disclosure is not limited to the described examples and embodiments. The scope of the disclosure should be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled.
Claims (20)
1. A method, comprising:
receiving, by a processing device, audio data comprising spoken words of a user;
analyzing context data associated with the user;
determining a match between the audio data and data of a text source; and
initiating a physical effect in response to the determination of the match, wherein the physical effect corresponds to the text source and is based on the contextual data.
2. The method of claim 1, wherein the physical effect corresponding to the text source modifies an environment of the user and comprises at least one of an acoustic effect, an optical effect, or a haptic effect.
3. The method of claim 1 or 2, wherein the contextual data comprises at least one of sound data, light data, time data, weather data, calendar data, or user profile data.
4. The method of any preceding claim, wherein the contextual data comprises sound data of the environment of the user, and wherein the physical effect comprises an acoustic effect in terms of volume based on the sound data.
5. The method of any preceding claim, wherein the contextual data comprises light data of an environment of the user, and wherein the physical effect comprises an optical effect that modifies a brightness of a light source based on the light data.
6. The method of any preceding claim, wherein the text source comprises words, and wherein initiating the physical effect is in response to detecting that the audio data comprises the words.
7. The method of claim 6, further comprising:
selecting the physical effect based on the words of the text source; and
updating a property of the physical effect based on the context data.
8. The method of any preceding claim, wherein determining the match between the audio data and data of a text source comprises: detecting that the audio data includes words of the text source using the phonological data of the text source.
9. The method of any preceding claim, wherein the data of the text source comprises phoneme data, and wherein determining the match comprises calculating a phoneme edit distance between the phoneme data of the text source and the phoneme data of the audio data.
10. The method of any preceding claim, wherein the contextual data comprises user profile data indicating an age of a child, and wherein the physical effect comprises an acoustic effect selected based on the age of the child.
11. A system comprising a processing device configured to:
receiving audio data comprising spoken words of a user;
analyzing context data associated with the user;
determining a match between the audio data and data of a text source; and
initiating a physical effect in response to determining the match, wherein the physical effect corresponds to the text source and is based on the contextual data.
12. The system of claim 10, wherein the physical effect corresponding to the text source modifies an environment of the user and comprises at least one of an acoustic effect, an optical effect, or a haptic effect.
13. The system of any of claims 11 or 12, wherein the contextual data comprises at least one of sound data, light data, time data, weather data, calendar data, or user profile data.
14. The system of any of claims 11 to 13, wherein:
the contextual data comprises sound data of an environment of the user, and wherein the physical effect comprises an acoustic effect in volume based on the sound data; and/or
The contextual data comprises light data of an environment of the user, and wherein the physical effect comprises an optical effect that modifies a brightness of a light source based on the light data.
15. The system of any of claims 11 to 14, wherein the text source comprises words, and wherein initiating the physical effect is in response to detecting that the audio data comprises the words.
16. The system of claim 15, the processing device configured to:
selecting the physical effect based on the words of the text source; and
updating a property of the physical effect based on the context data.
17. The system of any of claims 11 to 16, wherein determining the match between the audio data and data of a text source comprises using phonological data of the text source to detect that the audio data includes words of the text source.
18. The system of any of claims 11 to 17, wherein the data of the text source comprises phoneme data, and wherein determining the match comprises calculating a phoneme edit distance between the phoneme data of the text source and the phoneme data of the audio data.
19. The system of any of claims 11 to 18, wherein the contextual data comprises user profile data indicating an age of a child, and wherein the physical effect comprises an acoustic effect selected based on the age of the child.
20. A computer program product configured such that, when processed by a processing device, the processing device is caused to perform the method of any one of claims 1 to 10.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2018/049205 WO2020046387A1 (en) | 2018-08-31 | 2018-08-31 | Dynamic adjustment of story time special effects based on contextual data |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112889022A true CN112889022A (en) | 2021-06-01 |
Family
ID=63684519
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880098327.3A Pending CN112889022A (en) | 2018-08-31 | 2018-08-31 | Dynamic adjustment of story time special effects based on contextual data |
Country Status (4)
Country | Link |
---|---|
US (1) | US11501769B2 (en) |
EP (1) | EP3844605A1 (en) |
CN (1) | CN112889022A (en) |
WO (1) | WO2020046387A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN114352965A (en) * | 2022-01-21 | 2022-04-15 | 杭州鸿澄智家科技有限公司 | Collision desk lamp with adjustable color |
US20220165281A1 (en) * | 2019-04-02 | 2022-05-26 | Nokia Technologies Oy | Audio codec extension |
US11501769B2 (en) * | 2018-08-31 | 2022-11-15 | Google Llc | Dynamic adjustment of story time special effects based on contextual data |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP4191562A1 (en) | 2018-08-27 | 2023-06-07 | Google LLC | Algorithmic determination of a story readers discontinuation of reading |
EP3837597A1 (en) | 2018-09-04 | 2021-06-23 | Google LLC | Detection of story reader progress for pre-caching special effects |
WO2020050820A1 (en) | 2018-09-04 | 2020-03-12 | Google Llc | Reading progress estimation based on phonetic fuzzy matching and confidence interval |
JP7197786B2 (en) * | 2019-02-12 | 2022-12-28 | 日本電信電話株式会社 | Estimation device, estimation method, and program |
CN111629156A (en) * | 2019-02-28 | 2020-09-04 | 北京字节跳动网络技术有限公司 | Image special effect triggering method and device and hardware device |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150170648A1 (en) * | 2013-12-17 | 2015-06-18 | Google Inc. | Ebook interaction using speech recognition |
US20160225187A1 (en) * | 2014-11-18 | 2016-08-04 | Hallmark Cards, Incorporated | Immersive story creation |
US20180018373A1 (en) * | 2016-07-18 | 2018-01-18 | Disney Enterprises, Inc. | Context-based digital assistant |
Family Cites Families (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7143033B2 (en) | 2002-04-03 | 2006-11-28 | The United States Of America As Represented By The Secretary Of The Navy | Automatic multi-language phonetic transcribing system |
WO2004093078A1 (en) | 2003-04-18 | 2004-10-28 | Unisay Sdn. Bhd. | Process for adding subtitles to video content |
KR20060054678A (en) * | 2004-11-15 | 2006-05-23 | 김경희 | Apparatus and method for implementing character video synchronized with sound |
US8069397B2 (en) | 2006-07-10 | 2011-11-29 | Broadcom Corporation | Use of ECC with iterative decoding for iterative and non-iterative decoding in a read channel for a disk drive |
US20100028843A1 (en) | 2008-07-29 | 2010-02-04 | Bonafide Innovations, LLC | Speech activated sound effects book |
US8568189B2 (en) | 2009-11-25 | 2013-10-29 | Hallmark Cards, Incorporated | Context-based interactive plush toy |
US20110153330A1 (en) | 2009-11-27 | 2011-06-23 | i-SCROLL | System and method for rendering text synchronized audio |
KR101832693B1 (en) * | 2010-03-19 | 2018-02-28 | 디지맥 코포레이션 | Intuitive computing methods and systems |
US8302010B2 (en) | 2010-03-29 | 2012-10-30 | Avid Technology, Inc. | Transcript editor |
US20120001923A1 (en) | 2010-07-03 | 2012-01-05 | Sara Weinzimmer | Sound-enhanced ebook with sound events triggered by reader progress |
US8880399B2 (en) | 2010-09-27 | 2014-11-04 | Rosetta Stone, Ltd. | Utterance verification and pronunciation scoring by lattice transduction |
US8744856B1 (en) | 2011-02-22 | 2014-06-03 | Carnegie Speech Company | Computer implemented system and method and computer program product for evaluating pronunciation of phonemes in a language |
US10672399B2 (en) | 2011-06-03 | 2020-06-02 | Apple Inc. | Switching between text data and audio data based on a mapping |
US10109278B2 (en) | 2012-08-02 | 2018-10-23 | Audible, Inc. | Aligning body matter across content formats |
US10042603B2 (en) * | 2012-09-20 | 2018-08-07 | Samsung Electronics Co., Ltd. | Context aware service provision method and apparatus of user device |
CN105359546B (en) * | 2013-05-01 | 2018-09-25 | 乐盟交互公司 | Content for interactive video optical projection system generates |
US10249205B2 (en) | 2015-06-08 | 2019-04-02 | Novel Effect, Inc. | System and method for integrating special effects with a text source |
US20190189019A1 (en) | 2015-06-08 | 2019-06-20 | Novel Effect, Inc. | System and Method for Integrating Special Effects with a Text Source |
US10928915B2 (en) * | 2016-02-10 | 2021-02-23 | Disney Enterprises, Inc. | Distributed storytelling environment |
US20170262537A1 (en) | 2016-03-14 | 2017-09-14 | Amazon Technologies, Inc. | Audio scripts for various content |
US20180293221A1 (en) | 2017-02-14 | 2018-10-11 | Microsoft Technology Licensing, Llc | Speech parsing with intelligent assistant |
US10586369B1 (en) * | 2018-01-31 | 2020-03-10 | Amazon Technologies, Inc. | Using dialog and contextual data of a virtual reality environment to create metadata to drive avatar animation |
US11775479B2 (en) | 2018-05-24 | 2023-10-03 | Luther Systems Us Incorporated | System and method for efficient and secure private similarity detection for large private document repositories |
WO2020046387A1 (en) * | 2018-08-31 | 2020-03-05 | Google Llc | Dynamic adjustment of story time special effects based on contextual data |
EP3837597A1 (en) * | 2018-09-04 | 2021-06-23 | Google LLC | Detection of story reader progress for pre-caching special effects |
US20200111491A1 (en) | 2018-10-08 | 2020-04-09 | Alkira Software Holdings Pty Ltd. | Speech enabled user interaction |
-
2018
- 2018-08-31 WO PCT/US2018/049205 patent/WO2020046387A1/en unknown
- 2018-08-31 US US16/611,159 patent/US11501769B2/en active Active
- 2018-08-31 CN CN201880098327.3A patent/CN112889022A/en active Pending
- 2018-08-31 EP EP18778686.8A patent/EP3844605A1/en not_active Withdrawn
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150170648A1 (en) * | 2013-12-17 | 2015-06-18 | Google Inc. | Ebook interaction using speech recognition |
US20160225187A1 (en) * | 2014-11-18 | 2016-08-04 | Hallmark Cards, Incorporated | Immersive story creation |
US20180018373A1 (en) * | 2016-07-18 | 2018-01-18 | Disney Enterprises, Inc. | Context-based digital assistant |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11501769B2 (en) * | 2018-08-31 | 2022-11-15 | Google Llc | Dynamic adjustment of story time special effects based on contextual data |
US20220165281A1 (en) * | 2019-04-02 | 2022-05-26 | Nokia Technologies Oy | Audio codec extension |
CN114352965A (en) * | 2022-01-21 | 2022-04-15 | 杭州鸿澄智家科技有限公司 | Collision desk lamp with adjustable color |
Also Published As
Publication number | Publication date |
---|---|
EP3844605A1 (en) | 2021-07-07 |
US11501769B2 (en) | 2022-11-15 |
US20210183378A1 (en) | 2021-06-17 |
WO2020046387A1 (en) | 2020-03-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11501769B2 (en) | Dynamic adjustment of story time special effects based on contextual data | |
US11749279B2 (en) | Detection of story reader progress for pre-caching special effects | |
US11862192B2 (en) | Algorithmic determination of a story readers discontinuation of reading | |
CN107516511B (en) | Text-to-speech learning system for intent recognition and emotion | |
KR102582291B1 (en) | Emotion information-based voice synthesis method and device | |
JP6550068B2 (en) | Pronunciation prediction in speech recognition | |
US8036894B2 (en) | Multi-unit approach to text-to-speech synthesis | |
CN111312231B (en) | Audio detection method and device, electronic equipment and readable storage medium | |
US11074909B2 (en) | Device for recognizing speech input from user and operating method thereof | |
US11810471B2 (en) | Computer implemented method and apparatus for recognition of speech patterns and feedback | |
US10431188B1 (en) | Organization of personalized content | |
US11526671B2 (en) | Reading progress estimation based on phonetic fuzzy matching and confidence interval | |
CN112799630A (en) | Creating a cinematographed storytelling experience using network addressable devices | |
US11176943B2 (en) | Voice recognition device, voice recognition method, and computer program product | |
US20240135960A1 (en) | Algorithmic determination of a story readers discontinuation of reading | |
US11977816B1 (en) | Time-based context for voice user interface |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |